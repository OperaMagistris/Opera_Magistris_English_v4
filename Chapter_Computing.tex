 	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Numerical Methods/Analysis}\label{numerical methods}
	\lettrine[lines=4]{\color{BrickRed}T}{he} numerical analysis is a mathematical discipline. It is interested in both theoretical foundations as the implementation methods to resolve by purely numerical calculations and empirical approach, mathematical analysis problems.\\

	\textbf{Definition (\#\thesection.\mydef):} "\NewTerm{Numerical Methods/Analysis}\index{numerical methods/analysis}" is the study of algorithms or empirical scientific methods for solving or analysing mathematical continuous or discrete problems. This means that it mainly deals to respond numerically to real or complex variables questions like numerical linear algebra over the real or complex fields, looking for numerical solutions of differential equations and other problems occurring in the physical sciences or financial/statistical engineering.
	
	Some continuous mathematical problems can be solved exactly by an algorithm. These algorithms are then named "\NewTerm{direct methods}\index{direct methods}". Examples are the elimination of Gauss-Jordan for solving a system of linear equations or the simplex algorithm of linear programming (see further below). However, for some problems no direct method are known (and is even proved that for a class of problems known as "NP complete" - see further below - there is no direct calculation with finished algorithm in a polynomial time). In such cases, it is sometimes possible to use an iterative method to attempt to determine an approximation of the solution. Such a method starts from a guessed value or roughly estimated one and finds successive approximations that should converge to the solution under certain conditions. Even when a direct method exists, however, an iterative method may be preferable because it is often more effective and often more stable (in particular it allows most often to correct minor errors in intermediate calculations). 
	
	The use of numerical analysis has been greatly facilitated by modern computers. The Increasing availability and power of computers since the second half of the 120th century (holocene calendar) allowed the application of numerical methods in many scientific, technical and economic, often with revolutionary, accurate and significant effects.
	\begin{figure}[H]
		\centering
		\fbox{\includegraphics[scale=0.75]{img/computing/meaning_life.eps}}
	\end{figure}
	In numerical simulations of physical systems (multi-physics), the initial conditions are very important in solving differential equations (see the various sections of this book where chaotic effects appears). The fact that we can not know them exactly implies that the results of calculations can never be perfectly accurate (we know this fact very well for the weather forecasting which is the most glaring example known). This effect is a consequence of the results of fundamental physics (based on pure mathematics) which demonstrates that we can not perfectly know a system by performing measurements since it directly disrupts it (Heisenberg uncertainty principle as see in the section of Quantum Wave Theory) and these disturbances are the subject of Chaos Theory (classical or quantum).

With new computer tools available in the early 121st century (holocene calendar), it became practical and exciting to know the numerical methods to have fun with some softwares or programming languages (OpenGL, 3D Studio Max, Blender, Maple, MATLAB™, Mathematica, Comsol, R, C++, Python, etc.) to simulate 2D or 3D physical systems graphically.

	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} Many numerical methods used in computer science are based on arguments which we have already been studied in other sections of this books. We won't come back on this methods.\\
	
	\textbf{R2.} This section being on the boundary between engineering and Applied Mathematics, we decided to give some application examples of developed tools with various programming languages or software.\\
	
	\textbf{R3.} Many of the techniques presented below are available as complete code in the C++ book \cite{oliveira2015practical} that we strongly recommend to the reader.
	\end{tcolorbox}	
	
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.]  An "\NewTerm{algorithm}\index{algorithm}\label{algorithm}" is a finite sequence of rules to be applied in a specific order in a finite number of data to arrive in a finite number of steps (including the amount, or conversely the execution time is defined by the term "\NewTerm{cost}\index{algorithm cost}") to a certain result, independently of the data type.
		
		\item[D2.] The algorithms are integrated into computers through "\NewTerm{programs}\index{programs}" (including stuff like "functions", "objects", "classes", "methods", "properties", "pointers", etc. but this is more related to a programming language course) that are the realization (implementation) of an algorithm using a given language (on a given architecture). This is the implementation of the principle.
	\end{enumerate}
	
	Axioms of programming (anecdotal):

	\begin{enumerate}
		\item[A1.] More we write code, more errors we will produce.
		
		\item[A2.] There are no programs without possible errors (due to the program itself, to the underlying electronics or most often to the user himself).
	\end{enumerate}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Basically there a is minimum steps to follow when developing an algorithm and its corresponding code. A very good process to follow is the one proposed by the ISO/CEI 9126 norm.
	\end{tcolorbox}	
	\begin{theorem}
	There exist an infinite number of algorithms.
	\end{theorem}
	\begin{dem}
	For every integer $i \geq 1$, consider the following algorithm $A_i$ : Declare variable $x$, then assign $x$ the value $i$. Terminate.

	Every algorithm defined, as above, terminates in finite time. Every algorithm defined above is distinct.

	Notice that we have defined an algorithm, one for each positive integer (in technical language, there is a correspondence between the family of algorithms we defined above and $\mathbb{Z}^{+}$). There are infinitely many positive integers, therefore, there are infinitely many algorithms (for which at least a subset of them include the family of the algorithms we defined above).
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}

	When developing a scientific algorithm, it may be interesting, rigorous and even an obligation in very high level cases to analyse the "complexity" of the algorithm. Without going too far, let's see what it is exactly:
	
	\subsection{Computer Representation of Numbers}\label{computer representation of numbers}
	In a classical computer numbers are represented by binary digits $0$ and $1$. Computers employ binary arithmetic for performing operations on numbers. Since it gets cumbersome to display large numbers in binary form computers usually display them in hexadecimal or octal or decimal system. All of these number systems are positional 	systems. In a positional system a number is represented by a set of symbols. Each of these symbols denote a particular value depending on its position. The number of symbols  used in a positional system depends on its 'base'. Let us now discuss about	various positional number systems:
	
	\subsubsection{Decimal System}
	
	The decimal system uses $10$ as its base value and employs ten symbols $0$ to $9$ in representing numbers. Let us consider a decimal number $7402$ consisting of four symbols $7$,$4$,$0$,$2$. In terms of base $10$ it can be expressed as follows:
	$$7402=7\times10^{3}+4\times10^{2}+0\times10^{1}+2\times10^{0}$$
	So each of the symbols from a set of symbols denoting a number is multiplied with power of the base ($10$) depending on its position counted from the right. The count always begins with $0$.
	
	In general a decimal number $d_{m}d_{m-1}\ldots d_{1}d_{0}$ consisting of $(m+1)$ symbols can be expressed as:
	$$
	d_{m}\times10^{m}+d_{m-1}\times10^{m-1}+\ldots .+d_{1}\times10^{1}+d_{0}\times10^{0}=\sum\limits_{i=0}^{m}
	d_{i}10^{i}
	$$
	where $0\leq d_{i}\leq 9$ with $i=0,1,\ldots, m$
	
	Similarly, a fractional part of a decimal number can be expressed as $\sum\limits_{i=1}^{m}d_{i}10^{-i}$
	
	\subsubsection{Binary system} 
	Binary system is the positional system 	consisting of two symbols i.e. $\{0,1\}$, named "\NewTerm{bits}\index{bits}\index{binary digit}\label{bit}", and '$2$' as its base. Any binary number $d_{m}d_{m-1}\ldots d_{1}d_{0}$ actually represents a decimal value given by
	$$
	d_{m}2^{m}+d_{m-1}2^{m-1}+\ldots +d_{0}2^{0}=\sum\limits_{i=0}^{m}
	d_{i}2^{i}
	$$
	where $d_{i}=0\quad or\quad 1, \quad i=0,1,..m. $
	
	Consider the binary number 10101. The decimal equivalent of 10101 is given by
	\begin{eqnarray}
	(10101)_{2} & = & 1\times 2^{4}+0\times 2^{3}+1\times
	2^{2}+0\times 2^{1}+1\times 2^{0} \nonumber \\
	& = & 16+0+4+0+1=(21)_{10} \nonumber
	\end{eqnarray}
	
	Here are the representation of some integer positive values in binary notation:
	\begin{alignat*}{6}
	1&\qquad&1& \qquad\quad\qquad&  13&\qquad&1101&\qquad\quad\qquad &25&\quad&11001&\\
	2&&10&&  14&&1110&&   26&&11010&\\
	3&&11&&  15&&1111&&   27&&11011&\\
	4&&100&& 16&&10000&&   28&&11100&\\
	5&&101&& 17&&10001&&   29&&11101&\\
	6&&110&& 18&&10010&&   30&&11110&\\
	7&&111&& 19&&10011&&   31&&11111&\\
	8&&1000&&20&&10100&&  32&&100000&\\
	9&&1001&&21&&10101&&  33&&100001&\\
	10&&1010&&22&&10110&& 34&&100010&\\
	11&&1011&&23&&10111&& 35&&100011&\\
	12&&1100&&24&&11000&& 36&&100100&
	\end{alignat*}

	Notice that:
	\begin{itemize}
		\item The  number of different values representable in $n$ bits is $2^n$
		
		\item An $n$-bit binary number $X=x_{n-1}x_{n-2}\cdots x_1 x_0$ can represent  any integer value in the range $0 \le X \le 2^n-1$ (e.g., if $n=3$,  then $0 \le X \le 7$). Note that $n+1$ bits are needed to represent the value $X=2^n$
		
		\item The highest (left-most) bit $x_{n-1}$ of an $n$-bit number is named the "\NewTerm{most significant bit}\index{most significant bit}" (MSB) and the lowest bit (right-most) $x_0$ the "\NewTerm{least significant bit}\index{least significant bit}" (LSB)
	\end{itemize}
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/arithmetics/binary_joke.jpg}
	\end{figure}
	It should be obvious at this level for the reader that:
	\begin{itemize}
		\item $1000$ has $10$ bits because $512\leq 1000\leq 1024$, or $2^9\leq 1000\leq 2^{10}-1$
		\item $1344$ has $11$ bits because $1024\leq 1344 \leq 2047$, or $2^{10}\leq 1344\leq 2^{11}-1$
		\item $2527$ has $12$ bits because $2048\leq 2527\leq 4095$, or $2^{11}\leq 2527\leq 2^{12}-1$
		\item $5019$ has $13$ bits because $4096\leq 5019\leq 8191$, or $2^{12}\leq 5019\leq 2^{13}-1$
		\item $9999$ has $14$ bits because $8192\leq 9999\leq 16383$, or $2^{13}\leq 9999\leq 2^{14}-1$
	\end{itemize}
	Hence the minimum number of bit required $b_\mathrm{min}$ for a $d$ digit integer is computed simply by using the specific number formula on the minimum $d$ digit value:
	
	So for example, in 12018 (holocene calendar) the biggest prime number (important in cryptography) discovered had $23,249,425$ digits, thus corresponding using the above relation to $b_\mathrm{min}=25$.
	
	What we have seen so far works only for integer (i.e. "fixed point numbers"). What about decimal numbers??? This is what we will focus on right now!
	
	\paragraph{Floating Point (Real Numbers)}\mbox{}\\\\\
	The encoding scheme for floating point numbers is more complicated than for fixed point. The basic idea is the same as used in scientific notation, where a mantissa is multiplied by ten raised to some exponent. For instance, $5.4321 \cdot 10^6$, where $5.4321$ is the mantissa and $6$ is the exponent. Scientific notation is exceptional at representing very large and very small numbers. For example: $1.2 \cdot 10^{50}$, the number of atoms in the Earth, or $2.6 \cdot 10^{-23}$, the distance a turtle crawls in one second, compared to the diameter of our galaxy. Notice that numbers represented in scientific notation are normalized so that there is only a single nonzero digit left of the decimal point. This is achieved by adjusting the exponent as needed.

	Floating point representation is similar to scientific notation, except everything is carried out in base two, rather than base ten. While several similar formats are in use, the most common is ANSI/IEEE Std. 754-1985. This standard defines the format for $16$ bits number named "half-precision", $32$ bit numbers named "single precision", as well as $64$ bit numbers named "double precision". As shown in the figure below, the $32$ bits used in single precision are divided into three separate groups: bits $0$ through $22$ form the mantissa, bits $23$ through $30$ form the exponent, and bit $31$ is the sign bit. These bits form the floating point number, $v$, by the following relation:
	
	More generally:
	
	Also sometimes denoted:
	
	So the main relation is:
	
	In other words, $M=1+b_{22} 2^{-1}+b_{21} 2^{-2}+b_{20} 2^{-3} \ldots$. If bits $0$ through $22$ are all zeros, $M$ takes on the value of one. If bits $0$ through $22$ are all ones, $M$ is just a hair under two, i.e., $2-2^{-23}$.
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1500); %set diagram left start at 0, and has height of 1500
		
		%Shape: Grid [id:dp064665237304389] 
		\draw  [draw opacity=0] (39,37) -- (617,37) -- (617,54) -- (39,54) -- cycle ; \draw   (56,37) -- (56,54)(73,37) -- (73,54)(90,37) -- (90,54)(107,37) -- (107,54)(124,37) -- (124,54)(141,37) -- (141,54)(158,37) -- (158,54)(175,37) -- (175,54)(192,37) -- (192,54)(209,37) -- (209,54)(226,37) -- (226,54)(243,37) -- (243,54)(260,37) -- (260,54)(277,37) -- (277,54)(294,37) -- (294,54)(311,37) -- (311,54)(328,37) -- (328,54)(345,37) -- (345,54)(362,37) -- (362,54)(379,37) -- (379,54)(396,37) -- (396,54)(413,37) -- (413,54)(430,37) -- (430,54)(447,37) -- (447,54)(464,37) -- (464,54)(481,37) -- (481,54)(498,37) -- (498,54)(515,37) -- (515,54)(532,37) -- (532,54)(549,37) -- (549,54)(566,37) -- (566,54)(583,37) -- (583,54)(600,37) -- (600,54) ; \draw    ; \draw   (39,37) -- (617,37) -- (617,54) -- (39,54) -- cycle ;
		%Shape: Brace [id:dp8423798955622701] 
		\draw   (60,65) .. controls (60,69.67) and (62.33,72) .. (67,72) -- (115.43,72) .. controls (122.1,72) and (125.43,74.33) .. (125.43,79) .. controls (125.43,74.33) and (128.76,72) .. (135.43,72)(132.43,72) -- (183.86,72) .. controls (188.53,72) and (190.86,69.67) .. (190.86,65) ;
		%Straight Lines [id:da39949544821803484] 
		\draw    (42.86,93.55) -- (47.6,58.53) ;
		\draw [shift={(47.86,56.55)}, rotate = 97.7] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Brace [id:dp9240066489112826] 
		\draw   (201,65) .. controls (201,69.67) and (203.33,72) .. (208,72) -- (395.43,72) .. controls (402.1,72) and (405.43,74.33) .. (405.43,79) .. controls (405.43,74.33) and (408.76,72) .. (415.43,72)(412.43,72) -- (602.86,72) .. controls (607.53,72) and (609.86,69.67) .. (609.86,65) ;
		
		% Text Node
		\draw (602,57) node [anchor=north west][inner sep=0.75pt]  [font=\tiny] [align=left] {LSB};
		% Text Node
		\draw (193,57) node [anchor=north west][inner sep=0.75pt]  [font=\tiny] [align=left] {MSB};
		% Text Node
		\draw (175,57) node [anchor=north west][inner sep=0.75pt]  [font=\tiny] [align=left] {LSB};
		% Text Node
		\draw (56,57) node [anchor=north west][inner sep=0.75pt]  [font=\tiny] [align=left] {MSB};
		% Text Node
		\draw (41,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$31$};
		% Text Node
		\draw (56,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$30$};
		% Text Node
		\draw (74,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$29$};
		% Text Node
		\draw (91,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$28$};
		% Text Node
		\draw (108,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$27$};
		% Text Node
		\draw (125,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$26$};
		% Text Node
		\draw (142,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$25$};
		% Text Node
		\draw (158,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$24$};
		% Text Node
		\draw (176,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$23$};
		% Text Node
		\draw (98,86) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {\begin{minipage}[lt]{38.62pt}\setlength\topsep{0pt}
		\begin{center}
		{\footnotesize EXPONENT}\\{\footnotesize $\displaystyle 8$ bits}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (31,99) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {\begin{minipage}[lt]{25pt}\setlength\topsep{0pt}
		\begin{center}
		{\footnotesize SIGN}\\{\footnotesize 1 bits}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (193,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$22$};
		% Text Node
		\draw (211,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$21$};
		% Text Node
		\draw (227,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$20$};
		% Text Node
		\draw (244,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$21$};
		% Text Node
		\draw (261,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$20$};
		% Text Node
		\draw (277,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$19$};
		% Text Node
		\draw (294,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$18$};
		% Text Node
		\draw (311,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$17$};
		% Text Node
		\draw (328,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$16$};
		% Text Node
		\draw (345,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$15$};
		% Text Node
		\draw (362,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$14$};
		% Text Node
		\draw (379,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$13$};
		% Text Node
		\draw (380,86) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {\begin{minipage}[lt]{36.08pt}\setlength\topsep{0pt}
		\begin{center}
		{\footnotesize MANTISSA}\\{\footnotesize $\displaystyle 23$ bits}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (396,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$12$};
		% Text Node
		\draw (414,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$11$};
		% Text Node
		\draw (430,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$10$};
		% Text Node
		\draw (451,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$9$};
		% Text Node
		\draw (468,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$8$};
		% Text Node
		\draw (486,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$7$};
		% Text Node
		\draw (502,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$6$};
		% Text Node
		\draw (518,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$5$};
		% Text Node
		\draw (535,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$4$};
		% Text Node
		\draw (553,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$3$};
		% Text Node
		\draw (570,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$2$};
		% Text Node
		\draw (587,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$1$};
		% Text Node
		\draw (604,40.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Floating point (real numbers) in simple precision}
	\end{figure}
	Or for the half-precision system:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1500); %set diagram left start at 0, and has height of 1500
		
		%Shape: Grid [id:dp064665237304389] 
		\draw  [draw opacity=0] (170.86,29) -- (442.86,29) -- (442.86,46) -- (170.86,46) -- cycle ; \draw   (187.86,29) -- (187.86,46)(204.86,29) -- (204.86,46)(221.86,29) -- (221.86,46)(238.86,29) -- (238.86,46)(255.86,29) -- (255.86,46)(272.86,29) -- (272.86,46)(289.86,29) -- (289.86,46)(306.86,29) -- (306.86,46)(323.86,29) -- (323.86,46)(340.86,29) -- (340.86,46)(357.86,29) -- (357.86,46)(374.86,29) -- (374.86,46)(391.86,29) -- (391.86,46)(408.86,29) -- (408.86,46)(425.86,29) -- (425.86,46) ; \draw    ; \draw   (170.86,29) -- (442.86,29) -- (442.86,46) -- (170.86,46) -- cycle ;
		%Shape: Brace [id:dp8423798955622701] 
		\draw   (198.86,57) .. controls (198.86,61.67) and (201.19,64) .. (205.86,64) -- (223.36,64) .. controls (230.03,64) and (233.36,66.33) .. (233.36,71) .. controls (233.36,66.33) and (236.69,64) .. (243.36,64)(240.36,64) -- (260.86,64) .. controls (265.53,64) and (267.86,61.67) .. (267.86,57) ;
		%Straight Lines [id:da39949544821803484] 
		\draw    (172.86,84.55) -- (177.6,49.53) ;
		\draw [shift={(177.86,47.55)}, rotate = 97.7] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Brace [id:dp9240066489112826] 
		\draw   (279.86,57) .. controls (279.86,61.67) and (282.19,64) .. (286.86,64) -- (347.36,64) .. controls (354.03,64) and (357.36,66.33) .. (357.36,71) .. controls (357.36,66.33) and (360.69,64) .. (367.36,64)(364.36,64) -- (427.86,64) .. controls (432.53,64) and (434.86,61.67) .. (434.86,57) ;
		
		% Text Node
		\draw (427,49) node [anchor=north west][inner sep=0.75pt]  [font=\tiny] [align=left] {LSB};
		% Text Node
		\draw (271.86,49) node [anchor=north west][inner sep=0.75pt]  [font=\tiny] [align=left] {MSB};
		% Text Node
		\draw (255.86,49) node [anchor=north west][inner sep=0.75pt]  [font=\tiny] [align=left] {LSB};
		% Text Node
		\draw (187.86,49) node [anchor=north west][inner sep=0.75pt]  [font=\tiny] [align=left] {MSB};
		% Text Node
		\draw (208,78) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {\begin{minipage}[lt]{38.62pt}\setlength\topsep{0pt}
		\begin{center}
		{\footnotesize EXPONENT}\\{\footnotesize 5 bits}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (161,90) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {\begin{minipage}[lt]{25pt}\setlength\topsep{0pt}
		\begin{center}
		{\footnotesize SIGN}\\{\footnotesize 1 bits}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (170,32.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$15$};
		% Text Node
		\draw (187,32.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$14$};
		% Text Node
		\draw (204,32.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$13$};
		% Text Node
		\draw (333,78) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {\begin{minipage}[lt]{36.08pt}\setlength\topsep{0pt}
		\begin{center}
		{\footnotesize MANTISSA}\\{\footnotesize $\displaystyle 10$ bits}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (221,32.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$12$};
		% Text Node
		\draw (239,32.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$11$};
		% Text Node
		\draw (255,32.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$10$};
		% Text Node
		\draw (276,32.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$9$};
		% Text Node
		\draw (293,32.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$8$};
		% Text Node
		\draw (311,32.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$7$};
		% Text Node
		\draw (327,32.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$6$};
		% Text Node
		\draw (343,32.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$5$};
		% Text Node
		\draw (360,32.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$4$};
		% Text Node
		\draw (378,32.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$3$};
		% Text Node
		\draw (395,32.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$2$};
		% Text Node
		\draw (412,32.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$1$};
		% Text Node
		\draw (429,32.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Floating point (real numbers) in half precision}
	\end{figure}
	The term: $(-1)^S$, simply means that the sign bit, $S$, is $0$ for a positive number and $1$ for a negative number. The variable, $E$, is the number between $0$ and $255$ represented by the eight exponent bits. Subtracting $127$ from this number allows the exponent term to run from $10^{-127}$ to $10^{128}$ other words, the exponent is stored in offset binary with an offset of $127$.

	The mantissa, $M$, is formed from the $23$ bits as a binary fraction. For example, the decimal fraction: $2.783$ , is interpreted: $2+7 / 10+$ $8 / 100+3 / 1000$. The binary fraction: $1.0101$ , means: $1+0 / 2+1 / 4+0 / 8+1 / 16$. Floating point numbers are normalized in the same way as scientific notation, that is, there is only one nonzero digit left of the decimal point (named a" binary point" in base $2$). Since the only nonzero number that exists in base two is $1$, the leading digit in the mantissa will always be a $1$, and therefore does not need to be stored. Removing this redundancy allows the number to have an additional one bit of precision. The $23$ stored bits, referred to by the notation: $b_{22}, b_{21}, b_{21}, \ldots, b_0$, form the mantissa according to:
	
	Using this encoding scheme, the largest number that can be represented is: $\pm\left(2-2^{-23}\right) \cdot 2^{128}= \pm 6.8 \cdot 10^{38}$. Likewise, the smallest number that can be represented is: $\pm 1.0 \cdot 2^{-127}= \pm 5.9 \cdot 10^{-39}$. The IEEE standard reduces this range slightly to free bit patterns that are assigned special meanings. In particular, the largest and smallest numbers allowed in the standard are $\pm 3.4 \cdot 10^{38}$ and $\pm 1.2 \cdot 10^{-38}$ respectively. The freed bit patterns allow three special classes of numbers: 
	\begin{itemize}
		\item $\pm 0$ is defined as all of the mantissa and exponent bits being zero. 
		
		\item $\pm \infty$ is defined as all of the mantissa bits being zero, and all of the exponent bits being one.
		
		\item A group of very small unnormalized numbers between $\pm 1.2\cdot10^{-38}$ and $\pm 1.4 \cdot 10^{-45}$.
	\end{itemize}
	These are lower precision numbers obtained by removing the requirement that the leading digit in the mantissa be a one. Besides these three special classes, there are bit patterns that are not assigned a meaning, commonly referred to as NANs (Not A Number).

	The IEEE standard for double precision simply adds more bits to the single precision format. Of the $64$ bits used to store a double precision number, bits $0$ through $51$ are the mantissa, bits $52$ through $62$ are the exponent, and bit $63$ is the sign bit. As before, the mantissa is between one and just under two, i.e., $M=1+b_{51} 2^{-1}+b_{50} 2^{-2}+b_{49} 2^{-3} \ldots$. The $11$ exponent bits form a number between $0$ and $2047$ , with an offset of $1023$, allowing exponents from $2^{-1023}$ to $2^{1024}$. The largest and smallest numbers allowed are $\pm 1.8\cdot 10^{308}$ and $\pm 2.2 \cdot 10^{-308}$, respectively. These are incredibly large and small numbers! It is quite uncommon to find an application where single precision is not adequate. You will probably never find a case where double precision limits what you want to accomplish.
	
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Caution! Let's have a look at a number which is fine for us, but has the floating point issue for the personal computer: $1 / 10$. For us Homo Sapiens, $1/10$ is $0.1$, it's easy, only one decimal! For a digital computer, $1/10$ in floating precision is $0.0001100110011001101 \ldots$ These decimals have a recurring combination of numbers, ie $0011$. The computer will not calculate every single decimals as it is infinite! It has to stop at some point and round the number of decimals. This rounding is the floating point lack of precision. For comparison, in base $10$, we can't precisely define $1 / 3$, which is $0.3333$ recurring $3$. Overall, many spreadsheet and business intelligence softwares decimal issue is caused by a limitation of floating point precision. This is not an error per se, but can cause one. Understanding the logic of decimals-rounding will help you find the solution applicable to your situation.
	\end{tcolorbox}
	
	\paragraph{Binary arithmetic (for fixed point/integers)}\mbox{}\\\\\
	Here are the addition facts that you need when adding numbers in binary notation:
	
	
	Doing binary addition is like doing decimal addition excepted that we work on a binary set. 
	\newcommand*{\carry}[1][1]{\overset{#1}}
	\newcolumntype{B}[1]{r*{#1}{@{\,}r}}
	\begin{equation*}
		\begin{array}{B3}
		    \carry 0 & \carry 1\carry 1\carry 0\carry 0 & \carry 1\carry 0\carry 01 \\
		      {} + 0 & 1111 & 1111 \\ \hline
		           1 & 1100 & 1000 \\
		\end{array}
	\end{equation*}
	Here are the subtractions facts that you need when subtracting numbers in binary notation:
	
	The fact that $10-1=1$ will lead to borrowing since you can not take $1$ from $0$ for a certain place value, and so you must borrow from the $1$ in the next place value, much like how you need to borrow in decimal notation:
	\begin{equation*}
		\begin{array}{B3}
		    		\carry 1 \carry 0\carry 01 \\
		      	{} - 0110 \\ \hline
		            0011 \\
		\end{array}
	\end{equation*}
	We will come back more in details on binary arithmetic during our study of Boolean Algebra in the section of Logical Systems.
	
	\subsubsection{Hexadecimal System} 
	
	The Hexadecimal system is the 	positional system consisting of sixteen symbols, $0$,$1$,$2$,...,$9$,$A$,$B$,$C$,$D$,$E$,$F$, and '$16$' as its base. Here the symbols $A$ denotes $10$, $B$ denotes $11$ and so on. The decimal equivalent of the 	given hexadecimal number $d_{m} d_{m-1}\ldots d_{0}$ is given by:
	
	 For example consider $(15ACB)_{16}$:
	
	We can convert a binary number directly to a hexadecimal number by grouping the binary digits, starting from the right, into sets of four and converting each group to its equivalent hexadecimal digit. If in such a grouping the last set falls short of four binary digits then do the obvious thing of prefixing it with adequate number of binary digit '$0$'. For example let us find the 	hexadecimal equivalent of $(111\;011\;0101\;0010\;1110)_{2}$:
	
	The vice versa is also true.
	
	\subsubsection{Octal System}  
	The octal system is the positional system that uses $8$ as its base and $\{0,1,\ldots, 7\}$ as its symbol set of size $8$. The decimal equivalent of an octal number $(d_{m}d_{m-1}\ldots d_{0})_{8}$ is given by:
	
	 For example consider $(6741)_{8}$:
	 
	
	We can get the octal equivalent of a binary number by grouping the binary digits, starting from the right, into sets of three binary digits and converting each of these sets to its octal equivalent.
	
	If such a grouping results in a last set having less number of digits it may be prefixed with adequate number of binary digit 0.
	
	As an example the octal equivalent of $(1010\;110\;111\;001)_{2}$.
	
	So we have:
	
	
	\pagebreak
	\subsubsection{Conversion of decimal system to non-decimal system:}
	To convert a decimal number to a number of any other system we should consider the integer and fractional parts separately and follow the following procedure:
	
	Conversion of integer part:
	\begin{enumerate}
		\item Consider the integer part of a given decimal number and divide it by the base $b$ of the new number system. The remainder will constitute the rightmost digit of the integer part of the new number.

		\item Next divide the quotient again by the base $b$. The remainder will constitute second digit from the right in the new system
	\end{enumerate}
	
	Continue this process until we end up with a zero-quotient. The last remainder is the leftmost digit of the new number.
	
	Conversion of fractional part:
	\begin{enumerate}
		\item Consider the fractional part of the given decimal number and multiply it with the base $b$ of the new system. The integral part of the product constitutes the leftmost digit of the fractional part in the new system.

		\item Now again multiply the fractional part resulting in step (a) by the base $b$ of the new system. The integral part of the resultant product is the second digit from the left in the new system.
	\end{enumerate} 
	
	Repeat the above step until we encounter a zero-fractional part or a duplicate fractional part. The integer part of this last product will be the rightmost digit of the fractional part of the new number.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	We want to convert $54.45$ into its binary equivalent.\\
	
	\begin{enumerate}
		\item Consider the integer part i.e. $54$ and apply the steps listed under conversion of integer part i.e.

		\item conversion of fractional part:
		\vskip 10pt
	
		\begin{center}
			\begin{tabular}{ccccc}
			                  &   &  Product & integral part &
			Binary number \\ \hline $0.45$ $\times$ $2$   & = &  $0.90$    &   $0$ &
			$(.01\overline{1100})_2$ \\
			0.9 $\times$ 2   & = &  1.80    &    1 &  \\
			0.8 $\times$ 2   & = &  1.6     &    1 &  \\
			0.6 $\times$ 2   & = &  1.2     &    1 &  \\
			0.2 $\times$ 2   & = &  0.4     &    0 &  \\
			0.4 $\times$ 2   & = &  0.8     &    0 &  \\
			0.8 $\times$ 2   & = &  1.6     &    1 &  \\
			0.6 $\times$ 2   & = &  1.2     &    1 &  \\
			0.2 $\times$ 2   & = &  0.4     &    0 &  \\
			0.4 $\times$ 2   & = &  0.8     &    0 &  \\
			0.8 $\times$ 2   & = &  1.6     &    1 &  \\
			$\dots$          &   &  $\dots$ &    $\dots$ &  \\
			$\dots$          &   &  $\dots$ &    $\dots$ &  \\
			\hline
			\end{tabular}
		\end{center}
	\end{enumerate} 
	Therefore:
	
	and finally:
	
	\end{tcolorbox}
	
	Here the overbar denotes the repetition of the binary digits.
	
	Using binary system as an intermediate stage we can easily convert octal numbers to hexadecimal numbers and vice-versa:
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. $(423)_{8}=(100\quad 010\quad 011)_{2}$
	
	$\qquad\qquad\qquad=(0001\quad 0001\quad 0011)_{2}=(113)_{16}$\\
	
	E2. $(93AF)_{16}=(1001\quad 0011\quad 1010\quad 1111)_{2}$
	
	$\qquad\qquad\qquad=(010\quad 010\quad 011\quad 101\quad 011\quad 110)_{2}$
	
	$\qquad\qquad\qquad=(223536)_{8}$
	\end{tcolorbox}
	
	In the above two examples we have grouped the binary digits suitably either to quadruplets or triplets to convert octal to hexadecimal and hexadecimal to octal numbers respectively.



	\pagebreak	
	\subsection{Algorithm Complexity}\label{algorithm complexity}

	\textbf{Definition (\#\thesection.\mydef):} The "\NewTerm{complexity}\index{complexity}" of an algorithm is the measure of the number of fundamental operations it performs in the worst case on a dataset.

Measure the exact complexity is most of time irrelevant because often too complex given the size of the programs (too big algorithms). To avoid calculating in detail the complexity of an algorithm, we identify the fundamental operations. These basic operations can be: an assignment, a comparison between two variables, an arithmetic operation between two variables, etc.

Thus, the classical used hypothesis for the calculation of the complexity are:

	\begin{enumerate}
		\item[H1.] The four fundamental operations have the same time cost: $+ \equiv - \equiv \times \equiv \div$
		\item[H2.] A memory access has a cheaper time cost than an arithmetic operation.
		\item[H3.] A comparison check has a cheaper time cost than an arithmetic operation.
		\item[H4.] We work with only one single processor.
	\end{enumerate}
	
\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] We note $D_n$ the sets of data of size $n$ and $T(n)$ the cost (in time) of the algorithm on the data or the data set of size $n$.
		
		\item[D2.] The "\NewTerm{complexity at the best}\index{complexity at the best}" is given by the function:
			
		This is the smallest time that will need an algorithm to run on a data set (lines of code) of fixed size, here equal to $n$, that the cost (duration) execution is $C(d)$. This is a lower bound on the complexity of the algorithm on a data set of size $n$.
		
		\item[D3.] The "\NewTerm{complexity at worst}\index{complexity at worst}" (the most interesting for the practitioner because it is the one to minimize!):
			
		This is the biggest time that will need an algorithm to run on a data set (lines of code) of fixed size, here equal to $n$, that the cost (duration) execution is $C(d)$. This is a upper bound on the complexity of the algorithm on a data set of size $n$. The algorithm will always finish at this time or before but never after.
		\item[D4.] The "\NewTerm{average complexity}\index{average complexity}":
			
This is the average of the complexities of the algorithm on the data sets of size $n$ (strictly speaking, we must obviously take into account the probability of occurrence of each of the data sets). This average reflects the general behaviour of the algorithm if extreme cases are rare or complexity varies slightly depending on the data. However, the complexity in practice on a particular data set may be significantly greater than the average complexity; in this case the average complexity does not give a good indication of the behaviour of the algorithm.
	
		\item[D5.] The "\NewTerm{order of complexity}\index{order of complexity}" is defined as the complexity at worst of an algorithms that contains several terms (additions or subtractions) so that we only keep the term that is growing the fastest. Thus, an algorithm having a complexity of type:
			
will be said to have a complexity of order $\mathcal{O}(n!)$. This is the "\NewTerm{big $\mathcal{O}$ notation}". It is a member of a family of notations invented by Paul Bachmann, Edmund Landau, and others, collectively named "\NewTerm{Bachmann–Landau notation}\index{Bachmann–Landau notation}" or "\NewTerm{asymptotic notation}\index{asymptotic notation}".
	\end{enumerate}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Just as worst cases are given in terms of upper bounds, best cases are given in terms of lower bounds, which are specified with "\NewTerm{big $\Omega$ notation}". A best case of $\Omega(n)$ would mean that the function is always at least linear, but could grow faster.
	\end{tcolorbox}

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	Consider $N$ as the size of the data. For a decimal number this is the numbers $N$ of digits. Consider two decimal numbers like $0.a_1a_2a_3\ldots $ and $0.b_1b_2b_3\ldots $\\
	
	E1. Addition has linear complexity because in developed way addition is expressed as (\SeeChapter{see section Numbers page \pageref{number power decomposition}}):\\
		
	
	E2. Multiplication has quadratic complexity because in developed way multiplication is expressed as:\\
	
	\end{tcolorbox}

\textbf{Definition (\#\thesection.\mydef):} An algorithm is said to be an "\NewTerm{optimal algorithm}\index{optimal algorithm}" if its complexity has the minimal complexity among all other algorithms in its class.

As we made it understand implicitly earlier, we focus almost exclusively on the time complexity of the algorithms. Sometimes it is interesting to focus on other of their characteristics, such as space complexity (size of the memory used), the bandwidth required, etc.

For the result of the analysis of an algorithm to be relevant, we must have a model of a machine on which the algorithm will be implemented (as a program). We usually take as a reference, the "random access machine (RAM)" with a single processor where instructions are executed one after the other, without concurrent operations and without stochastic processes (in contrast to possible future quantum computers).

Most common algorithms can be classified into a number of broad classes of complexity whose order $\mathcal{O}$ varies somehow:
	\begin{enumerate}
		\item The algorithms with "\NewTerm{constant complexity $\mathcal{O}(1)$}\index{complexity!constant complexity}" that just do a boolean control (comparison).
		
		\item The algorithms with "\NewTerm{linear complexity $\mathcal{O}(n)$}\index{complexity!linear complexity}" and those in complexity in $\mathcal{O} (n \log (n))$ that are considered as fast.
		
		\item The algorithms with "\NewTerm{polynomial complexity in $\mathcal{O}(n^k)$}\index{complexity!polynomial complexity}" (for $k>3$ are considered as slow).
		
		\item The algorithms with "\NewTerm{sub-linear complexity}\index{complexity!sub-linear complexity}" whose complexity is usually of the order $\mathcal{O}(\log (n))$
		
		\item The algorithms with "\NewTerm{factorial complexity}\index{complexity!factorial complexity}" whose complexity is usually of the order $\mathcal{O}(n!)$
	\end{enumerate}
and so on... (we give here definition only for complexity for which we already have given a detailed example or will give examples further below).

It is important to see that (focusing only on complexities we have seen with examples or will see later below) for $n>4$:
	

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In practice a good complexity is considered as being of order $\mathcal{O}(n^k)$ for $k>3$. Poor complexity is considered as being of type $\mathcal{O}(e^n),\mathcal{O}(n!)$ or equation.
	\end{tcolorbox}
	For people that like better visual stuff:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}
		  \begin{axis}[
		    grid = major,
		    clip = true,
		    ticks = none,
		    width=0.8\textwidth,
		    height=0.6\textwidth,
		    every axis plot/.append style={very thick},
		    axis line style = ultra thick,
		    clip mode=individual,
		    restrict y to domain=0:10,
		    restrict x to domain=0:10,
		    axis x line = left,
		    axis y line = left,
		    domain = 0.00:10,
		    xmin = 0,
		    xmax = 11,
		    ymin = 0,
		    ymax = 11,
		    xlabel = $n$,
		    ylabel = no. of operations,
		   % xlabel style = {at={(axis description cs:0.5,-0.1)},anchor=south},
		   % ylabel style = {at={(axis description cs:-0.08,0.5)},anchor=north},
		    label style = {font=\large},
		  ]
		\addplot [
		    samples=100, 
		    color=red,
		]
		{x^2}node[above,pos=1,style={font=\large}]{$\mathcal{O}(n^2)$};
		\addplot [
		    samples=100, 
		    color=blue,
		]
		{x}node[above,pos=1,style={font=\large}]{$\mathcal{O}(n)$};
		\addplot [
		    samples=100, 
		    color=orange,
		]
		{log2 x}node[above,pos=1,style={font=\large}]{$\mathcal{O}(\log{n})$};
		\addplot [
		    samples=100, 
		    color=black,
		]
		{x*(log2 x)}node[right,pos=1,style={font=\large}]{$\mathcal{O}(n\log{}n)$};
		\addplot [
		    samples=100, 
		    color=magenta,
		]
		{1}node[above,pos=1,style={font=\large}]{$\mathcal{O}(1)$};
		\addplot [
		    samples=100, 
		    color=cyan,
		]
		{x^3}node[left,pos=1,style={font=\large}]{$\mathcal{O}(n^3)$};
		
		%Creates stair-step like plot
		\addplot [
		    samples=100, 
		    color=green,
		]{x!}node[right,pos=1,style={font=\large}]{$\mathcal{O}(n!)$};
		\end{axis}
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Big $\mathcal{O}$ notation complexities plot}
	\end{figure}
	We already saw above an example with linear complexity (addition of scalars) and of polynomial (quadratic) complexity (with multiplication of scalars).


Let us see some other very common examples. The first example use the results of the previous examples:

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. Evaluation of a polynomial:
	
	The direct estimate of the value of $P(x)$ leads to a complexity:
	
	E2. Another important example is the inversion matrix problem that is used a lot in supercomputer megaflops benchmark (sometimes named "\NewTerm{LINPACK test}\index{LINPACK test}"). As you can see in the MATLAB™ companions book there is a division by $n^3$ as the matrix inversion complexity is of order $n^3$. Indeed:\\
	
	At the beginning, when the first row has length $n$, it takes $n$ operations to zero out any entry in the first column (one division, and $n-1$ multiply-subtracts to find the new entries along the row containing that entry. To get the first column of zeroes therefore takes $n(n-1)$ operations.\\
   
    In the next column, we need $(n-1)(n-2)$ operations to get the second column zeroed out.\\

   In the third column, we need $(n-2)(n-3)$ operations.\\
   
   The sum of all of these operations is:
   
	which goes as $\mathcal{O}(n^3)$.
	\end{tcolorbox}
	
	Thanks to William George Horner we have a more efficient algorithm that uses a factorization of the polynomial in the form:
	
	We can see almost easily that this factorization holds the same number of additions to $(n)$ but reduces the number of multiplications to $(n)$.

	The resulting complexity is $\mathcal{O} (n)$. The gain is unquestionably important. In addition, this  factorization avoids computing power.

	Let us see now another well known example that almost all students that have learned scripting a little bit already know:

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	\label{algorithmic complexity dichtomous search}The famous example of algorithmic complexity is the search for an  information in a sorted column (list). A simple algorithm named "dichotomous search", also named "binary search", is to take the cell to mid-column and see if we find the desired value. Otherwise, research must continue on the same method in the top or bottom of the column (depending on the lexicographical order).\\
	
	The algorithm is recursive and allows at each step, to divide by two the size of the search space.\\
	
	If this size is initially of $n$ cells in the column, it is of size $\dfrac{n}{2}$ in step $1$, of size $\dfrac{n}{2^2}$ in step 2, and more generally of size $\dfrac{n}{2^k}$ in step $k$.\\
	
	At worst, the search ends when there is only one cell in the column to explore, i.e. when $k$ is such that $n<2^k$.\\
	
	We deduce the maximum number of steps: it is the smallest $k$ such that $n<2^k$, written in another way $\log_2(n)<k$, i.e. the sub-linear complexity:
	
	That result is strongly related to a famous guess game. The game is that if you have a set of integers $\{1,2,\ldots,n\}$. Host picks at random one of the numbers and gives you $k$ chances to guess it. After each guess the host says "higher", "lower", or "correct". The question is what is the optimal strategy (number of guesses)?\\
	
	If we use binary search algorithm with guesses lower or higher on every turn, the maximum number of guesses that is required to guess a number between $1-n$ is $\lceil\log_{2}(n)\rceil$ and this must be less than $k$ to guarantee that we guess the number within $k$ guesses. So:
	
	Therefore:
	
	Hence:
	
	So we have just proved that if $n\leq 2^{k-1}$ then there is a strategy that guarantees we will guess the number by the $k$th guess.
	\end{tcolorbox}
	The result of this last example is to compared with a sequential search (useful when sorting is too costly in resources). For example, in a column of $25,000$ data the complexity is $\mathcal{O}(n)$ thus $25,000$ while with the dichotomous method, the sub-linear complexity gives $\log_2(25,000)=15$. The gain is considerable (at the condition that data is sorted)!

	Other complexity that the reader must absolutely know are some elementary Linear Algebra calculations (see the section of the same name)! So without proof (because normally trivial given the previous examples) if we consider two square matrices $A$ and $B$ of dimensions $n$ the main operations have the following complexities:
	\begin{itemize}
		\item Read the components (nested loops): $\mathcal{O}(n^2)$
		
		\item Calculation of the trace: $\text{tr}(A)=\displaystyle\sum_{i=1}^{n}a_{ii}\rightarrow \mathcal{O}(n)$ 
		
		\item Addition $A+B=C$ so that $c_{ij}=a_{ij}+b_{ij}\rightarrow \mathcal{O}(n^2)$
		
		\item Product $A\cdot B=c$ so that $c_{ij}=\displaystyle\sum_{k=1}^na_{ik}b_{kj}\rightarrow \mathcal{O}(n^3)$
		
		\item Solving the travelling salesman problem via brute-force search is $\mathcal{O}(n!)$.
		
		\item Determinant (by the direct method of Cramer as detailed in the section of Linear Algebra page \pageref{Cramer's rule}). We can thus show that the complexity order of the determinant of a square matrix of dimensions $n$ is $n$ products, $n-1$ additions plus $n$ times the complexity of the determinant of a matrix of dimensions $n-1$ so that finally we have: $\text{det(A)}=\mathcal{O}(n\cdot n!)$
	\end{itemize}
	Assuming that the computer performs an elementary operation in equation seconds (which is already a good computer), we obtain the following calculations time for several values of $n$ for the determinant:

	\begin{table}[H]
		\begin{center}
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|p{1cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2.2cm}|}
				\hline
				{\cellcolor[gray]{0.75}$n$} & $5$ &  $10$ & $15$ & $20$ & $50$\\ \hline
				 {\cellcolor[gray]{0.75}}$t$ & $6\cdot 10^{-7}$ [s] & $0.04$ [s] & $5.5$ [h.] & $1,543$ [y.] & $4.8\cdot 10^{46}$ [y.] \\ \hline
		\end{tabular}
		\end{center}
		\caption[]{Various costs}
	\end{table}	
	hence the need to make sometimes a time complexity calculation before starting an algorithm (unless you are working exclusively for future generations, provided that there will still be future generations...).
	
	To close this subject let us indicate that there are other notations than just the big $\mathcal{O}$ and the big $\Omega$. So let us give a summary and at the same time the definition for the other one:
	\begin{itemize}
		\item $f(x) = \mathcal{O}(g(x))$ means that the growth rate of $f(x)$ is asymptotically less than or equal to the growth rate of $g(x)$
		
		\item $f(x) = \Omega(g(x))$ means that the growth rate of $f(x)$ is asymptotically greater than or equal to the growth rate of $g(x)$
		
		\item $f(x) = \mathcal{o}(g(x))$ means that the growth rate of $f(x)$ is asymptotically less than the growth rate of $g(x)$.
		
		\item $f(x) = \omega(g(x))$ means that the growth rate of $f(x)$ is asymptotically greater than the growth rate of $g(x)$
		
		\item $f(x) = \Theta(g(x))$ means that the growth rate of $f(x)$ is asymptotically equal to the growth rate of $g(x)$
	\end{itemize}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Here's a simple way to remember which notation means what. All of the big $\mathcal{O}$ notations can be considered to have a bar. When looking at a $\Omega$, the bar is at the bottom, so it is an (asymptotic) lower bound. When looking at a $\Theta$, the bar is obviously in the middle. So it is an (asymptotic) tight bound. When handwriting $\mathcal{O}$, you usually finish at the top, and draw a squiggle. Therefore $\mathcal{O}(g(x))$ is the upper bound of the function. To be fair, this one doesn't work with most fonts, but it is the original justification of the names.
	\end{tcolorbox}



	\subsubsection{NP-Completude}\label{np completude}
	We will introduce now for the general culture the concept of "\NewTerm{NP-completeness}\index{NP-completeness}", that is to say that we will try to define without too much formalism (as usual in this book).
	
	First let us introduce two non-formal definitions of a Turing machine.
	
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] An algorithm is said to be a "\NewTerm{deterministic Turing machine}\index{deterministic Turing machine}" when the set of its rules prescribes at most one action to be performed for any given situation.
		
		\item[D2.] A "\NewTerm{non-deterministic Turing machine}\index{non-deterministic Turing machine}" may have a set of rules that prescribes more than one action for a given situation. For example, a non-deterministic Turing machine may have both "If you are in state 2 and you see an 'A', change it to a 'B' and move left" and "If you are in state 2 and you see an 'A', change it to a 'C' and move right" in its rule set.
	\end{enumerate}
	
	Now let us define what type of problems are frequently considered:
	
	\textbf{Definitions (\#\thesection.\mydef):}
	
	\begin{enumerate}
		\item[D1.] "\NewTerm{Logarithmic problems L}\index{problem!logarithmic problems L}" contains all algorithms (problems) that can be solved by a deterministic Turing machine using a logarithmic amount of computation time.
		
		\item[D2.] "\NewTerm{Polynomial problems P}\index{problem!polynomial problems P}" contains all algorithms (problems) that can be \underline{solved} by a deterministic Turing machine using a polynomial amount of computation time.
		
		\item[D3.] "\NewTerm{Non-deterministic polynomial time problems NP}\index{problem!non-deterministic polynomial time problems NP}" are the set algorithms (problems) where the result instances can be \underline{controlled} in a polynomial time complexity by a non-deterministic Turing machine. 
	\end{enumerate}
	
	It should be noted at this stage of the discussion that the class P is included in the NP class so that $\text{P} \subset \text{NP} $. Indeed if we know a polynomial algorithm to solve a problem then we can at worst check the solution with a polynomial complexity algorithm also.

	But a difficult question is the following reciprocal: if a problem is NP (known solution can be controlled in polynomial time) but it seems we can not found yet an algorithm in a polynomial time (or less) to find the solution, does it always exist a polynomial algorithm to find the solution so that $P=NP$ that thus bring us to write  $\text{P} \subseteq \text{NP}$?
	
	In other words: If the solution of a problem can be quickly verified as being the correct one, can the solution also be found quickly?
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. The problem of finding a Hamiltonian cycle (cycle that passes once and only once by all the vertices of the graph - see section Graphs Theory page \pageref{hamiltonian cycle}) in a graph belongs to NP since, given a known cycle it is trivial to check in linear time $\mathcal{O}(n)$ it contains a well and once each vertex but found the cycle is at worst a factorial complexity $\mathcal{O}(n!)$.\\
	
	E2. Factoring an integer $n$ in product of prime factors (important in cryptography) is a NP problem. Indeed, given the prime factors $p_1,p_2,\ldots ,p_n$ it is trivial to control the solution $n=p_1,p_2,\ldots ,p_n$ as it is of order $\mathcal{O}(n^2)$. But we do not know if it exist a polynomial algorithm to find the prime numbers (to find the solution). So we do not know if the problem of finding prime numbers of an integer is a P-problem.
	\end{tcolorbox}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Apparently (we were not able to find the proof of this result and neither had opportunity to do so) the complexity of the best factorization algorithm for prime numbers is in the year 12007 (holocene calendar) of the type:
	
therefore there is still work to do (if a reader could provide us with the details that led to this result, we are interested).
	\end{tcolorbox}
	A problem $x$ that is in NP is also in NP-Complete if and only if every other problem in NP can be quickly (i.e. in polynomial time) transformed into $x$.

	In other words a problem $x$ is "\NewTerm{NP-Complete NPC}\index{NP-Complete NPC}" if:
	\begin{enumerate}
		\item $x$ is in NP
		\item Every problem in NP is reducible to $x$
	\end{enumerate}
	So, what makes NP-Complete so interesting is that if any one of the NP-Complete problems was to be solved quickly, then all NP problems can be solved quickly.

	A NPC problem is complete in that it contains most of the complexity of problems belonging to NP, and a polynomial solution to this problem involves a polynomial solution to all NP type problems.

	In other words: NPC problems have an exponential complexity and they all have the same complexity class (modulo polynomials).

	Finally, what is important to understand and retain about this idea is that if we find one day a polynomial algorithm for one of these really difficult problems that are the NPC problems, then in one stroke NP becomes equal to P and all difficult problems become easy such that:
	
	In other words: can we find in a polynomial time what can be proved (controlled) in polynomial time?

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	This issue is so important in computing science to reduce energy consumptions and time computations that it belongs (arbitrarily) to the 7 millennium problems, whose resolution is prized \$1 million  by the Clay Mathematics Institute. 
	\end{tcolorbox}
	
	Ok this definitions given let us now study of some typical applications of numerical methods which are very often used in the industry. We will go from simple to more complicated and not forgetting that many methods that are not in this section can sometimes be found in other sections of the book!

\subsection{Integer Part}\label{integer part}

	The biggest integer less than or equal to a real number x is expressed in this book by $[x]$, which will be read "\NewTerm{integer part of $x$}\index{integer part}" and according to the norm ISO 80000-2:2009 \textit{Mathematical signs and symbols to be used in the natural sciences and technology} it should denoted as: $\text{int} \; x$.

	The number $M$ is an integer if and only if $[M] = M$. Similarly, the natural number $A \in \mathbb{N}$ is divisible in the natural set by the natural number $B \in \mathbb{N}$ if and only if:
	
	We also denote by $\left\lbrace x\right\rbrace $ the fractional part of $x$ such that:
	
	That is to say:
	
	with $\mid \left\lbrace x\right\rbrace \mid < 1$.
	
	Consider $x,y \in \mathbb{R}$. Then we have the following properties (normally most of them don't need any proof):
	\begin{enumerate}
		\item[P1.] $[x]\leq x <[x]+1\Leftrightarrow 0\leq x-[x] <1$
		
		\item[P2.] $[x]=\displaystyle\sum_{n\leq x} 1$ for $x \geq 0$
		
		\item[P3.] $[x+m]=[x]+m$ if $m \in \mathbb{Z}$
		
		\item[P4.] $[x]+[y] \leq [x+y] \leq [x]+[y]+1$
		\begin{dem}
			A reader asked for the proof of this property. So let us see how to proceed. First we write:
			
			where $n,m \in \mathbb{Z}$ and where $0 \leq \theta < 1$ and $0 \leq \phi <1$. Thus:
			
			By writing $x=n+\theta$, where $0\leq 0 <1$, we have:
			
			where $0 < 1 -\theta \leq 1$.
			It follows that:
			
			if $\theta = 0$ and:
			
			if $0<\theta<1$.
			\begin{flushright}
				$\blacksquare$  Q.E.D.
			\end{flushright}
		\end{dem}
		
		\item[P5.] $[-x]=-[x]$ if $x \in \mathbb{Z}$ otherwise $[-x]=-[x]-1$ if $x \not\in \mathbb{Z}$
			\begin{dem}
			 The proof is already given at the end of the proof of property P4.
			\begin{flushright}
				$\blacksquare$  Q.E.D.
			\end{flushright}		
			\end{dem}
			
		\item[P6.] $\left[\dfrac{[x]}{m}\right]=\left[\dfrac{x}{m}\right]$ if $m \in \mathbb{N}$ 
			\begin{dem}
				For this proof we will write:
				
				where $0 < \theta <1$
				and:
				
			where $0 \leq r < m$ (\SeeChapter{see section Number Theory page \pageref{euclidean division}}). So we get:
				
				because $0 \leq r + \theta \leq m$. Besides :
				
				and thus we have the expected result.
				\begin{flushright}
					$\blacksquare$  Q.E.D.
				\end{flushright}
			\end{dem}

		\item[P7.] If $a \in \mathbb{N}$ then $\left[\dfrac{x}{a} \right]$ represents the number of integers less than or equal $x$ that are divisible by $a$.
		\begin{dem}
		For the last part, we observe that if $a, 2a,\ldots ,ma$ are all positive integers $\leq x$ that are divisible by $a$, it suffices to prove that $\left[\dfrac{x}{a}\right]=m$. 
		
		Since $(m+1)a>x$, then:
			
		That is to say:
			
		and thus we have the expected result.					
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}			
		\end{dem}
	\end{enumerate}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The rounding method of real values will be given in the section on Economy.
	\end{tcolorbox}
	
	\subsection{Heron's Square Root Algorithm}\label{Heron square root algorithm}
	One of the first things that many people learn in an introduction to Computer Sciences course is the algorithm for calculating the square root of a number. 
	
	There is such a simple algorithm for this purpose named "\NewTerm{Heron's algorithm}\index{Heron's algorithm}" or "\NewTerm{algorithm of Heron of Alexandria}\index{algorithm of Heron of Alexandria}" or even "\NewTerm{Babylonian method}\index{Babylonian method}" which converge to the value of this square root.
	
	Thus want to calculate the square root:
	
	\begin{dem}
		Here is a pseudo-proof because historically the algorithm was built on purely intuitive considerations (since approximately year 9900 according to holocene calendar algebra did not exist...). In high-school classes the result is given as a definition and just convergence is observed.
		 
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		The "\NewTerm{convergence ratio}\index{convergence ratio}" of a sequence $x^{(k)}$ to a constant $x_{0}$ is:
		
		if this limit exists. If the convergence ratio is greater than $0$ and less than $1$, the sequence is said to "\NewTerm{converge linearly}". If the convergence ratio is $0$, the sequence is said to "\NewTerm{converge superlinearly}".
		\end{tcolorbox}
	
		So for the demonstration, we will proceed as follows:
		
		And the trick is to write:
		
		This is named a "\NewTerm{fixed-point iteration}\index{fixed-point iteration}" (related to the theorem of the same name proved in the section of Sequences and Series page \pageref{fixed point theorem}). It is used in mainly situations in computer science. We will encounter it quite a lot of time further below...!
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
	\end{dem}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	We want to calculate:
	
	We take $A=10$ and this gives us the following table of iterations:
	
		\begin{table}[H]
	\begin{center}
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|p{1cm}|p{2cm}|p{1.8cm}|p{2.5cm}|p{2cm}|}
				\hline
				\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Iteration}} & 
\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$\dfrac{x_i}{2}$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$\dfrac{A}{2x_i}$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$x_{i+1}$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Gap}}\\ \hline
		1 & 5 & 0.5 & 5.50 & $\cong$ 2 \\ \hline
		2 & 2.750 & 0.90909 & 3.659090909 & $\cong$ 0.49 \\ \hline
		3 & 1.82954 & 1.3664 & 3.196005083 & $\cong$ 0.033 \\ \hline
		4 & 1.59800 & 1.5644 & 3.162455624  & $\cong$ 0.0002 \\ \hline
		5 & 1.58122 & 1.5810 & 3.162277665 & $\cong$ 0.5$\cdot 10^{-8}$ \\ \hline
	\end{tabular}
	\end{center}
	\caption[]{Iterations for Heron's square root algorithm}
	\end{table}	
	\end{tcolorbox}

	In the case of the cubic root, the proof is similar and we obtain:
	
	
	and so on...
		
	We can prove algebraically that the babylonian method converge. Note that all $x_n>0$ and:
	 
	Thus, all $x_{n}$ past the first satisfy $x_{n}^{2} \geq a .$ Therefore:
	 
	says that $x_{n+1} \leq x_{n} .$ Therefore, $\left\{x_{n}\right\}$ is a decreasing sequence, bounded below. Thus, the sequence
	converges. Furthermore, since the sequence converges:
	 
	Therefore:
	 
	To conclude it is perhaps interesting to know that the reader can find in the section of Number Theory the method used in the antiquity (at least an analogy) using continued fractions.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	We will see later that this above method is a special case of Newton's method introduced further below page \pageref{newton method}.
	\end{tcolorbox}
	
	\pagebreak
	\subsection{Archimedes Algorithm}
	
	The calculation of the universal constant "pi" denoted by $\pi$ is certainly the algorithm with the greatest interest since we found this constant almost everywhere in physics and mathematics (there are numerous books on the subject available on the market).
	
	We recall that we did not give the value of $\pi$ in the section on Geometry or in other sections of this book until now. So we will now tackle this task.
	
	We define in geometry the number named "pi", independently of the metric used, as the ratio of half the circumference of a circle with its radius such that:
	
	It seems that we own the first algorithm of the calculation of this constant by Archimedes (9713-9788 according to holocene calendar) and whose proof is given below:
	\begin{dem}
		Consider an $n$-polygon inscribed in a circle (we start with a square of $n$ sides of length $2u_n$ and if we go from a square to an octogone, two times more side, the sides length becomes $2u_{2n}$ and so on...):
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
			%uncomment if require: \path (0,592); %set diagram left start at 0, and has height of 592
			
			%Shape: Circle [id:dp03419565841401573] 
			\draw   (148,188.5) .. controls (148,97.65) and (221.65,24) .. (312.5,24) .. controls (403.35,24) and (477,97.65) .. (477,188.5) .. controls (477,279.35) and (403.35,353) .. (312.5,353) .. controls (221.65,353) and (148,279.35) .. (148,188.5) -- cycle ;
			%Shape: Square [id:dp6083656297614866] 
			\draw   (196.63,72.63) -- (428.38,72.63) -- (428.38,304.38) -- (196.63,304.38) -- cycle ;
			%Shape: Regular Polygon [id:dp6821164437854133] 
			\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]  (476.58,188.92) -- (428.52,304.94) -- (312.5,353) -- (196.48,304.94) -- (148.42,188.92) -- (196.48,72.91) -- (312.5,24.85) -- (428.52,72.91) -- cycle ;
			%Flowchart: Summing Junction [id:dp8939285175344049] 
			\draw   (307.5,188.92) .. controls (307.5,186.16) and (309.74,183.92) .. (312.5,183.92) .. controls (315.26,183.92) and (317.5,186.16) .. (317.5,188.92) .. controls (317.5,191.69) and (315.26,193.92) .. (312.5,193.92) .. controls (309.74,193.92) and (307.5,191.69) .. (307.5,188.92) -- cycle ; \draw   (308.96,185.39) -- (316.04,192.46) ; \draw   (316.04,185.39) -- (308.96,192.46) ;
			%Straight Lines [id:da7591269565361831] 
			\draw    (312.5,188.92) -- (312.5,26.85) ;
			\draw [shift={(312.5,24.85)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
			%Straight Lines [id:da03462132229758841] 
			\draw    (312.5,188.5) -- (427.1,74.32) ;
			\draw [shift={(428.52,72.91)}, rotate = 135.1] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
			%Shape: Arc [id:dp6475885952950422] 
			\draw  [draw opacity=0] (312.89,140.07) .. controls (325.87,139.14) and (338.13,146.78) .. (342.92,159.03) -- (315,170) -- cycle ; \draw   (312.89,140.07) .. controls (325.87,139.14) and (338.13,146.78) .. (342.92,159.03) ;  
			
			% Text Node
			\draw (297,112.4) node [anchor=north west][inner sep=0.75pt]    {$R$};
			% Text Node
			\draw (297,46.4) node [anchor=north west][inner sep=0.75pt]    {$A$};
			% Text Node
			\draw (360,140.4) node [anchor=north west][inner sep=0.75pt]    {$R$};
			% Text Node
			\draw (328,126.4) node [anchor=north west][inner sep=0.75pt]    {$\alpha $};
			% Text Node
			\draw (349,74.4) node [anchor=north west][inner sep=0.75pt]    {$u_{n}$};
			% Text Node
			\draw (346,44.4) node [anchor=north west][inner sep=0.75pt]    {$2u_{2n}$};
			
			\end{tikzpicture}
			\vspace*{3mm}
			\caption{The illustrated principle of Archimedes algorithm}
		\end{figure}
	The principle of Archimedes algorithm is as follows:
	
	Given the perimeter of a regular polygon of $n$ sides inscribed in a circle of radius $1/2$ we can see in the figure above that by induction (\SeeChapter{see section Trigonometry page \pageref{spherical trigonometry}}):
	
	With have for the perimeter of an $n$-polygon:
	
	and:
	
	with:
	
	hence:
	
	Therefore:
	
	We then just need a computer and several iterations to evaluate with a good accuracy the value of $\pi$. Obviously, we use the Heron algorithm to calculate the square root...
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	There are a large number of algorithms to compute $\pi$. The one shown above, without being the most aesthetic, seems historically to be the first and most simple one.
	\end{tcolorbox}
	
	\pagebreak
	\subsection{Euler's Number $e$}\label{euler number computation}
	Including the constant $\pi$, there are many other important mathematical constant that we need to generate with a computer (nowadays most constant values are stored as is and are not recalculated automatically). Among them, is the "\NewTerm{Euler number}\index{Euler number}" denoted by $e$ (\SeeChapter{see section Functional Analysis page \pageref{Euler number}}). Let's see how to calculate this number.
	
	Consider the Taylor series (\SeeChapter{see section Sequences and Series page \pageref{taylor series}}) for an infinitely differentiable function $f$ given by:
	
	As (\SeeChapter{see section Differential and Integral Calculus page \pageref{usual derivatives}}):
	
	Therefore we have:
	
	So finally:
	
	This relation provides an easy algorithm to calculate the Euler number to a given order $n$ of precision.

	\subsection{Stirling's factorial approximation}\label{stirling}
	Obviously, the factorial may be calculated with a simple iteration. However, this kind of method generates an exponential complexity algorithm which is not the best. Then there exists another method:

	In mathematics, Stirling's approximation (or Stirling's formula) is an approximation for factorials. It is a very powerful approximation, leading to a very useful result in Statistical Mechanics (see section of the same name page \pageref{statistical physics distributions}). 

	There are various approach that lead to different results. We will focus here on the only one that we will use later in theoretical physics and that is the worst easiest approximation.
	
	Either the definition of the factorial:
	
	
	And according to the properties of logarithms:
	
	
	If $n$ is very large (but very large) then the previous sum can be approximately written as an integral:
	
	
	Solving this integral we get (\SeeChapter{see section Differential and Integral Calculus page \pageref{usual primitives}})
	
	
	When $n \gg 1$, the lower limit is negligible and then (approximation that will be very useful in the section of Statistical Mechanics):
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/computing/stirling_factorial_approximation.jpg}
		\caption{Comparison of logarithmic formulations using Stirling formula}
	\end{figure}
	
	After a small elementary simplification, we obtain:
	
	
	The latter relation is useful of course only if we assumes that Euler's constant is a value stored in the machine...
	
	Let us consider the Gamma function (\SeeChapter{see section Differential and Integral Calculus page \pageref{gamma euler function}}):
	
	and recall that for integers $\lambda$ we have:
	
	We get:
	
	Substituting $y=t/\lambda$ and letting $g(x)=y-\log (y)$ We get: 
	
	Now we will use Laplace's method of integration (\SeeChapter{see section Differential and Integral Calculus page \pageref{laplace method of integration}}) and for this we differentiate twice and get:
	
	so that $y^*=1,g(y^*)=1$ and $g''(y^*)=1$. Laplace's method now
	yields to:
	
	The relation:
	
	which is known as "\NewTerm{Stirling's improved formula}\index{Stirling's improved formula}".
	
	\pagebreak
	\subsection{Linear Systems of Equations}\label{linear systems of equations}
	
	There are many methods for solving systems of linear equations. Most of them have been developed to address particular systems. We will here study for the moment only one, named the "\NewTerm{Gauss reduction method}\index{Gauss reduction method}" or "\NewTerm{Gauss pivot}\index{Gauss pivot}" or "\NewTerm{Gauss reduction algorithm}\index{Gauss reduction algorithm}" or also  "\NewTerm{Gauss elimination}\index{Gauss elimination}" which is well suited for solving small systems of linear equations (up to $50$ unknowns).
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} The validity of some of the operations that we will perform here to solve linear systems is proved in the section on Linear Algebra. In fact, to be brief, the whole method use vector spaces whose columns are linearly independent vectors.\\

	\textbf{R2}. Recall that linear systems admit a solution if and only if the rank of the augmented matrix is less than or equal to the number of equations (\SeeChapter{see section Linear Algebra page \pageref{rank of a matrix}}).
	\end{tcolorbox}
	
	\subsubsection{One equation with one unknown}
	
	We begin for sure with the easiest and smallest possible example... of one equation with one unknown:
	
	Where $a$ and $b$ are the coefficients of the equation and $x$ is unknown. Solving this equation consists to determine $x$ is according to $a$ and $b$. If $a$ is not equal to $0$ then:
		
	is the solution of the equation. If $a$ is equal to zero and if $b$ is different from $0$ then the equation above admits no solution. If $a$ and $b$ are equal to zero, then the equation has infinitely many solutions.
	
	\subsubsection{Two equations with two unknowns}
	
	A linear system of two equations with two unknowns can be written as we know:
		
		Where $a_{11},a_{12},a_{21},a_{22}$ are the coefficients of the equations, $x_1$ and $x_2$ are the unknowns.
		
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
The above-used notations have nothing to do with the tensor calculus!
		\end{tcolorbox}	
		
		To solve the system following the algorithm that interest us we proceed as follows:
		
		Using elementary algebraic manipulations (addition or subtraction of the various equalities between themselves - operations authorized by the linear independence of vectors-lines) we transform the system into another with one unknown less for one of the both equations given by (for example):
		
		The transformation between the two systems:
		
		is simply done by multiplying each coefficient of the first equality by $\dfrac{a_{21}}{a_{11}}$ and subtracting the second line with the resulting equation. This procedure is named "\NewTerm{row reduction}\index{row reduction}" or "\NewTerm{Gaussian elimination}\index{Gaussian elimination}\label{Gaussian elimination}". 
		
		In our case, the element $a_{11}$ is named the "\NewTerm{pivot}\index{pivot}" or "\NewTerm{leading coefficient}\index{leading coefficient}". In others words, if the $L_i$ is the notation for the each line number of the system, what we made is:
		
		Then, we solve the equation with only one unknown:
		
		We can therefore conclude with:
		
		But this is not the real algorithm. The real one use the augmented form:
		
		 Now we apply $L_2-\dfrac{a_{21}}{a_{11}}L_1\rightarrow L_2$ and $L_1-\dfrac{a_{12}}{a_{22}}L_2 \rightarrow L_1$ such that:
		
		As you can see the matrix has been put in diagonal form!
		Now to continue, we write:
		
		
		Now we just apply $\dfrac{1}{a_{11}^{\prime}}L_1 \rightarrow L_1$ and  $\dfrac{1}{a_{12}^{\prime}}L_2 \rightarrow L_2$ such that:
		
		Finally:
		
		
		\subsubsection{Three equations with three unknowns}
	Now consider the case of the linear systems of three equations with three unknowns:
		
	We can subsequently by elementary operations (see section Linear Algebra page \pageref{linear systems} and the previous case) reduce this linear system in the following echelon form system:
		
		And therefore we can trivially solve the last line:
		
		And afterwards the second line:
		
		And finally:
		
		Let us return to the systems transformations. It is carried out in two stages:
		\begin{enumerate}
			\item In the first line, we choose $a_{11}$ as the pivot and we eliminate the coefficients $a_{21}$ and $a_{31}$ as follows:
			
			We have to multiply each coefficient of the first line by $\dfrac{a_{21}}{a_{11}}$ and subtract this result of the second line and therefore $a_{21}$ disappears.
			
			Similarly, by multiplying the coefficients of the first line by $\dfrac{a_{31}}{a_{11}}$, and subtracting the result obtained from the third line, $a_{31}$ disappears.
			
			The linear system of equations can therefore be written as:
			
			\item The second step is to treat the linear system of two equations with two unknowns formed by the second and third lines of the previous system and that, in choosing $a_{22}^{\prime}$ as pivot. This method of resolution can seem complicated but it has the advantage of being generalized and be applied to solve linear systems of $n$ equations in $n$ unknowns.
		\end{enumerate}
		
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
		\textbf{{\Large \ding{45}}Example:}\\\\
		Let us see an example taken on Wikipedia (helps to copy/paste boring \LaTeX ...):
		
		Now we put:
		
		Therefore we get:
		
		Now we put:
		
		Therefore we get:
		
		To continue we put:
		
		Therefore we get:
		
		We put:
		
		Therefore we get:
		
		And finally we put:
		
		To get:
		
		So this is again an application of the row reduction method (or Gaussian elimination).
	\end{tcolorbox}
	For sure when you know that such systems can be solved using just a matrix inversion with a vector multiplication (\SeeChapter{see section Linear Algebra page \pageref{linear systems}}) all that stuff will be almost useless for most employees working in non-maths jobs. 
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	By extension if the procedure of solution showed earlier above and gives a line where $0=\text{'something non-zero'}$ or $0=0$, then it means that the system has no solution and respectively infinitely many solutions. We will see in the section of Linear Algebra that this is explained by the fact that the matrix of the system is a singular matrix (i.e. there is no inverse).
	\end{tcolorbox}

		\pagebreak
		\subsubsection{$n$ equations with $n$ unknowns}
	
		To simplify the writing, the coefficients will always be noted $a_{ij}$ and not $a_{ij}^{\prime},a_{ij}^{\prime\prime}$, etc. at each stage of the calculation.
		
		Given the linear system (we could also represent it as an augmented matrix to simplify the notations):
		
We will choose $a_{11}$ as the pivot to eliminate $a_{21},a_{31},\ldots ,a_{n1}$. Then, removing $a_{32},a_{42},\ldots ,a_{n2}$ is performed by taking $a_{22}$ as a pivot. Last pivot to consider is obviously $a_{n-1,n-1}$, it helps eliminate $a_{n,n-1}$. The system then takes the form:
		
		And we can therefore solve the last equation, then the fore last equation, and so on up to the first one.
		
		This method must however be fine-tuned to avoid pivots with $0$ values. The trick is therefore to switch the order in which the equations are written to choose the pivot coefficient whose absolute value is the largest. Thus, in the first column, the better is pivot is the coefficient such that $a_{j1}$:
		
It is taken to $a_{11}$ by permutation of first lines and $j$-th lines. Removal of the rest of the first column can then be performed. Then, we begin again with the $n-1$ remaining equations.

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	A last example with $4$ unknowns with a different representation that may help (we hope so!). Consider the following linear system:
	
	Let us apply the Gauss elimination algorithm:
	
	And the last steps should be obvious to the reader (if not we can detail them on request as always!).
	\end{tcolorbox}
	
	To summarize, given a $n \times n$ matrix $A$ and $n$-vector $\vec{b}$, the following algorithm reduces the system of linear equations $A \vec{x}=\vec{b}$ to an equivalent upper triangular system. $A$ and $\vec{b}$ are overwritten with the coefficients and right-hand side, respectively, of the reduced system.
	
	\begin{algorithm}[H]
	 \KwIn{$A$,$\vec{b}$}
	 \For{$j=1,2,\ldots$ \KwTo $n-1$}{
	  \For{$i=j+1,j+2,\ldots$ \KwTo $n$}{
	    $m_{ij} = a_{ij}/a_{jj} $\;
	    \For{$k=j+1,j+2,\ldots$ \KwTo $n$}{
	    	$a_{ik} = a_{ik}-m_{ij}a_{jk} $\;
	   	}
	   	$b_i=b_i-m_{ij}b_j$
	   }
	 }
	\caption{Gaussian elimination}
	\end{algorithm}

	\pagebreak
	\subsection{Polynomials}
	The basics polynomials with real coefficients has been studied in the section of Calculus quite in details (see page \pageref{polynomial}). Here we will address only the digital aspect of some problems related to polynomials (that is to say elementary algorithms or formulas useful for some operations not included by default in most computer programming languages).
	
	Apart from the addition and subtraction of polynomials which we assume as trivial (aside the optimization of the complexity aside as the Horner scheme), we will see how to multiply and divide two polynomials.
	
	Let's first see how to multiply two polynomials.
	
	Let:
	
	Therefore:
	
	With for $k=0,1,2,\ldots ,n+m$:
	
	it was easy...
	
	The second case of interests to us now is the Euclidean division of polynomials (\SeeChapter{see section Calculus page \pageref{polynomials division}}).
	
	Let us take again:
	
	but with the condition that $n \geq m$ that is to say $\text{deg}(f(x))<\text{deg}(g(x))$.
	
	The division can be written as we know (see the section of Number Theory or Calculus at page \pageref{polynomials division}):
	
	with:
	
	otherwise $r(x)=0$.
	
	It is normally known beforehand (since proved in the section Calculus again at page \pageref{polynomials division}) that we have:
	
	and:
	
	We have therefore by definition $q(x)$ that is the quotient of the division and $r(x)$ the remainder of the Euclidean division of $f(x)$ by $g (x)$.
	
	Therefore, nothing prevents us from writing in the most general possible way:
	
	To prove the expression of different $q_i$, we have preferred for educational reasons to use a specific example (see below) whose result will be generalized.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\textbf{{\Large \ding{45}}Example:}\\\\
	Let:
	
	So of what we have said before, we get (starting point):
	
	Using the fact that (as a reminder):
	
	So we have almost immediately:
	
	Then (still proceeding in the same way):
	
	And finally:
	
	\end{tcolorbox}
	So in general:
	
	As:
	
	The first remainder is then:
	
	After:
	
	The second remainder is then:
	
	and so on... we continue until $\deg(r_k(x))<m$.	
	
	\pagebreak
	\subsection{Regression Techniques}\label{regression techniques}
	Regressions are very useful and very important tools for statisticians, engineers, computer scientists, marketing analysts, physicists, physicians, economists wishing to establish a law of correlation between two (or more) variables, do a qualitative analysis, an extrapolation or even to separate signal form noise (as every point outside the regression will be considered as being noise).
	
	There are many regression  methods: the simple solution of first degree equations (when only two points of measurement are known) to equations that permits to obtain from a large number of points information that are essential to the establishment of a linear regression, polynomial, logistic or other law (or function).
	
	Let us give a list of the most used regressions techniques used in business and administrations (whose mathematical models are not all shown in this section yet but will be when we will have more time):
	\begin{enumerate}
		\item \textbf{Simple Linear Regression (SLR) model}\footnote{Also named \textbf{Classical Linear Regression (CLR)}} with one (\textbf{Univariate Linear Regression (CLR)}) or more variables (\textbf{Multiple Linear Regression (MLR)}) based on the method of least\footnote{Don't confuse "estimation methods" like OLS (ordinary least squares), TLS (total least squares), ML (maximum likelihood), REML (restricted maximum likelihood), PQL ( penalized quasi-likelihood) with "regression methods"!} squares with binary or continuous variables with response variable and coefficients\footnote{If the coefficients are required to be non-negative, we speak then of \textbf{Non-Negative Least Squares} and if we require the coefficient to be in some bounds then we speak of \textbf{Bounded-Variable Least-Squares} or \textbf{Constrained Least Squares}.} belonging to $\mathbb{R}$. Presented in detail in this section of the book (implicitly this model contains the interactions between variables and also some non-linear models).
		
		\item \textbf{Gaussian linear regression model} (statistical approach of linear regression based on the method of least squares) with binary or continuous variables with response variable also belonging to $\mathbb{R}$. When a regression model assumes that the errors are normally distributed, we speak commonly of \textbf{general linear model}. Presented in detail in this section of the book. A gaussian linear regression model is a special case of the generalized linear model (GLM).
		
		\item \textbf{Weighted least squares regression model} is used in the case where the assumption $\text{V}(\varepsilon_i)=\sigma_\varepsilon^2$ of Gaussian linear regression is not satisfied. The purpose is to apply a transformation trick using simple weights (see further below for more details), to keep this assumption valid.
		
		\item \textbf{Nonlinear regression models} with binary or continuous variables with response variable in $\mathbb{R}$. Presented in detail in this section of the if they can be reduced to linear case or not but then no interactions of explanatory variables. Otherwise based on quasi-Newton techniques type or Gauss-Newton presented also this section.
		
		\item \textbf{Polynomial regression model by the method of B-splines} or of the collocation polynomial with response variable in $\mathbb{R}$. The most common case is to restrict ourselves to the restriction of third order splines and then we speak of \textbf{restricted cubic splines regression (RCS)}. Presented in detail in this section of the book.
		
		\item \textbf{Logistic regression models} (binomial/multinomial/ordinal regressions) with binary, nominal variables (categorical) or ordinal or continuous with response variable bounded between $0$ and $1$. Presented summarily and naively in this section of the book. A logistic, probit or robit (robust version of the logit model) regression model is a special case of the generalized linear model (GLM).
		
		\item \textbf{Logic regression model} is a (generalized) regression methodology that is primarily applied when most of the covariates in the data to be analysed are binary. The goal of logic regression is to find predictors that are Boolean (logical) combinations of the original predictors.
		
		\item \textbf{Principal Component Regression (PCR)} instead of regressing the dependent variable on the explanatory variables directly, the principal components of the explanatory variables are used as regressors. Therefore PLS can reduces the number of predictors (dimensional reduction) to a smaller set of uncorrelated components and performs least squares regression on these components.
		
		\item \textbf{Partial Least squares Regression (PLR)} is a statistical method that bears some relation to principal components regression.  They differ in the methods used in extracting factor scores. In short, principal components regression produces like weight matrix reflecting the covariance structure between the predictor variables, while partial least squares regression produces like a weight matrix reflecting the covariance structure between the predictor and response variables.
		
		\item \textbf{Counting Poisson regression} (Poisson MLE, PMLE, GLM, Poisson-quasi-Lindley) or \textbf{negative binomial regression} (binomial MLE and QGPMLE) model with binary, nominal (categorical) or ordinal or continuous variables with positive integer answer in $\mathbb{N}$. All the regression models that do not assume the errors to me normally distributed, but following other well-known statistical distributions, are known under the name of \textbf{Generalized Linear regression Models (GLM)}.
		
		\item \textbf{Beta regression} is commonly used by practitioners to model variables that assume values in the standard unit interval $[0,1]$. It is based on the assumption that the dependent variable is beta-distributed and that its mean is related to a set of regressors through a linear predictor with unknown coefficients and a link function.  A beta regression model is also a special case of the generalized linear model (GLM).
		
		\item \textbf{Simplex regression} is actually (12019 according to holocene calendar) considered as a more robust and flexible alternative to the beta regression dedicated of rates or proportions. This regression is based on the simplex distribution, hence it's name...
		
		\item \textbf{Orthogonal linear regression model} (or Deming regression) that is used as complement to the paired $T$-test to check the stability of the measuring instruments in laboratories. This is a case where the explanatory and dependent variables are tainted with uncertainty. Notice that at the opposite of all regression techniques listed above and below that have no assumptions about the distribution of the predictor (independent) variables, the orthogonal linear regression is as far as we know, the only regression technique that has an assumption of the distribution of the predictor!
		
		\item \textbf{Total linear regression model} (also named Total Least Squares (TLS)) that is a generalization of Deming and Orthogonal linear regression. TLS aims to minimize the total geometric distance between the data points and the best-fit line rather than just the vertical distance as in Ordinary Least Squares (OLS).
		
		\item \textbf{Quantile regression model} (very useful in the medical and economic fields) based on the same idea as the regression by the method of least squares, but where we do not minimizes the sum of squared errors from the average, but the sum of absolute errors from a given quantile (median or other). It's also used sometimes to get rid of extreme values.
		
		\item \textbf{Theil–Sen estimator method}\index{Theil–Sen estimator method} also named Sen's slope estimator\index{Sen's slope estimator} or Slope selection method\index{slope selection method} or Single median method\index{single median method} or Kendall robust line-fit method\index{Kendall robust line-fit method} or Kendall–Theil robust line\index{Kendall–Theil robust line}... that is a very simple method for robustly fitting a line to a set of points (simple linear regression) that chooses the median $a_M$ of the slopes of all lines through pairs of two-dimensional sample points. For the estimate of the intercepts the USGS recommends the following calculation $b=y_M-a_Mx_M$. That's all... it's quite simple in fact.
		
		\item \textbf{Least Absolute Deviation (LAD) regression model} that use absolute values rather than square errors.  But absolute values are difficult to work with in mathematics (especially Calculus) as absolute values results in discontinuous derivatives that cannot be treated analytically
		
		\item \textbf{LOESS (LOcally Estimated Scatterplot Smoothing)} and \textbf{LOWESS (LOcally WEighted Scatterplot Smoothing)} are two strongly related nonparametric regression methods. These methods are purely numerical (does therefore not provide any unique formula) and are performed by fitting simple models to localized subsets (we speak then of "segmented regression"). In fact, one of the chief attractions of this method is that is not required to specify a global function of any form to fit a model to the data, only to fit segments of the data.
		
		\item \textbf{Multivariate adaptive regression splines (MARS)} (the term "MARS" is trademarked and licensed to Salford Systems. In order to avoid trademark infringements, many open source implementations of MARS are named "Earth"...) and as it names describes it well... is uses splines to interpolate and also extrapolate know data.
		
		\item \textbf{Bayesian linear regression model} is an approach to linear regression in which the statistical analysis is undertaken within the context of Bayesian inference, that means we have some prior knowledge about the regression coefficients and the error term.
		
		\item \textbf{Ridge regression model}\footnote{Technically speaking it's not a regression but a method of regularization! It's only a procedure affecting (sometimes improving) the method of fitting. It can be applied to almost any kind of regression.} that use a trick on the information matrix $X^TX$ (see further below) by adding a constant in it. But the constraints of usage are quite boring (no intercept and coefficients normalized) that make it quite difficult to interpret. In fact it is a special case of a regression family named \textbf{regularized linear regression models} and the underlying mathematical trick can be applied ton any general linear model (simple linear regression, logistic regressions, etc.).
		
		\item \textbf{Least Absolute Shrinkage and Selection Operator model (LASSO)}\footnote{Technically speaking it's also not a regression but a method of regularization!} that is based on the same ideas as that of the Ridge regression model and therefore also a special case of the Regularized Linear Regression models family (the only point that differs with the Ridge is that the theoretical model has somewhere a exponent equal to $1$ for LASSO when for it is equal to $2$ for Ridge).
		
		\item \textbf{Elastic net regression model}\footnote{Technically speaking it's also not a regression but a method of regularization!} is only a mathematical mixture of the LASSO and Ridge regression. This model therefore also belongs to the family of Regularized Linear Regression models.
		
		\item \textbf{Two-Stage Least Squares (2SLS) Regression} is used when the dependent variable's error terms are correlated with the independent variables. Additionally, it is useful when there are feedback loops in the model. In a first stage this regression builds instrumental variables that are uncorrelated with the error terms to compute estimated values of the problematic predictor(s), and then in a second stage uses those computed values to estimate a linear regression model of the dependent variable. This technique is used mainly in \textbf{Structural equation modelling (SEM)}.
		
		\item \textbf{Moderated Multiple Regression (MMR)} is used when one of the explicative variable has an interaction with another variable (product) that is continuous or dichotomous. The variable that interacts with the normal explicative variable is named the "moderator variable". A common error in practice is to make a confusion between Moderated Multiple Regression and a nested hierarchical model (i.e. Nested ANOVA).
		
		\item \textbf{Independent Component Regression (ICR)} is just a simple linear regression but based on a previous dimensional reduction of the exogenous variables based on an independent component analysis (ICA). That means the final model used the most explaining variance independent component as exogenous variables. Indeed, the reader must remember that PCA creates uncorrelated components but "uncorrelated" doesn't mean "independent" (!!!) as we have proved it in the section Statistics.
		
		\item \textbf{Support Vector Machine regression (SVMr)} use the concept of Support Vector Machine classification technique to evaluate existing values and interpolate new one.
		
		\item \textbf{Bootstrap or Jackknife regression model}\footnote{Technically speaking they are not "regression models" but methods related to the process of estimation!} using all previous models and then by resampling techniques (see further below) gives the possibility to have robust estimators that could in some situations be hard to get analytically. 
		
		\item \textbf{Isotonic regression} is a technique of fitting a free-form line to a sequence of observations under the following constraints: the fitted free-form line has to be non-decreasing (or non-increasing) everywhere, and it has to lie as close to the observations as possible.
		
		\item \textbf{Cox regression model} is a non-parametric technique based on the assumption of constant proportional hazard to analyse the survival rate in clinical trials.
		
		\item \textbf{Threshold Regression Models} is a parametric technique used to relax the assumption of constant proportional hazard of the Cox regression and therefore obviously also used to analyse the survival rate in clinical trials.
	\end{enumerate}
	... and ... for a given number of these approaches we differentiate mathematical models taking into account the censored data and uncensored data, constraints coefficients (like the fact that they should be non-negative, like non-negative least square NNLS, or sum up to $1$ or any other specific constraint that we categorize under the name of Bounded-Variable Least Squares (BVLS) or Constrained Least Square). This makes a bunch of theories/models to study in final and this is why this subsection is one of the biggest of the whole book and that some models are given in some other sections of the book (especially in the Statistics section).
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	There are also submodels families. Like the Panel Ordinary Least Squares (Panel OLS), linear mixed models (LMM) that includes fixed effects models (FE models) and random effects models (RE models) and mixed effect models (ME models) and longitudinal regression models. There is also Nonlinear Mixed Model (NLMM) that includes all linear mixed models, and Generalized Linear Models (to not be confuse with "general linear models) that includes NLMM. And also convex regression, etc.
	\end{tcolorbox}
	
	Here is a figure that summarize quite well the different regressions and their relationships (thanks to Adrian Olszweski for having authorized us to reproduce it!):
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/regression_models.pdf}
		\caption[Synoptic summary and relationships between various regression models]{Synoptic summary and relationships between various regression models (author: Adrian Olszweski)}
	\end{figure}
	For a more exhaustive list of regression techniques and their relationships, the reader can refer to the Data Science mind map at page \pageref{mindmap of data science}.
	
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Caution!!!!!!!!! There is a huge difference in science by only adjusting a mathematical function (having no real physical variables) to observations, and modelling observations with a physical model, ie including real physical variables and physics laws (the first one is naive, and most of time wrong, and the second one is obviously robust and most of time less wrong)!
	\end{tcolorbox}
	
	Finally, note that in the linear regression, explanatory variables form a linear expression but that does not mean they are themselves linear. Thus, if we consider the two expressions below:
	
	the first is linear in the parameters but the second is not!
	
	Finally, a word on a technique sometimes used for qualitative interpretation of the influential of explanatory variables trough their coefficients for simple or multivariate linear regressions:
	
	When the amplitudes of some explanatory variables (continuous!) have very different orders of magnitudes this raises a big problem of interpretation of the influence of each variable by reading their coefficient and also generates problems of calculations precision in algorithms because of differences in size order and thus also generates rounding errors!
	
	The traditional idea is then to center-reduce all the values of explanatory variables which helps greatly to interpretation of the influence of these variables (but we must then leave out the interpretation of the numerical value of the response variable). But take care to a common trap!!! Once the theoretical model obtained from  normalized (center-reduced) variables, the new values to explain should be obtained by having previously centered-reduced the new explanatory values but by subtracting the old average and reducing by the old standard deviations of respectively each of the injected explanatory variable in the model!
	
	\pagebreak
	\subsubsection{Univariate linear regression model}\label{simple linear regression}
	
	We will present here several algorithms (methods) useful and known in experimental science (we have already discussed about some of them during our study of statistics). The goal is to express the linear relation between two variables $x$ (explanatory variable) and $y$ (response variable) independently by a "\NewTerm{linear model LM}\index{linear model}" as simple as possible (otherwise it would take hundreds of pages to introduce the topic!).
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Writing this book we have hesitated a long time to put regression techniques in the Statistics section. But because in practice the choice of the type of regression is empirical it has seem to us most convenient to put this subject here.
	\end{tcolorbox}	
	
	\textbf{Definition (\#\thesection.\mydef):} In univariate regression we have:
	\begin{enumerate}
		\item[D1.] $x$ is the independent variable or "\NewTerm{explanatory variable}\index{explanatory variable}" also named "\NewTerm{covariate}\index{covariate}" or "\NewTerm{predictor}\index{predictor}" (in economics "\NewTerm{exogenous variable}"...). The $x$ values are set by the experimenter and are assumed to be known without error.
		
		\item[D2.] $y$ is the dependent variable or "\NewTerm{explained variable}\index{explained variable}" (e.g. the answer of the analyser) also named in economy "\NewTerm{endogenous variable}\index{endogenous variable}". $y$ values are most of time measured with an error (bias) of measurement. One goal of regression is to estimate precisely this error.
	\end{enumerate}
	We seek a relation of the form:
	
	This is the equation of a straight line (affine function), hence the term "\NewTerm{linear regression}\index{linear regression}" where $a$ is named in the study framework of regression techniques: "\NewTerm{regression coefficient}\index{regression coefficient}" instead of "slope" as seen in previous sections.
	
	In real life, linear relations are an exception because most phenomenon are nonlinear in reality and even non-continuous in certain situations... Furthermore, it is not because they are linear in a given interval of measurements that are still linear at a smaller-scale or larger scale (zoom bias)!
	
	However, in practice we make transformation to linearise functions either by elementary algebraic transformations like those used by spreadsheets softwares (e.g. Microsoft Excel) like for example the linearisation of a logarithmic function by making a simple change of variables:
	
	or for power and exponential functions by also making a small algebraic manipulation with the properties of logarithms as proved in the section of Functional Analysis at page \pageref{logarithms} (under the assumption that $a$ is strictly positive)\label{logarithmic and exponential linearization}:
	
	or by making Taylor series approximations (\SeeChapter{see section Sequences and Series page \pageref{taylor series}}).
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	If we seek to determine the value of $y$ for an unmeasured $x$ and lying beyond the original interval of measurement, then we speak of "\NewTerm{extrapolation}\index{extrapolation}" or in more complicated cases of "\NewTerm{forecast with prediction interval for $x$}\index{forecast with prediction interval}". We will see that further below.
	\end{tcolorbox}	
	
	\paragraph{Regression line}\label{regression line} \mbox{}\\\\\
	In the common way to make an univariate linear regression of the type:
	
	there exist multiple methods.
	
	This first and most simple method in our point of view relies on the properties of covariance and mean (\SeeChapter{see section Statistics page \pageref{covariance}}) and is widely used among others in elementary finance (but in fact in any filed where there are some statistics).
	
	Consider $X, Y$ two variables, one of which depends on the other (often it is $y$ that depends on $x$). According to the covariance bilinearity property (\SeeChapter{see section Statistics page \pageref{bilinearity of the variance}}) we recall that we have from:
	
	the following relation:
	
	So it comes for the regression coefficient (we will reuse this relation during our study of yield of a portfolio according to Sharpe model in the section Economy):
	
	
	Either in a more explicit form which that we will use later (using the relation determined earlier at page \pageref{regression line}):
	
	To determine the intercept we use the properties of the expected mean as proved in the section Statistics (see page \pageref{expected mean discrete variable}):
	
	Therefore we have $b$ as:
	
	
	\paragraph{Ordinary Least Squares Method (OLSM)}\label{least squares method}\mbox{}\\\\\
	Due to the error on $y$, the experimental points, of coordinates $(x_k,y_k)$ do not lie exactly on the theoretical line. We can therefore find the equation of the experimental  line passing closest to these points.
	
	The "\NewTerm{least squares method}\index{least squares method}" (LSM) will be under the particular study we are interested in, to look for the values of the parameters $a, b$ that minimize the sum of squares of residual $e_i$ (also named as we already know "Sum of Squared Residuals" (SSR) or "Sum of Squared Errors" (SSE)) between the observed values $y_k$ and the theoretical calculated values $y_k^{\prime}$. We then speak sometimes about the "\NewTerm{least squares method of ordinate deviations}\index{least squares method of ordinate deviations}"...:
	
	where $n$ is the number of measured points and the theoretical values given by:
	
	Therefore written explicitly:
	
	This relation shows the sum of squared deviations as a function of the parameters $a, b$. When this function is minimal (extremal), the derivatives with respect to these parameters cancels:
	
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	This method of minimum research (optimization) is named "\NewTerm{method of Lagrange multipliers}\index{method of Lagrange multipliers}" in the world of Economy (we will detail this method further below). In our example SSR is the scalar value that will be used as Lagrange multiplier.
	\end{tcolorbox}	
	
	Therefore after simplification and rearrangement:
	
	The above system is named "\NewTerm{normal equations}\index{normal equations}". This is a linear system of two equations with two unknowns. Let us write to simplify the notation:
	
	The system becomes:
	
	From the second line we get without surprise:
	
	By replacing in the first line, we get:
	
	From there we get from the second line:
	
	Thus, the terms of the slope and intercept of the straight line equation are:
	
	The last two relations are used by a majority of spreadsheet softwares such as in the English version of Microsoft Excel 11.8346 when using the \texttt{REGRESSION( )} function. The term $b$ (the $y$-intercept) may be obtained directly with the \texttt{INTERCEPT( )} function and $a$ with the \texttt{SLOPE( )} function and the whole with the \texttt{LINEST( )} function.
	
	In reality most softwares use the general case to found $(a,b)$ based on the Newton-Gradient descent (see page \pageref{gradient descent}) where it seeks for the minimum of the SSR as illustrated below:
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.9\textwidth]{img/computing/minimize_sse.pdf}
	\end{figure}
	Here for information an interesting little list of some very practical case with this spreadsheet software (because requested a lot)\label{linear regression for nonlinear functions}:
	\begin{itemize}
		\item For a straight line:\\
		
		$a$: \texttt{=SLOPE(y, x)}\\
		$b$: \texttt{= INTERCEPT(y, x)}
		
		\item For a logarithmic function (we see here again the change of variable given at the before):\\
		
		$a$: = \texttt{INDEX(LINEST(y, LN (x)), 1)}\\
		$b$: = \texttt{INDEX(LINEST(y, LN (x)), 1, 2)}
		
		\item For a power function (once again we see the change of variable given at the beginning):\\
		
		$a$: = \texttt{EXP(INDEX(LINEST(LN(y),LN(x),,),1,2))}\\
		$b$: = \texttt{INDEX(LINEST(LN(y),LN(x),,),1)}
		
		\item For an exponential function (we also find the change of variable given at the beginning):\\
		
		$a$: = \texttt{EXP(INDEX(LINEST(LN(y),x),1,2))}\\
		$b$: = \texttt{INDEX(LINEST(LN(y),x),1)}\\
	\end{itemize}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
We must keep in mind that the line of least squares, which can best summarize the cloud of observations points by minimizing SSR, necessarily passes through the center of gravity of the cloud, that is to say, by an average point that corresponds rarely to an observation (mean average of abscissas and ordinates).
	\end{tcolorbox}
	
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Caution! Keep in mind that least squared and linear regression are note the same! Least squares is a method to estimate the parameters. Linear regression is a model. Least squares is one method used to estimate the parameters in Linear Regression. There are other methods as well like Maximum Likelihood that we will see later!
	\end{tcolorbox}
	
	\pagebreak
	\paragraph{Univariate Regression Variance Analysis}\label{univariate regression variance analysis}\mbox{}\\\\\
	Before starting it is important that the reader abandons immediately the possible reflex that would be to try to bring by successive analogies the regression ANOVA we will see right now to the categorical ANOVA we have studied in the section Statistics (see page \pageref{anova})! 
	
	Let us start with:
	
	Either in discrete form:
	
	As well as by the construction of the least squares method we have the following relation:
	
	Now we assume that each measured value is attached by a residual error such that:
	
	Either by subtracting the last two relations:
	
	Now let us go through an intermediate result. Remember that we obtained earlier:
	
	And therefore:
	
	By replacing $b$ by its value:
	
	We therefore get:
	
	Multiplying the second line above by $\bar{x}$ and by subtracting from the first we get:
	
	Therefore after rearrangement:
	
	Now we go back to:
	
	If we put it all square and summing for all observations, we get:
	
	Therefore:
	
	But we have shown just before the double product was equal to zero. Therefore:
	
	This last relation is named "\NewTerm{ANOVA equation}\index{ANOVA!ANOVA equation}" or "\NewTerm{variance analysis equation}\index{ANOVA!variance analysis equation}". In fact, it is the sums of squares. We would need to divide it by $n$ to obtain biased variances.
	
	This last relation is often written:
	
	where SST is the "\NewTerm{total sum of squares}\index{total sum of squares}", SSE the "\NewTerm{sum of square errors}\index{sum of square errors}" and SSR "\NewTerm{sum of squares residuals}\index{sum of squares residuals}".
	
	Let us note now the estimated $y_k$ that minimize the errors such that the error is null in a different way and let us named that the "\NewTerm{a priori linear model}\index{a priori linear model}":
	
	Hence the equality we will reuse several times (it is just previous relations without error term):
	
	It is indeed important in practice to differentiate the a priori model that does not take into account the errors of the real model that does!
	
	Because of the previous equality the relation:
	
	Can be written:
	
	More explicitly:
	
	This last relation can be represented graphically as follows:
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/sst_sse_ssr_detailed.jpg}
		\caption{Graphical representation of respectively SST, SSE, SSR}
	\end{figure}

The last relation is sometimes denoted also in the literature in a most educational way as follows:

	which is just another way to write the variance decomposition (implicit variance):
	
	
	and it then comes immediately the relation sometimes used in practice to calculate residues (knowing the calculated values and measured values):
	
	It is important to remember that the above relation between SST, SSE and SSR are valid only in the case of a linear model!
	
	It is also important to note that in this particularly variance decomposition we have:
	
	Remember now that we have proved in the section Statistics, we found that the correlation coefficient was given (defined) by\label{correlation coefficient numerical methods}:
	
	Or else since we have shown above that (remember that the indicated variance is an estimated variance in practice!):
	
	We can therefore write the correlation coefficient in the form\label{slope and correlation coefficient relation}:
	
	So we deduce from this using the relations established in the section of statistics:
	
	Remember that we proved above that:
	
	Therefore:
	
	that is to say\label{correlation linear regression model}:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
This formulation of the correlation coefficient is extremely useful because, unlike the statistical formulation, the latter generalizes immediately to the multiple linear regression we will see a further below.
	\end{tcolorbox}
	And in the framework of regression models here are some typical cases of the value of the linear correlation coefficient with the first two lines and non-linear for the third line:
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{img/arithmetics/correlation_coefficients.jpg}
	\vspace*{3mm}
	\caption[Some values of the linear correlation coefficient]{Some values of the linear correlation coefficient (source: Wikipedia)}
\end{figure}
	Finally let us indicate that we also find very often the linear correlation coefficient as follows in softwares and literature:
	
	The last form highlights better that if the sum of the squares of residues SSR residues is zero, the measures are perfectly modelised by a linear relation in the range of study considered.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Some softwares communicate the "\NewTerm{predicted R-squared}\index{predicted R-squared}" defined as:
	
	 where PRESS, the "\NewTerm{predicted residual sum of squares}\index{predicted residual sum of squares}", is a form of cross-validation (see further below page \pageref{cross-validation}) used in regression analysis to provide a summary measure of the fit of a model to a sample of observations that were not themselves used to estimate the model. It is calculated as the sums of squares of the prediction residuals for those observations:
	 
	Therefore PRESS differs from the sum of squares of the residual error in that each fitted value, $\hat{y}_k$, for PRESS is obtained from the remaining $n-1$ observations.\\
	
	Given this procedure, the PRESS statistic can be calculated for a number of candidate model structures for the same dataset, with the lowest values of PRESS indicating the best structures. Models that are over-parametrised (over-fitted) would tend to give small residuals for observations included in the model-fitting but large residuals for observations that are excluded.\\
	
	Modern textbooks use the average of the PRESS such that we define the "\NewTerm{Leave-One-Out Cross-Validation score}\index{leave-one-out cross-validation score}\label{loocv}" (LOOCV) or "\NewTerm{ordinary cross-validation}\index{ordinary cross-validation}" (OCV):
	
	\end{tcolorbox}
	
	Finally, note that the ordinate value is not involved in the value of the correlation coefficient since (bilinearity property of covariance as proved in the section Statistics at page \pageref{covariance}):
	
	
	\pagebreak
	\paragraph{$F$-test for Regression (significance test for linear regression)}\label{dummy variable regression}\label{anova for linear regression}\index{statistical tests!$F$-test for regression}\mbox{}\\\\\
	Now comes a part that interests us mainly for practical scientific laboratories tests!!! 
	
	If your graduate statistical training was anything like mine, you learned ANOVA in one class and Linear Regression in another. My professors would often say things like « \textit{ANOVA is just a special case of Regression} » but give vague answers when pressed. Let us see why!
	
	Let us recall that in the section of Statistics we proved (see page \pageref{ANOVA Fisher test}) that for the one controlled factor ANOVA, the test of equality of means (through the use of the variances) was written (we just change the letter $x$ to $y$ to avoid confusion for the developments that will follow) is given by:
	
	which is used to compare, for example, the means of the cash-flows of two supposed independent chosen months (i.e. with $k=2$) over $10$ years (obviously in the case $k=2$ we could use a Student test if the conditions are of course satisfied!) supposing that for every year for the given month the values are normally distributed:
	\begin{table}[H]
	\centering
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|c|c|}
				\hline
				\cellcolor[gray]{0.75}\textbf{January} & \cellcolor[gray]{0.75}\textbf{February} \\ \hline
				$304$ & $280$ \\ \hline
				$284$ & $303$ \\ \hline
				$290$ & $294$ \\ \hline
				$310$ & $270$ \\ \hline
				$320$ & $276$ \\ \hline
				$270$ & $310$ \\ \hline
				$309$ & $290$ \\ \hline
				$293$ & $312$ \\ \hline
				$315$ & $260$ \\ \hline
				$301$ & $325$ \\ \hline
		\end{tabular}
	\end{table}
	Therefore a simple ANOVA with one controlled factor will give according to the calculations proven in the section of Statistics (see page \pageref{anova one way fixed factor}) and using a spreadsheet software like Microsoft Excel 14.0.7106 the following results:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/anova_one_factor_for_comparison_with_regression_anova_excel.jpg}
	\end{figure}
	With for recall:
	\begin{table}[H]\small
		\renewcommand{\arraystretch}{1.2}
		\centering
		\begin{tabular}{llcccc}\hline
		\textbf{Source} & \textbf{Sum of squares (SSE)} & $\chi^2$ \textbf{df} & \textbf{Mean squares} & $F$ & \textbf{Critical} $F$\\ \hline
		Inter-Class & $Q_A=\displaystyle\sum_{i}n_i\left(\bar{x}_{i}-\bar{\bar{x}}\right)^2$ & $k-1$ & $\text{MSk}=\displaystyle\dfrac{Q_A}{k-1}$ &
		$\displaystyle\dfrac{\text{MSk}}{\text{MSE}}$ & $P(F> F_{k-1,N-k})$ \\
		Intra-Class & $Q_R=\displaystyle\sum_{ij}\left(x_{ij}-\bar{x}_i\right)^2$ & $N-k$ & $ \text{MSE}=\displaystyle\dfrac{Q_R}{N-k}$  & & \\
		Total & $Q_T=\displaystyle\sum_{ij}\left(x_{ij}-\bar{\bar{x}}\right)^2$ & $N-1$ & & &\\ \hline
		\end{tabular}
		\caption[]{One way fixed factor ANOVA table}
	\end{table}
	
	But you might think: Why we go back on such an example ????

	Well simply because we can very well build a linear regression model of the cash-flows of these two months, which we had not mentioned in the Statistics section! Thus, when we have an ANOVA with a controlled factor, nothing prevents us from making a linear regression of the data with binary explanatory variables (and vice versa)! To do this, it is enough that we rewrite the table above in the following form (technique named "dummy coding" or "one hot encoding"):
	\begin{table}[H]
	\centering
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|c|c|c|c|}
				\hline
				\cellcolor[gray]{0.75}\textbf{Observation} & \cellcolor[gray]{0.75}\textbf{Cash-Flow $\pmb{y_i}$} & \cellcolor[gray]{0.75}\textbf{January $\pmb{x_{i1}}$} & \cellcolor[gray]{0.75}\textbf{February $\pmb{x_{i2}}$} \\ \hline
				$1$ & $304$ & $1$ & $0$ \\ \hline
				$2$ & $284$ & $1$ & $0$ \\ \hline
				$3$ & $290$ & $1$ & $0$ \\ \hline
				$4$ & $310$ & $1$ & $0$ \\ \hline
				$5$ & $320$ & $1$ & $0$ \\ \hline
				$6$ & $270$ & $1$ & $0$ \\ \hline
				$7$ & $309$ & $1$ & $0$ \\ \hline
				$8$ & $293$ & $1$ & $0$ \\ \hline
				$9$ & $315$ & $1$ & $0$ \\ \hline
				$10$ & $301$ & $1$ & $0$ \\ \hline
				$11$ & $280$ & $0$ & $1$ \\ \hline
				$12$ & $303$ & $0$ & $1$ \\ \hline
				$13$ & $294$ & $0$ & $1$ \\ \hline
				$14$ & $270$ & $0$ & $1$ \\ \hline
				$15$ & $276$ & $0$ & $1$ \\ \hline
				$16$ & $310$ & $0$ & $1$ \\ \hline
				$17$ & $290$ & $0$ & $1$ \\ \hline
				$18$ & $312$ & $0$ & $1$ \\ \hline
				$19$ & $260$ & $0$ & $1$ \\ \hline
				$20$ & $325$ & $0$ & $1$ \\ \hline
		\end{tabular}
	\end{table}
	Obviously we not always have in reality binary $(0,1)$ explanatory values but we can always normalize the control variables to fall back on such a situation!
	
	This necessarily can be summarized to:
	\begin{table}[H]
	\centering
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|c|c|c|c|}
				\hline
				\cellcolor[gray]{0.75}\textbf{$\pmb{n_i}$} & \cellcolor[gray]{0.75}\textbf{Mean cash flow $\pmb{\bar{y}_i}$} & \cellcolor[gray]{0.75}\textbf{January $\pmb{x_{i1}}$} & \cellcolor[gray]{0.75}\textbf{February $\pmb{x_{i2}}$} \\ \hline
				$10$ & $299.11$ & $1$ & $0$ \\ \hline
				$10$ & $292.00$ & $0$ & $1$ \\ \hline
		\end{tabular}
	\end{table}
	Therefore the regression model associated with this ANOVA with $1$ control factor at $2$ levels can therefore be written:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Notice that this last relation can also be written:
	
	And here maybe you recognize something well known to us when we have studied all the ANOVAs!!! So as we then see, checking that the means $\hat{\mu}_i$ are all equal or not in the ANOVA such that $\mu_1=\mu_2=\ldots=\mu$ is in the case of the regression equivalent to check that $\beta_1=\beta_2=0$. So under this form, the hypotheses are then:
	
	This is what we named the "\NewTerm{$F$-test of Overall Significance in Regression Analysis}\index{$F$-test of Overall Significance in Regression Analysis}" (unlike $T$-tests that can assess only one regression coefficient at a time as we will see further below page \pageref{variable importance GML}).
	\end{tcolorbox}
	But, with the preceding relation and the table summarized above, we have a system of two equations with three unknowns ... which is obviously insoluble for a least squares approach. Therefore, the idea consists in sacrificing one of the explanatory variables as (generalizable to more than two variables obviously):
	
	and therefore (special choice):
	
	and then we have indeed two equations with two unknowns:
	
	This explains the reason why statistical softwares will always give the coefficient of one of the two binary explanatory variables as zero (which obviously can be problematic in some cases and therefore it is enough to force the ordinate at the origin to be zero to have the two non-zero coefficients since then we fall back on a system of two equations with two unknowns). In the case of $k$ binary (dichotomous) explanatory variables, there will be $k - 1$ whose coefficients are non-zero (since one can always be explained by all others).
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The use of ANOVA, you probably guess, is only feasible if the residues are almost normally distributed and homoscedastic... This is why statistical softwares have outputs giving such analysis when running a F-test for regression. 
	\end{tcolorbox}
	Finally, all this to say that the ANOVA is only a special case (with binary explanatory variables rather than continuous one and let us recall that $\mathbb{N}\in\mathbb{R}$ not the inverse!) of the linear regression.

	Let us notice then that we can write:
	
	and as they are either $0$ or $1$ (or normalized to be such as). Then there remain only the terms $x_{ij}\hat{\beta}_j$ where $x_{ij}\neq 0$ in a quantity that for each $j$ we will denote by $n_j$. It comes then since we only keep the $x_{ij}=1$ (for the last equality if it is not obvious the reader can make an example on a sheet of paper with a simple numerical application!):
	
 	with obviously\footnote{In our above example $N=20$ and $n_1=n_2=10$} $\sum_{j=1}^k n_j=N$.

	By the method of the ordinary least squares and the values that can take the $x_{ij}$ we quickly see that the coefficients (parameters) of the regression will be given by (we can detail again on reader request):
	
	and therefore:
	
	and as in our case in comparison with the one fixed factor ANOVA we have the obvious correspondences:
	
	it follows, therefore, that there is a correspondence between the numerator of the Fisher test of ANOVA and the equivalent expression of the linear regression (and therefore with the same number of degrees of freedom) that brings us to write:
	
	But it remains for us to find the equivalent also of the denominator for the regression. For this we will proceed by similarity. Let us recall that in the one fixed factor ANOVA we have proved that:
	\begin{table}[H]
		\centering
		\begin{tabular}{ccccc}
		$Q_T$ & $=$ & $Q_A$ & $+$ & $Q_R$  \\
		$\displaystyle\sum_i (y_{ij}-\bar{\bar{y}})^2$ & $=$ & $\displaystyle\sum_i n_i(\bar{y}_i-\bar{\bar{y}})^2$ & $+$  & $\displaystyle\sum_{ij}(y_{ij}-\bar{y}_i)^2$  \\
		 $N-1$ & $=$ & $k-1$ & $+$ & $\displaystyle\sum_i (n_i-1)$  \\
		\end{tabular}
	\end{table}
	and let us recall that for the linear regression, we have proved earlier above (with the corresponding notations in usage):
	\begin{table}[H]
		\centering
		\begin{tabular}{ccccc}
		SST & $=$ & SSE & $+$ & SSR  \\
		$\displaystyle\sum_i (y_{i}-\bar{y})^2$ & $=$ & $\displaystyle\sum_i (\hat{y}_i-\bar{y})^2$ & $+$  & $\displaystyle\sum_i(y_i-\hat{y}_i)^2$  \\
		\end{tabular}
	\end{table}
	From what we have seen above we know that degrees of freedom of $\sum_i (\hat{y}_i-\bar{y})^2$ are $k-1$. If follows immediately that for $\sum_i (y_{i}-\bar{y})^2$ the degrees of freedom are $N-1$. Then we can write:
	\begin{table}[H]
		\centering
		\begin{tabular}{ccccc}
		SST & $=$ & SSE & $+$ & SSR  \\
		$\displaystyle\sum_i (y_{i}-\bar{y})^2$ & $=$ & $\displaystyle\sum_i (\hat{y}_i-\bar{y})^2$ & $+$  & $\displaystyle\sum_i(y_i-\hat{y}_i)^2$  \\
		$N-1$ & $=$ & $k-1$ & $+$ & $????$  \\
		\end{tabular}
	\end{table}
	It then follows that the degrees of freedom of the sum of the squares residuals is then of $N-k$ such that:
	\begin{table}[H]
		\centering
		\begin{tabular}{ccccc}
		SST & $=$ & SSE & $+$ & SSR  \\
		$\displaystyle\sum_i (y_{i}-\bar{y})^2$ & $=$ & $\displaystyle\sum_i (\hat{y}_i-\bar{y})^2$ & $+$  & $\displaystyle\sum_i(y_i-\hat{y}_i)^2$  \\
		$N-1$ & $=$ & $k-1$ & $+$ & $N-k$  \\
		\end{tabular}
	\end{table}
	Sum of the squares of residues that it is customary to write (useful for later !!!):
	
	and that is given in the output of some statistical softwares (sadly!) under the misnamed "\NewTerm{residual standard error}"\index{residual standard error} (instead than "\NewTerm{residual standard deviation}"):
		 
	As $Q_T$ doesn't appear in the Fisher test there is no a priori reason that the equivalent which is SST for regression appears there. By elimination, the correspondence is then immediate:
	
	The Fisher test then becomes for the linear regression:
	
	and is more famously known under the name "\NewTerm{omnibus test for OLS regression}"\index{omnibus test for OLS regression}\index{statistical tests!omnibus test for OLS regression} (i.e. that a significant percentage of statistical softwares give below the diagnostic table of the regression analysis!).
	
	It follows then that as for the one-fixed factor ANOVA, with linear regression, we can also make a table of the ANOVA (\SeeChapter{see section Statistics page \pageref{anova}}) as we will see later with an example!
	
	Now, let us prove a common and important form of this last relation. We have proved earlier above that the correlation coefficient could also be written in the form:
	
	However, let us notice that:
	
	And also let us recall once again that:
	
 	Thus explicitly as we have just seen:
	
	Therefore we can write:
	
	What we usually can find in some textbooks in the following form:
	
	We thus see that if the coefficient of determination $R^2$ is large (close to $1$) then the value of $F$ is large, the linear model will be considered as explaining significantly well the variable explained with respect to the explanatory variable (but this still doesn't mean that there is causality!).
	
	Let us come back to our companion example! A simple ordinary least squares regression with a spreadsheet software like Microsoft Excel of the below table:
	\begin{table}[H]
	\centering
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|c|c|c|}
				\hline
				\cellcolor[gray]{0.75}\textbf{Observation} & \cellcolor[gray]{0.75}\textbf{Cash-Flow $\pmb{y_i}$} & \cellcolor[gray]{0.75}\textbf{January $\pmb{x_{i1}}$} \\ \hline
				$1$ & $304$ & $1$\\ \hline
				$2$ & $284$ & $1$ \\ \hline
				$3$ & $290$ & $1$ \\ \hline
				$4$ & $310$ & $1$ \\ \hline
				$5$ & $320$ & $1$ \\ \hline
				$6$ & $270$ & $1$ \\ \hline
				$7$ & $309$ & $1$ \\ \hline
				$8$ & $293$ & $1$ \\ \hline
				$9$ & $315$ & $1$ \\ \hline
				$10$ & $301$ & $1$ \\ \hline
				$11$ & $280$ & $0$ \\ \hline
				$12$ & $303$ & $0$ \\ \hline
				$13$ & $294$ & $0$ \\ \hline
				$14$ & $270$ & $0$ \\ \hline
				$15$ & $276$ & $0$ \\ \hline
				$16$ & $310$ & $0$ \\ \hline
				$17$ & $290$ & $0$ \\ \hline
				$18$ & $312$ & $0$ \\ \hline
				$19$ & $260$ & $0$ \\ \hline
				$20$ & $325$ & $0$ \\ \hline
		\end{tabular}
	\end{table}
	gives (we make the choice not to force the ordinate at the origin and to take January as an explanatory variable with coefficient not zero otherwise we will not find the value of the calculation of the classic ANOVA seen earlier above):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/computing/anova_approach_univariate_regression_plot_ms_excel.jpg}
		\caption{Graphical representation of an univariate regression with the ANOVA approach in Microsoft Excel 14.0.7177}
	\end{figure}
	that is to say:
	
	We then have:
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\resizebox{\textwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		\cellcolor[gray]{0.75}\textbf{Observation} & \cellcolor[gray]{0.75}\textbf{Cash flow $\pmb{y_i}$} & \cellcolor[gray]{0.75}\textbf{Cash flow $\pmb{\hat{y}_i}$} & \cellcolor[gray]{0.75}\textbf{SSR $\pmb{(y_i-\hat{y}_i)^2}$} & \cellcolor[gray]{0.75}\textbf{SST $\pmb{(y_i-\bar{y})^2}$} & \cellcolor[gray]{0.75}\textbf{SSE $\pmb{(\hat{y}_i-\bar{y})^2}$}  \\ \hline
		$1$ & $304$ & $299.6$ & $19.36$ & $67.24$ & $14.44$ \\ \hline
		$2$ & $284$ & $299.6$ & $243.36$ & $139.24$ & $14.44$ \\ \hline
		$3$ & $290$ & $299.6$ & $92.16$ & $33.64$ & $14.44$ \\ \hline
		$4$ & $310$ & $299.6$ & $108.16$ & $201.64$ & $14.44$ \\ \hline
		$5$ & $320$ & $299.6$ & $416.16$ & $585.64$ & $14.44$ \\ \hline
		$6$ & $270$ & $299.6$ & $876.16$ & $665.64$ & $14.44$ \\ \hline
		$7$ & $309$ & $299.6$ & $88.36$ & $174.24$ & $14.44$ \\ \hline
		$8$ & $293$ & $299.6$ & $43.56$ & $7.84$ & $14.44$ \\ \hline
		$9$ & $315$ & $299.6$ & $237.16$ & $368.64$ & $14.44$ \\ \hline
		$10$ & $301$ & $299.6$ & $1.96$ & $27.04$ & $14.44$ \\ \hline
		$11$ & $280$ & $292$ & $144$ & $249.64$ & $14.44$ \\ \hline
		$12$ & $303$ & $292$ & $121$ & $51.84$ & $14.44$ \\ \hline
		$13$ & $294$ & $292$ & $4$ & $3.24$ & $14.44$ \\ \hline
		$14$ & $270$ & $292$ & $484$ & $665.64$ & $14.44$ \\ \hline
		$15$ & $276$ & $292$ & $256$ & $392.04$ & $14.44$ \\ \hline
		$16$ & $310$ & $292$ & $324$ & $201.64$ & $14.44$ \\ \hline
		$17$ & $290$ & $292$ & $4$ & $33.64$ & $14.44$ \\ \hline
		$18$ & $312$ & $292$ & $400$ & $262.44$ & $14.44$ \\ \hline
		$19$ & $260$ & $292$ & $1024$ & $1281.64$ & $14.44$ \\ \hline
		$20$ & $325$ & $292$ & $1048$ & $852.64$ & $14.44$ \\ \hhline{|=|=|=|=|=|=|}
		&  & \textbf{Total} & $\mathbf{5976.4}$ & $\mathbf{6265.2}$ & $ \mathbf{288.8}$\\ \hline
		\end{tabular}}
	\end{table}
	with obviously:
	
	The calculations of all the classical ANOVA terms of the regression then give still with the same version of Microsoft Excel (the reader can verify by hand using the relations proved earlier above that we find the values given by this spreadsheet software):
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/anova_for_regression_in_ms_excel.jpg}
	\end{figure}
	Or more explicitly:
	\begin{table}[H]\small
		\renewcommand{\arraystretch}{1.2}
		\centering
		\begin{tabular}{llcccc}\hline
		\rowcolor[gray]{0.75}\textbf{Source} & \textbf{Sum of squares (SSE)} & $\chi^2$ \textbf{df} & \textbf{Mean squares} & $F$ & \textbf{Critical} $F$\\ \hline
		Inter-Class & $Q_A=\displaystyle\sum_{i}\left(\hat{y}_{i}-\bar{y}\right)^2$ & $k-1$ & $\text{MSk}=\displaystyle\dfrac{Q_A}{k-1}$ &
		$\displaystyle\dfrac{\text{MSk}}{\text{MSE}}$ & $P(F> F_{k-1,N-k})$ \\
		Intra-Class & $Q_R=\displaystyle\sum_{i}\left(y_i-\hat{y}_i\right)^2$ & $N-k$ & $ \text{MSE}=\displaystyle\dfrac{Q_R}{N-k}$  & & \\
		Total & $Q_T=\displaystyle\sum_{i}\left(y_i-\bar{y}\right)^2$ & $N-1$ & & &\\ \hline
		\end{tabular}
		\caption{ANOVA table for regression}
	\end{table}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Notice that in the above ANOVA table we recognize something known to us:
	
	That is... the $\text{SEE}^2$, where for recall SEE is named the "standard error of estimate" or "standard regression error" (see page \pageref{standard error of estimate})! This is why in all statistical softwares you can simply rely the standard error of estimate of the regression with the intra-class mean sum of square (i.e. mean residuals errors) just by taking the square root of that latter such that: $\text{SEE}=\sqrt{\text{MSE}}$.
	\end{tcolorbox}
	And running the one-fixed factor ANOVA we had already introduced a little earlier above we have for comparison:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/anova_one_factor_for_comparison_with_regression_anova_excel.jpg}
	\end{figure}
	With for recall (again!) its general table:
	\begin{table}[H]\small
		\renewcommand{\arraystretch}{1.2}
		\centering
		\begin{tabular}{llcccc}\hline
		\rowcolor[gray]{0.75}\textbf{Source} & \textbf{Sum of squares (SSE)} & $\chi^2$ \textbf{df} & \textbf{Mean squares} & $F$ & \textbf{Critical} $F$\\ \hline
		Inter-Class & $Q_A=\displaystyle\sum_{i}n_i\left(\bar{x}_{i}-\bar{\bar{x}}\right)^2$ & $k-1$ & $\text{MSk}=\displaystyle\dfrac{Q_A}{k-1}$ &
		$\displaystyle\dfrac{\text{MSk}}{\text{MSE}}$ & $P(F> F_{k-1,N-k})$ \\
		Intra-Class & $Q_R=\displaystyle\sum_{ij}\left(x_{ij}-\bar{x}_i\right)^2$ & $N-k$ & $ \text{MSE}=\displaystyle\dfrac{Q_R}{N-k}$  & & \\
		Total & $Q_T=\displaystyle\sum_{ij}\left(x_{ij}-\bar{\bar{x}}\right)^2$ & $N-1$ & & &\\ \hline
		\end{tabular}
		\caption[]{One way fixed factor ANOVA table}
	\end{table}
	We see then the obvious similarity that there are with the two approaches!!! This is why some people say sometimes: « \textit{running a regression or an ANOVA are two equivalent thinks (assumed under some given conditions)} » as in the ANOVA, the categorical variable is effect coded, which means that each category's mean is compared to the grand mean. In the regression, the categorical variable is dummy coded, which means that each category intercept is compared to the reference group intercept! Since the intercept is defined as the mean value when all other predictors $= 0$, and there are no other predictors, the three intercepts are just means.
	
	Indeed, we have:
	
	and:
	
	
	\subparagraph{Test for Lack of Fit}\label{test for lack of fit}\index{statistical tests!Lack of fit $F$-test}\mbox{}\\\\\
	The "\NewTerm{test for lack-of-fit}\index{test for lack of fit}\index{lack of fit}" compares the variation around the model with "pure" variation within replicated observations (some statistical softwares use this analysis only for the adequacy of quadratic response surface and not for simple linear models...). In particular, if there are $n_i$ replicated observations $y_{i1}, \ldots  ,y_{in_i}$ of the response all at the same values $\vec{x}_i$ (vector of dimension $n$) of the factors, then we can predict the true response at $\vec{x}_i$ either by using the predicted value $\hat{y}_i$ based on the model or by using the mean $\bar{y}_i$ of the replicated values. The test for lack-of-fit decomposes the residual error into a component due to the variation of the replications around their mean value (the "\NewTerm{pure error}\index{pure error}"), and a component due to the variation of the mean values around the model prediction (the "\NewTerm{lack of fit}" also named the "\NewTerm{bias error}\index{bias error}"):
	
	or written differently:
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} The reader should easily notice that without repeated measurements, $\text{SSPE}=0$ and hence we cannot conduct a lack-of-fit analysis!\\
	
	\textbf{R2.} Obviously for one $x_i$ we have multiple $y_i$, hence the notation $y_{ij}$. But the model for a given $x_i$ gives only one and only one $\hat{y}_i$. Hence $\forall j$ and a given $i$, we have $\hat{y}_{ij}=\hat{y}_i$.
	\end{tcolorbox}	
	If the model is adequate, then both components estimate the nominal level of error; however, if the bias component of error is much larger than the pure error, then this constitutes evidence that there is significant lack of fit. The concept can be illustrated as following:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1215); %set diagram left start at 0, and has height of 1215
		
		%Straight Lines [id:da12790256924987742] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=0.75]  [dash pattern={on 4.5pt off 4.5pt}]  (380.5,89) -- (348.5,89) ;
		%Straight Lines [id:da5442549867436841] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=0.75]  [dash pattern={on 4.5pt off 4.5pt}]  (160.5,157) -- (128.5,157) ;
		%Shape: Axis 2D [id:dp4012440354243978] 
		\draw  (91,279.06) -- (443.5,279.06)(101.5,64) -- (101.5,293) (436.5,274.06) -- (443.5,279.06) -- (436.5,284.06) (96.5,71) -- (101.5,64) -- (106.5,71)  ;
		%Straight Lines [id:da3218034213482972] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (93,235) -- (426.5,99) ;
		%Shape: Circle [id:dp7069618401657551] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (155.5,173) .. controls (155.5,170.24) and (157.74,168) .. (160.5,168) .. controls (163.26,168) and (165.5,170.24) .. (165.5,173) .. controls (165.5,175.76) and (163.26,178) .. (160.5,178) .. controls (157.74,178) and (155.5,175.76) .. (155.5,173) -- cycle ;
		%Shape: Circle [id:dp24902944094830226] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (155.5,157) .. controls (155.5,154.24) and (157.74,152) .. (160.5,152) .. controls (163.26,152) and (165.5,154.24) .. (165.5,157) .. controls (165.5,159.76) and (163.26,162) .. (160.5,162) .. controls (157.74,162) and (155.5,159.76) .. (155.5,157) -- cycle ;
		%Shape: Circle [id:dp8722228969305306] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (155.5,141) .. controls (155.5,138.24) and (157.74,136) .. (160.5,136) .. controls (163.26,136) and (165.5,138.24) .. (165.5,141) .. controls (165.5,143.76) and (163.26,146) .. (160.5,146) .. controls (157.74,146) and (155.5,143.76) .. (155.5,141) -- cycle ;
		%Shape: Circle [id:dp3432759593629737] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (258.5,223) .. controls (258.5,220.24) and (260.74,218) .. (263.5,218) .. controls (266.26,218) and (268.5,220.24) .. (268.5,223) .. controls (268.5,225.76) and (266.26,228) .. (263.5,228) .. controls (260.74,228) and (258.5,225.76) .. (258.5,223) -- cycle ;
		%Shape: Circle [id:dp7375304863559105] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (258.5,207) .. controls (258.5,204.24) and (260.74,202) .. (263.5,202) .. controls (266.26,202) and (268.5,204.24) .. (268.5,207) .. controls (268.5,209.76) and (266.26,212) .. (263.5,212) .. controls (260.74,212) and (258.5,209.76) .. (258.5,207) -- cycle ;
		%Shape: Circle [id:dp29192371740251355] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (258.5,191) .. controls (258.5,188.24) and (260.74,186) .. (263.5,186) .. controls (266.26,186) and (268.5,188.24) .. (268.5,191) .. controls (268.5,193.76) and (266.26,196) .. (263.5,196) .. controls (260.74,196) and (258.5,193.76) .. (258.5,191) -- cycle ;
		%Shape: Circle [id:dp6614782796994347] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (375.5,105) .. controls (375.5,102.24) and (377.74,100) .. (380.5,100) .. controls (383.26,100) and (385.5,102.24) .. (385.5,105) .. controls (385.5,107.76) and (383.26,110) .. (380.5,110) .. controls (377.74,110) and (375.5,107.76) .. (375.5,105) -- cycle ;
		%Shape: Circle [id:dp5182754137339056] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (375.5,89) .. controls (375.5,86.24) and (377.74,84) .. (380.5,84) .. controls (383.26,84) and (385.5,86.24) .. (385.5,89) .. controls (385.5,91.76) and (383.26,94) .. (380.5,94) .. controls (377.74,94) and (375.5,91.76) .. (375.5,89) -- cycle ;
		%Shape: Circle [id:dp9175694916026644] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (375.5,73) .. controls (375.5,70.24) and (377.74,68) .. (380.5,68) .. controls (383.26,68) and (385.5,70.24) .. (385.5,73) .. controls (385.5,75.76) and (383.26,78) .. (380.5,78) .. controls (377.74,78) and (375.5,75.76) .. (375.5,73) -- cycle ;
		%Straight Lines [id:da5630504273249703] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=0.75]  [dash pattern={on 4.5pt off 4.5pt}]  (263.5,207) -- (231.5,207) ;
		%Straight Lines [id:da1635094886037458] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (173,136) -- (173,177) ;
		\draw [shift={(173,179)}, rotate = 270] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(173,134)}, rotate = 90] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da09776307980956189] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (277,187) -- (277,228) ;
		\draw [shift={(277,230)}, rotate = 270] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(277,185)}, rotate = 90] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da638200342546589] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (393,71) -- (393,107) ;
		\draw [shift={(393,109)}, rotate = 270] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(393,69)}, rotate = 90] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da5729612956397807] 
		\draw [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ]   (140,162) -- (140,213) ;
		\draw [shift={(140,215)}, rotate = 270] [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(140,160)}, rotate = 90] [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da3059648512074291] 
		\draw [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ]   (247.5,174) -- (247.5,205) ;
		\draw [shift={(247.5,207)}, rotate = 270] [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(247.5,172)}, rotate = 90] [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da021189863931652342] 
		\draw [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ]   (364.5,91) -- (364.5,122) ;
		\draw [shift={(364.5,124)}, rotate = 270] [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(364.5,89)}, rotate = 90] [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da8995389355558165] 
		\draw [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ]   (480,91) -- (480,134) ;
		\draw [shift={(480,136)}, rotate = 270] [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(480,89)}, rotate = 90] [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da4494947093929329] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (480,156) -- (480,197) ;
		\draw [shift={(480,199)}, rotate = 270] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(480,154)}, rotate = 90] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da051258287904169864] 
		\draw [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ]   (225.5,307) -- (225.5,338) ;
		\draw [shift={(225.5,340)}, rotate = 270] [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(225.5,305)}, rotate = 90] [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da49773411072472573] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (259.5,307) -- (259.5,338) ;
		\draw [shift={(259.5,340)}, rotate = 270] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(259.5,305)}, rotate = 90] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da9990355697223947] 
		\draw [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ]   (116.5,359) -- (116.5,390) ;
		\draw [shift={(116.5,392)}, rotate = 270] [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(116.5,357)}, rotate = 90] [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da16061556994192072] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (146.5,359) -- (146.5,390) ;
		\draw [shift={(146.5,392)}, rotate = 270] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(146.5,357)}, rotate = 90] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da9184190056264834] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (192.5,359) -- (192.5,390) ;
		\draw [shift={(192.5,392)}, rotate = 270] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(192.5,357)}, rotate = 90] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		
		% Text Node
		\draw (82,62.4) node [anchor=north west][inner sep=0.75pt]    {$\hat{y}$};
		% Text Node
		\draw (431,286.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (345,138.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,opacity=1 ]  {$\hat{y} =\alpha x+\beta $};
		% Text Node
		\draw (494,103) node [anchor=north west][inner sep=0.75pt]   [align=left] {lack of fit};
		% Text Node
		\draw (494,169) node [anchor=north west][inner sep=0.75pt]   [align=left] {pure error};
		% Text Node
		\draw (92,314) node [anchor=north west][inner sep=0.75pt]   [align=left] {Principle: compare \ \ \ \ to \ \ \ \ };
		% Text Node
		\draw (94,364) node [anchor=north west][inner sep=0.75pt]   [align=left] {if \ \ \ \ \ $\displaystyle -$ \ \ \ \ \ $\displaystyle  >$ \ \ \ \ \ then change the model!};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Lack of fit and pure error}
	\end{figure}
	
	\begin{itemize}
		\item As before, the degrees of freedom associated with SSE (i.e. $Q_R$) is as we know very well given by $n(k-1)=N-k$ (keep in mind that the $k$ comes from the fact that you estimate $k$ parameters - the slopes plus the intercept - whenever you fit a line to a set of data).
	
		\item The degrees of freedom associated with SSLF is $n-k$, where $c$ denotes the number of distinct $x$ values we have.
	
		\item The degrees of freedom associated with SSPE is $nk-n=N-n$, where again $n$ denotes the number of distinct $x$ values we have.
	\end{itemize}
	Therefore the degrees of freedom breakdown as:
	
	It then follows that the statistic:
	
	has an $F$-distribution with the corresponding number of degrees of freedom in the numerator and the denominator, provided that the model is correct. 
	
	To understand this test keep in mind that under the null hypothesis:
	\begin{itemize}
		\item The sum of squares due to pure error, divided by the error variance $\sigma_\varepsilon^2$, has a chi-squared distribution with $N-n$ degrees of freedom.

		\item The sum of squares due to lack of fit, divided by the error variance $\sigma_\varepsilon^2$, has a chi-squared distribution with $n-k$ degrees of freedom.

		\item The two sums of squares are probabilistically independent.
	\end{itemize}
	If the model is wrong, then the probability distribution of the denominator is still as stated above, and the numerator and denominator are still independent. But the numerator then has a noncentral chi-squared distribution (\SeeChapter{see section Statistics page \pageref{noncentral chi-square distribution}}) and consequently the quotient as a whole has a noncentral $F$-distribution.
	
	One uses this $F$-statistic to test the null hypothesis that there is no lack of linear fit. Since the non-central $F$-distribution is stochastically larger than the (central) $F$-distribution, one rejects the null hypothesis if the $F$-statistic is larger than the critical $F$ value.
	
	So we complete the previous analysis of variance table using the following relations:
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|l|l|l|}
		\hline
		\rowcolor[gray]{0.75} 
		\textbf{Source} & $\chi^2$ \textbf{df} & \textbf{SS} & \textbf{Means squares} & \textbf{$\pmb{F}$} \\ \hline
		Regression & $k-1$ & $\text{SSR}=\displaystyle\sum_{i=1}^{n}n_i(\hat{y}_{i}-\bar{y})^2$ & $\text{MSR}=\displaystyle\dfrac{\text{SSR}}{k-1}$ & $F=\displaystyle\dfrac{\text{MSR}}{\text{MSE}}$ \\ \hline
		Residual error & $N-k$ & $\text{SSE}=\displaystyle\sum_{i=1}^{n}\sum_{j=1}^{n_i}(y_{ij}-\hat{y}_{i})^2$ & $\text{MSE}=\displaystyle\dfrac{\text{SSE}}{N-k}$ &  \\ \hline
		Lack of Fit & $n-k$ & $\text{SSLF}=\displaystyle\sum_{i=1}^{n}n_i(\bar{y}_{i}-\hat{y}_{i})^2$ & $\text{MSLF}=\dfrac{\text{SSLF}}{n-k}$ & $F=\dfrac{\text{MSLF}}{\text{MSPE}}$ \\ \hline
		Pure error & $N-n$ & $\text{SSPE}=\displaystyle\sum_{i=1}^{n}\sum_{j=1}^{n_i}(y_{ij}-\bar{y}_{i})^2$ & $\text{MSPE}=\dfrac{\text{SSPE}}{N-k}$ &  \\ \hline
		Total & $N-1$ & $\text{SSTO}=\displaystyle\sum_{i=1}^{n}\sum_{j=1}^{n_i}(y_{ij}-\bar{y})^2$ &  &  \\ \hline
		\end{tabular}
		\caption{Lack-of-Fit ANOVA table}
	\end{table}
	The reader must be careful with the "Total" in the above table (last row). Indeed, the total must not take into account the row with the degrees of freedom $n-k$ and $N-n$ as their are the decomposition of the row with the $N-k$.
	
	\pagebreak
	\subsubsection{Univariate linear regression Gaussian Model}\label{univariate linear regression gaussian model}
	We will assume that for an individual $k$ picked randomly from the population, $x_k$ is known without error, and that $y_k$ is a realization of a random variable that we will now denote $Y_k$ and the theoretical least squares regression line will be written now under the general form of a "\NewTerm{stochastic linear model}\index{stochastic linear model}":
	
	where $\varepsilon_k$ is assumed identically distributed and independent residue (no correlation) for each point $k$ according to a centered Normal distribution (zero mean and standard deviation $\sigma$ for all $k$) such as $\forall i\neq j$:
\begin{equation}
  \addtolength{\fboxsep}{5pt}
   \boxed{
   \begin{gathered}
		\begin{aligned}
			\varepsilon_k=\mathcal{N}(0,\sigma)\\
			\text{cov}(\varepsilon_i,\varepsilon_j)=0
		\end{aligned}
   \end{gathered}
   }
\end{equation}
	So in other words\footnote{The conditions $\text{cov}(\varepsilon_i,\varepsilon_j)=0$, and $\text{E}(\varepsilon_k)=0$ and $\text{V}(\varepsilon_k)=\sigma_\varepsilon^2$ are named the "\NewTerm{Gauss-Markov assumptions}" (see page \pageref{Gauss-Markov theorem}) and are at the basis of the "\NewTerm{Gauss-Markov Linear Model}\index{Gauss-Markov linear model}" (GMLM).}:
	
	where the residue is defined by the difference between the theoretical ordinates $Y_k$ (considered as random variable) and the measured (experimental) ordinates $y_k$:
	
	and since by hypothesis $\varepsilon_k=\mathcal{N}(0,\sigma)$, it immediately comes by the stability of the Normal law (\SeeChapter{see section Statistics page \pageref{stability of the sum in statistics}}) that:
	
	This is why this specific model is named "\NewTerm{Gaussian linear model}\index{Gaussian linear model}" .... Explicitly, we have:
	
	This is why theoretical model is formally denoted: 
	
	We will choose the symbol $\sim$ to say "follows the law ..." in order to avoid any notation confusion:
	
	Which graphically is equivalent to have (so normally the statistical analysis of the Gaussian regression occurs only if and only if we have taken several measures of the dependent variable for fixed and identical values of the independent variables!!!!!!!!):
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/gaussian_regression.jpg}
		\caption{Graphical representation of the idea behind the Gaussian linear regression}
	\end{figure}
	
	 \begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Caution! Since the model is Gaussian, the variable to explain has its domain of definition which is unbounded (support of the Normal distribution). Some companies (especially auditing firms...) wish sometimes to create a simple linear model to model a  probability (that for reminder is bounded in [0,1]) typically for the probability of bankruptcy/default based on various factors (explanatory variables ). Therefore the measured probability must first be turn into $Z$ values (quantiles) of the Normal distribution to have again an infinite range. This type of approach is then named "\NewTerm{linear $Z$-score model}\index{linear $Z$-score model}".
	\end{tcolorbox}
	

	Almost all statistical analysis software provide a pattern (figure) of residues according to the $x$ values. Thus, these type of figures help to accept or reject the use of a linear Gaussian model:

	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1077); %set diagram left start at 0, and has height of 1077
		
		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_16emh80iz}{
		\pgfdeclarepatternformonly[\mcThickness,\mcSize]{_16emh80iz}
		{\pgfqpoint{0pt}{0pt}}
		{\pgfpoint{\mcSize+\mcThickness}{\mcSize+\mcThickness}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathmoveto{\pgfqpoint{0pt}{0pt}}
		\pgfpathlineto{\pgfpoint{\mcSize+\mcThickness}{\mcSize+\mcThickness}}
		\pgfusepath{stroke}
		}}

		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_iuiai66by}{
		\pgfdeclarepatternformonly[\mcThickness,\mcSize]{_iuiai66by}
		{\pgfqpoint{0pt}{0pt}}
		{\pgfpoint{\mcSize+\mcThickness}{\mcSize+\mcThickness}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathmoveto{\pgfqpoint{0pt}{0pt}}
		\pgfpathlineto{\pgfpoint{\mcSize+\mcThickness}{\mcSize+\mcThickness}}
		\pgfusepath{stroke}
		}}
		
		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_h28cmsxlq}{
		\pgfdeclarepatternformonly[\mcThickness,\mcSize]{_h28cmsxlq}
		{\pgfqpoint{0pt}{0pt}}
		{\pgfpoint{\mcSize+\mcThickness}{\mcSize+\mcThickness}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathmoveto{\pgfqpoint{0pt}{0pt}}
		\pgfpathlineto{\pgfpoint{\mcSize+\mcThickness}{\mcSize+\mcThickness}}
		\pgfusepath{stroke}
		}}
		
		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_yon4l0pbc}{
		\pgfdeclarepatternformonly[\mcThickness,\mcSize]{_yon4l0pbc}
		{\pgfqpoint{0pt}{0pt}}
		{\pgfpoint{\mcSize+\mcThickness}{\mcSize+\mcThickness}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathmoveto{\pgfqpoint{0pt}{0pt}}
		\pgfpathlineto{\pgfpoint{\mcSize+\mcThickness}{\mcSize+\mcThickness}}
		\pgfusepath{stroke}
		}}
		
		%Shape: Axis 2D [id:dp342402131873474] 
		\draw  (69,242) -- (309,242)(83,46) -- (83,257) (302,237) -- (309,242) -- (302,247) (78,53) -- (83,46) -- (88,53) (103,237) -- (103,247)(123,237) -- (123,247)(143,237) -- (143,247)(163,237) -- (163,247)(183,237) -- (183,247)(203,237) -- (203,247)(223,237) -- (223,247)(243,237) -- (243,247)(263,237) -- (263,247)(283,237) -- (283,247)(78,222) -- (88,222)(78,202) -- (88,202)(78,182) -- (88,182)(78,162) -- (88,162)(78,142) -- (88,142)(78,122) -- (88,122)(78,102) -- (88,102)(78,82) -- (88,82)(78,62) -- (88,62) ;
		\draw   ;
		%Straight Lines [id:da4514830478087919] 
		\draw    (67,94) -- (67,185) ;
		\draw [shift={(67,187)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(67,92)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da036814829983431885] 
		\draw    (103,102) -- (287,102) ;
		%Straight Lines [id:da9719925499843172] 
		\draw    (103,181) -- (287,181) ;
		%Shape: Rectangle [id:dp4846543443064013] 
		\draw  [draw opacity=0][pattern=_16emh80iz,pattern size=6pt,pattern thickness=0.75pt,pattern radius=0pt, pattern color={rgb, 255:red, 155; green, 155; blue, 155}] (103,102) -- (287,102) -- (287,181) -- (103,181) -- cycle ;
		%Shape: Axis 2D [id:dp3202542061076281] 
		\draw  (373,243) -- (613,243)(387,47) -- (387,258) (606,238) -- (613,243) -- (606,248) (382,54) -- (387,47) -- (392,54) (407,238) -- (407,248)(427,238) -- (427,248)(447,238) -- (447,248)(467,238) -- (467,248)(487,238) -- (487,248)(507,238) -- (507,248)(527,238) -- (527,248)(547,238) -- (547,248)(567,238) -- (567,248)(587,238) -- (587,248)(382,223) -- (392,223)(382,203) -- (392,203)(382,183) -- (392,183)(382,163) -- (392,163)(382,143) -- (392,143)(382,123) -- (392,123)(382,103) -- (392,103)(382,83) -- (392,83)(382,63) -- (392,63) ;
		\draw   ;
		%Straight Lines [id:da49311514634434594] 
		\draw    (371,95) -- (371,186) ;
		\draw [shift={(371,188)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(371,93)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da6434433588000086] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (83,142) -- (298,142) ;
		%Straight Lines [id:da9343634730660049] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (389.5,142.5) -- (604.5,142.5) ;
		%Shape: Trapezoid [id:dp4316457957887996] 
		\draw  [draw opacity=0][pattern=_iuiai66by,pattern size=6pt,pattern thickness=0.75pt,pattern radius=0pt, pattern color={rgb, 255:red, 155; green, 155; blue, 155}] (583,191) -- (401,155) -- (401,129) -- (583,93) -- cycle ;
		%Straight Lines [id:da9208009574732485] 
		\draw    (401,129) -- (583,93) ;
		%Straight Lines [id:da9330655243106389] 
		\draw    (401,155) -- (583,191) ;
		%Shape: Axis 2D [id:dp35094498319278467] 
		\draw  (69.5,533) -- (309.5,533)(83.5,337) -- (83.5,548) (302.5,528) -- (309.5,533) -- (302.5,538) (78.5,344) -- (83.5,337) -- (88.5,344) (103.5,528) -- (103.5,538)(123.5,528) -- (123.5,538)(143.5,528) -- (143.5,538)(163.5,528) -- (163.5,538)(183.5,528) -- (183.5,538)(203.5,528) -- (203.5,538)(223.5,528) -- (223.5,538)(243.5,528) -- (243.5,538)(263.5,528) -- (263.5,538)(283.5,528) -- (283.5,538)(78.5,513) -- (88.5,513)(78.5,493) -- (88.5,493)(78.5,473) -- (88.5,473)(78.5,453) -- (88.5,453)(78.5,433) -- (88.5,433)(78.5,413) -- (88.5,413)(78.5,393) -- (88.5,393)(78.5,373) -- (88.5,373)(78.5,353) -- (88.5,353) ;
		\draw   ;
		%Straight Lines [id:da8124794636946449] 
		\draw    (67.5,385) -- (67.5,476) ;
		\draw [shift={(67.5,478)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(67.5,383)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Axis 2D [id:dp0780095815492734] 
		\draw  (373.5,534) -- (613.5,534)(387.5,338) -- (387.5,549) (606.5,529) -- (613.5,534) -- (606.5,539) (382.5,345) -- (387.5,338) -- (392.5,345) (407.5,529) -- (407.5,539)(427.5,529) -- (427.5,539)(447.5,529) -- (447.5,539)(467.5,529) -- (467.5,539)(487.5,529) -- (487.5,539)(507.5,529) -- (507.5,539)(527.5,529) -- (527.5,539)(547.5,529) -- (547.5,539)(567.5,529) -- (567.5,539)(587.5,529) -- (587.5,539)(382.5,514) -- (392.5,514)(382.5,494) -- (392.5,494)(382.5,474) -- (392.5,474)(382.5,454) -- (392.5,454)(382.5,434) -- (392.5,434)(382.5,414) -- (392.5,414)(382.5,394) -- (392.5,394)(382.5,374) -- (392.5,374)(382.5,354) -- (392.5,354) ;
		\draw   ;
		%Straight Lines [id:da09047659402306052] 
		\draw    (371.5,386) -- (371.5,477) ;
		\draw [shift={(371.5,479)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(371.5,384)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da739494887003487] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (83.5,433) -- (298.5,433) ;
		%Straight Lines [id:da2351066573925642] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (390,433.5) -- (605,433.5) ;
		%Shape: Parallelogram [id:dp0714505256363569] 
		\draw  [draw opacity=0][pattern=_h28cmsxlq,pattern size=6pt,pattern thickness=0.75pt,pattern radius=0pt, pattern color={rgb, 255:red, 155; green, 155; blue, 155}] (282.82,403.99) -- (98.84,512.15) -- (99.18,462.01) -- (283.16,353.85) -- cycle ;
		%Straight Lines [id:da37511840651896655] 
		\draw    (99.18,462.01) -- (283.16,353.85) ;
		%Straight Lines [id:da9588077283131775] 
		\draw    (98.84,512.15) -- (282.82,403.99) ;
		%Shape: Polygon Curved [id:ds6981106453377466] 
		\draw  [draw opacity=0][pattern=_yon4l0pbc,pattern size=6pt,pattern thickness=0.75pt,pattern radius=0pt, pattern color={rgb, 255:red, 155; green, 155; blue, 155}] (405,443) .. controls (405,430) and (457.3,389.69) .. (498,390) .. controls (538.7,390.31) and (586.97,436.34) .. (588,442) .. controls (589.03,447.66) and (588,476) .. (588,484) .. controls (588,492) and (540.73,440.23) .. (498,440) .. controls (455.27,439.77) and (406,499) .. (405,489) .. controls (404,479) and (405,456) .. (405,443) -- cycle ;
		%Curve Lines [id:da06425736807684324] 
		\draw    (405,443) .. controls (471,364) and (525,378) .. (588,442) ;
		%Curve Lines [id:da6510058796200882] 
		\draw    (405,489) .. controls (482,426) and (510,429) .. (588,484) ;
		
		% Text Node
		\draw (42,17.4) node [anchor=north west][inner sep=0.75pt]    {$\varepsilon =Y_{k} -y_{k}$};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (59,127) -- (77,127) -- (77,151) -- (59,151) -- cycle  ;
		\draw (62,131.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (46,104.4) node [anchor=north west][inner sep=0.75pt]    {$+$};
		% Text Node
		\draw (47,159.4) node [anchor=north west][inner sep=0.75pt]    {$-$};
		% Text Node
		\draw (19.5,167.5) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {residuals};
		% Text Node
		\draw (159,258) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle Y$ values};
		% Text Node
		\draw (346,18.4) node [anchor=north west][inner sep=0.75pt]    {$\varepsilon =Y_{k} -y_{k}$};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (363,128) -- (381,128) -- (381,152) -- (363,152) -- cycle  ;
		\draw (366,132.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (350,105.4) node [anchor=north west][inner sep=0.75pt]    {$+$};
		% Text Node
		\draw (351,160.4) node [anchor=north west][inner sep=0.75pt]    {$-$};
		% Text Node
		\draw (323.5,168.5) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {residuals};
		% Text Node
		\draw (463,259) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle Y$ values};
		% Text Node
		\draw (42.5,308.4) node [anchor=north west][inner sep=0.75pt]    {$\varepsilon =Y_{k} -y_{k}$};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (59.5,418) -- (77.5,418) -- (77.5,442) -- (59.5,442) -- cycle  ;
		\draw (62.5,422.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (46.5,395.4) node [anchor=north west][inner sep=0.75pt]    {$+$};
		% Text Node
		\draw (47.5,450.4) node [anchor=north west][inner sep=0.75pt]    {$-$};
		% Text Node
		\draw (20,458.5) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {residuals};
		% Text Node
		\draw (159.5,549) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle Y$ values};
		% Text Node
		\draw (346.5,309.4) node [anchor=north west][inner sep=0.75pt]    {$\varepsilon =Y_{k} -y_{k}$};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (363.5,419) -- (381.5,419) -- (381.5,443) -- (363.5,443) -- cycle  ;
		\draw (366.5,423.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (350.5,396.4) node [anchor=north west][inner sep=0.75pt]    {$+$};
		% Text Node
		\draw (351.5,451.4) node [anchor=north west][inner sep=0.75pt]    {$-$};
		% Text Node
		\draw (324,459.5) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {residuals};
		% Text Node
		\draw (463.5,550) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle Y$ values};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Examples of "plot" of residuals}
	\end{figure} 

	In the above figure, the graph on the top left is what we should expect to have to be able to apply the statistical tests for the Gaussian linear model. The graph on the top right shows that the residuals variance is not constant and therefore we have a violation of the assumption of homoscedasticity (typically checked with statistical hypothesis tests like Levene, Bartlett, Fligner, Brown-Forsythe, etc.)! The graph at the bottom left shows that the variance is constant but that our model has missing endogenous variables that added could perhaps explain the shift that grows up linearly. The graph at the bottom right indicates a constant variance, but the model looks like to be more nonlinear than linear.

	Previous assumptions about the moments of residues (mean, variance) are named "\NewTerm{Gauss-Markov assumptions}\index{Gauss-Markov assumptions}" and the particular hypothesis of equal variances is named as we saw it in the section Statistics "\NewTerm{homoscedasticity}\index{homoscedasticity}" (while the fact that the variances are not equal is named for reminder "heteroskedasticity").

	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
\textbf{R1. }The majority of softwares (including Microsoft Excel 11.8346) propose a graph which shows the residuals in function of the ordinate values $x$. Obviously, it is better that the points representing the residues are not too divergent ... otherwise the homoscedasticity assumption will be not satisfied.\\

\textbf{R2.} Practitioners sometimes transform the endogenous and exogenous variables by a logarithmic or exponential function or other to try to stabilize the residuals as most as possible.
	\end{tcolorbox}	
	
	\begin{theorem}
	We have through the property of the mean (\SeeChapter{see section Statistics page \pageref{properties of the mean}}):
	
	\end{theorem}
	Then under the above assumptions, we will show that $a$ and $b$ are unbiased estimators (\SeeChapter{see section Statistics page \pageref{unbiased estimator}}) of $\alpha$ and $\beta$ and it is possible to estimate the standard deviation from the SSR which is an important result named "\NewTerm{Gauss-Markov theorem}\index{Gauss-Markov theorem}" (see page \pageref{Gauss-Markov theorem}).
	
	Before seeing the proof let us do a recall and give some definitions of the variables that we have already handled and the new one that we will handle (if the vocabulary seems technical to the reader then it should read or reread the sections of Probabilities and Statistics):
	
	\begin{table}[H]
	\begin{center}
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|p{1cm}|p{10cm}|}
				\hline
				\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Variable}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Description}} \\ \hline
				$a$ & Slope (director coefficient) of the linear model of the least squares method (LSM). This is a punctual estimate value (deterministic). \\ \hline
				$b$ & Intercept of the linear model of the least squares method (LSM). This is a punctual estimate value (deterministic).\\ \hline
				$A$ & Random variable of the slope (director coefficient) according to the statistical Gaussian model approach and for which $a$ is a realization. The expected mean of $A$ being an unbiased estimator of $\alpha$.\\ \hline
				$B$ & Random variable of the ordinate at the origin according to the statistical approach of the linear Gaussian model and for which $b$ is a realization. The expected mean of $B$ being an unbiased estimator of $\beta$.  \\ \hline
				$\alpha$ &	Unbiased expected mean of the variable $A$ representing the slope (director coefficient) within the framework of the statistical approach of the Gaussian linear model.  \\ \hline
				$\beta$ & Unbiased expected mean of the variable $B$ representing the $y$-intercept (ordinate at origin) in the framework of the statistical approach of linear Gaussian model.  \\ \hline
		\end{tabular}
	\end{center}
	\caption{Reminder of notations for the study of the linear regression}
	\end{table}
	
	\begin{dem}
	According to the adopted model, $a$ must now be regarded as a realization of the random variable given by (shown above as the ratio of the covariance and variance):
	
	and $b$ as a realization of the random variable given by:
	
	So we differentiate random and non-random coefficient values by passing the lowercase notation in uppercase (as it is the tradition in the field of Statistics).
	
	Taking into account that theoretical dependent (endogenous) variable is considered as the realization of a random variable is therefore given by:
	
	we can put $A$ in the form\label{estimator slope OLS}:
	
	Therefore:
	
	
	And for $B$:
	
	Therefore and using also the definitions:
	
	with the same conclusion.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	So the expected mean of $A$ and $B$ are unbiased estimators (i.e.. with minimum variance as seen in the section of Statistics) of. As they are estimators, in the literature, they are often noted $\hat{a},\hat{}$ and therefore it comes the common alternative notation:
		
	Finally, we must also calculate the variances of $A$ and $B$ using its properties (\SeeChapter{see section Statistics page \pageref{properties of the variance}}) and the assumptions on the residuals, we have:
	
	As by assumption we have that all the $\text{V}(\varepsilon_k)$ are equal and that there is no autocorrelation (what is typically tested with the Durbin-Watson test derived at page \pageref{Durbin-Watson test}), we can write:
	
	And if $n$ is large enough we will write:
	
	Before determining the variance of $B$, remember that by hypothesis:
	
	therefore by the property of linearity of the Normal distribution, the random variables $A$ and $B$ also follow a Normal distribution.
	
	After the recall of this assumption, it follows immediately (\SeeChapter{see section Statistics page \pageref{standard error}}):
	
	Therefore:
	
	Remember the Huygens theorem (\SeeChapter{see section Statistics page \pageref{huygens relation}}):
	
	in the case of equitable probabilities (Normal Law estimator of the mean as prove in the section Statistics).
	
	Finally we have:
	
	where obviously the notation of the variance in the denominator is very unfair (because $x$ is not a random variable in this model) but very convenient to condense the notation.
	
	The problem now lies in determining $\sigma_\varepsilon^2$. Obviously to do this we will be forced to go through a statistical estimator.
	
	We know we can write according to what was seen in the section Statistics regarding to estimators:
	
	because the Normal distribution is centered for residuals and therefore $\bar{\varepsilon}=0$... and the residue is implicitly dependent of two the sum of two random variable that are $A$ and $B$ this is why we have a $-2$ at the denominator of the first fraction (we have to take away two degrees of freedom).
	
	Let us also indicate that in practice we frequently note the last result by mixing the notations of the random and deterministic appearance (hence noting everything with lower case):
	
	where SEE\footnote{Sadly sometimes (like in Minitab outputs) just denoted by the letter "S"...} means "\NewTerm{Standard Error of Estimate}\index{standard error of estimate}\label{standard error of estimate}" or sometimes also named  the "\NewTerm{standard regression error}\index{standard regression error}" which is obtained with the English version of Microsoft Excel trough square of the function \texttt{STEYX( )}.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	This bring us obviously to consider that standard R squared estimator uses biased estimators of $\sigma^{2}_\varepsilon$ and $\text{V}(Y)$, by using the divisor $n$ (or $n-1$) for both. These estimators are biased because they ignore the degrees of freedom used to estimate the regression coefficients and mean of $Y$. If instead, we estimate $\text{V}(Y)$ by the usual unbiased sample variance (which uses $n-1$ as the divisor) and $\sigma^{2}_\varepsilon$ by its unbiased estimator which uses $n-p-1$ (where $p$ is the number of predictors) as the divisor, we obtain the "\NewTerm{adjusted R-squared $R^2_\text{adj.}$}\index{adjusted R-squared}\label{adjusted R-squared}" estimator:
	
	From this relation, we can see that the adjusted estimator is always less than the standard one. We can also see the larger $p$ is relative to $n$, the larger the adjustment. A final point: although the adjusted $R$ squared estimator uses unbiased estimators of the residual variance and the variance of $Y$, it is not unbiased! This is because, as we know it, the mean of a ratio is not generally equal to the ratio of the means.\\
	
	Furthermore, notice, as $\hat{R}^{2}=1-\text{SSE}/\text{SST}$, we have:
	
	But for any value of $p\geq 1$:
	
	If we multiply both side by $(1-\hat{R}^{2})$, we get:
	
	Subtracting $-1$:
	
	multiplying by $-1$:
	
	Therefore:
	
	From this relation, we can see that the adjusted estimator is always less than the standard one!
	\end{tcolorbox}
	
	So we have to summarize the unbiased estimators variances of $A$ and $B$:
	
	Relations which are therefore only valid for a linear regression with one unique explanatory variable (and under the assumptions of the Gaussian linear model). Knowing that by construction of the initial hypothesis that $A$ and $B$ follow a Normal distribution of respective expected mean $\alpha,\beta$ and whose variance is given just above, so we know completely the distribution that characterizes them.
	
	What is nice knowing these variances is that we can also therefore easily estimate the variance of the dependent variable in our regression (using the properties of the variance proved in the section Statistics at page \pageref{properties of the variance}).
	
	It would be interesting to make statistical inference on the mean of the parameters $A$ and $B$ (i.e. the slope and intercept) given their known empirical mean (i.e. average). For this, remember that we have proved in the section Statistics (see page \pageref{student confidence interval of the mean}) the following confidence interval:
	
	It follows by making a parallel like engineers and physicists like to do ... that as $A$ is an unbiased estimator of the average of the slope $a$ and that:
	
	is in fact the standard error of the mean $A$, then by analogy:
	
	and then (this is a reasoning to be take with caution an it is better to use the developments that will follow later):
	
	which therefore gives the confidence interval of the slope of a linear Gaussian with one unique explanatory variable (that's what gives Microsoft Excel 11.8346 for each coefficient). The approach is the same for the intercept.
	
	Warning! If the explanatory variable is a random variable, then we use naively (the are more accurate and correct model that we will see later when the explanatory variable is a also a random variable):
	
	
	In the case of a linear regression with several explanatory variables, therefore assimilated to the concept of "\NewTerm{degrees of freedom DoF}\index{degrees of freedom}", the idea is the same but the calculations are longer (we don't have yet the will to do the mathematical developments for this case).
	
	Finally, remember that we got for the empirical correlation coefficient:
	
	We then verbatim get the famous confidence interval of the correlation coefficient:
	
	The reader should know that as calculating the confidence interval for the slope or for the correlation coefficient are two equivalent things, many softwares (Tanagra, Minitab, Microsoft Excel, etc.) give only the value of the Student's distribution at the critical value of this latter only for the slope and... they then assume that the user of the software knows that it is the same for the correlation coefficient.
	
	\paragraph{Pearson Correlation Coefficient Test}\index{Pearson's test}\index{statistical tests!Pearson's test}\mbox{}\\\\\
	The calculation obtained above for the confidence interval of the correlation coefficient is a little difficult in practice. It is for this reason that many practitioners and statistical softwares implements a very simple alternative communicated only in the minimal form that is the $p$-value.
	
	To see this approach, remember that we proved in the section of Statistics (see page \pageref{covariance}) that (this time we will adopt the correct notation...):
	
	And we saw just above that:
	
	Similarly, we have the Pearson correlation coefficient estimator which is (using here the various possible notations that we can find in the literature...):
	
	and therefore:
	
	The hypothesis test we want to do is therefore:
	
	that is equivalent to:
	
	The null hypothesis obviously being that the Pearson correlation is statistically significantly different from zero. So this is a bilateral test!
	
	To find a simple form of the test, remember that we obtained:
	
	and also:
	
	which brings us mixing the two to have:
	
	Therefore:
	
	But we remember that we have also proved that if $n$ is large enough:
	
	But if $n$ is small we fall back on:
	
	Therefore:
	
	and with the null hypothesis $a=0$, we get:
	
	
	Be careful with the use of this test often and logically named "\NewTerm{Student's $T$-test for univariate regression slope}\index{Student's $T$-test for univariate regression slope}" or "\NewTerm{$T$-test for coefficient slope}\index{$T$-test for coefficient slope}", depending if the Pearson correlation coefficient is negative or positive and don't forget that it is bilateral!
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. We have calculated for a series of data a positive Pearson correlation coefficient $R$ of value $0.298$ and the explanatory variable has $7$ values. So we have with the English version Microsoft Excel 11.8346 the $p$-value that is given by (we find exactly the same value as with Minitab 15.1.1):\\
	
	\texttt{=2*(1-T.DIST(0.298/SQRT((1-0.298\string^2)/(7-2));7-2;1))\\=2*(1-0.741869)=0.51626}\\
	
	In this case we sadly can't reject the null hypothesis as the Pearson correlation coefficient is equal to zero at a threshold of $5\%$.\\

	E2. We have calculated for a data set a positive Pearson correlation coefficient $R$ of  $-0.084$ and the explanatory variable contains $19$ values. So we have with the English version of Microsoft Excel 11.8346 the $p$-value that is equal to (we find exactly the same value as with Minitab 15.1.1):\\
	
	\texttt{=2* T.DIST((-0.084)/SQRT((1-(-0.084)\string^2)/(19-2));19-2;1)\\=2*(1-0.366)=0.74186}\\
	
	In this case we also sadly can't reject the null hypothesis as what the Pearson correlation coefficient is equal to zero at a threshold of $5\%$.
	\end{tcolorbox}
	This small trap makes that finally we take the absolute value of the Pearson correlation coefficient and we therefore use the always the same calculation method.
	
	\pagebreak
	\paragraph{Confidence interval of predicted values}\mbox{}\\\\\
	We wish for each measured value of the dependent variable, know the confidence interval. In other words, we would like to know the statistical variance estimator of $Y$ (we do not write the subscripts anymore to save time):
	
	Unfortunately, we will go into the wall because the covariance is difficult to calculate ($A$ and $B$ are not independent as shown by the expressions we got previously).
	
	By cons, being a good observer, we see that if we use the result seen above:
	
	Then:
	
	The problem being circumvented, we now have using the properties of variance:
	
	Therefore:
	
	Then we have finally:
	
	Now let us recall (\SeeChapter{see section Statistics page \pageref{reduced centered variable}}) that:
	
	and as $Y$ is distributed according to a Normal distribution for whose the unbiased estimator of the mean and standard deviation are given by the prior-previous  relation it comes immediately:
	
	In practice you must also check that this ratio follows a Normal distribution in order to make the confidence intervals and statistical tests that follow.
	
	Let us recall now that we have proved in the section Statistics (see page \pageref{student distribution}) that:
	
	follows a Student law of degrees of freedom $k$ and the variable $U$ follows a chi-square law of degree of freedom $k$.
	
	Now let us come back to the expression of the $Z$ obtained above and remember that:
	
	Therefore:
	
	But as we have the assumption:
	
	Therefore:
	
	and obviously:
	
	corresponds to a sum of squares of reduced centered Normal laws. And therefore according to what we have proved in section Statistics (see page \pageref{chi-squared sample variance proof}) it follows that:
	
	Therefore:
	
	That is to say:
	
	This is one reason why many statisticians note directly and without detours:
	
	
	Which is not necessarily obvious at first glance. This is why, following the request of a reader, we have detailed a little bit the mechanism behind this involvement\label{likelihood binomial logistic regression}.
	
	This done, we have finally:
	
	and we deduce from this immediately a bilateral confidence interval of a given threshold level $\alpha$ for a fixed $x$ whose expression is:
	

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The same kind of development can be done for the slope and the intercept. This is why software like Microsoft Excel, SPSS, Minitab, Statistica, etc. give the value of the Student $T$-distribution as well as the confidence interval for a given threshold level $\alpha$. But for this to be meaningful, you must never forget that all the assumptions of the model must be met.
	\end{tcolorbox}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/regression_confidence_interval.jpg}
		\caption{Print screen of a confidence interval obtained with Minitab 15}
	\end{figure}
	The reader will may have noticed that:
	\begin{itemize}
		\item It is very boring to obtain without software or without coding the plot of the confidence interval for the ordinary least squares since we have to calculate it for each point...
		
		\item The confidence interval is curved which is sometimes considered as common sense, at least in the temporal version of the regression: furthest is the forecast less accurate it will be
	\end{itemize}
	
	The true value of $Y$ is given by:
	
	with the variance:
	
	which is independent of the estimator $Y$. Therefore, the difference between $Y$ and $y$ (therefore between estimator and real value) has for variance:
	
	Therefore it is customary to consider that the  "\NewTerm{prediction interval}\index{prediction interval}" (not to be confused with the confidence interval of the estimator) is taken as:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The same kind of development can be done for the slope and the intercept. This is why software like Microsoft Excel, IBM SPSS, Minitab, Statistica, etc. give the value of the Student $T$-distribution as well as the confidence interval for a given threshold level $\alpha$. But for this to be meaningful, you must never forget that all the assumptions of the model must be met.
	\end{tcolorbox}
	This gives us the following type of chart with a software such as Minitab 15:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/computing/regression_prediction_interval.jpg}
		\caption[Print screen of a prediction interval (PI) obtained with Minitab 15]{Print screen of a prediction interval (PI) in green obtained with Minitab 15}
	\end{figure}
	Where we can see the prediction interval (PI) in green and the confidence interval (CI) in red and its is obvious to see by the property of the variance that CI<PI.
	
	These two intervals are often confused. To understand these, it's important to realize that a regression model fits a relationship between the input variables and the mean value of the outcome variable - this is the red line in the below chart. The confidence interval indicates a range of certainty around the mean value of the outcome variable - usually $95\%$ - and is illustrated by the green lines above. The prediction interval indicates a range of certainty around any value of the outcome variable and is illustrated by the purple lines above. Another way of looking at this is to imagine taking a  vertical slice of the below chart. This would translate - under the most common models - to a bell curve for the value of the outcome variable at that specific  value of the input variable. The red point indicates the most likely center of the bell curve, the green lines represents the $95\%$ likelihood range for that center  and the purple lines represents the likelihood for the $95\%$ range (i.e. width) of all these bell curves.
	
	So what must be well understand by the reader so far, is that if we are dealing with univariate linear regression or any other type of regression and want to plot the following data that corresponds to a sample for each $x$-value:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/computing/regression_excel_list_data_sample.jpg}
	\end{figure}
	Therefore using a spreadsheet software to make a plot of that the average of each year as following (most common case see in Fortune 500 companies by top managers and also by auditing companies like Klynveld Peat Marwick Goerdeler/KPMG and Ernst \& Young/EY):
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/regression_excel_sample_plot.jpg}
	\end{figure}
	using a smooth chart is everything but scientific. Indeed:
	\begin{enumerate}
		\item No spreadsheet software is able to summary the data list above automatically in quick an efficient way (with Microsoft Excel even Pivot Charts cannot do smooth scatter charts of such a list of data...).
		
		\item Smooth scatter plot in spreadsheet softwares use splines interpolation and these are not statistical models that can be used for samples! So reading such charts in a spreadsheet software introduce a huge deterministic bias in the mind of the board committee or any customer/supplier.
		
		\item As the averages come from samples there is an implicit confidence interval THAT HAS TO BE SHOWN to the board committee or customer/supplier. Do not show a confidence interval on sample data is a high-school level error that is not acceptable when done by a senior manager or business analyst or even a consultant of a big $5$ auditing companies.
	\end{enumerate}
	So following the detailed steps given in our \texttt{R} companion book we get something truly scientific and robust that is:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/computing/regression_r_sample_plot.jpg}
	\end{figure}
	
	\pagebreak
	\subsubsection{Linear univariate regression forced through the origin (RTO)}
	One very common requested case in laboratories (and generally in other departments) of corporations, is to force the linear regression through the origin. This technique is named "\NewTerm{regression through origin}\index{regression through origin}" and abbreviated RTO.
	
	We will see now (in the univariate case) that the approach is only a simplified variant of the method of the ordinary least squares.
	
	We use as before:
	
	where $n$ is the number of points. But this time, let us write:
	
	then:	
	
	This relation make appears the sum of squared deviations as a function of the parameter $a$. When this function is minimal (extremal), the derivatives with respect to its parameters are cancelled:
	
	After simplification:
	
	Finally:
	
	You can also easily check with any spreadsheet software (Microsoft Excel for example) that the calculations corresponds well.
	
	However the reader must know now that the RTO subject is surprisingly
controversial among statisticians. Indeed,  forcing the regression line through the origin is generally inconsistent with the best fit.

	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Caution! The text below is mainly a reproduction with some adaptations of the following article \cite{eisenhauer2003regression}. Thanks to the donations we were able to buy the rights (225 US\$...) to John Wiley \& Sons, Inc. to reproduce it in this book. But in no manner the reader is authorized to reproduce the same text without the agreement of John Wiley \& Sons, Inc. !!!
	\end{tcolorbox}
	
	The proper method for evaluating RTO has long been disputed. To appreciate the controversy, note the familiar identity:
	
	Squaring both sides and summing across all observation gives:
	
	However, as we have proved earlier above, the cross-product term must be equal to zero in the case of OLS. The remaining terms therefore constitute the usual analysis of variance decomposition:
	
	And we know that the coefficient of determination for OLS is then defined by:
	
	Some authors claim that because this diagnostic measure is based on an identity, it should not depend on the inclusion or exclusion of a constant term in the regression. From that perspective, the previous relation is equally valid for RTO and OLS.
	
	However, when there is no constant in the regression, $\sum(y_k-\hat{y}_k)(\hat{y}_k -\bar{y})$ will generally take a non-zero value, so the variance decomposition:
	
	is not a valid basis for analysis of variance in RTO. And if the RTO model provides a sufficiently poor fit, the data may exhibit more variation around the regression line than around $y$, in which case:
	
	Heedlessly applying:
	
	would then result in an implausibly negative (and thus non-interpretable) coefficient of determination as well as a negative $F$ ratio. Moreover, it is often argued that defining SST as the sum of squared deviations from the mean is inappropriate when the regression line is forced 	through the origin but does not necessarily pass through $(\bar{x},\bar{y})$, when so viewed, the starting relation:
	
	is replaced by identity:
	
	Squaring and summing yields:
	
	but the final (cross-product) term in this equation equals zero under RTO, because:
	
	Thus, the variance OLS decomposition:
	
	is replaced by:
	
	Applying this latter relation one rather than the classical OLS one to RTO, one finds that SSE is unchanged, but $\text{SST}=\sum y_k^2$ and $\text{SSR}=\sum \hat{y}_k^2$. Redefining SST and SSR in this manner results in:
	
	ie, also a strictly non-negative coefficient of determination that equals or exceeds the classical $R^2$ measurement. Of course, these definitions also affect the adjusted $R^2$ and $F$ statistics.
	
	Note that, without a constant, the degrees of freedom for SST, SSR and SSE are $n$, $k$ and $n – k$, respectively, where $n$ is the sample size and $k$ is the number of independent variables. Thus:
	
	regardless of how SST is defined.
	
	The controversy over SST is not merely academic: practitioners (and students) running RTO got various outputs depending on which computer packages they use a few dozens of years ago (since they almost all give the same result based on the adapted definition of the correlation coefficient).
	
	\pagebreak
	\subsubsection{Deming regression (orthogonal regression)}
	As we have already mentioned it, the "\NewTerm{orthogonal linear regression model}" \index{orthogonal linear regression} or "\NewTerm{Deming regression}"\index{Deming regression}  is used as complement to the paired $T$-test to check the stability of the measuring instruments in laboratories. This is a case where the explanatory and dependent variables are tainted with uncertainty (we speak then of "\NewTerm{stochastic regressors}" at the opposite classical case where we have "\NewTerm{nonstochastic regressors}"). In other words, this regression is used when working in a situation where it is desired to compare two linearly increasing or decreasing variables and where each is tainted with measurement errors including error values (of the two variables! ) are independent.
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} Chemists do this analysis sometimes in  a qualitative form using the "Bland-Altman plots".\\
	
	\textbf{R2.} Deming regression is a special case of "Total Least Squares regression" (TLSR) that minimize the total geometric distance between data points and the best-fit line rather than just the vertical distance as in Ordinary Least Squares regression (OLSR).
	\end{tcolorbox}	

	We will see now the derivation of the maximum likelihood estimates related to the Deming regression model. It is based on the book \textit{Models in regression and related topics} (chapter three), from 1969 by Peter Sprent, but with more detailed calculations included (big thanks to Anders Jensen for the \LaTeX{} code of the proof!)
	.
	\paragraph{Deming orthogonal regression log-likelihood derivation}\mbox{}\\\\
	The mathematical model $\eta=\alpha+\beta\xi$ describes a linear relationship between two variables $\xi$ and $\eta$. Observations $x$ and $y$ of two variables are usually described by a regression of $y$ on $x$ where $x$ is assumed to be observed without error (or, equivalently using the conditional distribution of $y$ given $x$). In 	linear regression with observations subject to additive random 	variation on both $x$ and $y$ and observed values for individuals $(x_i,y_i), i=1,\ldots,n$, a model may be written:
	
	where $e_{xi}$ and $e_{y_i}$ denotes the random part of the model. 	This is known as a functional relationship because the $\xi_i$'s are assumed to be fixed parameters, as opposed to a structural relationship where some distribution for the $\xi_i$'s is assumed. In  the following it is assumed that the $e_{xi}$s are  independent and identically distributed with:
	
	and that the $e_{yi}$s are independent and identically distributed with:
	
	for some $\lambda>0$. Furthermore $e_{xi}$ is assumed to be independent of $e_{yi}$.
	
	The aim of this document is to derive the maximum
	likelihood estimates for $\alpha, \beta, \xi_i$ and $\sigma^2$ in the functional model stated above.

	The likelihood function:
	
	denoted $f$ is:
	
	and the log-likelihood, denoted $\mathcal{L}$, is:
	
	It follows that the likelihood function is not bounded from above when $\sigma^2$ goes to $0$, so in the following it is assumed that $\sigma^2>0$.

	Let us solve for $\xi_i$ now! The differentiation of $\mathcal{L}$ with respect to $\xi_i$ gives:
	
	Setting $\frac{\partial \mathcal{L}}{\partial \xi_i}$ equal to zero yields:
	
	So to estimate $\xi_i$, estimates for $\beta$ and $\alpha$ are needed. Therefore focus is turned to the derivation of $\hat{\alpha}$.

	Let us now solve for $\alpha$. The differentiation of $\mathcal{L}$ with respect to $\alpha$ gives:
	
	and putting $\frac{\partial \mathcal{L}}{\partial \alpha}$ equal to zero yields to:
	
	Now one can use:
	
	to dispense with $\xi_i$:
		
	
	Hence the estimate for $\alpha$ becomes:
	

	Solving for $\beta$ by differentiating  $\mathcal{L}$ with respect to $\beta$ gives:
	
	Setting $\frac{\partial \mathcal{L}}{\partial \beta}$ equal to zero yields:
	
	and using again:
	
	we get:
	
	This implies that:
	
	Dividing with $\lambda$ and using the fact that:
	
	it is seen that:
	
	Splitting up the sums even more gives:
	
Finally the terms are sorted and collected according to powers of $\beta$:
	
	Since:
	\begin{itemize}
		\item
		$\displaystyle\sum_{i=1}^n\overline{x}^2
		-\overline{x}\displaystyle\sum_{i=1}^nx_i=0$
		\item
		$\overline{y}\displaystyle\sum_{i=1}^nx_i
		-\displaystyle\sum_{i=1}^nx_iy_i
		-2\displaystyle\sum_{i=1}^n\overline{y}\overline{x}
		+2\displaystyle\sum_{i=1}^ny_i\overline{x}=-\text{SPD}_{xy}$
		\item
		$\displaystyle\sum_{i=1}^ny_i^2
		-\lambda\displaystyle\sum_{i=1}^nx_i^2
		+\displaystyle\sum_{i=1}^n\overline{y}^2
		-2\displaystyle\sum_{i=1}^ny_i\overline{y}
		+\lambda\overline{x}\displaystyle\sum_{i=1}^nx_i
		=\text{SSD}_y-\lambda\text{SSD}_x$
		\item
		$\displaystyle\sum_{i=1}^nx_iy_i
		-\overline{y}\displaystyle\sum_{i=1}^nx_i=\text{SPD}_{xy}$
	\end{itemize}
	it is clear that the derivation of $\beta$ comes down to solve:
	
	For $\text{SPD}_{xy}\neq 0$ this implies that:
	
	Since:
	
	there is always a positive and a negative solution to (\ref{equation}). The desired solution should always have the same sign as $\text{SPD}_{xy}$, hence the solution with the positive numerator is selected. Therefore:
	

	Let us solve for $\xi_i$ - again... With estimates for $\beta$ and $\alpha$ it is now possible to estimate $\xi_i$ using:
	
	we get:
	

	Let us now solve for $\sigma^2$ by differentiating $\mathcal{L}$ with respect to $\sigma^2$ gives:
	
	and setting $\frac{\partial \mathcal{L}}{\partial \sigma^2}$ equal to zero yields:
	
	To get a central estimate of $\sigma^2$ one must divide by $n-2$
instead of $2n$ since there are $n+2$ parameters to be estimated,
namely $\xi_1,\xi_2,\ldots,\xi_n,\alpha$ and $\beta$. Hence the
degrees of freedom are $2n-(n+2)=n-2$. Therefore:
	
	Finally summing up:
	
	These results are implemented in Minitab 16.1.2 and also in the \texttt{Deming()} function in the MethComp package of \texttt{R} as the ready can see it in the corresponding companion books.

	\subsection{Multiple linear regression Gaussian Model}\label{multiple linear regression gaussian model}
	Of course, in some situations, linear regression is too simple or just not suitable. The most typical case that will concern us now in what follows are the situations where we have several explanatory variables (multivariate case!).
	
	The idea of multiple linear regression is relatively simple. We want to determine the dependent variable $y$ from $p-1$ independent variables (i.e. in the absence of "colinearity"!) - and therefore of $p$ parameters to determine - connected by a linear relation of the general form:
	
	In a sample of $n$ individuals we measure $y_i,x_{i,1},\ldots ,x_{i,p-1}$ for $i=1\ldots n$:
	\begin{table}[H]
	\begin{center}
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
				\hline
				\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Observation}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$y_i$}}  & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$x_{i,1}$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$\ldots$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$x_{i,p-1}$}} \\ \hline
				\centering\arraybackslash\ $1$ & \centering\arraybackslash\ $y_1$ & \centering\arraybackslash\ $x_{1,1}$ & \centering\arraybackslash\ $\ldots$ & \centering\arraybackslash\ $x_{1,p-1}$ \\ \hline
				\centering\arraybackslash\ $2$ & \centering\arraybackslash\ $y_2$ & \centering\arraybackslash\ $x_{2,1}$ & \centering\arraybackslash\ $\ldots$ & \centering\arraybackslash\ $x_{2,p-1}$ \\ \hline
				\centering\arraybackslash\ $\vdots$ & \centering\arraybackslash\ $\vdots$ & \centering\arraybackslash\ $\vdots$ & \centering\arraybackslash\ $\ldots$  & \centering\arraybackslash\ $\ldots$\\ \hline
				\centering\arraybackslash\ $n$ & \centering\arraybackslash\ $y_n$ & \centering\arraybackslash\ $x_{n,1}$ & \centering\arraybackslash\ $\ldots$ & \centering\arraybackslash\ $x_{n,p-1}$ \\ \hline
		\end{tabular}
	\end{center}
	\end{table}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
    The terms "multivariate\index{multivariate}" and "multivariable\index{multivariable}" are often used interchangeably in the statistical literature. However, these terms actually represent two very distinct types of analyses! A "multivariable model" can be thought of as a model in which multiple variables are found on the right side of the model equation. "Multivariate", by contrast, refers to the modelling of data that have multiple outpout variables for multiple variables on the right side of the model equation. Unfortunately "multivariable regression" is often mistakenly named "multivariate regression", or vice versa. 
	\end{tcolorbox}	
	In facts, to estimate the parameters $\beta_0,\ldots ,\beta_{p-1}$ (estimated values which we denote by $\hat{\beta}_0,\ldots ,\hat{\beta}_{p-1}$ to respect traditions) the approach is very simple because it is just a generalization of the method of least squares we saw earlier for the simple univariate linear regression.
	
	So in the end we rewrite the relation of Sum of Squared Residuals seen above by modifying a little bit because we now have the multi-linear stuff:
	
	with the estimated theoretical model:
	
	So we have to minimize:
	
	The above parentheses can be rewritten in the form (using what we learned in the sections of Vector Calculus and Linear Algebra):
	
	Whose condensed form is:
	
	where $X$ is named the "\NewTerm{designed matrix}", also known as "\NewTerm{regressor matrix}\index{regressor matrix}" or "\NewTerm{model matrix}\index{model matrix}" or "\NewTerm{data matrix}\index{data matrix}".
	
	We then have the vector of residues that can be written:
	
	As we know, the least squares method is to find the vector $\vec{\hat{\beta}}$ that minimizes:
		
	Therefore explicitly:
	
	Notice that we have:
	
	and:
	
	as each of the elements of the multiplication is a simple vector!
	
	Therefore we have (caution! do not forget that some multiplications in the relation that will follow are dot products!!!) the quadratic multivariate function of coefficients of vectors:
	
	Let us derivate this latter "object function $F$" at the order of vector $\hat{\beta}$ (it is as internal derivative component by component). What we will write:
	
	Now let us rewrite this form a vector notation to a pure matrix notation:
	
	We seek the $\vec{\hat{\beta}}$ that cancel this derivative. Therefore we must solve the following equation:
	
	Therefore:
	
	Remember before we continue that:
	
	Therefore the prior-previous relation can be written:
	
	As Linear Algebra is associative, let us write without the parenthesis:
	
	We can obviously not simplify right and left by $\vec{u}^TX^T$ as it is not a squared matrix, this term is then by obligation not reversible. The only one thing we can do is identify the elements such that:
	
	this implies obviously:
	
	We find then that if the squared matrix $X^TX$ is reversible then:
	
	The matrix $X^TX$ that we will see again in the field of multiple linear regression and in the field of design of experiments (\SeeChapter{see section Industrial Engineering page \pageref{doe}}) is named "\NewTerm{information matrix}\index{information matrix}\label{information matrix}" or "\NewTerm{dispersion matrix}\index{dispersion matrix}" for a reason that will be very obvious later.
	
	The expression $(X^TX)^{-1}X^T$ is named the "\NewTerm{left pseudo-inverse}\index{left pseudo-inverse}", or simply "\NewTerm{pseudoinverse}\index{pseudoinverse}", and even sometimes "\NewTerm{Moore-Penrose pseudoinverse}\index{Moore-Penrose pseudoinverse}" of $X$ and is denoted $X^-$ in some textbooks.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	We say that the regression is "\NewTerm{balanced and orthogonal}\index{balanced and orthogonal}" when the information matrix is diagonal. We say that the regression is "\NewTerm{orthogonal}\index{orthogonal regression}" when the sub-matrix of the information matrix excluding the first row and first columns is orthogonal. We say that the regression is just "\NewTerm{balanced}\index{balanced regression}" when all the values of the first row and of the first column excepted the one at the intersection are equal to zero.
	\end{tcolorbox}	
	To show that this seem correct a priori, let us fall back on the results we get of the simple univariate regression:
	
	Then supposing 2 observations, we have therefore:
	
	Using the relation proved in the section of Linear Algebra (see page \pageref{determinant matrix inverse}) to calculate in generality the inverse of a matrix $A(a_{ij})$ in $A^{-1}(b_{ij})$:
	
	We have in our special case:
		
	Therefore we have:
	
	and since we have a square matrix of dimension $2$ only, the calculation of the four determinants is reduced to selecting the components of $X^TX$ (\SeeChapter{see section Linear Algebra page \pageref{determinant of two by two matrix}}):
	
	Therefore:
	
	So to a change in notations for the indices and experimental measurements, we fall back on the results that we obtained during our study of the simple linear univariate regression that was (for refresh...):
	
	Now, we need a quality indicator regarding our multi-linear regression. Remember that in the context of our study of the univariate linear regression, we proved that the linear correlation coefficient can be written as (see page \pageref{correlation linear regression model}):
	
	and in fact it also applies directly to the multiple linear regression, since it does not presuppose the number of explanatory variables!!
	
	But it is the tradition to rewrite that latter in matrix form. So at it is beautiful, let us do the work.
	
	First we start with:
	
	and finally:
	
	Hence:
	
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The reader interested in the practical application of these results may, just as for simple univariate regression, refer to the server exercises on the companion website - Numerical Methods section - where there are practical examples with Microsoft Excel.
	\end{tcolorbox}	
	Obviously with multiple linear regression, we can now, with a small tip, do linear regression ... of polynomials (we will see later how to apply directly the ordinary least squares method on a polynomial). Indeed, consider a polynomial of the form:
	
	That we can consider, when rewritten, as:
	
	So in spreadsheet softwares like Microsoft Excel, just use the Regression Analysis Tool with the input variable $x$ column, a second column that we will have taken care to create with inside the square of $x$ (i.e. $x^2$) and which will be considered as the explanatory variable $w$ and a third column we have also taken care to create as the cube of $x$ (i.e. $x^3$) and which will be considered as the explanatory variable $z$.
	
	We can also directly obtain the polynomials coefficients with functions already presented earlier above with Microsoft Excel (but you will not have all the relevant results of the Analysis Tool). For example for a polynomial of second degree:
	
	\texttt{a: =INDEX(LINEST(y,x\string^{1,2}),1)\\
	b: =INDEX(LINEST(y,x\string^{1,2}),1,2)\\
	c: =INDEX(LINEST(y,x\string^{1,2}),1,3)
	}
	
	and for a third-degree polynomial:
	
	\texttt{a: =INDEX(LINEST(y,x\string^{1,2,3}),1)\\
	b: =INDEX(LINEST(y,x\string^{1,2,3}),1,2)\\
	c: =INDEX(LINEST(y,x\string^{1,2,3}),1,3)\\
	d: =INDEX(LINEST(y,x\string^{1,2,3}),1,4)}
	
	This is this trick that allows us to understand why and how the linear correlation coefficient also applies to polynomials in most spreadsheets softwares and statistical softwares. However we can have a more direct approach that does not require this transformation but which therefore is a little longer.
	
	Now let us go back to the Gaussian linear model with a concept a little more rigorous and adapted to the multi-linear case and in particular to highlight the distinction between estimators of the slope of the regression and exact values:
	
	But under this notation convention we have:
	
	and written in vector form:
	
	Now, we use the technique of maximum likelihood (\SeeChapter{see section Statistics page \pageref{likelihood estimators}}) and we seek the coefficients that maximizes therefore:
	
	That we can write in matrix form:
	
	Taking as in the section of Statistics the log-likelihood to facilitate future calculations and using the property of the transposed matrices proved in section of Linear Algebra, we get\label{likelihood linear regression}:
	
	Now let us look to the expression of $\vec{\beta}$ that maximizes the log-likelihood. It comes then:
	
	Let us use the property of the transposed matrix proved in the section of Linear Algebra (see page \pageref{transposed matrix}):
	
	Therefore we have:
	
	For reasons which will appear evident a little further below we chose the second equivalence. Therefore, we have (and remembering that seeking for an optimum is equivalent to have partial derivative equal to zero):
	
	Therefore:
	
	That is to say, after rearrangement:
	
	That is to say exactly the same expressions that we got just a little earlier above with the least squares method in the multi-linear case and that was for reminder:
	
	This shows that the statistical of the linear model by the maximum likelihood can fall back into the multi-linear case (and thus also the univariate case) on the results of the least squares method. This has something almost divine ...
	
	Finally, let us indicate that we find here the almost famous "\NewTerm{hat matrix}\index{hat matrix}\label{hat matrix}" or "\NewTerm{influence matrix}\index{influence matrix}" $H$ that connects alone all information between the real and explained theoretical values (and depending only on $X$!) and therefore verbatim the error of the theoretical model:
	
	We have then something interesting to observe:
	
	By definition, the influence of the observation $i$ in the regression and named the  "\NewTerm{leverage}\index{leverage}" (or "\NewTerm{leverage score}\index{leverage score}") is defined by:
	
	$H_i$ is therefore the influence of $y_i$ on its own fitted value! It is a qualitative method (available in many statistical softwares) to judge the influence of points that could be considered as outliers. The idea is to compare the values of leverage between them.
	
	Let us now give the famous explicit expression of the leverage of the simple linear regression.

	We start be remembering that by definition:
	
	and we have proved earlier above that in the simple linear regression that we have:
	
	and also:
	
	Let us put by definition (to make notations more light...):
	
	Hence we may rewrite:
	
	Hence:
	
	Finally:
	
	This relation has a nice interpretation in the SLR model: if the distance from $x_i$ to $\bar{x}$ is large relative to the other $x$'s then $H_{ii}$ will be close to $1$.

	Leverages have nice mathematical properties, for example, they satisfy:
	
	and their sum is:
	
	A rule of thumb is to consider leverage values to be large if they are more than double their average size (which is $2/n$ according to the previous relations). So leverages larger than $4/n$ are suspect. Another rule of thumb is to say that values bigger than $0.5$ indicate high leverage, while values between $0.3$ and $0.5$ indicate moderate leverage.

	\subsubsection{Fisher Matrix}
	We have proved just earlier that:
	
	And we have afterwards also prove that (changing the notation just a bit):
	
	And we also get immediately:
	
	From the both relation above we immediately deduce (note that $\hat{\sigma}^2$ is biased!):
	
	To compute $\mathcal{I}(\vec{\theta})$, notice that we have immediately:
	
	Since:
	
	Then the expected mean of the Hessian is (\SeeChapter{see section Statistics page \pageref{Fisher information matrix}}):
	
	Without forgetting that (\SeeChapter{see section Statistics page \pageref{Fisher information matrix}}):
	
	And finally:
	
	It may seems to the reader in a first view that it is a $2\times 2$ matrix. But this is forget that the term in the upper left corner is itself a $p\times p$ (where $p$ is the number of predictors and we put $+1$ because of the intercept). Hence the matrix dimension is $(p+1)\times (p+1)$.

	\subsubsection{$R^2$ decomposition in multiple regression}
	Let us see now how to decompose $R^2$ into components that capture the percentage of variation explained by predictor in a multiple linear regression!

	Consider $n$ predictors and $K$ observations in the sample:
	
	where all variables have been standardized to have mean zero and variance one. That is, we have centered and rescaled the observations such that for $i = 1, \ldots ,n$:
	\begin{equation}
	\begin{aligned}
	\sum_{k=1}^{K} y_{k} &=0=\sum_{k=1}^{K} x_{i k} \\ 
	\dfrac{1}{K-1} \sum_{k=1}^{K} y_{k}^{2} &=1=\dfrac{1}{K-1} \sum_{k=1}^{K} x_{i k}^{2} 
	\end{aligned}
	\end{equation}
	Standardizing all variables in this manner is without loss of generality since $R^2$ is manifestly invariant to centering and rescaling of variables. Our multiple linear model is then given by:
	
	since standardization ensures that the intercept in the regression above is identically zero.
	
	We estimate the slope coefficient and obtain the fitted values:
	
	where $\hat{\beta}_i$ are the estimated slope coefficients for predictors $i=1,\ldots,n$. Let $\text{cov}(\cdot,\cdot)$ denote the sample covariance operator, defined for centered vectors $x$ and $y$ by:
	
	Then by definition and construction:
	
	That is, we have the decomposition:
	
	We can therefore define the percentage of variation explained by the predictor $i$, denoted $R_i^2$, by:
	

	\subsubsection{Influential points}\label{influential points}
	An "outlier\index{outlier}" as we already know is a data point which is very far, somehow, from the rest of a group of the data. They are often worrisome, but not always a problem. When we are doing regression modelling, in fact, we don't really care about whether some data point is far from a group of data, but whether it breaks a pattern the rest of
the data seems to follow and this is what we name an "\NewTerm{influential point}\index{influential point}". We will look here some empirical ways of quantifying how much influence particular data points have on the model. 

	The points marked in red and blue in the figure below are clearly not like the main cloud of the data points, even though their $x$ and $y$ coordinates are quite typical of the data as a whole: the $x$ coordinates of those points aren't related to the $y$ coordinates in the right way, they break a pattern. On the other hand, the point marked in green, while its coordinates are very weird on both axes, does not break that pattern - it was positioned to fall right on the regression line.
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/computing/outliers_vs_influentials.jpg}
		\caption[Outliers vs Influential points]{Outliers (red) vs Influential (blue) points}
	\end{figure}
	
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Be very very very careful with influential points!!! Even if they follow the global pattern, they may quite significantly (and surprisingly) modify the regression equation and the importance of the different coefficients of that same equation (see further below for an example)!
	\end{tcolorbox}
	
	We have seen just earlier the "leverage". But let us introduce other influential indicators that are very common in the 120th century (holocene calendar) and beginning of 121st century in many statistical softwares and textbooks!
	
	The "\NewTerm{semistudentized residuals}\index{semistudentized residuals}" are defined as:
	
	But... Let us recall that we have proved just earlier that:
	
	Hence:
	
	Let us rewrite this in term of noise:
	
	Since $\text{E}(\vec{\varepsilon})=\vec{0}$ and $\text{V}(\vec{\varepsilon})=\sigma^2_\varepsilon\vec{\mathds{1}}$, we have:
	
	and:
	
	What does this imply for the residual at the $i$th data point? First, and obviously, it has expectation of $0$:
	
	and it has variance which depends on $i$ through the hat matrix:
	
	In other words, the bigger the leverage of $i$, the smaller the variance of the residual is. This is yet another sense in which points with high leverage are points which the model tries very hard to fit.
	
	Previously, when we looked at the residuals, we expected them to all be of roughly the same magnitude. This rests on the leverages $H_{ii}$ being all about the same! If there are substantial variations in leverage across the data points, it's better to scale the residuals by their expected size.

	The usual way to do this is through the "\NewTerm{standardized residuals}" or "\NewTerm{internally studentized residuals}\index{internally studentized residuals}" or just simply "\NewTerm{studentized residuals}\index{studentized residuals}":
	
	Why "studentized"? Because we're dividing by an estimate of the standard error, just like in Student's $T$-test for differences in means.
	
	However the single $e_i$ and $\hat{\sigma}_\varepsilon$ are non independent, so $r_i$ can't have a $T$ distribution.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Indeed, let us recall that in the section Statistics we have seen that given $Z$ a random variable of Normal law centered and reduced $\mathcal{N}(0,1)$ and $U$ a variable independent of $Z$ and distributed according to the law of $\chi^2$ at $k$ degrees of freedom. By definition, the variable:
	
	follows a Student law of $k$ degrees of freedom.
	\end{tcolorbox}
	 The procedure is then to delete the $i$th observation, fit the regression function to the remaining $n-1$ observations, and get new $\hat{y}$'s which can be denoted by $\hat{y}_{i(i)}$. The difference:
	
	is named "\NewTerm{deleted residual}\index{deleted residual}". 
	
	Then we define the "\NewTerm{studentized deleted residuals}\index{studentized deleted residuals}" or "\NewTerm{externally studentized residuals}\index{externally studentized residuals}":
	
	Then if the errors are independent and normally distributed with expected value $0$ and variance $\sigma^2_\varepsilon$, then the probability distribution of the $i$ externally studentized residual is a Student's $T$-distribution with $N-k-1$ degrees of freedom:
	
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In some statistical softwares we can find a influential indicator named DFITS (or also sometimes DFFITS) that is the acronym of "\NewTerm{DiFference In fiTS}\index{difference in fits (DFIT)}" and defined by:
	
	DFITS is very similar to the externally studentized residual, and is in obviously equal to the latter times $\sqrt{H_{ii}/(1-H_{ii})}$.
	\end{tcolorbox}
	Probably one of the most famous influential indicator is the "\NewTerm{Cook's distance}\index{Cook's distance}". Omitting point $i$ will generally change all of the fitted values, not just the fitted value at that point. Hence the idea to measure the total change and to define this distance as the sum of all the changes in the regression model when observation $i$ is removed from it:
	
	To make that relation more comparable across data sets, it's conventional to divide it by $p$, since there are really only $p$ independent coordinates here, each of which might contribute something on the order of $\hat{\sigma}_{\varepsilon}$:
	
	There are different opinions regarding what cutoff values to use for spotting highly influential points. A simple operational guideline of $D_{i}>1$ has been suggested. Others have indicated that $D_i>4/n$ where $N$ is the number of observations, might be used.
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/computing/influencial_point.jpg}
		\caption{Importance of an influential point on regression equation}
	\end{figure}
	
	\subsubsection{Significance of regression coefficients (variable importance)}\label{variable importance GML}\index{statistical tests!$T$-test for regression variable importance}
	When you study variable importance, you should always first remember to standardize all your predictors (thus, centered and scaled by the sample standard deviations).
	
	Now let us deal first with the Gaussian multivariate regression case! Let us recall that (see page \pageref{law of GLM coefficients}):
	
	Thus:
	
	under the null $H_0:\beta_i=0$ we would actually have a simple $Z$ test:
	
	This relation is again a kind of "\NewTerm{Wald statistics}\index{Wald statistics}" and the related hypothesis also belong to the family of a Wald tests\index{Wald's test} (not the first we meet in this book). Also often denoted:
	
	or:
	
	Now let us introduce a necessary intermediate result. Remember that for the bivariate regression, we have proved that:
	
	Let us divide by the supposed true value of $\sigma_\varepsilon$:
	
	We recognize here a sum of square of Normal distribution that led us to write:
	
	That we will guess can be generalized to the multivariate case by:
	
	Let us rearrange and take the square root:
	
	Let us divide:
	
	by this previous result. Then:
	
	We recognize here well the term on the right. It's the definition itself the Student distribution (\SeeChapter{see Statistics section page \pageref{student distribution}}). Then we can write:
	
	That latter relation is also sometimes denoted:
	
	It explains why, in regression analysis and in variable importance, greater the Student $T(n-k)$ value is (in absolute value) the more the coefficient is significant ($H_1:\;\beta_i\neq 0$) and the more the corresponding predictor is important (hence the expression "\NewTerm{variable importance}\index{variable importance}")!
	
	In logistic (and Poisson) regression that we will see later, the variance of the residuals is related to the mean! Indeed, if $Y \sim \mathcal{B}(n, p)$, the mean is $\text{E}(Y)=n p$ and the variance is $\text{V}(Y)=n p(1-p)=\text{E}(Y)(1-p)$ so the variance and the mean are related.
	
	In statistics, the "dispersion parameter" $\phi$ indicates if we have more or less than the expected variance. If $\phi=1$ this means we observe the expected amount of variance, whereas $\phi<1$ means that we have less than the expected variance (named "underdispersion") and $\phi>1$ means that we have extra variance beyond the expected variance (named "overdispersion"). The dispersion parameter in logistic and Poisson regression is fixed at $1$ which means that we can use the $Z$-score (that's why statistical softwares give the $Z$-score for the coefficients of such regression models!). In other regression types such as Normal linear regression, we have to estimate the residual variance and thus this is why, a $T$-value is used for calculating the $p$-values (that's why statistical softwares give the $T$-score for the coefficients of such regression models!). 

	\begin{itemize}
		\item Linear Regression:
		
		The use of $T$-tests in linear regression comes from the distribution of Normally distributed error terms: $y_{i}=X_{i}^T \beta+\varepsilon_{i}$ where $\varepsilon_{i} \sim \mathcal{N}(0,1)$ are independent and identically distributed. It follows then as seen above that $\frac{\hat{\beta}_{j}-\beta_{j 0}}{\widehat{\text{se}}\left(\hat{\beta}_{j}\right)} \sim T(n-k)$. 
		
		Note that the default in most regression software packages test the hypothesis that $\hat{\beta}_{j}=0,$ i.e. setting $\beta_{j 0}$ equal to zero.

		\item Logistic Regression:
		
		Logistic regression assumes errors follow the logistic distribution. Consequently, the term $\frac{\hat{\beta}_{j}-\beta_{j 0}}{\widehat{\text{se}}\left(\hat{\beta}_{j}\right)}$ does not follow a $T$-distribution. Instead, we can use the Wald test, which relies on asymptotic normality as is implied by the Central limit Theorem.
	\end{itemize}
	
	Note, though, that even if the $T$-test and Wald test are asymptotically equivalent (i.e. as the sample size, $n \rightarrow +\infty,$ they will reject the same cases); certainly some people - if a bit loosely - name a test based on a $T$-statistic a "Wald test", whether the statistic is compared with the asymptotic Normal distribution or the small sample result ($T$-distribution)...
	
	
	\subsubsection{Variance Inflation Factor (multicollinearity detection)}
	In statistics, the "\NewTerm{variance inflation factor}\index{variance inflation factor}\label{variance inflation factor}" (VIF) quantifies the severity of multicollinearity in an ordinary least squares regression analysis. It provides an index that measures how much the variance (the square of the estimate's standard deviation) of an estimated regression coefficient is increased because of collinearity. 

	To start, the reader must know that during our study of Principal Component Analyses (\SeeChapter{see section Statistics page \pageref{principal component analysis}}) we have proved in a trivial way that the variance-covariance matrix is given by:
	
	with for recall (for a regression model with $p$ terms, and hence $p-1$ variables):
	
	Therefore for example (and for recall again) for the special case of a $2\times 2$ design matrix:
	
	Hence:
	
	And for a $3\times 3$ matrix:
	
	And as the matrix of correlations is given by (\SeeChapter{see section Statistics page \pageref{correlation matrix}}):
	
	Then we get:
	
	The ideal would also be to be able to put the inverse of the standard deviations in a matrix form in order to have a totally matrix expression. After some trial and error we quickly find that we can write the correlation matrix in the form:
	
	where $S^{-1}$ is the usual notation to say that it is the diagonal matrix containing the inverse of the $\sigma_j$, or explicitly (so that it is clear to the reader!):
	
	So for now, we have two relations (out of three ... waiting for the proof of the third one) that will be very useful to us (attention the components of the vectors of the explanatory variables are always centered henceforth what implies a forced linear regression by the origin!):
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Because $X_c^TX_c$ has the first row and column only with zeros (due to recall by the constant of the model) that generates issues when inverting the matrix (i.e. $(X_c^TX_c)^{-1}$), we then eliminate this column and row. To highlight this choice, a few textbooks write instead:
	
	where the $(-1)$ is here to indicate that the first row and first column were removed. For what will follow we did not make this choice of notation, so keep in mind now that when you see $X_c$, it is the "design matrix" but without the first column, or said differently: $X_c^TX_c$ without the first column \underline{and} without the first row.
	\end{tcolorbox}
	Now let's start from (don't forget that starting from now $\vec{\beta}$ does not contain $\beta_0$ anymore!!!):
	
 	and let us rewrite this in a somewhat more explicit form:
	
	Therefore:
	
	We understand now better why $(X_c^TX_c)^{-1}$ is often named "\NewTerm{dispersion matrix}\index{dispersion matrix}" as we have already mentioned earlier above.

	Let us now take the matrix of the covariance-variances matrix of the coefficients (measurement of multicollinearity):
	
 	Using the following property of the transposed matrices proved in the section Linear Algebra (see page \pageref{transposed matrix}):
	
	we then have using this property a first time:
	
	and using it a second time:
	
	Using now the following property proven in the section Linear Algebra (see page \pageref{transposed matrix}):
	
	We then have:
	
	and reusing:
	
	it comes:
	
	Using the associativity property of the matrix product, we will write:
	
	\label{variance coefficient ols}And by taking out the elements which are not random variables of the expected mean, it comes:
	
	It is sadly often traditional (in spite of the confusion) to write this last result in the following form which we will see (and use) in the section of Industrial Engineering during our study of the Box-Behnken experimental designs:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	At this point, notice something important! As by assumption for the Gaussian multivariate linear regression we have $Y=\mathcal{N}(X\vec{\beta},\sigma_\varepsilon^2\mathds{1}_k)$ as a consequence, we then have using the result above\label{law of GLM coefficients}:
	
	This relation will be important for us later in our study of variable importance in the case of regression models and statistical significant test for the coefficient of the regression!
	\end{tcolorbox}
	This last matrix $\text{V}\left(\vec{\hat{\beta}}\right)$ contains in the diagonal the general expression in the general case of multi-linear regression the "\NewTerm{standard error of the regression coefficients}\index{standard error of the regression coefficients}\label{standard error of the regression coefficients}" (relation that uses among other the spreadsheet software Microsoft Excel in the case of the multi-linear regression for the column "Standard error" of the coefficients ... except that it takes the square root to have the standard deviation and not the variance!). If the linear regression is not forced at the origin, it will be necessary to use obviously:
	
	with for recall:
	
	where $k$ is for recall the number of coefficients of the multiple linear model.

	We have now three important relations\footnote{There is a fourth one in reality that is also important...! The bias-variance tradeoff of the ordinary least squares regression as derived in details at page \pageref{bias-variance tradeoff ols}.}:
	
	Let us take up the first relation and manipulate it a little bit:
	
	It comes then:
	
	Then we have:
	
	By substituting the right-hand term for equality in the third of the equalities, we then have:
	
	If we denote by $\text{V}\left(\vec{\hat{\beta}}\right)_j$ the $j$-th diagonal element of the covariance-variance matrix of the $\vec{\hat{\beta}}$ and $\text{VIF}_j$, the "\NewTerm{Variance Inflation Factor (VIF)}\index{variance inflation factor}", the $j$-th diagonal element of the matrix $R^{-1}$ then it trivially follows that:
	
	The "\NewTerm{scaled Variance Inflation Factor}" is sometimes used instead and defined by:
	 
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Remember that if the variances are all equal to the unit ($\text{V}_i=1$) then the covariance matrix is equal to the correlation matrix, ie $R=\Sigma$. Hence if we work with the $\text{VIF}_{js}$, all the information is then also contained in the inverse of the covariance matrix, ie $\Sigma^{-1}=\Theta$ named the "\NewTerm{precision matrix}\index{precision matrix}".
	\end{tcolorbox}
	However, we must find the explicit formulation of the $\text{VIF}_j$. For this, let us recall that $R$ is a positive definite symmetric matrix (\SeeChapter{see section Linear Algebra page \pageref{positive definite matrix}}). Let us, for example, calculate the inverse of such a matrix of dimension $2\times 2$ explicitly by using the relation proved in the section Linear Algebra (see page \pageref{determinant matrix inverse}) that is for recall:
	
	In the special case that interests us, we have then:
	
	and since the correlation matrix is symmetric and unit diagonal, we have:
	
	It comes then that if we focus on the elements of the diagonal:
	
	Repeating the job with a $3\times 3$ matrix, we have:
	
	Let us focus on the first diagonal component and rearrange it a bit to get something similar to the $2\times 2$ case:
	
	Now we put:
	
	So that:
	
	Where we define:
	
	as the "\NewTerm{multiple correlation coefficient}\index{multiple correlation coefficient}" (for three variables...). This is by definition the way we calculate the correlation between $x_1$ and the other explanatory variables:
	
	
	And the reader can check that regardless of the size of the matrix, the $\text{VIF}_j$ can always be put in the form:
	
	In the case of $2\times 2$ correlation matrix there is therefore only one coefficient $\text{VIF}_j$ for the simple and good reason that the associated linear regression has only two coefficients ... 
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Some textbooks give the following relation for the general case:
	
	But this is false as far as I know!
	\end{tcolorbox}
	
	Most statistical softwares simply use the following relation:
	
	So we understand (how the name suggests...) why the variance inflation factor (VIF) quantifies how much the variance of the coefficients is inflated when multicollinearity exists.
	
	Therefore:
	
	It is important here to notice that the VIF (multicolinearity) is proportional to the variance of the beta coefficients estimation. Hence as we have proved earlier (see page \pageref{variable importance GML}) that the significance of regression coefficients is inversely proportional to the square of the variance of the beta coefficients estimation, multicollinearity implies therefore small $p$-values and thus leading to some variables becoming significant even though they are actually not significant.
	
	Either in the case where the data are such that the regression is not forced at the origin:
	
	Therefore the version I prefer to use and to teach (especially for customers and students having only a spreadsheet software at their disposition is):
	
 	and as all terms are known to us in practice, the calculation of the $\text{VIF}_j$ doesn't make us problems anymore!
 	
	A rule of thumb is that if $\text{VIF}_j>10$ then multicollinearity is high (solving backwards this translates into an $R_j^2$ value of $0.90$). Obviously a VIF of $1$ means that there is no correlation among the $k$th predictor and the remaining predictor variables, and hence the variance of $\beta_k$ is not inflated at all. VIFs exceeding $4$ should warrant further investigation.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Some softwares instead calculates the tolerance which is just the reciprocal of the VIF. The choice of which to use is a matter of personal preference.
	\end{tcolorbox}
	The square root of the variance inflation factor finally indicates how much larger the standard error is, compared with what it would be if that variable were uncorrelated with the other predictor variables in the model.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	If the variance inflation factor of a predictor variable is equal to $5.27$ ($\sqrt{5.27} = 2.3$), this means that the standard error for the coefficient of that predictor variable is $2.3$ times as large as it would be if that predictor variable was uncorrelated with the other predictor variables.
	\end{tcolorbox}
	Regardless of your criterion for what constitutes a high VIF, there are situations in which a high VIF is not a problem and can be safely ignored. For example, if you specify a regression model with both $x$ and $x^2$, there is a good chance that those two variables will be highly correlated. 
	
	\paragraph{Gauss-Markov theorem}\label{Gauss-Markov theorem}\mbox{}\\\\
	After all this we finally have the tools and results necessary to prove the Gauss-Markov theorem in an easy way!
	
	In statistics, the "\NewTerm{Gauss–Markov theorem}\index{Gauss–Markov theorem}" (or simply Gauss theorem for some authors) states that the ordinary least squares (OLS) estimator has the lowest sampling variance within the class of linear unbiased estimators, if the errors in the linear regression model are uncorrelated, have equal variances and expectation value of zero. The errors do not need to be Normally distributed, nor do they need to be independent and identically distributed (only uncorrelated with mean zero and homoscedastic with finite variance). 
	
	It's not a very important proof for the engineer but statisticians like a lot such properties! 
	
	First we need a definition!
	
	\textbf{Definition (\#\mydef):} Given a parameter vector $\vec\omega$ in $q$ dimensions, and a vector $\vec{y}$ from which to estimate it, we say that the estimator $\vec\omega^{*}$ is the best unbiased estimator of $\vec\omega$, or "\NewTerm{Best Linear Unbiased Estimator}\index{best linear unbiased estimator}\label{best linear unbiased estimator}" (BLUE) of $\vec\omega$, if:
	\begin{enumerate}
		\item $\vec\omega^{*}$ is a linear estimator of $\vec\omega$, in other words there is a matrix $A$ such that $\vec\omega^{*}=A \vec{y}$;
	
		\item $\vec\omega^{*}$ is unbiased, in other words, $\text{E}\left(\vec\omega^{*}\right)=\vec\omega$
		
		\item Whatever the unbiased linear estimator $\tilde{\vec\omega}$ of $\vec\omega$, the variance of this estimator is greater than or equal to that of $\vec\omega^{*}$.
	\end{enumerate}
	\begin{theorem}
	Given $n$ observations $\vec{x}^{1}, \vec{x}^{2}, \ldots, \vec{x}^{n} \in \mathbb{R}^{p}$ and their labels $y^{1}, y^{2}, \ldots, y^{n} \in \mathbb{R} .$ Under the assumption that, for all $i, y^{i}= \beta_{0}+\sum_{j=1}^{p} \beta_{j} x_{j}^{i}+\varepsilon^{i}$ and that the $\varepsilon^{i}$ are normally distributed and centered at $0$, then the estimator of $\vec{\beta}$ by the least squares method is its unique BLUE!
	\end{theorem}
	
	\begin{dem}
	First as we know:
	
	and $\vec{\beta}^{*}$ is therefore linear. It's expected mean is:
	
	As $X, \vec{y}$ and $\vec{\beta}$ are not random, and that $\varepsilon$ has a $0$ expected mean, we get:
	
	 Therefore, $\vec{\beta}^{*}$ is unbiased.
	
	Its variance is equal to (see page \pageref{variance coefficient ols}):
	
	Finally, suppose now that $\tilde{\beta}=A \vec{y}$ is another linear unbiased estimator of $\vec{\beta}$. By definition, therefore $\text{E}(\tilde{\beta})=\vec{\beta}$. By replacing $\vec{y}$ by it value, we get $\text{E}\left(A(X \vec{\beta}+\varepsilon)\right)=\vec{\beta}$ and therefore $A X \vec{\beta}=\vec{\beta}$. As this is true for every $\vec{\beta}$, we conclude that $A X=I$ (this result will be useful for later below).
	
	Let us put $D=A-\left(X^T X\right)^{-1} X^T$, such that $\tilde{\vec\beta}-\vec{\beta}^{*}=D \vec{y}$. The variance of $\tilde{\vec\beta}$ is equal to:
	
	We can then express $A A^T$ as follows:
	
	As $A X=I$, then:
	
	 and it is immediate that we have similarly $(DX)^T=X^T D^T=0$ .
	
	We therefore get:
	
	Using this trick we can rewrite the variance as:
	
	
	As $\sigma_\varepsilon^{2}>0$ and $D D^T$ has by construction every component that is $>0$, we have:
	
	excepted if $D=0$, in which case $\tilde{\vec\beta}=\vec{\beta}^{*}$.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	As already mentioned at the beginning, we see then that the Normality assumption on $\varepsilon$ is not necessary! It suffices that the errors $\left\{\varepsilon_{i}\right\}_{i=1, \ldots, n}$ have zero expectation, have the same finite variance $\sigma^{2}$ (homoscedasticity), and are not correlated with each other ($\text{cov}\left(\varepsilon_{i}, \varepsilon_{l}\right)=0$ for $i \neq l$).
	\end{tcolorbox}
	
	\subsubsection{Weighted Least Squares (WLS)}
	The method of ordinary least squares\footnote{A rule of thumb for OLS regression is that it isn't too impacted by heteroscedasticity as long as the maximum variance is not greater than four times the minimum variance.} assumes that there is constant variance in the errors (which is named "homoscedasticity" for recall). The method of "\NewTerm{weighted least squares}\index{weighted least squares}" (WLS) can be used when the ordinary least squares assumption of constant variance in the errors is violated (which is named "heteroscedasticity" for recall). The model under consideration is:
	
	where we consider now that the $\varepsilon_i =\mathcal{N}(0,\sigma_i)$. Hence the errors may not be homoscedastic!
	
	Let us recall the likelihood for the multivariate linear gaussian regression:
	
	In the heteroscedatic case that interest us here the likelihood can only be written as:
	
	named the "\NewTerm{generalized least squares regression likelihood}\index{generalized least squares regression likelihood}\label{generalized least squares regression likelihood}".
	
	So in the case of standard linear regression we have $\sigma_i=\sigma\; \forall i$ and all of diagonal terms that are zero, therefore:
	
	If we take the logarithm of the likelihood and ignoring the terms that doesn't content the $\beta_i$ we then get:
	
	The idea is to rewrite the previous relation as:
	
	with:
	
	Or in matrix form:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The reader must understand that the reason some statistical softwares require the matrix $W$ is just because they will replace the calculated $\hat{\sigma}_i$ by the corresponding $w_i$ and not multiply them together! And by the way... as all off-diagonal terms of $W$ are zero, most statistical softwares require only a vector corresponding to the values of the diagonal of $W$.
	\end{tcolorbox}
	Or in matrix notation this will be denoted:
	
	
	So for weighted regression, it is the weighted residuals that we try to make homoscedastic! Such that:
	
	
	Let us consider the univariate case:
	
	To determine the intercept we then calculate:
	
	Hence:
	
	And similarly:
	
	Hence:
	
	And similarly:
	
	Notice that for the two relations:
	
	If $w_i=1$ for all $i$ then we fall back on OLS.
	
	Now let us generalize that result. First notice that:
	
	will be rewritten in matrix form (we get rid of the factor $-1/2$ that doesn't bring anything):
	
	Let us expand the last equality (using the property of transposed matrices as seen on page \pageref{transposed matrix} in the section of Linear Algebra):
	
	Now we seek the minimum of the latter relation (Weighted Sum of Squares Errors) with respect to the $\beta_i$:
	
	For this we differentiate with respect to $\vec{\beta}$. Obviously the first term will vanish so it remains only:
	
	Notice now that we have also using the properties of transposed matrices:
	
	without forgetting that as $W$ is diagonal we have $W=W^T$. Therefore:
	
	Rearranging we get:
	
	Again we notice that if $W$ is a diagonal unitary matrix, we fall back on the ordinary multivariate regression coefficients. 
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Notice that exactly the same developments can lead us to the coefficient of the general ordinary least squares then given by:
	
	Since we can write $\Sigma=SS^T$, where $S$ is a triangular matrix using the Cholesky decomposition (\SeeChapter{see section Linear Algebra page \pageref{Cholesky decomposition}}), we have for the general sum of squares errors:
	
	So GLS is like regressing $S^{-1}X$ on $S^{-1}\vec{y}$. Furthermore:
	
	So we have a new regression equation $y'$ where as we can see from the GSSE can be assimilated a simple regression equation which has uncorrelated errors with equal variance.
	\end{tcolorbox}
	
	In practice, for other types of dataset, the structure of $W$ is usually unknown, so we have to perform an ordinary least squares (OLS) regression first. Provided the regression function is appropriate, the $i$-th squared residual from the OLS fit is an estimate of $\sigma^2_i$ and the $i$-th absolute residual is an estimate of $\sigma_i$ (which tends to be a more useful estimator in the presence of outliers). The residuals are much too variable to be used directly in estimating the weights, $w_i$, so instead we use either the squared residuals to estimate a variance function or the absolute residuals to estimate a standard deviation function. We then use this variance or standard deviation function to estimate the weights.
	
	Some key points regarding weighted least squares are:
	\begin{itemize}
		\item The difficulty, in practice to determine the right weights (in some cases, the values of the weights may be based on theory or prior research).
		
		\item Weighted least squares estimates of the coefficients will usually be nearly the same as the "ordinary" unweighted estimates\footnote{Same applies for the ANOVA of the regression obviously.}. In cases where they differ substantially, the procedure can be iterated until estimated coefficients stabilize (often in no more than one or two iterations); this is named "\NewTerm{iteratively reweighted least squares}\index{iteratively reweighted least squares}".
		
		\item In designed experiments with large numbers of replicates, weights can be estimated directly from sample variances of the response variable at each combination of predictor variables.
		
		\item Use of weights will (legitimately) impact the widths of statistical intervals.
		
		\item In the transformed model, the interpretation of the coefficient estimates can be difficult. In weighted least squares the interpretation remains the same as before.

		\item Weighted least squares gives us an easy way to remove one observation from a model by setting its weight equal to 0.

		\item We can also downweight outlier or influential points to reduce their impact on the overall model.
	\end{itemize}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The reader must keep in mind that this technique of weights can be applied to numerous techniques of regression. Not only linear, but also polynomial, logistic and so on!
	\end{tcolorbox}
	
	\subsubsection{Model Selection Criterias}
	Much of modern scientific enterprise is concerned with the question of model choice. An experimenter or researcher collects data, often in the form of measurements on many different aspects of the observed units, and wants to study how these variables affect some outcome of interest. Which measures are important to the outcome? Which are not? Are there interactions between the variables that need to be taken into account? Statisticians are also naturally involved in the question of model selection, and so it should come as no surprise that many approaches have been proposed over the years for dealing with this key issue. Both frequentist and Bayesian schools have weighed in on the matter, spawning methods such as $F$ tests, Akaike information criterion (AIC), Mallows's $C_p$, exhaustive search, stepwise, backward, and forward selection procedures, cross-validation, Bayes factors of various flavours (partial, intrinsic, pseudo, fractional, posterior), Bayesian information criterion (BIC), and Bayesian model averaging, to name some of the more popular and well-known methods. Some of these, such as stepwise selection, are algorithms for picking a 'good' (or maybe useful) model; others, for example, AIC, are criteria for judging the quality of a model. Given this wealth of choices, how is a statistician able to decide what to do? An approach that cannot be implemented or understood by the scientific community will not gain acceptance. This implies that, at the very least, we need a method that can be carried out easily and yields results that can be interpreted by scientifically and numerically literate end users. From a statistical point of view, we want a method that is coherent and general enough to handle a wide variety of problems. Among the demands we could make on our method would be that it obeys the likelihood principle, that it has some frequentist (asymptotic) justification, and that it corresponds to a Bayesian decision problem. So let's begin from easier do derive to the most complicated one keeping in mind that there is actually in this beginning of the 121st century (holocene calendar) no magic procedures to get you the "best model" (this still remains an unsolved problem)!
	
	\paragraph{Mallows's $C_p$ Linear Regression Prediction}\mbox{}\\\\
	We estimated our model by minimizing the mean squared error on our data\footnote{The whole text and developments below are mostly a copy paste of the PDF of the professor Cosma Shalizi}:
		
	Different linear models will amount to different choices of the design matrix $X$ --- we add or drop variables, we add or drop interactions or polynomial terms, etc., and this adds or removes columns from the design matrix (this is typical of how works the "subset method" in many statistical softwares).  We might consider doing a selection among models themselves by minimizing the MSE.  This may sometimes (but not always!) be a very bad idea, for a fundamental reason: Every model can be too optimistic about how well it will actually predict (sometimes the ultimate validation of a model is to test its predictive capacity).
	
	Let's be very clear about what it would mean to predict well.  The most challenging case would be that we see a new random point, with predictor values $X_1, \ldots X_p$ and response $Y$, and our old $\widehat{\vec{\beta}}$ has a small expected squared error:
	
	Here both $Y$ and the $X$'s are random (hence the capital letters), so we might be asking the model for a prediction at a point it never saw before (of course if we have multiple identically distributed $(X,Y)$ pairs, say $q$ of them, the expected MSE over those $q$ points is just the same as the expected squared error at one point.)
	
	An easier task would be to ask the model for predictions at the same values of the predictor variables as before, but with different random noises. That is, we fit the model to:
	
	The test data consist of new outcome data drawn from the same true model and at the same $x$-locations as the training data (notice that this is a practical construction to derive the criterion, but not a necessity to use this for model selection. However, you should not use model selection criteria for a specific range of $x$ and assume you can predict well on a different range of $x$!).
	
	Let us note for the training data the model as following:
	
	with $\varepsilon = \mathcal{N}(0,\sigma_\varepsilon^2)$ and for the test data:
	
	where $\varepsilon$ and $\varepsilon'$ are independent but identically distributed!  The design matrix is the same, the true parameters $\vec{\beta}$ are the same, but the noise is different\footnote{If we really  are in an experimental setting, we really could get a realization of $\vec{Y}^{\prime}$ just by running the experiment a second time.  With  surveys or with observational data, it would be harder to actually realize $\vec{Y}^\prime$, but mathematically at least it's unproblematic.}.  We now want to see if the coefficients we estimated from $(X, \vec{Y})$ can predict $(X, \vec{Y}\,')$.  Since the only thing that's changed is the noise, if the coefficients can't predict well any more, that means that they were really just memorizing the noise, and not actually doing anything useful.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	If we have $p-1$ variables, there are $2^{p-1}$ possible subset models!
	\end{tcolorbox}
	Our "\NewTerm{out-of-sample expected MSE}", then, is:
	
	It will be convenient to break this down into an average over data points, and to abbreviate $X\hat{\vec{\beta}} = \hat{\vec{m}}$, the vector of fitted values.  Notice that since the predictor variables and the coefficients aren't changing, our predictions are the same both in and out of sample. At point $i$, we will predict $\vec{m}_i$.
	
	In this notation, then, the expected out-of-sample MSE is:
	
	We will compare this to the "\NewTerm{expected in-sample MSE}":
	
	Notice that $\hat{m}_i$ is a function of the $y_i$ (among other things), so those are dependent random variables, while $\hat{m}_i$ and $y_i^\prime$ are completely statistically independent\footnote{That might sound weird, but remember we're holding $X$ fixed in this exercise, so what we mean is that knowing $\hat{m}_i$ doesn't give us an extra information about $y_i^\prime$ beyond what we'd get from knowing the values of the $X$ variables.}.
	
	Break this down term by term.  What's the expected value of the
	$i^{\mathrm{th}}$ in-sample squared error? Using Huygens relation given for recall by $\text{V}(X)=\text{E}(X^2)-\text{E}(X)^2$ we then have:
	
	The covariance term is not (usually) zero, because, as already mentioned, $\hat{m}_i$ is a function of, in part, $y_i$.
	
	On the other hand, what's the expected value of the $i$-th squared error on new data? Using again Huygens relation we get:
	
	The $y_i'$ is independent of $y_i$, but has the same distribution.  This tells us that:
		
	So:
	
	Averaging over data points:
	
	
	Clearly, we need to get a handle on that sum of covariances.
	
	For a linear model, though (using the results proved also during our study of influential points on page \pageref{hat matrix}):
	
	as for recall:
	
	So, for linear models:
	
	and we know that with $p$ predictors and one intercept\footnote{Be careful, on some Internet forum, people use $X_c$ instead of $X$ then the result lead to $p$ and not $p+1$ as the first component of the diagonal for $X_c$ is equal to $0$.}:
		
	Thus, for linear models:
	
	Of course, we don't actually know the expectation on the right-hand side, but we do have a sample estimate of it, which is the in-sample MSE.  If the law of large numbers is still our friend:
	
	
	The second term on the right, $(2/n)\sigma_\varepsilon^2 (p+1)$, is the "\NewTerm{optimism}" of the model, ie the amount by which its in-sample MSE systematically under-estimates its true expected squared error.  Notice that this:
	\begin{itemize}
		\item Grows with $\sigma_\varepsilon^2$: more noise gives the model more opportunities to seem to fit well by capitalizing on chance.
	
		\item Shrinks with $n$: at any fixed level of noise, more data makes it harder to pretend the fit is better than it really is.
	
		\item Grows with $p$: every extra parameter is another control which can be adjusted to fit to the noise.
	\end{itemize}
	Minimizing the in-sample MSE completely ignores the bias from optimism, so it is guaranteed to pick models which are too large and predict poorly out of sample. If we could calculate the optimism term, we could at least use an unbiased estimate of the true MSE on new data. From one point of view, the optimism is just an estimate of the bias. 	From another point of view, it's a cost we're imposing on models for having extra parameters. 
	
	\textbf{Definition (\#\thesection.\mydef):} If $p$ regressors are selected from a set of $k>p$, the "\NewTerm{Mallows's $C_p$}" statistic for that particular set of regressors is defined as (it's the previous relation multiplied by $n$ and divided by $\sigma_\varepsilon^2$):
	
	
	The $C_p$ criterion, that compares the precision and bias of the full model to models with a subset of the predictors, suffers from two main limitations:
	\begin{enumerate}
		\item The $C_p$ approximation is only valid for large sample size
			
		\item The $C_p$ cannot handle complex collections of models as in the variable selection (or feature selection\footnote{Feature subset selection is the process of identifying and removing as much irrelevant and redundant information as possible. This reduces the dimensionality of the data and may allow learning algorithms to operate faster and more effectively. In some cases, accuracy on future classification can be improved; in others, the result is a more compact, easily interpreted representation of the target concept.}) problem.
	\end{enumerate}
	
	The $C_p$ statistic is often used as a stopping rule for various forms of stepwise regression (don't forget that it is more a feature selection technique than a regression one!). 
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		One way to automatically select a model is to begin with the largest model you can, and then prune (simplify) it, which can be done in several way:
	\begin{itemize}
		\item Eliminate the least-significant coefficient (thanks to the $p$-value)
	
		\item Pick your favourite model selection criterion, consider deleting each coefficient in turn, and pick the sub-model with the best value of the criterion.
	\end{itemize}
	Having eliminated a variable, one then re-estimates the model, and repeats the procedure. Stop when either all the remaining coefficients are significant (under the first option), or nothing can be eliminated without worsening the criterion. That latter procedure is named "\NewTerm{backward stepwise regression}\index{backward stepwise regression}". The method that starts with intercept only and adds variables in the same fashion is named the "\NewTerm{forward stepwise regression}\index{forward stepwise regression}" (don't forget that these two methods are more feature selection techniques than regression one!). There are naturally forward-backward hybrids.\\
	
	Because the inter-correlation between the regressors affect the order of term entry and removal.  Since we are approaching the final model from two different directions this aspect can cause the methods to converge on different models. 
	\end{tcolorbox}	
	Colin Lingwood Mallows proposed the statistic as a criterion for selecting among many alternative subset regressions. Under a model not suffering from appreciable lack of fit (bias), $C_p$ has expectation nearly equal to $p$. Indeed, if your model with $p$ parameters is correct it holds that $\text{SSE}_p\cong (n-p)\sigma_\varepsilon^2$. If your other model is already correct as well, it holds $\text{SSE}_q\cong (n-q)\sigma_\varepsilon^2$, therefore:
	
	
	Otherwise the expectation is roughly $p$ plus a positive bias term. 

	Nevertheless, even though it has expectation greater than or equal to $p$, there is nothing to prevent $C_p<p$ or even $C_p < 0$ in extreme cases. It is suggested that one should choose a subset that has $C_p$ approaching $p$, from above, for a list of subsets ordered by increasing $p$. In practice, the positive bias can be adjusted for by selecting a model from the ordered list of subsets, such that $C_p < 2p$.
	
	To summarize, we should look for models where Mallows's $C_p$ is small and close to the number of predictors in the model plus the constant $p$. A small Mallows's $C_p$ value indicates that the model is relatively precise (has small variance) in estimating the true regression coefficients and predicting future responses. A Mallows's $C_p$ value that is close to the number of predictors plus the constant indicates that the model is relatively unbiased in estimating the true regression coefficients and predicting future responses. Models with lack-of-fit and bias have values of Mallows's $C_p$ larger than $p$.
	
	\pagebreak
	\paragraph{Akaike Information Criterion (AIC)}\mbox{}\\\\
	The Akaike information criterion, AIC, was developed by Akaike to estimate the expected Kullback-Leibler discrepancy (relative quality of statistical models for a given set of dataset) between the model generating the data and a fitted candidate model. In instances where the sample size is large and the dimension of the candidate model is relatively small, AIC serves as an approximately unbiased estimator. In other settings, AIC may be characterized by a large negative bias which limits its effectiveness as a model selection criterion\footnote{For such instances, Hurvic and Tasi (11989 according to holocene calendar) proposed the corrected Akaike information criterion, AICc.}.
	
	Sometimes we have a set of possible models and we want to choose the best model. Model selection methods help us choose a good model. Here are some examples:
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. Suppose we use a polynomial to model the regression function:
	
	We will need to choose the order of polynomial $p$. We can think of this as a sequence of models $\mathcal{M}_1,\ldots,\mathcal{M}_k$ index by $k$.\\
	
	E2. Suppose you have data $y_1,\ldots,y_n$ on age at death of $n$ people. You want to model the distribution of $y$. Some popular models are:
	\begin{itemize}
		\item $\mathcal{M}_1$: The exponential distribution: $p(y,\theta)=\theta e^{-\theta y}$
		
		\item $\mathcal{M}_2$: The gamma distribution: $p(y,a,b)=(b^a/\Gamma(a))y^{a-1}e^{-by}$
		
		\item $\mathcal{M}_3$: The log-normal distribution: we take $\log(y)\cong \mathcal{N}(\mu,\sigma^2)$
	\end{itemize}
	
	E3. Suppose you have time series data $y_1, y_2,\ldots$ A common model is the AR (autoregressive model):
	
	where $\varepsilon_t\cong \mathcal{N}(0,\sigma^2)$. The number $k$ is the order of the model as we know. We need to choose $k$.\\
	
	E4. In a linear regression model, you need to choose which variables to include in the regression. This is named "variable selection\index{variable selection}".
	\end{tcolorbox}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The AICc is useful for selecting between models in the same class. For example, we can use it to select an ARIMA model (\SeeChapter{see section Economy page \pageref{arima}}) between candidate ARIMA models or an ETS model between candidate ETS models (\SeeChapter{see section Economy page \pageref{ETS models}}). However, it cannot be used to compare between ETS and ARIMA models for example because they are most of the times based on too much different mathematical tools.
	\end{tcolorbox}
	Suppose we have models $\mathcal{M}_1,\ldots,\mathcal{M}_k$ where each model is a set of densities (to simplify the notations we will consider only univariate models):
	
	We have data $y_1,\ldots,y_n$ drawn from some density $f$. We do not assume that $f$ is any of the models $\mathcal{M}_j$!

	Let $\hat{\theta}_j$ be the maximum likelihood estimator from model $j$. An estimate of $P$, based on model $j$ is $\hat{p}_j(y)=p(y,\hat{\theta})$. The quality of $\hat{p}_j(y)$ as an estimate of $f$ can be measured by the Kullback-Leibler divergence given for recall in its continuous form by (\SeeChapter{see section Statistical Mechanics page \pageref{kullback-leibler divergence}}):	
	
	That we will write here as:
	
	The first term does not depend on $j$. So minimizing $D_\text{KL}(p,\hat{p}_j)$ over $j$ is the same as maximizing (don't forget that we have proved that $D_\text{KL}$ was positive definite such that $D_\text{KL}\geq 0$):
	
	We need to estimate $D_\text{KL,j}$. Intuitively, a parametric estimate of $D_\text{KL,j}$ is given by the arithmetic average (not such as good as taking the median but more easy to deal for mathematical developments):
	
	 However, apart from the fact that this estimate is non-robust, it is also probably quite biased. Akaike proved that the bias is approximately $d_j/n$ where $d_j=\text{dim}(\vec{\Theta}_j)$. Therefore we use for the model $\mathcal{M}_j$:
	
	That is to say a measure of a in-sample performance plus a penalty (or bias). Defining on the way the "\NewTerm{Akaike information criterion\footnote{Actually, in his original paper, Hirotugu Akaike, proposed using the factor $2$ to simplify some calculation involving chi-squared distribution. Many subsequent specialists have since kept the factor of $2$ which of course will not change which model is selected. Also some authors define AIC as negative of this, and then minimize it; again, clearly the same thing!}}\index{Akaike information criterion}\label{Akaike information criterion}" (AIC):
	
	Notice that maximizing $\hat{D}_{\text{KL},j}$ is the same as maximizing $\text{AIC}(j)$ over $j$. But keep in mind that from an information theory point of view, the divergence $\hat{D}_{\text{KL},j}$ tells us how much information we lost due to our approximation of probability distribution with respect to the true probability distribution. When comparing models, we choose the models with the lowest AIC because in turn it means that the Kullback-Leibler divergence also would be minimum. Low AIC means little information bias. 
	
	And why do we multiply by $2n$? Just for historical reasons! We can multiply by any constant (that's why Data Scientists typically multiply by $-1$; it won't change which model we pick. In fact, different texts use different versions of AIC. A few common notations we can find in Data Science textbooks are for example is:
	
	Written in that way a smaller AIC score is preferable to a larger score. Using the rewritten formula, one can see how the AIC score of the model will increase in proportion to the growth in the value of the numerator, which contains the number of parameters in the model (i.e. a measure of model complexity). And the AIC score will decrease in proportion to the growth in the denominator which contains the maximized log likelihood of the model (which, as we just saw, is a measure of the goodness-of-fit of the model). Keep in mind that the AIC score is useful only when its used to compare two models! The $2k$ indicated well that the less parameters we will have, the less likely will be the overfitting of the model.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Some opine why the need of for AIC when cross-validation techniques are already available. Well, cross validation helps in assessing only the predictive accuracy of the model at different section of the data. Cross validation is a diagnostic not an overfitting prevention strategy. Moreover cross validation does not provide information on how parsimonious the model (number of parameters) is.
	\end{tcolorbox}
	
	\begin{dem}
	Let us recall that during the study of the Fisher information matrix (\SeeChapter{see section Statistics page \pageref{Fisher information matrix}}) we have defined the score function for a parameter $\theta_j$ of a distribution function depending of a set of parameters $\vec{\theta}$ as (keep in mind that we can take the $\ln$ or the $\log$, the result remains the same!):
	
	We know that the score function is used to build the estimate of parameters of the distribution (when we put the score equal to zero). And that therefore if the true parameters are known, then it always equal to zero (at least for non-strange distribution functions...)!
	
	Let us recall that also during the study of this of bivariate Taylor series  (\SeeChapter{see section Sequences and Series page \pageref{hessian matrix}}), we have introduced the Hessian matrix, the matrix of second derivatives, that we will denote $H(\vec{\theta})$ (meaning implicitly that it is evaluated at $\vec{\hat{\theta}}$).
	
	Let us recall that during our study of the central limit theorem (\SeeChapter{see section Statistics page \pageref{central limit theorem}}) we have proved that:
	
	Therefore:
	
	For a bivariate or multivariate Normal distribution (\SeeChapter{see section Statistics page \pageref{bivariate normal distribution}}), this will be written:
	
	For the case that interest us here, let us denote this:
	
	With as we have seen during our study of the Fisher information Matrix (\SeeChapter{see section Statistics page \pageref{Fisher information matrix}}):
	
	Now that we have finish the recalls. Let us take the Taylor series of:
	
	considering $\vec{\hat{\theta}}$ as the variable and doing the development around the true value of the estimator that we will denote $\vec{\theta}_0$:
	
	At the true value $\vec{\theta}_0$ we have (don't forget that this partial derivative is the score, ie the partial derivative of the likelihood and that latter is always equal to zero for the true model with the real parameters!):
	
	Then it remains:
	
	That we can also write:
	
	The $\vec{Z}_n$ doesn't depend explicitly on $y$, then we can write:
	
	Now let us recall that we have seen during our study of the Fisher Information matrix (\SeeChapter{see section Statistics page \pageref{Fisher information matrix}}) that:
	
	So in the previous integral between the parenthesis we have the same expression apart of the notation: $p$ instead of $L$ and $\log$ instead on $\ln$ but however the manipulated objects are the same! Hence:
	
	That we will denote to simplify the next developments as following:
	
	Now let us do the same Taylor series but for the approximate model, with the real data:
	
	Now we assume (...) when $n\rightarrow +\infty$ that:
	
	Therefore:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Notice that for obscure reasons we don't assume that for $n\rightarrow +\infty$:
	
	\end{tcolorbox}
	Then:
	
	Now let us recall that during our study of the Fisher information matrix (\SeeChapter{see section Statistics page \pageref{Fisher information matrix}}) we have proved that:
	
	And we have shown earlier above that:
	
	As $Z_n$ is a vector, and $-\text{E}(H)$ is a non-random matrix, according to the following relation we have proved during our study of the variance-covariance matrix:
	
	We then have for the transformed random variable $-\text{E}(H)Z_n$:
	
	As $\text{E}(H)$ is symmetric, we have $\text{E}(H)^T=\text{E}(H)$ then:
	 
	But we have proved during our study of the Fisher information matrix (\SeeChapter{see section Statistics page \pageref{Fisher information matrix}}) the information matrix equality $-\text{E}(H)(\vec{\theta})=\mathcal{I}(\vec{\theta})$. Therefore:
	 
	ie:
	
	and we recognize here $\sqrt{n}\vec{\hat{\mathcal{S}}}(\theta_0)$. So that finally:
	
	So:
	
	Now we will use the following relation also proved during our study of the variance-covariance matrix (\SeeChapter{see section Statistics page \pageref{quadratic relation for akaike information criterion}}):
	
	Therefore:
	
	But by construction, $\vec{Z}_n$ we have $\vec{\mu}=\vec{0}$ and also using the matrix information equality and the definition of the Fisher information matrix:
	 
	ie:
	
	So that finally:
	
	Remembering as we have proved it that the bigger is the better!
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	As we can factorize the $1/n$ and that when we compare two or more AIC by subtracting them the constants vanishes (the number of parameters $d$ of different models may not be the same!), the latter relation is often used in the following form:
	
	that will be used as the definition of the AIC itself (without any $2n$ factor)! These different possible choices explains why some software packages may well give completely different AICs on the same data for the same model!
	\end{tcolorbox}
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	As the reader has very likely (...) noticed it, there are several steps where we are making a bunch of approximations and assumptions! Some of these approximations (especially those involving the Taylor expansions) can be shown to be OK asymptotically (i.e. as $n\rightarrow +\infty$) by more careful maths. The last steps however, where we invoke the Fisher information matrix (\SeeChapter{see section Statistics page \pageref{Fisher information matrix}}) and the score are rather more dubious. So AIC is a very crude indicator. Cross-validation is much more reliable!
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. Let:
	
	We want to compare two models (notice that the first one has zero parameters to estimate and the second one has only one parameter to estimate: $\mu$):
	
	We want to test:
	
	The test statistics is as we already know:
	
	We reject $H_0$ if $|Z|>Z_\alpha/2$. For $\alpha=0.05$, we reject $H_0$ if $|Z|>2$, ie if:
	
	The likelihood is obviously proportional to (we ignore the value of the variance as for both models it is equal to $1$):
	
	Hence:
	
	Recall that one of the definition of the Akaike information criterion is $\text{AIC} = \mathcal{L}(Y_i,\hat{\theta}_j)-d $. The AIC scores are then:
	
	And:
	
	After, depending on the experimental values of the $Y_i$, we have to compare:
	
	Keeping the one that has the biggest value (as in this example their both negative, that means the one that is the nearest to zero, or the smallest AIC in absolute value)!\\
	
	E2. Remember that during our study of multiple linear regression, we have proved that: 
	
	And also earlier we have proved that:
	
	denoted also sometimes:
	
	Recall that another definition of the Akaike information criterion (the most common in statistical softwares) is $\text{AIC} = -2\mathcal{L}(Y_i,\hat{\theta}_j)+2d$. The AIC score is then when $\sigma$ is unknown for a multiple regression:
	
	What interest us when we use the AIC is to compared different models (subtract the different AIC). As $n$ remains constant between all models, we can eliminate some useless terms that will automatically vanish, this is why in many textbooks we will found:
	
	And we have seen earlier that the Fisher information matrix of the multiple linear regression had $p+1$ dimension, therefore:
	
	What is of denoted in some textbooks as:
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	A software like \texttt{R} deals with $\text{AIC} = -2\mathcal{L}(Y_i,\hat{\theta}_j)+2d$ that we have used above. Therefore the smaller is better!
	\end{tcolorbox}
	
	The most common model selections methods are (among many others as we can see on the Data Science map at page \pageref{mindmap of data science}):
	\begin{enumerate}
		\item $p$-value
		\item AIC (and related methods like $C_p$)
		\item Cross-validation (see further below)
		\item BIC (see further below)
	\end{enumerate}
	AIC is motivated by the estimation of the generalization error (like Mallows's $C_p$, BIC,...). If you want the model for predictions, better use one of these criteria. If you want your model for explaining a phenomenon, use $p$-values.
	We need to distinguish between two goals:
	\begin{enumerate}
		\item Find the model that gives the best prediction (without assuming that any of the models
	are correct).
		\item Assume one of the models is the true model and not the "true" model.
	\end{enumerate}
	Generally speaking, AIC and cross-validation are used for goal (1) while BIC is used for goal (2).
	
	AIC is founded on information theory. When a statistical model is used to represent the process that generated the data, the model will almost never be exact; so some information will be lost by using the model to represent the process. AIC estimates the relative information lost by a given model: the less information a model loses, the higher the quality of that model.

	To apply AIC in practice, we start with a set of candidate models, and then find the models' corresponding AIC values. There will almost always be information lost due to using a candidate model to represent the "true model" (i.e. the process that generated the data). We wish to select, from among the candidate models, the model that minimizes the information loss. We cannot choose with certainty, but we can minimize the estimated information loss.
	
	\paragraph{Cross-Validation metrics}\mbox{}\\\\
	In Machine Learning, "\NewTerm{cross-validation}\index{cross-validation}\label{cross-validation}" (CV) is a resampling method used for model evaluation to avoid testing a model on the same dataset on which it was trained. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. This is a common mistake, especially that a separate testing dataset is not always available. However, this usually leads to inaccurate performance measures (as the model will have an almost perfect score since it is being tested on the same data it was trained on). To avoid this kind of mistakes, cross validation is usually preferred.
	
	Two types of cross-validation can be distinguished, exhaustive and non-exhaustive cross-validation:
	\begin{itemize}
		\item Exhaustive cross-validation methods are cross-validation methods which learn and test on all possible ways to divide the original sample into a training and a validation set.
		
		\item Non-exhaustive cross validation methods do not compute all ways of splitting the original sample.
	\end{itemize}
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,712); %set diagram left start at 0, and has height of 712
		
		%Shape: Rectangle [id:dp5872209300005498] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (73,44) -- (98.5,44) -- (98.5,62) -- (73,62) -- cycle ;
		%Shape: Rectangle [id:dp47458644534676075] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (98.5,44) -- (124,44) -- (124,62) -- (98.5,62) -- cycle ;
		%Shape: Rectangle [id:dp23920291081142087] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (73,62) -- (98.5,62) -- (98.5,78) -- (73,78) -- cycle ;
		%Shape: Rectangle [id:dp33331256059624104] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (98.5,62) -- (124,62) -- (124,78) -- (98.5,78) -- cycle ;
		%Shape: Rectangle [id:dp683903708984889] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (73,78) -- (98.5,78) -- (98.5,102) -- (73,102) -- cycle ;
		%Shape: Rectangle [id:dp42348977349618666] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (98.5,78) -- (124,78) -- (124,102) -- (98.5,102) -- cycle ;
		%Shape: Rectangle [id:dp35085149826288453] 
		\draw   (73,102) -- (98.5,102) -- (98.5,119) -- (73,119) -- cycle ;
		%Shape: Rectangle [id:dp14026376757878545] 
		\draw   (98.5,102) -- (124,102) -- (124,119) -- (98.5,119) -- cycle ;
		%Shape: Rectangle [id:dp8709973279459922] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (140,44) -- (165.5,44) -- (165.5,62) -- (140,62) -- cycle ;
		%Shape: Rectangle [id:dp935889953722125] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (165.5,44) -- (191,44) -- (191,62) -- (165.5,62) -- cycle ;
		%Shape: Rectangle [id:dp8460531132170881] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (140,62) -- (165.5,62) -- (165.5,78) -- (140,78) -- cycle ;
		%Shape: Rectangle [id:dp22005916355786037] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (165.5,62) -- (191,62) -- (191,78) -- (165.5,78) -- cycle ;
		%Shape: Rectangle [id:dp09968781158102269] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (140,78) -- (165.5,78) -- (165.5,102) -- (140,102) -- cycle ;
		%Shape: Rectangle [id:dp2447915223789836] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (165.5,78) -- (191,78) -- (191,102) -- (165.5,102) -- cycle ;
		%Shape: Rectangle [id:dp7315101159832083] 
		\draw   (140,102) -- (165.5,102) -- (165.5,119) -- (140,119) -- cycle ;
		%Shape: Rectangle [id:dp2604573700873538] 
		\draw   (165.5,102) -- (191,102) -- (191,119) -- (165.5,119) -- cycle ;
		%Shape: Rectangle [id:dp5469502329453304] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (208,44) -- (233.5,44) -- (233.5,62) -- (208,62) -- cycle ;
		%Shape: Rectangle [id:dp056334926961020715] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (233.5,44) -- (259,44) -- (259,62) -- (233.5,62) -- cycle ;
		%Shape: Rectangle [id:dp3149372129426069] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (208,62) -- (233.5,62) -- (233.5,78) -- (208,78) -- cycle ;
		%Shape: Rectangle [id:dp39678266430551545] 
		\draw   (233.5,62) -- (259,62) -- (259,78) -- (233.5,78) -- cycle ;
		%Shape: Rectangle [id:dp8128625029274765] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (208,78) -- (233.5,78) -- (233.5,102) -- (208,102) -- cycle ;
		%Shape: Rectangle [id:dp8175391428197365] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (233.5,78) -- (259,78) -- (259,102) -- (233.5,102) -- cycle ;
		%Shape: Rectangle [id:dp5121856342364495] 
		\draw   (208,102) -- (233.5,102) -- (233.5,119) -- (208,119) -- cycle ;
		%Shape: Rectangle [id:dp866937823468432] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (233.5,102) -- (259,102) -- (259,119) -- (233.5,119) -- cycle ;
		%Shape: Rectangle [id:dp06950512131996445] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (276,44) -- (301.5,44) -- (301.5,62) -- (276,62) -- cycle ;
		%Shape: Rectangle [id:dp0915188985899027] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (301.5,44) -- (327,44) -- (327,62) -- (301.5,62) -- cycle ;
		%Shape: Rectangle [id:dp8425631304651999] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (276,62) -- (301.5,62) -- (301.5,78) -- (276,78) -- cycle ;
		%Shape: Rectangle [id:dp9620545164628691] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (301.5,62) -- (327,62) -- (327,78) -- (301.5,78) -- cycle ;
		%Shape: Rectangle [id:dp5192598873202616] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (276,78) -- (301.5,78) -- (301.5,102) -- (276,102) -- cycle ;
		%Shape: Rectangle [id:dp3549713084800885] 
		\draw   (301.5,78) -- (327,78) -- (327,102) -- (301.5,102) -- cycle ;
		%Shape: Rectangle [id:dp059348084498777265] 
		\draw   (276,102) -- (301.5,102) -- (301.5,119) -- (276,119) -- cycle ;
		%Shape: Rectangle [id:dp19020478677590646] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (301.5,102) -- (327,102) -- (327,119) -- (301.5,119) -- cycle ;
		%Shape: Rectangle [id:dp8911241643012155] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (347,44) -- (372.5,44) -- (372.5,62) -- (347,62) -- cycle ;
		%Shape: Rectangle [id:dp46870817922573416] 
		\draw   (372.5,44) -- (398,44) -- (398,62) -- (372.5,62) -- cycle ;
		%Shape: Rectangle [id:dp6180344884819471] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (347,62) -- (372.5,62) -- (372.5,78) -- (347,78) -- cycle ;
		%Shape: Rectangle [id:dp9974812374282085] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (372.5,62) -- (398,62) -- (398,78) -- (372.5,78) -- cycle ;
		%Shape: Rectangle [id:dp9091078684834291] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (347,78) -- (372.5,78) -- (372.5,102) -- (347,102) -- cycle ;
		%Shape: Rectangle [id:dp815738174696161] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (372.5,78) -- (398,78) -- (398,102) -- (372.5,102) -- cycle ;
		%Shape: Rectangle [id:dp6757211184948324] 
		\draw   (347,102) -- (372.5,102) -- (372.5,119) -- (347,119) -- cycle ;
		%Shape: Rectangle [id:dp6642393870926155] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (372.5,102) -- (398,102) -- (398,119) -- (372.5,119) -- cycle ;
		%Shape: Rectangle [id:dp3574624352627789] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (74,169) -- (98.5,169) -- (98.5,193) -- (74,193) -- cycle ;
		%Shape: Rectangle [id:dp13508518390422175] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (98.5,169) -- (123,169) -- (123,193) -- (98.5,193) -- cycle ;
		%Shape: Rectangle [id:dp6830840115089409] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (74,193) -- (98.5,193) -- (98.5,209) -- (74,209) -- cycle ;
		%Shape: Rectangle [id:dp30387879312324495] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (98.5,193) -- (123,193) -- (123,209) -- (98.5,209) -- cycle ;
		%Shape: Rectangle [id:dp7790274890743532] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (74,209) -- (98.5,209) -- (98.5,226) -- (74,226) -- cycle ;
		%Shape: Rectangle [id:dp17523683412821356] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (98.5,209) -- (123,209) -- (123,226) -- (98.5,226) -- cycle ;
		%Shape: Rectangle [id:dp11451088060330972] 
		\draw   (74,226) -- (98.5,226) -- (98.5,243) -- (74,243) -- cycle ;
		%Shape: Rectangle [id:dp7602957926883729] 
		\draw   (98.5,226) -- (123,226) -- (123,243) -- (98.5,243) -- cycle ;
		%Shape: Rectangle [id:dp3946171286416913] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (141,169) -- (165.5,169) -- (165.5,193) -- (141,193) -- cycle ;
		%Shape: Rectangle [id:dp3530045052626498] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (165.5,169) -- (190,169) -- (190,193) -- (165.5,193) -- cycle ;
		%Shape: Rectangle [id:dp326542189700344] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (141,193) -- (165.5,193) -- (165.5,209) -- (141,209) -- cycle ;
		%Shape: Rectangle [id:dp6622315199615636] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (165.5,193) -- (190,193) -- (190,209) -- (165.5,209) -- cycle ;
		%Shape: Rectangle [id:dp7499568787990554] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (141,209) -- (165.5,209) -- (165.5,226) -- (141,226) -- cycle ;
		%Shape: Rectangle [id:dp6914883439608812] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (165.5,209) -- (190,209) -- (190,226) -- (165.5,226) -- cycle ;
		%Shape: Rectangle [id:dp16927434768486793] 
		\draw   (141,226) -- (165.5,226) -- (165.5,243) -- (141,243) -- cycle ;
		%Shape: Rectangle [id:dp7971741362764997] 
		\draw   (165.5,226) -- (190,226) -- (190,243) -- (165.5,243) -- cycle ;
		%Shape: Rectangle [id:dp9351721564419304] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (211,169) -- (235.5,169) -- (235.5,193) -- (211,193) -- cycle ;
		%Shape: Rectangle [id:dp47788087437534243] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (235.5,169) -- (260,169) -- (260,193) -- (235.5,193) -- cycle ;
		%Shape: Rectangle [id:dp603554732958951] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (211,193) -- (235.5,193) -- (235.5,209) -- (211,209) -- cycle ;
		%Shape: Rectangle [id:dp32061179856665634] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (235.5,193) -- (260,193) -- (260,209) -- (235.5,209) -- cycle ;
		%Shape: Rectangle [id:dp13749729638661945] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (211,209) -- (235.5,209) -- (235.5,226) -- (211,226) -- cycle ;
		%Shape: Rectangle [id:dp7621267671822614] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (235.5,209) -- (260,209) -- (260,226) -- (235.5,226) -- cycle ;
		%Shape: Rectangle [id:dp25184316947891583] 
		\draw   (211,226) -- (235.5,226) -- (235.5,243) -- (211,243) -- cycle ;
		%Shape: Rectangle [id:dp012471258791993733] 
		\draw   (235.5,226) -- (260,226) -- (260,243) -- (235.5,243) -- cycle ;
		%Shape: Rectangle [id:dp002589522937559652] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (279,169) -- (303.5,169) -- (303.5,193) -- (279,193) -- cycle ;
		%Shape: Rectangle [id:dp05445871957141568] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (303.5,169) -- (328,169) -- (328,193) -- (303.5,193) -- cycle ;
		%Shape: Rectangle [id:dp7149427804957793] 
		\draw   (279,193) -- (303.5,193) -- (303.5,209) -- (279,209) -- cycle ;
		%Shape: Rectangle [id:dp8472869852865679] 
		\draw   (303.5,193) -- (328,193) -- (328,209) -- (303.5,209) -- cycle ;
		%Shape: Rectangle [id:dp7060776777216784] 
		\draw   (279,209) -- (303.5,209) -- (303.5,226) -- (279,226) -- cycle ;
		%Shape: Rectangle [id:dp08701628184787835] 
		\draw   (303.5,209) -- (328,209) -- (328,226) -- (303.5,226) -- cycle ;
		%Shape: Rectangle [id:dp15613144892149244] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (279,226) -- (303.5,226) -- (303.5,243) -- (279,243) -- cycle ;
		%Shape: Rectangle [id:dp371517948175764] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (303.5,226) -- (328,226) -- (328,243) -- (303.5,243) -- cycle ;
		%Shape: Rectangle [id:dp6581464436389022] 
		\draw   (349,169) -- (373.5,169) -- (373.5,193) -- (349,193) -- cycle ;
		%Shape: Rectangle [id:dp40206658404164397] 
		\draw   (373.5,169) -- (398,169) -- (398,193) -- (373.5,193) -- cycle ;
		%Shape: Rectangle [id:dp3140471721792333] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (349,193) -- (373.5,193) -- (373.5,209) -- (349,209) -- cycle ;
		%Shape: Rectangle [id:dp3120996325404337] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (373.5,193) -- (398,193) -- (398,209) -- (373.5,209) -- cycle ;
		%Shape: Rectangle [id:dp3180561433192146] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (349,209) -- (373.5,209) -- (373.5,226) -- (349,226) -- cycle ;
		%Shape: Rectangle [id:dp5193981651658108] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (373.5,209) -- (398,209) -- (398,226) -- (373.5,226) -- cycle ;
		%Shape: Rectangle [id:dp9490941946512168] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (349,226) -- (373.5,226) -- (373.5,243) -- (349,243) -- cycle ;
		%Shape: Rectangle [id:dp4063635012277762] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (373.5,226) -- (398,226) -- (398,243) -- (373.5,243) -- cycle ;
		%Shape: Rectangle [id:dp9051603135037152] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (74,281) -- (98.25,281) -- (98.25,299) -- (74,299) -- cycle ;
		%Shape: Rectangle [id:dp1643220536850809] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (98.25,281) -- (122.5,281) -- (122.5,299) -- (98.25,299) -- cycle ;
		%Shape: Rectangle [id:dp6897774238026873] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (74,299) -- (98.25,299) -- (98.25,315) -- (74,315) -- cycle ;
		%Shape: Rectangle [id:dp7168861019941448] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (98.25,299) -- (122.5,299) -- (122.5,315) -- (98.25,315) -- cycle ;
		%Shape: Rectangle [id:dp7108169214127738] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (74,315) -- (98.25,315) -- (98.25,332) -- (74,332) -- cycle ;
		%Shape: Rectangle [id:dp5496156551296494] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (98.25,315) -- (122.5,315) -- (122.5,332) -- (98.25,332) -- cycle ;
		%Shape: Rectangle [id:dp8283545794371432] 
		\draw   (74,332) -- (98.25,332) -- (98.25,349) -- (74,349) -- cycle ;
		%Shape: Rectangle [id:dp5310720317979363] 
		\draw   (98.25,332) -- (122.5,332) -- (122.5,349) -- (98.25,349) -- cycle ;
		%Shape: Rectangle [id:dp6171470119435183] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (141.25,298.5) -- (165.5,298.5) -- (165.5,315.5) -- (141.25,315.5) -- cycle ;
		%Shape: Rectangle [id:dp18762028811112597] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (165.5,298.5) -- (189.75,298.5) -- (189.75,315.5) -- (165.5,315.5) -- cycle ;
		%Shape: Rectangle [id:dp5973916746534238] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (141.25,315.5) -- (165.5,315.5) -- (165.5,332.5) -- (141.25,332.5) -- cycle ;
		%Shape: Rectangle [id:dp9159713730876722] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (165.5,315.5) -- (189.75,315.5) -- (189.75,332.5) -- (165.5,332.5) -- cycle ;
		%Shape: Rectangle [id:dp8350275781896841] 
		\draw   (141.25,332.5) -- (165.5,332.5) -- (165.5,349.5) -- (141.25,349.5) -- cycle ;
		%Shape: Rectangle [id:dp018938889805229175] 
		\draw   (165.5,332.5) -- (189.75,332.5) -- (189.75,349.5) -- (165.5,349.5) -- cycle ;
		%Shape: Rectangle [id:dp1509124245414919] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (213.5,298) -- (237.75,298) -- (237.75,315) -- (213.5,315) -- cycle ;
		%Shape: Rectangle [id:dp37698588683632583] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (237.75,298) -- (262,298) -- (262,315) -- (237.75,315) -- cycle ;
		%Shape: Rectangle [id:dp5919595602005328] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (213.5,315) -- (237.75,315) -- (237.75,332) -- (213.5,332) -- cycle ;
		%Shape: Rectangle [id:dp2811357093916005] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (237.75,315) -- (262,315) -- (262,332) -- (237.75,332) -- cycle ;
		%Shape: Rectangle [id:dp6819239463583058] 
		\draw   (213.5,332) -- (237.75,332) -- (237.75,349) -- (213.5,349) -- cycle ;
		%Shape: Rectangle [id:dp5743486344668638] 
		\draw   (237.75,332) -- (262,332) -- (262,349) -- (237.75,349) -- cycle ;
		%Shape: Rectangle [id:dp7330143311580186] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (284.25,298.5) -- (308.5,298.5) -- (308.5,315.5) -- (284.25,315.5) -- cycle ;
		%Shape: Rectangle [id:dp3707543051996136] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (308.5,298.5) -- (332.75,298.5) -- (332.75,315.5) -- (308.5,315.5) -- cycle ;
		%Shape: Rectangle [id:dp04283017757655583] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (284.25,315.5) -- (308.5,315.5) -- (308.5,332.5) -- (284.25,332.5) -- cycle ;
		%Shape: Rectangle [id:dp4056193722656616] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (308.5,315.5) -- (332.75,315.5) -- (332.75,332.5) -- (308.5,332.5) -- cycle ;
		%Shape: Rectangle [id:dp21747433225983026] 
		\draw   (284.25,332.5) -- (308.5,332.5) -- (308.5,349.5) -- (284.25,349.5) -- cycle ;
		%Shape: Rectangle [id:dp15637358206458085] 
		\draw   (308.5,332.5) -- (332.75,332.5) -- (332.75,349.5) -- (308.5,349.5) -- cycle ;
		%Shape: Rectangle [id:dp20976971875311268] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (355.5,298.5) -- (379.75,298.5) -- (379.75,315.5) -- (355.5,315.5) -- cycle ;
		%Shape: Rectangle [id:dp37759797815186924] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (379.75,298.5) -- (407.75,298.5) -- (407.75,315.5) -- (379.75,315.5) -- cycle ;
		%Shape: Rectangle [id:dp9294598147121356] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (355.5,315.5) -- (379.75,315.5) -- (379.75,332.5) -- (355.5,332.5) -- cycle ;
		%Shape: Rectangle [id:dp5230970019945533] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (379.75,315.5) -- (407.75,315.5) -- (407.75,332.5) -- (379.75,332.5) -- cycle ;
		%Shape: Rectangle [id:dp8578023983509857] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (355.5,332.5) -- (379.75,332.5) -- (379.75,349.5) -- (355.5,349.5) -- cycle ;
		%Shape: Rectangle [id:dp8913928860195823] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (379.75,332.5) -- (407.75,332.5) -- (407.75,349.5) -- (379.75,349.5) -- cycle ;
		%Shape: Rectangle [id:dp34409097285891366] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (74.75,382.5) -- (98.25,382.5) -- (98.25,405.5) -- (74.75,405.5) -- cycle ;
		%Shape: Rectangle [id:dp7362626101445264] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (98.25,382.5) -- (121.75,382.5) -- (121.75,405.5) -- (98.25,405.5) -- cycle ;
		%Shape: Rectangle [id:dp6285600939820672] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (74.75,405.5) -- (98.25,405.5) -- (98.25,422.5) -- (74.75,422.5) -- cycle ;
		%Shape: Rectangle [id:dp1675679934686587] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (98.25,405.5) -- (121.75,405.5) -- (121.75,422.5) -- (98.25,422.5) -- cycle ;
		%Shape: Rectangle [id:dp09755304930587849] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (74.75,422.5) -- (98.25,422.5) -- (98.25,439.5) -- (74.75,439.5) -- cycle ;
		%Shape: Rectangle [id:dp7911621188050444] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (98.25,422.5) -- (121.75,422.5) -- (121.75,439.5) -- (98.25,439.5) -- cycle ;
		%Shape: Rectangle [id:dp1573180111425394] 
		\draw   (74.75,439.5) -- (98.25,439.5) -- (98.25,456.5) -- (74.75,456.5) -- cycle ;
		%Shape: Rectangle [id:dp5272553532230633] 
		\draw   (98.25,439.5) -- (121.75,439.5) -- (121.75,456.5) -- (98.25,456.5) -- cycle ;
		%Shape: Rectangle [id:dp7674612678913348] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (142.25,382.5) -- (165.75,382.5) -- (165.75,405) -- (142.25,405) -- cycle ;
		%Shape: Rectangle [id:dp2451215474056745] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (165.75,382.5) -- (189.25,382.5) -- (189.25,405) -- (165.75,405) -- cycle ;
		%Shape: Rectangle [id:dp8438616105240277] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (142.25,405) -- (165.75,405) -- (165.75,422) -- (142.25,422) -- cycle ;
		%Shape: Rectangle [id:dp24895524897606536] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (165.75,405) -- (189.25,405) -- (189.25,422) -- (165.75,422) -- cycle ;
		%Shape: Rectangle [id:dp17006609992633215] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (213.75,422.5) -- (237.25,422.5) -- (237.25,439.5) -- (213.75,439.5) -- cycle ;
		%Shape: Rectangle [id:dp6477012629119447] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (237.25,422.5) -- (260.75,422.5) -- (260.75,439.5) -- (237.25,439.5) -- cycle ;
		%Shape: Rectangle [id:dp6580268503033042] 
		\draw   (213.75,439.5) -- (237.25,439.5) -- (237.25,456.5) -- (213.75,456.5) -- cycle ;
		%Shape: Rectangle [id:dp7789191308028904] 
		\draw   (237.25,439.5) -- (260.75,439.5) -- (260.75,456.5) -- (237.25,456.5) -- cycle ;
		
		% Text Node
		\draw (446,64) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{Permutation}\\Randomization test};
		% Text Node
		\draw (446,192) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{Bootstrap}};
		% Text Node
		\draw (446,316) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{Jackknife}};
		% Text Node
		\draw (446,416) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{Cross validation}};
		% Text Node
		\draw (75,122) node [anchor=north west][inner sep=0.75pt]   [align=left] {sample};
		% Text Node
		\draw (141,122.4) node [anchor=north west][inner sep=0.75pt]    {$r=1$};
		% Text Node
		\draw (209,122.4) node [anchor=north west][inner sep=0.75pt]    {$r=2$};
		% Text Node
		\draw (277,122.4) node [anchor=north west][inner sep=0.75pt]    {$r=3$};
		% Text Node
		\draw (348,122.4) node [anchor=north west][inner sep=0.75pt]    {$r=4$};
		% Text Node
		\draw (75,246) node [anchor=north west][inner sep=0.75pt]   [align=left] {sample};
		% Text Node
		\draw (142,246.4) node [anchor=north west][inner sep=0.75pt]    {$r=1$};
		% Text Node
		\draw (212,246.4) node [anchor=north west][inner sep=0.75pt]    {$r=2$};
		% Text Node
		\draw (280,246.4) node [anchor=north west][inner sep=0.75pt]    {$r=3$};
		% Text Node
		\draw (350,246.4) node [anchor=north west][inner sep=0.75pt]    {$r=4$};
		% Text Node
		\draw (75,352) node [anchor=north west][inner sep=0.75pt]   [align=left] {sample};
		% Text Node
		\draw (142.25,351.9) node [anchor=north west][inner sep=0.75pt]    {$r=1$};
		% Text Node
		\draw (214.5,352.4) node [anchor=north west][inner sep=0.75pt]    {$r=2$};
		% Text Node
		\draw (284.5,351.9) node [anchor=north west][inner sep=0.75pt]    {$r=3$};
		% Text Node
		\draw (356.5,352.9) node [anchor=north west][inner sep=0.75pt]    {$r=4$};
		% Text Node
		\draw (76.75,459.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {sample};
		% Text Node
		\draw (138.75,459.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {training};
		% Text Node
		\draw (215.75,459.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {test};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Some famous techniques of cross-over validation}
	\end{figure}

	The concept of cross-validation is actually simple: Instead of using the whole dataset to train and then test on same data, we could randomly divide our data into training and testing datasets.

	There are several types of cross-validation methods (LOOCV – Leave-One-Out-Cross validation\footnote{We already know that latter as it is just the Jackknife method}, the holdout method, $K$-fold cross validation). Here, we will discuss the "\NewTerm{$K$-Fold cross validation method}\index{$K$-fold cross validation method}".

	The idea is illustrated as following:
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/cross_over_kfold.jpg}
		\caption[$K$-fold cross-over validation]{$K$-fold cross-over validation (author: ?)}
	\end{figure}	
	
	So following the above illustration, the $K$-fold basically consists of the below steps:
	\begin{enumerate}
		\item Split the data in a train and a test set
		
		\item Randomly split the train set into $K$ subsets, also named "folds".
		
		\item Train the model on all $K$ subsets excepted the first one
		
		\item Fit the model on the first set that was ignored previously and keep its quality metric (i.e. "cross validated metric")
		
		\item Repeat the procedure (3) to (4) (but by ignoring the second set, after the third, and so on...)
		
		\item Average all the quality metrics (ordinary cross-validation: OCV)
		
		\item At the end, use all the $K$-folds to train the model and fit it on the test set
		
		\item Compare if the last fit perform better that the average of all previous fits
		
		\item If not, use one of the $K$ as reference model based on the quality metric. If yes, keep the model made in step (8)
	\end{enumerate}
	
	The typical basis of cross-over quality metrics is the mean square error MSE. Then the average quality metric, or the "\NewTerm{ordinary cross-validation}\index{ordinary cross-validation}", for a $K$ fold will be (assuming that each fitted model has a sub-sample size of $n_k$):
	
	If we denoted the model (in a univariate case) by $\hat{y}_i^{[k]}$ for the subset (submodel) $k$ and the true values by $y_i$ we then have:
	
	Therefore the OCV for an univariate model based one the MSE is :
	
	For example with LASSO, ridge or elastic net regularized regression, we have:
	
	And the "best lambda" $\lambda_0$ is the one that minimize the $\text{OCV}_\text{MSE}$ such that:
	
	
	Let us now speak about a very common relation in statistical textbooks, the cross-validation for one of the subsets of the Leave One Out method (i.e. Jackknife):
	
	Here, $y_i$ is the actual label value of training point $i$, ${y}_i^{[k]}$ is the value predicted by the cross-validation model trained on all points except $i$, $\hat{y}^i$ is the value predicted by the regression model trained on all points (including point $i$), and $H_{ii}$ is as we know the leverage of point $i$, given for recall by:
	
	and:
	
	Notice that the left side of the above relation is the LOOCV sum of squares error (the quantity we seek), while the right can be evaluated given only the model trained on the full data set. Fantastically, this allows us to evaluate the LOOCV error using only a single regression!!!
	\begin{dem}
	Consider the LOOCV step where we construct a model trained on all points except training example $k$. Using a linear model of form $y^{[k]}=\vec{x}^T\vec{\beta}^{[k]}$ - with $\vec{\beta}^{[k]}$ the coefficient vector - the sum of squares that must be minimized is:
	
	Here, we are using an exponent $k$ on $\vec{\beta}^{[k]}$ to highlight the fact that the above corresponds to the case where example $k$ is held out. We minimize the above relation by taking the gradient with respect to $\vec{\beta}^{[k]}$. Setting this to zero gives the following equation:
	
	Rearranging and simplifying a bit gives:
	
	Similarly, the full model (trained on all points) coefficient vector $\vec{\beta}$ (here the notation $\vec{\beta}$ means in fact $\hat{\vec{\beta}}$ obviously!) satisfies:
	
	But we have  that:
	
	can we rewritten as for the left term:
	
	and for the right term:
	
	Therefore:
	
	But as we have proved just earlier that:
	
	Then substituting in the previous relation, this lead us to:
	
	Rearranging:
	
	That can be written using the definition of $y_i^{[k]}$:
	
	Hence:
	
	Left multiplication by $\vec{x}_k^T\left(\sum_i\vec{x}_i\vec{x}_i^T\right)^{-1}$ (without forgetting that the term into parenthesis is a scalar) gives:
	
	So it only remains (rearranging a bit):
	
	We recognize here the vector definition of $H_{kk}$. Therefore:
	
	that is:
	
	That latter can be rewritten as:
	
	Hence:
	
	Squaring and summing finally leads to:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Again... Fantastically, this allows us to evaluate the LOOCV error using only a single regression!!!
	
	A minor variant on cross validation is, so-named "\NewTerm{generalized cross validation}\index{generalized cross validation}\label{generalized cross validation}", which, of course, like most things statisticians call "generalized" isn't... It replaces the $H_{kk}$ in the denominator with their average trace $\text{tr}(H_{kk})/n$ giving:
	
	
	\pagebreak
	\subsection{Ridge, LASSO and Elastic Net regularization}\label{regularization}
	"\NewTerm{Regularization}\index{regularization}" also named "\NewTerm{regularized regression}\index{regularized regression}" or "\NewTerm{penalized regression}\index{penalized regression}" has been intensely studied on the interface between statistics and computer science as it is an empirical method can be used to avoid overfitting in almost all regression models (but is mainly known for its application to logistic regression).

	There are two types of regularization as follows:
	\begin{itemize}
		\item $L_1$ Regularization or LASSO Regularization (see details further below)
		
		\item $L_2$ Regularization or Ridge Regularization (see details further below)
	\end{itemize}
	Where for recall $L_p:\mathbb{R}^n\mapsto\mathbb{R}$ (\SeeChapter{see section Topology page \pageref{distance}}) is defined:
	
	with $p>0$. For $p=2$, this is the familiar Euclidean distance (don't confuse with the notation of the lost function $L$ if possible...). 
	
	Ridge and Lasso regularizations are also known as "\NewTerm{shrinkage methods}\index{shrinkage methods}", because they reduce or shrink the coefficients in the resulting regression. This reduces the variance in the model: as input variables are changed, the models prediction changes less than it would have without the regularization. Why would you want to reduce the variance of a model? To avoid overfit!	
	
	$L_1$ and $L_2$ are the most common types of regularization. These update the general cost function by adding another term known as the "regularization term":
	
	Or more formally if we denote by $R(f)$ the regularization term :
	
	where $V$ is an underlying loss function (most of times for regression models it's the residual sum of squares RSS) that describes the cost of predicting $f(x)$ when the label is $y$ and $\lambda$ is a parameter which controls the importance of the regularization term. $R(f)$ is typically chosen to impose a penalty on the complexity of $f$. Concrete notions of complexity used include restrictions for smoothness and bounds on the vector space norm.
	
	Notice that softwares compute estimates for a large number of values for $\lambda$ at once.  The optimal $\lambda$ is selected by cross validation of some sort... The most common one in the end of the 121st century (holocene calendar) being the generalized cross-validation metric (see further below page \pageref{generalized cross validation}).
	
	From a Bayesian point of view, many regularization techniques correspond to imposing certain prior distributions on model parameters. 
	
	\begin{dem}
	Consider that the regularized loss function $L$ has a similar role as the probability of a parameter configuration $\beta$ given the observations $X$,$\vec{y}$. Applying the Bayes theorem (\SeeChapter{see section Probabilities page \pageref{bayes formula}}), we get:
	
	Taking the log of the expression gives us:
	
	Now, let's say $L(\theta)$ is the negative\footnote{Negative since we want to maximize the probability but minimize the cost.} log-posterior, $-\log(P(\theta|X,\vec{y}))$. Since the last term does not depend on $\theta$, we can ignore it without changing the minimum. We are left with two terms:
	\begin{itemize}
		\item the likelihood term $\log (P(X,\vec{y}|\theta))$ depending on $X$ and $\vec{y}$, and 
	
		\item the prior term $\log (P(\theta))$ depending on $\theta$ only. These two terms correspond exactly to the loss and the regularization term.
	\end{itemize}
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	It can be proved (see further below) that $L_2$ regularization is the case with a Gaussian prior analytically and the $L_1$ is equivalent to a Laplacian prior.
	\end{tcolorbox}
	
	We can also see regularization as an application of the Lagrange Multiplier method (see previously page \pageref{Lagrange multipliers method}).

	\subsubsection{$L_2$ Regularization or Ridge Regularization}\label{ridge}
	When learning a linear function $f$, characterized by an unknown vector $\vec{\beta}$ such that $f(x)=\vec{\beta}\circ \vec{x}$, the $L_2$-norm loss corresponds to "\NewTerm{Tikhonov regularization}\index{Tikhonov regularization}". This is one of the most common forms of regularization, is also known as "\NewTerm{ridge regression}\index{ridge regression}", and is expressed as:
	
	where $\lambda$ is the "\NewTerm{tuning parameter}\index{tuning parameter}" or "\NewTerm{shrinkage parameter}\index{shrinkage parameter}".
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	If the different predictor variables don't have physically comparable units it's a good idea to standardize them first, so they all have mean $0$ and variance $1$ and also no physical units. Otherwise penalizing the $p$ predictor with the $L_2$-norm using $\sum _{i=1}^{p}\beta_i^2$ seems to be adding up apples, oranges, and the occasional bout of regret (some people always pre-standardize the predictors).
	\end{tcolorbox}
	
	The learning problem with the least squares loss function and Tikhonov regularization can be solved analytically. Written in matrix form, the optimal $\vec{\beta}$ will be the one for which the gradient of the loss function with respect to $\vec{\beta}$ is $\vec{0}$.
	
	Let us write this explicitly:
	
	which is just the usual least squares criterion with a penalty determined by $\lambda$ for large coefficient estimates. This is why it's named "\NewTerm{penalized residual sum of squares}\index{penalized residual sum of squares}" (PRSS). If $\lambda=0$ the lasso is the same as OLS; as $\lambda$ increases, shorter coefficients are preferred. Therefore we can say that most of times (but now always!) if we increase the value of $\lambda$ then the magnitude of the coefficients decreases.
	
	 Notice also that this is a well defined, convex, differentiable optimization problem even after including $L_2$ function. The $L_2$ function acts as a penalizer: $\|\vec{\beta}\|^{2}$ grows large as the coefficients $\vec{\beta}$ grow large, thus the minimization program tries to make w smaller.
	 
	 There are many variations on this procedure, including application of it to other-than the linear model.
	
	We take the derivative:
	
	To find the optimum we put:
	
	Therefore:
	
	By construction of the optimization problem, other values of $\vec{\beta}$  would give larger values for the loss function. Notice that in particular the regularization term solves the problem of non-invertibility of the first term between parenthesis.
	
	Notice that the solution is indexed by the parameter $\lambda$. So for each $\lambda$, we have a solution. The fact that we add $\lambda n\mathds{1}$ to the diagonal of ${X}^{T}{X}$ (which correspondence to the correlation matrix if $X$ is previously centered and reduced) is why this method is named "ridged" (and the term $\lambda n\mathds{1}$ is often named the "ridged").
	
	The reader may also notice that with:
	
	excepted is some very special cases, the ridge regression will never take the coefficients $\hat{\vec{\beta}}_\lambda^\text{ridge}$ to zero as the denominator will never be practically infinite! But we can see obviously that as $\lambda \rightarrow +\infty$ then $\hat{\vec{\beta}}_\lambda^\text{ridge}\rightarrow \vec{0}$.

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Inclusion of $\lambda$ makes problem non-singular even if $X^{T}X$ is not invertible. It seems that this was the original motivation for ridge regression (Hoerl and Kennard, 11970 according to holocene calendar).
	\end{tcolorbox}
	
	Let us prove now that the ridge regression corresponds indeed to a Gaussian prior on the coefficients.
	\begin{dem}
	Let us imagine that you want to infer some parameter $\beta$ from some observed input-output pairs $(x_1,y_1),\ldots,(x_n,y_n)$. Let us assume that the outputs are linearly related to the inputs via $\beta$ and that the data are corrupted by some noise $\varepsilon$:
	
	where $\varepsilon$ is Gaussian noise with as we know mean $0$ and variance $\sigma^2_\varepsilon$. This gives rise to a Gaussian likelihood:
	
	Let us regularise parameter $\beta$ by imposing the Gaussian prior $\mathcal{N}(\beta|0,\lambda^{-1})$, where $\lambda$ is a strictly positive scalar. Hence, combining the likelihood and the prior we simply have:
	
	Let us take the logarithm of the above expression. Dropping some constants we get:
	
	If we maximise the above expression with respect to $\beta$, we get the so named "\NewTerm{maximum a-posteriori estimate}\index{maximum a-posteriori estimate}" for $\beta$, or "MAPE for ridge regression" for short. In this expression it becomes apparent why the Gaussian prior can be interpreted as a $L_2$ regularisation term.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Let us now prove that the $\hat{\vec{\beta}}_\lambda^\text{ridge}$ are biased. 
	\begin{dem}
	First let us recall that we have proved that:
	
	Now let us rewrite this:
	
	Remember that we have proved in the section of Linear Algebra that if $A$ and $B$ are invertible, then:
	
	Then:
	
	So:
	
	Therefore if $\lambda \neq 0$ then:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	We know that if we actually have the "right" model, then, say, OLS parameter estimates will be unbiased and have minimal variance among all unbiased (linear) estimators (they are BLUE). Predictions from an OLS model will be best linear unbiased predictions (BLUPs). That sounds good.
	
	So we have $\hat{\vec{\beta}}_\lambda^\text{ridge}=(X^TX+\lambda\mathds{1})^{-1}X^TY$ which is biased; but if $X$ is ill conditioned then $\text{V}(\hat{\beta})\propto (X^TX)^{-1}$ may be monstrous whereas $\text{V}(\hat{\vec{\beta}}_\lambda^\text{ridge})$ can be much more modest. Indeed, let us recall that we have proved during our study of multiple linear regression that (see above page \pageref{standard error of the regression coefficients}):
	
	Exactly the same developments would lead us to:
	
	So increasing $\lambda$ decrease indeed the variances of the $\hat{\vec{\beta}}_\lambda^\text{ridge}$.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	If we assume the coefficients of the regression to be Gaussian distributed with mean $0$ and variance $\sigma_\varepsilon^2$ and we take $\lambda=1/\sigma_\varepsilon^2$, we speak sometimes then of "\NewTerm{Gauss reguralization}\index{Gauss regularization}".
	\end{tcolorbox}
	
	\pagebreak
	\subsubsection{$L_1$ Regularization or LASSO Regularization}
	We describe the basic idea through the "\NewTerm{LASSO regularization}\index{LASSO regularization}\label{LASSO regularization}" (LASSO stands for: Least Absolute Shrinkage and Selection Operator) as applied in the context of linear regression.  The method starts by assuming a model like:
	
	which is just the usual least squares criterion with a penalty determined by $\lambda$ for large coefficient estimates.  Again, if $\lambda=0$ the lasso is the same as OLS; as $\lambda$ increases, shorter vectors are preferred.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	If the different predictor variables don't have physically comparable units it's a good idea to standardize them first, so they all have mean $0$ and variance $1$ and also no physical units. Otherwise penalizing the $p$ predictor with the $L_1$-norm using $\sum _{i=1}^{p}|\beta_i|$ seems to be adding up apples, oranges, and the occasional bout of regret (some people always pre-standardize the predictors).
	\end{tcolorbox}
	
	Immediately we lose the differentiability of the optimization function: the absolute values are non-differentiable. 
	
	Thus we cannot solve the optimization problem using standard calculus techniques in the most general case. What do we gain then? First we can consider only the case $\vec{\beta}\neq\vec{0}$.
	
	We take the derivative:
	
	To find the optimum we put:
	
	Therefore:
	
	If you observe the numerator, it may be equal to zero, since we are subtracting some value of $\lambda$. And therefore the values of $\hat{\vec{\beta}}_\lambda^\text{LASSO}$ may be all equal to zero at the opposite of the ridge regression!
	
	One can show that for some large enough $\lambda$, the solution is a sparse solution. A sparse solution means a solution for $\vec{\beta}$ that has many zeros in it, effectively removing the corresponding variable from the system. There are then two immediate consequences:
	\begin{enumerate}
		\item The system will give a coefficient of zero to non-relevant variables, instead of some insanely small coefficient. This means independent or near-independent variables will be discarded instead of included.
		
		\item It gives us the most important variables in the regression model! We can speak confidently that variable $i$ has a strong connection with the response variable $\vec{y}$.
	\end{enumerate}

	Again notice that softwares compute estimates for a large number of values for $\lambda$ at once.  The optimal $\lambda$ is selected by cross validation of some sort.
	
	Let us prove now that the LASSO regression corresponds to a Laplace prior on the coefficients.
	\begin{dem}
	Let us imagine that you want to infer some parameter $\beta$ from some observed input-output pairs $(x_1,y_1),\ldots,(x_n,y_n)$. Let us assume that the outputs are linearly related to the inputs via $\beta$ and that the data are corrupted by some noise $\varepsilon$:
	
	where $\varepsilon$ is Gaussian noise with as we know mean $0$ and variance $\sigma^2_\varepsilon$. This gives rise to a Gaussian likelihood:
	
	First, let us recall that a Laplace distribution is given by (difference of two exponential distribution as we have proved it in the section of Statistics at page \pageref{Laplace distribution}):
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Notice that the Normal and Laplace prior have a strong similitude. This is why they are commonly denoted:
	
	Ridge regression ($p=2$) corresponds to a Gaussian prior and the LASSO ($p=1$) to a Laplacian (or
double-exponential) prior. 
	\end{tcolorbox}
	Let us regularise parameter $\beta$ by imposing the Laplace prior $\mathcal{L}(\beta|0,b)$, where $b$ is a strictly positive scalar (and mean $\mu=0$). Hence, combining the likelihood and the prior we simply have:
	
	Let us take the logarithm of the above expression. Dropping some constants we get:
	
	Let $1/b=\lambda$, we get then:
	
	If we maximise the above expression with respect to $\beta$, we get the so named "\NewTerm{maximum a-posteriori estimate}\index{maximum a-posteriori estimate}" for $\beta$, or "MAPE for LASSO regression" for short. In this expression it becomes apparent why the Laplace prior can be interpreted as a $L_1$ regularisation term.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	If we assume the coefficients of the regression to be Laplace distributed with variance $\sigma_\varepsilon^2$ and we take $\lambda=\sqrt{2}/\sigma_\varepsilon$, we speak sometimes then of "\NewTerm{Laplace reguralization}\index{Laplace regularization}".
	\end{tcolorbox}
	
	\subsubsection{$L_1+L_2$ Regularization or Elastic Net Regularization}
	In statistics and, in particular, in the fitting of linear or binomial logistic regression models, the elastic net is a regularized regression method that linearly combines the $L_1$ and $L_2$ penalties of the lasso and ridge methods.
	
	The elastic net method overcomes the limitations of the LASSO (least absolute shrinkage and selection operator) method which uses a penalty function based for recall on:
	
	Use of this penalty function seems to have several limitations. For example, in the "large $n$, small $N$" case (high-dimensional data with few examples), the LASSO seems to select at most $n$ variables before it saturates. Also if there is a group of highly correlated variables, then the LASSO tends to select one variable from a group and ignore the others (i.e. LASSO tends to select only one predictor among the predictors that are highly correlated). To overcome these limitations, the elastic net adds a quadratic part to the penalty $\|\beta \|^{2}$, which when used alone is ridge regression. The estimates from the elastic net method are defined by\label{elastic net}:
	
	The quadratic penalty term makes the loss function strictly convex, and it therefore has a unique minimum. The elastic net method includes the LASSO and ridge regression: in other words, each of them is obviously a special case where $\lambda_{1}=\lambda ,\lambda_{2}=0$ or $\lambda_{1}=0,\lambda_{2}$.
	
	Another very common notation is: 
	
	where $0\leq \alpha \leq 1$ is a compromise between ridge ($\alpha=0$) and lasso ($\alpha=1$).
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Depending on the softwares, the algorithms implement some variations of the relation above. The most common one are:
	
	or:
	
	and so on.... Hence the sometimes the huge differences in the optimal $\lambda$ value.
	\end{tcolorbox}
	
	A common question about regularization is that in traditional statistics models, while building a model, we check for multicollinearity using methods such as estimates of the variance inflation factor (VIF), but in Machine Learning, we instead use regularization for feature selection and don't seem to check whether features are correlated at all. Why do we do that?
	
	The reason is because the goals of "traditional statistics" are different from many Machine Learning techniques.

	In traditional statistics regressions, we are trying to understand the impact the independent variables have on the dependent variable. If there is strong multicollinearity, this is simply not possible. No algorithm is going to fix this. If studiousness is correlated with class attendance and grades, we cannot know what is truly causing the grades to go up - attendance or studiousness.

	However, in Machine Learning techniques that focus on predictive accuracy, all we care about is how we can use a set of variables to predict another set. We don't care about the impact these variables have on each other.

	Basically, the fact that we don't check for multicollinearity in Machine Learning techniques isn't a consequence of the algorithm, it's a consequence of the goal!
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	For more information we strongly recommend the reader to refer to the excellent lecture notes of Wessel N. van Wieringen \cite{1509.09169}.
	\end{tcolorbox}

	
	\pagebreak
	\subsubsection{Polynomial regression}
	We will now see how to determine the for example of the best second degree polynomial that passes by any number of points, but without transforming the function contrary to what we have just done just before! As we like physics in this book, we will take a classic case of Kinematic to join business with pleasure...
	
	So consider that we are looking for a polynomial of the second degree of the form:
	
	knowing that the method is easily applicable to higher order polynomials (at least as far as we know).
	
	Relation that it is customary to write in the field of polynomial regression as follows:
	
	where $i$ represents the number of points available to us.
	
	For the rest, let us once again base our developments on the least squares method. In other words, we seek the coefficients $c_1,c_2,c_3$ that minimize the error:
	
	and we attack again with partial derivatives for each coefficient:
	
	Therefore after a small rearrangement and simplification:
	
	Similarly:
	
	Therefore after a small rearrangement and simplification:
	
	And finally:
	
	Therefore after a small rearrangement and simplification:
	
	So using the notation of linear algebra, we finally have to solve the following system:
	
	and so we just have to solve this simple linear system either by hand using the relations proved in the section of Linear Algebra, or with a simple spreadsheet software (like Microsoft Excel for example).
	
		
	\pagebreak
	\subsubsection{Kernel regression}\label{kernel regression}
	We have already discussed and introduce quite in details the concepts of kernel smoothing in the section Statistics at page \pageref{kernel smoothing}.
	
	We estimate a regression model $\hat{y}(x)$ by the "\NewTerm{kernel regression estimate}\index{kernel regression estimate}" defined by:
	
	Note that each $\hat{y}(x)$ is a weighted average of the $y_i$, which is the operation:
	
	where the $p_i$ are non-negative constants that sum to one and obviously given by:
	
	Again, a very common smoother is for example the gaussian Kernel:
	
	For examples of such kernel regression, the reader can refer to our \texttt{R} companion book.
	
	An improvement to kernel smoothing is "\NewTerm{local polynomial smoothing}\index{local polynomial smoothing}" or also named "\NewTerm{locally estimated scatterplot smoothing}\index{locally estimated scatterplot smoothing}" (LOESS), which does the following: suppose we choose to use a first degree polynomial (that is, use linear regression), then for each $x$ we gave $\hat{\alpha}(x)$ and $\hat{\beta}(x)$ that minimize the weighted residual sum of squares:
	
	and set:
	
	Note that we have to do this for each $x$ for which we wish to evaluate $\hat{y}(x)$.
	
	In the the general case, the above relation we will be written:
	
	
	Again, for an example of such kernel regressions, the reader can refer to our \texttt{R} companion book. 
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	As far as we know there is no closed form inferential techniques for the coefficients or any other regression diagnostics.
	\end{tcolorbox}
	
	\pagebreak
	\subsubsection{Logistic regressions (LOGIT)}\label{logistic regression logit}
	Often, statistics data are relative to qualitative informations (prediction if a customer is at risk for a credit, prediction if an individual is at risk for sickness, prediction of customer crunch\footnote{Prediction if a customer will leave your business}, prediction of buying a next given item, etc.) or bounded in a $[0,1]$ interval (quite common case in Design of Experiments!). However, as we shall see, the traditional inference methods do not allow to model and study this type of variables (in fact they can do but performs poorly most of time!).
	
	A well known simple model that performs most of time quite poorly for binary classification is the "\NewTerm{signum least squares classifier}\index{signum least squares classifier}" defined as:
	
	where we used the signum function already defined in the section of Arithmetic Operators page \pageref{signum function}.
	
	The intuition behind this "least squares classifier\index{least squares classifier}" is simple. The value $f(\vec{x})$ is a number, which ideally is near $+1$ when $y_i=+1$, and near $-1$ when $y_i=-1$. When $f(\vec{x})$ is near $+1$ we have confidence in our guess $y_i=+1$; when it is small and negative (say, $f(x)=-0.03$), we guess that $y_i=-1$ but our confidence in the guess will be low. So we won't pursue this idea further as we can't associate any probability with it and furthermore the signum function is not continuous (of class $\mathcal{C}^1$) and this can make problems in some computer implementations).
	
	Specific methods should be used taking into account for example the lack of continuity of the processed variables or absence of the natural order between the terms that can take the qualitative variable. We will therefore see now the most simple of these methods.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	We can use Logistic Regression in a non classification way (!), in that latter case it is named "\NewTerm{decile analysis}\index{decile analysis}". Decile analysis has become more like a lost art, even if it was once a popularly used technique, however the convention of teaching and bucketing machine learning problems into either "classification" or "regression" types, lead many people to forget decile analysis type analyses (excepted in finance, insurance and risk management). It is very likely that most freshly minted data scientists would not have even heard of Decile analysis. So, decile analysis is simply used to categorize dataset from highest to lowest values or vice versa (based on predicted probabilities). As obvious from the name, the analysis involves dividing the dataset (thanks to the response variable) into ten groups.
	\end{tcolorbox}	
	
	\paragraph{Binomial Logistic regression}\mbox{}\\\\
	As we have seen above, the simple linear regression therefore aims to model the relation between an unbounded quantitative dependent variable and an unbounded quantitative explanatory variable.
	
	When the "\NewTerm{class variable}\index{class variable}" to explain $Y$ is binary (yes-no, presence-absence, 0-1, etc.), the idea is to approach it at first by a probability function $P(Y=1)$ which gives at the opposite the probability of belonging to the class $Y=0$ or $Y=1$, which we will name the "\NewTerm{logistic binomial regression}\index{logistic binomial regression}" or "\NewTerm{logit regression}\index{logit regression}" or simply "\NewTerm{binomial regression}\index{binomial regression}" and also "\NewTerm{binary regression}\index{binary regression}" (often used in the context of artificial neural networks that we will see later). Then, in a second step, we define a for a binary case a "\NewTerm{cutoff}\index{cutoff}" value. For example, if we take a cutoff of $0.5$ then the cases for which $P(Y=1)>0.5$ will belong to the class $1$ (and vice versa in the opposite case). This is why the binomial\footnote{The reader should remember that the Bernoulli distribution is a special case of the binomial distribution where a single trial is conducted!} logistic regression is often qualified of "\NewTerm{binary classifier}\index{binary classifier}".
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} In fact, logistic regression is a simple probability distribution law in our case (we will see another logistic regression in the section Economy during our study of time series and yet another one in the section of Populations Dynamics).\\
	
	\textbf{R2.} It is obviously not possible to apply systematically logistic regression to any type of data sample! Sometimes we have to look elsewhere...\\
	
	\textbf{R3.} When the number of modalities is equal to $2$, we talk about "\NewTerm{dummy variable}\index{dummy variable}" (yes-no) or a "\NewTerm{dichotomic model}\index{dichotomic model}" or even of "\NewTerm{indicator variable}\index{indicator variable}"; if it is greater than $2$, we talk about "\NewTerm{polytomous variables}\index{polytomous variables}" (polytomous logistic regression) or "\NewTerm{multinomial logistic regression}\index{multinomial logistic regression}" (see further below). Therefore the logit binomial model is a "dichotomous model".
	\end{tcolorbox}
	For example, consider the dichotomous variable: "graduation". This takes two forms: "ongoing", "finished". Age is a possible predictor of this variable and we seek to model the probability of completing studies as a function of age.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	To build the graph below, we calculated and shown on the ordinate, for youth of various ages $x$, the percentage of those who have left school.
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,728); %set diagram left start at 0, and has height of 728
		
		%Shape: Axis 2D [id:dp36018839594230334] 
		\draw  (110,264.64) -- (519.5,264.64)(150.95,31) -- (150.95,290.6) (512.5,259.64) -- (519.5,264.64) -- (512.5,269.64) (145.95,38) -- (150.95,31) -- (155.95,38) (202.95,259.64) -- (202.95,269.64)(254.95,259.64) -- (254.95,269.64)(306.95,259.64) -- (306.95,269.64)(358.95,259.64) -- (358.95,269.64)(410.95,259.64) -- (410.95,269.64)(462.95,259.64) -- (462.95,269.64)(145.95,212.64) -- (155.95,212.64)(145.95,160.64) -- (155.95,160.64)(145.95,108.64) -- (155.95,108.64)(145.95,56.64) -- (155.95,56.64) ;
		\draw   ;
		%Straight Lines [id:da508058581302737] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (152,57) -- (470.5,57) ;
		%Curve Lines [id:da24710308507007683] 
		\draw [line width=1.5]    (150.95,264.64) .. controls (314.5,269) and (337.5,60) .. (506.5,59) ;
		%Shape: Circle [id:dp13664362532619845] 
		\draw  [fill={rgb, 255:red, 184; green, 233; blue, 134 }  ,fill opacity=1 ] (265.6,264.3) .. controls (265.6,260.82) and (268.42,258) .. (271.9,258) .. controls (275.38,258) and (278.2,260.82) .. (278.2,264.3) .. controls (278.2,267.78) and (275.38,270.6) .. (271.9,270.6) .. controls (268.42,270.6) and (265.6,267.78) .. (265.6,264.3) -- cycle ;
		%Shape: Circle [id:dp1674519758090296] 
		\draw  [fill={rgb, 255:red, 184; green, 233; blue, 134 }  ,fill opacity=1 ] (257.4,57.1) .. controls (257.4,49.09) and (263.89,42.6) .. (271.9,42.6) .. controls (279.91,42.6) and (286.4,49.09) .. (286.4,57.1) .. controls (286.4,65.11) and (279.91,71.6) .. (271.9,71.6) .. controls (263.89,71.6) and (257.4,65.11) .. (257.4,57.1) -- cycle ;
		%Shape: Circle [id:dp6523709032859695] 
		\draw  [fill={rgb, 255:red, 184; green, 233; blue, 134 }  ,fill opacity=1 ] (359.6,57.3) .. controls (359.6,53.82) and (362.42,51) .. (365.9,51) .. controls (369.38,51) and (372.2,53.82) .. (372.2,57.3) .. controls (372.2,60.78) and (369.38,63.6) .. (365.9,63.6) .. controls (362.42,63.6) and (359.6,60.78) .. (359.6,57.3) -- cycle ;
		%Shape: Circle [id:dp19773341292728364] 
		\draw  [fill={rgb, 255:red, 184; green, 233; blue, 134 }  ,fill opacity=1 ] (351.4,264.1) .. controls (351.4,256.09) and (357.89,249.6) .. (365.9,249.6) .. controls (373.91,249.6) and (380.4,256.09) .. (380.4,264.1) .. controls (380.4,272.11) and (373.91,278.6) .. (365.9,278.6) .. controls (357.89,278.6) and (351.4,272.11) .. (351.4,264.1) -- cycle ;
		%Straight Lines [id:da706810387904439] 
		\draw    (365.9,59.3) -- (365.9,119) ;
		\draw [shift={(365.9,121)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(365.9,57.3)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da6629162815743224] 
		\draw    (365.9,123) -- (365.9,262.1) ;
		\draw [shift={(365.9,264.1)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(365.9,121)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		
		% Text Node
		\draw (132,270) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
		% Text Node
		\draw (110,203.4) node [anchor=north west][inner sep=0.75pt]    {$0.25$};
		% Text Node
		\draw (114,152.4) node [anchor=north west][inner sep=0.75pt]    {$0.5$};
		% Text Node
		\draw (110,100.4) node [anchor=north west][inner sep=0.75pt]    {$0.75$};
		% Text Node
		\draw (119.5,51.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (84,9) node [anchor=north west][inner sep=0.75pt]   [align=left] {Probability};
		% Text Node
		\draw (489,273) node [anchor=north west][inner sep=0.75pt]   [align=left] {Age};
		% Text Node
		\draw (331,80.4) node [anchor=north west][inner sep=0.75pt]    {$30\%$};
		% Text Node
		\draw (369,182.4) node [anchor=north west][inner sep=0.75pt]    {$70\%$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[]{Part at school according to Age}
	\end{figure}
	But how do we obtain such a graph with a dichotomic (dummy) variable ??? In fact it's relatively simple ... Imagine a sample of $100$ individuals. For these $100$ individuals assume for a given age that $70\%$ "has finished" and $30\%$ are "ongoing". Well the curve is simply the ratio of the two classes for a given age $x$. It is sometimes given the size of classes with circles over the length of the horizontal asymptotes to mean that this is a dichotomous variable.
	\end{tcolorbox}
	Points are distributed according to an $S$-curve ("\NewTerm{sigmoid}"\index{sigmoid}): there are two horizontal asymptotes as the proportion is between $0$ and $1$. We see immediately that a linear model would be manifestly inadequate (especially as the dependent variable a linear model sweeps the whole real numbers $\mathbb{R}$ and is not confined to the range $[0, 1]$).
	
	This curve evokes for some, rightly, a cumulative curve representing a distribution function (of a Normal distribution for instance, but other continuous distributions have almost the same shape). Thus, to fit a curve to this representation, we could move towards the distribution function of a Normal distribution, and instead of estimating the parameters $a$ and $b$ of the linear regression, we could estimate the parameters $\mu,\sigma$ of the Normal law (which is very similar to the logistic law as will be shown below). We then speak of a "\NewTerm{probit model}\index{probit model}" (probability-unit). We could also move towards a Student distribution. We then speak of a "\NewTerm{robit model}\index{robit model}" (robust-unit model) because the $T$-distribution has fatter tails than the Normal distribution, the model allows for occasional large errors, and as a result the estimate for $\beta$ is less affected by outliers. Robit regression is equivalent to probit regression when its degrees of freedom $k \rightarrow +\infty$, and is close to logistic regression when $k=7$.
	
	Note that various sources and softwares interpret gompit, Cloglog, loglog, nloglog in different ways. All these models are closely related to each other and one can obtain any of them by using the gompit link function. For instance, if another source names complementary loglog (Cloglog) the function we name here gompit, one can switch between the two models by reversing the $0$s and $1$s in the dependent variable (in binary dependent variable models).
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Is logit better than probit, or vice versa? Both methods will yield similar inferences (though not identical: in practice the end result of these different distributional assumptions is that coefficients differ, usually logit estimate should be divided by approx $1.6$ to match probit estimate of the same data). But if we look at marginal effects (meaning the effects on the predicted mean of the outcome holding other covariates at the mean or averaging over observed values) the logit and probit models will make essentially the same predictions.\\
	
	On the other hand, if we are not going to go about calculating the margins then logit has the obvious advantage of generating coefficients that can be transformed into the familiar odds ratio by exponentiating the coefficient. Probit and Robit coefficients are essentially uninterpretable.\\
	
	Logit – also known as logistic regression – is more popular in health sciences like epidemiology partly because coefficients can be interpreted in terms of odds ratios. Probit models can be generalized to account for non-constant error variances in more advanced econometric settings (known as heteroskedastic probit models) and hence are used in some contexts by economists and political scientists. If these more advanced applications are not of relevance, than it does not matter which method you choose to go with.
	\end{tcolorbox}	
	
	The law that will interest us, however, is the logistic law. Unlike the Normal distribution, we know how to evaluate the expression of its distribution function (cumulative probability) that is of the type (his first advantage!):
	
	for a single predictor variable $x$ where $P$ is obviously a probability between $0$ and $1$. We will see a little further the historical reason for this choice.
	
	We immediately see that this last relation being the integral of a density function (see the proof a little bit further below) it is therefore indeed a cumulative function as:
	
	If there are several predictor variables then we write:
	
	When we choose the logistic distribution function, we get the logistic regression model, or "\NewTerm{logit model}\index{logit model}" for the choice of the "\NewTerm{link function}\index{link function}" and this is its second advantage (the most important one in fact!): we can make statistics on binary variable as if we were doing a simple linear regression!
	
	Thus, to come back on our previous example, we estimate the cumulative probability for an individual of age $x$ to have finish his studies (there are several ways to write the law following the traditions and the context) with the following logistic distribution function\label{logistic distribution}\index{logistic distribution}:
	
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Again, the logit and probit mainly differ on the link function:
	\begin{itemize}
		\item In Logit: $P(Y=1|\vec{x})=\left(1+e^{-\vec{x}^T\vec{\beta}}\right)^{-1}$
	
		\item In Probit: $P(Y=1|\vec{x})=\Phi(\vec{x}^T\vec{\beta})$ (cumulative Normal probability density function)
	\end{itemize}
	In other way, logistic has slightly flatter tails. i.e the probit curve approaches the axes more quickly than the logit curve.\\
	
	Notice that in probit models latent variable is assumed to be of the form: $Y=a+bx+\varepsilon$ with $\varepsilon = \mathcal{N}(0,1)$.\\

	A question that may arise is what if $\varepsilon = \mathcal{N}(0,\sigma)$? In fact as we know by the property of the Normal distribution it is equivalent to: $Y=a+bx+\sigma\varepsilon$ with $\varepsilon = \mathcal{N}(0,1)$. Then:
	
	The only thing that change are the parameters!
	\end{tcolorbox}
	
	It therefore follows the distribution function\footnote{If the reader is looking for the second derivative and inflection point of the sigmoid function, see page \pageref{sigmoid second derivative and inflection point}}:
	
	Obviously, depending on the value of the probability we associate with the age $x$ the fact of not having finished his studies (state associated with the binary value: $0$) or have them finished (state associated with the binary value: $1$).	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	After a change of variables, we fall back on the logistic law as defined on Wikipedia:
	
	\end{tcolorbox}	
	Let us indicate that if $a$ is set as $1$, and $b$ as $0$, then we have the "\NewTerm{standard logistic law}\index{standard logistic law}" given by:
	
	We can also calculate the mean of the distribution of the function by applying what has already been study in the Statistics section but part of this integral can only be solved numerically (at least as far as we know)... if we put:
	
	as being the random variable then we can formally calculate the mean of the logistic law (the reader may have noticed that it is as if we posed as $a=1$ and $b=0$). Indeed, starting from:
	
	Therefore it comes:
	
	that after a numerical integration gives $0$. We then also get the following result:
	
	Let us calculate the integral:
	
	Thus we see that if we put:
	
	We fall back on a distribution function with the same position and dispersion parameters of a Normal distribution centered reduced variable (zero mean and unit variance).
	
	The distribution function:
	
	can also be transformed in a very well known and important form!:
	
	Therefore:
	
	In fact this is where lies historically the trick of the origin of logistic regression. We transform a variable $P$ taking values in the in the range $[0,1]$ thanks to the logarithm of the ratio $P / (1-P)$ in a variable taking its values on the set $\mathbb{R}$ and therefore it is possible to associate to it a standard linear regression. Certainly it is empirical, but the idea was pretty good!

	What some also write...:
	
	The result of the last transformation is named the "\NewTerm{logit}\index{logit}". It is equal to the logarithm of the "\NewTerm{odds}\index{odds}" (which will be discussed in more detail soon):
	
	So this is just the ratio of a likelihood of an event on the probability of the complementary event (or opposite event if you prefer).
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	So what logit and probit do, in essence, is take the linear model and feed it through a function to yield a non-linear relationship. Whereas the linear regression predictor looks like:
	
	The logit and probit predictor can be written as:
	
	Both functions will take any number and rescale it to fall between $0$ and $1$. Hence, whatever $ax+b$ equals, it can be transformed by the function to yield a predicted probability.
	\end{tcolorbox}	
	
	So when the coefficients $a$ and $b$ have been determined, the above expression is used to determine $P$ knowing $x$ easily (it comes to solve a linear equation) and vice versa! Moreover, since $x$ is a dichotomic (dummy) variable coefficients are easily interpretable.
	
	\begin{table}[H]
		\centering
		\begin{tabular}{llll}
		\rowcolor[gray]{0.75}\hline \textbf{Link type} & \textbf{CDF} & \textbf{Link function} & \textbf{Tolerance model} \\
		\hline Logit & Logistic & $\vec{x}_{i}^{T} \vec{\beta}=\ln \left(\dfrac{\pi_{i}}{1-\pi_{i}}\right)$ & $\pi_{i}=\dfrac{\exp \left(\vec{x}_{i}^{T} \vec{\beta}\right)}{1+\exp \left(\vec{x}_{i}^{T} \vec{\beta}\right)}$ \\
		Probit & Normal & $\vec{x}_{i}^{T} \vec{\beta}=\Phi^{-1}\left(\pi_{i}\right)$ & $\pi_{i}=\Phi\left(\vec{x}_{i}^{T} \vec{\beta}\right)$ \\
		Robit & Student & $\vec{x}_{i}^{T} \vec{\beta}=T^{-1}_{(\nu-1)/\nu}\left(\pi_{i}\right)$ & $\pi_{i}=T_{(\nu-1)/\nu}\left(\vec{x}_{i}^{T} \vec{\beta}\right)$ \\
		loglog (nloglog) & Gumbel & $\vec{x}_{i}^{T} \vec{\beta}=-\log(-\log(\pi_i))$ & $\pi_i=\exp \left(-\exp \left(\vec{x}_{i}^{T} \vec{\beta}\right)\right)$\\
		Cloglog (gompit) & Gumbel & $\vec{x}_{i}^{T} \vec{\beta}=\log \left(-\log \left(1-\pi_{i}\right)\right)$ & $\pi_{i}=1-\exp \left(-\exp \left(\vec{x}_{i}^{T} \vec{\beta}\right)\right)$\\
		Cauchit & Cauchy & $\vec{x}_{i}^{T} \vec{\beta}=\tan(\Phi(\pi-0.5))$ & $\pi_i=\dfrac{1}{\pi}\arctan(\vec{x}_i^T\vec{\beta})+0.5$\\
		\hline
		\end{tabular}
		\caption[Some binomial regression models and their corresponding tolerance model and link function]{Some binomial regression models and their corresponding tolerance model and link function (using most common notation in multivariate case)}
	\end{table}

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The odds is also sometimes named the "\NewTerm{rating}\index{rating}" by analogy to the rating of horses to the triple forecast. For example, if a horse has $3$ chances on $4$ to be a winner (thus verbatim $1$ on $4$ chance of being non-winner) its rating is a $3$ against $1$ ratio, that is to say an odds equal to $3$:
	$$\text{O.R.}=\dfrac{3/4}{1-3/4}=3$$
	We can also introduce the concept of "\NewTerm{odds ratio}\index{odds ratio}\label{odds ratio logistic regression}" (O.R.) for the rating ratio which is a very widely used indicator in medicine. Thus, if the occurrence of an event in a group $A$ is $p$, and $q$ in the group $B$, the odds ratio is then simply given by:
	
	The odds ratio is always by design greater than or equal to zero. If the odds ratio is close to $1$, the event is independent of the group, if it is greater than $1$, the event is more common in group $A$ than in group $B$, if it is less than $1$ the event is less frequent in group $A$ than in group $B$.\\
	
	Odds ratio or "\NewTerm{crude ratio}" are obtained when we consider the effect of only one predictor variable. However when we include more variables in the analysis we get what is named the "\NewTerm{adjusted odds ratio}".
	\end{tcolorbox}
	Let us come back over the odds because it is possible to introduce the concept of logistics function by doing the opposite approach from the one presented above (i.e. to start with the definition of odds to get to the logit) and this can sometimes be even more educational.
	
	Let us suppose we start from the size (height) of a person to predict whether this person is a man or a woman. So we can talk about probability of being a man or a woman. Suppose the probability of a man for a given height is $90\%$. So the odds of being a man is:
	
	In our example, the odds will be $0.90 / 0.10$ therefore equal to $9$. Now, the probability of being a woman will be $0.10 / 0.90$ therefore equal to $0.11$. This asymmetry of values is not talking because the odds of being a man should be the opposite of the odds of being a woman ideally. We solve precisely this asymmetry using the natural logarithm. Thus we have:
	
	and:
	
	In this way the logit (logarithm of the odds) is exactly the opposite of this of being a woman by the property of the logarithm:
	
	To introduce this tool let us suppose a bank wants to make a scoring of its debtors. As it has several subsidiaries it (the bank) built the following data tables for some of them (all subsidiaries are then not presented):
	\begin{itemize}
    	\item 1st Subsidiary:
		\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|p{2cm}|p{2cm}|p{2cm}|}
					\hline
					\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Credit Amount}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Paid}}  & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Not Paid}}\\ \hline
					\centering\arraybackslash\ $27,200$ & \centering\arraybackslash\ $1$ & \centering\arraybackslash\ $9$ \\ \hline
					\centering\arraybackslash\ $27,700$ & \centering\arraybackslash\ $7$ & \centering\arraybackslash\ $3$ \\ \hline
					\centering\arraybackslash\ $28,300$ & \centering\arraybackslash\ $13$ & \centering\arraybackslash\ $0$ \\ \hline
					\centering\arraybackslash\ $28,400$ & \centering\arraybackslash\ $7$ & \centering\arraybackslash\ $3$ \\ \hline
					\centering\arraybackslash\ $29,900$ & \centering\arraybackslash\ $10$ & \centering\arraybackslash\ $1$ \\ \hline
			\end{tabular}
		\end{center}
		\caption[]{Debtors Scoring by credit amount of subsidiary 1}
		\end{table}
		\item 2nd Subsidiary:
			\begin{table}[H]
			\begin{center}
				\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|p{2cm}|p{2cm}|p{2cm}|}
						\hline
						\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Credit Amount}} & 
		  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Paid}}  & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Not Paid}}\\ \hline
						\centering\arraybackslash\ $27,200$ & \centering\arraybackslash\ $0$ & \centering\arraybackslash\ $8$ \\ \hline
						\centering\arraybackslash\ $27,700$ & \centering\arraybackslash\ $4$ & \centering\arraybackslash\ $2$ \\ \hline
						\centering\arraybackslash\ $28,300$ & \centering\arraybackslash\ $6$ & \centering\arraybackslash\ $3$ \\ \hline
						\centering\arraybackslash\ $28,400$ & \centering\arraybackslash\ $5$ & \centering\arraybackslash\ $3$ \\ \hline
						\centering\arraybackslash\ $29,900$ & \centering\arraybackslash\ $8$ & \centering\arraybackslash\ $0$ \\ \hline
				\end{tabular}
			\end{center}
			\caption[]{Debtors Scoring by credit amount of subsidiary 2}
			\end{table}
		\item 3rd Subsidiary:
		\begin{table}[H]
			\begin{center}
				\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|p{2cm}|p{2cm}|p{2cm}|}
						\hline
						\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Credit Amount}} & 
		  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Paid}}  & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Not Paid}}\\ \hline
						\centering\arraybackslash\ $27,200$ & \centering\arraybackslash\ $1$ & \centering\arraybackslash\ $8$ \\ \hline
						\centering\arraybackslash\ $27,700$ & \centering\arraybackslash\ $6$ & \centering\arraybackslash\ $2$ \\ \hline
						\centering\arraybackslash\ $28,300$ & \centering\arraybackslash\ $7$ & \centering\arraybackslash\ $1$ \\ \hline
						\centering\arraybackslash\ $28,400$ & \centering\arraybackslash\ $7$ & \centering\arraybackslash\ $2$ \\ \hline
						\centering\arraybackslash\ $29,900$ & \centering\arraybackslash\ $9$ & \centering\arraybackslash\ $0$ \\ \hline
				\end{tabular}
			\end{center}
			\caption[]{Debtors Scoring by credit amount of subsidiary 3}
		\end{table}
	\end{itemize}
	We can see that the total proportion of good debtors in the three subsidiaries is of $91/136 \cong 0.67$.
	
	When the credit is less than $27,500$, the percentage of good debtors is of $2/27\cong 0.07$. When the amount of credit is less than $28,000$ the proportion of good debtors is of $19/51\cong 0.37$.
	
	When the amount of credit is less than $28,500$, the percentage of good debtors is of $64/108 \cong 0.59$ and for amounts below $30,000$ the proportion is of $91/936\cong 0.67$.
	
	We will put for this logistic regression that $Y=1$ is a good credit risk and that $Y=0$ is a bad risk. Next, we create the following table that is a summary of data from all subsidiaries:
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|p{2cm}|p{4cm}|}
					\hline
					\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Credit Amount}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Proportion $P$}} \\ \hline
					\centering\arraybackslash\ $27,200$ & \centering\arraybackslash\ $=2/27=0.0741$  \\ \hline
					\centering\arraybackslash\ $27,700$ & \centering\arraybackslash\ $=17/24=0.7083$ \\ \hline
					\centering\arraybackslash\ $28,300$ & \centering\arraybackslash\ $=26/30=0.8667$ \\ \hline
					\centering\arraybackslash\ $28,400$ & \centering\arraybackslash\ $=19/27=0.7037$ \\ \hline
					\centering\arraybackslash\ $29,900$ & \centering\arraybackslash\ $=27/28=0.9643$ \\ \hline
			\end{tabular}
		\end{center}
		\caption[]{Proportion of good debtors}
	\end{table}
	Which gives graphically in Kilo-dollars:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,512); %set diagram left start at 0, and has height of 512
		
		%Shape: Axis 2D [id:dp6119481859315035] 
		\draw  (146,300.37) -- (484.38,300.37)(171.38,50.28) -- (171.38,319.28) (477.38,295.37) -- (484.38,300.37) -- (477.38,305.37) (166.38,57.28) -- (171.38,50.28) -- (176.38,57.28) (193.38,295.37) -- (193.38,305.37)(215.38,295.37) -- (215.38,305.37)(237.38,295.37) -- (237.38,305.37)(259.38,295.37) -- (259.38,305.37)(281.38,295.37) -- (281.38,305.37)(303.38,295.37) -- (303.38,305.37)(325.38,295.37) -- (325.38,305.37)(347.38,295.37) -- (347.38,305.37)(369.38,295.37) -- (369.38,305.37)(391.38,295.37) -- (391.38,305.37)(413.38,295.37) -- (413.38,305.37)(435.38,295.37) -- (435.38,305.37)(457.38,295.37) -- (457.38,305.37)(166.38,278.37) -- (176.38,278.37)(166.38,256.37) -- (176.38,256.37)(166.38,234.37) -- (176.38,234.37)(166.38,212.37) -- (176.38,212.37)(166.38,190.37) -- (176.38,190.37)(166.38,168.37) -- (176.38,168.37)(166.38,146.37) -- (176.38,146.37)(166.38,124.37) -- (176.38,124.37)(166.38,102.37) -- (176.38,102.37)(166.38,80.37) -- (176.38,80.37) ;
		\draw   ;
		%Shape: Circle [id:dp7900288774621489] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (250.72,141.14) .. controls (250.72,138.85) and (252.57,137) .. (254.86,137) .. controls (257.15,137) and (259,138.85) .. (259,141.14) .. controls (259,143.43) and (257.15,145.28) .. (254.86,145.28) .. controls (252.57,145.28) and (250.72,143.43) .. (250.72,141.14) -- cycle ;
		%Shape: Circle [id:dp06513271876675808] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (198.72,285.14) .. controls (198.72,282.85) and (200.57,281) .. (202.86,281) .. controls (205.15,281) and (207,282.85) .. (207,285.14) .. controls (207,287.43) and (205.15,289.28) .. (202.86,289.28) .. controls (200.57,289.28) and (198.72,287.43) .. (198.72,285.14) -- cycle ;
		%Shape: Circle [id:dp9803489220334805] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (302.72,107.14) .. controls (302.72,104.85) and (304.57,103) .. (306.86,103) .. controls (309.15,103) and (311,104.85) .. (311,107.14) .. controls (311,109.43) and (309.15,111.28) .. (306.86,111.28) .. controls (304.57,111.28) and (302.72,109.43) .. (302.72,107.14) -- cycle ;
		%Shape: Circle [id:dp8440560685419429] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (314.72,143.14) .. controls (314.72,140.85) and (316.57,139) .. (318.86,139) .. controls (321.15,139) and (323,140.85) .. (323,143.14) .. controls (323,145.43) and (321.15,147.28) .. (318.86,147.28) .. controls (316.57,147.28) and (314.72,145.43) .. (314.72,143.14) -- cycle ;
		%Shape: Circle [id:dp011534641983247829] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (445.72,87.14) .. controls (445.72,84.85) and (447.57,83) .. (449.86,83) .. controls (452.15,83) and (454,84.85) .. (454,87.14) .. controls (454,89.43) and (452.15,91.28) .. (449.86,91.28) .. controls (447.57,91.28) and (445.72,89.43) .. (445.72,87.14) -- cycle ;
		
		% Text Node
		\draw (152,73.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$1$};
		% Text Node
		\draw (145,95.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.9$};
		% Text Node
		\draw (145,119.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.8$};
		% Text Node
		\draw (145,141.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.7$};
		% Text Node
		\draw (145,162.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.6$};
		% Text Node
		\draw (145,184.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.5$};
		% Text Node
		\draw (145,207.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.4$};
		% Text Node
		\draw (145,228.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.3$};
		% Text Node
		\draw (145,249.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.2$};
		% Text Node
		\draw (145,271.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.1$};
		% Text Node
		\draw (184,307.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$27$};
		% Text Node
		\draw (224.33,307.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$27.5$};
		% Text Node
		\draw (273.66,307.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$28$};
		% Text Node
		\draw (362.32,307.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$29$};
		% Text Node
		\draw (450,307.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$30$};
		% Text Node
		\draw (311.99,307.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$28.5$};
		% Text Node
		\draw (400.65,307.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$29.5$};
		% Text Node
		\draw (154,34.4) node [anchor=north west][inner sep=0.75pt]    {$P$};
		% Text Node
		\draw (395,328) node [anchor=north west][inner sep=0.75pt]   [align=left] {Amount (KUSD)};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[]{Cumulative percentage of good debtors based on credit}
	\end{figure}
	Once done, we use the logit transformation:
	
	Which gives:
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|p{2cm}|p{2cm}|p{2cm}|}
					\hline
					\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Credit Amount}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Proportion $P$}}  & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Logit}}\\ \hline
					\centering\arraybackslash\ $27,200$ & \centering\arraybackslash\ $0.0741$ & \centering\arraybackslash\ $	
-2.5257$ \\ \hline
					\centering\arraybackslash\ $27,700$ & \centering\arraybackslash\ $0.7083$ & \centering\arraybackslash\ $0.8873$ \\ \hline
					\centering\arraybackslash\ $28,300$ & \centering\arraybackslash\ $0.8667$ & \centering\arraybackslash\ $1.8718$ \\ \hline
					\centering\arraybackslash\ $28,400$ & \centering\arraybackslash\ $0.7037$ & \centering\arraybackslash\ $0.8650$ \\ \hline
					\centering\arraybackslash\ $29,900$ & \centering\arraybackslash\ $0.9643$ & \centering\arraybackslash\ $3.2958$ \\ \hline
			\end{tabular}
		\end{center}
		\caption[]{Proportion of good debtors and logit}
	\end{table} 
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The reader must keep in mind the social responsibilities when running such models! First their accuracy (wrong rate of classification), but also especially the confidence interval of the predicted proportion (probability). A statistician or data scientist should never communicate punctual values alone, but always with their confidence interval!\\
	
	Be careful also to have balanced (unbiased and correctly sampled) datasets, or to weight them correctly if necessary, to not have biased results especially when your classification methods or algorithms have important social impacts on people!!!
	\end{tcolorbox}
	A linear regression by least squares method gives:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%Straight Lines [id:da30562953591864916] 
		\draw [line width=1.5]    (151.38,256.28) -- (474.38,131.28) ;
		%Shape: Axis 2D [id:dp6119481859315035] 
		\draw  (146,217.32) -- (484.38,217.32)(169.38,50.28) -- (169.38,318.28) (477.38,212.32) -- (484.38,217.32) -- (477.38,222.32) (164.38,57.28) -- (169.38,50.28) -- (174.38,57.28) (191.38,212.32) -- (191.38,222.32)(213.38,212.32) -- (213.38,222.32)(235.38,212.32) -- (235.38,222.32)(257.38,212.32) -- (257.38,222.32)(279.38,212.32) -- (279.38,222.32)(301.38,212.32) -- (301.38,222.32)(323.38,212.32) -- (323.38,222.32)(345.38,212.32) -- (345.38,222.32)(367.38,212.32) -- (367.38,222.32)(389.38,212.32) -- (389.38,222.32)(411.38,212.32) -- (411.38,222.32)(433.38,212.32) -- (433.38,222.32)(455.38,212.32) -- (455.38,222.32)(164.38,195.32) -- (174.38,195.32)(164.38,173.32) -- (174.38,173.32)(164.38,151.32) -- (174.38,151.32)(164.38,129.32) -- (174.38,129.32)(164.38,107.32) -- (174.38,107.32)(164.38,85.32) -- (174.38,85.32)(164.38,239.32) -- (174.38,239.32)(164.38,261.32) -- (174.38,261.32)(164.38,283.32) -- (174.38,283.32)(164.38,305.32) -- (174.38,305.32) ;
		\draw   ;
		%Shape: Circle [id:dp7900288774621489] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (250.72,199.14) .. controls (250.72,196.85) and (252.57,195) .. (254.86,195) .. controls (257.15,195) and (259,196.85) .. (259,199.14) .. controls (259,201.43) and (257.15,203.28) .. (254.86,203.28) .. controls (252.57,203.28) and (250.72,201.43) .. (250.72,199.14) -- cycle ;
		%Shape: Circle [id:dp06513271876675808] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (198.72,272.14) .. controls (198.72,269.85) and (200.57,268) .. (202.86,268) .. controls (205.15,268) and (207,269.85) .. (207,272.14) .. controls (207,274.43) and (205.15,276.28) .. (202.86,276.28) .. controls (200.57,276.28) and (198.72,274.43) .. (198.72,272.14) -- cycle ;
		%Shape: Circle [id:dp9803489220334805] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (301.72,179.14) .. controls (301.72,176.85) and (303.57,175) .. (305.86,175) .. controls (308.15,175) and (310,176.85) .. (310,179.14) .. controls (310,181.43) and (308.15,183.28) .. (305.86,183.28) .. controls (303.57,183.28) and (301.72,181.43) .. (301.72,179.14) -- cycle ;
		%Shape: Circle [id:dp8440560685419429] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (312.72,201.14) .. controls (312.72,198.85) and (314.57,197) .. (316.86,197) .. controls (319.15,197) and (321,198.85) .. (321,201.14) .. controls (321,203.43) and (319.15,205.28) .. (316.86,205.28) .. controls (314.57,205.28) and (312.72,203.43) .. (312.72,201.14) -- cycle ;
		%Shape: Circle [id:dp011534641983247829] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (445.72,146.14) .. controls (445.72,143.85) and (447.57,142) .. (449.86,142) .. controls (452.15,142) and (454,143.85) .. (454,146.14) .. controls (454,148.43) and (452.15,150.28) .. (449.86,150.28) .. controls (447.57,150.28) and (445.72,148.43) .. (445.72,146.14) -- cycle ;
		
		% Text Node
		\draw (150,102.2) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$5$};
		% Text Node
		\draw (150,124) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$4$};
		% Text Node
		\draw (150,145.8) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$3$};
		% Text Node
		\draw (150,167.6) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$2$};
		% Text Node
		\draw (150,189.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$1$};
		% Text Node
		\draw (144,231.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$-1$};
		% Text Node
		\draw (144,254.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$-2$};
		% Text Node
		\draw (144,275.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$-3$};
		% Text Node
		\draw (144,296.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$-4$};
		% Text Node
		\draw (184,225.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$27$};
		% Text Node
		\draw (224.33,225.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$27.5$};
		% Text Node
		\draw (273.66,225.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$28$};
		% Text Node
		\draw (362.32,225.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$29$};
		% Text Node
		\draw (450,225.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$30$};
		% Text Node
		\draw (311.99,225.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$28.5$};
		% Text Node
		\draw (400.65,225.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$29.5$};
		% Text Node
		\draw (429,239) node [anchor=north west][inner sep=0.75pt]   [align=left] {Amount (KUSD)};
		% Text Node
		\draw (154,28) node [anchor=north west][inner sep=0.75pt]   [align=left] {Logit};
		% Text Node
		\draw (150,80.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$6$};
		\end{tikzpicture}
		\caption[]{Logit of good debtors based on the credit amount}
	\end{figure}
	with for equation (where $x$ is in USD and not in KUSD):
	
	The logistic function with its representation comes then immediately (the $x$ units are in thousands of francs)
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}
		    \begin{axis}%
		    [
		        %grid=major,     
		        axis x line=bottom,
		        ytick={0,0.2,...,1},
		        ymax=1.1,
		        xlabel={Amount (KUSD)},
		        ylabel={$P$}
		       % axis y line=middle,
		    ]
		        \addplot%
		        [
		            blue,%
		            mark=none,
		            samples=200,
		            domain=24:31,
		        ]
		        %Equation in KUSD
		        (x,{1/(1+exp(-(-51.1116+1.8371*x)))});
		    \end{axis}
		\end{tikzpicture}
	\end{figure}
	Thus, it is possible to say in this example, what is the proportion $P$ of good or bad debtors according to the credit value $X$ smaller than or equal to a certain given value. Since $0$ is a bad credit risk, we see that the bigger is the credit amount, the lower is the risk (in this hypothetical case...). Moreover, with software like Minitab (see the corresponding companion book), the difference between the quick calculations carried out by hand and those made with the binary logistic regression tool  of the software is in the order of $10\%$ (because of course ... Minitab uses the concept of maximum likelihood estimators seen in the Statistics section to determine the coefficients and the constant).
	
	A software like Minitab gives automatically an interesting output which is the "\NewTerm{confusion matrix $\mathcal{C}$}\index{confusion matrix}". It compares the model to reality with a traditional cutoff set at $50\%$ (obviously if the model perfectly match to the sample, the following matrix is a diagonal one):
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/confusion_matrix.jpg}
		\caption[Confusion matrix Minitab 15.0 output example]{Confusion matrix Minitab 15.0 output example (screenshot)}
	\end{figure}
	where the good debtors have the value $1$ (no credit risk) and the bad one the value $0$ (credit risk). We will detail further below how to get this matrix with a spreadsheet software. Finally let us indicate that in many Data Mining softwares, it is customary to define the "\NewTerm{score}\index{score}" of the model as (in the specific case of one unique explanatory variable but can easily be generalized to many):
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In some statistical softwares, when you run a logistic regression, you can get sometimes a message of the type: «\textit{complete separation of data points}». Indeed, take a look at the following data set consisting of the response variable, $Y$, and one predictor variable, $X$:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
		\hline
		\cellcolor[HTML]{C0C0C0} $\mathbf{X}$ & $1$ & $2$ & $3$ & $4$ & $4$ & $5$ & $5$ & $6$ \\ \hline
		\cellcolor[HTML]{C0C0C0} $\mathbf{Y}$ & $0$ & $0$ & $0$ & $0$ & $0$ & $1$ & $1$ & $1$ \\ \hline
		\end{tabular}
	\end{table}
	Notice the key pattern... This data set can be simply described as follows: if $X\leq 4$, then $Y=0$ without fail. Similarly, if $X>4$, then $Y=1$, again without fail. This is hat is knows as "separation". This perfect prediction of the response is what causes the estimates, and thus the model, to fail.
	\end{tcolorbox}
	Following the request of a reader, here is the maximum likelihood approach of the logistic regression.

	Let us consider the notation:
	
	Assuming that the observations are independent of one another, we can define $L(D,\vec{\beta})$, the likelihood of the data $D$ with respect to the logistic model parameter by $\vec{\beta}$. It is the product of the probabilities according to this model that each individual in $D$ belongs to the observed class with $y_i=1$ or $y_i=0$:
	
	It is therefore necessary to determine the vector of parameters $\hat{\vec{\beta}}$ maximizing the likelihood of the data $D$:
	
	To identify $\hat{\vec{\beta}}$, the first thing to do is to derive the likelihood with respect to each component of $\vec{\beta}$, $\dfrac{\partial }{\partial \beta_j}L(D,\vec{\beta})$, then solve:
	
	since $L(D,\vec{\beta})$ is convex in $\vec{\beta}$ and $\dfrac{\partial }{\partial \beta_j}L(D,\vec{\beta})$ vanishes when the maximum is reached.
	
	However, the logarithm is a strictly increasing function, maximizing the likelihood is equivalent to maximizing the logarithm of the likelihood as we already know it, whose expression is easier to take the derivative (the logarithm transforming the products into sums and the exponents into factors):
	 
	Notice that some statistical softwares return the "\NewTerm{deviance of the proposed model}\index{deviance}" defined as:
	 
	The deviance is always larger or equal than zero, being zero only if the fit is perfect.
	
	A benchmark for evaluating the magnitude of the deviance is the "\NewTerm{deviance of the null model}\index{deviance of the null model}":
	 
	which is the deviance of the worst model, the one fitted without any predictor (i.e. only with the intercept)! The null deviance serves for comparing how much the model has improved by adding the predictors.
	
	Here is a summary of what you can see in some statistical software (be careful! the concept of "null deviance" must not be confused with the concept of "deviance of the null"!):
	
	where:
	\begin{itemize}
		\item The "saturated model" is a model that assumes each data point has its own parameters (which means we have $n$ parameters to estimate).
	
		\item The "null model" assumes the exact opposite, in that it assumes one parameter for all of the data points, which means we have only to estimate $1$ parameter.
	
		\item The "proposed model" assumes we can explain our data points with $p$ parameters + an intercept term, so we have $p+1$ parameters.	
	\end{itemize}
	Having defined these two deviances we can if we wish build a likelihood-ratio test from them (\SeeChapter{see section Statistics page \pageref{likelihood ratio tests}}). That test is sometimes named the "\NewTerm{deviance test}\index{deviance test}".
	
	Let us now just prove that the log-likelihood of the saturated model is always zero. As the saturated model is the model that \underline{perfectly} fits the observed response.
	
	Let us remind that the likelihood as already seen above is given by:
	
	Therefore as the model is perfect:
	
	and clearly the logarithm of this is $0$.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Notice that we define the "\NewTerm{cross-entropy}\index{cross-entropy}\label{cross-entropy}":
	
	where $p$ and $q$ denote a "true" and an "empirical/estimated" distribution, respectively. Both are discrete distributions, hence we can sum over their individual components, denoted with $i$.  Above, when we refer to a "distribution", it means with respect to a single training data point, and not the "distribution of training data points". That's a different concept.
	\end{tcolorbox}
	So we have now therefore to determine the vector $\hat{\vec{\beta}}$ maximizing the log-likelihood of the data $D$:
	
	We shall therefore have to express $\dfrac{\partial }{\partial \beta_j}\ln(L(D,\vec{\beta}))$. Before that, we can agree a slightly different notation, in order to improve the readability of the formulas. We consider that each vector $\vec{x}\in D$ has an additional component, $x_0$, always equal to $1$ (same technique as the one used to generalize the univariate regression). Thus we can rewrite $P$, so that the parameter of the sigmoid function is the dot product of the vectors $\vec{\beta}$ and $\vec{x}$ (both of dimension $d + 1$):
	
	Where $\vec{\beta}\circ\vec{x}$ is commonly known as the "\NewTerm{prognostic index}\index{prognostic index}" (PI). The expression to be derived is thus:
	
	First we compute the first term:
	
	Now we compute the second term:
	
	So finally:
	
	As far as we know... we can not solve this analytically:
	
	Therefore we will have to approach the solution by a numerical method anyway... (gradient descent again)!
	
	\subparagraph{Binomial Logistic regression relative variable importance}\label{variable importance BLM}\mbox{}\\\\
	We have seen for the Gaussian multivariate linear model how to calculate variable importance earlier above (see page \pageref{variable importance GML}). Now for logistic regression we use rather "\NewTerm{relative variable importance}". 
	
	To introduce that latter let us first calculate the partial derivative of the logistic regression relatively to one predictor (we used the quotient rule proved in the section of Differential and Integral Calculus page \pageref{quotient rule}):
	
	So this result measures the effect of $x_i$ on $P(Y)$. This effect is a function of $x_i$. However, the relative importance of two predictors is:
	
	which is independent of $\vec{x}$. Thus, provided we have standardized all predictors, we can look at the estimates of the model coefficients as indicators of the relative importance of the predictors\index{variable importance} for what it concerns the variation of the output.
	
	\subparagraph{Binomial logistic regression residuals}\label{binomial logistic regression residuals}\mbox{}\\\\
	Residual analysis for logistic regression is more difficult than for linear regression models because the responses $y_i$ take only the values $0$ and $1$. Consequently the $i$th ordinary residual between the real value $y_i$ and the model fitted value, that we will denote now by $\hat{\pi}$, is given obviously by:
	
	And obviously we have that (as $y_i$ take only the values $0$ and $1$):
	
	The ordinary residuals will not be normally distributed and, indeed, their distribution under the assumption that the fitted model is correct is unknown. Plots of ordinary residuals against fitted values or predictor variables will generally be uninformative.
	
	The ordinary residuals can be made more comparable by dividing them by the estimated standard deviation of $y_{i},$ namely for the binomial distribution (as we have proved it during our study of the binomial distribution):  
		
	The resulting "\NewTerm{Pearson residuals}\index{Pearson residuals}" are given by:
	
	If we square:
	
	Now notice that this can be written:
	
	So we recognize here that all the terms are of the form:
	
	Therefore the same applies to the sum of square of the Pearson residuals and we can write the "\NewTerm{Pearson chi-square statistic}\index{Pearson chi-square statistic}":
	
	Hence, we see that the sum of the squares of the Pearson residuals is numerically equal to the Pearson chi-square test statistic. Therefore the square of each Pearson residual measures the contribution of each binary response to the Pearson chi-square test statistic and can be used as an adequation test for our model.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Notice that some practitioners run not only a chi-square test on residuals but also on the resulting contingency classification table of all discrete $x$ values of the binomial regression. For our example above the corresponding contingency table being:
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|p{2cm}|p{2cm}|p{2cm}|}
			\hline
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Credit Amount}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Expected}}  & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Observed}}\\ \hline
			\centering\arraybackslash\ $27,200$ & \centering\arraybackslash\ $0$ & \centering\arraybackslash\ $2$ \\ \hline
			\centering\arraybackslash\ $27,700$ & \centering\arraybackslash\ $0$ & \centering\arraybackslash\ $17$ \\ \hline
			\centering\arraybackslash\ $28,300$ & \centering\arraybackslash\ $30$ & \centering\arraybackslash\ $26$ \\ \hline
			\centering\arraybackslash\ $28,400$ & \centering\arraybackslash\ $27$ & \centering\arraybackslash\ $10$ \\ \hline
			\centering\arraybackslash\ $29,900$ & \centering\arraybackslash\ $28$ & \centering\arraybackslash\ $27$ \\ \hline
		\end{tabular}
	\end{table}
	Unfortunately, it is common that there are not enough observations (or even worse...: zero observations) for each possible combinations of values of the $x$ variables, so the Pearson chi-squared statistic cannot be readily calculated (see a practical example in our \texttt{R} companion book). A solution to this problem is the Hosmer-Lemeshow statistic (see below). The key concept of the Hosmer-Lemeshow statistic is that, instead of observations being grouped by the values of the $x$ variable(s), the observations are grouped by expected probability. That is, observations with similar expected probability are put into the same group, usually to create approximately $10$ groups.
	\end{tcolorbox}
	
	\subparagraph{Hosmer–Lemeshow test}\label{Hosmer–Lemeshow test}\index{statistical tests!Hosmer–Lemeshow test}\mbox{}\\\\
	The Hosmer–Lemeshow test is a statistical test for goodness of fit for binary logistic regression models (however a generalized Hosmer–Lemeshow goodness-of-fit test for multinomial logistic regression models exists!). It is used frequently in risk prediction models. The test assesses whether or not the observed event rates match expected event rates in subgroups of the model population. The Hosmer–Lemeshow test specifically identifies subgroups as the deciles of fitted risk values. Models for which expected and observed event rates in subgroups are similar are said to be "well calibrated".
	
	The Hosmer-Lemeshow test statistic is given by:
	
	Here $O_{1 g}, E_{1 g}, O_{0 g}, E_{o g}, N_{g}$, and $\pi_{g}$ denote the observed $Y=1$ events, expected $Y=1$ events, observed $Y=0$ events, expected $Y=0$ events, total observations, predicted risk for the $g^{\text {th }}$ risk decile group, and $G$ is the number of groups. The test statistic asymptotically follows a $\chi^{2}$ distribution with $G-2$ degrees of freedom (see remark further below!!!). Hence:
	
	The number of risk groups may be adjusted depending on how many fitted risks are determined by the model. This helps to avoid singular decile groups.
	
	Another very common notation that we can find in textbooks is, given $m_j$ the number of observations:
	
	where:
	
	and $c_k$ is the number of covariate patterns in the $k$th group. With a bit algebra we have therefore:
	
	where $\bar{\pi}_k$ is the average estimated probability in the $k$th group:
	
	
	The Hosmer–Lemeshow test seems to have some limitations. Frank Harrell describes several in an answer on the forum \textit{Cross Validated}:
	\begin{itemize}
		\item The Hosmer-Lemeshow test is for overall calibration error, not for any particular lack of fit such as quadratic effects. It does not properly take overfitting into account, is arbitrary to choice of bins and method of computing quantiles, and often has power that is too low.
		
		\item For these reasons the Hosmer-Lemeshow test is no longer recommended. Hosmer et al. have a better one df omnibus test of fit, implemented in the \texttt{R} software rms package residuals.lrm function.
		
		\item Other alternatives have been developed to address the limitations of the Hosmer-Lemeshow test. These include the Osius-Rojek test and the Stukel test.
	\end{itemize}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In the 11982 (holocene calendar) paper by Lemeshow and Hosmer \textit{A review of goodness of fit statistics for use in the development of logistic regression models}, American Journal of Epidemiology 115:92-106. In this they write (page 96) in reference to their statistic that:
	
	\begin{center}
	\og \textit{The theoretical development given by Hosmer and Lemeshow (1980) requires only that $G>(p+1)$.} \fg{}
	\end{center}
	
	where $p$ is the number of parameters in the model and following this that:
	
	\begin{center}
	« \textit{Hosmer and Lemeshow (1980) have shown via computer simulations that if the number of covariates plus one is less than the number of groups (i.e. $p+1 < G$), then the statistic $C^*_G$ has a distribution which is closely approximated by a chi-square distribution with $G-2$ degrees of freedom when $H_0$ is true.} »
	\end{center}
	
	To be more precise, in Hosmer and Lemeshow's 11980 (holocene calendar) paper, Theorem 2 states that the asymptotic distribution of $\hat{C}_{g}^{*}$ (the usual Hosmer-Lemeshow test statistic) is:
	$$
	\chi_{2 g-g-(p+1)}^{2}+\sum_{i=1}^{p+1} \lambda_{i} \chi_{i}^{2}(1)
	$$
	where the $\lambda_{i}$ are eigenvalues of a matrix (specified in the paper, not relevant to this question). Then, they show through simulations that $\sum_{i=1}^{p+1} \lambda_{i} \chi_{i}^{2}(1)$ is approximately $\chi_{p-1}^{2}$, which leads to the usual $g-2$ degrees of freedom in the Hosmer-Lemeshow test.\\
	
	This means that given our fitted model, the p-value can be calculated as the right hand tail probability of the corresponding chi-squared distribution using the calculated test statistic. If the $p$-value is small, this is indicative of poor fit.
	\end{tcolorbox}
	It should be emphasized, same as any other null hypothesis statistical test, that a large $p$-value does not mean the model fits well, since lack of evidence against a null hypothesis is not equivalent to evidence in favour of the alternative hypothesis. In particular, if our sample size is small, a high $p$-value from the test may simply be a consequence of the test having lower power to detect misspecification, rather than being indicative of good fit.
	
	The reader can refer to our \texttt{R} companion book where we provide simulation evidence that the above claim is quite accurate!
	
	\subparagraph{Logistic Kolmogorov-Smirnov statistic}\label{logistic Kolmogorov-Smirnov statistic}\index{logistic Kolmogorov-Smirnov statistic}\mbox{}\\\\
	The "\NewTerm{KS Statistic}", also named "\NewTerm{maximum  Kolmogorov-Smirnov  vertical  difference}\index{maximum  Kolmogorov-Smirnov  vertical  difference}" or "\NewTerm{Kolmogorov-Smirnov statistic}\index{Kolmogorov-Smirnov statistic}", is the maximum difference between the cumulative true positive and cumulative false positive rate. It is often used as the deciding metric to judge the efficacy of models in credit scoring. The higher the KS stat, the more efficient is the model at capturing the responders (Ones).
	
	In other words:
	
	More technically and more precise, let us remind, the null hypothesis for a logistic regression of a binary/dichotomous random variable $Y_{k}$ on real numbers $x_{j, k}$ (with $k=1,2, \ldots, n$, and $j=1,2, \ldots, m \ll n$ ), given observations $y_{1}, y_{2}, \ldots, y_{n}$ of $Y_{1}, Y_{2}, \ldots, Y_{n}$, respectively, is $\text{H}_{0}: y_{1}, y_{2}, \ldots, y_{n}$ are draws from independent Bernoulli distributions with means $\hat{\mu}_{1}, \hat{\mu}_{2}, \ldots, \hat{\mu}_{n}$, respectively:
	
	for $k=1,2, \ldots, n$, where the vector $\beta$ is a nuisance parameter, $\hat{\beta}$ is its maximum-likelihood estimate, $l$ is a nonnegative integer no greater than $m$, and:
	
	or something similarly aggregative). A cumulative measure of the distance between the observed data and the model assumed under the null hypothesis is the discrete KolmogorovSmirnov statistic (recommended by Horn (1977) and many others), namely:
	
	where the ordering $\sigma$ is a permutation of the integers $1,2, \ldots, n$, and the residual $r_{k}$ is:
	
	for $k=1,2, \ldots, n$.
	
	Let us see a small example:
	\begin{table}[H]
		\resizebox{\textwidth}{!}{\begin{tabular}{|c|c|c|r|r|r|r|r|}
		\hline
		\rowcolor[HTML]{C0C0C0} 
		\textbf{Decile} & \textbf{Non Event} & \textbf{Event} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{\% Non Event}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{\% Event}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{\begin{tabular}[c]{@{}c@{}}Cum \\ \%Event\end{tabular}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{\begin{tabular}[c]{@{}c@{}}Cum \\ \% Non-Event\end{tabular}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{KS}} \\ \hline
		1 & 51 & 49 & $5.7 \%$ & $48.5 \%$ & $48.5 \%$ & $5.7 \%$ & $42.8 \%$ \\ \hline
		2 & 81 & 19 & $9.0 \%$ & $18.8 \%$ & $67.3 \%$ & $14.7 \%$ & $52.6 \%$ \\ \hline
		3 & 84 & 16 & $9.3 \%$ & $15.8 \%$ & $83.2 \%$ & $24.0 \%$ & \cellcolor[HTML]{F8FF00}$59.1 \%$ \\ \hline
		4 & 90 & 10 & $10.0 \%$ & $9.9 \%$ & $93.1 \%$ & $34.0 \%$ & $59.0 \%$ \\ \hline
		5 & 95 & 5 & $10.6 \%$ & $5.0 \%$ & $98.0 \%$ & $44.6 \%$ & $53.4 \%$ \\ \hline
		6 & 99 & 1 & $11.0 \%$ & $1.0 \%$ & $99.0 \%$ & $55.6 \%$ & $43.4 \%$ \\ \hline
		7 & 99 & 1 & $11.0 \%$ & $1.0 \%$ & $100.0 \%$ & $66.6 \%$ & $33.4 \%$ \\ \hline
		8 & 100 & 0 & $11.1 \%$ & $0.0 \%$ & $100.0 \%$ & $77.8 \%$ & $22.2 \%$ \\ \hline
		9 & 100 & 0 & $11.1 \%$ & $0.0 \%$ & $100.0 \%$ & $88.9 \%$ & $11.1 \%$ \\ \hline
		10 & 100 & 0 & $11.1 \%$ & $0.0 \%$ & $100.0 \%$ & $100.0 \%$ & $0.0 \%$ \\ \hline
		\end{tabular}}
	\end{table}
	 In this case, KS is maximum at third decile and KS score is $59.1$. Ideally, it should be in first three deciles and score lies between $40$ and $70$. And there should not be more than $10$ points (in absolute) difference between training and validation KS score. Score above $70$ is susceptible and might be overfitting so rigorous validation is required.
	 
	 \begin{tcolorbox}[enhanced,title=Remark,colframe=black,arc=10pt,drop lifted shadow,after skip=15pt plus 2pt]
	The tests of Hosmer and Lemeshow (11980, 12000 according to holocene calendar) replace the Kolmogorov-Smirnov statistics KS defined earlier above with $\chi^2$ for the model of binomial distributions corresponding to quantiles (typically the deciles) for the estimated means of the postulated Bernoulli distributions. It has also the advantage of being a statistical test rather than a simple statistical estimator.
	\end{tcolorbox}
	
	\paragraph{Multinomial Logistic regression}\label{multinomial logistic regression}\mbox{}\\\\
	"\NewTerm{Multinomial logistic regression}\index{multinomial logistic regression}\label{multinomial logistic regression}" is a classification method that generalizes logistic regression to multiclass problems, i.e. with more than two possible discrete outcomes. That is, it is a model that is used to predict the probabilities of the different possible outcomes of a categorically distributed dependent variable, given a set of independent variables (which may be real-valued, binary-valued, categorical-valued, etc.).

	Multinomial logistic regression is known by a variety of other names, "\NewTerm{including polytomous LR}\index{including polytomous LR}", "\NewTerm{multiclass LR}\index{multiclass LR}", "\NewTerm{softmax regression}\index{softmax regression}", "\NewTerm{multinomial logit}\index{multinomial logit}", the "\NewTerm{maximum entropy (MaxEnt) classifier}\index{maximum entropy (MaxEnt) classifier}", and the "\NewTerm{conditional maximum entropy model}\index{conditional maximum entropy model}".
	
	The basic setup is the same as in logistic regression, the only difference being that the dependent variables are categorical rather than binary, i.e. there are $K$ possible outcomes rather than just two. The following description is somewhat shortened.
	
	As in other forms of linear regression, multinomial logistic regression uses a linear predictor function $f(k,i)$ to predict the probability that observation $i$ has outcome $k$, of the following form:
	
	where $\beta _{m,k}$ is a regression coefficient associated with the $m$th explanatory variable and the $k$th outcome. As it is common, the regression coefficients and explanatory variables are normally grouped into vectors of size $M+1$, so that the predictor function can be written more compactly:
	
	where $\vec{\beta}_k$ is the set of regression coefficients associated with outcome $k$, and $\vec{x}_i$ (a row vector) is the set of explanatory variables associated with observation $i$.
	
	One fairly simple way to arrive at the multinomial logit model is to imagine, for $K$ possible outcomes, running $K-1$ independent binary logistic regression models, in which one outcome is chosen as a "pivot" and then the other $K-1$ outcomes are separately regressed against the pivot outcome. This would proceed as follows, if outcome $K$ (the last outcome) is chosen as the pivot:
	
	Note that we have introduced separate sets of regression coefficients, one for each possible outcome.
	
	That latter relation is also often denoted:
	
	for each $j=1,\ldots,K-1$.

	If we exponentiate both sides, and solve for the probabilities, we get:
	
	Using the fact that all $K$ of the probabilities must sum to one, we find:
	
	That latter relation is also often denoted (notation that we will use further below):
	
	Re-injecting in the previous relations sets, we get:
	
	Hence:
	
	Often denoted (as the sum in the denominator is a constant\footnote{However, it is definitely not constant with respect to the explanatory variables, or crucially, with respect to the unknown regression coefficients $\vec{\beta}_k$, which we will need to determine through some sort of optimization procedure.}):
	
	The fact that we run multiple regressions reveals why the model relies on the assumption of independence of irrelevant alternatives!
	
	Note that the prior-previous relation is also denoted:
	
	So we have:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Note that when $K=2$ the multinomial and logistic regression models become one and the same. Also for information, the softmax function is used in multi-class classification neural networks as activation function as we will see further below during our study of neural networks (page \pageref{neural network}).
	\end{tcolorbox}
	
	The following function:
	
	is referred to as the "\NewTerm{softmax function}\index{softmax function}\label{softmax function}". The reason is that the effect of exponentiating the values $x_{1},\ldots ,x_{n}$ is to exaggerate the differences between them. As a result $\text{softmax}(k,x_{1},\ldots ,x_{n})$ will return a value close to $0$ whenever $x_{k}$ is significantly less than the maximum of all the values, and will return a value close to $1$ when applied to the maximum value, unless it is extremely close to the next-largest value.
	
	When using multinomial logistic regression, one category of the dependent variable is chosen as the reference category. Separate odds ratios are determined for all independent variables for each category of the dependent variable with the exception of the reference category, which is omitted from the analysis. The exponential beta coefficient represents the change in the odds of the dependent variable being in a particular category vis-a-vis the reference category, associated with a one unit change of the corresponding independent variable.
	
	Following the request of a reader, here is the maximum likelihood approach of the logistic regression.

	Let us recall that the multinomial distribution is given (\SeeChapter{see section Statistics page \pageref{multinomial distribution}}) by:
	
	In other words if there are $m$ random variables, i.e. $X_i$, $i\in[1,n]$, where $X_i$ represents the number of occurrences of item $i$ in a choice of $n$ items, with entry $i$ in the vector of probabilities $\vec{P}$, $P_i$ giving the probability of drawing item $i$. The probability of selecting $k_1$ of item $1$... $k_m$ of item $m$ is then given by $\mathcal{M}$ above! 
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Remember that if $m=2$  then the above relation becomes:
	
	But as $k_1+k_2=n$ then:
	
	and as $p_1+p_2=1$ we finally have:
	
	We recognize here the binomial distribution!
	\end{tcolorbox}
	Let us denote the joint probability density (likelihood) function for the multinomial logistic function as:
	
	Since we want to maximize the above relation with respect to $\vec{\beta}$, the factorial terms that do not contain any of the $\pi_{ij}$ terms can be treated as constants. Thus, the kernel of the log likelihood function for multinomial logistic regression models is:
	
	Replacing the $J$th terms, the previous relations becomes:
	
	Since $a^{x+y}=a^xa^y$, the sum in the exponent in the denominator of the last term becomes a product over the first $J-1$ terms of $j$. Continue by grouping together the terms that are raised to the $y_{ij}$ power for each $j$ up to $J-1$:
	
	Now we substitute for $\pi_{ij}$ and $\pi_{iJ}$ the both relations that we have derived earlier:
	
	This gives us then:
	
	Taking the logarithm of the above relation gives us the log-likelihood function for the multinomial logistic regression model:
	
	
	\pagebreak
	\subparagraph{$F_1$ score}\label{F1 score}\mbox{}\\\\
	Now that we have introduced a well known multiclass classifier, it is time to introduce the "\NewTerm{$F_1$ score}\index{$F_1$ score}" (also named "\NewTerm{$F$-score}" or "\NewTerm{$F$-measure}") which is a measure of a classifier quality (among many others empirical quality indicators...). It considers both the precision $P$ and the recall $R$ of the test to compute the score. To introduce these two indicators, let us consider for companion example, the following generic three-class ($n=3$) confusion matrix $\mathcal{C}$ for a total of $N$ records to classify:
	\begin{table}[H]
		\centering
		\begin{tabular}{|
		>{\columncolor[gray]{0.75}}l |
		>{\columncolor[HTML]{EFEFEF}}c |c|c|c|}
		\hline
		 & \multicolumn{4}{c|}{\cellcolor[gray]{0.75}\textbf{Predicted}} \\ \hline
		\cellcolor[gray]{0.75} &  & \cellcolor[HTML]{EFEFEF}\textbf{Class $1$} & \cellcolor[HTML]{EFEFEF}\textbf{Class $2$} & \cellcolor[HTML]{EFEFEF}\textbf{Class $3$} \\ \cline{2-5} 
		\cellcolor[gray]{0.75} & \textbf{Class $1$} & $\mathcal{C}_{11}$ & $\mathcal{C}_{12}$ & $\mathcal{C}_{13}$ \\ \cline{2-5} 
		\cellcolor[gray]{0.75} & \textbf{Class $2$} & $\mathcal{C}_{21}$ & $\mathcal{C}_{22}$ & $\mathcal{C}_{23}$ \\ \cline{2-5} 
		\multirow{-4}{*}{\cellcolor[gray]{0.75}\textbf{Actual}} & \textbf{Class $3$} & $\mathcal{C}_{31}$ & $\mathcal{C}_{32}$ & $\mathcal{C}_{33}$ \\ \hline
		\end{tabular}
		\caption[]{A typical three class confusion matrix}
	\end{table}	
	Let us recall first that the "\NewTerm{accuracy}\index{accuracy}" is given by:
	
	The "\NewTerm{precision}\index{precision}" $P_i$ for a multiclass classifier (of $n$ classes) is defined as the fraction of correct predictions for a certain class\footnote{For a practical example, see our \texttt{R} companion book on the $F$-score.}!:
		
	and the "\NewTerm{recall}\index{recall}" $R_i$ for a multiclass classifier (of $n$ classes) is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive):
	
	This can nicely be illustrated as following:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1215); %set diagram left start at 0, and has height of 1215
		
		%Shape: Rectangle [id:dp7246562280238784] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 203; green, 224; blue, 180 }  ,fill opacity=1 ] (589.43,265) -- (615.43,265) -- (615.43,312) -- (589.43,312) -- cycle ;
		%Shape: Rectangle [id:dp7335265701903284] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 203; green, 224; blue, 180 }  ,fill opacity=1 ] (57,105) -- (188.5,105) -- (188.5,411) -- (57,411) -- cycle ;
		%Shape: Rectangle [id:dp9516021254111056] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 218; green, 218; blue, 218 }  ,fill opacity=1 ] (188.5,105) -- (319,105) -- (319,411) -- (188.5,411) -- cycle ;
		%Shape: Chord [id:dp16300800817400352] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 184; green, 233; blue, 134 }  ,fill opacity=1 ] (186.68,345.98) .. controls (186.27,345.99) and (185.86,345.99) .. (185.44,345.99) .. controls (134.22,345.99) and (92.69,303.46) .. (92.69,250.99) .. controls (92.69,198.52) and (134.22,155.99) .. (185.44,155.99) .. controls (185.9,155.99) and (186.35,155.99) .. (186.8,156) -- cycle ;
		%Shape: Chord [id:dp13674717724326824] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 250; green, 176; blue, 147 }  ,fill opacity=1 ] (187,156) .. controls (187.41,155.99) and (187.83,155.99) .. (188.24,155.99) .. controls (240.05,156.05) and (282,198.62) .. (281.95,251.09) .. controls (281.89,303.56) and (239.85,346.05) .. (188.04,345.99) .. controls (187.59,345.99) and (187.14,345.99) .. (186.68,345.98) -- cycle ;
		%Shape: Circle [id:dp5397895301904114] 
		\draw  [line width=1.5]  (91,252) .. controls (91,198.98) and (133.98,156) .. (187,156) .. controls (240.02,156) and (283,198.98) .. (283,252) .. controls (283,305.02) and (240.02,348) .. (187,348) .. controls (133.98,348) and (91,305.02) .. (91,252) -- cycle ;
		%Shape: Circle [id:dp08026727642350284] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=1.5]  (77,140) .. controls (77,136.13) and (80.13,133) .. (84,133) .. controls (87.87,133) and (91,136.13) .. (91,140) .. controls (91,143.87) and (87.87,147) .. (84,147) .. controls (80.13,147) and (77,143.87) .. (77,140) -- cycle ;
		%Shape: Circle [id:dp3707625746613781] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]  (197,194) .. controls (197,190.13) and (200.13,187) .. (204,187) .. controls (207.87,187) and (211,190.13) .. (211,194) .. controls (211,197.87) and (207.87,201) .. (204,201) .. controls (200.13,201) and (197,197.87) .. (197,194) -- cycle ;
		%Shape: Circle [id:dp10252317244126297] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]  (238,284) .. controls (238,280.13) and (241.13,277) .. (245,277) .. controls (248.87,277) and (252,280.13) .. (252,284) .. controls (252,287.87) and (248.87,291) .. (245,291) .. controls (241.13,291) and (238,287.87) .. (238,284) -- cycle ;
		%Shape: Circle [id:dp7753234255295804] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]  (203,315) .. controls (203,311.13) and (206.13,308) .. (210,308) .. controls (213.87,308) and (217,311.13) .. (217,315) .. controls (217,318.87) and (213.87,322) .. (210,322) .. controls (206.13,322) and (203,318.87) .. (203,315) -- cycle ;
		%Shape: Circle [id:dp044043989349809465] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]  (214,131) .. controls (214,127.13) and (217.13,124) .. (221,124) .. controls (224.87,124) and (228,127.13) .. (228,131) .. controls (228,134.87) and (224.87,138) .. (221,138) .. controls (217.13,138) and (214,134.87) .. (214,131) -- cycle ;
		%Shape: Circle [id:dp9705404706928706] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]  (279,146) .. controls (279,142.13) and (282.13,139) .. (286,139) .. controls (289.87,139) and (293,142.13) .. (293,146) .. controls (293,149.87) and (289.87,153) .. (286,153) .. controls (282.13,153) and (279,149.87) .. (279,146) -- cycle ;
		%Shape: Circle [id:dp0670493070343432] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]  (291,209) .. controls (291,205.13) and (294.13,202) .. (298,202) .. controls (301.87,202) and (305,205.13) .. (305,209) .. controls (305,212.87) and (301.87,216) .. (298,216) .. controls (294.13,216) and (291,212.87) .. (291,209) -- cycle ;
		%Shape: Circle [id:dp23075791411255242] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]  (289,309) .. controls (289,305.13) and (292.13,302) .. (296,302) .. controls (299.87,302) and (303,305.13) .. (303,309) .. controls (303,312.87) and (299.87,316) .. (296,316) .. controls (292.13,316) and (289,312.87) .. (289,309) -- cycle ;
		%Shape: Circle [id:dp05610887097182582] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]  (203,388) .. controls (203,384.13) and (206.13,381) .. (210,381) .. controls (213.87,381) and (217,384.13) .. (217,388) .. controls (217,391.87) and (213.87,395) .. (210,395) .. controls (206.13,395) and (203,391.87) .. (203,388) -- cycle ;
		%Shape: Circle [id:dp8284270883685074] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]  (246,353) .. controls (246,349.13) and (249.13,346) .. (253,346) .. controls (256.87,346) and (260,349.13) .. (260,353) .. controls (260,356.87) and (256.87,360) .. (253,360) .. controls (249.13,360) and (246,356.87) .. (246,353) -- cycle ;
		%Shape: Circle [id:dp1961677054125084] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]  (273,384) .. controls (273,380.13) and (276.13,377) .. (280,377) .. controls (283.87,377) and (287,380.13) .. (287,384) .. controls (287,387.87) and (283.87,391) .. (280,391) .. controls (276.13,391) and (273,387.87) .. (273,384) -- cycle ;
		%Shape: Circle [id:dp14552894043469933] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=1.5]  (106,152) .. controls (106,148.13) and (109.13,145) .. (113,145) .. controls (116.87,145) and (120,148.13) .. (120,152) .. controls (120,155.87) and (116.87,159) .. (113,159) .. controls (109.13,159) and (106,155.87) .. (106,152) -- cycle ;
		%Shape: Circle [id:dp8885917876590714] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=1.5]  (157,139) .. controls (157,135.13) and (160.13,132) .. (164,132) .. controls (167.87,132) and (171,135.13) .. (171,139) .. controls (171,142.87) and (167.87,146) .. (164,146) .. controls (160.13,146) and (157,142.87) .. (157,139) -- cycle ;
		%Shape: Circle [id:dp2039428528018552] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=1.5]  (146,199) .. controls (146,195.13) and (149.13,192) .. (153,192) .. controls (156.87,192) and (160,195.13) .. (160,199) .. controls (160,202.87) and (156.87,206) .. (153,206) .. controls (149.13,206) and (146,202.87) .. (146,199) -- cycle ;
		%Shape: Circle [id:dp4515109129874162] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=1.5]  (123,220) .. controls (123,216.13) and (126.13,213) .. (130,213) .. controls (133.87,213) and (137,216.13) .. (137,220) .. controls (137,223.87) and (133.87,227) .. (130,227) .. controls (126.13,227) and (123,223.87) .. (123,220) -- cycle ;
		%Shape: Circle [id:dp6870841748655325] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=1.5]  (70,203) .. controls (70,199.13) and (73.13,196) .. (77,196) .. controls (80.87,196) and (84,199.13) .. (84,203) .. controls (84,206.87) and (80.87,210) .. (77,210) .. controls (73.13,210) and (70,206.87) .. (70,203) -- cycle ;
		%Shape: Circle [id:dp17593904821216855] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=1.5]  (152,285) .. controls (152,281.13) and (155.13,278) .. (159,278) .. controls (162.87,278) and (166,281.13) .. (166,285) .. controls (166,288.87) and (162.87,292) .. (159,292) .. controls (155.13,292) and (152,288.87) .. (152,285) -- cycle ;
		%Shape: Circle [id:dp978008881366911] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=1.5]  (163,323) .. controls (163,319.13) and (166.13,316) .. (170,316) .. controls (173.87,316) and (177,319.13) .. (177,323) .. controls (177,326.87) and (173.87,330) .. (170,330) .. controls (166.13,330) and (163,326.87) .. (163,323) -- cycle ;
		%Shape: Circle [id:dp10956135804102618] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=1.5]  (118,293) .. controls (118,289.13) and (121.13,286) .. (125,286) .. controls (128.87,286) and (132,289.13) .. (132,293) .. controls (132,296.87) and (128.87,300) .. (125,300) .. controls (121.13,300) and (118,296.87) .. (118,293) -- cycle ;
		%Shape: Circle [id:dp11671042056637426] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=1.5]  (87,337) .. controls (87,333.13) and (90.13,330) .. (94,330) .. controls (97.87,330) and (101,333.13) .. (101,337) .. controls (101,340.87) and (97.87,344) .. (94,344) .. controls (90.13,344) and (87,340.87) .. (87,337) -- cycle ;
		%Shape: Circle [id:dp5692504623521719] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=1.5]  (79,381) .. controls (79,377.13) and (82.13,374) .. (86,374) .. controls (89.87,374) and (93,377.13) .. (93,381) .. controls (93,384.87) and (89.87,388) .. (86,388) .. controls (82.13,388) and (79,384.87) .. (79,381) -- cycle ;
		%Shape: Circle [id:dp8813981299838793] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=1.5]  (135,386) .. controls (135,382.13) and (138.13,379) .. (142,379) .. controls (145.87,379) and (149,382.13) .. (149,386) .. controls (149,389.87) and (145.87,393) .. (142,393) .. controls (138.13,393) and (135,389.87) .. (135,386) -- cycle ;
		%Shape: Brace [id:dp17820578185588798] 
		\draw   (185.5,99) .. controls (185.5,94.33) and (183.17,92) .. (178.5,92) -- (133,92) .. controls (126.33,92) and (123,89.67) .. (123,85) .. controls (123,89.67) and (119.67,92) .. (113,92)(116,92) -- (67.5,92) .. controls (62.83,92) and (60.5,94.33) .. (60.5,99) ;
		%Straight Lines [id:da7164329396292326] 
		\draw    (221,342) -- (281.71,438.24) -- (354.5,438.24) ;
		%Straight Lines [id:da23886169833864201] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (488,150) -- (488,374) ;
		%Straight Lines [id:da11100013203692849] 
		\draw    (425,260.5) -- (479.5,260.5) ;
		%Straight Lines [id:da049643463882306005] 
		\draw    (580,260.5) -- (634.5,260.5) ;
		%Shape: Chord [id:dp6306462297702837] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][fill={rgb, 255:red, 184; green, 233; blue, 134 }  ,fill opacity=1 ] (459.25,257.5) .. controls (446.85,257.39) and (436.83,249.04) .. (436.83,238.75) .. controls (436.83,228.47) and (446.84,220.12) .. (459.23,220) -- cycle ;
		%Shape: Chord [id:dp07654659708720701] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][fill={rgb, 255:red, 184; green, 233; blue, 134 }  ,fill opacity=1 ] (615.92,257.5) .. controls (603.51,257.39) and (593.5,249.04) .. (593.5,238.75) .. controls (593.5,228.47) and (603.51,220.12) .. (615.9,220) -- cycle ;
		%Shape: Chord [id:dp2578883396276983] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 184; green, 233; blue, 134 }  ,fill opacity=1 ] (452.45,307.5) .. controls (440.97,307.38) and (431.7,299.03) .. (431.7,288.75) .. controls (431.7,278.47) and (440.96,270.13) .. (452.43,270) -- cycle ;
		%Shape: Chord [id:dp7333295066235554] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 250; green, 176; blue, 147 }  ,fill opacity=1 ] (451.4,307.5) .. controls (462.51,307.38) and (471.48,299.03) .. (471.48,288.75) .. controls (471.48,278.48) and (462.52,270.13) .. (451.42,270) -- cycle ;
		%Shape: Chord [id:dp9466212116383588] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][fill={rgb, 255:red, 184; green, 233; blue, 134 }  ,fill opacity=1 ] (614.43,307) .. controls (602.95,306.88) and (593.68,298.53) .. (593.68,288.25) .. controls (593.68,277.98) and (602.94,269.63) .. (614.42,269.5) -- cycle ;
		%Shape: Circle [id:dp7074395746903408] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][line width=1.5]  (433.98,288.75) .. controls (433.98,278.4) and (442.37,270) .. (452.73,270) .. controls (463.08,270) and (471.48,278.4) .. (471.48,288.75) .. controls (471.48,299.11) and (463.08,307.5) .. (452.73,307.5) .. controls (442.37,307.5) and (433.98,299.11) .. (433.98,288.75) -- cycle ;
		
		% Text Node
		\draw (192,247) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {\textcolor[rgb]{0.82,0.01,0.11}{false positives}};
		% Text Node
		\draw (99,246) node [anchor=north west][inner sep=0.75pt]  [font=\small,color={rgb, 255:red, 130; green, 204; blue, 56 }  ,opacity=1 ] [align=left] {\textcolor[rgb]{0.31,0.6,0.01}{true positives}};
		% Text Node
		\draw (64,62) node [anchor=north west][inner sep=0.75pt]   [align=left] {relevent elements};
		% Text Node
		\draw (74,108) node [anchor=north west][inner sep=0.75pt]   [align=left] {false negatives};
		% Text Node
		\draw (200,108) node [anchor=north west][inner sep=0.75pt]   [align=left] {true negatives};
		% Text Node
		\draw (285,417) node [anchor=north west][inner sep=0.75pt]   [align=left] {selected elements};
		% Text Node
		\draw (343,153) node [anchor=north west][inner sep=0.75pt]   [align=left] {how many selected\\items are relevent?};
		% Text Node
		\draw (501,153) node [anchor=north west][inner sep=0.75pt]   [align=left] {how many relevant\\items are selected?};
		% Text Node
		\draw (331,247.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {Precision $\displaystyle =$};
		% Text Node
		\draw (505,247.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {Recall $\displaystyle =$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[Precision and Recall]{Precision and Recall (source: Wikipedia)}
	\end{figure}
	The $F_1$ score is the harmonic average of the precision and recall (for a given class $i$) where a $F_1$ score reaches its best value at $1$ (perfect precision and recall) and worst at $0$ (it will seems obvious to the reader with the details given further below).
	
	The $F$-score is also used in Machine Learning as a suitable measure of models tested with unbalanced datasets. Note, however, that the $F$-measures do not take the true negatives into account, and that measures such as the Matthews correlation coefficient, Informedness or Cohen's kappa may be preferable to assess the performance of a binary classifier.
	
	David Hand and others criticize the widespread use of the $F$-score since it gives equal importance to precision and recall. In practice, different types of miss-classifications incur different costs. In other words, the relative importance of precision and recall is an aspect of the problem.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The text that follows is completely inspired and copied from the excellent publication of Professor Yutaka Sasaki (see \cite{sasaki2007truth}) with it's kind authorization.
	\end{tcolorbox}
	
	
	The $F$-measure is defined as a harmonic mean (\SeeChapter{see section Statistics page \pageref{harmonic mean}}) of precision $P$ and recall $R$:
	
	If you are satisfied with this definition and need no further information, that's it. However, if you are deeply interested in the definition of the $F$-measure, you should recap the definitions of the arithmetic and harmonic means.
	
	The arithmetic mean $\mu_a$ (an average in a usual sense) and the harmonic mean $\mu_h$ are defined for recall as follows:
	
	When $x_1 = P$ and $x_2 = R$, $\mu_a$ and $\mu_h$ will be:
	
	Let us recall that harmonic mean is more intuitive than the arithmetic mean when computing a mean of ratios. Indeed, suppose that you have a finger print recognition system and its precision and recall be $1.0$ and $0.2$, respectively. Intuitively, the total performance of the system should be very low because the system covers only $20\%$ of the registered finger prints, which means it is almost useless.

	The arithmetic mean of $1$ and $0.2$ is $0.6$ whereas the harmonic mean of them is:
	
	As you see in this example, the harmonic mean ($0.\bar{3}$) is a more reasonable score than the arithmetic mean ($0.6$).
	
	Some researchers named the definition of the $F$-measure given earlier above the "$F_1$-measure". What is stands the $1$ of $F_1$ for?
	
	The full definition of the $F$-measure is given as follows:
	
	$\beta$ is a parameter that controls a balance between $P$ and $R$. When $\beta=1$, $F_1$ comes to be equivalent to the harmonic mean of $P$ and $R$. If $\beta> 1$, $F$ becomes more recall-oriented and if $\beta < 1$, it becomes more precision-oriented, e.g., $F_0 = P$.
	
	However it seems that the real original definition comes from the $E$ "\NewTerm{effectiveness function}" given by:
	
	where:
	
	Let's remove $\alpha$ using $\beta$:
	
	Now you see that:
	
	Note that $F$ rises if $R$ or $P$ gets better whereas $E$ becomes small if $R$ or $P$ improves. This seems the reason why $F$ is more commonly used than $E$.
	
	Some people use $\alpha$ as a parameter of $F$:
	
	There is nothing wrong with this definition of $F$ but use of this definition might cause an unnecessary confusion because $F_{\alpha=0.5}=F_{\beta=1}$. An attention is needed that the commonly used notation $F_1$ means $F_{\beta=1}$, not $F_{\alpha=1}$.
	
	Still, some of you are not sure why $\beta^2$ is used instead of $\beta$ in $\alpha=\frac{1}{\beta^2+1}$ let my try an explanation of the reason.
	
	 $\beta$ is the parameter that controls the weighting between $P$ and $R$. Formally, is defined as follows:
	
	where:
	
	The motivation behind this condition is that at the point where the gradients of $E$ with respect to $P$ and $R$ are equal, the ratio of $R$ against $P$ should be a desired ratio $\beta$.
	
	Please recall that $E$ is defined as follows:
	
	Now we calculate $\partial E/\partial P$ and $\partial E/\partial R$. By the quotient rule on the derivative of a composite function:
	
	For conciseness, let;
	
	Then:
	
	 Then:
	 
	 is equivalent to:
	 
	which can be simplified to:
	
	As $\beta=R / P$, we can replace $R$ with $\beta P^{2}$:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The per-class metrics above (precision $P_i$, recall $R_i$ and $F_{1,i}$-score) can be averaged over all the classes $n$ resulting in "\NewTerm{macro-averaged precision}" (MP), \NewTerm{macro-averaged recall}" (MR) and "\NewTerm{macro-averaged $F_1$}" ($\text{MF}_1$):
	
	When the instances are not uniformly distributed over the classes, it is useful to look at the performance of a multi-class classifier with respect to one class at a time before averaging the metrics. The idea is to compute the "\NewTerm{one-vs-all confusion matrix for each class}" ($3$ matrices in the case of our companion example here!). You can think of the problem as three binary classification tasks where one class is considered the positive class while the combination of all the other classes make up the negative class.\\
	
	Summing up the values of all these matrices results in a unique confusion matrix and allows us to compute weighted metrics such as "\NewTerm{average accuracy}" and "\NewTerm{micro-averaged $R$, $P$ and $F_1$ metrics}". Similar to the overall accuracy, the average accuracy is defined as the fraction of correctly classified instances in the sum of one-vs-all matrices matrix!
	\end{tcolorbox}
	
	\paragraph{Ordinal Logistic regression}\label{ordinal logistic regression}\mbox{}\\\\
	In statistics, ordinal regression (also named "ordinal classification") is a type of regression analysis used for predicting an ordinal variable, i.e. a variable whose value exists on an arbitrary scale where only the relative ordering between different values is significant.
	
	To introduce the "\NewTerm{ordinal logistic regression}\index{ordinal logistic regression}", let us recall that given a response variable $Y$ having $K$ ordered categories $j=1,2,\ldots,K$, with probabilities:
	
	the multinomial logistic model, considered the $K-1$ ratios:
	
	for $j=1,2,\ldots,K-1$.
	
	Now we will consider the $K-1$ cumulative probabilities instead!
	
	for $j=1,\ldots,K-1$ and write down a model for each of them.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Note that $\gamma_i^{(K)}=P(Y_i\leq K)=1$ always, so it need not be modelled!
	\end{tcolorbox}
	The following holds:
	 
	 In other words, the ordinal logistic model considers as for the multinomial version, a set of dichotomies, one for each possible cutoff of the response categories into two sets, of "high" and "low" responses. But this concept of "high" and "low" at the opposite of multinomial regression is meaningful only if the categories of $Y$ do have an ordering!
	 
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In the software Strata and in the \texttt{polr} package of the \texttt{R} software the ordinal logistic regression model is parametrized as:
	
	instead of:
	
	where $\eta_i=-\beta_{1}$ hence the notation above with the minus signs.
	\end{tcolorbox}
	
	If we consider the ordered labels: Strongly disagree (SD), Disagree (D), Agree (A) and Strongly Agree (SA), the cutoffs are:
	\begin{itemize}
		\item SD vs (D, A or SA)
		\item (SD or D) vs (A or SA)
		\item (SD, D or A) vs SA
	\end{itemize}
	A binary logistic model is then defined for the log-odds of each of these cuts!
	
	The model for the cumulative probabilities is (we omit the index $i$ without loss of generality but just to simplify the notations!):
	
	The intercept terms must be $\alpha^{(1)}<\alpha^{(2)}<\ldots<\alpha^{(K-1)},$ to
guarantee that $\gamma^{(1)}<\gamma^{(2)}<\ldots<\gamma^{(K-1)}$. The $\beta_1,\beta_2,\ldots,\beta_n$ are assumed to be the same for each value of $j$.

	From this the practitioner must remember that:
	\begin{itemize}
		\item There is thus only one set of regression coefficients, not $K-1$ as in a multinomial logistic model
		
		\item The curves for $\gamma^{(1)},\gamma^{(2)},\ldots,\gamma^{(K-1)}$  are translated (i.e. "proportional") as seen in the plots below
		
		\item This is the assumption of "proportional odds". This is why ordinal
logistic model is also known as the "\NewTerm{proportional odds model}\index{proportional odds model}" or "\NewTerm{parallel regression model}\index{parallel regression model}"!
	\end{itemize}
	This is good for the parsimony of the model, because it means that the effect of an explanatory variable on the ordinal response is described by one parameter! However, it is also a restriction on the flexibility of the model, which may or may not be adequate for the data.
	
	In other words, one of the assumptions underlying ordinal logistic (and ordinal probit) regression is that the relationship between each pair of outcome groups is the same. In other words, ordinal logistic regression assumes that the coefficients that describe the relationship between, say, the lowest versus all higher categories of the response variable are the same as those that describe the relationship between the next lowest category and all higher categories, etc. Because the relationship between all pairs of groups is the same, there is only one set of coefficients! If this was not the case, we would need different sets of coefficients in the model to describe the relationship between each pair of outcome groups. 
	
	Thus, in order to assess the appropriateness of our model, we need to evaluate whether the proportional odds assumption is tenable. Statistical tests to do this are available in some software packages. However, it seems that these tests have been criticized for having a tendency to reject the null hypothesis (that the sets of coefficients are the same), and hence, indicate that there the parallel slopes assumption does not hold, in cases where the assumption does hold (see \cite{harrell2001regression} page 335). However, Harrell does recommend a graphical method for assessing the parallel slopes assumption. The values displayed in his proposed graph are essentially (linear) predictions from a logit model, used to model the probability that $y$ is greater than or equal to a given value (for each level of $y$), using one predictor $(x)$ variable at a time.

	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Suppose the proportions of members of the statistical population who would answer "poor", "fair", "good", "very good", and "excellent" are respectively $p_1, p_2, p_3, p_4, p_5$. Then the logarithms of the odds (not the logarithms of the probabilities) of answering in certain ways are:
	
	The proportional odds assumption is that the number added to each of these logarithms to get the next is the same in every case. In other words, these logarithms form an arithmetic sequence. The model states that the number in the last column of the table - the number of times that the logarithm must be added - is some linear combination of the other observed variables.
	\end{tcolorbox}

	The probabilities of individual categories are:
	
	Illustrated below with plots for a case with $K = 4$ categories, first the cumulative probabilities:
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.75\textwidth]{img/computing/ordinal_logistic_regression.jpg}
	\end{figure} 
	and the probabilities of individual categories:
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.75\textwidth]{img/computing/ordinal_logistic_regression_individual_categories_probabilities.jpg}
	\end{figure} 
	Everything here is unchanged from binary logistic models:
	\begin{itemize}
		\item Parameters are estimated using maximum likelihood estimation
		
		\item Hypotheses of interest are typically of the form $\beta_j=0$, for one or more coefficients $\beta_j$
		
		\item Exponentiated coefficients are interpreted as partial odds ratios for being in the higher rather than the lower half of the dichotomy
		
		\item Wald tests, likelihood ratio tests and confidence intervals are the same defined and used as before
	\end{itemize}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	In Stata and \texttt{R} (package \texttt{polr}) the ordinal logistic regression model is parametrized a:
	\begin{equation*}
		\text{logit}(P(Y \leq j))=\beta_{j 0}-\eta_{1} x_{1}-\ldots-\eta_{p} x_{p}
	\end{equation*}
	where for recall $\eta_{i}=-\beta_{i}$.\\
	
	Suppose we want to see whether a binary predictor parental education (\textit{pared}) predicts an ordinal outcome of students who are unlikely, somewhat likely and very likely to apply to a college (\textit{apply}).\\

	Due to the parallel odds assumption, even though we have categories, the coefficient of parental education (\textit{pared}) stays the same across the two categories. Then the two equations for pared $=1$ and pared $=0$ are:
	\begin{equation*}
	\begin{array}{l}
	{\text{logit}\left(P(Y \leq j) | x_{1}=1\right)=\beta_{j 0}-\eta_{1}} \\
	{\text{logit}\left(P(Y \leq j) | x_{1}=0\right)=\beta_{j 0}}
	\end{array}
	\end{equation*}
	Then:
	\begin{equation*}
		\text{logit}\left(P(Y \leq j) | x_{1}=1\right)-\operatorname{logit}\left(P(Y \leq j) | x_{1}=0\right)=-\eta_{1}
	\end{equation*}
	To run an ordinal logistic regression in \texttt{R} we run (see our \texttt{R} companion book for more details):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/computing/ordinal_logistic_regression_example.jpg}
	\end{figure}
	The output shows that for students whose parents attended college, the log odds of being unlikely to college (versus somewhat or very likely) is actually $-\hat{\eta}_{1}=-1.13$ or $1.13$ points lower than the students who did not attend college. Recall that $-\eta_{i}=\beta_{i}$ for $j=1,2$ only since logit $(P(Y \leq 3))$ is undefined. So the formulations for the first and second category becomes:
	\begin{equation*}
	\begin{array}{l}
	{\text{logit}(P(Y \leq 1))=0.377-1.13 x_{1}} \\
	{\text{logit}(P(Y \leq 2))=2.452-1.13 x_{1}}
	\end{array}
	\end{equation*}
	To see the connection between the proportional odds assumption, we exponentiate both sides of the equations above and we use the property that $\log (b)-\log (a)=\log (b / a)$ to calculate the odds of \textit{pared} for each level of \textit{apply}:
	\begin{equation*}
		\begin{array}{l}
		{\dfrac{P\left(Y \leq 1 | x_{1}=1\right)}{P\left(Y>1 | x_{1}=1\right)}=\dfrac{e^{0.377}}{e^{1.13}}} \\
		{\dfrac{P\left(Y \leq 1 | x_{1}=0\right)}{P\left(Y>1 | x_{1}=0\right)}=e^{0.377}} \\
		{\dfrac{P\left(Y \leq 2 | x_{1}=1\right)}{P\left(Y>2 | x_{1}=1\right)}=\dfrac{e^{2.45}}{e^{1.13}}} \\
		{\dfrac{P\left(Y \leq 2 | x_{1}=0\right)}{P\left(Y>2 | x_{1}=0\right)}=e^{2.45}}
		\end{array}
	\end{equation*} 
	From the odds of each level of pared, we can calculate the odds ratio of \textit{pared} for each level of \textit{apply}:
	\begin{gather*}
		\begin{aligned}
		&\dfrac{\dfrac{P\left(Y \leq 1 | x_{1}=1\right)}{P\left(Y>1 | x_{1}=1\right)}}{\dfrac{P\left(Y \leq 1 | x_{1}=0\right)}{P\left(Y>1 | x_{1}=0\right)}}=\dfrac{1}{e^{1.13}}=e^{-1.13}\\
		&\dfrac{\dfrac{P\left(Y \leq 2 | x_{1}=1\right)}{P\left(Y>2 | x_{1}=1\right)}}{\dfrac{P\left(Y \leq 2 | x_{1}=0\right)}{P\left(Y>2 | x_{1}=0\right)}}=\dfrac{1}{e^{1.13}}=e^{-1.13}
		\end{aligned}
	\end{gather*}
	The proportional odds assumption ensures that the odds ratios across all $J-1$ categories are the same. In our example, the proportional odds assumption means that the odds of being unlikely versus somewhat or very likely to apply $(j=1)$ is equal to the odds of being unlikely and somewhat likely versus very likely to apply $(j=2)$.\\
	
	The proportional odds assumption is not simply that the odds are the same but that the odds ratios are the same across categories. These odds ratios can be derived by exponentiating the coefficients (in the log-odds metric), but the interpretation is a bit unexpected. Recall that the coefficient  $-\eta_1$ represents a one unit change in the log odds of applying for students whose parents went to college versus parents who did not:
	\begin{gather*}
		\text{logit}(P(Y \le j)|x_1=1) -\text{logit}(P(Y \le j)|x_1=0) =-\eta_{1}
	\end{gather*}
	Since the exponent is the inverse function of the log, we can simply exponentiate both sides of this equation, and by using the property that $\log(b)-\log(a)=\log(b/a)$:
	\begin{gather*}
		\dfrac{\dfrac{P(Y \le j |x_1=1)}{P(Y>j|x_1=1)}}{\dfrac{P(Y \le j |x_1=0)}{P(Y>j|x_1=0)}}=e^{-\eta_{1}}
	\end{gather*}
	For simplicity of notation and by the proportional odds assumption, let:
	\begin{gather*}
		\dfrac{P(Y \le j |x_1=1)}{P(Y>j|x_1=1)}  = \dfrac{p_1}{1-p_1}\qquad\text{and}\qquad	\dfrac{P(Y \le j |x_1=0)}{P(Y>j|x_1=0)}  = \dfrac{p_0}{1-p_0}
	\end{gather*}
	Then the odds ratio is defined as:
	\begin{gather*}
		\dfrac{p_1 / (1-p_1) }{p_0 / (1-p_0)} = e^{-\eta_{1}}
	\end{gather*}
	In our example, $e^{-1.13}=0.323$, which means that students whose parents attended college have a $67.6\%$ lower odds of being less  likely to apply to college.\\
	
	But you won't get this value with \texttt{R} and Stata (they retrieve the value $3.087$ instead). Let's see why!\\
	
	Since:
	\begin{gather*}
		 e^{-\eta_{1}}=\dfrac{1}{e^{\eta_{1}}}
	\end{gather*}
	From the output $\eta=1.13$, which means that $3.086$ come from actually:
	\begin{gather*}
		e^{\eta_{1}}=\dfrac{p_0 / (1-p_0) }{p_1 / (1-p_1)}=3.086
	\end{gather*}
	This suggests that students whose parents did not go to college have higher odds of being less likely to apply.\\
	
	Double negation can be logically confusing. Suppose we wanted to interpret the odds of being more likely to apply to college. We can perform a slight manipulation of our original odds ratio:
	\begin{equation*}
		\begin{aligned}
		e^{-\eta_{1}} &=\frac{p_{1} /\left(1-p_{1}\right)}{p_{0} /\left(1-p_{0}\right)} =\dfrac{p_{1}\left(1-p_{0}\right)}{p_{0}\left(1-p_{1}\right)}=\dfrac{\left(1-p_{0}\right) / p_{0}}{\left(1-p_{1}\right) / p_{1}} \\
		&=\dfrac{P(Y>j | x=0) / P(Y \leq j | x=0)}{P(Y>j | x=1) / P(Y \leq j | x=1)}
		\end{aligned}
	\end{equation*}
	Since $ e^{-\eta_{1}}=1/ e^{\eta_{1}}$, then we have:
	\begin{gather*}
		\dfrac{P(Y>j | x=1) / P(Y \leq j | x=1)}{P(Y>j | x=0) / P(Y \leq j | x=0)}=e^{\eta_{1}}
	\end{gather*}
	Instead of interpreting the odds of being in the $j$ th category or less, we can interpret the odds of being greater than the $j$ th category by exponentiating $\eta$ itself. In our example, $e^{\eta_{1}}=e^{1.127}=3.086$ means that students whose parents went to college have $3.086$ times the odds of being very likely to apply (vs. somewhat or unlikely) compared to students whose parents did not go to college. The results here are consistent with our intuition because it removes double negatives. As a general rule, it is easier to interpret the odds ratios of $x_{1}=1$ vs. $x_{1}=0$ by simply exponentiating $\eta$ itself rather than interpreting the odds ratios of $x_{1}=0$ vs. $x_{1}=1$ by exponentiating $-\eta$. However by doing so, we flip the interpretation of the outcome by placing $P(Y>j)$ in the numerator.
	\end{tcolorbox}
	
	\pagebreak
	\paragraph{Log-Loss}\mbox{}\\\\
	Let us recall the relation that we have introduced just earlier for the binomial logistic regression:
	
	It is often rewritten as following and defined as the "\NewTerm{Log-loss}\index{log-loss}" (often used in Data Mining/Data Science that like to use the log in base $10$ instead of the natural logarithm...):
	
	with $y_i\in\{0,1\}$ type of data with estimated $\hat{y}_i\in \{0,1\}$, thus $\text{LL}>0$ (the Log-loss gradually declines as the predicted probability improves as the reader can notice it). In other words, Log-loss is used when we have $\{0,1\}$ response. This is usually because when we have $\{0,1\}$ response, the best models give us values in terms of probabilities.
	
	It is also sometimes written as following:
	
	
	In simple words (even if the Log-loss function is simply implicitly the objective function to minimize, in order to  fit a log linear probability model to a set of binary labelled examples!), Log-loss measures the uncertainty of the probabilities of your model by comparing them to the true labels. Let us look closely at its formula and see how it measures the uncertainty 

	Now the question is, your training labels are $0$ and $1$ but your training predictions are $0.4$, $0.6$, $0.89$, $0.1122$, etc. So how do we calculate a measure of the error of our model ? If we directly classify all the observations having values $> 0.5$ into $1$ then we are at a high risk of increasing the misclassification. This is because it may so happen that many values having probabilities $0.4$, $0.45$, $0.49$ can have a true value of $1$.

	This is where Log-loss comes into picture!
	
	Now let us closely follow the formula of Log-loss. There can be four major cases for the values of $y_i$ and $p_i$:
	\begin{enumerate}
		\item[C1.] $y_i=1$ , $p_i =$ High , $1-y_i=0$ , $1-p_i =$ Low
	
		\item[C2.] $y_i=1$ , $p_i =$ Low , $1-y_i=0$ , $1-p_i =$ High
	
		\item[C3.] $y_i=0$ , $p_i =$ Low , $1-y_i=1$ , $1-p_i =$ High
	
		\item[C4.] $y_i=0$ , $p_i =$ High , $1-y_i=1$ , $1-p_i =$ Low
	\end{enumerate}
	\begin{itemize}
		\item Case 1:

		In this case $y = 1$ and $p =$ High implies that we have got things right! Because the true value of the response agrees with our high probability. Now look closely! Occurrence of Case 1 will significantly decrease the sum because, $y_i\log(p_i)$ would be small and simultaneously the other term in the summation would be zero since $1 - y_i = 1 - 1 = 0$. So more occurrences of Case 1 would decrease the sum and consequently decrease the mean.
		
		Also note that this is possible because if:
		
		
		\item Case 2:
		
		In this case $y = 1$ and $p =$ Low. This is a totally undesirable case because our probability of $y$ being $1$ is low but still the true value of $y$ is $1$. Now again looking at the Log-loss closely, the second term in the summation would be very small since $1-y_i$ would almost equal to zero. And since $p =$ Low, $y_i\log(p_i)$ would inflate the sum at the opposite of Case 1. So Case 2 would ultimately affect the sum a lot.
	\end{itemize}
	
	Similarly the occurrences of Case 3 would not change the sum and occurrences of Case 4 significantly.
	
	Now coming back to the main question, how does Log-loss measure uncertainty of your model? The answer is simple! Suppose we have more of Case 1 and Case 3, then the sum inside the Log-loss relation would be small (would tend to decrease). This would imply that the mean ($/n$) would also tend to decrease and will be substantially small in comparison to what it would have been if Case 2 and Case 4 got added. So now this value is as small as possible at Case 1 and Case 3 which indicates a good prediction. This is why smaller the value, better is the model i.e. smaller the Log-loss, better is the model i.e. smaller the uncertainty.
	
	\paragraph{Odds Ratio Confidence Interval}\label{odds ratio confidence interval}\mbox{}\\\\
	We have seen earlier above during our study of the binomial logistic regression (see page \pageref{odds ratio logistic regression}) that the odds ratio was defined by:
	
	What we can also write as:
	
	But that many practitioners write it as following:
	
	Since the ratio of proportions is equivalent to the ratio of the size of the concerned populations, it is customary to write the estimator of the odds ratio as being:
	
	Which then relates to a table typically of the following kind in the medical field (and other fields obviously!):
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|c|c|c|}
		\hline
		 & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Sick}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Non-Sick}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Odds}} \\ \hline
		\cellcolor[gray]{0.75}\textbf{Treated Group} & $n_1$ & $n_2$ & $n_1/n_2$ \\ \hline
		\cellcolor[gray]{0.75}\textbf{Non-Treated Group} & $n_3$ & $n_4$ & $n_3/n_4$\\ \hline
		\cellcolor[gray]{0.75}\textbf{Totals} & $n_1+n_3$ & $n_2+n_4$ & $\widehat{\text{O.R.}}=\dfrac{\frac{n_1}{n_2}}{\frac{n_3}{n_4}}$\\ \hline
		\end{tabular}
	\end{table}
	In the field of survival analysis (\SeeChapter{see section Statistics page \pageref{survival analysis}}) such a table will be represented as following:
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|c|c|c|}
		\hline
		 & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Observed}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Expected}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Relative failure rate}} \\ \hline
		\cellcolor[gray]{0.75}\textbf{Survival Curve 1} & $O_1$ & $E_1$ & $O_1/E_1$ \\ \hline
		\cellcolor[gray]{0.75}\textbf{Survival Curve 2} & $O_2$ & $E_2$ & $O_2/E_2$\\ \hline
		\cellcolor[gray]{0.75}\textbf{Totals} & $O_1+O_2$ & $E_2+E_4$ & $\widehat{\text{O.R.}}=\dfrac{\frac{O_1}{E_1}}{\frac{O_2}{E_2}}$\\ \hline
		\end{tabular}
	\end{table}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	 An O.R. of $1.00$ means that the two groups were equally likely to remain sick. An O.R. higher than $1.00$ means that a group is more likely to experience the event than the second group. An O.R. of less than $1.00$ means that a group was less likely to experience the event. However, an O.R. value below $1.00$ is not directly interpretable. It is important to put the group expected to have higher odds of the event in the first row.
	\end{tcolorbox}
	Now, let us consider that the $n_i$ follow an asymptotically Normal law such as according to what we had seen in our study of the construction of the Normal Law:
	
	Now the idea will be to make an additional approximation ... otherwise we will not find anything simple. We will put that:
	
	It is very very approximate as approach .... since rigorously:
	
	Simulations show that we have a Normal law beyond $n$ equating $30$, but although it is a Normal centered law ... it is hardly reduced (therefore not of unit variance...).
	
	Regarding the last choice established, we have:
	
	This last approximation then allows us to write (we detail the steps a maximum on the request of one of our reader):
	
	Now let us take the logarithm (it is customary to take the natural logarithm):
	
	The second term can be developed in Taylor series of the first order (\SeeChapter{see section Sequences and Series  page \pageref{usual maclaurin developments}}):
	
	It comes then after many approximations ...:
	
	We then have:
	
	However, we observe that (do not forget that the variance of a random variable multiplied by a constant puts the constant squared):
	
	We then first consider that the variance part of the random variables as independent (we will take into account the covariance just after):
	
	We now have to take into account the following six covariances (refer to the section Statistics for the proof that when we sum $n$ random variables there are as many covariances to compute as possible pairs of possible variables):
	
	Recall that during our study of the multinomial law and as part of an application for the McNemar test (page \pageref{mcnemar test}), we proved that:
	
	which in our context with a more general notation is written:
	
	If we center these two random variables (as is the case in our developments so far), the covariance does not change:
	
	If we reduce the random variables by $\sqrt{n}$, then we have:
	
	and therefore:
	
	Among the six covariances that we need to calculate we have therefore:
	
	So the sum is worth $-2$. It is necessary to multiply by $2$ the whole (reminder of the proof of the general case which makes a factor $2$ appear before each term of the covariance) to obtain the global value of the covariance terms. So in the end we have a value of $-4$ for the sum of the pure covariances:
	
	The total variance is ultimately given by:
	
	And finally we have the approximation sometimes named "\NewTerm{Woolf method}\index{Woolf method}":
	
	More explicitly (notice that the lower and upper bound are symmetrical):
	
	Relationship that is often written as:
	
	Some softwares calculate the upper and lower bound by using the exponential (especially softwares plotting the famous "Four-fold plot"), and therefore the upper and lower bound are no longer symmetrical:
	
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us consider the following data:
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|c|c|}
		\hline
		 Group / Heart Attack & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Yes}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{No}} \\ \hline
		\cellcolor[gray]{0.75}\textbf{Placebo} & $53$ & $58$ \\ \hline
		\cellcolor[gray]{0.75}\textbf{Treated Group} & $11$ & $40$ \\ \hline
		\end{tabular}
	\end{table}
	The odds ratio is then:
	
	So the Placebo group is almost three times more likely to experience the event of interest (Heart Attack).\\
	
	You must keep in mind the three following scenarios (refresh!):
	\begin{itemize}
		\item The odds ratio is almost or equal to $1.0$. means that the odds of exposure among cases is the same as the odds of exposure among controls. In other words, the exposure is not associated with the disease.
	
		\item The odds ratio is greater than $1.0$ means that the odds of exposure among cases is greater than the odds of exposure among controls. In other words, the exposure may be a risk factor for the disease.
	
		\item The odds ratio is less than $1.0$ means that the odds of exposure among cases is lower than the odds of exposure among controls. In other words, the exposure may be protective against the disease.
	\end{itemize}
	
	The confidence interval of $\ln(\text{O.R.})$ at a confidence level of $95\%$ is then
approximately given by:
	
	Thus:
	
	Hence:
	
	and so taking the exponential we finally have the interval that interests us (read further below the text on the corresponding four-fold plot!):
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Caution!!! While the estimator of the natural logarithm of the odds ratio is in the middle of the confidence interval, this is obviously not the case with the standard odds ratio!
	\end{tcolorbox}
	
	
	\subparagraph{Four-fold plot}\mbox{}\\\\
	The "\NewTerm{four-fold plot}\index{four-fold plot}", or "\NewTerm{fourfold plot}, is a graphic designed to display the frequencies in a $2\times 2$ contingency table. In this display the frequency in each cell is shown by a quarter circle, whose area is proportional to the cell count in a given $2\times 2$ layer, in a way that depicts the odds ratios for $1$ strata. Confidence rings for the odds ratio can be superimposed to provide a visual test of the hypothesis of no association in each stratum.
	
	Before we deal with the maths let us look with the \texttt{R} 3.4.2 software how to generate a four-fold plot for the previous example:
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/fourfold_plot.jpg}
		\caption{Four-fold plot with \texttt{R} 3.4.2}
	\end{figure}
	In this display the frequency $n_i$ in each cell of a four-fold table is shown by a quarter circle (on a vertical and horizontal axis of range $[-1,+1]$ with major ticks each $0.2$ and minor ticks each $0.1$), whose radius is proportional to $\sqrt{n_i}$ so that they are is proportional to the cell count (otherwise it would be proportional to the square of the cell count as $S=\pi R^2$).
	
	The four-fold display is constructed so that the four quadrants will align vertically and horizontally when the odds ratio is $1$. Confidence rings for the observed odds ratio provide a visual test of the hypothesis of no association. They have the property that rings for adjacent quadrants overlap if and only if the observed counts are consistent with this null hypothesis. In the above figure the confidence intervals do not overlap, indicating a significant association between \textit{Heart attack} and \textit{Group}. 
	
	The radius of the circles are given by:
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|c|c|}
		\hline
		 Group / Heart Attack & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Yes}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{No}} \\ \hline
		\cellcolor[gray]{0.75}\textbf{Placebo} & $\sqrt{\dfrac{n_1}{\max n_i}}$ & $\sqrt{\dfrac{n_2}{\max n_i}}$ \\ \hline
		\cellcolor[gray]{0.75}\textbf{Treated Group} & $\sqrt{\dfrac{n_3}{\max n_i}}$ & $\sqrt{\dfrac{n_4}{\max n_i}}$ \\ \hline
		\end{tabular}
	\end{table}
	Explicitly:
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|c|c|}
		\hline
		 Group / Heart Attack & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Yes}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{No}} \\ \hline
		\cellcolor[gray]{0.75}\textbf{Placebo} & $\sqrt{\dfrac{53}{58}}=0.955$ & $\sqrt{\dfrac{58}{58}}=1.000$ \\ \hline
		\cellcolor[gray]{0.75}\textbf{Treated Group} & $\sqrt{\dfrac{11}{58}}=0.435$ & $\sqrt{\dfrac{40}{58}}=0.830$ \\ \hline
		\end{tabular}
	\end{table}
	
	\paragraph{Risk Ratio and its Confidence Interval}\label{risk ratio}\mbox{}\\\\
	The "\NewTerm{risk ratio}\index{risk ratio}" or "\NewTerm{relative risk}\index{relative risk}" is defined quite naturally by:
	
	Consider the typical case in the medical field:
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|c|c|c|}
		\hline
		 & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Sick}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Non-Sick}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Total}} \\ \hline
		\cellcolor[gray]{0.75}\textbf{Treated Group} & $n_1$ & $n_2$ & $n_A$ \\ \hline
		\cellcolor[gray]{0.75}\textbf{Non-Treated Group} & $n_3$ & $n_4$ & $n_B$\\ \hline
		\end{tabular}
	\end{table}
	Thus the risk of illness for the treated group (TG), also named "Experimental Event Rate" (EER), is:
	
	and for the non-treated group (NTG) (or "control group"), also named "Control Event Rate" (CER):
	
	and hence the risk ratio is defined by:
	
	remembering that:
	
	Risk ratios (RR) and odds ratios (OR) are both used to analyse factors associated with increased risk of disease. They are different in that a risk ratio is the ratio of two risks and an odds ratio is the ratio of two odds. Risks and odds are calculated differently. For example, with one toss of a fair die, the risk of rolling a $1$ is $1/6 = 0.167$. The odds of rolling a $1$ are $1/5 = 0.200$. In textbook terms, risk is successes divided by total trials and odds are successes divided by failures. In this example rolling a $1$ is a success and rolling $2$ through $5$ is a failure.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us consider a R.R of $1.50$. That means obviously an increase of the risk of $50\%$ in the treatment group in comparison of the control group. So if the control group has a risk of $20\%$, then the treatment group has a risk of $20\%+1.5\cdot 20\% = 30\%$.
	\end{tcolorbox}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	If $n_1$ is small relatively to $n_2$ and $n_3$ is small relatively to $n_4$ we fall back on the odds ratio:
	
	we then better understand why the odds ratio is sometimes named "\NewTerm{approximate relative risk}".\\
	
	So in the medical field, we say that if the disease is rare in treated and non-treated groups the interpretation of relative risk (i.e. risk ratio) and odds ratio is the same.
	\end{tcolorbox}
	Let us take the log of the relative risk (i.e. risk ratio):
	
	It then comes for the variance since $ p_1 $ and $ p_2 $ are assumed to be independent:
	 
	To determine the variance of the natural logarithm of the proportions we will have to use a technique that we name the "\NewTerm{univariate delta method}\index{delta method}\index{univariate delta method}\label{delta method}" (indeed, because of the presence of a sum at the denominator, we will not be able to take the same approach as the one used for the odds ratio) which consists of approximating the exact variance by the corresponding asymptotically normal variance.

	To see what the univariate delta method is, let's remember that we have proved  in the section Statistics (see page \pageref{normal centered reduced variable}) that:
	
	Now, let us consider the following linear function:
	
	where $a$ and $b$ are any two constants. From then on, using the linearity property of the mean, we immediately have:
	
	Now, let us ask ourselves the question, what is the limit of:
	
	We then have:
	
	We immediately deduce that:
	
	Now, let us develop in series of Taylor in the first order around $\mu$ the function $g(t)$, which would not necessarily be linear, then we have:	
	
	By identification with the simple linear case, we then have:
	
	It then comes in the case of a function $g (t)$ that is not necessarily linear, but approximated as such by a series development of Taylor and under the assumption that the variable is continuous that:
	
	So don't forget what this method means (!!): If $X_n$ is (asymptotically) Normal with mean $0$ and variance $\sigma^2$, $g$ is differentiable and $g'(\mu)\neq 0$, then $g(X_n)$ is approximately given by:
	
	For the purpose of demonstrating the confidence interval of the risk ratio, let us apply that to the binomial distribution with:
	
	It then comes from the previous results that the approximation of the variance by a Normal law and from the knowledge that the variance and the mean of the binomial law are given for recall by (\SeeChapter{see section Statistics page \pageref{binomial distribution}}):
	
	that the univariate delta method gives:
	
	So the variance corresponding to $\ln(p_i)$ is given asymptotically by:
	
	Therefore:
	
	Hence:
	
	Thus, we have asymptotically the following interval for the risk ratio if the assumptions are satisfied (independence and underlying Normal distribution):
	
	or in condensed form:
	
	Note that in the medical literature, the reader will also find the standard deviation of the risk ratio in many other forms that are here (which are of course all equivalent):
	
	If the risk ratio is the same for both target groups, the Risk Ratio will obviously be equal to $100\%$ (i.e. equal to $1$). If it is less than $100\%$ (i.e. less than $1$), it simply means that the risk of illness in the treated group is lower than that of the non-treated group, etc.
	
	The typical range of values of the odds ratio and risk ratio are designated (wrongly!) traditionally as following in scientific publications:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1215); %set diagram left start at 0, and has height of 1215
		
		%Shape: Rectangle [id:dp5202884238593755] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 165; green, 136; blue, 103 }  ,fill opacity=1 ] (68,110) -- (148.5,110) -- (148.5,159) -- (68,159) -- cycle ;
		%Shape: Rectangle [id:dp7820776762113879] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 204; green, 174; blue, 146 }  ,fill opacity=1 ] (148.5,110) -- (237.5,110) -- (237.5,159) -- (148.5,159) -- cycle ;
		%Shape: Rectangle [id:dp2718531229212964] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 236; green, 214; blue, 191 }  ,fill opacity=1 ] (237.5,110) -- (322.5,110) -- (322.5,159) -- (237.5,159) -- cycle ;
		%Shape: Rectangle [id:dp7884381750172509] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 236; green, 214; blue, 191 }  ,fill opacity=1 ] (322.5,110) -- (407.5,110) -- (407.5,159) -- (322.5,159) -- cycle ;
		%Shape: Rectangle [id:dp9643880961824352] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 204; green, 174; blue, 146 }  ,fill opacity=1 ] (407.5,110) -- (497.5,110) -- (497.5,159) -- (407.5,159) -- cycle ;
		%Shape: Rectangle [id:dp28980633142877] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 165; green, 136; blue, 103 }  ,fill opacity=1 ] (497.5,110) -- (582.5,110) -- (582.5,159) -- (497.5,159) -- cycle ;
		%Straight Lines [id:da8750152544505168] 
		\draw    (323,95) -- (323,165) ;
		
		% Text Node
		\draw (291,76) node [anchor=north west][inner sep=0.75pt]   [align=left] {No effect};
		% Text Node
		\draw (264.6,125.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {Small};
		% Text Node
		\draw (344.4,125.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {Small};
		% Text Node
		\draw (162.8,125.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {Moderate};
		% Text Node
		\draw (418.2,125.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {Moderate};
		% Text Node
		\draw (86,125.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {Large};
		% Text Node
		\draw (520,125.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {Large};
		% Text Node
		\draw (311,167.4) node [anchor=north west][inner sep=0.75pt]    {$1.0$};
		% Text Node
		\draw (226,167.4) node [anchor=north west][inner sep=0.75pt]    {$0.8$};
		% Text Node
		\draw (135,167.4) node [anchor=north west][inner sep=0.75pt]    {$0.5$};
		% Text Node
		\draw (394,167.4) node [anchor=north west][inner sep=0.75pt]    {$1.2$};
		% Text Node
		\draw (487,167.4) node [anchor=north west][inner sep=0.75pt]    {$2.0$};
		% Text Node
		\draw (126,198) node [anchor=north west][inner sep=0.75pt]   [align=left] {size of the relative risk (risk ratio), odds ratio or hazard ratio};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Odds Ratio and Risk Ratio range values subjective qualifications}
	\end{figure}
	
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Caution!!! While the natural logarithm Risk Ratio estimator is in the middle of the confidence interval, this is obviously not the case for the standard Risk Ratio!
	\end{tcolorbox}
	
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us consider the following data:
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|c|c|c|}
		\hline
		Group/Heart attack & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Yes}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{No}} \\ \hline
		\cellcolor[gray]{0.75}\textbf{Placebo} & $53$ & $58$ \\ \hline
		\cellcolor[gray]{0.75}\textbf{Aspirin} & $11$ & $40$ \\ \hline
		\end{tabular}
	\end{table}
	The relative risk is then (which corresponds to what the MedCalc software returns):
	
	The confidence interval of $\ln(\widehat{\text{R.R.}})$ at a confidence level of $95\%$ is then approximately given by:
	
	Hence:
	
	So finally:
	
	and so taking the exponential we finally have the interval that interests us:
	
	This perfectly matches the data returned by the MedCalc software! The Absolute risk increase is given by:
	
	\end{tcolorbox}
	The reader or practitioner must keep in mind that the odds ratio, as the risk ratio, are tools as we have just seen, used to make null hypothesis statistical testing (NHST) as they involve confidence intervals. So don't forget that what we are seeking is to reject the null hypothesis $H_0$ ! Because of this, a lot of people read medical studies wrongly as they have forgotten how NHST works (or they just don't know...)!
	
	So for recall:
		\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,970); %set diagram left start at 0, and has height of 970
		
		%Rounded Rect [id:dp3023374255338609] 
		\draw   (72,42.3) .. controls (72,29.43) and (82.43,19) .. (95.3,19) -- (577.2,19) .. controls (590.07,19) and (600.5,29.43) .. (600.5,42.3) -- (600.5,215.7) .. controls (600.5,228.57) and (590.07,239) .. (577.2,239) -- (95.3,239) .. controls (82.43,239) and (72,228.57) .. (72,215.7) -- cycle ;
		%Rounded Same Side Corner Rect [id:dp7313578506166059] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (72,32.08) .. controls (72,24.86) and (77.86,19) .. (85.08,19) -- (587.42,19) .. controls (594.64,19) and (600.5,24.86) .. (600.5,32.08) -- (600.5,57) .. controls (600.5,57) and (600.5,57) .. (600.5,57) -- (72,57) .. controls (72,57) and (72,57) .. (72,57) -- cycle ;
		%Straight Lines [id:da6802517514414794] 
		\draw    (122,104) -- (236.5,104) ;
		\draw [shift={(236.5,104)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (0,11.18) -- (0,-11.18)   ;
		\draw [shift={(122,104)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (0,11.18) -- (0,-11.18)   ;
		%Shape: Diamond [id:dp27963957121442307] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (179.25,97.5) -- (185.75,104) -- (179.25,110.5) -- (172.75,104) -- cycle ;
		%Straight Lines [id:da43200301726053314] 
		\draw    (429,105) -- (543.5,105) ;
		\draw [shift={(543.5,105)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (0,11.18) -- (0,-11.18)   ;
		\draw [shift={(429,105)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (0,11.18) -- (0,-11.18)   ;
		%Shape: Diamond [id:dp7380285068558268] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (486.25,98.5) -- (492.75,105) -- (486.25,111.5) -- (479.75,105) -- cycle ;
		%Shape: Ellipse [id:dp5089716563118276] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (268,157.5) .. controls (268,142.31) and (277.07,130) .. (288.25,130) .. controls (299.43,130) and (308.5,142.31) .. (308.5,157.5) .. controls (308.5,172.69) and (299.43,185) .. (288.25,185) .. controls (277.07,185) and (268,172.69) .. (268,157.5) -- cycle ;
		%Shape: Circle [id:dp2703868566155203] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (224.16,164.5) .. controls (224.16,153.18) and (233.34,144) .. (244.66,144) .. controls (255.98,144) and (265.16,153.18) .. (265.16,164.5) .. controls (265.16,175.82) and (255.98,185) .. (244.66,185) .. controls (233.34,185) and (224.16,175.82) .. (224.16,164.5) -- cycle ;
		%Shape: Circle [id:dp42309101449102915] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (187.3,168) .. controls (187.3,158.61) and (194.91,151) .. (204.3,151) .. controls (213.69,151) and (221.3,158.61) .. (221.3,168) .. controls (221.3,177.39) and (213.69,185) .. (204.3,185) .. controls (194.91,185) and (187.3,177.39) .. (187.3,168) -- cycle ;
		%Shape: Circle [id:dp8230161647199501] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (155.44,170.5) .. controls (155.44,162.49) and (161.93,156) .. (169.94,156) .. controls (177.95,156) and (184.44,162.49) .. (184.44,170.5) .. controls (184.44,178.51) and (177.95,185) .. (169.94,185) .. controls (161.93,185) and (155.44,178.51) .. (155.44,170.5) -- cycle ;
		%Shape: Circle [id:dp2245398198823585] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (132.58,175) .. controls (132.58,169.48) and (137.06,165) .. (142.58,165) .. controls (148.1,165) and (152.58,169.48) .. (152.58,175) .. controls (152.58,180.52) and (148.1,185) .. (142.58,185) .. controls (137.06,185) and (132.58,180.52) .. (132.58,175) -- cycle ;
		%Shape: Circle [id:dp12616214692754468] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (116.72,178.5) .. controls (116.72,174.91) and (119.63,172) .. (123.22,172) .. controls (126.81,172) and (129.72,174.91) .. (129.72,178.5) .. controls (129.72,182.09) and (126.81,185) .. (123.22,185) .. controls (119.63,185) and (116.72,182.09) .. (116.72,178.5) -- cycle ;
		%Shape: Circle [id:dp7604390213299845] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (103.86,180) .. controls (103.86,177.24) and (106.1,175) .. (108.86,175) .. controls (111.62,175) and (113.86,177.24) .. (113.86,180) .. controls (113.86,182.76) and (111.62,185) .. (108.86,185) .. controls (106.1,185) and (103.86,182.76) .. (103.86,180) -- cycle ;
		%Shape: Circle [id:dp7511531276030263] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (95.67,182.33) .. controls (95.67,180.86) and (96.86,179.67) .. (98.33,179.67) .. controls (99.81,179.67) and (101,180.86) .. (101,182.33) .. controls (101,183.81) and (99.81,185) .. (98.33,185) .. controls (96.86,185) and (95.67,183.81) .. (95.67,182.33) -- cycle ;
		%Straight Lines [id:da02952111019295045] 
		\draw    (98.33,182.33) -- (85.5,182.33) ;
		\draw [shift={(82.5,182.33)}, rotate = 360] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		
		%Shape: Ellipse [id:dp4116041313830201] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (405,157.5) .. controls (405,142.31) and (395.93,130) .. (384.75,130) .. controls (373.57,130) and (364.5,142.31) .. (364.5,157.5) .. controls (364.5,172.69) and (373.57,185) .. (384.75,185) .. controls (395.93,185) and (405,172.69) .. (405,157.5) -- cycle ;
		%Shape: Circle [id:dp5386131009725297] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (448.84,164.5) .. controls (448.84,153.18) and (439.66,144) .. (428.34,144) .. controls (417.02,144) and (407.84,153.18) .. (407.84,164.5) .. controls (407.84,175.82) and (417.02,185) .. (428.34,185) .. controls (439.66,185) and (448.84,175.82) .. (448.84,164.5) -- cycle ;
		%Shape: Circle [id:dp8088507874286328] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (485.7,168) .. controls (485.7,158.61) and (478.09,151) .. (468.7,151) .. controls (459.31,151) and (451.7,158.61) .. (451.7,168) .. controls (451.7,177.39) and (459.31,185) .. (468.7,185) .. controls (478.09,185) and (485.7,177.39) .. (485.7,168) -- cycle ;
		%Shape: Circle [id:dp629739750069181] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (517.56,170.5) .. controls (517.56,162.49) and (511.07,156) .. (503.06,156) .. controls (495.05,156) and (488.56,162.49) .. (488.56,170.5) .. controls (488.56,178.51) and (495.05,185) .. (503.06,185) .. controls (511.07,185) and (517.56,178.51) .. (517.56,170.5) -- cycle ;
		%Shape: Ellipse [id:dp4790214395749679] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (540.42,175) .. controls (540.42,169.48) and (535.94,165) .. (530.42,165) .. controls (524.9,165) and (520.42,169.48) .. (520.42,175) .. controls (520.42,180.52) and (524.9,185) .. (530.42,185) .. controls (535.94,185) and (540.42,180.52) .. (540.42,175) -- cycle ;
		%Shape: Circle [id:dp8708115841624691] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (556.28,178.5) .. controls (556.28,174.91) and (553.37,172) .. (549.78,172) .. controls (546.19,172) and (543.28,174.91) .. (543.28,178.5) .. controls (543.28,182.09) and (546.19,185) .. (549.78,185) .. controls (553.37,185) and (556.28,182.09) .. (556.28,178.5) -- cycle ;
		%Shape: Circle [id:dp5292478963678806] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (569.14,180) .. controls (569.14,177.24) and (566.9,175) .. (564.14,175) .. controls (561.38,175) and (559.14,177.24) .. (559.14,180) .. controls (559.14,182.76) and (561.38,185) .. (564.14,185) .. controls (566.9,185) and (569.14,182.76) .. (569.14,180) -- cycle ;
		%Shape: Ellipse [id:dp21302788662724814] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (577.33,182.33) .. controls (577.33,180.86) and (576.14,179.67) .. (574.67,179.67) .. controls (573.19,179.67) and (572,180.86) .. (572,182.33) .. controls (572,183.81) and (573.19,185) .. (574.67,185) .. controls (576.14,185) and (577.33,183.81) .. (577.33,182.33) -- cycle ;
		%Straight Lines [id:da5813607150346494] 
		\draw    (574.67,182.33) -- (587.5,182.33) ;
		\draw [shift={(590.5,182.33)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		
		%Rounded Rect [id:dp44475153008232815] 
		\draw   (72,272.3) .. controls (72,259.43) and (82.43,249) .. (95.3,249) -- (577.2,249) .. controls (590.07,249) and (600.5,259.43) .. (600.5,272.3) -- (600.5,445.7) .. controls (600.5,458.57) and (590.07,469) .. (577.2,469) -- (95.3,469) .. controls (82.43,469) and (72,458.57) .. (72,445.7) -- cycle ;
		%Rounded Same Side Corner Rect [id:dp01402197265201588] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (72,262.08) .. controls (72,254.86) and (77.86,249) .. (85.08,249) -- (587.42,249) .. controls (594.64,249) and (600.5,254.86) .. (600.5,262.08) -- (600.5,287) .. controls (600.5,287) and (600.5,287) .. (600.5,287) -- (72,287) .. controls (72,287) and (72,287) .. (72,287) -- cycle ;
		%Straight Lines [id:da4213007807696467] 
		\draw    (122,334) -- (236.5,334) ;
		\draw [shift={(236.5,334)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (0,11.18) -- (0,-11.18)   ;
		\draw [shift={(122,334)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (0,11.18) -- (0,-11.18)   ;
		%Shape: Diamond [id:dp11370545129346454] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (179.25,327.5) -- (185.75,334) -- (179.25,340.5) -- (172.75,334) -- cycle ;
		%Straight Lines [id:da3881074290742075] 
		\draw    (429,335) -- (543.5,335) ;
		\draw [shift={(543.5,335)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (0,11.18) -- (0,-11.18)   ;
		\draw [shift={(429,335)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (0,11.18) -- (0,-11.18)   ;
		%Shape: Diamond [id:dp9146167730880479] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (486.25,328.5) -- (492.75,335) -- (486.25,341.5) -- (479.75,335) -- cycle ;
		%Shape: Ellipse [id:dp6089863129224486] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (268,387.5) .. controls (268,372.31) and (277.07,360) .. (288.25,360) .. controls (299.43,360) and (308.5,372.31) .. (308.5,387.5) .. controls (308.5,402.69) and (299.43,415) .. (288.25,415) .. controls (277.07,415) and (268,402.69) .. (268,387.5) -- cycle ;
		%Shape: Circle [id:dp2961151400541908] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (224.16,394.5) .. controls (224.16,383.18) and (233.34,374) .. (244.66,374) .. controls (255.98,374) and (265.16,383.18) .. (265.16,394.5) .. controls (265.16,405.82) and (255.98,415) .. (244.66,415) .. controls (233.34,415) and (224.16,405.82) .. (224.16,394.5) -- cycle ;
		%Shape: Circle [id:dp7623416273855765] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (187.3,398) .. controls (187.3,388.61) and (194.91,381) .. (204.3,381) .. controls (213.69,381) and (221.3,388.61) .. (221.3,398) .. controls (221.3,407.39) and (213.69,415) .. (204.3,415) .. controls (194.91,415) and (187.3,407.39) .. (187.3,398) -- cycle ;
		%Shape: Circle [id:dp5106514740558328] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (155.44,400.5) .. controls (155.44,392.49) and (161.93,386) .. (169.94,386) .. controls (177.95,386) and (184.44,392.49) .. (184.44,400.5) .. controls (184.44,408.51) and (177.95,415) .. (169.94,415) .. controls (161.93,415) and (155.44,408.51) .. (155.44,400.5) -- cycle ;
		%Shape: Circle [id:dp9857854534904193] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (132.58,405) .. controls (132.58,399.48) and (137.06,395) .. (142.58,395) .. controls (148.1,395) and (152.58,399.48) .. (152.58,405) .. controls (152.58,410.52) and (148.1,415) .. (142.58,415) .. controls (137.06,415) and (132.58,410.52) .. (132.58,405) -- cycle ;
		%Shape: Circle [id:dp775472846062693] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (116.72,408.5) .. controls (116.72,404.91) and (119.63,402) .. (123.22,402) .. controls (126.81,402) and (129.72,404.91) .. (129.72,408.5) .. controls (129.72,412.09) and (126.81,415) .. (123.22,415) .. controls (119.63,415) and (116.72,412.09) .. (116.72,408.5) -- cycle ;
		%Shape: Circle [id:dp06711616179307356] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (103.86,410) .. controls (103.86,407.24) and (106.1,405) .. (108.86,405) .. controls (111.62,405) and (113.86,407.24) .. (113.86,410) .. controls (113.86,412.76) and (111.62,415) .. (108.86,415) .. controls (106.1,415) and (103.86,412.76) .. (103.86,410) -- cycle ;
		%Shape: Circle [id:dp20345364685277323] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (95.67,412.33) .. controls (95.67,410.86) and (96.86,409.67) .. (98.33,409.67) .. controls (99.81,409.67) and (101,410.86) .. (101,412.33) .. controls (101,413.81) and (99.81,415) .. (98.33,415) .. controls (96.86,415) and (95.67,413.81) .. (95.67,412.33) -- cycle ;
		%Straight Lines [id:da13468909601557066] 
		\draw    (98.33,412.33) -- (85.5,412.33) ;
		\draw [shift={(82.5,412.33)}, rotate = 360] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		
		%Shape: Ellipse [id:dp6454623857569486] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (405,387.5) .. controls (405,372.31) and (395.93,360) .. (384.75,360) .. controls (373.57,360) and (364.5,372.31) .. (364.5,387.5) .. controls (364.5,402.69) and (373.57,415) .. (384.75,415) .. controls (395.93,415) and (405,402.69) .. (405,387.5) -- cycle ;
		%Shape: Circle [id:dp42395873379034343] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (448.84,394.5) .. controls (448.84,383.18) and (439.66,374) .. (428.34,374) .. controls (417.02,374) and (407.84,383.18) .. (407.84,394.5) .. controls (407.84,405.82) and (417.02,415) .. (428.34,415) .. controls (439.66,415) and (448.84,405.82) .. (448.84,394.5) -- cycle ;
		%Shape: Circle [id:dp14006941915437743] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (485.7,398) .. controls (485.7,388.61) and (478.09,381) .. (468.7,381) .. controls (459.31,381) and (451.7,388.61) .. (451.7,398) .. controls (451.7,407.39) and (459.31,415) .. (468.7,415) .. controls (478.09,415) and (485.7,407.39) .. (485.7,398) -- cycle ;
		%Shape: Circle [id:dp758800909986544] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (517.56,400.5) .. controls (517.56,392.49) and (511.07,386) .. (503.06,386) .. controls (495.05,386) and (488.56,392.49) .. (488.56,400.5) .. controls (488.56,408.51) and (495.05,415) .. (503.06,415) .. controls (511.07,415) and (517.56,408.51) .. (517.56,400.5) -- cycle ;
		%Shape: Ellipse [id:dp7308328573362559] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (540.42,405) .. controls (540.42,399.48) and (535.94,395) .. (530.42,395) .. controls (524.9,395) and (520.42,399.48) .. (520.42,405) .. controls (520.42,410.52) and (524.9,415) .. (530.42,415) .. controls (535.94,415) and (540.42,410.52) .. (540.42,405) -- cycle ;
		%Shape: Circle [id:dp6384968581326209] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (556.28,408.5) .. controls (556.28,404.91) and (553.37,402) .. (549.78,402) .. controls (546.19,402) and (543.28,404.91) .. (543.28,408.5) .. controls (543.28,412.09) and (546.19,415) .. (549.78,415) .. controls (553.37,415) and (556.28,412.09) .. (556.28,408.5) -- cycle ;
		%Shape: Circle [id:dp49611072586102845] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (569.14,410) .. controls (569.14,407.24) and (566.9,405) .. (564.14,405) .. controls (561.38,405) and (559.14,407.24) .. (559.14,410) .. controls (559.14,412.76) and (561.38,415) .. (564.14,415) .. controls (566.9,415) and (569.14,412.76) .. (569.14,410) -- cycle ;
		%Shape: Ellipse [id:dp013067945455296659] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (577.33,412.33) .. controls (577.33,410.86) and (576.14,409.67) .. (574.67,409.67) .. controls (573.19,409.67) and (572,410.86) .. (572,412.33) .. controls (572,413.81) and (573.19,415) .. (574.67,415) .. controls (576.14,415) and (577.33,413.81) .. (577.33,412.33) -- cycle ;
		%Straight Lines [id:da43723715269799945] 
		\draw    (574.67,412.33) -- (587.5,412.33) ;
		\draw [shift={(590.5,412.33)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		
		
		% Text Node
		\draw (161,28) node [anchor=north west][inner sep=0.75pt]  [font=\large] [align=left] {\begin{minipage}[lt]{250pt}\setlength\topsep{0pt}
		\begin{center}
		\textbf{\textcolor[rgb]{1,1,1}{Relative risk, odds ratio or hazard ratio}}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (297,62) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{No effect}};
		% Text Node
		\draw (153,69) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 95\%$ CI};
		% Text Node
		\draw (460,70) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 95\%$ CI};
		% Text Node
		\draw (327,94.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (114,194) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle p$-value gets \textbf{\textit{smaller}} as\\CI moves further \textbf{\textit{below }}$\displaystyle 1$};
		% Text Node
		\draw (397,193) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle p$-value gets \textbf{\textit{smaller}} as\\CI moves further \textbf{above\textit{ }}$\displaystyle 1$};
		% Text Node
		\draw (98,258) node [anchor=north west][inner sep=0.75pt]  [font=\large] [align=left] {\begin{minipage}[lt]{340pt}\setlength\topsep{0pt}
		\begin{center}
		\textbf{\textcolor[rgb]{1,1,1}{Risk difference, percentage in risk, mean or difference}}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (297,292) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{No effect}};
		% Text Node
		\draw (153,299) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 95\%$ CI};
		% Text Node
		\draw (460,300) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 95\%$ CI};
		% Text Node
		\draw (327,324.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (114,424) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle p$-value gets \textbf{\textit{smaller}} as\\CI moves further \textbf{\textit{below }}$\displaystyle 0$};
		% Text Node
		\draw (397,423) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle p$-value gets \textbf{\textit{smaller}} as\\CI moves further \textbf{above\textit{ }}$\displaystyle 0$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Risk Ratio (relative risk), Odds Ratio or Hazard Ratio rules}
	\end{figure} 
	So far we have seen that in calculation of patient risks there are two indicators. But that's quite wrong in practice... Indeed, it is quite common to find the following indicators in many scientific papers:
	\begin{itemize}
		\item Odds ratio:
		
	
		\item Relative risk (risk ratio):
		
	
		\item Experimental event rate (EER):
		
	
		\item Control event rate (CER):
		
	
		\item Absolute risk increase (also named "attributable risk"):
		
	
		\item Absolute risk reduction:
		
	
		\item Number needed to harm:
		
	\end{itemize}
	It should be noticed that any scientific paper in clinical research should communicate at least the Risk Ratio (RR), the Odds Ratio (OR) the Absolute Risk Reduction (ARR) and their respective confidence intervals at a given threshold! We have already derived how to compute the confidence interval for the RR and OR. But we didn't for ARR. So let's do it! 
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\
	\begin{itemize}
		\item Cancer risk for smokers:
		$$\dfrac{10}{30}=0.3\bar{3}$$
		
		\item Cancer risk for non-smokers:
		$$\dfrac{1}{30}=0.03\bar{3}$$
		
		\item Risk difference\index{risk difference}:
		$$\widehat{\text{R.D.}}=\dfrac{10}{30}-\dfrac{1}{30}=0.3$$
		
		\item Relative risk (risk ratio) for the smokers:
		$$\widehat{\text{R.R}}=\dfrac{10/30}{1/30}=10$$
		
		\item Relative risk (risk ratio) for the non-smokers:
		$$\widehat{\text{R.R}}=\dfrac{1/30}{10/30}=0.1$$
		
		\item Relative risk (risk ratio) reduction for the non-smokers ($\widehat{\text{R.R}}=0.1$):
		$$\widehat{\text{R.R.R}}=100\cdot(1-\widehat{\text{R.R}})=100\cdot(1-0.1)=90\%$$
		
		\item Number need to treat (number of people who must be treated to avoid a case of bad outcome):
		$$\widehat{\text{N.D.T.}}=\dfrac{1}{0.3}=3.\bar{3}$$
	\end{itemize}
	\end{tcolorbox}
	
	It is obvious that C.E.R and E.E.R. are binomial random variables that belong to the range $[0,1]$ and furthermore independent (i.e. without correlation) by construction of the experimental design. Therefore the variance of A.R.R. is the sum of the variance of two independent binomial variables. Sadly there are eleven methods to calculate such an interval (...). The most common and naive one for two binomial independent random variable is given by (\SeeChapter{see section Statistics page \pageref{Wald interval}}):
	
	And keep always in mind what we already mentioned during our study of Chemistry:
	\begin{fquote}[Paracelsus]All things are poisons, for there is nothing without poisonous qualities. It is only the dose which makes a thing poison!
 	\end{fquote}
 	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	A famous example of personal patient risk appreciation is about industrial drugs or medicines usage risk! Indeed, it seems that on the $18,000$ drugs commercially released this last century around the World only approximately $177$ of them (source: \url{https://en.wikipedia.org/wiki/List_of_withdrawn_drugs}) were withdrawn because of risks for patients, but also because of commercial reasons (e.g. lack of demand and relatively high production costs).
	\end{tcolorbox}
	
	Let us take an example taken during the COVID-19 pandemic published the 2021-04-03 by FranceInfo (french news channel): According to the numbers that the EMA unveiled on Wednesday, there were at this stage $62$ cases of cerebral venous thrombosis in the world, including $44$ in the $30$ countries of the European Economic Area (EU, Iceland, Norway, Liechtenstein) for $9.2$ million doses of vaccine administered. $9$ deaths have been recorded in Germany out of $2.8$ millions AstraZeneca vaccine injected. In France $4$ death have been reported out of $1.9$ million injections, $3$ death cases in Norway out of $120,000$ injections in the Netherlands. But as with any drug, knowing the risk is not enough, it must be compared with the benefits provided by the product. The Risk Ratio of the AstraZeneca vaccine in preventing COVID-19, which leads to hospitalizations and much more deaths, obviously outweigh the risks of side effects.
	
	The fact that a fraction of the population doesn't know and doesn't understand (or don't want because they have a political or economic agenda) the concept of risk benefit ratio in the case of vaccines, medicines or any activity/action in daily life isn't something new as we can see it from the screenshot below of a 11975 (holocene calendar) french book where the author (which was an artist and evidence show us that we can't expect obviously any scientific literacy from the huge majority of people working in the artistic field) argue because some people died in their car because they had put their security belt is the proof that there shouldn't be any law to oblige people to put the security belt (...):
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.9\textwidth]{img/computing/ceinture.png}
	\end{figure}
	
	\paragraph{ROC and Lift curves}\label{ROC and Lift curves}\mbox{}\\\\
	ROC (Receiver Operating Characteristic) curve and Lift chart are techniques for visualizing, organizing, improving and selecting classifiers based on their performance. They facilitate our conception of classifiers and are therefore useful in research and in result presentation.

	ROC curve is a plot of the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. It is used to evaluate the performance of a binary classifier.

	Lift chart is a measure of the effectiveness of a predictive model. It compares the results of the model against a random guess and tells us how much better we are doing than random guessing.

	\subparagraph{ROC curve}\mbox{}\\\\
	Classifier  performance evaluation should consider all the score range instead of a single point, unless specified by the application domain requirements. That  is  probably  the  reason  why  the  area  under  the  curve (AUC) of the Receiver  Operating  Characteristics (ROC) became so popular among scientists for binary    classification  quality assessment, instead of confusion  matrix, error  rates, maximum vertical distance of the ROC curve, maximum  Kolmogorov-Smirnov  vertical  difference and all other single point specific metrics that are useful for the human brain but not very reliable.
	
	The "\NewTerm{ROC curve}\index{ROC curve}", that stands for "\NewTerm{Receiver Operating Characteristic curve}\index{ROC curve}", is a measure of the performance of a binary classifier, that is to say a system that aims to categorize entities in two distinct groups on the basis of one or more of their characteristics. 

	Graphically, we often represents the ROC measurement  in the form of a curve which gives the true positive rate (sensitivity: fraction of positives that are correctly detected) according to the rate of false positive rate (proportion of negatives which are incorrectly detected) for the same group. ROC curves are often used in statistics to show the progress realized with a binary classifier when "\NewTerm{discrimination threshold}\index{discrimination threshold}" (cutoff) varies.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	We present this tool because many statistical software return automatically a ROC curve as output but the interest of this tool is in my personal opinion very questionable. However, we will show a more useful tool right after.
	\end{tcolorbox}
	Let us see practical case by taking our previous example (the Bank and its subsidiaries) with the spreadsheet software Microsoft Excel 14.0.7106. First to build the ROC curve we must obtain the confusion matrix $\mathcal{C}$ that we presented just a little higher with Minitab. For this with a spreadsheet software and without doing code, here is a simple solution (but this is not the more condensed solution for pedagogical reasons).
	
	The goal will be first to build the following table (this is only a part of the table of the previous example since in reality there are 136 rows of data) whose columns \texttt{B} and \texttt{C} comes from the three small tables of Credits used just before, column \texttt{A} is just the cumulative frequency of individuals $1/136$ ... $2/136$ ... $3/136$ ... and so on):
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/roc_curve_excel_data.jpg}
		\caption[]{List to obtain in Microsoft Excel the ROC curve}
	\end{figure}
	Now let us see what is in columns \texttt{D}, \texttt{E} and \texttt{F} and that are directly related to the result obtained above, which was for recall:
	
	but that we have refined with a specialized statistical software to obtain:
	
	which then allows us to build the three previous mentioned columns (here we have only a few of the first rows because it is sufficient to just increment the formulas to the bottom of the list):
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/roc_curve_excel_model_and_score.jpg}
		\caption[]{Spreadsheet formulas for the probability of the model and the score}
	\end{figure}
	The formula in column \texttt{F} refers to the cell \texttt{M3} which as we shall see a little further contains the empirical choice of the value of the cutoff we had mentioned during the presentation of the theoretical model of the logistics regression (by default we defined it at $50\%$).
	
	Then we must build the columns of true positive, false positive, true negative, false negatives using basic spreadsheet formulas:
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/roc_curve_excel_positive_and_negative.jpg}
		\caption[]{Spreadsheet formulas for true / false positives and negatives}
	\end{figure}
	Once we have this data, we can rebuild the confusion matrix $\mathcal{C}$ that was given to us by Minitab and we will need to develop the ROC curve:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/confusion_matrix_formulas.jpg}
		\caption[]{Spreadsheet formulas to build the confusion matrix}
	\end{figure}
	which explicitly gives:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/confusion_matrix_values.jpg}
		\caption{Corresponding values of the construction of the confusion matrix}
	\end{figure}
	compared with the confusion matrix that we gave and coming from Minitab:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/confusion_matrix.jpg}
		\caption{Confusion matrix with Minitab 16.1.1}
	\end{figure}
	Now observe the ROC curve given by Minitab:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/minitab_roc_curve.jpg}
		\caption{ROC curve with Minitab 16.1.1}
	\end{figure}
	Why the abscissa-represents:
	\begin{gather*}
		1-\%\text{false positives}
	\end{gather*}
	you will probably ask yourself when the interpretation would have been easier if we had just the:
	\begin{gather*}
		\%\text{false positives}
	\end{gather*}
	Well for two reasons: the first is that practitioners like strictly increasing functions... and the second reason, the most important, is that if the binary classifier is fully effective, the surface is then equal to 1$ $(that is to say, $100\%$). Which admittedly is nicer than to say that a null surface corresponds to a $100\%$ efficiency. Well that being said ... let us continue.
	
	Before learning how to interpret this chart, how to get the same curve in a spreadsheet software? Well just simply observe our column \texttt{D} of our Microsoft Excel list. Logically we have there $5$ different cumulative probabilities (as there were only $5$ credits values) which are respectively:
	\begin{gather*}
		\begin{aligned}
		0.235214057\\
		0.481608302\\
		0.777818743\\
		0.813679766\\
		0.991760974
		\end{aligned}
	\end{gather*}
	Therefore the idea of the construction of the ROC curve is to take each of the accumulated probabilities as cutt-off values. This gives respectively:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/confusion_matrix_cutoff1.jpg}
		\includegraphics{img/computing/confusion_matrix_cutoff2.jpg}
		\includegraphics{img/computing/confusion_matrix_cutoff3.jpg}
		\includegraphics{img/computing/confusion_matrix_cutoff4.jpg}
		\includegraphics{img/computing/confusion_matrix_cutoff5.jpg}
		\caption{Values of the confusion matrix for various cutoff values}
	\end{figure}
	So we get each time the coordinates of points of the ROC curve in function of cutoff values. Well this done, now let us turn to the interpretation by taking the below manually completed graph:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/minitab_roc_curve_completed.jpg}
		\caption[ROC curves]{ROC curves\\ (blue: average classifier, red: good classifier, green: perfect classifier, black: poor classifier)}
	\end{figure}
	\begin{table}[H]
		\resizebox{\textwidth}{!}{
		\begin{tabular}{|c|c|c|c|c|}
		\hline
		\rowcolor[HTML]{C0C0C0} 
		\textbf{Credit Amount} & \textbf{Proportion $P$} & \textbf{Cutt-Off Value} & \textbf{$x$ Coordinate of  ROC Curve point} & \textbf{$y$ Coordinate of ROC Curve point} \\ \hline
		 &  &  & $0$ & $0$ \\ \hline
		$27,200$ & $0.0741$ & $23.53\%$ & $0$ & $0$ \\ \hline
		$27,700$ & $0.7083$ & $48.20\%$ & $0.022$ & $0.297$ \\ \hline
		$28,300$ & $0.8667$ & $77.80\%$ & $0.200$ & $0.505$ \\ \hline
		$28,400$ & $0.7037$ & $81.40\%$ & $0.289$ & $0.791$ \\ \hline
		$29,900$ & $0.9643$ & $99.2\%$ & $0.444$ & $0.978$ \\ \hline
		 &  &  & $1$ & $1$ \\ \hline
		\end{tabular}
		}
	\end{table}
	Then as shown in the graph above and the intuition, a perfect binary classifier is one whose true positive rate is constant and always equal to $100\%$. So the binary classifier in our example is moderately good.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The biggest vertical distance between the black line in the figure above and the blue ROC curved is sometimes used as a quality metric for classification named the "\NewTerm{maximum vertical distance}\index{maximum vertical distance}" and abbreviated MVD.
	\end{tcolorbox}
	
	But from my personal point of view a good binary classifier is one that for a given value of the cutoff maximizes the sum of the rate of true positives and true negatives. Thus, with a spreadsheet software such as Microsoft Excel in "evolutionary mode" it is very easy to find this cutoff value that maximizes this objective. But however be careful to the trap: the value may not be unique, it can also be an interval (which is the case in our example!).
	
	Finally, if we denote by TP the true positive, TN the true negative, FP the false positives or Type I error (in terms of decision theory or tests theory), and FN false negative or Type II error we can then define a set of indicators to judge the quality of our predictor/classifier (or rather of our score):
	
	\begin{itemize}
		\item $\text{TPR} = \dfrac{\text{TP}}{\text{P}} = \dfrac{\text{TP}}{\text{TP+FN}}$ named "\NewTerm{sensitivity}\index{classification!sensitivity}", corresponding to the rate of true positives.
		
		\item $\text{RPF} = \dfrac{\text{FP}}{\text{N}} = \dfrac{\text{FP}}{\text{FP+TN}}$ corresponding to the rate of false positives.
		
		\item $\text{ACC} = \dfrac{\text{TP+TN}}{\text{P+N}}$ named "\NewTerm{accuracy}\index{classification!accuracy}" (fraction of instances that are correctly classified).
		
		 \item $\text{SPC} = \dfrac{\text{TN}}{\text{N}} = \dfrac{\text{TN}}{\text{FP+TN}}=1-\text{FPR}$ named "\NewTerm{specificity}\index{classification!specificity}" or true negative rate.
		 
		 \item $\text{PPV} = \dfrac{\text{TP}}{\text{TP+TP}}$ the "\NewTerm{positive predictive value}\index{classification!positive predictive value}".
		 
		 \item $\text{NPV} = \dfrac{\text{TN}}{\text{TN+FN}}$ the "\NewTerm{negative predictive value}\index{classification!negative predictive value}".
		 
		 \item $\text{FDR} = \dfrac{\text{FP}}{\text{FP+TP}}$ corresponding to the  "\NewTerm{false discovery rate}\index{classification!false discovery rate}".
	\end{itemize}
	And we can build many quality metrics like (for the most famous one!) the Matthews correlation coefficient (\SeeChapter{see section Statistics page \pageref{Matthews correlation coefficient}}), the $F_1$ score (see page \pageref{F1 score}) or the "\NewTerm{Rand Index}\index{classification!Rand index}" (also sometimes named "\NewTerm{model accuracy}\index{classification!model accuracy}") defined by:
	
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In practice, the sensitivity and specificity\index{sensitivity and specificity} of a test give an assessment of its intrinsic validity. It is considered in practice that taken separately, they do not mean anything. For example, it should be obvious for the reader a test with $95\%$ sensitivity has no value if its specificity is only $5\%$. It is considered in practice that if the sum of sensitivity and specificity is equal to $100\%$ the test is as good a random.
	\end{tcolorbox}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Consider a disease test sensibility and sensitivity analysis with the following values:
	\begin{table}[H]
		\centering
		\begin{tabular}{ll|c|c|l}
		\cline{3-4}
		 &  & \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}Patients with bowel cancer\\ (as confirmed by endoscopy)\end{tabular}} &  \\ \cline{3-4}
		 &  & \cellcolor[HTML]{EFEFEF}\textbf{Positive} & \cellcolor[HTML]{EFEFEF}\textbf{Negative} &  \\ \hline
		\multicolumn{1}{|l|}{\cellcolor[HTML]{FFFFFF}} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}\textbf{Positive}} & \cellcolor[HTML]{9AFF99}TP$=2$ & \cellcolor[HTML]{FFCCC9}FP$=18$ & \multicolumn{1}{l|}{\parbox{1.9cm}{$\dfrac{\text{TP}}{\text{TP}+\text{FP}}\\=\dfrac{2}{2+18}\\=10\%$}} \\ \cline{2-5} 
		\multicolumn{1}{|l|}{\multirow{-2}{*}{\cellcolor[HTML]{FFFFFF}Disease Test}} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}\textbf{Negative}} & \cellcolor[HTML]{FFCCC9}FN$=1$ & \cellcolor[HTML]{9AFF99}TN$=182$ & \multicolumn{1}{l|}{\parbox{1.9cm}{$\dfrac{\text{TN}}{\text{TN}+\text{FN}}\\=\dfrac{182}{1+182}\\=99.5\%$}} \\ \hline
		 &  & \multicolumn{1}{l|}{\parbox{3.6cm}{Sensitivity\\$=\dfrac{\text{TP}}{\text{TP}+\text{FN}}\\=\dfrac{2}{2+1}=66.\bar{6}\%$}} & \multicolumn{1}{l|}{\parbox{3.6cm}{Specificity\\$=\dfrac{\text{TN}}{\text{FP}+\text{TN}}\\=\dfrac{182}{18+182}=91\%$}} &  \\ \cline{3-4}
		\end{tabular}
		\caption{Example of sensibility and specificity analysis}
	\end{table}
	There are some cases where sensitivity is important and need to be near to $1$. There are business cases where specificity is important and need to be near to $1$ (ideally we want to maximize both sensitivity \& specificity, but this is not always possible always). We need to understand the business problem and decide the importance of sensitivity and specificity.
	\end{tcolorbox}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Current generation HIV tests are considered extremely accurate. The blood-based HIV ELISA has a demonstrated sensitivity of between $99.3\%$  to 99.7 percent, with a specificity of between $99.91\%$ and $99.97\%$. When combined with a Western blot, this translates to approximately one false positive out of every $250,000$ tests in the general U.S. population. Newer, fourth generation combination tests, which test for both HIV antibodies and antigens, are reported to have a clinical sensitivity of $99.9\%$.
	\end{tcolorbox}
	
	As you may have notice in the previous chart above. Below the title of the chart there was a mention of something named "\NewTerm{Area Under the Curve}\index{area under the curve (AUC)}" and that is well known under the abbreviation AUC.

	Obviously, the names mean what it is: It measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from $[0,0]$ to $[1,1]$.

	AUC provides an aggregate measure of performance across all possible classification thresholds. One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example. For example, given the following examples, which are arranged from left to right in ascending order of logistic regression predictions:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,896); %set diagram left start at 0, and has height of 896
		
		%Shape: Circle [id:dp4901702720269272] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (62.33,152.67) .. controls (62.33,150.55) and (64.05,148.83) .. (66.17,148.83) .. controls (68.28,148.83) and (70,150.55) .. (70,152.67) .. controls (70,154.78) and (68.28,156.5) .. (66.17,156.5) .. controls (64.05,156.5) and (62.33,154.78) .. (62.33,152.67) -- cycle ;
		%Shape: Circle [id:dp5952412930597712] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (75.05,152.67) .. controls (75.05,150.55) and (76.77,148.83) .. (78.88,148.83) .. controls (81,148.83) and (82.72,150.55) .. (82.72,152.67) .. controls (82.72,154.78) and (81,156.5) .. (78.88,156.5) .. controls (76.77,156.5) and (75.05,154.78) .. (75.05,152.67) -- cycle ;
		%Shape: Circle [id:dp547992620677243] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (87.77,152.67) .. controls (87.77,150.55) and (89.48,148.83) .. (91.6,148.83) .. controls (93.72,148.83) and (95.43,150.55) .. (95.43,152.67) .. controls (95.43,154.78) and (93.72,156.5) .. (91.6,156.5) .. controls (89.48,156.5) and (87.77,154.78) .. (87.77,152.67) -- cycle ;
		%Shape: Circle [id:dp9391842162630955] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (113.2,152.67) .. controls (113.2,150.55) and (114.92,148.83) .. (117.03,148.83) .. controls (119.15,148.83) and (120.87,150.55) .. (120.87,152.67) .. controls (120.87,154.78) and (119.15,156.5) .. (117.03,156.5) .. controls (114.92,156.5) and (113.2,154.78) .. (113.2,152.67) -- cycle ;
		%Shape: Circle [id:dp01961107480805291] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (100.48,152.67) .. controls (100.48,150.55) and (102.2,148.83) .. (104.32,148.83) .. controls (106.43,148.83) and (108.15,150.55) .. (108.15,152.67) .. controls (108.15,154.78) and (106.43,156.5) .. (104.32,156.5) .. controls (102.2,156.5) and (100.48,154.78) .. (100.48,152.67) -- cycle ;
		%Shape: Circle [id:dp8726611589717539] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (125.92,152.67) .. controls (125.92,150.55) and (127.63,148.83) .. (129.75,148.83) .. controls (131.87,148.83) and (133.58,150.55) .. (133.58,152.67) .. controls (133.58,154.78) and (131.87,156.5) .. (129.75,156.5) .. controls (127.63,156.5) and (125.92,154.78) .. (125.92,152.67) -- cycle ;
		%Shape: Circle [id:dp2587686923804291] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (138.63,152.67) .. controls (138.63,150.55) and (140.35,148.83) .. (142.47,148.83) .. controls (144.58,148.83) and (146.3,150.55) .. (146.3,152.67) .. controls (146.3,154.78) and (144.58,156.5) .. (142.47,156.5) .. controls (140.35,156.5) and (138.63,154.78) .. (138.63,152.67) -- cycle ;
		%Shape: Circle [id:dp2327003182708618] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (151.35,152.67) .. controls (151.35,150.55) and (153.07,148.83) .. (155.18,148.83) .. controls (157.3,148.83) and (159.02,150.55) .. (159.02,152.67) .. controls (159.02,154.78) and (157.3,156.5) .. (155.18,156.5) .. controls (153.07,156.5) and (151.35,154.78) .. (151.35,152.67) -- cycle ;
		%Shape: Circle [id:dp9473353674870999] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (164.07,152.67) .. controls (164.07,150.55) and (165.78,148.83) .. (167.9,148.83) .. controls (170.02,148.83) and (171.73,150.55) .. (171.73,152.67) .. controls (171.73,154.78) and (170.02,156.5) .. (167.9,156.5) .. controls (165.78,156.5) and (164.07,154.78) .. (164.07,152.67) -- cycle ;
		%Shape: Circle [id:dp5421781630075269] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (176.78,152.67) .. controls (176.78,150.55) and (178.5,148.83) .. (180.62,148.83) .. controls (182.73,148.83) and (184.45,150.55) .. (184.45,152.67) .. controls (184.45,154.78) and (182.73,156.5) .. (180.62,156.5) .. controls (178.5,156.5) and (176.78,154.78) .. (176.78,152.67) -- cycle ;
		%Shape: Circle [id:dp35409302578638036] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (189.5,152.67) .. controls (189.5,150.55) and (191.22,148.83) .. (193.33,148.83) .. controls (195.45,148.83) and (197.17,150.55) .. (197.17,152.67) .. controls (197.17,154.78) and (195.45,156.5) .. (193.33,156.5) .. controls (191.22,156.5) and (189.5,154.78) .. (189.5,152.67) -- cycle ;
		%Shape: Circle [id:dp6851657548050492] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (202.22,152.67) .. controls (202.22,150.55) and (203.93,148.83) .. (206.05,148.83) .. controls (208.17,148.83) and (209.88,150.55) .. (209.88,152.67) .. controls (209.88,154.78) and (208.17,156.5) .. (206.05,156.5) .. controls (203.93,156.5) and (202.22,154.78) .. (202.22,152.67) -- cycle ;
		%Shape: Circle [id:dp6181119277272658] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (214.93,152.67) .. controls (214.93,150.55) and (216.65,148.83) .. (218.77,148.83) .. controls (220.88,148.83) and (222.6,150.55) .. (222.6,152.67) .. controls (222.6,154.78) and (220.88,156.5) .. (218.77,156.5) .. controls (216.65,156.5) and (214.93,154.78) .. (214.93,152.67) -- cycle ;
		%Shape: Circle [id:dp04022546303248209] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (227.65,152.67) .. controls (227.65,150.55) and (229.37,148.83) .. (231.48,148.83) .. controls (233.6,148.83) and (235.32,150.55) .. (235.32,152.67) .. controls (235.32,154.78) and (233.6,156.5) .. (231.48,156.5) .. controls (229.37,156.5) and (227.65,154.78) .. (227.65,152.67) -- cycle ;
		%Shape: Circle [id:dp6348611017888648] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (240.37,152.67) .. controls (240.37,150.55) and (242.08,148.83) .. (244.2,148.83) .. controls (246.32,148.83) and (248.03,150.55) .. (248.03,152.67) .. controls (248.03,154.78) and (246.32,156.5) .. (244.2,156.5) .. controls (242.08,156.5) and (240.37,154.78) .. (240.37,152.67) -- cycle ;
		%Shape: Circle [id:dp5532594896704657] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (265.8,152.67) .. controls (265.8,150.55) and (267.52,148.83) .. (269.63,148.83) .. controls (271.75,148.83) and (273.47,150.55) .. (273.47,152.67) .. controls (273.47,154.78) and (271.75,156.5) .. (269.63,156.5) .. controls (267.52,156.5) and (265.8,154.78) .. (265.8,152.67) -- cycle ;
		%Shape: Circle [id:dp8939330675784376] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (291.23,152.67) .. controls (291.23,150.55) and (292.95,148.83) .. (295.07,148.83) .. controls (297.18,148.83) and (298.9,150.55) .. (298.9,152.67) .. controls (298.9,154.78) and (297.18,156.5) .. (295.07,156.5) .. controls (292.95,156.5) and (291.23,154.78) .. (291.23,152.67) -- cycle ;
		%Shape: Circle [id:dp8816162030155743] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (316.67,152.67) .. controls (316.67,150.55) and (318.38,148.83) .. (320.5,148.83) .. controls (322.62,148.83) and (324.33,150.55) .. (324.33,152.67) .. controls (324.33,154.78) and (322.62,156.5) .. (320.5,156.5) .. controls (318.38,156.5) and (316.67,154.78) .. (316.67,152.67) -- cycle ;
		%Shape: Circle [id:dp5075562165080121] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (342.1,152.67) .. controls (342.1,150.55) and (343.82,148.83) .. (345.93,148.83) .. controls (348.05,148.83) and (349.77,150.55) .. (349.77,152.67) .. controls (349.77,154.78) and (348.05,156.5) .. (345.93,156.5) .. controls (343.82,156.5) and (342.1,154.78) .. (342.1,152.67) -- cycle ;
		%Shape: Circle [id:dp11754557188030268] 
		\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (253.08,152.67) .. controls (253.08,150.55) and (254.8,148.83) .. (256.92,148.83) .. controls (259.03,148.83) and (260.75,150.55) .. (260.75,152.67) .. controls (260.75,154.78) and (259.03,156.5) .. (256.92,156.5) .. controls (254.8,156.5) and (253.08,154.78) .. (253.08,152.67) -- cycle ;
		%Shape: Circle [id:dp20773645726800138] 
		\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (278.52,152.67) .. controls (278.52,150.55) and (280.23,148.83) .. (282.35,148.83) .. controls (284.47,148.83) and (286.18,150.55) .. (286.18,152.67) .. controls (286.18,154.78) and (284.47,156.5) .. (282.35,156.5) .. controls (280.23,156.5) and (278.52,154.78) .. (278.52,152.67) -- cycle ;
		%Shape: Circle [id:dp8109986989471543] 
		\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (303.95,152.67) .. controls (303.95,150.55) and (305.67,148.83) .. (307.78,148.83) .. controls (309.9,148.83) and (311.62,150.55) .. (311.62,152.67) .. controls (311.62,154.78) and (309.9,156.5) .. (307.78,156.5) .. controls (305.67,156.5) and (303.95,154.78) .. (303.95,152.67) -- cycle ;
		%Shape: Circle [id:dp4642144705853022] 
		\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (329.38,152.67) .. controls (329.38,150.55) and (331.1,148.83) .. (333.22,148.83) .. controls (335.33,148.83) and (337.05,150.55) .. (337.05,152.67) .. controls (337.05,154.78) and (335.33,156.5) .. (333.22,156.5) .. controls (331.1,156.5) and (329.38,154.78) .. (329.38,152.67) -- cycle ;
		%Shape: Circle [id:dp7980485829934745] 
		\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (354.82,152.67) .. controls (354.82,150.55) and (356.53,148.83) .. (358.65,148.83) .. controls (360.77,148.83) and (362.48,150.55) .. (362.48,152.67) .. controls (362.48,154.78) and (360.77,156.5) .. (358.65,156.5) .. controls (356.53,156.5) and (354.82,154.78) .. (354.82,152.67) -- cycle ;
		%Shape: Circle [id:dp5652589750606174] 
		\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (367.53,152.67) .. controls (367.53,150.55) and (369.25,148.83) .. (371.37,148.83) .. controls (373.48,148.83) and (375.2,150.55) .. (375.2,152.67) .. controls (375.2,154.78) and (373.48,156.5) .. (371.37,156.5) .. controls (369.25,156.5) and (367.53,154.78) .. (367.53,152.67) -- cycle ;
		%Shape: Circle [id:dp4024510004964661] 
		\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (380.25,152.67) .. controls (380.25,150.55) and (381.97,148.83) .. (384.08,148.83) .. controls (386.2,148.83) and (387.92,150.55) .. (387.92,152.67) .. controls (387.92,154.78) and (386.2,156.5) .. (384.08,156.5) .. controls (381.97,156.5) and (380.25,154.78) .. (380.25,152.67) -- cycle ;
		%Shape: Circle [id:dp5767157991708625] 
		\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (392.97,152.67) .. controls (392.97,150.55) and (394.68,148.83) .. (396.8,148.83) .. controls (398.92,148.83) and (400.63,150.55) .. (400.63,152.67) .. controls (400.63,154.78) and (398.92,156.5) .. (396.8,156.5) .. controls (394.68,156.5) and (392.97,154.78) .. (392.97,152.67) -- cycle ;
		%Shape: Circle [id:dp16104563756865886] 
		\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (405.68,152.67) .. controls (405.68,150.55) and (407.4,148.83) .. (409.52,148.83) .. controls (411.63,148.83) and (413.35,150.55) .. (413.35,152.67) .. controls (413.35,154.78) and (411.63,156.5) .. (409.52,156.5) .. controls (407.4,156.5) and (405.68,154.78) .. (405.68,152.67) -- cycle ;
		%Shape: Circle [id:dp4211084195939603] 
		\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (418.4,152.67) .. controls (418.4,150.55) and (420.12,148.83) .. (422.23,148.83) .. controls (424.35,148.83) and (426.07,150.55) .. (426.07,152.67) .. controls (426.07,154.78) and (424.35,156.5) .. (422.23,156.5) .. controls (420.12,156.5) and (418.4,154.78) .. (418.4,152.67) -- cycle ;
		%Shape: Circle [id:dp816479678637976] 
		\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (431,152.67) .. controls (431,150.55) and (432.72,148.83) .. (434.83,148.83) .. controls (436.95,148.83) and (438.67,150.55) .. (438.67,152.67) .. controls (438.67,154.78) and (436.95,156.5) .. (434.83,156.5) .. controls (432.72,156.5) and (431,154.78) .. (431,152.67) -- cycle ;
		%Straight Lines [id:da4354363911640382] 
		\draw    (50.67,179.67) -- (448.17,179.67) ;
		%Straight Lines [id:da5627185193261877] 
		\draw    (50,160.33) -- (50,198.67) ;
		%Straight Lines [id:da40254864967927184] 
		\draw    (449,160.33) -- (449,198.67) ;
		%Shape: Circle [id:dp013538211973771253] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (479.1,154.67) .. controls (479.1,152.55) and (480.82,150.83) .. (482.93,150.83) .. controls (485.05,150.83) and (486.77,152.55) .. (486.77,154.67) .. controls (486.77,156.78) and (485.05,158.5) .. (482.93,158.5) .. controls (480.82,158.5) and (479.1,156.78) .. (479.1,154.67) -- cycle ;
		%Shape: Circle [id:dp9066098121087232] 
		\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (479.1,174.67) .. controls (479.1,172.55) and (480.82,170.83) .. (482.93,170.83) .. controls (485.05,170.83) and (486.77,172.55) .. (486.77,174.67) .. controls (486.77,176.78) and (485.05,178.5) .. (482.93,178.5) .. controls (480.82,178.5) and (479.1,176.78) .. (479.1,174.67) -- cycle ;
		
		% Text Node
		\draw (59.67,158.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {N};
		% Text Node
		\draw (72.72,158.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {N};
		% Text Node
		\draw (85.77,158.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {N};
		% Text Node
		\draw (97.82,158.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {N};
		% Text Node
		\draw (109.87,158.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {N};
		% Text Node
		\draw (122.92,158.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {N};
		% Text Node
		\draw (135.97,158.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {N};
		% Text Node
		\draw (149.02,158.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {N};
		% Text Node
		\draw (161.07,158.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {N};
		% Text Node
		\draw (174.12,158.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {N};
		% Text Node
		\draw (186.5,158.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {N};
		% Text Node
		\draw (199.22,158.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {N};
		% Text Node
		\draw (211.93,158.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {N};
		% Text Node
		\draw (225.65,158.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {N};
		% Text Node
		\draw (237.37,158.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {N};
		% Text Node
		\draw (251.08,158.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {P};
		% Text Node
		\draw (300.95,158.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {P};
		% Text Node
		\draw (262.8,158.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {N};
		% Text Node
		\draw (276.52,158.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {P};
		% Text Node
		\draw (288.23,158.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {N};
		% Text Node
		\draw (313.67,158.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {N};
		% Text Node
		\draw (327.38,158.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {P};
		% Text Node
		\draw (340.1,158.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {N};
		% Text Node
		\draw (352.82,158.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {P};
		% Text Node
		\draw (365.87,158.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {P};
		% Text Node
		\draw (377.92,158.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {P};
		% Text Node
		\draw (389.97,158.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {P};
		% Text Node
		\draw (403.63,158.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {P};
		% Text Node
		\draw (416.35,158.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {P};
		% Text Node
		\draw (428.07,158.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {P};
		% Text Node
		\draw (38.33,199.9) node [anchor=north west][inner sep=0.75pt]    {$0.0$};
		% Text Node
		\draw (438,199.9) node [anchor=north west][inner sep=0.75pt]    {$1.0$};
		% Text Node
		\draw (127.33,186.33) node [anchor=north west][inner sep=0.75pt]   [align=left] {output of logistic regression model};
		% Text Node
		\draw (492,146) node [anchor=north west][inner sep=0.75pt]   [align=left] {actual negative};
		% Text Node
		\draw (493,167) node [anchor=north west][inner sep=0.75pt]   [align=left] {actual positive};
		\end{tikzpicture}
	\end{figure}
	AUC represents the probability that a random positive (green) example is positioned to the right of a random negative (red) example.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	If we have a model with an AUC of $80\%$, it means that if we choose randomly two individuals in the population, one negative and the other positive, the model will classify correctly these two individuals in their respective groups $8$ times on $10$.
	\end{tcolorbox}
	
	The corresponding mathematical expression is quite obviously given by:
	
	Here $i$ runs over all $m$ data points with true label $1,$ and $j$ runs over all $n$ data points with true label $0 ; p_{i}$ and $p_{j}$ denote the probability score assigned by the classifier to data point $i$ and $j$, respectively. $1$ is always the indicator function: it outputs $1$ if and only if the condition ($p_{i}>p_{j}$) is satisfied!
	
	AUC ranges in value from $0$ to $1$. A model whose predictions are $100\%$ wrong has an AUC of $0.0$; one whose predictions are $100\%$ correct has an AUC of $1.0$. A model that doesn't better than chance will have an AUC of less than $50\%$.
	
	Testing the fact evidence that the AUC is significantly different of $0.5$ is like the null hypothesis $H_0$: \textit{The distribution of the ranks in the two groups are equal}. It's then the equivalent of running a Wilcoxon rank sum test (\SeeChapter{see section Statistics page \pageref{Wilcoxon rank sum test}}).
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In logistic regression if we consider then probabilities $\left\lbrace \left(\hat{\pi}(\hat{\vec{\beta}}|\vec{x}_i\right),\hat{\pi}\left(\hat{\vec{\beta}},\vec{x}_j\right)\right\rbrace$, the $C$-index can be defined as:
	
	Which is equivalent to:
	
	\end{tcolorbox}
	
	AUC is desirable for the following two reasons:
	\begin{itemize}
		\item AUC is scale-invariant. It measures how well predictions are ranked, independently of their absolute values.

		\item AUC is classification-threshold-invariant. It measures the quality of the model's predictions irrespective of what classification threshold is chosen.
	\end{itemize}
	Notice that there is a simple relation between the AUC and the Gini index. Indeed, as we have proved it in the section of Quantitative Management (see page \pageref{gini index}), we have:
	
	But as:
	
	We then have:
	
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In multi-class models, we can plot $N$ number of AUC ROC curves for $N$ number classes using the "One vs All methodology". So for example, if you have three classes named $A$, $B$ and $C$, you will have one ROC for $A$ classified against $B$ and $C$, another ROC for $B$ classified against $A$ and $B$, and a third one $C$ classified against $A$ and $B$.
	\end{tcolorbox}
	
	Having done this, let us move to the second curve.
	
	\subparagraph{Lift curve}\mbox{}\\\\
	The principle of the "\NewTerm{lift curve}\index{lift curve}" is very simple but is based on a questionable assumption (which fortunately has not however a great importance) that is that without the theoretical model (logistic model in this case) we will absolutely know nothing of the real behaviour that will have the customers (debtors). The hypothesis is to consider that if we took $50\%$ of the individuals in our sample, we would have $50\%$ of true positives (real bad debtors), if we took $25\%$ of our sample, we would have $25\%$ of true positives (true bad debtors) and so on. This initial hypothesis (considered by practitioners as the worst case) will be represented graphically by the following straight line:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/lift_curve_at_worst.jpg}
		\caption[]{Lift curve at worst (as assumption)}
	\end{figure}
	But that's just to have an empirical reference when working with a single model of statistical clustering. In fact, given the multiplicity of statistical modelling methods, where almost each has its own empirical indicators of quality, statisticians have sought general criteria for the performance of a model (this obviously in the idea to be able to compare the accuracy of various models between them).
	
	Thus, an empirical and intuitive enough choice is to say that a statistical method is better than another if for a given subset of the sample, its predictive power (the cumulative number of true positives for example) is better or not. This is the purpose of the lift curve.
	
	The lift curve is therefore a variation of the ROC curve. The lift curve classify the individuals by descending score (again for reasons of simplification of interpretation of the surface under the curve and especially to have in first the target group of interest to monitor), by grouping them by percentiles for example, determining the percentage of events of interest in each percentile (normally the true positives) and then by plotting the cumulative curve of these percentages, so that a point of the coordinates $(n, m)$ on the curve means that the $n\%$ of individuals with the highest score concentrate $m\%$ of events. This is the way to build this curve which makes we speak of "predictive performance of targeted marketing".
	
	Let's see how to build such a curve, always with the same spreadsheet software and always the same data. The start of construction is to take some columns that we had used for the ROC curve (first 31 rows out of 136):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/computing/lift_curve_basis_formula.jpg}
		\caption[]{Formulas of the first columns in Microsoft Excel to get the lift curve}
	\end{figure}
	where the reader may have noticed that we have arbitrarily chosen to set the cutoff at $50\%$ (thus in reality, and the reader must never forget it, the whole lift curve changes for each value of the cut- off !!!).
	
	Which explicitly provides (remember that column \texttt{A}, that as for the ROC curve is trivial, as its simply represent the cumulative effective):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/computing/lift_curve_values.jpg}
		\caption[]{Explicit value in Microsoft Excel to obtain for the lift curve}
	\end{figure}
	One difference now compared to the ROC curve is that we want the events of interest first (the bad debtors). Then we must sort the column Model (that is to say the: Score) in descending order. This will give:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/computing/lift_curve_values_sorted.jpg}
		\caption[]{Explicit sorted value in Microsoft Excel to obtain for the lift curve}
	\end{figure}
	Then we add the column of \%Cumulative of true positives:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/computing/lift_curve_values_sorted_cumulated_formula.jpg}
		\caption[]{Column of \%Cumulatived of true positives for the lift curve}
	\end{figure}
	What gives globally:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/computing/lift_curve_global_values.jpg}
		\caption{Overview of all constructed columns for the lift curve}
	\end{figure}
	The last step is now simply to build a graph of \%cumulative number of true positive (column G) relative to the \%cumulative of the sample size (column \texttt{A}) to get:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/lift_curve.jpg}
		\caption[]{Lift curve (in red at worst, in blue our model)}
	\end{figure}
	So obviously we can not do in this case an interpretation of the predictive power in terms of "lift" in comparison to another statistical model. By cons we can compare the predictive power of the logistic model used in this case in terms of lift compared to the case considered at worst (the diagonal for reminder ...). So in this case, if we take the $20\%$ of customers (debtors) having the highest score, that's what we have:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1077); %set diagram left start at 0, and has height of 1077
		
		%Straight Lines [id:da14036875476926491] 
		\draw    (144,345) -- (144,60) (140,319) -- (148,319)(140,293) -- (148,293)(140,267) -- (148,267)(140,241) -- (148,241)(140,215) -- (148,215)(140,189) -- (148,189)(140,163) -- (148,163)(140,137) -- (148,137)(140,111) -- (148,111)(140,85) -- (148,85) ;
		\draw [shift={(144,58)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da3776530680791679] 
		\draw    (144,345) -- (527,345) (180,341) -- (180,349)(216,341) -- (216,349)(252,341) -- (252,349)(288,341) -- (288,349)(324,341) -- (324,349)(360,341) -- (360,349)(396,341) -- (396,349)(432,341) -- (432,349)(468,341) -- (468,349)(504,341) -- (504,349) ;
		\draw [shift={(529,345)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da5982789928145993] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (144,345) -- (504,85) ;
		%Straight Lines [id:da026550407589029934] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (144,345) -- (217,248.6) ;
		%Straight Lines [id:da7821798151778738] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (227,248.6) -- (217,248.6) ;
		%Straight Lines [id:da0647589008977627] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (244.2,223.8) -- (227,248.6) ;
		%Straight Lines [id:da23739802782076258] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (244.2,223.8) -- (252.6,223.8) ;
		%Straight Lines [id:da13970874103229547] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (265.4,205.4) -- (252.6,223.8) ;
		%Straight Lines [id:da1703799500820531] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (265.4,205.4) -- (271.8,205.4) ;
		%Straight Lines [id:da8004796628769542] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (271.8,205.4) -- (323.8,134.2) ;
		%Straight Lines [id:da2255067913232094] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (323.8,134.2) -- (331.8,134.2) ;
		%Straight Lines [id:da719431819682204] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (331.8,134.2) -- (347.4,112.2) ;
		%Straight Lines [id:da9308345294162954] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (352.2,112.2) -- (347.4,112.2) ;
		%Straight Lines [id:da008064185813943414] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (370.4,85) -- (352.2,112.2) ;
		%Straight Lines [id:da1376940982371544] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (370.4,85) -- (504,85) ;
		%Straight Lines [id:da13728550416799834] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (216,227) -- (216,343) ;
		%Straight Lines [id:da14721873978091526] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (217,296.8) -- (144,296.8) ;
		%Straight Lines [id:da0669880603597095] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (217,248.6) -- (144,248.6) ;
		
		% Text Node
		\draw (94,76.4) node [anchor=north west][inner sep=0.75pt]    {$100\%$};
		% Text Node
		\draw (103,102.18) node [anchor=north west][inner sep=0.75pt]    {$90\%$};
		% Text Node
		\draw (103,127.96) node [anchor=north west][inner sep=0.75pt]    {$80\%$};
		% Text Node
		\draw (103,153.74) node [anchor=north west][inner sep=0.75pt]    {$70\%$};
		% Text Node
		\draw (103,179.52) node [anchor=north west][inner sep=0.75pt]    {$60\%$};
		% Text Node
		\draw (103,205.3) node [anchor=north west][inner sep=0.75pt]    {$50\%$};
		% Text Node
		\draw (103,231.08) node [anchor=north west][inner sep=0.75pt]    {$40\%$};
		% Text Node
		\draw (103,256.86) node [anchor=north west][inner sep=0.75pt]    {$30\%$};
		% Text Node
		\draw (103,282.64) node [anchor=north west][inner sep=0.75pt]    {$20\%$};
		% Text Node
		\draw (103,308.4) node [anchor=north west][inner sep=0.75pt]    {$10\%$};
		% Text Node
		\draw (118,345.4) node [anchor=north west][inner sep=0.75pt]    {$0\%$};
		% Text Node
		\draw (57,318) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {$\displaystyle \%$ cumulated of True Positives (TP)};
		% Text Node
		\draw (163,349.86) node [anchor=north west][inner sep=0.75pt]    {$10\%$};
		% Text Node
		\draw (200,349.86) node [anchor=north west][inner sep=0.75pt]    {$20\%$};
		% Text Node
		\draw (236,349.86) node [anchor=north west][inner sep=0.75pt]    {$30\%$};
		% Text Node
		\draw (272,349.86) node [anchor=north west][inner sep=0.75pt]    {$40\%$};
		% Text Node
		\draw (308,349.86) node [anchor=north west][inner sep=0.75pt]    {$50\%$};
		% Text Node
		\draw (344,349.86) node [anchor=north west][inner sep=0.75pt]    {$60\%$};
		% Text Node
		\draw (380,349.86) node [anchor=north west][inner sep=0.75pt]    {$70\%$};
		% Text Node
		\draw (416,349.86) node [anchor=north west][inner sep=0.75pt]    {$80\%$};
		% Text Node
		\draw (451,349.86) node [anchor=north west][inner sep=0.75pt]    {$90\%$};
		% Text Node
		\draw (485,349.86) node [anchor=north west][inner sep=0.75pt]    {$100\%$};
		% Text Node
		\draw (219,324.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize,color={rgb, 255:red, 155; green, 155; blue, 155 }  ,opacity=1 ]  {$20\%$};
		% Text Node
		\draw (148,280.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize,color={rgb, 255:red, 155; green, 155; blue, 155 }  ,opacity=1 ]  {$20\%$};
		% Text Node
		\draw (149,231.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize,color={rgb, 255:red, 155; green, 155; blue, 155 }  ,opacity=1 ]  {$37\%$};
		% Text Node
		\draw (280,383) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \%$ of Sample Size};
		% Text Node
		\draw (272,11) node [anchor=north west][inner sep=0.75pt]  [font=\Large] [align=left] {\textbf{LIFT CURVE}};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[]{Lift Analysis} 
	\end{figure}
	So in this particular example, this means that if we focus our analysis / research / target marketing or other... than on the $20\%$ of the sample being the best score (for example ... for cost reasons), we have an overperformance of $37\% / 20\% = 1.85$. Then we say that the model has a lift of $1.85$. The idea then when we compare several models is to keep the one with the greatest lift (leverage).
	
	\pagebreak
	\subsubsection{General Linear Models (GLM)}\label{general linear models}
	So far we have seen multiple linear regression techniques (simple, multiple linear regression, Gaussian linear model, linearisation of non-linear regressions, logistic regressions, etc.).
	
	There are, however, still other types of regressions named "\NewTerm{general linear models}\index{general linear models}" (not to be confused with "generalized linear models") which are intended for cases where the regression of the variable to be explained (and therefore the residuals) takes values whose range is not necessarily in $]-\infty, +\infty[$ as it is the case for the Gaussian linear model, but only takes integer positive values in $\mathbb{N} $ , or that takes real values in the interval $ [0,1] $, etc.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	We will focus here on the essential concepts of the generalized linear regression models because for each of the submodels that we will see below it is possible to find textbooks of 300 to 500 pages which are dedicated to it! In the same way as for classical linear regression (Gaussian model) we one can find textbooks of almost 1,000 pages.
	\end{tcolorbox}
	
	To introduce this type of model, recall what characterizes a general regression model (think by reading the following lines to the particular case of Gaussian linear model as it may help for the understanding):
	\begin{enumerate}
		\item[C1.] The response variable $Y$ is a random component with a probability distribution: Normal (the most frequent case for continuous data), Binomial (positive integer successes in $n$ trials), Poisson (positive integer successes for rare events), Bernoulli (whole positive success with one unique try), Binomial-Negative (positive integer success in a sequence of independent and identically distributed Bernoulli trials before a specified (non-random) number of failures occurs). We note that these are laws belonging to the exponential family (the underlying idea being to linearise it with logarithm) as we will see later.
		
		For example, for the linear Gaussian model seen earlier above, we have shown that for the case with a unique explanatory variable:
		
		was a model with which we relied on the least squares method via the log-likelihood maximum.
		
		\item[C2.] The explanatory variables $ x_1, x_2, \ldots, x_k $ with the intercept constant used as predictors in the model define as a linear (or non-linear  by multiplying the variables by pairs) the deterministic components.
		
		\item[C3.] The link describing the functional relationship between the linear combination of the explanatory variables and the expectation of the response variable $Y$ is named the "\NewTerm{link function}\index{link function}" or "\NewTerm{canonical parameter}\index{canonical parameter}" and is traditionally denoted $g(\text{E}(Y_i))$.
	\end{enumerate}
	
	In general, when the relation between $\text{E}(Y_i)$ and the explanatory variables is linear we speak of "linear model", when the relation between a function $g(\text{E}(Y_i))$ and the parameters is linear, we speak then of "general linear model".
	
	For example, in the case of the linear Gaussian model with an explanatory variable, we have (it is immediate by the property of the underlying distribution):
	
	Thus, in the case of the Gaussian linear model, the expectation is modelled directly. Thus, the link function for the Normal distribution is the mean itself! We talk then about an "\NewTerm{identity link relation}".
	
	We will obviously not be able to use the linear Gaussian model given the definition domain taken by the variable to be explained when that latter is defined only on integers. On the other hand
we could try an approach by saying that each count:
	
	Follows a Poisson distribution:
	
	of mean (still in the special case with a unique explanatory variable!):
	
	The studied model is then written:
	
	and we deduce from it:
	
	We say that $\log (N_i)$ is an "\NewTerm{offset variable}" (in the industry it is often a value representing the time spent until the event of interest) and that the "link function" is then the logarithm for the Poisson regression. We speak more generally of "\NewTerm{log-linear Poisson model}\index{log-linear Poisson model}" which is associated with a "\NewTerm{logarithmic link}".
	
	Obviously the above approach is very far-fetched but a little further we will see the rigorous approach to get this link function.
	
	Let us recall some important things in this model which are that the hypothesis are similar to the Gaussian linear model: the $Y_i$ are all supposed to follow a Poisson distribution, to have all the same variance and to be independent, and so on...
	
	Note that since we have:
	
	The mean is therefore exponential, influenced by the sign of $\alpha$ (it is logical but it can always be useful to specify it).
	In the general case, we will see that the law of probability density associated with can be written in the general form:
	
	and we say then that the law belongs to the "exponential family" (the Normal, Poisson, Bernoulli and Gamma laws are part of it for example).
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Very few specialists designate by the abbreviation "GLZM" the GLM other than those based on the Normal law.
	\end{tcolorbox}
	
	\paragraph{Normal GLM}\mbox{}\\\\
	Let's see this with the Normal law by analogy (the homoscedastic case to simplify...): 
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1.3]{img/computing/glm_normal.pdf}
		\vspace*{2mm}
		\caption[Underlying idea of the Normal GLM]{Underlying idea of the Normal GLM (author: Walmes Zeviani)}
	\end{figure}	
	We therefore start from:
	
	So it comes by identification with:
	
	that:
	
	Note also that (this is very important for what will follow!):
	
	So we fall back on the expected mean and the variance of the Normal law. It would be interesting to check if we have the same kind of property for another law of the exponential family to see if it is generalizable or not ???
	
	Let us also indicate that compared to the famous "canonical parameter" (since mean and variance ultimately depend on it), we have in this case:
	
	so the canonical function is then (without too much surprise in this case...):
	
	that it is customary to write in a relatively general case as we already know in vector notation (see the study of multiple linear regression for the notation):
	
	In this case the canonical function is the unitary application. We speak then of "\NewTerm{canonical identity link}" and it is the only case where the method of ordinary least squares is confused (to understand: is an identical approach) to the use of the GLM.
	
	We note that the assumptions of use of the Normal GLM trivially remain the independence of the explanatory variables, the homoscedasticity, the linearity of the model and the linearity of the expected mean.
	
	\paragraph{Poisson GLM}\mbox{}\\\\
	Let's do the same as before but with the Poisson's law. The idea is therefore that the variable to be explained is discrete and whereas in the same order of idea of the Normal regression we have:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1.3]{img/computing/glm_normal.pdf}
	\end{figure}
	then with the Poisson regression, the idea is to have (notice that as the mean increase, the variance seems also to increase and we will prove this further below even if it's quite obvious regarding to the moment of the Poisson distribution):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1.3]{img/computing/glm_poisson.pdf}
		\vspace*{2mm}
		\caption[Underlying idea of the Poisson GLM]{Underlying idea of the Poisson GLM (author: Walmes Zeviani)}
	\end{figure}
	However, the reader must be keep in mind that even a continuous variable can have an excellent fit to a Poisson's law, which is why Poisson's regression is also sometimes used for variables to be explained that are continuous (typically for heteroscedastic regressions where weighted regression is to difficult to apply or fails).
	
	We will therefore look for a regression model which, like Gaussian GLM regression, gives the slip of the expected mean of the Gaussian law as a function of explanatory variables but this time with a model that gives the slip of the expected mean of the Poisson's law (yes indeed as a reminder ... the two moments of the Poisson's law are directly connected to each other). We start from:
	
	Last arrangement that we had to guess ... to make an identification with:
	
	Therefore it comes:
	
	Let us indicate for the small parenthesis that some authors use the Euler Gamma function (\SeeChapter{see section Differential and Integral Calculus page \pageref{gamma euler function}}) and write:
	
	Now, let's have a look at what gives the derivatives used above:
	
	So we can once again find the expected mean and variance but this time of the Poisson's law. It would be interesting to check again if we have the same kind of property for another law of the exponential family in order to see if it is generalizable or not ???
	
	So we notice a second time that the parameter $\theta$ controls the mean and the variance through its first and second derivative. The variance is therefore a function of the mean for all the laws of the family of exponential laws.
	
	Let us also indicate that relatively to the famous "canonical parameter" (since mean and variance ultimately depend on it), we have in this case:
	
	The canonical function for the Poisson's law is then:
	
	So the canonical function is the logarithmic application. We then talk about "\NewTerm{canonical logarithmic link}".
	
	Before going further let us compare, the generalized form of the Normal law of the regression with that of Poisson:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	We will not go too much further with the Poisson's law for regression because according to the experience feedback of the senior practitioners it seems that it suffers from the major problem that as the expected mean is equal to the variance whereas in reality the variance tends to be higher than the expected mean. The negative binomial model (see below) should be preferred to that of Poisson.
	\end{tcolorbox}
	For the parameters to be estimated, it is customary to also use the maximum log-likelihood as we have done for many previous regression models. Then the likelihood is given in this case by:
	
	The log-likelihood becomes:
	
	Now to proceed as we did with the Gaussian linear regression, let us recall that the $y_i$ are given by the experiment. We will write then:
	
	and that, as for Gaussian linear regression, the explicit expression of the $\mu_i$ must allow us to determine the expression of parameters $\beta_j$ of the regression.
	
	However we have seen earlier above that the link function for the Poisson regression was the natural logarithm which in the general case is written as follows:
	
	 
	So we have so far:
	
	Therefore for the Poisson logarithm link:
	
	Hence the log-likelihood will be written:
	
	To solve this kind of log-likelihood, we then use Newton-Raphson type algorithms (see further below in this section page \pageref{newton raphson method}).
	
	We note that the hypotheses of use of the GLM Poisson remain trivially the independence of the explanatory variables, the homoscedasticity is on the other hand no more assumed since the variance depends on the expected mean, the linearity of the model is always a hypothesis and the expected mean is linear only under a logarithmic transformation.
	
	Finally, let us say that, just as for the Gaussian regression, we can use the result of the Poisson regression to make the probabilistic inference. Indeed, since the density probability function of the Poisson distribution is given by:
	
	and since we have:
	
	 Then:
	
	and in this case you have to change the notation so that it makes sense:
	
	Thus, for any abscissa value $x_i$ of our model (which in the present case is univariate), we can calculate the prior probability of the realization of an event of interest for an ordinate value $y_i \in \mathbb{N}$.
	
	\paragraph{Binomial GLM}\mbox{}\\\\
	Let's do the same with the binomial law! 
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1.3]{img/computing/glm_binomial.pdf}
		\caption[Underlying idea of the binomial GLM]{Underlying idea of the binomial GLM (author: Walmes Zeviani)}
	\end{figure}
	We start from:
	
	Now, let's do the identification with:
	
	Therefore we get (take a particular care at how looks like the first parameter...):
	
	Now, let's have a look at what gives the derivatives used earlier above:
	
	and:
	
	So we can once again fall back on the expected mean and variance but this time the of the binomial law!
	
	Let us also indicate that relatively to the famous "canonical parameter" (since expected mean and variance ultimately depend on it), we have in this case:
	
	So the canonical function is:
	
	in this case the canonical function is named, as we already know, the "\NewTerm{logit link}" or simply the "\NewTerm{logit}" (which we had already discovered otherwise in our study of logistic regression).
	
	Let us compare now the generalized form of the Normal law of the regression with that of Poisson and the Binomial law:
	
	and also the link functions:
	
	We note that the assumptions of use of the Binomial GLM remain trivially the independence of the explanatory variables, the homoscedasticity is on the other hand no more assumed, the linearity of the model is always a hypothesis and the expected mean is linear under a logit transformation .
	
	We will not develop here the likelihood model of the binomial regression as we already did it during our study of the binomial logistic regression on page \pageref{likelihood binomial logistic regression}.
	
	\paragraph{Binomial Negative GLM}\mbox{}\\\\
	Negative binomial regression (sometimes abbreviated in general as N.B.R.M. for Negative Binomial Regression Model) is probably the most widely used in practice outside binomial logistic regression or Gaussian regression. However it suffers from a major problem ... Indeed there are about twenty derived models such as: NB-C (the simplest that we will study here), NB-1, NB-2 (generalization of GLM Poisson), truncated NB / NB-1 / NB-C, NB with zero inflation (ZINB), censored NB, NB with obstacle, NB-P, heterogeneous-NB (NBH), finite mixed model, conditional fixed effects model, random effects model, NB endogenous stratification, NB sampling, NB latent class, NB bivariate, etc. In short enough to write several thousand pages and study for several years ...
	
	Well this being said, let's take a look at the case of canonical negative binomial regression.
	
	We thus start from one of the possible expressions of the density function of the negative binomial law (see the section Statistics page \pageref{negative binomial distribution} for the demonstration of its origin):
	
	We then adapt this notation to the GLM sauce, admitting that we are interested in failures $E$:
	
	Now, let's do the identification with:
	
	Therefore:
	
	Now, let's have a look at what gives the derivatives used earlier above:
	
	Which corresponds well to the expected mean of the number of failures before the $R$-th success (result to be compared with the expected mean of the number of successes of the negative binomial law as demonstrated in the section Statistics) since it is this that we seek to model by linear regression. Do not forget that we can reverse the roles failures / successes.
	
	And:
	
	Which corresponds well to the variance of the number of failures (result to be compared with the variance of the number of successes of the negative binomial law as demonstrated in the section Statistics) since this is what we seek to model by the regression.
	
	We then have:
	
	Let's now make a usual change of notation:
	
	Which is also sometimes denoted:
	
	In this form, we can rewrite:
	
	If we were more interested in successes rather than failures we would have (this is the case most often chosen in the specialized literature):
	
	Then, it is customary to write this with the Gamma Euler function (\SeeChapter{see section page \pageref{gamma euler function}}). What then gives (with different common notations):
	
	
	In this form, we immediately see that expected mean is written then:
	
	Now, to return to our previous development, let us recall that we have proved in the section of Differential and Integral Calculus during our study of the Euler's Gamma function (\SeeChapter{see section Differential and Integral Calculus page \pageref{gamma euler function}}) that:
	
	We then have by extension:
	
	It then comes in this case:
	
	Let us integrate with the following variable change:
	
	We then have:
	
	We find under the integral two density functions which are respectively the Poisson density function and the Gamma density function (\SeeChapter{see section Statistics page \pageref{poisson distribution} and page \pageref{gamma distribution}}):
	
	We have a result that it is customary to associate with a generalization of the Poisson's law and which we name "\NewTerm{Poisson-Gamma density law}\index{Poisson-Gamma density law}" or "\NewTerm{mixed Poisson-Gamma law}\index{mixed Poisson-Gamma law}".
	
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Thus, the negative binomial distribution can be considered as a generalization of the Poisson distribution. Indeed, the negative binomial law can be seen as a Poisson distribution where the Poisson parameter is itself a random variable, distributed according to a Gamma distribution !!!\\
	
	An important application is in the area of risk insurance where $k$ the number of undesirable events is modelled by a Poisson law of parameter $\lambda$ which itself is therefore tainted by an uncertainty that is characteristic of the risk category of the insured. Thus, the parameter $\lambda$ can itself be seen as a random variable traditionally denoted $\Theta$ in the field of actuarial science and following a gamma law such that the density function of $\Theta$ is then given by (with the traditional notation in actuarial science):
	
	\end{tcolorbox}
	For the parameters to be estimated, it is customary to also use the maximum log-likelihood as we did for all previous regressions. Then, in order to determine the maximum of the log-likelihood, it is traditional in the case of the negative binomial law to start from the following form demonstrated above:
	
	We then have:
	
	The log-likelihood then becomes:
	
	Now to proceed as we did with the Gaussian linear regression, let us recall that the $y_i$ are given by the experience. We will write then:
	
	and that, as for the Gaussian linear regression, the explicit expression of the $\mu_i$  must allow us to determine the expression of the parameter $\alpha$ of the regression.
	
	Now, let us recall that:
	
	and that:
	
	and let us denoted this $\lambda$. Then we get:
	
	Therefore:
	
	Now let us put that $R \rightarrow +\infty$. It comes then:
	
	since each term of the product tends to $1$ and since there is a constant number of terms in the product (that is, $k$ terms) the whole tends to $1$.
	
	And as we saw in the section of Functional Analysis (see page \pageref{natural exponential function}):
	
	Therefore:
	
	We thus fall back in this limit case on the Poisson law!
	
	Therefore, as the Poisson law is a special case of the Negative Binomial law, a possible model for the link function of the Negative Binomial Law is to take up the logarithm (hence the Poisson regression link function).
	
	The canonical function is then that of the Poisson function:
	
	From then on, the log-likelihood will be written:
	
	that it is customary to name the "\NewTerm{NB2 log-likelihood}\index{NB2 log-likelihood}" (which is the one used in the majority of statistical softwares).
	
	But there is another version of log-likelihood that uses a combination of approximation and exact value. Indeed, remember that we had obtained:
	
	Therefore, using:
	
	and by mixing (rather dubious mix...) with the approximation approach using Poisson link function:
	
	We have (happy mix ...):
	
	The log-likelihood is written then:
	
	that is customary named the "\NewTerm{NB-C log-likelihood}\index{NB-C log-likelihood}".
	
	To solve this kind of log-likelihood, we then use again Newton-Raphson type algorithms (see further below in this section page \pageref{newton raphson method}).
	
	Let us compare, the generalized form of the Normal law of regression with that of Poisson, of the Binomial and Negative Binomial law:
	
	and also the link functions:
	
	
	We note that the hypotheses of use of the Binomial Negative GLM remain trivially the independence of the explanatory variables, the homoscedasticity is on the other hand no more assumed, the linearity of the model is always a hypothesis and the expected mean is linear only under a logarithmic  transformation chosen a little bit empirically in the canonical case...
	
	\paragraph{Gamma GLM}\mbox{}\\\\
	Let's do the same as before but now with the Gamma law (\SeeChapter{see section Statistics page \pageref{gamma distribution}}). We therefore start from:
	
	by just changing the notation a bit to not confuse with one of the GLM parameters:
	
	assuming that the scale parameter $\lambda$ is known (...). Why? Because the expected mean of the Gamma law is for recall given by (see the proof in the section Statistics page \pageref{gamma distribution}):
	
	and therefore controlling only one of the two parameters is enough to scan the entire desired definition domain (yes, simply!).
	
	The use of the Gamma law is justified, besides by its domain of definition which is not in the negatives, by the fact that the relation between its expected mean and its variance gives:
	
	So the relationship between expected mean is the variance is constant regardless of their respective values (so when mean increases, the variance increase too as for the Poisson distribution)! Another way of looking at this is that the variance is proportional to the expected mean since:
	
	The reason why the GLM Gamma is interesting is that we can "adjust" the relationship between variance and expected mean by choosing the value of $\lambda$, which other models seen so far can not do!
	
	Therefore we get:
	
	Now, let us as always the identification with:
	
	We therefore get:
	
	Now, let's have a look at what gives the derivatives used earlier above:
	
	and:
	
	So we can once again fall back on the expected mean and variance but this time of the Gamma law!
	
	Let us also indicate that compared to the famous "canonical parameter" (since expected mean and variance ultimately depend on it), we have in this case:
	
	so the canonical function is:
	
	in this case, the canonical function is named the "\NewTerm{negative inverse link}" and is often denoted in the following form (because finally put that $-\alpha_i=1$ is simply equivalent to change the values of the parameters $\beta_j$):
	
	So in view of this expression we can make regression of very various type of curves.
	
	Let us now compare the generalized form of the Normal law of the regression with that of Poisson and the Binomial law and the Gamma law:
	
	and also the link functions:
	
	We note that the hypotheses of use of the GLM Gamma remain trivially the independence of the explanatory variables, the homoscedasticity is on the other hand no more assumed, the linearity of the model is always a hypothesis and the expected mean is linear only under an inverse transformation.
	
	Obviously (as always...) the subject is more complex than it may seems. There are plenty of other GLM models that exist. The reader can see a sample of the most known one in the table below:
	\begin{table}[H]
		\centering
		\begin{tabular}{lll}
		\rowcolor[gray]{0.75}\hline \textbf{Family} & \textbf{Link type} & \textbf{Mean function}\\
		\hline Gaussian & Identity & $\mu_{i}=\vec{x}_{i}^T \vec{\beta}$ \\
		Binomial & Logit (logistic) & $\mu_{i}=\frac{\exp \left(\vec{x}_{i}^T \vec{\beta}\right)}{1+\exp \left(\vec{x}_{i}^T \vec{\beta}\right)}$ \\
		Binomial & Probit & $\mu_{i}=\Phi\left(\vec{x}_{i}^T \vec{\beta}\right)$  \\
		Binomial & Robit & $\mu_{i}=T_{(\nu-1)/\nu}\left(\vec{x}_{i}^{T} \vec{\beta}\right)$  \\
		Binomial & loglog (nloglog) & $\mu_i=\exp \left(-\exp \left(\vec{x}_{i}^{T} \vec{\beta}\right)\right)$ \\
		Binomial & Cloglog (gombit) & $\mu_{i}=1-\exp \left(-\exp \left(\vec{x}_{i}^T \vec{\beta}\right)\right)$ \\
		Binomial & Cauchit &  $\mu_i=\dfrac{1}{\pi}\arctan(\vec{x}_i^T\vec{\beta})+0.5$\\ 
		Poisson & Log & $\mu_{i}=\exp \left(\vec{x}_{i}^T \vec{\beta}\right)$\\
		Poisson & Identity & $\mu_{i}=\vec{x}_{i}^T \vec{\beta}$  \\
		Poisson & Sqrt & $\mu_{i}=\left(\vec{x}_{i}^T \vec{\beta}\right)^{2}$ \\
		Gamma & Inverse & $\mu_{i}=\left(\vec{x}_{i}^T \vec{\beta}\right)^{-1}$\\
		Gamma & Identity & $\mu_{i}=\vec{x}_{i}^T \vec{\beta}$ \\
		Gamma & Log & $\mu_{i}=\exp \left(\vec{x}_{i}^T \vec{\beta}\right)$ \\
		Inverse gaussian & Inverse squared & $\mu_{i}=\left(\vec{x}_{i}^T \vec{\beta}\right)^{-1 / 2}$\\
		$\ldots$ & $\ldots$ & $\ldots$\\
		\hline
		\end{tabular} 
		\vspace*{3mm}
		\caption{Link functions and mean functions of some Generalized Linear Models (GLM)}
	\end{table}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. Let us now see after all these developments a first practical example using the most important case: the negative binomial regression. So that everyone can practice, we are going to do this regression with the spreadsheet software of Microsoft (a version made with \texttt{R} is also available in the corresponding companion book).\\

	We collected a dataset of days of absence from $316$ young college students (below the reader can see only the 32 first). Our explanatory variables of interest are their average score in foreign languages and their gender (boy or girl: B/G):
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/raw_data_nb2_glm_microsoft_excel.jpg}
	\end{figure}
	Before going any further, it is first trivial to notice that the days of absence are non-negative with integer values and that the majority are never absent, a Gaussian regression is not the most suitable because its support sweeps away all the positive and negative real numbers (this does not necessarily mean that the result would be bad, however!). In addition, the standard deviation of days of absence is significantly different from the average of these same days of absence, therefore in extenso the Poisson regression is not the most suitable ... (however, this also does not mean necessarily that the result would be bad!).\\
	
	So here is how to proceed with Microsoft Excel 14.0.7106 (the reader can find the equivalent example made with the \texttt{R} software in the corresponding companion book):\\

	First we prepare an area which contains the $2$ coefficients, the intercept and the parameter $\alpha$ of the negative binomial model (which for reminder is the inverse of the number of successes or respectively of failures and whose value must be at least equal the unit!!!) in an area of the sheet as visible below to which we associate a weak positive value written in hard on the keyboard (therefore no formulas here!):
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/glm_negative_binomial_initial_values.jpg}
	\end{figure}
	Then, in column \texttt{D}, we will write the relation of the NB-2 log-likelihood  which for reminder is given by:
	
	and since this spreadsheet software does not have Euler's Gamma function built in at the day we write these lines, we will rewrite it as follows:
	
	Which gives with the Microsoft Excel formula syntax:\\
	
	\texttt{=LN(LN(FACT(1/\$G\$2+C2-1)))-LN(LN(FACT(1/\$G\$2-1)))
-IFERROR(LN(LN(FACT(C2+1-1)));1)
-\\(C2+1/\$G\$2)*LN(\$G\$2*EXP(\$G\$3+\$G\$4*A2+\$G\$5*B2)+1)\\
+C2*LN(\$G\$2*EXP(\$G\$3+\$G\$4*A2+\$G\$5*B2))}\\

	By writing this formula into column \texttt{D} and dragging it down we get first the following values:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/raw_data_poisson_glm_with_log_likelihood_microsoft_excel.jpg}
	\end{figure}
	Now in a cell, we will report the sum of the NB-2 log-likelihood values. In this case we have chosen to put it in \texttt{G7}:
	\end{tcolorbox}
	
	With everything that we have seen since the beginning of this book in the field of Statistics and Numerical methods, we can now provide the following summary table of some naive statistical tests that we have derived mathematically with all necessary details previously and that therefore the reader should therefore be able to grasp:
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/summary_naive_statistical_tests.jpg}
		\caption{Summary of naive fundamental statistical tests}
	\end{figure}
	
	\pagebreak
	\subsubsection{Robust, $M$-estimators and $W$-estimators}\label{m-estimators}
	The $M$-Estimators naturally appear in statistics when we see the problem of the aspect of the variance which is a function of squared deviations from the mean (hence a parabola) that is therefore a little too sensitive to extreme values (this is especially the case of  many linear regression techniques, hence the fact that $M$-estimators are introduced mainly in textbooks related to regression techniques!). The idea then is to ask if it is not possible to construct something other than a square of deviations (hence a more robust statistic!) and then to define the corresponding sample mean, everything being distribution free, that we will name a "\NewTerm{$M$-estimator of location}\footnote{Then by extension $M$-estimators are a broad class of extremum estimators for which the objective function is a sample average!}\index{$M$-Estimator of location}" (the reader should know that there are also $M$-estimators for the sample variance named "\NewTerm{$M$-estimator of scale}" like the $\tau$-estimator!).
	
	We have placed this subject in the section of Theoretical Computing instead of the section of Statistics because for most of these estimators, as we will see it further below, no closed form solution exists and an iterative approach to computation is required\footnote{It is possible to use standard function optimization algorithms, such as Newton–Raphson (solver!)}, the mathematics are quite ugly and most of the time there are different worldwide definitions for the same thing and also computer implementations that differ from their theoretical definition! .

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Peter J. Huber (11964, 11967 according to holocene calendar) introduced $M$-estimators and their asymptotic properties (the proposed estimating the center of symmetry of symmetric distributions). They played an important part of the development of modern robust statistics. Liang and Zeger (11986 according to holocene calendar) helped popularize $M$-estimators in the biostatistics literature under the name "\NewTerm{generalized estimating equations}" (GEE). Since then many other robust estimators has been developed like MM-estimators, $S$-estimators, GM-estimators, $\tau$-estimators, etc. (basically, MM-estimators are $M$-estimators initialized by an $S$-estimator...).
	\end{tcolorbox}

	The best-known example of $M$-estimator is the arithmetic mean $\mu_a$, as we have just mentioned, indeed it is it that trivially maximizes the function of the sum of squares of the deviations as we have seen earlier above:
	
	But some statistician have had the idea (in their purpose to always generalize everything thank to a lot of coffee) to define an $M$-estimator as the expression of the average that maximizes any function $f$ whose argument is by definition $x_i-\theta$ such that:
	
	And for example, in the case of the most familiar variance, this function $f$ is the square of the argument, ie $f(x_i-\theta)=(x_i-\theta)^2$. It is customary, however, in statistics to denote the function $f$ with the letter $\rho$ (so do not confuse with the correlation coefficient or anything of the same kind!). We will continue our explanations by aligning ourselves with this traditional notation!
	
	What ultimately makes us look for a $\theta$ that for a particular function $\rho'$ gives:
	
	where $\psi$ is named the "\NewTerm{influence function}\index{influence function}" (both $\rho$ and $\psi$ should be obviously symmetric!). When the partial derivative exist, we speak of "$\psi$-type $M$-estimator" otherwise of "$\rho$-type $M$-estimator".
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	If $\rho=f$, the probability density function related to the random variable $X$, then the reader may have noticed that we fall back on the MLE (Maximum Likelihood Estimator) as introduced in the section of Statistics page \pageref{maximum likelihood estimators}!
	\end{tcolorbox}
	We would like any good $M$-estimator of location to be location and scale equivariant, that is, we would like it to respond in a reasonable manner to linear changes across the sample. An $M$-estimator is "location equivariant" if, when every observation is shifted by some amount $a$, the $\theta$ of that shifted sample also shifts by $a$. An $M$-estimator of location is scale equivariant if, when every observation is multiplied by some non-zero constant $b$, the $\theta$ of that altered sample is $b$ times the $\theta$ of the unaltered sample. An $M$-estimator of location that possesses both these characteristics is location-and-scale equivariant. In other words, we want
the following to hold:
	
	In order for an $M$-estimator of location to be location-and-scale-equivariant, it is often necessary to scale the observations when computing $\rho$ and  $\psi$. This is often done whether rescaling is necessary or not because it makes notation simpler.
	
	The matter of location-equivariance is fairly simple! It can be satisfied by making the input of the form $x_i-\theta$. Adjusting for scale is not as straightforward. We must pick some estimator of the scale of the sample, noted (without surprise...) $s_i$, which is a function of the observations $x_1,x_2,\ldots,x_n$. $s$ is scale-equivariant and location invariant, meaning that it is unaffected by a shift in the sample as described above.

	It is then common to define the standardized residual:
	
	where $s_i$ is a strictly positive robust estimator of scale (typically the median absolute deviation but it's not the only choice!) and $c$ is a tuning constant, then we can write obviously:
	
	
	\textbf{Definition (\#\thesection.\mydef):} Thus, more generally, a "\NewTerm{$M$-estimator of location}" may be defined to the value of a sample mean $\hat{\theta}$ that leads to a zero value of an estimating function given by:
	
	
	Ideally, it is desirable that the function $\rho'$ had as main property of being bounded (which immediately eliminates the arithmetic mean as an ideal $M$-estimator) which avoids typical deviations from the standard deviation (where $\rho(r_i)=r_i^2/2$ and hence $\psi(r_i)=r_i$):
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/m_estimator_influence_function_L_2.jpg}
		\caption{$L^2$-norm (influence function)}
	\end{figure}
	But we also require at least the four following properties for the $\rho(r_i)$ function:
	\begin{itemize}
		\item[P1.] $\rho(r_i)\geq 0$ for all $r_i$ and has a minimum at $0$
		
		\item[P2.] $\rho(r_i)=\rho(-r_i)$ for all $r_i$
		
		\item[P3.] $\rho(r_i)$ increases as $r_i$ increases from $0$, but doesn't get too large as $r$ increases
		
		\item[P4.] $\rho(r_i)$ should be differentiable
	\end{itemize}
	and the following four properties for the $\psi(r_i)$ function:
	\begin{itemize}
		\item[P1.] Be piece-wise continuous function
		
		\item[P2.] Be an odd function, ie $\psi(-r_i)=-\psi(r_i)$
		
		\item[P3.] $\psi(r_i)\geq 0$ for $r_i\geq 0$ and $\psi(r_i)>0$ for $0<r_i<k$
		
		\item[P4.] Its slope is $1$ at $0$, ie $\psi'(0)=1$
	\end{itemize}
	Note that the last property is not strictly required mathematically, but we use it for standardization in those case where $\psi$ is continuous at $0$. Then if follow from the first property that $\psi(0)=0$, and we require $\psi(0)=0$ also for the case where $\psi$ is discontinuous in $0$, as it is, e.g., for the $M$-estimator defining the median.
	
	Albert E. Beaton and John W. Tukey in their article \cite{beaton1974} also define a new weighted function $\omega$ (without justifying it, but it's probably a mathematical trick to simplify some calculations that we will see further below) such that:
	
	Thus:
	
	What E. Beaton and John. W. Tukey write explicitly in their original article:
	
	We can eliminate the $c\cdot s_i$ of the second fraction if we assume, same like Beaton and Tukey did, that it is equal for all $i$ such that (relation that we use later):
	

	\textbf{Definition (\#\thesection.\mydef):} The "\NewTerm{breakpoint of an estimator}" is the maximum proportion of observations (hence a number between $0$ and $1$) that can be changed without
changing the estimator. For example the breakpoint of the arithmetic average is $0$, that of the median is (obviously) $0.5$.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Andrews et al. (11972 according to holocene calendar) (the Princeton Robustness Study), at which time it was expected that all statistical analyses would, by default, be robust wrote: «\textit{any author of an applied article who did not use the robust alternative would be asked by the referee for an explanation}»  and also that «\textit{From the 1970s to 2000 we would see... extensions to linear models, time series, and multivariate models, and widespread adoption where every statistical package would take the robust method as the default...}»\\
	
	Sadly, we see that even in 12018 (the day we write these lines according to holocene calendar), it's always not the case even if all softwares have largely the possibility to do so using nonparametric or brute force statistical methods (human inertia to change issue...).
	\end{tcolorbox}
	 
	Before introducing a list of some well-known  $\rho$, $\psi$ and $\omega$ functions, let us introduce two one well known robust estimator and one well known $M$-estimators of location (these both being introduced in the form of the objective function introduced just earlier above!).
	
	\begin{enumerate}
		\item The "\NewTerm{trimmed mean}\index{trimmed mean}\label{trimmed mean}", also named "\NewTerm{truncated mean}" or "\NewTerm{pruned mean}", is the robust estimator consisting in the arithmetic mean obtained by removing from the observed values those below and above a given quantile. It is customary to denote $\alpha/2$ the corresponding percentile and that if we have $n$ values then we return the $[\alpha n]$ largest values and the $n-[\alpha n]$ smaller (remember that $[\ldots]$ is the notation for "integer value" as seen on page \pageref{integer part}).
		
		Mathematically, we denote elegantly the pruned average of ordered realizations $x_i$ in descending order to increasing of a random variable $X$ in the following form:
		
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		We can find trimmed mean in many spreadsheets softwares for example under the function name \texttt{TRIMMEAN( )}, or in statistical software like \texttt{R} (just by passing an argument to the default \texttt{MEAN( )} function!
		\end{tcolorbox}
		
		It is quite obvious that this robust estimator is an $M$-Estimator as it respects the definition seen above since we are only replacing some values of the original series with other values and that's finally equivalent to an arithmetic mean!
		
		\item The "\NewTerm{Winzorised (Huber) mean}\index{Winzorised (Huber) mean}" is the robust estimator consisting in the arithmetic mean calculated after that the $[\alpha n]$ smallest values have been replaces by the value $x_{[\alpha n]+1}$ and the $[\alpha n]$ biggest values by the value $x_{n-[\alpha n]}$.
		
		Mathematically we denote elegantly the Winsorized mean of the ordered realizations $x_i$ in descending order to increasing of a random variable $X$ in the following way (discrete version):
		
		Again it is obvious that this robust estimator is an $M$-Estimator as it respects the definition seen above since we are only replacing some values of the original series with other values and that's finally equivalent to an arithmetic mean!
		
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		We can find the Huber's mean in software like \texttt{R} using the function \texttt{HUBER( )} of the MASS package
		\end{tcolorbox}
		
		\item The "\NewTerm{median}\index{median}", that we already well know, that satisfies (discrete version under the form of an $M$-estimator):
		
		where:
		
		is for recall named the "\NewTerm{signum function}\index{signum function}" or simply "\NewTerm{sign function}\index{sign function}".
		
		\begin{figure}[H]
			\centering
			\includegraphics{img/computing/m_estimator_median.jpg}
			\caption{Median (influence function)}
		\end{figure}
	\end{enumerate}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Consider the values $2,4,5,10,200$. We then have first:
	\begin{gather*}
		\mu_a=44.2\qquad M_e=5
	\end{gather*}
	We then have for the $20\%$ trimmed mean:
	\begin{gather*}
		\mu_{T_\alpha}=\dfrac{4+5+10}{3}=6.\bar{3}
	\end{gather*}
	and for the $20\%$ Winsorized mean:
	\begin{gather*}
		\mu_W=\dfrac{4+4+5+10+10}{5}=6.6
	\end{gather*}
	\end{tcolorbox}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The mean $\mu_a$, the trimmed mean $\mu_{T_\alpha}$, the Winsorized mean $\mu_W$ and the median $M_e$ are particular cases of "\NewTerm{$L$-statistic}\index{$L$-statistic}" (linear combination of order statistics):
	\begin{gather*}
		M_l\left(y_{1}, \ldots, y_{n}\right)=\sum_{i=1}^{n} a_{i} Y_{(i)} \quad a_{i, n}=\frac{1}{n} \Rightarrow \overline{y}
	\end{gather*}
	For example:
	\begin{gather*}
		a_{i, n}=\frac{1}{n} \Rightarrow \overline{y}
	\end{gather*}
	lead to the mean $\mu_a$:
	\begin{gather*}
		a_{i,n}=\left\{\begin{array}{l}{\text { for } n \text{ odd}} \\ {1\qquad i=\dfrac{n+1}{2}}\\ {0\qquad i\neq\dfrac{n+1}{2}}\end{array}\right. \qquad a_{i,n}=\left\{\begin{array}{l}{\text { for } n \text{ even}} \\ {\dfrac{1}{2}\qquad i=\dfrac{n}{2},\dfrac{n}{2}+1}\\ {0\qquad \text{otherwise}}\end{array}\right.
	\end{gather*}
	leads to the median $M_e$.
	\end{tcolorbox}
	
	Let's look at some of $\rho$, $\psi$ and $\omega$ functions that build some common $M$-estimators:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|c|}{\cellcolor[gray]{0.75}\textbf{Type}} & \multicolumn{1}{c|}{\cellcolor[gray]{0.75}\pmb{${\rho(r_i)}$}} & \multicolumn{1}{c|}{\cellcolor[gray]{0.75}\pmb{${\psi(r_i)}$}} & \multicolumn{1}{c|}{\cellcolor[gray]{0.75}\pmb{${\omega(r_i)}$}} \\ \hline
		$L_2$ (least-squares) & $r_i^2/2$ & $r_i$ & $1$ \\ \hline
		$L_1$ (least-absolute) & $|r_i|$ & $\mathrm{sgn}(r_i)$ & $\dfrac{1}{|r_i|}$ \\ \hline
		$L_1-L_2$ & $2\left(\sqrt{1+r_i^2/2}-1\right)$ & $\dfrac{r_i}{\sqrt{1+r_i^2/2}}$ & $\dfrac{1}{\sqrt{1+r_i^2/2}}$ \\ \hline
		$L_p$ & $\dfrac{|r_i|^{\nu}}{\nu}$ & $\mathrm{sgn}(r_i)|r_i|^{\nu-1}$ & $|r_i|^{\nu-2}$ \\ \hline
		"Fair" & $c^{2}\left[\dfrac{|r_i|}{c}-\log \left(1+\dfrac{|r_i|}{c}\right)\right]$ & $\dfrac{r_i}{1+|r_i| / c}$ & $\dfrac{1}{1+|r_i| / c}$ \\ \hline
		Huber $\left\{\begin{array}{l}{\text { if }|r_i| \leq k} \\ {\text { if }|r_i| \geq k}\end{array}\right.$ & $\left\{\begin{array}{l}{r_i^{2} / 2} \\ {k(|r_i|-k / 2)}\end{array}\right.$ & $\left\{\begin{array}{l}{r_i} \\ {k \mathrm{sgn}(r_i)}\end{array}\right.$ & $\left\{\begin{array}{l}{1} \\ {k /|r_i|}\end{array}\right.$ \\ \hline
		Cauchy & $\dfrac{c^{2}}{2} \log \left(1+(r_i / c)^{2}\right)$ & $\dfrac{r_i}{\left(1+r_i^{2}\right)^{2}}$ & $\dfrac{1}{\left(1+r_i^{2}\right)^{2}}$ \\ \hline
		Geman-MacClure & $\dfrac{r_i^{2} / 2}{1+r_i^{2}}$ & $\dfrac{r_i}{\left(1+r_i^{2}\right)^{2}}$ & $\dfrac{1}{\left(1+r_i^{2}\right)^{2}}$ \\ \hline
		Welsch & $\dfrac{c^{2}}{2}\left[1-\exp \left(-(r_i / c)^{2}\right)\right]$ & $r_i \exp \left(-(r_i / c)^{2}\right)$ & $\left.\exp \left(-(r_i / c)^{2}\right)\right)$ \\ \hline
		\multicolumn{1}{|l|}{Tukey $\left\{\begin{array}{l}{\text { if }|r_i| \leq c} \\ {\text { if }|r_i|>c}\end{array}\right.$} & \multicolumn{1}{l|}{$\left\{\begin{array}{l}{\dfrac{c^{2}}{6}\left(1-\left[1-(r_i / c)^{2}\right]^{3}\right)} \\ {\left(c^{2} / 6\right)}\end{array}\right.$} & \multicolumn{1}{l|}{$\left\{\begin{array}{l}{r_i\left[1-(r_i / c)^{2}\right]^{2}} \\ {0}\end{array}\right.$} & \multicolumn{1}{l|}{$\left\{\begin{array}{l}{\left[1-(r_i / c)^{2}\right]^{2}} \\ {0}\end{array}\right.$} \\ \hline
		\end{tabular}
		\caption{Table of a few common dispersion $M$-estimators}
	\end{table}
	As far as I know (as redactor of this topic on $M$-estimators), they are all empirical and there is no "beautiful" way to derive them from a logical process. 
	
	Here is a graphical representation of the different functions of the above table:
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/m_estimators.jpg}
		\caption{Graphic representation of a few common dispersion $M$-estimators}
	\end{figure}
	Briefly we give a few description of these functions:
	\begin{itemize}
		\item The "\NewTerm{$L_2$ (least-squares) estimators}", associated to the classic well known Pearson variance, are not robust because their influence function is not bounded.
		
		\item The "\NewTerm{$L_1$ (least-absolute) estimators}", associated to the mean absolute deviation, are not stable because the $\rho$-function $|r_i|$ is not strictly convex in $r_i$. Indeed, the second derivative at $r_i=0$ is unbounded, and an indeterminate solution may result. This estimator in comparison of the $L_2$ reduce the influence of large errors, however they still have an influence because the influence function has no cutoff point.
		
		\item The "\NewTerm{$L_1-L_2$ estimators}" take both the advantage of the $L_1$ estimators to reduce the influence of large errors and that of $L_2$ estimators to be convex.
		
		\item The "\NewTerm{$L_p$ (least-power) estimators}" represents a family of functions. It is $L_2$ with $p=2$ and $L_1$ with $p=1$. It appears that $p$ must be fairly moderate to provide a relatively robust estimator or, in other words, to provide an estimator scarcely perturbed by outlying data. The selection of an optimal $p$ seems to have been investigated, and for $p$ around $1.2$, a good estimate may be expected. However, many difficulties are encountered in the computation when parameter $p$ is in the rage of $1<p<2$, because zero residuals are troublesome.
		
		\item The "\NewTerm{fair estimators}" have everywhere defined continuous derivatives of first three orders, and yields a unique solution. The $95\%$ asymptotic efficiency on the standard Normal distribution is obtained with the tuning constant $c=1.3998$.
		
		\item The "\NewTerm{Huber's estimators}" are parabola in the vicinity of zero, and increases linearly at a given level $|r_i|>k$. The idea is to penalize small residuals quadratically, and large residuals linearly. The $95\%$ asymptotic efficiency on the standard Normal distribution is obtained with the tuning constant $k=1.345$. These estimators are so satisfactory that it has been recommended for almost all situations; very rarely they have been found to be inferior to some other $\rho$-function. However, from time to time, difficulties are encountered, which may be due to the lack of stability in the gradient values of the $\rho$-function of discontinuous second derivative:
		
		
		\item The "\NewTerm{Cauchy's estimators}", also known as the "\NewTerm{Lorentzian estimators}", do not guarantee a unique solution. With a descending first derivative, such a function has a tendency to yield erroneous solutions in a way which cannot be observed. The $95\%$ asymptotic efficiency on the standard Normal distribution is obtained with the tuning constant $c=2.3849$.
		
		\item The other remaining functions have the same problem as the Cauchy function. As can be seen from the influence function, the influence of large errors only decreases linearly with their size. The "\NewTerm{Geman-McClure estimators} and "\NewTerm{Welsch estimators}" try to further reduce the effect of large errors, and the "\NewTerm{Tukey's biweight estimators}" even suppress the outliers. The $95\%$ asymptotic efficiency on the standard Normal distribution of the Tukey's biweight function is obtained with the tuning constant $c=4.6851$; that of the Welsch function, with $c=2.9846$.
	\end{itemize}
	There still exist many other $\rho$-functions, such as Andrew's cosine wave function, the GGW (Generalized Gauss-Weight), the LQQ (Linear Quadratic Quadratic), the "Optimal", etc.
	
	\textbf{Definition (\#\thesection.\mydef):} "\NewTerm{Redescending $M$-estimators}" are $\psi$-type $M$-estimators which have $\psi$ functions that are non-decreasing near the origin, but decreasing toward $0$ far from the origin. Their $\psi$ functions can be chosen to redescend smoothly to zero, so that they usually satisfy $\psi(r_i) = 0$ for all $r_i$ with $|r_i| > k$, where $k$ is referred to as the "\NewTerm{minimum rejection point}". Due to these properties of the $\psi$ function, these kinds of estimators are very efficient, have a high breakdown point and, unlike other outlier rejection techniques, they do not suffer from a masking effect. They are efficient because they completely reject gross outliers, and do not completely ignore moderately large outliers (like median).
	
	Regarding to the previous figure, the Cauchy, Geman-McClure, Welsch, Tukey but also Andrew $M$-estimators are Redescending $M$-estimators!

	Let us see now practically how explicitly, as it is not really obvious..., are computationally implemented two of these estimator typically with a language like \texttt{R} (notice the iteration in the Huber estimator, it is the iterative reweighted least squares (IRLS)!):
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{img/computing/m_estimators_r_script.jpg}
		\caption[Implementation of Tukey's biweight and Huber $M$-estimator in \texttt{R}]{Implementation of Tukey's biweight and Huber $M$-estimator in \texttt{R} (source: \texttt{R} affy package)}
	\end{figure}	
	As the reader may have noticed by reading these script, we are very far from what we have theoretically introduced earlier before! So to understand why, let us focus for example on Tukey's biweight, and the reader will see that it will lead us to the concept of "$W$-estimators"!
	
	For this, the reader let us recall that Beaton and Tukey had introduced in their article:
	
	We can extract $\theta$ quite easily :
	
	In the case of Tukey's biweight we have then (this corresponds to the \texttt{t.bi.mu} in the script above):
	
	As there is a circular reference between the $\theta$ on the left and the $\theta$ on the right, the idea of some Tukey's biweight computer implementation is to take an estimate of the $\theta$ on the right (same idea to estimate $S$). Then we can see for example that Tukey's biweight $M$-estimator of location is computed in the affy package of the \texttt{R} software as:
	
	where we put $w_i=0$ if:
	
	Such an approach (one-shot iteration using a weight function trick) to calculate an $M$-estimator should, according to Beaton and Tukey article, be named a "\NewTerm{$W$-estimator}". But we can iterate the process by writing:
	
	where the weights for a particular $\theta^{[k]}$ depend upon the previous values of $\theta^{[k-1]}$. We simply start with a reasonable guess of the values of $\theta^{[0]}$ (as we did just above) and also especially of the first initial estimate $\theta^{[0]}$, and iterate until the estimates have converged to within whatever we consider to be a reasonable margin of accuracy, that is, until $(\theta^{[k+1]}-\theta^{[k]})^2$ is less than some predicted values. This procedure is an example of the process of iteratively reweighted least squares (IRLS) that Beaton and Tukey in their article recommend to name $W^k$-estimators depending on the number $[k]$ of iterations...
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	IRLS can have some negative effects. Indeed, when IRLS is employed, more and more points can be eliminated from the data set with each iteration. At each step, the most extreme outliers are dropped from the sample because they are outside of a certain main body of the data. Left unchecked, IRLS can sometimes give zero weight to every point and whittle the data set away to nothing, so IRLS estimators need constraints applied to them that will counteract the effect of the redescending weight function. Such constrained estimators are named "\NewTerm{$S$-estimators}".
	\end{tcolorbox}
	
	So far, the ready should have a better understanding why $M$-estimators are more a subject related to "statistical engineering" rather than "pure Statistics": different implementations and also even sometimes different definitions, quite arbitrary parameters, etc...!!!!
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	It can be proved that like the trimmed mean, $M$-estimators, $\tau$-estimator and MM estimator are unbiased and asymptotically Normal at symmetric distributions (useful property to calculate their confidence intervals!).
	\end{tcolorbox}
	
	\pagebreak
	\subsection{Interpolation Techniques}
	The main difference between interpolation and regression, is the definition of the problem they solve.

	Given $n$ data points, when you interpolate, you look for a function that is of some predefined form that has the values in that points exactly as specified. That means given pairs $(x_i,y_i)$ you look for $f$ of some predefined form that satisfies $f(x_i)=y_i$. Most commonly $f$ is chosen to be polynomial, spline (low degree polynomials on intervals between given points).
	
	When you do regression, you look for a function that minimizes some cost, usually sum of square of errors, You don't require the function to have the exact values at given point, you just want a good approximation. In general, your found function $f$ might not satisfy $f(x_i)=y_i$ for any data point, but the cost function, i.e. $\displaystyle\sum_{i=1}^n(f(x_i)-y_i)^2$ will be the smallest possible of all the functions of given form.
	
	There are numerous polynomials interpolation techniques more or less complex and sophisticated. We propose here to present some in ascending order of difficulty.
	
	\subsubsection{Bézier Curves (Bézier Splines)}
	The Russian engineer Pierre Bézier, at the beginnings of the Computer Aided Design (CAD), in the years 11960 (holocene calendar), gave a way to define curves and surfaces from points. This allows direct manipulation, geometric, of curves without having to give a series of equation to the machine to describe a complex curve!!
	
	The application field of Bezier curves is a multifaceted to, very rich, at the crossroads of many diverse mathematical areas: Analysis, Kinematic, Differential Geometry,  Affine Geometry, Projective Geometry, Fractal Geometry, Probabilities, Statistics, Finance (rates curve), etc.
	
	The Bezier are also become essential in their concrete applications in industry, and computer graphics. Most non-engineer and non-scientific people know them through the usage of some drawing tools included in Adobe Illustrator, Adobe Photoshop, 3D Studio Max, Blender, Rhino, Adobe InDesign, Microsoft Office Visio and even Microsoft Office Word and Microsoft Office PowerPoint. Most computer fonts are made with Bézier curves too!
	
	Let us present and study in detail the mathematical approach of this technique:
	
	First, we know that the equation of a segment line which we will denote in this field of study $M$ (with respect for the tradition) joining two points $A(x_1,y_1),B(x_2,y_2)$ is:
	
	and named a "linear interpolation" (LERP).
	
	This is right since when $t=0$ we are on $A$ and when $t=1$ we are on $B$. Therefore $t\in[0,1]$ and the point $M$ describes the whole straight segment $[AB]$.
	
	By definition, the straight segment denoted $[AB]$ is a "\NewTerm{Bézier curve of degree $1$}\index{Bézier curve}" with control points $A$ and $B$ and the polynomials on $t$ or on $t-1$ are the "\NewTerm{Bernstein polynomials of degree $1$}\index{Bernstein polynomials}\label{bernstein polynomial}".
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}
		  \tikzset{
		  ctrlpoint/.style={%
		    draw=gray,
		    circle,
		    inner sep=0,
		    minimum width=1ex,
		  }
		}
		\newcommand\Bezier[2]{% \bezier (lowercase 'b') was already defined elsewhere
		  \node (p1) [ctrlpoint,label=90:$P_1$] at (#1) {};
		  \node (p2) [ctrlpoint,label=90:$P_2$] at (#2) {};
		  \draw [gray] (p1) -- (p2);
		  \draw [blue] (#1) -- (#2);
		}
		  \Bezier{0,0}{3,0}
		  \begin{scope}[xshift=4cm]
		    \Bezier{0,0}{9,2}
		  \end{scope}
		  \begin{scope}[yshift=-5cm]
		    \Bezier{0,0}{1,3}
		  \end{scope}
		  \begin{scope}[xshift=8cm,yshift=-5cm]
		    \Bezier{0,0}{5,0}
		  \end{scope}
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{A few $1$st order Bézier curves}
	\end{figure}
	
	Let us now build a parametric curve by adding a second step to what we just seen:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,728); %set diagram left start at 0, and has height of 728
		
		%Straight Lines [id:da6428494176725479] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ]   (193.5,301) -- (305.5,116.5) ;
		%Straight Lines [id:da543465286737197] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (193.5,301) -- (486.5,301) ;
		%Straight Lines [id:da26773188988559915] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (200.5,288) -- (475.5,288) ;
		%Straight Lines [id:da5462301337351476] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (207.5,274) -- (462.5,274) ;
		%Straight Lines [id:da6047525044328423] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (213.5,262) -- (448.5,262) ;
		%Straight Lines [id:da8418167967972063] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (220.5,248) -- (438.5,248) ;
		%Straight Lines [id:da9965098033637569] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (226.5,236) -- (423.5,236) ;
		%Straight Lines [id:da9724889793485605] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (231.5,224) -- (412.5,224) ;
		%Straight Lines [id:da7109649079487246] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (238.5,213) -- (402.5,213) ;
		%Straight Lines [id:da09958937325776684] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (243,201.5) -- (390.5,201.5) ;
		%Straight Lines [id:da4210142606822509] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (248,188.5) -- (377.5,188.5) ;
		%Straight Lines [id:da45436435326232094] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (255,177.5) -- (367.5,177.5) ;
		%Straight Lines [id:da8374145599839107] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (263,164.5) -- (353.5,164.5) ;
		%Straight Lines [id:da7241978052485321] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (268,152.5) -- (342.5,152.5) ;
		%Straight Lines [id:da8630703810112259] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (274,141.5) -- (331.5,141.5) ;
		%Straight Lines [id:da6195097567171592] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (274,141.5) -- (331.5,141.5) ;
		%Straight Lines [id:da006948164470480522] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (278,129.5) -- (317.5,129.5) ;
		%Straight Lines [id:da8130674057295186] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (285,116.5) -- (305.5,116.5) ;
		%Straight Lines [id:da7293766389917116] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ]   (193.5,301) -- (305.5,116.5) ;
		%Straight Lines [id:da8786608211616187] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ]   (200.5,288) -- (317.5,129.5) ;
		%Straight Lines [id:da15343012978948112] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ]   (207.5,274) -- (331.5,141.5) ;
		%Straight Lines [id:da449982362746816] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ]   (213.5,262) -- (342.5,152.5) ;
		%Straight Lines [id:da42560088798402407] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ]   (220.5,248) -- (353.5,164.5) ;
		%Straight Lines [id:da21983624297915294] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ]   (226.5,236) -- (367.5,177.5) ;
		%Straight Lines [id:da16441755190514118] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ]   (231.5,224) -- (377.5,188.5) ;
		%Straight Lines [id:da2495146557760375] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ]   (238.5,213) -- (390.5,201.5) ;
		%Straight Lines [id:da9757289949693684] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ]   (243,202.5) -- (402.5,213) ;
		%Straight Lines [id:da30917447207759574] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ]   (248,188.5) -- (412.5,224) ;
		%Straight Lines [id:da20473196633978863] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ]   (255,177.5) -- (423.5,236) ;
		%Straight Lines [id:da2510758968525839] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ]   (263,164.5) -- (438.5,248) ;
		%Straight Lines [id:da9215149688277129] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ]   (268,152.5) -- (448.5,262) ;
		%Straight Lines [id:da9820368458906632] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ]   (274,141.5) -- (462.5,274) ;
		%Straight Lines [id:da5320582472611521] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ]   (278,129.5) -- (475.5,288) ;
		%Straight Lines [id:da24632135592758253] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ]   (285,116.5) -- (488.5,299) ;
		%Shape: Circle [id:dp235019327439151] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 189; green, 16; blue, 224 }  ,fill opacity=1 ] (346.15,210.85) .. controls (346.15,208.31) and (348.21,206.25) .. (350.75,206.25) .. controls (353.29,206.25) and (355.35,208.31) .. (355.35,210.85) .. controls (355.35,213.39) and (353.29,215.45) .. (350.75,215.45) .. controls (348.21,215.45) and (346.15,213.39) .. (346.15,210.85) -- cycle ;
		%Straight Lines [id:da5204678096171613] 
		\draw [line width=1.5]    (292.5,104) -- (488.5,299) ;
		\draw [shift={(488.5,299)}, rotate = 44.85] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=1.5]      (0, 0) circle [x radius= 4.36, y radius= 4.36]   ;
		\draw [shift={(292.5,104)}, rotate = 44.85] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=1.5]      (0, 0) circle [x radius= 4.36, y radius= 4.36]   ;
		%Straight Lines [id:da05030639552583027] 
		\draw [line width=1.5]    (292.5,104) -- (193.5,301) ;
		\draw [shift={(193.5,301)}, rotate = 116.68] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=1.5]      (0, 0) circle [x radius= 4.36, y radius= 4.36]   ;
		\draw [shift={(292.5,104)}, rotate = 116.68] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=1.5]      (0, 0) circle [x radius= 4.36, y radius= 4.36]   ;
		%Straight Lines [id:da2170076316371643] 
		\draw [color={rgb, 255:red, 189; green, 16; blue, 224 }  ,draw opacity=1 ][line width=1.5]    (248,188.5) -- (412.5,224) ;
		\draw [shift={(412.5,224)}, rotate = 12.18] [color={rgb, 255:red, 189; green, 16; blue, 224 }  ,draw opacity=1 ][fill={rgb, 255:red, 189; green, 16; blue, 224 }  ,fill opacity=1 ][line width=1.5]      (0, 0) circle [x radius= 4.36, y radius= 4.36]   ;
		\draw [shift={(248,188.5)}, rotate = 12.18] [color={rgb, 255:red, 189; green, 16; blue, 224 }  ,draw opacity=1 ][fill={rgb, 255:red, 189; green, 16; blue, 224 }  ,fill opacity=1 ][line width=1.5]      (0, 0) circle [x radius= 4.36, y radius= 4.36]   ;
		
		% Text Node
		\draw (162,294.4) node [anchor=north west][inner sep=0.75pt]    {$A$};
		% Text Node
		\draw (303,92.4) node [anchor=north west][inner sep=0.75pt]    {$B$};
		% Text Node
		\draw (499,294.4) node [anchor=north west][inner sep=0.75pt]    {$C$};
		% Text Node
		\draw (213,174.4) node [anchor=north west][inner sep=0.75pt]    {$M_{1}$};
		% Text Node
		\draw (427,214.4) node [anchor=north west][inner sep=0.75pt]    {$M_{2}$};
		% Text Node
		\draw (339,219.4) node [anchor=north west][inner sep=0.75pt]    {$M$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Idea behind 2nd order Bézier curves}
	\end{figure}
	\begin{enumerate}
		\item 1st step:
		\begin{enumerate}
			\item Given $M_1(t)$ the barycenter of $(A,1-t)$ and $(B,t)$ and where $M_1(t)$ describes the segment $[AB]$.
			\item  Given $M_2(t)$ the barycenter of $(B,1-t)$ and $(C,t)$ and where $M_2(t)$ describes the segment $[BC]$.
		\end{enumerate}
		\item 2nd step:
		
		Given $M(t)$ the barycenter of $(M_1(t),1-t),(M_2(t),t)$.\\
			
		By construction $M (t)$ is therefore at the same proportion of the segment $[M_1(t),M_2(t)]$ relative to the segment $[AB]$ or to $M_2(t)$ compared to the segment $[BC]$.\\
			
		The curve obtained is then the envelope of the segments $[M_1(t),M_2(t)]$: at any point $M(t)$, the tangent to the curve is therefore the segment $[M_1(t),M_2(t)]$.
	\end{enumerate}
	$M(t)$ then describes a Bézier curve of order $2$, which, by construction starts at $A$ and ends at $C$, and has for tangents $[AB]$ on $A$ and $[CB]$ on $C$.
	
	This is in fact a parabolic arc (that we denote logically by $[ABC]$ in this field of study):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}
		  \tikzset{
		  ctrlpoint/.style={%
		    draw=gray,
		    circle,
		    inner sep=0,
		    minimum width=1ex,
		  }
		}
		\newcommand\Bezier[3]{% \bezier (lowercase 'b') was already defined elsewhere
		  \node (p1) [ctrlpoint,label=90:$P_1$] at (#1) {};
		  \node (p2) [ctrlpoint,label=90:$P_2$] at (#2) {};
		  \node (p3) [ctrlpoint,label=90:$P_3$] at (#3) {};
		  \draw [gray] (p1) -- (p2) -- (p3);
		  \draw [blue] (#1) .. controls (#2) .. (#3);
		}
		  \Bezier{0,0}{1,1}{2,-1}
		  \begin{scope}[xshift=4cm]
		    \Bezier{0,0}{9,2}{7,0}
		  \end{scope}
		  \begin{scope}[yshift=-5cm]
		    \Bezier{0,0}{1,3}{7,0}
		  \end{scope}
		  \begin{scope}[xshift=8cm,yshift=-5cm]
		    \Bezier{0,0}{-2,4}{5,0}
		  \end{scope}
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{A few $2$nd order Bézier curves}
	\end{figure}
	This is the magic of Bézier curves! We get smooth curves only by using combinations of linear interpolations (LERP)!! The example above is for example a quadratic Bézier curve, or a Bézier curve of order $2$.
	
	By the same pattern, we can define a Bézier curve of $n$ points $P_i$ with $i=1...n$. This is what we name the "\NewTerm{Casteljau algorithm}\index{Casteljau algorithm}" (the underlying idea is to recursively LERP until there is nothing more to LERP). Thus, given:
	
	We get:
	
	The recurrence ending for:
	
	Thus, for $n=2$ we have:
	
	Therefore:
	
	Thus, we have necessarily with two points the equation of a straight line (Bézier curve of order $1$).
	
	Now consider $M_3(t)$ the Bézier curve of order $2$, therefore we have the points always defined by:
	
	We then have by the recurrence relation:
	
	where we have eliminated the terms containing unspecified points.
	
	We have therefore:
	
	Therefore it comes for the 2nd order Bézier curves:
	
	or in vector form (more consistent with the usual mathematical notation and therefore we also better understand the meaning of "\NewTerm{vectorial drawing}\index{vectorial drawing}"):
	
	and in matrix form (the matrix visible below is named the "Bézier caracteristic matrix" sometimes):
	
	By the same reasoning, we get for a Bézier curve of order $3$ (it's by far the most common Bézier curve in graphic design!):
	
	or in vector form:
	
	Which corresponds generically to:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,728); %set diagram left start at 0, and has height of 728
		
		%Shape: Axis 2D [id:dp6113697579449013] 
		\draw  (201,266.84) -- (489.5,266.84)(229.85,17) -- (229.85,294.6) (482.5,261.84) -- (489.5,266.84) -- (482.5,271.84) (224.85,24) -- (229.85,17) -- (234.85,24)  ;
		%Curve Lines [id:da6902691444338058] 
		\draw    (258.5,103.6) .. controls (414.5,30.6) and (294.5,232.6) .. (446.5,219.6) ;
		\draw [shift={(446.5,219.6)}, rotate = 355.11] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		\draw [shift={(258.5,103.6)}, rotate = 334.92] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		%Straight Lines [id:da13568231092779204] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (258.5,103.6) -- (414.5,31.6) ;
		\draw [shift={(414.5,31.6)}, rotate = 335.22] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		%Straight Lines [id:da06718856111929994] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (446.5,219.6) -- (293.5,231.6) ;
		\draw [shift={(293.5,231.6)}, rotate = 175.52] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		
		% Text Node
		\draw (215,269) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
		% Text Node
		\draw (247,76.4) node [anchor=north west][inner sep=0.75pt]    {$P_{1}$};
		% Text Node
		\draw (424,25.4) node [anchor=north west][inner sep=0.75pt]    {$P_{2}$};
		% Text Node
		\draw (268,222.4) node [anchor=north west][inner sep=0.75pt]    {$P_{3}$};
		% Text Node
		\draw (458,210.4) node [anchor=north west][inner sep=0.75pt]    {$P_{4}$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Examples of Bézier curves of order $3$}
	\end{figure}
	Or:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}
		  \tikzset{
		  ctrlpoint/.style={%
		    draw=gray,
		    circle,
		    inner sep=0,
		    minimum width=1ex,
		  }
		}
		\newcommand\Bezier[4]{% \bezier (lowercase 'b') was already defined elsewhere
		  \node (p1) [ctrlpoint,label=90:$P_1$] at (#1) {};
		  \node (p2) [ctrlpoint,label=90:$P_2$] at (#2) {};
		  \node (p3) [ctrlpoint,label=90:$P_3$] at (#3) {};
		  \node (p4) [ctrlpoint,label=90:$P_4$] at (#4) {};
		  \draw [gray] (p1) -- (p2) -- (p3) -- (p4);
		  \draw [blue] (#1) .. controls (#2) and (#3) .. (#4);
		}
		  \Bezier{0,0}{1,1}{2,-1}{3,0}
		  \begin{scope}[xshift=4cm]
		    \Bezier{0,0}{9,2}{-2,2}{7,0}
		  \end{scope}
		  \begin{scope}[yshift=-5cm]
		    \Bezier{0,0}{1,3}{2,3}{7,0}
		  \end{scope}
		  \begin{scope}[xshift=8cm,yshift=-5cm]
		    \Bezier{0,0}{-2,4}{4,-1}{5,0}
		  \end{scope}
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{A few $3$rd order Bézier curves}
	\end{figure}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us plot now a Bézier curve of order $4$ with Maple 4.00b. For this purpose we will use the following commands:\\
	
	\texttt{>restart:with(plots):\\
	>x[0]:=1: y[0]:=4:\\
	>x[1]:=6: y[1]:=6:\\
	>x[2]:=1: y[2]:=2:\\
	>x[3]:=8: y[3]:=2.5:\\
	>f:=t->x[0]*(1-t)\string^3+3*x[1]*t*(1-t)\string^2+3*x[2]*t\string^2*(1-t)+x[3]*t\string^3;\\
	>g:=t->y[0]*(1-t)\string^3+3*y[1 ]*t*(1-t)\string^2+3*y[2]*t\string^2*(1-t)+y[3]*t\string^3\\
	>G:=plot([f(t),g(t),t=0..1],thickness=2):\\
	>t0:=textplot([x[0],y[0],`P[0]`],align=ABOVE):\\	
	>t1:=textplot([x[1],y[1],`P[1]`],align=RIGHT):\\	
	>t2:=textplot([x[2],y[2],`P[2]`],align=RIGHT):\\
	>t3:=textplot([x[3],y[3],`P[3]`],align=ABOVE):\\
	>Or:=textplot([0,0,`Origin`],align=ABOVE):\\
	>pp:=pointplot({[x[0],y[0]],[x[1],y[1]],[x[2],y[2]],[x[3],y[3]]},\\
	symbol=circle,color=navy):\\
	>display(G,t0,t1,t2,t3,pp,Or);\\}
	
	This will give:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/b_splines_order_4_maple.jpg}
	\end{figure}
	\end{tcolorbox}
	Now let us take again the previous Bezier curve with a different notation:
	
	We first notice easily the following proportionality:
	
	and if we look more closely at the coefficients, we note that we have also:
	
	It is neither more nor less than the Pascal's triangle !! So the coefficients are simply the binomial coefficients (\SeeChapter{see section Calculus page \pageref{binomial theorem}}) given for the order $n$ in our example by:
	
	Thus, "\NewTerm{Bernstein polynomials}\index{Bernstein polynomials}" are defined by:
	
	and finally the Bernstein curves of order $n$ are given by:
	
	In fact, if we had denoted previously the sum as follows:
	
	we would then have the Bernstein polynomials that are given (which is more respectful of the traditions ...) by:
	
	This is a very practical relation because it allows easily and quickly to calculate the polynomial corresponding to a sequence of Bezier curve of order $n$. The Bernstein polynomials can be geometrically interpreted as a factor of each point taken as a vector and summing them up to get the final Bézier curve! These polynomials might seem to come out of nowhere but we have seen that's what we get when rearranging the LERP maths to be expressed as a facto of each point. These polynomials act as influence of each control point. So in the beginning, the first point has all influence, this is then slowly sifht over the second control point, etc. until de last control point has all the influence. This is a special type of weighted sum, named "\NewTerm{convex combination}\index{convex combination}".
	
	Then we have:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	A Bezier curve is completely changed as soon as at least on point is moved. Then we say that the method of Bezier is a "global method".
	\end{tcolorbox}	
	Bézier curves have a huge issue however. If we move a control point, all the curve is influenced! In other words: we have no local control of the curve! Every movement of every point, affects the entire curve! And another issue is that given the number of LERP or polynomials to calculate, it's computationnaly unstable/expensive to calculate!

	A well-known example of Bézier curves of order $3$ is the "Pen tool" of Adobe Photoshop or Adobe Illustrator softwares. Indeed, these tools create a series of Bézier curves of order $3$ whose point $P_2$ is set afterwards with the mouse using something named "Handles" or "Control Points" in Adobe practitioners language... Here is below an example taken from one of these programs done with the Pen tool with $5$ points (thus $4$ splines):
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/adobe_spline.jpg}
		\caption{Screenshot of a Bézier spline with a drawing program}
	\end{figure}
	As the user does not move the points handles all points are aligned on the straight line. We then feel like to have a spline of order $2$.
	
	In the figure below we have a spline with 4 Bézier curves having obviously each one its own Bernstein Polynomials:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/bezier_spline_construction.jpg}
		\caption[Example of a Bézier spline with its underlying construction]{Example of a Bézier spline with its underlying construction (author: Freya Holmér)}
	\end{figure}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	A circle, drawn by a professional drawing software is in practice composed of $4$ Bezier arcs. To observe this particularity, simply draw a circle with Adobe Illustrator for example, and then select it to reveal the Bezier control points arcs that defines it.\\
	
	We will look at the best way to choose the control points of these arcs so that they look like circle quadrants, and then we will see the difference between the drawing produced (vector circle) and true (bitmap) circle:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,728); %set diagram left start at 0, and has height of 728
		
		%Shape: Circle [id:dp3311317072744797] 
		\draw   (178.4,182.3) .. controls (178.4,114.2) and (233.6,59) .. (301.7,59) .. controls (369.8,59) and (425,114.2) .. (425,182.3) .. controls (425,250.4) and (369.8,305.6) .. (301.7,305.6) .. controls (233.6,305.6) and (178.4,250.4) .. (178.4,182.3) -- cycle ;
		%Straight Lines [id:da363257091724301] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (178.4,182.3) -- (178.4,106.6) ;
		\draw [shift={(178.4,106.6)}, rotate = 270] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		\draw [shift={(178.4,182.3)}, rotate = 270] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		%Straight Lines [id:da6340417677076868] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (178.4,182.3) -- (178.4,260.6) ;
		\draw [shift={(178.4,260.6)}, rotate = 90] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		%Straight Lines [id:da7128556415917933] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (425,182.3) -- (425,106.6) ;
		\draw [shift={(425,106.6)}, rotate = 270] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		\draw [shift={(425,182.3)}, rotate = 270] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		%Straight Lines [id:da6406253288535666] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (425,182.3) -- (425,260.6) ;
		\draw [shift={(425,260.6)}, rotate = 90] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		%Straight Lines [id:da3775312615670119] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (301.7,59) -- (222.5,59) ;
		\draw [shift={(222.5,59)}, rotate = 180] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		\draw [shift={(301.7,59)}, rotate = 180] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		%Straight Lines [id:da17914744617774447] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (301.7,59) -- (385.5,59) ;
		\draw [shift={(385.5,59)}, rotate = 0] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		\draw [shift={(301.7,59)}, rotate = 0] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		%Straight Lines [id:da1923476602020886] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (299.7,306) -- (220.5,306) ;
		\draw [shift={(220.5,306)}, rotate = 180] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		\draw [shift={(299.7,306)}, rotate = 180] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		%Straight Lines [id:da7828273533578465] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (299.7,306) -- (383.5,306) ;
		\draw [shift={(383.5,306)}, rotate = 0] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		\draw [shift={(299.7,306)}, rotate = 0] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Example of constructing a circle with Bézier curves}
	\end{figure}
	Let us take the first quadrant or radius $1$ centered at the origin:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,728); %set diagram left start at 0, and has height of 728
		
		%Shape: Axis 2D [id:dp6113697579449013] 
		\draw  (201,266.84) -- (489.5,266.84)(229.85,17) -- (229.85,294.6) (482.5,261.84) -- (489.5,266.84) -- (482.5,271.84) (224.85,24) -- (229.85,17) -- (234.85,24)  ;
		%Curve Lines [id:da31041900415508383] 
		\draw    (230,82) .. controls (334.5,83.6) and (411.38,170.58) .. (411.5,266.6) ;
		\draw [shift={(411.5,266.6)}, rotate = 89.93] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		\draw [shift={(230,82)}, rotate = 0.88] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		%Straight Lines [id:da8063792684392477] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (230,82) -- (330.5,82) ;
		\draw [shift={(330.5,82)}, rotate = 0] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		%Straight Lines [id:da7843964087457806] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (411.5,266.6) -- (411.5,171.6) ;
		\draw [shift={(411.5,171.6)}, rotate = 270] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		
		% Text Node
		\draw (215,269) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
		% Text Node
		\draw (401,279.4) node [anchor=north west][inner sep=0.75pt]    {$P_{1}$};
		% Text Node
		\draw (419,159.4) node [anchor=north west][inner sep=0.75pt]    {$P_{2}$};
		% Text Node
		\draw (323,56.4) node [anchor=north west][inner sep=0.75pt]    {$P_{3}$};
		% Text Node
		\draw (203,71.4) node [anchor=north west][inner sep=0.75pt]    {$P_{4}$};
		
		\end{tikzpicture}
	\end{figure}
	It is approached by a Bézier arc whose control points are $P_1,P_2,P_3,P_4$. The ends of the Bézier arc being $P_1$ and $P_4$, it is natural to choose $P_1(1,0)$ and $P_4(0,1)$. Intuition leads us to choose $P_2(1,k)$ and $P_3(k,1)$ and it remains to find a positive value of $k$ so that the Bezier curve looks like a circular arc.\\
	
	We thus obtain the parametric equation of the Bézier curve:
	
	Therefore:
	
	We can look for example the value of $k$ for which the arc passes through the point:
	
	in $t=0.5$. It then becomes very simple from the parametric equation to determine $k$. It is simply for $x$ (or $y$) of a simple equation of one unknown.
	\end{tcolorbox} 
	With this tool we can now create plenty of illustrations (static or animated\footnote{However animation with Bézier splines have huge flaws that requires to build a new kind of spline named "Hermite spline"}) on computers!
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/bezier_curves_application.jpg}
		\caption[]{A few practical example of Bézier splines (author: Freya Holmér)}
	\end{figure}
	There are plenty of other splines useful in practice used to eliminate various flaws (but each of these splines has also some flaws): Uniform splines, Nonuniform splines, Natural splines, Hermite splines, Cardinal splines, B-splines (the "B" is not an abreviation for Bézier but for "basis"), Catmull-Rome splines, Interpolating splines, Non-uniform rational B-spline (NURBS)...
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/naive_splines_summary.jpg}
		\caption[A visual summary of naive splines]{A visual summary of naive splines (author: Freya Holmér)}
	\end{figure}
	\begin{table}[H]
		\resizebox{\textwidth}{!}{\begin{tabular}{|r|c|c|c|c|c|}
		\hline
		\rowcolor[HTML]{C0C0C0} 
		\textbf{Name} & \textbf{Degree} & \textbf{Continuity} & \textbf{Tangents} & \textbf{Interpol.} & \textbf{Use cases} \\ \hline
		Bézier & 3 & $\mathcal{C}^0 / \mathrm{C}^1$ & manual & some & \begin{tabular}[c]{@{}c@{}}shapes,\\ fonts, vector graphics\end{tabular} \\ \hline
		Hermite & 3 & $\mathrm{C}^0 / \mathrm{C}^1$ & explicit & all & \begin{tabular}[c]{@{}c@{}}animation, physics simulation,\\  interpolation\end{tabular} \\ \hline
		Catmull-Rom & 3 & $\mathrm{C}^1$ & auto & all & animation, path smoothing \\ \hline
		B-Spline & 3 & $\mathrm{C}^2$ & auto & none & \begin{tabular}[c]{@{}c@{}}curvature-sensitive shapes,\\ animations such as camera paths\end{tabular} \\ \hline
		Linear & 1 & $\mathrm{C}^0$ & auto & all & \begin{tabular}[c]{@{}c@{}}dense data \& interpolation\\ where smoothness doesn't matter\end{tabular} \\ \hline
		\multicolumn{1}{|l|}{...} & \multicolumn{1}{l|}{...} & \multicolumn{1}{l|}{...} & \multicolumn{1}{l|}{...} & \multicolumn{1}{l|}{...} & \multicolumn{1}{l|}{...} \\ \hline
		\end{tabular}}
		\caption[Table summary of naive splines]{Table summary of naive splines (inspired by Freya Holmér)}
	\end{table}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Keep in mind, as the vocabulary may be confusing, that:
	\begin{itemize}
		\item If there is only one (polynomial) segment, the spline is often named a "Bézier curve".
	
		\item If each segment is expressed in Bézier form (using Bernstein basis functions), then you might say that the spline is a "Bézier spline".
	
		\item If each polynomial segment has degree $3$, the spline is named a "cubic spline".
	
		\item If each segment is described by its ending positions and derivatives, it is said to be in "Hermite form".
	
		\item The B-spline approach (for which we will detail the underlying mathematics in the future) gives a way of ensuring continuity between segments. In fact, we show that every spline can be represented in B-spline form. So, in that sense, every spline is a B-spline.
	\end{itemize}
	\end{tcolorbox}	
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/bezier_curves_calculation_methods.jpg}
		\caption[Summary of different ways of calculating Bézier curves]{Summary of different ways of calculating Bézier curves (author: Freya Holmér)}
	\end{figure}
	
	\subsubsection{Linear ordering isotonic regression}\index{linear ordering isotonic regression}
	"\NewTerm{Linear ordering isotonic regression}\index{isotonic regression}\label{isotonic regression}" can be understood as approximating given series of $1$-dimensional observations with non-decreasing function. It is similar to inexact smoothing splines, with the difference that we use monotonicity, rather than smoothness, to remove noise from the data. Isotonic relation is quite well known in the field of machine learning for models calibration.
	
	Given values $\vec{a}\in\mathbb{R}^n$ and weights $\vec{w}\in \mathbb{R}^n$ such that $w_i>0$ for all $i$, we want to approximate them by $y_1, \ldots y_n$ as close as possible, subject to a set of constraints of kind $y_i\geq y_j$, ie we want to minimize (obviously sometimes other metrics are used instead of the Euclidean metric!):
	
	with respect to $\vec{y}\in\mathbb{R}^n$ subject to $y_1 \le y_2\le\;\ldots\;\le y_n$.
	
	If all weights equal to $1$, the problem is named "\NewTerm{unweighted isotonic regression}", otherwise it is named "\NewTerm{weighted isotonic regression}".
	
	In pseudocode (non-unique and very likely not optimized):\\\\	
	\begin{algorithm}[H]
	\KwData{$\vec{a}$,$\vec{w}$}
	\KwResult{$\vec{y}$}
	initialization\;
	$a'_1 := a_1$\;
	$w'_1 := w_1$\;
	$j:=1$\;
	$S_0:=0$\;
	$S_1:=1$\;
	\For{$i=2,3,\ldots ,n$}{
        $j:=j+1$ \;
        $a'_j:= a_i$ \;
		$w'_j := w_i$\;
		\While{$j>1$  AND  $a'_j < a'_{j-1}$}{
			$a'_{j-1} =\displaystyle {w'_j a'_j + w'_{j-1} a'_{j-1} \over w'_j + w'_{j-1}}$\;
			$w'_{j-1} = w'_j + w'_{j-1}$\;
			$j := j-1$\;
		}
		$S_j := i$\;
	}
	\For{$k=1,2,\ldots,j$}{
		\For{$l=S_{k-1}+1,\ldots,S_k$}{
		$y_l=a'_k$\;
		}
	}
	\caption{Pool Adjacent Violators Algorithm (PAVA) pseudocode algorithm}
	\end{algorithm}
	Here $S$ defines to which old points each new point corresponds.

	Let us see a very detailed companion example (thanks to Wojciech Kotłowski for having authorized us to reproduce it!).
	
	Let us consider:	
	\begin{table}[H]
	\centering	
	\begin{tabular}{c|ccccccccccccc}
    \toprule
  $x$ & $7$ & $-1$ & $-2$ & $9$ & $2$ & $0$ & $6$ & $3$ & $-3$ & $5$ & $-3$ & $7$ & $-5$  \\
  $y$ & $1$ & $0.4$ & $0.2$ & $0.7$ & $0.7$ & $0.6$ & $0.8$ & $0.2$ & $0.3$ & $0.6$ & $0.4$ & $1$ & $0$ \\
   \bottomrule
 	\end{tabular}
 	\end{table}
 	
 	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,896); %set diagram left start at 0, and has height of 896
		
		%Straight Lines [id:da6127260346035952] 
		\draw    (105.5,384) -- (564.5,384) ;
		\draw [shift={(566.5,384)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da2274438581051741] 
		\draw    (280.5,429) -- (280.5,99) ;
		\draw [shift={(280.5,97)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da45592472815982354] 
		\draw    (287.5,123) -- (273.5,123) ;
		%Straight Lines [id:da9019840442086284] 
		\draw    (287,175) -- (273.5,175) ;
		%Straight Lines [id:da7023107498730674] 
		\draw    (288.5,227) -- (272.5,227) ;
		%Straight Lines [id:da780163053190188] 
		\draw    (287.5,279) -- (273.5,279) ;
		%Straight Lines [id:da5308048341595095] 
		\draw    (288.5,332) -- (273.5,332) ;
		%Straight Lines [id:da16131974723097753] 
		\draw    (153,376) -- (153,392) ;
		%Straight Lines [id:da614881006596903] 
		\draw    (217,376) -- (217,392) ;
		%Straight Lines [id:da8849194578957542] 
		\draw    (342,376) -- (342,392) ;
		%Straight Lines [id:da9871333900526291] 
		\draw    (406,376) -- (406,392) ;
		%Straight Lines [id:da4561559132394859] 
		\draw    (468,376) -- (468,392) ;
		%Straight Lines [id:da33082963442966484] 
		\draw    (531,376) -- (531,392) ;
		%Shape: Circle [id:dp877877861546212] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (117,384) .. controls (117,381.24) and (119.24,379) .. (122,379) .. controls (124.76,379) and (127,381.24) .. (127,384) .. controls (127,386.76) and (124.76,389) .. (122,389) .. controls (119.24,389) and (117,386.76) .. (117,384) -- cycle ;
		%Shape: Circle [id:dp6703287018116855] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (211,332) .. controls (211,329.24) and (213.24,327) .. (216,327) .. controls (218.76,327) and (221,329.24) .. (221,332) .. controls (221,334.76) and (218.76,337) .. (216,337) .. controls (213.24,337) and (211,334.76) .. (211,332) -- cycle ;
		%Shape: Circle [id:dp6699362678729335] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (179,306) .. controls (179,303.24) and (181.24,301) .. (184,301) .. controls (186.76,301) and (189,303.24) .. (189,306) .. controls (189,308.76) and (186.76,311) .. (184,311) .. controls (181.24,311) and (179,308.76) .. (179,306) -- cycle ;
		%Shape: Circle [id:dp80477776959556] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (179,279) .. controls (179,276.24) and (181.24,274) .. (184,274) .. controls (186.76,274) and (189,276.24) .. (189,279) .. controls (189,281.76) and (186.76,284) .. (184,284) .. controls (181.24,284) and (179,281.76) .. (179,279) -- cycle ;
		%Shape: Circle [id:dp3500308983578324] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (242,280) .. controls (242,277.24) and (244.24,275) .. (247,275) .. controls (249.76,275) and (252,277.24) .. (252,280) .. controls (252,282.76) and (249.76,285) .. (247,285) .. controls (244.24,285) and (242,282.76) .. (242,280) -- cycle ;
		%Shape: Circle [id:dp8607784034784998] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (276,227) .. controls (276,224.24) and (278.24,222) .. (281,222) .. controls (283.76,222) and (286,224.24) .. (286,227) .. controls (286,229.76) and (283.76,232) .. (281,232) .. controls (278.24,232) and (276,229.76) .. (276,227) -- cycle ;
		%Shape: Circle [id:dp34712841989459453] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (337,202) .. controls (337,199.24) and (339.24,197) .. (342,197) .. controls (344.76,197) and (347,199.24) .. (347,202) .. controls (347,204.76) and (344.76,207) .. (342,207) .. controls (339.24,207) and (337,204.76) .. (337,202) -- cycle ;
		%Shape: Circle [id:dp9098861268529994] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (369,333) .. controls (369,330.24) and (371.24,328) .. (374,328) .. controls (376.76,328) and (379,330.24) .. (379,333) .. controls (379,335.76) and (376.76,338) .. (374,338) .. controls (371.24,338) and (369,335.76) .. (369,333) -- cycle ;
		%Shape: Circle [id:dp8114111631139136] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (433,228) .. controls (433,225.24) and (435.24,223) .. (438,223) .. controls (440.76,223) and (443,225.24) .. (443,228) .. controls (443,230.76) and (440.76,233) .. (438,233) .. controls (435.24,233) and (433,230.76) .. (433,228) -- cycle ;
		%Shape: Circle [id:dp6326164379324597] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (558,202) .. controls (558,199.24) and (560.24,197) .. (563,197) .. controls (565.76,197) and (568,199.24) .. (568,202) .. controls (568,204.76) and (565.76,207) .. (563,207) .. controls (560.24,207) and (558,204.76) .. (558,202) -- cycle ;
		%Shape: Circle [id:dp4842450162607601] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (464,175) .. controls (464,172.24) and (466.24,170) .. (469,170) .. controls (471.76,170) and (474,172.24) .. (474,175) .. controls (474,177.76) and (471.76,180) .. (469,180) .. controls (466.24,180) and (464,177.76) .. (464,175) -- cycle ;
		%Shape: Circle [id:dp4569960673505069] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (495,123) .. controls (495,120.24) and (497.24,118) .. (500,118) .. controls (502.76,118) and (505,120.24) .. (505,123) .. controls (505,125.76) and (502.76,128) .. (500,128) .. controls (497.24,128) and (495,125.76) .. (495,123) -- cycle ;
		
		% Text Node
		\draw (264,83.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (564,389.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (246,112.4) node [anchor=north west][inner sep=0.75pt]    {$1.0$};
		% Text Node
		\draw (246,164.4) node [anchor=north west][inner sep=0.75pt]    {$0.8$};
		% Text Node
		\draw (246,216.4) node [anchor=north west][inner sep=0.75pt]    {$0.6$};
		% Text Node
		\draw (246,268.4) node [anchor=north west][inner sep=0.75pt]    {$0.4$};
		% Text Node
		\draw (246,320.4) node [anchor=north west][inner sep=0.75pt]    {$0.2$};
		% Text Node
		\draw (265,388.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (143,393.4) node [anchor=north west][inner sep=0.75pt]    {$-4$};
		% Text Node
		\draw (206,393.4) node [anchor=north west][inner sep=0.75pt]    {$-2$};
		% Text Node
		\draw (336,393.4) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (400,393.4) node [anchor=north west][inner sep=0.75pt]    {$4$};
		% Text Node
		\draw (462,393.4) node [anchor=north west][inner sep=0.75pt]    {$6$};
		% Text Node
		\draw (525,393.4) node [anchor=north west][inner sep=0.75pt]    {$8$};
		\end{tikzpicture}
	\end{figure} 
	Step 1: We sort the data in the increasing order of $x$.
	\begin{table}[H]
	\centering
	\begin{tabular}{c|ccccccccccccc}
	    \toprule
	  $x$ & $7$ & $-1$ & $-2$ & $9$ & $2$ & $0$ & $6$ & $3$ & $-3$ & $5$ & $-3$ & $7$ & $-5$  \\
	  $y$ & $1$ & $0.4$ & $0.2$ & $0.7$ & $0.7$ & $0.6$ & $0.8$ & $0.2$ & $0.3$ & $0.6$ & $0.4$ & $1$ & $0$ \\
	   \bottomrule
	 \end{tabular}
	\end{table}
	\vspace*{-1cm}	
	 $$\Downarrow \qquad \Downarrow \qquad \Downarrow$$	 
	\begin{table}[H]
	\centering
	 \begin{tabular}{c|ccccccccccccc}
	    \toprule
	  $x$ & $-5$ & $-3$ & $-3$ & $-2$ & $-1$ & $0$ & $2$ & $3$ & $5$ & $6$ & $7$ & $7$ & $9$ \\
	  $y$ & $0$ & $0.4$ & $0.3$ & $0.2$ & $0.4$ & $0.6$ & $0.7$ & $0.2$ & $0.6$ & $0.8$ & $1$ & $1$ & $0.7$ \\
	   \bottomrule
	 \end{tabular}
	\end{table}
	
	Step 2: We split the data into blocks $B_1,\ldots,B_r$, such that points  with the same $x_t$ fall into the same block.
  
  We assign value $f_i$ to each block ($i=1,\ldots,r$) which is the average of labels in this block.
  
	\begin{table}[H]
	\centering
	\begin{tabular}{c|ccccccccccccc}
	    \toprule
	  $x$ & $-5$ & \color{blue}{$-3$} & \color{blue}{$-3$} & $-2$ & $-1$ & $0$ & $2$ & $3$ & $5$ & $6$ & \color{red}{$7$} & \color{red}{$7$} & $9$ \\
	  $y$ & $0$ & \color{blue}{$0.4$} & \color{blue}{$0.3$} & $0.2$ & $0.4$ & $0.6$ & $0.7$ & $0.2$ & $0.6$ & $0.8$ & \color{red}{$1$} & \color{red}{$1$} & $0.7$ \\
	   \bottomrule
	 \end{tabular}
	\end{table}

	\vspace*{-0.5cm}

  $$\Downarrow \qquad \Downarrow \qquad \Downarrow$$

	\begin{table}[H]
	\centering
	{\setlength{\tabcolsep}{0.4em}
	\begin{tabular}{c|ccccccccccc}
	\toprule
	block & $B_1$ & \color{blue}{$B_2$} & $B_3$ & $B_4$ & $B_5$ & $B_6$ & $B_7$ & $B_8$ & $B_9$ & \color{red}{$B_{10}$} & $B_{11}$   \\[1mm]
	  data & $\{1\}$ & \color{blue}{$\{2,3\}$} & $\{4\}$ & $\{5\}$ & $\{6\}$ & $\{7\}$ & $\{8\}$ & $\{9\}$ & $\{10\}$ & \color{red}{$\{11,12\}$} & $\{13\}$ \\[1mm]
	  $f_i$ & $0$ & \color{blue}{$0.35$} & $0.2$ & $0.4$ & $0.6$ & $0.7$ & $0.2$ & $0.6$ & $0.8$ & \color{red}{$1$} & $0.7$ \\
	\bottomrule
	\end{tabular}}
	\end{table}
	
	Step 3: While there exists a "violator", i.e. a pair of blocks $B_i, B_{i+1}$ such that $f_i > f_{i+1}$, we merge $B_i$ and $B_{i+1}$ and assign a weighted average:
 	
 	
 	\begin{table}[H]
 	 \centering
  {\setlength{\tabcolsep}{0.4em}
 \begin{tabular}{c|ccccccccccc}
    \toprule
  block & $B_1$ & \color{red}{$B_2$} & \color{red}{$B_3$} & $B_4$ & $B_5$ & $B_6$ & $B_7$ & $B_8$ & $B_9$ & $B_{10}$ & $B_{11}$  \\[1mm]
  data & $\{1\}$ & \color{red}{$\{2,3\}$} & \color{red}{$\{4\}$} & $\{5\}$ & $\{6\}$ & $\{7\}$ & $\{8\}$ & $\{9\}$ & $\{10\}$ & $\{11,12\}$ & $\{13\}$ \\[1mm]
  $f_i$ & $0$ & \color{red}{$0.35$} & \color{red}{$0.2$} & $0.4$ & $0.6$ & $0.7$ & $0.2$ & $0.6$ & $0.8$ & $1$ & $0.7$ \\
   \bottomrule
 \end{tabular}}
\end{table}

\vspace*{-1cm}

  $$\Downarrow \qquad \Downarrow \qquad \Downarrow$$

  \begin{table}[H]
  \centering
  {\setlength{\tabcolsep}{0.4em}
 \begin{tabular}{c|cccccccccc}
    \toprule
  block & $B_1$ & \color{red}{$B_2$} & $B_3$ & $B_4$ & $B_5$ & $B_6$ & $B_7$ & $B_8$ & $B_9$ & $B_{10}$ \\[1mm]
  data & $\{1\}$ & \color{red}{$\{2,3,4\}$} & $\{5\}$ & $\{6\}$ & $\{7\}$ & $\{8\}$ & $\{9\}$ & $\{10\}$ & $\{11,12\}$ & $\{13\}$ \\[1mm]
  $f_i$ & $0$ & \color{red}{$0.3$} & $0.4$ & $0.6$ & $0.7$ & $0.2$ & $0.6$ & $0.8$ & $1$ & $0.7$ \\
   \bottomrule
 \end{tabular}}
\end{table}

 	We iterate again:
 	
 	\begin{table}[H]
 	  \centering
  	{\setlength{\tabcolsep}{0.4em}
 	\begin{tabular}{c|cccccccccc}
    \toprule
  block & $B_1$ & $B_2$ & $B_3$ & $B_4$ & \color{red}{$B_5$} & \color{red}{$B_6$} & $B_7$ & $B_8$ & $B_9$ & $B_{10}$ \\[1mm]
  data & $\{1\}$ & $\{2,3,4\}$ & $\{5\}$ & $\{6\}$ & \color{red}{$\{7\}$} & \color{red}{$\{8\}$} & $\{9\}$ & $\{10\}$ & $\{11,12\}$ & $\{13\}$ \\[1mm]
  $f_i$ & $0$ & $0.3$ & $0.4$ & $0.6$ & \color{red}{$0.7$} & \color{red}{$0.2$} & $0.6$ & $0.8$ & $1$ & $0.7$ \\
   \bottomrule
 	\end{tabular}}
	\end{table}

\vspace*{-1cm}

  $$\Downarrow \qquad \Downarrow \qquad \Downarrow$$

  \begin{table}[H]
  \centering
  {\setlength{\tabcolsep}{0.4em}
 \begin{tabular}{c|ccccccccc}
    \toprule
  block & $B_1$ & $B_2$ & $B_3$ & $B_4$ & \color{red}{$B_5$} & $B_6$ & $B_7$ & $B_8$ & $B_9$ \\[1mm]
  data & $\{1\}$ & $\{2,3,4\}$ & $\{5\}$ & $\{6\}$ & \color{red}{$\{7,8\}$} & $\{9\}$ & $\{10\}$ & $\{11,12\}$ & $\{13\}$ \\[1mm]
  $f_i$ & $0$ & $0.3$ & $0.4$ & $0.6$ & \color{red}{$0.45$}  & $0.6$ & $0.8$ & $1$ & $0.7$ \\
   \bottomrule
 \end{tabular}}
\end{table}

	We iterate once again:
	
	 \begin{table}[H]
	 \centering
  {\setlength{\tabcolsep}{0.4em}
 \begin{tabular}{c|ccccccccc}
    \toprule
  block & $B_1$ & $B_2$ & $B_3$ & \color{red}{$B_4$} & \color{red}{$B_5$} & $B_6$ & $B_7$ & $B_8$ & $B_9$ \\[1mm]
  data & $\{1\}$ & $\{2,3,4\}$ & $\{5\}$ & \color{red}{$\{6\}$} & \color{red}{$\{7,8\}$} & $\{9\}$ & $\{10\}$ & $\{11,12\}$ & $\{13\}$ \\[1mm]
  $f_i$ & $0$ & $0.3$ & $0.4$ & \color{red}{$0.6$} & \color{red}{$0.45$}  & $0.6$ & $0.8$ & $1$ & $0.7$ \\
   \bottomrule
 \end{tabular}}
\end{table}

\vspace*{-1cm}

  $$\Downarrow \qquad \Downarrow \qquad \Downarrow$$

  \begin{table}[H]
  \centering
  {\setlength{\tabcolsep}{0.4em}
 \begin{tabular}{c|cccccccc}
    \toprule
  block & $B_1$ & $B_2$ & $B_3$ & \color{red}{$B_4$} & $B_5$ & $B_6$ & $B_7$ & $B_8$ \\[1mm]
  data & $\{1\}$ & $\{2,3,4\}$ & $\{5\}$ & \color{red}{$\{6,7,8\}$} & $\{9\}$ & $\{10\}$ & $\{11,12\}$ & $\{13\}$ \\[1mm]
  $f_i$ & $0$ & $0.3$ & $0.4$ & \color{red}{$0.5$}  & $0.6$ & $0.8$ & $1$ & $0.7$ \\
   \bottomrule
 \end{tabular}}
\end{table}

	We iterate once again:
	
	\begin{table}[H]
	\centering
  {\setlength{\tabcolsep}{0.4em}
 \begin{tabular}{c|cccccccc}
    \toprule
  block & $B_1$ & $B_2$ & $B_3$ & $B_4$ & $B_5$ & $B_6$ & \color{red}{$B_7$} & \color{red}{$B_8$} \\[1mm]
  data & $\{1\}$ & $\{2,3,4\}$ & $\{5\}$ & $\{6,7,8\}$ & $\{9\}$ & $\{10\}$ & \color{red}{$\{11,12\}$} & \color{red}{$\{13\}$} \\[1mm]
  $f_i$ & $0$ & $0.3$ & $0.4$ & $0.5$  & $0.6$ & $0.8$ & \color{red}{$1$} & \color{red}{$0.7$} \\
   \bottomrule
 \end{tabular}}
\end{table}

\vspace*{-1cm}

  $$\Downarrow \qquad \Downarrow \qquad \Downarrow$$

  \begin{table}[H]
  \centering
  {\setlength{\tabcolsep}{0.4em}
 \begin{tabular}{c|cccccccc}
    \toprule
  block & $B_1$ & $B_2$ & $B_3$ & $B_4$ & $B_5$ & $B_6$ & \color{red}{$B_7$}\\[1mm]
  data & $\{1\}$ & $\{2,3,4\}$ & $\{5\}$ & $\{6,7,8\}$ & $\{9\}$ & $\{10\}$ & \color{red}{$\{11,12,13\}$} \\[1mm]
  $f_i$ & $0$ & $0.3$ & $0.4$ & $0.5$  & $0.6$ & $0.8$ & \color{red}{$0.9$} \\
   \bottomrule
 \end{tabular}}
\end{table}

	There is no more violators yet!
	
	Reading out the solution:
	
	\begin{table}[H]
	\centering
  {\setlength{\tabcolsep}{0.4em}
 \begin{tabular}{c|cccccccc}
    \toprule
  block & $B_1$ & $B_2$ & $B_3$ & $B_4$ & $B_5$ & $B_6$ & \color{red}{$B_7$}\\[1mm]
  data & $\{1\}$ & $\{2,3,4\}$ & $\{5\}$ & $\{6,7,8\}$ & $\{9\}$ & $\{10\}$ & \color{red}{$\{11,12,13\}$} \\[1mm]
  $f_i$ & $0$ & $0.3$ & $0.4$ & $0.5$  & $0.6$ & $0.8$ & \color{red}{$0.9$} \\
   \bottomrule
 \end{tabular}}
\end{table}

	\vspace*{-1cm}
  $$\Downarrow \qquad \Downarrow \qquad \Downarrow$$

\begin{table}[H]
\centering
  {\setlength{\tabcolsep}{0.5em}
  \begin{tabular}{c|ccccccccccccc}
    \toprule
  $x$ & $-5$ & $-3$ & $-3$ & $-2$ & $-1$ & $0$ & $2$ & $3$ & $5$ & $6$ & $7$ & $7$ & $9$ \\[1mm]
  $y$ & $0$ & $0.4$ & $0.3$ & $0.2$ & $0.4$ & $0.6$ & $0.7$ & $0.2$ & $0.6$ & $0.8$ & $1$ & $1$ & $0.7$ \\[1mm]
  \color{red}{$f^*$} & \color{red}{$0$} & \color{red}{$0.3$} & \color{red}{$0.3$} & \color{red}{$0.3$} & \color{red}{$0.4$} & \color{red}{$0.5$} & \color{red}{$0.5$} & \color{red}{$0.5$} & \color{red}{$0.6$} & \color{red}{$0.8$} & \color{red}{$0.9$} & 
  \color{red}{$0.9$} & \color{red}{$0.9$}\\
   \bottomrule
 \end{tabular}}
\end{table}

	Which gives:
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,896); %set diagram left start at 0, and has height of 896
		
		%Straight Lines [id:da6127260346035952] 
		\draw    (105.5,384) -- (564.5,384) ;
		\draw [shift={(566.5,384)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da2274438581051741] 
		\draw    (280.5,429) -- (280.5,99) ;
		\draw [shift={(280.5,97)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da45592472815982354] 
		\draw    (287.5,123) -- (273.5,123) ;
		%Straight Lines [id:da9019840442086284] 
		\draw    (287,175) -- (273.5,175) ;
		%Straight Lines [id:da7023107498730674] 
		\draw    (288.5,227) -- (272.5,227) ;
		%Straight Lines [id:da780163053190188] 
		\draw    (287.5,279) -- (273.5,279) ;
		%Straight Lines [id:da5308048341595095] 
		\draw    (288.5,332) -- (273.5,332) ;
		%Straight Lines [id:da16131974723097753] 
		\draw    (153,376) -- (153,392) ;
		%Straight Lines [id:da614881006596903] 
		\draw    (217,376) -- (217,392) ;
		%Straight Lines [id:da8849194578957542] 
		\draw    (342,376) -- (342,392) ;
		%Straight Lines [id:da9871333900526291] 
		\draw    (406,376) -- (406,392) ;
		%Straight Lines [id:da4561559132394859] 
		\draw    (468,376) -- (468,392) ;
		%Straight Lines [id:da33082963442966484] 
		\draw    (531,376) -- (531,392) ;
		%Shape: Circle [id:dp877877861546212] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (117,384) .. controls (117,381.24) and (119.24,379) .. (122,379) .. controls (124.76,379) and (127,381.24) .. (127,384) .. controls (127,386.76) and (124.76,389) .. (122,389) .. controls (119.24,389) and (117,386.76) .. (117,384) -- cycle ;
		%Shape: Circle [id:dp6703287018116855] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (211,332) .. controls (211,329.24) and (213.24,327) .. (216,327) .. controls (218.76,327) and (221,329.24) .. (221,332) .. controls (221,334.76) and (218.76,337) .. (216,337) .. controls (213.24,337) and (211,334.76) .. (211,332) -- cycle ;
		%Shape: Circle [id:dp6699362678729335] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (179,306) .. controls (179,303.24) and (181.24,301) .. (184,301) .. controls (186.76,301) and (189,303.24) .. (189,306) .. controls (189,308.76) and (186.76,311) .. (184,311) .. controls (181.24,311) and (179,308.76) .. (179,306) -- cycle ;
		%Shape: Circle [id:dp80477776959556] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (179,279) .. controls (179,276.24) and (181.24,274) .. (184,274) .. controls (186.76,274) and (189,276.24) .. (189,279) .. controls (189,281.76) and (186.76,284) .. (184,284) .. controls (181.24,284) and (179,281.76) .. (179,279) -- cycle ;
		%Shape: Circle [id:dp3500308983578324] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (242,280) .. controls (242,277.24) and (244.24,275) .. (247,275) .. controls (249.76,275) and (252,277.24) .. (252,280) .. controls (252,282.76) and (249.76,285) .. (247,285) .. controls (244.24,285) and (242,282.76) .. (242,280) -- cycle ;
		%Shape: Circle [id:dp8607784034784998] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (276,227) .. controls (276,224.24) and (278.24,222) .. (281,222) .. controls (283.76,222) and (286,224.24) .. (286,227) .. controls (286,229.76) and (283.76,232) .. (281,232) .. controls (278.24,232) and (276,229.76) .. (276,227) -- cycle ;
		%Shape: Circle [id:dp34712841989459453] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (337,202) .. controls (337,199.24) and (339.24,197) .. (342,197) .. controls (344.76,197) and (347,199.24) .. (347,202) .. controls (347,204.76) and (344.76,207) .. (342,207) .. controls (339.24,207) and (337,204.76) .. (337,202) -- cycle ;
		%Shape: Circle [id:dp9098861268529994] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (369,333) .. controls (369,330.24) and (371.24,328) .. (374,328) .. controls (376.76,328) and (379,330.24) .. (379,333) .. controls (379,335.76) and (376.76,338) .. (374,338) .. controls (371.24,338) and (369,335.76) .. (369,333) -- cycle ;
		%Shape: Circle [id:dp8114111631139136] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (433,228) .. controls (433,225.24) and (435.24,223) .. (438,223) .. controls (440.76,223) and (443,225.24) .. (443,228) .. controls (443,230.76) and (440.76,233) .. (438,233) .. controls (435.24,233) and (433,230.76) .. (433,228) -- cycle ;
		%Shape: Circle [id:dp6326164379324597] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (558,202) .. controls (558,199.24) and (560.24,197) .. (563,197) .. controls (565.76,197) and (568,199.24) .. (568,202) .. controls (568,204.76) and (565.76,207) .. (563,207) .. controls (560.24,207) and (558,204.76) .. (558,202) -- cycle ;
		%Shape: Circle [id:dp4842450162607601] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (464,175) .. controls (464,172.24) and (466.24,170) .. (469,170) .. controls (471.76,170) and (474,172.24) .. (474,175) .. controls (474,177.76) and (471.76,180) .. (469,180) .. controls (466.24,180) and (464,177.76) .. (464,175) -- cycle ;
		%Shape: Circle [id:dp4569960673505069] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (495,123) .. controls (495,120.24) and (497.24,118) .. (500,118) .. controls (502.76,118) and (505,120.24) .. (505,123) .. controls (505,125.76) and (502.76,128) .. (500,128) .. controls (497.24,128) and (495,125.76) .. (495,123) -- cycle ;
		%Straight Lines [id:da7298345892133209] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (122,384) -- (184,306) ;
		%Straight Lines [id:da37847775556883856] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (215.5,306) -- (184,306) ;
		%Straight Lines [id:da17036861878158716] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (280.5,249) -- (215.5,306) ;
		%Straight Lines [id:da14269417479765467] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (373.5,249) -- (280.5,249) ;
		%Straight Lines [id:da4863943327782918] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (438,228) -- (373.5,249) ;
		%Straight Lines [id:da30170357734627906] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (469,175) -- (438,228) ;
		%Straight Lines [id:da015073756429622609] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (500.5,145) -- (469,176) ;
		%Straight Lines [id:da06735522152861151] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (562.5,145) -- (500.5,145) ;
		
		% Text Node
		\draw (264,83.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (564,389.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (246,112.4) node [anchor=north west][inner sep=0.75pt]    {$1.0$};
		% Text Node
		\draw (246,164.4) node [anchor=north west][inner sep=0.75pt]    {$0.8$};
		% Text Node
		\draw (246,216.4) node [anchor=north west][inner sep=0.75pt]    {$0.6$};
		% Text Node
		\draw (246,268.4) node [anchor=north west][inner sep=0.75pt]    {$0.4$};
		% Text Node
		\draw (246,320.4) node [anchor=north west][inner sep=0.75pt]    {$0.2$};
		% Text Node
		\draw (265,388.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (143,393.4) node [anchor=north west][inner sep=0.75pt]    {$-4$};
		% Text Node
		\draw (206,393.4) node [anchor=north west][inner sep=0.75pt]    {$-2$};
		% Text Node
		\draw (336,393.4) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (400,393.4) node [anchor=north west][inner sep=0.75pt]    {$4$};
		% Text Node
		\draw (462,393.4) node [anchor=north west][inner sep=0.75pt]    {$6$};
		% Text Node
		\draw (525,393.4) node [anchor=north west][inner sep=0.75pt]    {$8$};
		\end{tikzpicture}
	\end{figure} 
	This corresponds perfectly to what we get with the \texttt{R} software (see our companion book on the subject!).
	
	\subsubsection{Euler Method}\label{Euler method}
	Strictly speaking the Euler Method is much more than an interpolation method. But it can be introduced first as a liner piece-wise interpolation technique. In fact it provides an approximation (in the broadest sense) of a function $f (x)$ whose first derivative is known.
	
	The interpolated points are not necessarily equidistant as the interpolation is piece-wise but as soon we extrapolate, the step is therefore by tradition equidistant and denoted by $h$ and the distance between two points given by: $x_i=x_0+ih$. We denote by $f(x)$ the exact value and $y_i$ the approximate value.
	
	The idea is quite simple. If in $\mathbb{R}^2$ we know two points $(y_i,x_i)$ and $(y_{i+1},x_{i+1})$ we can calculate piece-wise the slope as:
	
	In more complicated examples and real applications of the Euler methods we know the true derivative on $x_i$.
	
	The previous relation can be written:
	
	And therefore for small $h$ we can do a linear interpolation between two points.
	
	If the derivative is known (and the derivative can be of non-linear function!) we have the more general relation:
	
	That is traditionally written:
	
	named also the "\NewTerm{Euler difference equation}\index{Euler difference equation}". This relation can also be used to solve some simple numerical differential equation as illustrated by the following example:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. Let's solve the following initial value problem ("Cauchy problem" style with $y'(x)=f'(x)$ and $y(x_0)=y_0$):
	\begin{gather*}
		y^{\prime}= y\, ;\, y(0) = 1
	\end{gather*}
	using the Euler's method. The recurrence relation can be written in is this special case:
	\begin{gather*}
		y_{n+1}= (1+h) y_{n}\qquad y_{0}=1
	\end{gather*}
	i.e.:
	\begin{gather*}
		y_{n}=(1+h)^{n}
	\end{gather*}
	For a given $x$ we have:
	\begin{gather*}
		n=\dfrac{x}{h}
	\end{gather*}
	and thus:
	\begin{gather*}
		y_{n}=(1+h)^{\frac{x}{h}}=[(1+h)^{\frac{1}{h}}]^{x}
	\end{gather*}
	For  $h\rightarrow 0_+$ the approximate solution $y_n$ converges to the exact solution ${e}^{x}$.\\
	
	E2. Suppose that we are given the equation $5y' - 3y = 0$ with boundary condition $y = y_0$ at $x = 0$. This gives a unique function $y$. We can approximate this by considering discrete steps of length $h$ between $x_n$ and $x_{n+1}$ (using the simple Euler numerical scheme) we can approximate the equation by:
	\[
	  5\frac{y_{n+1} - y_n}{h} - 3y_n \approx 0
	\]
	Rearranging the terms, we have $y_{n+1} \approx (1 + \frac{3}{5}h)y_n$. Applying the relation successively, we have
	\begin{align*}
	  y_n &= \left(1 + \frac{3}{5}h\right)y_{n - 1}\\
	  &= \left(1 + \frac{3}{5}h\right)\left(1 + \frac{3}{5}h\right)y_{n - 2}\\
	  &= \left(1 + \frac{3}{5}h\right)^ny_0
	\end{align*}
	For each given value of $x$, choose $h = x/n$, so:
	\[
	  y_n = y_0\left(1 + \frac{3}{5}(x/n)\right)^n
	\]
	Taking the limit as $n\to \infty$, we have:
	\[
	  y(x) = \lim_{n\to +\infty} y_0\left(1 + \frac{3x/5}{n}\right)^n = y_0 e^{3x/5}
	\]
	in agreement with the solution of the differential equation.
	\begin{center}
	  \begin{tikzpicture}[yscale = 0.15]
	    \draw [->] (-2, 0) -- (3.5, 0) node [right] {$x$};
	    \draw [->] (0, -5) -- (0, 25) node [above] {$y$};
	
	    \draw [blue, semithick, domain=-2:3] plot (\x, {exp(\x)}) node [right] {solution of diff. eq.};
	    \node [anchor = south east] at (0, 1) {1};
	    \node [circ] at (0, 1) {};
	
	    \draw [green, semithick] (0, 1) -- (1, 2) -- (2, 4.7) -- (3, 12) node [right] {discrete approximation};
	    \draw [dashed] (1, 2) -- (1, 0);
	    \draw [dashed] (2, 4.7) -- (2, 0);
	    \draw [dashed] (3, 12) -- (3, 0);
	
	    \node [anchor = north east] at (0, 0) {O};
	    \node [anchor = north west] at (0, 0) {$x_0$};
	    \node [below] at (1, 0) {$x_1$};
	    \node [below] at (2, 0) {$x_2$};
	    \node [below] at (3, 0) {$x_3$};
	  \end{tikzpicture}
	\end{center}
	\end{tcolorbox}

	The "\NewTerm{local truncation error}\index{local truncation error}" (LTE) of the Euler method is an error made in a single step. It is the difference between the numerical solution after one step.
	
	To calculate this error we can write first:
	
	For the exact solution (considered as exact...), we use the Taylor expansion (\SeeChapter{see section Sequences and Series page \pageref{taylor series}}):
	
	Also sometimes written:
	
	This relation can also be used to solve some simple numerical differential equation as illustrated by the following example:
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\	
	We want to use the method of Taylor's expansion of the third order to solve the following initial value problem (again a "Cauchy problem" style with $y'(x)=f'(x)$ and $y(x_0)=y_0$):
	\begin{gather*}
		y'= \dfrac{4}{x^2} - y^2-\dfrac{y}{x}  \qquad \text{with: } y(1)=0
	\end{gather*}
	For $n=0,1,2,\dots$ we have:
	\begin{gather*}
		 y(x_{n}+h):= y_{n+1}=y_{n}+h y'(x_{n})+\dfrac{h^{2}}{2}y''(x_{n})+\dfrac{h^{3}}{3!}y'''(x_{n})+\mathcal{O}(h^{4})
	\end{gather*}
	Here:
	 \begin{eqnarray}
	 \hspace{-10mm}
	 x_0=1 \,, & & \qquad y_0=0 \,, \nonumber \\
	 y'(x_n) & = & \displaystyle\frac{4}{x_n^2}- y_n^2
	 -\displaystyle\frac{y_n}{x_n} \,, \nonumber \\
	  y''(x_n) & = &  -\displaystyle\frac{8}{x_n^3}-2 y_n y'(x_n)-
	  \displaystyle\frac{y'(x_n) x_n-y_n}{x_n^2}
	   \,=\, -\displaystyle\frac{12}{x_n^3}-
	  \displaystyle\frac{6y_n}{x_n^2}
	 +\displaystyle\frac{3y_n^2}{x_n}+2 y_n^3 \,, \nonumber \\
	 y'''(x_n) & = &\displaystyle\frac{24}{x_n^4}-2(y'(x_n))^2-2y_n
	 y''(x_n)-\frac{y''(x_n) x_n^2-2(y'(x_n)x_n-y_n)}{x_n^3}\nonumber\\
	 & = &\displaystyle\frac{12}{x_n^4}-
	  \displaystyle\frac{42y_n}{x_n^3}
	 +\displaystyle\frac{21y_n^2}{x_n^2}-\displaystyle\frac{12y_n^3}{x_n}
	 -6y_n^4 \,.\nonumber
	 \end{eqnarray}
	The table below	shows the computed values of the solution in the point $x_N=2$ for various $N$ (and thus for various $h=1/N$):
	 \begin{table}[H]
	   \centering
	 $$
	 \begin{array}{|l|c|c|c|c|c|c|c|} \\[-10mm] \hline
	 & x& 1 & 1.2 & 1.4 & 1.6 & 1.8 & 2 \\ \hline
	 h=0.2& & 0 & 0.576000 & 0.835950 & 0.920226 & 0.920287 & 0.884745
	 \\
	  h=0.1 & y(x)  & 0 & 0.581645 & 0.838338 & 0.919251 & 0.918141 &
	  0.882631 \\
	  h=0.05 & & 0 & 0.582110 & 0.838443 & 0.919062 & 0.917872 &
	 0.882386 \\ \hline
	 \multicolumn{8}{|c|}
	 {\mbox{Exact solution:}\qquad y(x)=
	 \displaystyle\frac{2(x^4-1)}{x(x^4+1)}\,,
	 \qquad y(2)= 0.882353} \\ [+0.2cm] \hline
	 \end{array}
	 $$
	 \end{table}
	\end{tcolorbox}
	
	The local truncation error (one step!!!) introduced by the Euler method is given by the difference between these equations:
	
	This shows that for small $h$, the local truncation error is approximately proportional to $h^2$. This makes the Euler method less accurate (for small $h$) than other higher-order techniques such as Runge-Kutta methods that we will see later.
	
	\pagebreak
	\subsubsection{Polynomial of collocation}
	In the section of Sequences and Series we discussed the general unsuitability of Taylor polynomials for approximation. These polynomials are useful only over small intervals for functions whose derivatives exist and are easily evaluated. In this section we find approximating polynomials that can be determined simply by specifying certain points on the plane through which they must pass.

	Given $y=f(x)$ a known function in explicit form or in tabulated form, and suppose that a given number of values:
	
	are given. The known points $(x_i,f(x_i))$ are named "\NewTerm{support points}\index{support points}".
	
	"\NewTerm{Interpolate $f$}\index{interpolate}" in this context means to estimate the values of $f$ for the horizontal $x$ knows values located between $x_0$ and $x_n$, that is to say in the range of interpolation by an approximate function $y=P(x)$, which satisfies the "collocations conditions" (nothing to do with your room-mate...!):
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
		
		%Shape: Axis 2D [id:dp12522984413681781] 
		\draw  (103.5,175.6) -- (462.5,175.6)(290.5,25.6) -- (290.5,202.6) (455.5,170.6) -- (462.5,175.6) -- (455.5,180.6) (285.5,32.6) -- (290.5,25.6) -- (295.5,32.6)  ;
		%Curve Lines [id:da4836172100785172] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (123.5,162.6) .. controls (230.5,16) and (261.5,131) .. (307.5,130) .. controls (353.5,129) and (358.5,118) .. (394.56,136.59) .. controls (430.62,155.18) and (455.36,146.02) .. (462.5,145) ;
		%Straight Lines [id:da7385853362974422] 
		\draw    (148,171) -- (148,180) ;
		%Straight Lines [id:da5689907425443099] 
		\draw    (195,171) -- (195,180) ;
		%Straight Lines [id:da07355939467818717] 
		\draw    (225,171) -- (225,180) ;
		%Straight Lines [id:da5636806025702079] 
		\draw    (279,171) -- (279,180) ;
		%Straight Lines [id:da28139622681525545] 
		\draw    (325,171) -- (325,180) ;
		%Straight Lines [id:da3264870473768693] 
		\draw    (356,171) -- (356,180) ;
		%Straight Lines [id:da8796793159782199] 
		\draw    (445,171) -- (445,180) ;
		%Curve Lines [id:da6988061224875504] 
		\draw [line width=1.5]    (125.5,218) .. controls (132.89,212.46) and (152.5,92.6) .. (170.5,67.6) .. controls (188.5,42.6) and (197.9,131.73) .. (212.5,116.6) .. controls (227.1,101.47) and (227.5,70.6) .. (245.5,63.6) .. controls (263.5,56.6) and (286.5,170.6) .. (304.5,168.6) .. controls (322.5,166.6) and (319.5,122.6) .. (337.5,117) .. controls (355.5,111.4) and (386.5,220.4) .. (408.5,217) .. controls (430.5,213.6) and (452.5,103.6) .. (454.5,92) ;
		%Straight Lines [id:da6943436591560572] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (148,133.4) -- (148,175) ;
		%Straight Lines [id:da556231937851388] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (195,95.6) -- (195,171) ;
		%Straight Lines [id:da975885992274484] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (148,133.4) -- (290.5,133.4) ;
		%Straight Lines [id:da2558771644929754] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (195,95.6) -- (290.5,95.6) ;
		
		% Text Node
		\draw (272,14.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (465,176.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (467,138.4) node [anchor=north west][inner sep=0.75pt]    {$y=f( x)$};
		% Text Node
		\draw (461,65.4) node [anchor=north west][inner sep=0.75pt]    {$y=p( x)$};
		% Text Node
		\draw (138,180.9) node [anchor=north west][inner sep=0.75pt]    {$x_{0}$};
		% Text Node
		\draw (185,180.9) node [anchor=north west][inner sep=0.75pt]    {$x_{1}$};
		% Text Node
		\draw (218,180.9) node [anchor=north west][inner sep=0.75pt]    {$x_{2}$};
		% Text Node
		\draw (269,180.9) node [anchor=north west][inner sep=0.75pt]    {$x_{3}$};
		% Text Node
		\draw (318,180.9) node [anchor=north west][inner sep=0.75pt]    {$x_{4}$};
		% Text Node
		\draw (346,180.9) node [anchor=north west][inner sep=0.75pt]    {$x_{5}$};
		% Text Node
		\draw (436,180.9) node [anchor=north west][inner sep=0.75pt]    {$x_{n}$};
		% Text Node
		\draw (292,130.4) node [anchor=north west][inner sep=0.75pt]    {$f_{0}$};
		% Text Node
		\draw (292,87.4) node [anchor=north west][inner sep=0.75pt]    {$f_{1}$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Illustration of the interpolation concept with the collocation technique}
	\end{figure}
	The function $P$ is named "\NewTerm{collocation function}\index{collocation function}" on the $x_i$. When $P$ is a polynomial, we speak of "\NewTerm{collocation polynomial}\index{collocation polynomial}" or of "\NewTerm{interpolation polynomial}\index{interpolation polynomial}".
	
	"\NewTerm{Extrapolate}\index{extrapolate}" a function means study the $f(x)$ by $P(x)$ for $x$-located "outside" of the interpolation interval.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	It goes without saying that the interpolation is a very important tool for all researchers, statisticians and others.
	\end{tcolorbox}	
	When we know a polynomial of degree $n$ on $n + 1$ points, we can know then by a simple way (but not very fast - but there are several methods) this polynomial completely.
	
	To determine the polynomial, we will use the results presented above in our study of systems of linear equations. The disadvantage of the method presented here is that you have to guess to what type of polynomial you are dealing with (the order at least) and know what are the good points to chose (when there is a choice...).
	
	A particular example should suffice for understanding this method, the generalization being quite simple (see further below).
	
	Given the univariate second degree polynomial:
	
	and we know the following point (you will notice the ingenuity of the points selected by the authors of these lines ...):
	
	We deduce therefore the following system of equations:
	
	System that once solved using the state of the art techniques (\SeeChapter{see section Linear Algebra page \pageref{linear systems}}) gives us:
	
	Let us see the general case:
	\begin{theorem}
	Given  $(x_i,f(x_i))$  support points, with $x_i\neq x_j$ if $i\neq j$. Then there exists a polynomial $P_n(x)$ of degree less than or equal to $n$, and only one, such as $P_n(x_i)=f_i$ for $i=0,1,\ldots ,n$.
	\end{theorem}
	\begin{dem}
	Let us write:
	
	The collocation conditions:
	
	are thus written:
	
	This is a system of $n + 1$ equations in $n + 1$ unknowns.
	
	Its determinant is written (\SeeChapter{see section Linear Algebra page \pageref{determinant}}):
	
	relation that we name "\NewTerm{Vandermonde determinant}\index{Vandermonde determinant}". We know that if the system has a solution, the determinant of the system must be non-zero (\SeeChapter{see section Linear Algebra page \pageref{determinant matrix inverse}}).
	
	Let us show by an example (by taking a polynomial of the same degree as the one we used above) that the determinant is calculated using the previous relation (the reader will generalize by induction):
	
	Therefore in the case $n=2$, we consider the determinant:
	
	thus corresponding to the system (for reminder):
	
	Let us calculate this determinant following the column $1$ (by making use of cofactors as proved in the section of Linear Algebra at page \pageref{determinant matrix inverse}):
	
	This polynomial can be written:
	
	Which is written:
	
	As the $x_9$ is in the statement of our problem are all different such that $x_i\neq x_j$ then the system has a unique solution! This proves that there is always an interpolation polynomial.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}	
	\end{dem}
	It should be noted however that it is not a method of polynomial regression. Indeed, with a method of polynomial regression, we might choose a higher degree for the polynomial as the number of points we have.
	
	\pagebreak
	\subsubsection{Lagrange polynomials}\label{lagrange polynomial interpolation method}
	The interpolation  polynomial Lagrange method (used a lot in practice because the algorithm is very simple and therefore effective) consider that we initially have (know) $n + 1$ points such as:
	
	and that we are looking for a collocation polynomial that passes through all the points. The idea of polynomial interpolation of Lagrange is therefore simple and very clever (as always someone must have think to it first...). Let us observe the chart below where we have $5$ points by which we seek to pass a collocation polynomial:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/lagrange_polynomial.jpg}
		\caption[Illustration of concept of Lagrange interpolation]{Illustration of concept of Lagrange interpolation (source: Wikipedia)}
	\end{figure}
	To find the collocation polynomial in red (we obviously suppose we don't know it initially), the idea is that for every $x_i$ we associate a different polynomial of degree $n$ and not null on $f(x_i)$ but null on all other points $(x_j)_{j\neq i}$. For this, as we see in the graph above, it is necessary that for each $i$ we associate a polynomial that has $n$ roots as:
	
	So we see that we have $5$ polynomials, each with $4$ roots (hence of order $4$) and which are respectively zero on all the points $x_j\neq x_i$ (as you can see on the graph). The coefficients $A_i$ are constants to be determined.
	
	Now nothing prevents us to sum all polynomial together since we will always have the right value on $f(x_i)$ (since the other polynomials are zero on this same point). Either by generalization the notation, the sum becomes:
	
	By injecting respectively $x_0,x_1,\ldots ,x_i,\ldots ,x_n$, we get in the general case:
	
	We deduce then immediately in the general case:
	
	Substituting the values of the constants in the initial expression of the sum, we get:
	
	Which can be written in condensed form (this relation will be useful to us later for some numerical integration methods!):
	
	where the term:
	
	is named "\NewTerm{Lagrange polynomial interpolation coefficients}\index{Lagrange polynomial interpolation coefficients}". 
	
	Notice that if we use the Lagrange polynomial to approximate a known analytic and continuous function (hence with an infinite number of points), then obviously there may be an approximation error. This will be denoted:
	
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us approximate the function $y=f(x)=x\,\sin(2x+\pi/4)+1$ by a polynomial $P_n$ of degree $n=3$, based on the following $n+1=4$ points:
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|l|l|l|}
		\hline
		$i$ & $0$ & $1$ & $2$ & $3$ \\ \hline
		$x_i$ & $-1$ & $0$ & $1$ & $2$ \\ \hline
		$y_i=f(x_i)$ & $1.937$ & $1.000$ & $1.349$ & $-0.995$ \\ \hline
		\end{tabular}
	\end{table}
	Based on these points, we construct the Lagrange polynomials as the basis functions of the polynomial space:
	\begin{gather*}
	\begin{aligned} 
		L_{0}(x) &=\frac{(x-0)(x-1)(x-2)}{-6}=\frac{x^{3}-3 x^{2}+2 x}{-6} \\ 
		L_{1}(x) &=\frac{(x+1)(x-1)(x-2)}{2}=\frac{x^{3}-2 x^{2}-x+2}{2} \\ 
		L_{2}(x) &=\frac{(x+1)(x-0)(x-2)}{-2}=\frac{x^{3}-x^{2}-2 x}{-2} \\ 
		L_{3}(x)&=\frac{(x+1)(x-0)(x-1)}{6}=\frac{x^{3}-x}{6} 	
	\end{aligned}
	\end{gather*}
	Note that indeed $L_0(x)+L_1(x)+L_2(x)+L_3(x)=1$. The interpolating polynomial can be obtained as a weighted sum of these basis functions:
	\begin{gather*}
		P_{3}(x)=1.937 L_{0}(x)+1.0 L_{1}(x)+1.349 L_{2}(x)-0.995 L_{3}(x)=1.0+0.369 x+0.643 x^{2}-0.663 x^{3}
	\end{gather*}
	\centering
		\includegraphics[scale=0.90]{img/computing/lagrange_interpolation_example.jpg} 
	\end{tcolorbox}
	
	\subsubsection{Newton polynomials (divided differences)}
	As we have just seen, the Lagrange interpolation relies on the $n+1$ interpolation points:
	
	all of which need to be available to calculate each of the basis polynomials $l_i(x)$.
	
	The goodness of an approximation depends on the number of approximating points and also
on their locations. One problem with the Lagrange interpolating polynomial is that we need $n$
additions, $2n^2+2n$ subtractions, $2n^2+n-1$ multiplications, and $n+1$ divisions to evaluate $P(x)$ at a given point $x$. Even after all the denominators have been calculated once and for all we still need $n$ additions, $n^2+n$ subtractions, and $n^2+n$ multiplications.
	
	Furthermore, if additional points are to be used when they become available, all basis polynomials need to be recalculated. Another problem is that in practice, one may be uncertain as to how many interpolation points to use. So one may want to increase them over time and see whether the approximation gets better. In doing so, one would like to use the old approximation. It is not clear how to do that easily with the Lagrange form. In this sense, the Lagrange form is not incremental (plus it is also awkward to program).

	In comparison, in the Newton interpolation, when more data points are to be used, additional basis polynomials and the corresponding coefficients can be calculated, while all existing basis polynomials and their coefficients remain unchanged. Due to the additional terms, the degree of interpolation polynomial is higher and the approximation error may be reduced (e.g., when interpolating higher order polynomials).

	Specifically, the basis polynomials of the Newton interpolation are calculated as below:
	
	and the Newton interpolating polynomial is constructed:
	
	Note that the last data point $(x_n, y_n)$ is not used in any of the basis polynomials, but it is used for calculating the last coefficient $c_n$, as shown below. For the $n$th degree polynomial $N_n(x)$ to pass all $n+1$ points $(x_i,\;y_i),\;(i=0,\ldots,n)$, it needs to satisfy the following $n+1$ equations:
	
	The last relation is named the "\NewTerm{Newton's divided-difference formula}\index{Newton's divided-difference formula}".

	The relation can also be expressed in matrix form:
	
	
	The $n+1$ coefficients $c_0,\ldots,c_n$ can be obtained by solving these $n+1$ equations in the triangular equation system progressively from top down:
	
	
	In general, we have the:
	
	which is the expanded form of the $k$th divided differences $f[x_0,\ldots,x_k]$ of the first $k+1$ points. Now the Newton polynomial interpolation can be written as:
	

	Due to the uniqueness of the polynomial interpolation (here "uniqueness" means only in the way that all previous polynomial interpolation methods are such that for some given points $\{(x_i,y_i)\}_{i=0}^n$ we just have $p_n(x_i)=y_i$... and nothing more!), this Newton interpolation polynomial is the same as that of the Lagrange and the power (i.e. Taylor) function interpolations:
	
	They are the same $n$th degree polynomial but expressed in terms of different basis polynomials weighted by different coefficients.

	We can now consider some important facts all related to the Newton polynomial interpolation.	
	
	\begin{itemize}
		\item When an additional node point $(x_{n+1},\,y_{n+1})$ is available and to be used, all previous basis polynomials and their corresponding coefficients remain unchanged, we only need to obtain a new basis polynomial of degree $n+1$:
		
		together with its coefficient, the $(n+1)$th divided difference $c_{n+1}=f[x_0,\ldots,x_n,x_{n+1}]$. The new interpolation polynomial of degree $n+1$ can then be obtained by including an extra term in the summation above:
		
		As $N_{n+1}(x)$ passes through the new point $(x_{n+1},\,y_{n+1})$, we have:
		
		But $x_{n+1}$ is just an arbitrary point, we can replace it by $x$, and get:
		
		
		\item If all $n+1$ points $x_0=a\le x_1\le\ldots\le x_{n-1}\le x_n=b$ are equally spaced, i.e.:
		
				then Newton's divided difference interpolation can take a simpler form. For any point $x\in(a,\,b)$, we let $c=(x-x_0)/h$ so that it can be represented as $x=x_0+ch$, and $x-x_i=(x_0+ch)-(x_0+ih)=(c-i)h$, now the Newton polynomial can be written as:
		
		where we used the binomial-coefficient notation:
		
	\end{itemize}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us approximate the function $y=f(x)=x\,\sin(2x+\pi/4)+1$ by a polynomial of degree $n=3$, based on the following $n+1=4$ points:
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|l|l|l|}
		\hline
		$i$ & $0$ & $1$ & $2$ & $3$ \\ \hline
		$x_i$ & $-1$ & $0$ & $1$ & $2$ \\ \hline
		$y_i=f(x_i)$ & $1.937$ & $1.000$ & $1.349$ & $-0.995$ \\ \hline
		\end{tabular}
	\end{table}
	Based on $f[x_i]=f(x_i),\;(i=0,\cdots,n)$, we can find all other divided differences recursively in tabular form as shown below. In general, $f[x_i,\cdots,x_j]$ can be found based on its left neighbour $f[x_{i+1},\cdots,x_j]$ and top-left neighbour $f[x_i,\cdots,x_{j-1}]$:
	\begin{gather*}
		f[x_i,\cdots,x_j]=\frac{f[x_{i+1},\ldots,x_j]-f[x_i,\ldots,x_{j-1}]}{x_j-x_i}
	\end{gather*}
	\begin{table}[H]
		\resizebox{\textwidth}{!}{\begin{tabular}{|l|l|l|l|l|}
		\hline
		$x_i$ & $0$th & $1$st & $2$nd & $3$rd \\ \hline
		$x_0=-1$ & $f[x_0]=1.937$ &  &  &  \\ \hline
		$x_1=0$ & $f[x_1]=1.000$ & $f[x_0,x_1]=-0.937$ &  &  \\ \hline
		$x_2=1$ & $f[x_2]=1.349$ & $f[x_1,x_2]=0.349$ & $f[x_0,x_2]=0.643$ &  \\ \hline
		$x_3=2$ & $f[x_3]=-0.995$ & $f[x_2,x_3]=-2.343$ & $f[x_1,x_3]=-1.346$ & $f[x_0,x_3]=-0.663]$ \\ \hline
		\end{tabular}}
	\end{table}
	The coefficients are the four divided differences along the diagonal:
	\begin{gather*} 
		c_0=f[x_0]=f(x_0)=y_0=1.937, c_1=f[x_0,x_1]=-0.937, c_2=f[x_0,x_1,x_2]=0.643
	\end{gather*}		
	and $c_3=f[x_0,x_1,x_2,x_3]=-0.663$. Alternatively, they can also be represented in the expanded form:
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\begin{gather*}
		c_i=f[x_0,\ldots,x_i]=\sum_{j=0}^n\frac{f(x_j)}{\prod_{i=0,\,i\ne j}^n(x_j-x_i)}
	\end{gather*}
	Now the Newton interpolating polynomial can be obtained as:
	\begin{gather*}
		\begin{aligned}
		N_3(x)&= \sum_{i=0}^3 c_i n_i(x) = 1.937-0.937(x+1)+0.643(x+1)(x-0)\\
			&=-0.663(x+1)(x-0)(x-1)=1+0.369x+0.643x^2-0.663x^3
	 	\end{aligned}
 	\end{gather*}
 	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/newton_polynomial_interpolation_example.jpg}
	\end{figure}
	\end{tcolorbox}
	Using the Newton interpolating polynomials is then usually the best choice in comparison of Taylor, Vandermonde (i.e. collocation polynomial\footnote{Using  the Vandermonde  matrix is  indeed not  a  very  good  method  for  any  situation. The system is ill-conditioned and therefore the coefficients may be calculated very inaccurately. Also the amount of work is excessive!}) or Lagrange polynomials.  It has the advantage that data pairs can be added and interpolated by merely adding one additional term to the previous interpolating polynomial.  Under other restrictions the coefficients give information about the derivatives of a function being approximated as well as the error.
	
	\subsubsection{Errors in Polynomial Interpolation}
	There are multiple ways to derive the approximation error of polynomial interpolation. In our personal and subjective point of view, all proofs known to us so far are not really convincing...
	
	So let us introduce the error term in an engineer way rather than in a tricky mathematician one. For this, let us recall that during our Study of Taylor series, we have proved that the Lagrange remainder (\SeeChapter{see section Sequences and Series page \pageref{Lagrange Remainder}}) was given by:
	
	For the case of polynomial where we have multiple points $x_0,x_1,\ldots,x_n$ rather than a unique one $x_0$, we can guess that the error may be written:
	
	In the field of polynomial interpolation (that we use the Taylor, Lagrange or Newton methods) that latter is often written:
	
	where $\xi$ is some (unknown) point in the interval $[a,b]$. The precise location of this point depends on $\{x_i\}_{i=0}^n$. Here $f^{(n+1)}(\xi)$ is the $(n+1)$st derivative of $f(x)$ evaluated at the point $x=\xi_x$.
	
	Therefore, for example (an example that will be useful to us when we will study numerical integration), the Lagrange polynomial given for recall by:
		
	Will be written in it's full form as:
	
	
	\pagebreak
	\subsection{Roots search}
	Many equation encountered in practice or in theory cannot be solved by closed form or analytical methods. Consequently, only a numerical approach can be obtained in a finite number of operations.

	The mathematician Évariste Galois has proved, in particular, that the polynomial equation $P_n(x)$ (\SeeChapter{see section Calculus page \pageref{polynomial}}) has no algebraic solution if $P_n(x)$ is of degree $n>4$.

	There are numerous algorithms that gives the possibility to calculate the roots of equations of the type $f(x)=0$ with an almost arbitrary precision. We will see in this section only the main one.

	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Caution! The implementation of such algorithms need always at least an approximate knowledge (randomly or deterministic) of the searched value and also the behaviour (shape) of the function near the root. A table of the function values (\SeeChapter{see section Functional Analysis page \pageref{table of variations}}) or its graphical representation (see same section) can in simple cases help to acquire this prerequisites knowledge.
	\end{tcolorbox}
	
	
	If the univariate equation to solve is under the form $g(x)=h(x)$, we plot the curves representing $g$ and $h$. The roots of the equation $g(x)=h(x)$ being given by the abscissa of the intersection points of the both curves.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Before solving by numerical methods the equation $f(x)=0$, we have to check that the function $f$ satisfy some constraints. For example, the function $f$ has to be at least strictly monotone near the root that we will denote here by $\bar{x}$, when the Newton's method is applied. It is many times useful, even absolutely necessary, to determine an interval $[a,b]$ such that:
	\begin{itemize}
		\item $f$ is continuous on $[a,b]$ of class $\mathcal{C}^1$

		\item $f(a)f(b)<0$
		
		\item $\exists \bar{x}$ unique, $\bar{x}\in [a,b],f(r)=0$
	\end{itemize}
	\end{tcolorbox}

	\subsubsection{Proportional parts methods}
	The implementation, on a computer, of this method is particularly simple. The conditions to be satisfied being only that in the interval $[a,b]$:
	\begin{itemize}
		\item $f$ must be continuous on $[a,b]$ of class $\mathcal{C}^1$

		\item $f$ must be monotone near the root $r$
		
		\item $f(a)f(b)<0$ to be sure that there is a root
	\end{itemize}
	As we already know, in a small interval, we can replace a curve by a straight line segment. There are several possible situations but here is a special one but that can be generalized easily to anything:
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,638); %set diagram left start at 0, and has height of 638
		
		%Straight Lines [id:da4532635103021656] 
		\draw    (178.5,217.6) -- (472.5,217.6) ;
		\draw [shift={(474.5,217.6)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Curve Lines [id:da8615258959199659] 
		\draw [line width=1.5]    (196,267) .. controls (298.5,252.6) and (262.5,80.6) .. (455.5,79.6) ;
		%Straight Lines [id:da5898377552469714] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (294,156) -- (294,226.6) ;
		%Straight Lines [id:da3678079292337961] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (430,77) -- (429.5,221.6) ;
		%Straight Lines [id:da584701330726181] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (203,204) -- (203,274.6) ;
		%Straight Lines [id:da956134301883578] 
		\draw    (186.5,228.6) -- (455.5,65.6) ;
		
		% Text Node
		\draw (483,210.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (288,221.4) node [anchor=north west][inner sep=0.75pt]    {$a$};
		% Text Node
		\draw (423,221.4) node [anchor=north west][inner sep=0.75pt]    {$b$};
		% Text Node
		\draw (262,220.4) node [anchor=north west][inner sep=0.75pt]    {$\overline{x}$};
		% Text Node
		\draw (181,194.4) node [anchor=north west][inner sep=0.75pt]    {$x_{1}$};
		% Text Node
		\draw (183,272.4) node [anchor=north west][inner sep=0.75pt]    {$f( x_{1})$};
		% Text Node
		\draw (296,159.4) node [anchor=north west][inner sep=0.75pt]    {$f( a)$};
		% Text Node
		\draw (431,83.4) node [anchor=north west][inner sep=0.75pt]    {$f( b)$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Local approximation of a curve by a straight line segment}
	\end{figure}
	In this figure, we get by using the theorems of Thales (\SeeChapter{see section Euclidean Geometry page \pageref{thales theorem}}):
	
	therefore:
	
	If $|f(a)| \ll |f(b)|$, we can neglect $f(a)$ at the denominator and it comes:
	
	The algorithm consists therefore in performing the following steps:
	\begin{enumerate}
		\item We fix $\varepsilon>0$ as upper bound of the admissible error tolerance.		
		
		\item Choose $a$ and $b$ with opposite signs, calculate $f(a)$ and $f(b)$
		
		\item We calculate $x_1=a-(b-a)k$. If $|f(x_1)|$ is small enough, we stop the calculation and we display $x_1$ and $f(x_1)$.

		\item Otherwise we proceed as following:
		\begin{itemize}
			\item We replace $b$ by $a$ and $f(b)$ by $f(a)$
			\item We replace $a$ by $x_1$ and $f(a)$ by $f(x_1)$
		\end{itemize} 
		and we go back to point (2).
	\end{enumerate}
	In pseudocode (non-unique and not optimized):\\\\
	\begin{algorithm}[H]
	 \KwData{$a$,$ b$, $\varepsilon$ expression of $f$ }
	 \KwResult{$x_1$}
	 initialization\;
	$f(a)$,$f(b)$\;
	 \While{$|f(x_1)|>\varepsilon$}{
	  $x_1=a-(b-a)\displaystyle\frac{f(a)}{f(b)}$\;
	  \If{$|f(x_1)|>\varepsilon$}{
	   $b:=a$\;
	   $f(b):=f(a)$\;
	   $a:=x_1$\;
	   $f(a):=f(x_1)$\;
	   }
	  Display $x_1$\;
	 }
	 \caption{Proportional Parts pseudocode algorithm}
	\end{algorithm}
	Obviously this is not the best algorithm especially if there is no root or if the computer is slow. Then a good advice is to take as input a number of limited iterations that have to be done (we will see a more elaborated way to handle such situations in the next algorithm).
	
	\pagebreak
	\subsubsection{Secant method (Regula Falsi or False Position)}\label{secant method}
	The "\NewTerm{secant method}\index{secant method}" also named or "\NewTerm{regula falsi}\index{regula falsi}" (for: regularly false) or also named "\NewTerm{false position method}\index{false position method}" is still a root search algorithm to zero. It's almost exactly the same as the proportional parts methods at the difference we don't do the assumption $|f(a)| \ll|f(b)|$.
	
	To introduce it, consider the following figure:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,728); %set diagram left start at 0, and has height of 728
		
		%Shape: Axis 2D [id:dp45935879271882674] 
		\draw  (138,253.6) -- (486.5,253.6)(178.5,31) -- (178.5,314.6) (479.5,248.6) -- (486.5,253.6) -- (479.5,258.6) (173.5,38) -- (178.5,31) -- (183.5,38)  ;
		%Curve Lines [id:da8941718355849049] 
		\draw [line width=1.5]    (215.75,87) .. controls (226.4,258.6) and (417.4,282.6) .. (455.75,298) ;
		%Straight Lines [id:da46187068706161605] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (217.4,106.6) -- (443.4,293.6) ;
		%Straight Lines [id:da8022477676990181] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (221.4,127.6) -- (221.4,256.6) ;
		%Straight Lines [id:da6998443445745315] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (217.4,106.6) -- (177.5,106.6) ;
		%Straight Lines [id:da10089348047484759] 
		\draw    (330.4,246.6) -- (330.4,260.6) ;
		%Straight Lines [id:da5509833912324322] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (395.4,254) -- (395.4,280.6) ;
		%Straight Lines [id:da9351543039433077] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (443.4,293.6) -- (178.4,293.6) ;
		%Straight Lines [id:da7854094951448642] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (443.4,254.6) -- (443.4,293.6) ;
		%Straight Lines [id:da765495785119559] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (217.4,106.6) -- (395.4,280.6) ;
		%Straight Lines [id:da48515029951036115] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (367.4,253) -- (367.4,271.6) ;
		
		% Text Node
		\draw (159,258) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
		% Text Node
		\draw (492,251.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (157,22.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (323,229.4) node [anchor=north west][inner sep=0.75pt]    {$\overline{x}$};
		% Text Node
		\draw (215,256.4) node [anchor=north west][inner sep=0.75pt]    {$a$};
		% Text Node
		\draw (142,98.4) node [anchor=north west][inner sep=0.75pt]    {$f( a)$};
		% Text Node
		\draw (200,57.4) node [anchor=north west][inner sep=0.75pt]    {$f( x)$};
		% Text Node
		\draw (397,233.4) node [anchor=north west][inner sep=0.75pt]    {$x_{1}$};
		% Text Node
		\draw (365,233.4) node [anchor=north west][inner sep=0.75pt]    {$x_{2}$};
		% Text Node
		\draw (224,95.4) node [anchor=north west][inner sep=0.75pt]    {$A$};
		% Text Node
		\draw (449,278.4) node [anchor=north west][inner sep=0.75pt]    {$B$};
		% Text Node
		\draw (438,233.4) node [anchor=north west][inner sep=0.75pt]    {$b$};
		% Text Node
		\draw (142,284.4) node [anchor=north west][inner sep=0.75pt]    {$f( b)$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Regula falsi method scheme} 
	\end{figure}
	The implementation, on a computer, of this method is particularly simple. The conditions to be satisfied being only that in the interval $[a,b]$:
	\begin{itemize}
		\item $f$ must be continuous on $[a,b]$ of class $\mathcal{C}^1$

		\item $f$ must be monotone near the root $x^{*}$
		
		\item $f(a)f(b)<0$ to be sure that there is a root
	\end{itemize}
	If $P_n$ is the point of coordinates $(x_n,0)$, then the points $(A,B,P_n)$ are aligned on the secant. The following proportion (application of Thales theorem) is then true:
	
	hence we deduce that:
	
	The algorithm consists therefore in performing the following steps:
	\begin{enumerate}
		\item We fix $\varepsilon>0$ as upper bound of the admissible error tolerance.
	
		\item Calculation of $x_n=\dfrac{af(b)-bf(a)}{f(b)-f(a)}$

		\item Evaluation of $f(x_n)$

		\item If $|f(x_n)|<\varepsilon$, the job is done. We have to display $x_n$

		\item Otherwise we proceed as following:
			\begin{enumerate}
				\item we replace $a$ by $x_n$ if $f(x_n)f(a)>0$.
	
				\item we replace $b$ by $x_n$ if $f(x_n)f(b)>0$ or $f(x_n)f(a)<0$.
				
				\item we go back in (2)
			\end{enumerate}
 	\end{enumerate}
 	Once again, the previous step (4) imposes the condition for stopping the calculations. Sometimes it is better to choose another criterion calculation ending. It requires the calculated solution to be contained in an interval of length equation containing the root $x^{*}$. This test is enunciate as follows:
	\begin{enumerate}
		\item[4'.] If $|b-a|<\varepsilon$, the job is finished and $x_n=\dfrac{af(b)-bf(a)}{f(b)-f(a)}$ is displayed. It is for sure obvious that $|x-\bar{x}|<\varepsilon/2$
	\end{enumerate}
 	In pseudocode (non-unique and not optimized):\\\\
	\begin{algorithm}[H]
	 \KwData{$a$,$ b$, $\varepsilon$ expression of $f$ }
	 \KwResult{$\bar{x}$}
	 initialization\;
	$x_n=\dfrac{af(b)-bf(a)}{f(b)-f(a)}$\;
	\While{$|f(x)|>\varepsilon$}{
	    \uIf{$f(x)f(a)>0$}{
     		$a:=x_n$\;
	 	}
		\uElseIf{$f(x_n)f(b)>0\; \vee \; f(x_n)f(a)<0$}{
			$b:=x$\;
		}
		$x_n=\dfrac{af(b)-bf(a)}{f(b)-f(a)}$\;
	 }
	 Display $x_n,f(x_n)$\;
	 \caption{Regula-Falsi pseudocode algorithm}
	\end{algorithm}
	Notice that the initial above relation can also be rewritten:
	
	If we change a little bit the notation we get:
	
	often also denoted:
	
	We then use this new value of $x$ in the prior previous relation as $x_2$ and repeat the process using $x_1$ and $x_2$ instead of $x_0$ and $x_1$. We continue this process, solving for $x_3$, $x_4$, etc., until we reach a sufficiently high level of precision (a sufficiently small difference between $x_n$ and $x_n - 1$):
	
	This again an application of the "\NewTerm{fixed-point iteration}\index{fixed-point iteration}" (related to the theorem of the same name proved in the section of Sequences and Series page \pageref{fixed point theorem}).
	
	\pagebreak
	\subsubsection{Bisection method (interval-halving)}\label{bisection method}
	The bisection method in mathematics is also a root-finding method that repeatedly bisects an interval and then selects a subinterval in which a root must lie for further processing. It is a very simple and robust method, but it is also relatively slow. Because of this, it is often used to obtain a rough approximation to a solution which is then used as a starting point for more rapidly converging methods. The method is also named the "\NewTerm{interval halving method}\index{interval halving method}", the "\NewTerm{binary search method}\index{binary search method}", or the "\NewTerm{dichotomy method}\index{dichotomy method}".
	
	The method is applicable for numerically solving the equation $f(x) = 0$ for the real variable $x$, where $f$ is a continuous function defined on an interval $[a, b]$ and where $f(a)$ and $f(b)$ have opposite signs such that $f(a)f(b)<0$. In this case $a$ and $b$ are also said to bracket a root since, by the intermediate value theorem, the continuous function f must have at least one root in the interval $[a, b]$.

	At each step the method divides the interval in two by computing the midpoint $c = (a+b) / 2$ of the interval and the value of the function $f(c)$ at that point. Unless $c$ is itself a root (which is very unlikely, but possible) there are now only two possibilities: either $f(a)$ and $f(c)$ have opposite signs and bracket a root, or $f(c)$ and $f(b)$ have opposite signs and bracket a root. The method selects the subinterval that is guaranteed to be a bracket as the new interval to be used in the next step. In this way an interval that contains a zero of $f$ is reduced in width by $50\%$ at each step. The process is continued until the interval is sufficiently small.

	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,592); %set diagram left start at 0, and has height of 592
		
		%Shape: Axis 2D [id:dp960263086314056] 
		\draw  (156,234.6) -- (489.3,234.6)(190.3,47) -- (190.3,335.6) (482.3,229.6) -- (489.3,234.6) -- (482.3,239.6) (185.3,54) -- (190.3,47) -- (195.3,54)  ;
		%Curve Lines [id:da9274721724160462] 
		\draw [line width=1.5]    (203.3,321.6) .. controls (237.3,206.6) and (327.3,108.6) .. (443.3,71.6) ;
		%Straight Lines [id:da3940127961171409] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (408,85) -- (174.3,85) ;
		%Straight Lines [id:da02341230464382016] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (305,146) -- (175.3,146) ;
		%Straight Lines [id:da8342796347566726] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (227.3,295) -- (179.3,295) ;
		%Straight Lines [id:da8698217950655682] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (408,313.6) -- (408,85) ;
		%Straight Lines [id:da5338493385907954] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (305,244.6) -- (305,146) ;
		%Straight Lines [id:da03946533952522602] 
		\draw    (240,227) -- (240,245.6) ;
		%Straight Lines [id:da7389975498251984] 
		\draw    (310,213.6) -- (404.3,213.6) ;
		\draw [shift={(406.3,213.6)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(308,213.6)}, rotate = 0] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da0833126302374716] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (211,312.6) -- (211,269.6) -- (211,225.6) ;
		%Straight Lines [id:da9477839105502965] 
		\draw    (214,302.6) -- (403.3,302.6) ;
		\draw [shift={(405.3,302.6)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(212,302.6)}, rotate = 0] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		
		% Text Node
		\draw (164,42.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (492,239.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (138,78.4) node [anchor=north west][inner sep=0.75pt]    {$f( b)$};
		% Text Node
		\draw (138,137.4) node [anchor=north west][inner sep=0.75pt]    {$f(\overline{x})$};
		% Text Node
		\draw (150,286.4) node [anchor=north west][inner sep=0.75pt]    {$f( a)$};
		% Text Node
		\draw (225,214.4) node [anchor=north west][inner sep=0.75pt]    {$x^{*}$};
		% Text Node
		\draw (446,58.4) node [anchor=north west][inner sep=0.75pt]    {$y=f( x)$};
		% Text Node
		\draw (337,170.4) node [anchor=north west][inner sep=0.75pt]    {$\dfrac{b-a}{2}$};
		% Text Node
		\draw (288,244.4) node [anchor=north west][inner sep=0.75pt]    {$\overline{x} =\dfrac{a+b}{2}$};
		% Text Node
		\draw (199.3,236) node [anchor=north west][inner sep=0.75pt]    {$a$};
		% Text Node
		\draw (294,304.4) node [anchor=north west][inner sep=0.75pt]    {$b-a$};
		% Text Node
		\draw (409.3,239) node [anchor=north west][inner sep=0.75pt]    {$b$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Bisection scheme}
	\end{figure}

	Explicitly, if $f(a)$ and $f(c)$ have opposite signs, then the method sets $c$ as the new value for $b$, and if $f(b)$ and $f(c)$ have opposite signs then the method sets $c$ as the new $a$. (If $f(c)=0$ then $c$ may be taken as the solution and the process stops.) In both cases, the new $f(a)$ and $f(b)$ have opposite signs, so the method is applicable to this smaller interval.
	
	The implementation, on a computer, of this method is particularly simple. The conditions to be satisfied being only that in the interval $[a,b]$:
	\begin{itemize}
		\item $f$ must be continuous on $[a,b]$ of class $\mathcal{C}^1$

		\item $f$ must be monotone near the root $\bar{x}$
		
		\item $f(a)f(b)<0$ to be sure that there is a root
	\end{itemize}
	
	The algorithm consists therefore in performing the following steps:
	\begin{enumerate}
		\item We fix $\varepsilon>0$ as upper bound of the admissible error tolerance.
		
		\item We calculate $x=(a+b)/2$

		\item We evaluate $f(x)$
		
		\item If $|f(x)|<\varepsilon$ then the job is done, we have to display $x$ and $f(x)$
	
		\item Otherwise we proceed as following
			\begin{enumerate}
				\item we replace $a$ by $x$ if $f(x)f(a)>0$.
	
				\item we replace $b$ by $x$ if $f(x)f(b)>0$ or $f(x)f(a)<0$.
				
				\item we go back in (2)
			\end{enumerate}
	\end{enumerate}
	The previous step (4) imposes the condition for stopping the calculations. Sometimes it is better to choose another criterion calculation ending. It requires the calculated solution to be contained in an interval of length equation containing the root $x^{*}$. This test would then be enunciate as follows:
	\begin{enumerate}
		\item[4'.] If $|b-a|<\varepsilon$, the job is finished and $x=(a+b)/2$ is displayed. It is for sure obvious that $|x-x^{*}|<\varepsilon/2$
	\end{enumerate}
	In pseudocode (non-unique and not optimized):\\\\
	\begin{algorithm}[H]
	 \KwData{$a$,$ b$, $\varepsilon$ expression of $f$ }
	 \KwResult{$x^{*}$}
	 initialization\;
	$x=(a+b)/2$\;
	\While{$|f(x)|>\varepsilon$}{
	    \uIf{$f(x)f(a)>0$}{
     		$a:=x$\;
	 	}
		\uElseIf{$f(x)f(b)>0\; \vee \; f(x)f(a)<0$}{
			$b:=x$\;
		}
		$x=(a+b)/2$\;
	 }
	 Display $x,f(x)$\;
	 \caption{Proportional Parts bisection pseudocode algorithm}
	\end{algorithm}
	The equivalent Maple 4.00b code is given by:
	
	\texttt{>zero:=proc(f,a,b,pre)
	local M;\\
	M:=f((a+b)/2);\\
	if abs(M)<pre then \\
	     RETURN((a+b)/2)\\
	elif M>0 then\\
	     zero(f,a,(a+b)/2,pre)\\
	else zero(f,(a+b)/2,b,pre)\\
	     fi\\
	end:}
	
	
	\pagebreak
	\subsubsection{Newton's method}\label{newton method}
	A few years after its discovery of the theory of gravitation, Newton developed a special technique to fast compute the solutions of any equation. This "supernaturally" fast convergence has been used to prove some of the most significant theoretical results of the 120th century (holocene calendar): the Kolmogorov stability theorem, the isometric embedding theorem of Nash ... Alone, this technique transcends the distinction between pure mathematics and applied mathematics!!!
	
	To study the "\NewTerm{Newton's method}\index{Newton's method}" (also named "\NewTerm{Newton-Raphson method}\index{Newton-Raphson method}\footnote{Newton's method was first published in 11685 and in 11690 (holocene calendar), Joseph Raphson published a simplified description. Raphson also applied the method only to polynomials, but he avoided Newton's tedious rewriting process by extracting each successive correction from the original polynomial. This allowed him to derive a reusable iterative expression for each problem. Finally, in 11740, (holocene calendar) Thomas Simpson described Newton's method as an iterative method for solving general non-linear equations using calculus. In the same publication, Simpson also gives the generalization to systems of two equations and notes that Newton's method can be used for solving optimization problems by setting the gradient to zero.}" or "\NewTerm{Newton approximation scheme"}\index{Newton approximation scheme}) in the plane (hence in the univariate case), consider the following figure:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,728); %set diagram left start at 0, and has height of 728
		
		%Shape: Axis 2D [id:dp45935879271882674] 
		\draw  (138,253.6) -- (486.5,253.6)(178.5,31) -- (178.5,314.6) (479.5,248.6) -- (486.5,253.6) -- (479.5,258.6) (173.5,38) -- (178.5,31) -- (183.5,38)  ;
		%Curve Lines [id:da8941718355849049] 
		\draw [line width=1.5]    (215.75,87) .. controls (226.4,258.6) and (417.4,282.6) .. (455.75,298) ;
		%Straight Lines [id:da46187068706161605] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (222.4,127.6) -- (257.4,251.6) ;
		%Straight Lines [id:da8022477676990181] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (221.4,127.6) -- (221.4,256.6) ;
		%Straight Lines [id:da6998443445745315] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (222.4,127.6) -- (175.5,127.6) ;
		%Straight Lines [id:da10089348047484759] 
		\draw    (330.4,246.6) -- (330.4,260.6) ;
		%Straight Lines [id:da7530176231321806] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (257.4,197) -- (178.5,197) ;
		%Straight Lines [id:da9910396066545233] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (257.4,197) -- (257.4,257.6) ;
		%Straight Lines [id:da5509833912324322] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (293.4,229) -- (293.4,257.6) ;
		%Straight Lines [id:da06137587476000239] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (293.4,229) -- (178.4,229) ;
		%Straight Lines [id:da9080724235541109] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (257.4,197) -- (292.4,252.6) ;
		
		% Text Node
		\draw (159,258) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
		% Text Node
		\draw (492,251.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (157,22.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (330,228.4) node [anchor=north west][inner sep=0.75pt]    {$\overline{x}$};
		% Text Node
		\draw (211,256.4) node [anchor=north west][inner sep=0.75pt]    {$x_{0}$};
		% Text Node
		\draw (136,118.4) node [anchor=north west][inner sep=0.75pt]    {$f( x_{0})$};
		% Text Node
		\draw (200,57.4) node [anchor=north west][inner sep=0.75pt]    {$f( x)$};
		% Text Node
		\draw (249,256.4) node [anchor=north west][inner sep=0.75pt]    {$x_{1}$};
		% Text Node
		\draw (287,259.4) node [anchor=north west][inner sep=0.75pt]    {$x_{2}$};
		% Text Node
		\draw (136,185.4) node [anchor=north west][inner sep=0.75pt]    {$f( x_{1})$};
		% Text Node
		\draw (136,222.4) node [anchor=north west][inner sep=0.75pt]    {$f( x_{2})$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Newton's method in the plane}
	\end{figure}
	If $x_0$ is an approximation of the root $\bar{x}$, we notice that $x_1$ is as better one. The point $x_1$ is the intersection of the tangent to the curve $(x_0,f(x_0))$ with the $x$-axis. The point $x_2$ is even a better approximate of $\bar{x}$, the point $x_2$ is obtained in the same manner as $x_1$ but from $(x_1,f(x_1))$.
	
	We may ask ourselves at this point what is the difference between Newton-Raphson and secant method on the basis of geometric interpretation? The two methods are almost the same, from a geometric perspective. The difference is that Newton's method uses a line that is tangent to one point, while the secant method uses a line that is secant to two points.
	
	To use this technique, remember that if we take a function $f$ that is differentiable on $x_0$, then we can rewrite it in the form (\SeeChapter{see section Sequences and Series page \pageref{taylor series}}):
	
	where $f'(x_0)$ is the derivative of $f$ in $x_0$ and $\mathcal{O}(x-x_0)$ is a function that tends to $0$ as $(x-x_0)^n$ when $n \geq 2$ when $x$ tends to $x_0$ (this is as we know a corrective term for the superior orders of the Taylor series).
	
	Applying this result to solve $f(x)=0$, we get:
	
	The function does not permit the resolution of this equation relatively to $\bar{x}$. By neglecting this term we get obviously:
	
	This is just the expression of the discrete derivative. Indeed:
	 
	The latter relation is easily resolved with respect to $\bar{x}$. To see this, let us begin by putting $\bar{x}=x_1$:
	
	But $x_1$ does not satisfy, in general, the equality $f(x_1)=0$. But as we have already pointed it out, $|f(x_1)|$  is smaller than $|f(x_0)|$ if the function $f$ satisfies some conditions.
	
	Newton's method consists in replacing the relation:
	
	by:
	
	and to iteratively solve this relation.
	
	The following conditions are sufficient to ensure the convergence of the method in an interval $[a, b]$ including $x_0$ and $\bar{x}$:
	\begin{enumerate}
		\item The function is twice differentiable (i.e. of class $\mathcal{C}^2$)

		\item The derivative $f'$ does not vanish (monotony)
		\item The second derivative $f''$ is continuous and does not vanish (no inflection point)
	\end{enumerate}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	It is often sufficient to check the conditions (1) and (2) for the process to converge.
	\end{tcolorbox}
	The condition (2) is obvious, indeed if $f'(x)=0$ the iteration can lead to a calculation overflow (singularity).
	
	The third condition (3) is less obvious, but the following figure shows a case of non-convergence. In this case, the process loops calculating alternately $x_i$ and $x_j$:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,728); %set diagram left start at 0, and has height of 728
		
		%Shape: Axis 2D [id:dp45935879271882674] 
		\draw  (138,253.6) -- (486.5,253.6)(178.5,31) -- (178.5,314.6) (479.5,248.6) -- (486.5,253.6) -- (479.5,258.6) (173.5,38) -- (178.5,31) -- (183.5,38)  ;
		%Curve Lines [id:da8941718355849049] 
		\draw [line width=1.5]    (217.5,84.6) .. controls (239.44,88.01) and (257.56,99.49) .. (273.65,115.46) .. controls (333.08,174.48) and (364.63,294.81) .. (457.5,295.6) ;
		%Straight Lines [id:da46187068706161605] 
		\draw    (225.5,70.6) -- (430.5,259.6) ;
		%Straight Lines [id:da8022477676990181] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (268,110) -- (268,179.6) -- (268,261.6) ;
		%Straight Lines [id:da07437727674347872] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (424,250) -- (424,291.6) ;
		%Straight Lines [id:da6998443445745315] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (268,110) -- (172.5,110) ;
		%Straight Lines [id:da197772665706194] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (424,291.6) -- (177.5,291.6) ;
		%Straight Lines [id:da10089348047484759] 
		\draw    (267.5,253.6) -- (456.75,298) ;
		
		% Text Node
		\draw (159,258) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
		% Text Node
		\draw (492,251.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (157,22.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (426,231.4) node [anchor=north west][inner sep=0.75pt]    {$x_{j}$};
		% Text Node
		\draw (250,256.4) node [anchor=north west][inner sep=0.75pt]    {$x_{i}$};
		% Text Node
		\draw (139,101.4) node [anchor=north west][inner sep=0.75pt]    {$f( x_{i})$};
		% Text Node
		\draw (139,281.4) node [anchor=north west][inner sep=0.75pt]    {$f( x_{j})$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Non-convergence example of Newton's method}
	\end{figure}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Consider the problem of finding the positive number $x$ with $\cos(x) = x^3$. We can rephrase that as finding the zero of $f(x) = \cos(x) - x^3$. We have $f'(x) = -\sin(x) - 3x^2$. Since $\cos(x) \leq 1$ for all $x$ and $x^3 > 1$ for $x > 1$, we know that our solution lies between $0$ and $1$. We try a starting value of $x_0 = 0.5$. (Note that a starting value of $0$ will lead to an undefined result, showing the importance of using a starting point that is close to the solution!).
	\begin{gather*}
		\begin{matrix}
  x_1 & = & x_0 - \dfrac{f(x_0)}{f'(x_0)} & = & 0.5 - \dfrac{\cos(0.5) - (0.5)^3}{-\sin(0.5) - 3(0.5)^2} & = & 1.112141637097 \\
  x_2 & = & x_1 - \dfrac{f(x_1)}{f'(x_1)} & = & \vdots & = & \underline{0.}909672693736 \\
  x_3 & = & \vdots & = & \vdots & = & \underline{0.86}7263818209 \\
  x_4 & = & \vdots & = & \vdots & = & \underline{0.86547}7135298 \\
  x_5 & = & \vdots & = & \vdots & = & \underline{0.8654740331}11 \\
  x_6 & = & \vdots &= & \vdots & = & \underline{0.865474033102}
	\end{matrix}	
	\end{gather*}
	The correct digits are underlined in the above example. In particular, $x_6$ is correct to the number of decimal places given. We see that the number of correct digits after the decimal point increases from $2$ (for $x_3$) to $5$ and $10$, illustrating the quadratic convergence.
	\end{tcolorbox}
	Here is the corresponding pseudo-code:
	
	\begin{algorithm}[H]
	 \SetAlgoLined
	 \LinesNumbered
	 \SetKwInOut{Input}{Input}
	 \Input{Initial guess $x_0$, $f$}
	 \Input{number of steps $x_{\max}$}
	 \SetKwProg{Function}{function}{}{end}
	 \SetKwRepeat{Do}{do}{while}
	 $n \leftarrow 1$\;
	 \Do{$n\leq x_{\max}$}{ 
	 	$x_{n+1}=x_n-\dfrac{f(x_n)}{f'(x_n)}$\;
	 	$n=n+1$\;
	  }
	 \caption{Newton's Method}
	\end{algorithm}
	
	If the function $f$ is given analytically, its derivative can be determined analytically. But in many cases it is advisable or even necessary to replace $f'(x_n)$ by the differential quotient:
	
	where $h$ should be chosen as small enough so that the difference:
	
	is also small enough.

	The iteration is then written:
	
	If the resolution method is converging, the gap between $x_{n+1}$ and $\bar{x}$ decreases at each iteration. This is ensured, for example, if the interval $[a, b]$ containing $x_{n+1}$, sees its length decreasing at each step. 

	\begin{theorem}
	Newton's method is interesting because the convergence is quadratic:
	
	while the convergence of other methods is linear such that:
	
	Let us consider, for example, the method of bisection seen previously. At each iteration the length of the interval $[a, b]$ is halved. This ensures us that the gap $|x_{n+1}-\bar{x}|$ is halved at each step of the calculation:
	
	\end{theorem}
	\begin{dem}
	To prove the quadratic convergence of the Newton's method, we have to make use of the limited Taylor series of $f$ and $f'$ in the neighbourhood of $\bar{x}$:
	
	But:
	
	therefore:
	
	Subtracting $\bar{x}$ left and right of the equality and putting the two terms of the second member to the same denominator, we get:
	
	and when $x_n-\bar{x}$ is small enough, the denominator can be simplified.
	
	which shows that convergence is quadratic.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Here is an application with Maple 4.00b of this method:\\
	
	\texttt{
	>with(plots): with(plottools):\\
	>f:=x->exp(x)*x\string^2-36;\\
	>D(f)(x);\\
	>x[0]:=3;\\
	>n:=7;\\
	>g:=x->f(x[i-1])+D(f)(x[i-1])*(x-x[i-1]);\\
	>for i from 1 by 1 to n do;\\
	>x[i]:=evalf(solve(g(x)=0,x));\\
	>od;\\
	>lines:={}:\\
	>for i from 1 by 1 to n do;\\
	>lines:=lines union \{line([x[i-1],0],[x[i-1],f(x[i-1])],color=green), line([x[i-1],f(x[i-1])],[x[i],0],color=green)\};\\
	>od:\\
	>display({plot(f(x),x=2..3.01)} union lines);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/newton_method_with_maple4.jpg}
		\caption{Maple 4.00b application of Newton's method}
	\end{figure}
	\end{tcolorbox}
	Now let us prove that Newton's method is reduced to the Babylonian method seen earlier above at page \pageref{Heron square root algorithm}! Indeed. we want to solve $\sqrt{A}$, i.e. looking for a root of $f(x)=x^2-A$. Then:
	
	That's it!
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Let us finally communicate that we will study the Newton's method with several variables during our study of non-linear optimization. It is an pedagogical choice that seemed to us the best subjectively the best one.
	\end{tcolorbox}	
	
	\pagebreak
	\subsection{Numerical Differentiation}
	In numerical analysis, numerical differentiation describes algorithms for estimating the derivative of a mathematical function or function subroutine using values of the function and perhaps other knowledge about the function.
	
	Many modelling techniques or numerical resolution technique that we will see further use derivatives as for example the search of optimums (see further below), the finite element methods (see also further below). For example, to name the most famous case, the solver of Microsoft Excel 2007 and earlier offers some of the most elementary numerical derivatives that we will study here and reuse further:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/excel_solver_derivatives.jpg}
		\caption{Screenshot of Microsoft Excel 2003 Solver}
	\end{figure}
	To allow a computer processing, the various derivatives  present in many algorithms must be approximated numerically. To do this, we use in the most basic case the principle of centered finite difference which is based on the following Taylor series expansion (\SeeChapter{see section Sequences and Series page \pageref{taylor series}}):
	
	On the base of this principle we get then the following development of the second order:
	
	It comes then when neglecting the higher order terms and subtracting and simplifying the two series above:
	
	Relation that we name "\NewTerm{first centered derivative with tangent estimate}\index{first centered derivative with tangent estimate}" (because we neglect all non-linear terms) or also "\NewTerm{symmetric difference quotient}\index{symmetric difference quotient}". We also find often this latter relation in the following equivalent form:
	
	Now let us see what we name the "\NewTerm{right first derivative}\index{right first derivative}" also named "\NewTerm{forward derivatives}\index{forward derivatives}" (or "forward difference"), which consist simply in the application of the following intuitive  algorithm:
	
	and incidentally we can also define the "\NewTerm{left first derivative}\index{left first derivative}" also named "\NewTerm{backward derivative}\index{backward derivative}" (or "backward difference"):
	
	We see therefore the central derivatives require more calculations but are also more accurate. This below figure gives a quite summary of the previous relations with a special case:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
		
		%Shape: Axis 2D [id:dp7477355718914274] 
		\draw  (57.5,301.2) -- (510.5,301.2)(103.5,68.2) -- (103.5,338.2) (503.5,296.2) -- (510.5,301.2) -- (503.5,306.2) (98.5,75.2) -- (103.5,68.2) -- (108.5,75.2) (146.5,296.2) -- (146.5,306.2)(189.5,296.2) -- (189.5,306.2)(232.5,296.2) -- (232.5,306.2)(275.5,296.2) -- (275.5,306.2)(318.5,296.2) -- (318.5,306.2)(361.5,296.2) -- (361.5,306.2)(404.5,296.2) -- (404.5,306.2)(447.5,296.2) -- (447.5,306.2)(490.5,296.2) -- (490.5,306.2)(98.5,258.2) -- (108.5,258.2)(98.5,215.2) -- (108.5,215.2)(98.5,172.2) -- (108.5,172.2)(98.5,129.2) -- (108.5,129.2)(98.5,86.2) -- (108.5,86.2) ;
		\draw   ;
		%Curve Lines [id:da866622119538206] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (72,333) .. controls (123.5,307.2) and (324.5,-22.4) .. (528.5,116.6) ;
		%Straight Lines [id:da168928218633547] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ]   (173.5,229) -- (279.5,136) ;
		\draw [shift={(279.5,136)}, rotate = 318.74] [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ][fill={rgb, 255:red, 245; green, 166; blue, 35 }  ,fill opacity=1 ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		%Straight Lines [id:da22857042799403549] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ]   (381.5,87.6) -- (279.5,136) ;
		\draw [shift={(279.5,136)}, rotate = 154.62] [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ][fill={rgb, 255:red, 245; green, 166; blue, 35 }  ,fill opacity=1 ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		%Straight Lines [id:da05654762390984591] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (173.5,229) -- (381.5,87.6) ;
		\draw [shift={(381.5,87.6)}, rotate = 325.79] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ][line width=1.5]      (0, 0) circle [x radius= 4.36, y radius= 4.36]   ;
		\draw [shift={(173.5,229)}, rotate = 325.79] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ][line width=1.5]      (0, 0) circle [x radius= 4.36, y radius= 4.36]   ;
		%Straight Lines [id:da02688764760453788] 
		\draw    (190.5,139.55) -- (217.75,139.55) -- (226.1,180.54) ;
		\draw [shift={(226.5,182.5)}, rotate = 258.48] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da14409117112763825] 
		\draw    (326.5,190.6) -- (310.5,190.6) -- (289.56,157.29) ;
		\draw [shift={(288.5,155.6)}, rotate = 57.85] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da042368735575313066] 
		\draw    (353.5,46.2) -- (333.5,46.2) -- (319.89,115.54) ;
		\draw [shift={(319.5,117.5)}, rotate = 281.11] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		
		% Text Node
		\draw (85,62.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (498,308.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (105.5,304.2) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
		% Text Node
		\draw (172,237.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$( 1-h,f( 1-h))$};
		% Text Node
		\draw (235,114.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$( 1,f( 1))$};
		% Text Node
		\draw (270,310.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (438,310.4) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (376,96.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$( 1+h,f( 1+h))$};
		% Text Node
		\draw (490,123.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$y=f( x)$};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (51,98) -- (167,90) -- (167,118) -- (51,118) -- cycle  ;
		\draw (54,88) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {Backward difference\\quotient};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (51,122) -- (188,122) -- (188,160) -- (51,160) -- cycle  ;
		\draw (54,126) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {slope$\displaystyle =\dfrac{f( 1-h) -f( 1)}{-h}$};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (330,159) -- (450,159) -- (450,187) -- (330,187) -- cycle  ;
		\draw (333,160) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {Symmetric difference\\quotient};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (330,191) -- (488,191) -- (488,229) -- (330,229) -- cycle  ;
		\draw (333,195) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {slope$\displaystyle =\dfrac{f( 1+h) -f( 1-h)}{2h}$};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (355,4) -- (462,4) -- (462,32) -- (355,32) -- cycle  ;
		\draw (358,7) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {Forward difference\\quotient};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (355,36) -- (493,36) -- (493,74) -- (355,74) -- cycle  ;
		\draw (358,40) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {slope$\displaystyle =\dfrac{f( 1+h) -f( 1)}{h}$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Forward/Backward derivatives illustration}
	\end{figure}
	We can also develop more elaborate relations through Taylor expansions with superior orders, do averages between different methods and so on... it's quite endless...
	
	\pagebreak
	\subsection{Numerical Integration}\label{numerical integration}
	In numerical analysis, "\NewTerm{numerical integration}\index{numerical integration}" constitutes a broad family of algorithms for calculating the numerical value of a definite integral, and by extension, the term is also sometimes used to describe the numerical solution of differential equations. This subsection focuses on calculation of definite integrals. 
	
	The basic problem in numerical integration is to compute an approximate solution to a definite integral overt the interval $[a,b]$ by evaluating $f(x)$ at a finite number of sample points:
	
	to a given degree of accuracy (with an error $\varepsilon(f)$. If $f(x)$ is a smooth function integrated over a small number of dimensions, and the domain of integration is bounded, there are many methods for approximating the integral to the desired precision.
	
	\textbf{Definition (\#\thesection.\mydef):} Suppose that $a=x_0<x_1<\ldots<x_n=b$. A relation of the form:
	
	with the property that:
	
	is named a "\NewTerm{numerical integration}\index{numerical integration}" or "\NewTerm{quadrature formula}\index{quadrature formula}\footnote{In mathematics, quadrature is a historical term which means the process of determining area. This term is still used nowadays in the context of differential equations, where "solving an equation by quadrature" means expressing its solution in terms of integrals.}".  The term $\varepsilon(f)$  is named the "\NewTerm{truncation error for integration}".  The values $\{x_i\}_{i=0}^n$ are named the quadrature nodes and $\{w_i\}_{i=0}^n$ are named the "\NewTerm{weights}".

 	Depending on the application, the nodes  $\{x_i\}_{i=0}^n$  are chosen in various ways.  For the Trapezoidal Rule, Simpson's Rule, and Boole's Rule, the nodes are chosen to be equally spaced.  For Gauss-Legendre quadrature, the nodes are chosen to be the zeros of certain Legendre polynomials.  When the integration formula is used to develop a predictor formula for differential equations, all the nodes are chosen less than $b$.  For all applications, it is necessary to know something about the accuracy of the numerical solution. 
	
	Let us consider the following figure:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,638); %set diagram left start at 0, and has height of 638
		
		%Shape: Axis 2D [id:dp6699430883615789] 
		\draw  (109,263.5) -- (511.3,263.5)(149.23,25) -- (149.23,290) (504.3,258.5) -- (511.3,263.5) -- (504.3,268.5) (144.23,32) -- (149.23,25) -- (154.23,32)  ;
		%Curve Lines [id:da2506748694065859] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (182,204) .. controls (194.3,178) and (211.3,231.8) .. (233.3,204) .. controls (255.3,176.2) and (242.3,193.2) .. (267.3,159.2) .. controls (292.3,125.2) and (322.3,118.2) .. (346.3,86.2) .. controls (370.3,54.2) and (434.3,216.2) .. (470.3,113) ;
		%Straight Lines [id:da2849352007001158] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (238,199) -- (238,277.2) ;
		%Straight Lines [id:da7886939248819052] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (404,127) -- (404,278.2) ;
		
		% Text Node
		\draw (126,31.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (517,258.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (235.17,278.9) node [anchor=north west][inner sep=0.75pt]    {$a$};
		% Text Node
		\draw (398.13,278.9) node [anchor=north west][inner sep=0.75pt]    {$b$};
		% Text Node
		\draw (451,90.4) node [anchor=north west][inner sep=0.75pt]    {$f( x)$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Illustration of an interval under a curve}
	\end{figure}
	We would like to calculate the area between the $x$ axis, the curve $f$ and the straight vertical lines of equations $x=a$ and $x=b$. We assume in this case that the function $f$ is with positive values:
	
	This problem, in its generality, is difficult or impossible to solve analytically in the most general cases. Below we will see some mainstream numerical methods for the approximate calculation of this area in increasing complexity order (sometimes these methods are used in corporations by employees who have only spreadsheets softwares like Microsoft Excel or OpenOffice Calc to calculate integrals...):
	\begin{itemize} 
		\item Newton-Cotes formulas: In this case, we obtain methods for numerical integration which can be derived from the Lagrange interpolating method. Alternatively the formulas can also be derived from Taylor expansion. The idea is similar to the way we obtain numerical differentiation schemes. We can easily derive not just integration formulas but also their errors using this technique. The schemes which we develop here will be based on the assumption of equidistant points.
	
		\item Composite, Newton - Cotes formulas (open and closed): These methods are composite since they repeatedly apply the simple formulas derived previously to cover longer intervals. This idea allows for piecewise estimates of the integral thus improving the error of our integration (we will also assume equidistant nodes in our introduction of these methods).
	
		\item Romberg Integration: This method allows us to improve the error of our integration methods by doing minimal extra work. The idea is based on the Richardson extrapolation (actually we don't present this method here).
	
		\item: Adaptive Integration: Here we are free to choose the points over which we calculate the numerical integral of $f(x)$ so as to minimize our error. Adaptive integration does not therefore require equidistant nodes. Thus if the function is not very smooth at some interval the step size $h$ of the numerical integration method decreases to make sure we do not accumulate too much error in our calculation (actually we don't present this method here). 
	
		\item Gaussian Integration: We explore methods which can achieve optimal error reduction provided we place the nodes at specific locations. Computing the best weights for our numerical quadratures guarantees optimal approximation of our integral (actually we don't present this method here).
	
		\item Monte Carlo Integration: Use random number generation and a ratio of the target random points relatively to all random generated points.  At this method employs a non-deterministic approach: each realization provides a different outcome! There are different methods to perform a Monte Carlo integration, such as uniform sampling, stratified sampling, importance sampling, sequential Monte Carlo (also known as a particle filter), and mean field particle methods (see page \pageref{monte carlo simulations}).
	\end{itemize}
	
	\subsubsection{Rectangles method}\label{rectangle integration method}
	The "\NewTerm{rectangle methods}\index{rectangle methods}" computes an approximation to a definite integral, made by finding the area of a collection of rectangles whose heights are determined by the values of the function by different approach.
	
	We divide the interval $[a,b]$ into $n$ subintervals which bounds are the $x_i$. The lengths of these subintervals are $h_i=x_{i+1}-x_i$. We build rectangles which sides are $h_i$ and in the case of the "\index{left rectangle methods}" the height is $f(x_i)$.
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
		
		%Shape: Axis 2D [id:dp13945459493840429] 
		\draw  (115,263.6) -- (396.5,263.6)(134.93,39) -- (134.93,287.6) (389.5,258.6) -- (396.5,263.6) -- (389.5,268.6) (129.93,46) -- (134.93,39) -- (139.93,46) (164.93,258.6) -- (164.93,268.6)(194.93,258.6) -- (194.93,268.6)(224.93,258.6) -- (224.93,268.6)(254.93,258.6) -- (254.93,268.6)(284.93,258.6) -- (284.93,268.6)(314.93,258.6) -- (314.93,268.6)(344.93,258.6) -- (344.93,268.6)(374.93,258.6) -- (374.93,268.6)(129.93,233.6) -- (139.93,233.6)(129.93,203.6) -- (139.93,203.6)(129.93,173.6) -- (139.93,173.6)(129.93,143.6) -- (139.93,143.6)(129.93,113.6) -- (139.93,113.6)(129.93,83.6) -- (139.93,83.6) ;
		\draw   ;
		%Shape: Rectangle [id:dp6882645628604773] 
		\draw  [fill={rgb, 255:red, 122; green, 181; blue, 245 }  ,fill opacity=1 ] (164.5,203.6) -- (195.5,203.6) -- (195.5,263.6) -- (164.5,263.6) -- cycle ;
		%Shape: Rectangle [id:dp22030926390027128] 
		\draw  [fill={rgb, 255:red, 122; green, 181; blue, 245 }  ,fill opacity=1 ] (195.5,164.6) -- (224.5,164.6) -- (224.5,263.6) -- (195.5,263.6) -- cycle ;
		%Shape: Rectangle [id:dp08290387296205082] 
		\draw  [fill={rgb, 255:red, 122; green, 181; blue, 245 }  ,fill opacity=1 ] (224.5,143.6) -- (255.5,143.6) -- (255.5,263.6) -- (224.5,263.6) -- cycle ;
		%Shape: Rectangle [id:dp6356956702604009] 
		\draw  [fill={rgb, 255:red, 122; green, 181; blue, 245 }  ,fill opacity=1 ] (255.5,128.6) -- (284.5,128.6) -- (284.5,263.6) -- (255.5,263.6) -- cycle ;
		%Shape: Rectangle [id:dp5933291746944445] 
		\draw  [fill={rgb, 255:red, 122; green, 181; blue, 245 }  ,fill opacity=1 ] (284.5,116.6) -- (315.5,116.6) -- (315.5,263.6) -- (284.5,263.6) -- cycle ;
		%Shape: Rectangle [id:dp9124919741926767] 
		\draw  [fill={rgb, 255:red, 122; green, 181; blue, 245 }  ,fill opacity=1 ] (315.5,107.6) -- (344.5,107.6) -- (344.5,263.6) -- (315.5,263.6) -- cycle ;
		%Shape: Rectangle [id:dp4903880339015563] 
		\draw  [fill={rgb, 255:red, 122; green, 181; blue, 245 }  ,fill opacity=1 ] (344.5,102.6) -- (375.5,102.6) -- (375.5,263.6) -- (344.5,263.6) -- cycle ;
		%Curve Lines [id:da07604670988929141] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (150.5,248.6) .. controls (172.5,113.6) and (344.5,109.6) .. (390.5,87.6) ;
		%Straight Lines [id:da816468574627272] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (119.5,128.6) -- (255.5,128.6) ;
		%Straight Lines [id:da04912405133766273] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (120.5,203.6) -- (164.5,203.6) ;
		
		% Text Node
		\draw (115,268) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
		% Text Node
		\draw (115,37.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (398,244.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (79,193.4) node [anchor=north west][inner sep=0.75pt]    {$f( x_{0})$};
		% Text Node
		\draw (75,115.4) node [anchor=north west][inner sep=0.75pt]    {$f( x_{i})$};
		% Text Node
		\draw (136.93,272) node [anchor=north west][inner sep=0.75pt]    {$x_{0} =a$};
		% Text Node
		\draw (193.5,272) node [anchor=north west][inner sep=0.75pt]    {$x_{1}$};
		% Text Node
		\draw (250.5,272) node [anchor=north west][inner sep=0.75pt]    {$x_{i}$};
		% Text Node
		\draw (275.5,272) node [anchor=north west][inner sep=0.75pt]    {$x_{i+1}$};
		% Text Node
		\draw (352.5,268) node [anchor=north west][inner sep=0.75pt]    {$x_{n} =b$};
		% Text Node
		\draw (345,66.4) node [anchor=north west][inner sep=0.75pt]    {$\textcolor[rgb]{0.82,0.01,0.11}{y=f(x)}$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Approach of the area under a curve by lower left rectangles method}
	\end{figure}
	The area of these rectangles is:
	
	If the $h_i$ are small enough, $A_G$ is a good approximation of the sought approached area by the left method.

	We can start this exercise again by choosing $h_i$ and $f(x_{i+1})$ as sides of the rectangles (so the approach is named the "right rectangle method\index{right rectangle methods}"). We then get:
	
	The correspondent figure is therefore the following:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
		
		%Shape: Axis 2D [id:dp13945459493840429] 
		\draw  (115,263.6) -- (396.5,263.6)(134.93,39) -- (134.93,287.6) (389.5,258.6) -- (396.5,263.6) -- (389.5,268.6) (129.93,46) -- (134.93,39) -- (139.93,46) (164.93,258.6) -- (164.93,268.6)(194.93,258.6) -- (194.93,268.6)(224.93,258.6) -- (224.93,268.6)(254.93,258.6) -- (254.93,268.6)(284.93,258.6) -- (284.93,268.6)(314.93,258.6) -- (314.93,268.6)(344.93,258.6) -- (344.93,268.6)(374.93,258.6) -- (374.93,268.6)(129.93,233.6) -- (139.93,233.6)(129.93,203.6) -- (139.93,203.6)(129.93,173.6) -- (139.93,173.6)(129.93,143.6) -- (139.93,143.6)(129.93,113.6) -- (139.93,113.6)(129.93,83.6) -- (139.93,83.6) ;
		\draw   ;
		%Shape: Rectangle [id:dp6882645628604773] 
		\draw  [fill={rgb, 255:red, 122; green, 181; blue, 245 }  ,fill opacity=1 ] (164.5,164.6) -- (195.5,164.6) -- (195.5,263.6) -- (164.5,263.6) -- cycle ;
		%Shape: Rectangle [id:dp22030926390027128] 
		\draw  [fill={rgb, 255:red, 122; green, 181; blue, 245 }  ,fill opacity=1 ] (195.5,144.6) -- (224.5,144.6) -- (224.5,263.6) -- (195.5,263.6) -- cycle ;
		%Shape: Rectangle [id:dp08290387296205082] 
		\draw  [fill={rgb, 255:red, 122; green, 181; blue, 245 }  ,fill opacity=1 ] (224.5,129.6) -- (255.5,129.6) -- (255.5,263.6) -- (224.5,263.6) -- cycle ;
		%Shape: Rectangle [id:dp6356956702604009] 
		\draw  [fill={rgb, 255:red, 122; green, 181; blue, 245 }  ,fill opacity=1 ] (255.5,117.6) -- (284.5,117.6) -- (284.5,263.6) -- (255.5,263.6) -- cycle ;
		%Shape: Rectangle [id:dp5933291746944445] 
		\draw  [fill={rgb, 255:red, 122; green, 181; blue, 245 }  ,fill opacity=1 ] (284.5,108.6) -- (315.5,108.6) -- (315.5,263.6) -- (284.5,263.6) -- cycle ;
		%Shape: Rectangle [id:dp9124919741926767] 
		\draw  [fill={rgb, 255:red, 122; green, 181; blue, 245 }  ,fill opacity=1 ] (315.5,102.6) -- (344.5,102.6) -- (344.5,263.6) -- (315.5,263.6) -- cycle ;
		%Shape: Rectangle [id:dp4903880339015563] 
		\draw  [fill={rgb, 255:red, 122; green, 181; blue, 245 }  ,fill opacity=1 ] (344.5,92.6) -- (375.5,92.6) -- (375.5,263.6) -- (344.5,263.6) -- cycle ;
		%Curve Lines [id:da07604670988929141] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (150.5,248.6) .. controls (172.5,113.6) and (344.5,109.6) .. (390.5,87.6) ;
		%Straight Lines [id:da816468574627272] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (119.5,117.6) -- (255.5,117.6) ;
		%Straight Lines [id:da04912405133766273] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (120.5,205.6) -- (164.5,205.6) ;
		
		% Text Node
		\draw (115,268) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
		% Text Node
		\draw (115,37.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (398,244.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (66,195.4) node [anchor=north west][inner sep=0.75pt]    {$f( x_{0})$};
		% Text Node
		\draw (66,104.4) node [anchor=north west][inner sep=0.75pt]    {$f( x_{i+1})$};
		% Text Node
		\draw (136.93,272) node [anchor=north west][inner sep=0.75pt]    {$x_{0} =a$};
		% Text Node
		\draw (193.5,272) node [anchor=north west][inner sep=0.75pt]    {$x_{1}$};
		% Text Node
		\draw (248,272) node [anchor=north west][inner sep=0.75pt]    {$x_{i}$};
		% Text Node
		\draw (277.5,272) node [anchor=north west][inner sep=0.75pt]    {$x_{i+1}$};
		% Text Node
		\draw (352.5,268) node [anchor=north west][inner sep=0.75pt]    {$x_{n} =b$};
		% Text Node
		\draw (345,66.4) node [anchor=north west][inner sep=0.75pt]    {$\textcolor[rgb]{0.82,0.01,0.11}{y=f(x)}$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Approach of the area under a curve by upper right rectangles method}
	\end{figure}
	Again, the area of these rectangles approaches the area searched. To simplify computer code, it is useful to choose identical length intervals:
	
	If we have $n$ rectangles, $h$ is then equal to $(b-a)/n$. The areas $A_D$ and $A_G$ become:
	
	We can also mixed the both method above as we will illustrate it further below in the figure that will summarize the for more common methods.
		
	
	\subsubsection{Trapezoidal method}\label{trapezoidal numerical integration}
	In the purpose to increase the accuracy, it is possible to calculate:
	
	In the case where all the intervals are of equal length, $A_T$ is equal to:
	
	That we often find in the academic literature, in the form:
	
	There are many other methods for solving this type of problem (including the Monte Carlo method that we will see further below).
	
	In the case where the function $f$ is not made of only positive values, we no longer speak anymore about "area" but of "\NewTerm{Riemann sum}\index{Riemann sum}". The sum to calculate are then:
	
	and:
	
	Hence to summarize we have the following four following more typical univariate integral numerical methods:
	\begin{figure}[H]
		\centering	
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1,scale=1.3]
		%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
		
		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_nrimi6ci6}{
		\pgfdeclarepatternformonly[\mcThickness,\mcSize]{_nrimi6ci6}
		{\pgfqpoint{0pt}{-\mcThickness}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathmoveto{\pgfqpoint{0pt}{\mcSize}}
		\pgfpathlineto{\pgfpoint{\mcSize+\mcThickness}{-\mcThickness}}
		\pgfusepath{stroke}
		}}
		\makeatother
		
		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_xgoo1ecg9}{
		\pgfdeclarepatternformonly[\mcThickness,\mcSize]{_xgoo1ecg9}
		{\pgfqpoint{0pt}{-\mcThickness}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathmoveto{\pgfqpoint{0pt}{\mcSize}}
		\pgfpathlineto{\pgfpoint{\mcSize+\mcThickness}{-\mcThickness}}
		\pgfusepath{stroke}
		}}
		\makeatother
		
		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_vq5amy2ve}{
		\pgfdeclarepatternformonly[\mcThickness,\mcSize]{_vq5amy2ve}
		{\pgfqpoint{0pt}{-\mcThickness}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathmoveto{\pgfqpoint{0pt}{\mcSize}}
		\pgfpathlineto{\pgfpoint{\mcSize+\mcThickness}{-\mcThickness}}
		\pgfusepath{stroke}
		}}
		\makeatother
		
		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_89mqwdxzz}{
		\pgfdeclarepatternformonly[\mcThickness,\mcSize]{_89mqwdxzz}
		{\pgfqpoint{0pt}{-\mcThickness}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathmoveto{\pgfqpoint{0pt}{\mcSize}}
		\pgfpathlineto{\pgfpoint{\mcSize+\mcThickness}{-\mcThickness}}
		\pgfusepath{stroke}
		}}
		
		%Shape: Rectangle [id:dp5489813974917861] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][pattern=_nrimi6ci6,pattern size=6pt,pattern thickness=0.75pt,pattern radius=0pt, pattern color={rgb, 255:red, 155; green, 155; blue, 155}] (155,246.6) -- (234.5,246.6) -- (234.5,336.6) -- (155,336.6) -- cycle ;
		%Shape: Rectangle [id:dp638848876222071] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][pattern=_xgoo1ecg9,pattern size=6pt,pattern thickness=0.75pt,pattern radius=0pt, pattern color={rgb, 255:red, 155; green, 155; blue, 155}] (392,71.6) -- (471.5,71.6) -- (471.5,179.6) -- (392,179.6) -- cycle ;
		%Straight Lines [id:da7411577325562555] 
		\draw    (142,180) -- (273.5,180) ;
		\draw [shift={(275.5,180)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da8754753106241235] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (156,65) -- (156,185.6) ;
		%Straight Lines [id:da6121254170976436] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (235,65) -- (235,185.6) ;
		%Straight Lines [id:da5776168183704551] 
		\draw    (378,180) -- (509.5,180) ;
		\draw [shift={(511.5,180)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da8891608258968502] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (392,65) -- (392,185.6) ;
		%Straight Lines [id:da044584093192255425] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (471,65) -- (471,185.6) ;
		%Curve Lines [id:da40037257164558104] 
		\draw [line width=1.5]    (367.5,155.6) .. controls (422.5,84.6) and (452.5,72.6) .. (495.5,65.6) ;
		%Straight Lines [id:da2910709441237156] 
		\draw    (141,337) -- (272.5,337) ;
		\draw [shift={(274.5,337)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da3436336163795777] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (155,222) -- (155,342.6) ;
		%Straight Lines [id:da8162253860206494] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (234,222) -- (234,342.6) ;
		%Curve Lines [id:da4632826976404134] 
		\draw [line width=1.5]    (130.5,312.6) .. controls (185.5,241.6) and (215.5,229.6) .. (258.5,222.6) ;
		%Straight Lines [id:da6752619044340724] 
		\draw    (377,337) -- (508.5,337) ;
		\draw [shift={(510.5,337)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da5080835831059585] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (391,222) -- (391,342.6) ;
		%Straight Lines [id:da7872888926793162] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (470,222) -- (470,342.6) ;
		%Shape: Rectangle [id:dp2563653526397929] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][pattern=_vq5amy2ve,pattern size=6pt,pattern thickness=0.75pt,pattern radius=0pt, pattern color={rgb, 255:red, 155; green, 155; blue, 155}] (156,125.3) -- (235.5,125.3) -- (235.5,179.6) -- (156,179.6) -- cycle ;
		%Curve Lines [id:da5679337278158443] 
		\draw [line width=1.5]    (131.5,155.6) .. controls (186.5,84.6) and (216.5,72.6) .. (259.5,65.6) ;
		%Straight Lines [id:da6052365376870315] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (194,247) -- (194,335.6) ;
		%Shape: Polygon [id:ds7373438300210473] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][pattern=_89mqwdxzz,pattern size=6pt,pattern thickness=0.75pt,pattern radius=0pt, pattern color={rgb, 255:red, 155; green, 155; blue, 155}] (469.5,228.6) -- (470.5,336.6) -- (391.5,336.6) -- (391,282.3) -- cycle ;
		%Curve Lines [id:da7953222393991779] 
		\draw [line width=1.5]    (366.5,312.6) .. controls (421.5,241.6) and (451.5,229.6) .. (494.5,222.6) ;
		
		% Text Node
		\draw (277,108) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{A}};
		% Text Node
		\draw (150,186.9) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$a$};
		% Text Node
		\draw (217,183.9) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$a+h$};
		% Text Node
		\draw (513,108) node [anchor=north west][inner sep=0.75pt]   [align=left] {B};
		% Text Node
		\draw (386,186.9) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$a$};
		% Text Node
		\draw (453,183.9) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$a+h$};
		% Text Node
		\draw (276,265) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{C}};
		% Text Node
		\draw (149,343.9) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$a$};
		% Text Node
		\draw (216,340.9) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$a+h$};
		% Text Node
		\draw (512,265) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{D}};
		% Text Node
		\draw (385,343.9) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$a$};
		% Text Node
		\draw (452,340.9) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$a+h$};
		% Text Node
		\draw (125,113.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$f( a)$};
		% Text Node
		\draw (339,65.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$f(a+h)$};
		% Text Node
		\draw (88,241.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$f(a+h/2)$};
		% Text Node
		\draw (167,340.9) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$a+h/2$};
		% Text Node
		\draw (363,268.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$f( a)$};
		% Text Node
		\draw (344,222.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$f( a+h)$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[]{\textbf{A}: left rectangle method, \textbf{B}: right rectangle point, \textbf{C}: mid-point method, \textbf{D}: Trapezoidal method}
	\end{figure}
	
	\subsubsection{Newton–Cotes formulas}
	The "\NewTerm{Newton–Cotes formulas}\index{Newton–Cotes formulas}", also named the "\NewTerm{Newton–Cotes quadrature rules}" or simply "\NewTerm{Newton–Cotes rules}", are a group of formulas for numerical integration (also named "\NewTerm{quadrature formulas} as we have already mention it) based on evaluating the integrand at equally spaced points (they are named after Isaac Newton and Roger Cotes) and where the weights $w_i$ of:
	
	are derived from the Lagrange basis polynomials (see page \pageref{lagrange polynomial interpolation method}).
	
	Let us see now three mainstream of the many "\NewTerm{Newton-Cotes quadrature formulas}". They are all based on Lagrange Polynomials (with or without the error term):
	
	Since the Taylor polynomials (\SeeChapter{see section Sequences and Series page \pageref{Taylor polynomial}}) have the property that all the information used in the approximation is concentrated at the single point $x_0$, it is not uncommon for these polynomials to give inaccurate approximations as we move away from $x_0$. This limits Taylor polynomial approximation to the situation in which approximations are needed only at points close to $x_0$.
	
	The more the simple Newton-Cotes methods are based on high degree polynomials, the slower they are and the more difficult they are to code, but the more precise they are. Most often in practice, the total integration domain $[a, b]$ is much too large and the function varies too much on this domain for these methods to give satisfactory results. They are therefore almost never used as such.

	The total domain $[a, b]$ is therefore subdivided into a large number of small intervals over each of which the simple Newton-Cotes methods can be successfully applied. We then speak of "\NewTerm{composite Newton-Cotes method}\index{composite Newton-Cotes method}".
	
	\paragraph{Trapezoidal Rule}\mbox{}\\\\
	We derive the Trapezoidal rule for approximating $\int_a^b f(x)\mathrm{d}x$  using the Lagrange polynomial method, with the linear Lagrange polynomial (see page \pageref{lagrange polynomial interpolation method}).
	
	Let $x_0 = a$, $x_1 = b$, and $h = b-a$:
	
	Thus, the Trapezoidal rule is:
	
	We fall back here on the relation that we have proved earlier but with another approach (a more analytic one...!).	
	
	Since the error term for the Trapezoidal rule involves $f''$, the rule gives the exact result when applied to any function whose second derivative is identically zero. That is, the Trapezoidal rule gives the exact result for polynomials of degree up to or equal to one.
	
	Now we are home free. To get the Riemann sum for:
	
	using the trapezoidal local approximation, assuming $f(x)$ is any integrable function.
	
	Therefore on a long non-local interval splitted in $n$ value such that $h=(b-a)/n$ we use:
	
	Here is the trivial associated pseudocode:
	
	\begin{algorithm}[H]
	 \KwIn{$a$,$b$,$n$,$f$}
	 \KwOut{$\mathrm{Area}$}
	 $\mathrm{Area}:=0$\\
	 $h:=(b-a)/n$\\
	 \For{$i=1,2,\ldots$ \KwTo $n$}{
	  	$\mathrm{Area}=\mathrm{Area}+\dfrac{(a+i\cdot h)-(a+(i-1)\cdot h)}{2}\left(f(a+i\cdot h)+f(a+(i-1)\cdot h)\right)$
	 }
	 \Return $\mathrm{Area}$
	\caption{Trapezoidal's rule}
	\end{algorithm}
	
	\paragraph{Composite Trapezoidal Rule}\mbox{}\\\\
	When the trapezoidal rule is applied on the subintervals it is named a "composite trapezoidal rule".
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}
		\coordinate (p1) at (0.7,3);
		\coordinate (p2) at (1,3.3);
		\coordinate (p3) at (2,2.5);
		\coordinate (p4) at (3,2.5);
		\coordinate (p5) at (4,3.5);
		\coordinate (p6) at (5,4.1);
		\coordinate (p7) at (6,3.4);
		\coordinate (p8) at (7,4.1);
		\coordinate (p9) at (8,4.6);
		\coordinate (p10) at (9,4);
		\coordinate (p11) at (9.5,4.7);
		
		% The cyan background
		\fill[pink!10] 
		  (p2|-0,0) -- (p2) -- (p3) -- (p4) -- (p5) -- (p6) -- (p7) -- (p8) -- (p9) -- (p10) -- (p10|-0,0) -- cycle;
		% the dark cyan stripe
		\fill[pink!30] (p6|-0,0) -- (p6) -- (p7) -- (p7|-0,0) -- cycle;
		% the curve
		\draw[ultra thick, red] 
		  (p1) to[out=70,in=180] (p2) to[out=0,in=150] 
		  (p3) to[out=-50,in=230] (p4) to[out=30,in=220] 
		  (p5) to[out=50,in=150] (p6) to[out=-30,in=180] 
		  (p7) to[out=0,in=230] (p8) to[out=40,in=180] 
		  (p9) to[out=-30,in=180] (p10) to[out=0,in=260] (p11);
		% the broken line connecting points on the curve
		\draw (p2) -- (p3) -- (p4) -- (p5) -- (p6) -- (p7) -- (p8) -- (p9) -- (p10);
		% vertical lines and labels
		\foreach \n/\texto in {2/{a=x_0},3/{x_1},4/{},5/{},6/{x_{j-1}},7/{x_j},8/{},9/{x_{n-1}},10/{b=x_n}}
		{
		  \draw (p\n|-0,0) -- (p\n);
		  \node[below,text height=1.5ex,text depth=1ex,font=\small] at (p\n|-0,0) {$\texto$};
		}
		% The axes
		\draw[->] (-0.5,0) -- (10,0) coordinate (x axis);
		\draw[->] (0,-0.5) -- (0,6) coordinate (y axis);
		% labels for the axes
		\node[below] at (x axis) {$x$};
		\node[left] at (y axis) {$y$};
		% label for the function
		\node[above,text=cyan] at (p11) {$y=f(x)$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Composite trapezoidal rule illustration}
	\end{figure}
	To see this, let us define:
	
	Then:
	
	Thus, the Composite Trapezoidal rule is:
	
	
	\paragraph{Simpson's Rule}\label{Simpson's rule}\mbox{}\\\\
	Simpson's rule can be derived by integrating the second Lagrange polynomial (see page \pageref{lagrange polynomial interpolation method}). However, this derivation gives only an $\mathcal{O}\left(h^{4}\right)$ error term involving $f^{(3)}$. To get a better error term, we use Taylor polynomial to derive the Simpson's rule.

	Let $x_{0}=a, x_{2}=b,$ and $x_{1}=\dfrac{a+b}{2}=a+\dfrac{b-a}{2}=a+h .$ That is, $x_{1}-x_{0}=h$ and $x_{2}-x_{1}=h$. Then:
	
	Thus, the Simpson's rule is:
	
	There are plenty of other possible derivations with plenty of different final formulation that the reader can find in the literature!
	
	Now we are home free. To get the Riemann sum for:
	
	using the quadratic local approximation, assuming $f(x)$ is any integrable function.
	
	Therefore on a long non-local interval splitted in $n$ value such that $h=(b-a)/n$ we use:
	
	Here is the trivial associated pseudocode:
	
	\begin{algorithm}[H]
	 \KwIn{$a$,$b$,$n$,$f$}
	 \KwOut{$\mathrm{Area}$}
	 $\mathrm{Area}:=0$\\
	 $h:=(b-a)/n$\\
	 \For{$i=0,1,\ldots$ \KwTo $n-1$}{
	  	$\mathrm{Area}=\mathrm{Area}+\dfrac{h}{3}\left[f\left(a+i\cdot h\right)+4 f\left(\dfrac{(a+i\cdot h)+(a+(i+1)\cdot h)}{2}\right)+f\left(a+(i+1)h\right)\right]$
	 }
	 \Return $\mathrm{Area}$
	\caption{Simpson's rule}
	\end{algorithm}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	When Simpson's rule is applied on two subintervals it is named a "\NewTerm{composite Simpson's  rule}".
	\end{tcolorbox}
	All what we have seen so far can be summarized by the following figure:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[scale=0.8]
	
		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		%  Left Endpoint Rule
		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		
		\begin{scope}[declare function={ f(\x) = 1.6+sin(deg(2*\x)); }]
		\def\startx{0}; % domain begins
		\def\endx{3};   % domain ends
		% control spread between graphs:
		\pgfmathsetmacro\scopespace{\endx-\startx + 0.7};
		
		\node[right] at (\scopespace cm,4) {\large Left Endpoint Rule};
		% list values of n to display
		\foreach \n [count=\i] in {2,4,8,16,32} {
		   \begin{scope}[xshift=\i*\scopespace cm]
		   \draw(\startx,0)--(\endx,0);
		   \pgfmathsetmacro\dx{(\endx-\startx)/\n};
		   \pgfmathsetmacro\lastr{int(\n-1)};
		
		   \foreach \j in {0,...,\lastr} {
		       \pgfmathsetmacro\mx{\dx*\j}
		       \pgfmathsetmacro\my{f(\mx)}
		       \draw[fill=cyan!50, fill opacity=0.4, very thin] (\mx,0) rectangle (\mx+\dx,\my);
		   }
		
		   \draw[domain=\startx:\endx,smooth,variable=\x, ultra thick, black,opacity=0.6]  plot ({\x},{f(\x)});
		   \node[above right] at (0,3) {$n = $ \n};
		
		   \draw(\startx,0)--(\startx,-0.1);
		   \draw(\endx,0)--(\endx,-0.1);
		   \node[below] at (\startx,-0.1) {$x_0$};
		   \node[below] at (\endx,-0.1) {$x_{\n}$};
		   \end{scope}
		}
		\end{scope}
		 
		
		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		%  Right Endpoint Rule
		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		
		
		\begin{scope}[yshift=-6cm,declare function={ f(\x) = 1.6+sin(deg(2*\x)); }]
		\def\startx{0}; % domain begins
		\def\endx{3};   % domain ends
		% control spread between graphs:
		\pgfmathsetmacro\scopespace{\endx-\startx + 0.7};
		
		\node[right] at (\scopespace cm,4) {\large Right Endpoint Rule};
		
		% list values of n to display
		\foreach \n [count=\i] in {2,4,8,16,32} {
		
		\begin{scope}[xshift=\i*\scopespace cm]
		\draw(\startx,0)--(\endx,0);
		\pgfmathsetmacro\dx{(\endx-\startx)/\n};
		\pgfmathsetmacro\lastr{int(\n-1)};
		
		\foreach \j in {1,...,\n} {
		    \pgfmathsetmacro\mx{\dx*\j}
		    \pgfmathsetmacro\my{f(\mx)}
		    \draw[fill=cyan!50, fill opacity=0.4, very thin] (\mx-\dx,0) rectangle (\mx,\my);
		}
		
		\draw[domain=\startx:\endx,smooth,variable=\x, ultra thick, black,opacity=0.6]  plot ({\x},{f(\x)});
		\node[above right] at (0,3) {$n = $ \n};
		
		\draw(\startx,0)--(\startx,-0.1);
		\draw(\endx,0)--(\endx,-0.1);
		\node[below] at (\startx,-0.1) {$x_0$};
		\node[below] at (\endx,-0.1) {$x_{\n}$};
		
		\end{scope}
		}
		\end{scope}
		 
		
		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		%  Midpoint Rule
		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		
		
		\begin{scope}[yshift=-12cm,declare function={ f(\x) = 1.6+sin(deg(2*\x)); }]
		\def\startx{0}; % domain begins
		\def\endx{3};   % domain ends
		% control spread between graphs:
		\pgfmathsetmacro\scopespace{\endx-\startx + 0.7};
		
		\node[right] at (\scopespace cm,4) {\large Midpoint Rule};
		
		% list values of n to display
		\foreach \n [count=\i] in {2,4,8,16,32} {
		
		\begin{scope}[xshift=\i*\scopespace cm]
		\draw(\startx,0)--(\endx,0);
		\pgfmathsetmacro\dx{(\endx-\startx)/\n};
		\pgfmathsetmacro\lastr{int(\n-1)};
		
		\foreach \j in {0,...,\lastr} {
		    \pgfmathsetmacro\mx{\dx*\j + 0.5*\dx}
		    \pgfmathsetmacro\my{f(\mx)}
		    \draw[fill=cyan!50, fill opacity=0.4, very thin] (\j*\dx,0) rectangle (\j*\dx+\dx,\my);
		}
		
		\draw[domain=\startx:\endx,smooth,variable=\x, ultra thick, black,opacity=0.6]  plot ({\x},{f(\x)});
		\node[above right] at (0,3) {$n = $ \n};
		
		\draw(\startx,0)--(\startx,-0.1);
		\draw(\endx,0)--(\endx,-0.1);
		\node[below] at (\startx,-0.1) {$x_0$};
		\node[below] at (\endx,-0.1) {$x_{\n}$};
		
		\end{scope}
		}
		\end{scope}
		 
		
		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		%  Trapezoid Rule
		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		
		
		\begin{scope}[yshift=-18cm,declare function={ f(\x) = 1.6+sin(deg(2*\x)); }]
		\def\startx{0}; % domain begins
		\def\endx{3};   % domain ends
		% control spread between graphs:
		\pgfmathsetmacro\scopespace{\endx-\startx + 0.7};
		
		\node[right] at (\scopespace cm,3.7) {\large Trapezoid Rule};
		
		% list values of n to display
		\foreach \n [count=\i] in {2,4,8,16,32} {
		
		\begin{scope}[xshift=\i*\scopespace cm]
		\draw(\startx,0)--(\endx,0);
		\pgfmathsetmacro\dx{(\endx-\startx)/\n};
		\pgfmathsetmacro\lastr{int(\n-1)};
		
		\foreach \j in {0,...,\lastr} {
		    \pgfmathsetmacro\thisa{\dx*\j}
		    \pgfmathsetmacro\thisb{\dx*\j+\dx}
		    \pgfmathsetmacro\thisay{f(\thisa)}
		    \pgfmathsetmacro\thisby{f(\thisb)}
		    \draw[fill=cyan!50, fill opacity=0.4, very thin] (\thisa,0) --(\thisa,\thisay) --(\thisb,\thisby)--(\thisb,0)--cycle;
		}
		
		\draw[domain=\startx:\endx,smooth,variable=\x, ultra thick, black,opacity=0.6]  plot ({\x},{f(\x)});
		\node[above right] at (0,2.7) {$n = $ \n};
		
		\draw(\startx,0)--(\startx,-0.1);
		\draw(\endx,0)--(\endx,-0.1);
		\node[below] at (\startx,-0.1) {$x_0$};
		\node[below] at (\endx,-0.1) {$x_{\n}$};
		
		\end{scope}
		}
		\end{scope}
		 
		
		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		%  Simpson's Rule
		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		
		
		\begin{scope}[yshift=-24cm,declare function={ f(\x) = 1.6+sin(deg(2*\x)); }]
		\def\startx{0}; % domain begins
		\def\endx{3};   % domain ends
		% control spread between graphs:
		\pgfmathsetmacro\scopespace{\endx-\startx + 0.7};
		
		\node[right] at (\scopespace cm,3.7) {\large Simpson's Rule};
		
		% draw n=2 case separately
		\pgfmathsetmacro\dx{(\endx-\startx)/2};
		\pgfmathsetmacro\thisxa{\startx}
		\pgfmathsetmacro\thisxb{\thisxa + \dx}
		\pgfmathsetmacro\thisxc{\thisxa + 2*\dx}
		\pgfmathsetmacro\thisya{f(\thisxa)}
		\pgfmathsetmacro\thisyb{f(\thisxb)}
		\pgfmathsetmacro\thisyc{f(\thisxc)}
		
		\begin{scope}[xshift=\scopespace cm]
		\draw[domain=\startx:\endx,smooth,variable=\x, fill=cyan!50, fill opacity=0.4, very thin]  plot ({\x},{
		  \thisya*((\x-\thisxb)*(\x-\thisxc))/((\thisxa-\thisxb)*(\thisxa-\thisxc)) +
		  \thisyb*((\x-\thisxa)*(\x-\thisxc))/((\thisxb-\thisxa)*(\thisxb-\thisxc)) +
		  \thisyc*((\x-\thisxa)*(\x-\thisxb))/((\thisxc-\thisxa)*(\thisxc-\thisxb))
		}) -- (\thisxc,0)--(\thisxa,0)--cycle;
		\draw[dotted,thin] (\thisxb,0)--(\thisxb,\thisyb);
		
		\draw[domain=\startx:\endx,smooth,variable=\x, ultra thick, black,opacity=0.6]  plot ({\x},{f(\x)});
		\node[above right] at (0,2.7) {$n = 2$};
		
		\draw(\startx,0)--(\startx,-0.1);
		\draw(\endx,0)--(\endx,-0.1);
		\node[below] at (\startx,-0.1) {$x_0$};
		\node[below] at (\endx,-0.1) {$x_2$};
		\end{scope}
		
		% list values of n to display
		\foreach \n [count=\i from 2] in {4,8,16,32} {
		
		\begin{scope}[xshift=\i*\scopespace cm]
		\draw(\startx,0)--(\endx,0);
		\pgfmathsetmacro\dx{(\endx-\startx)/\n};
		\pgfmathsetmacro\lastr{int(\n-2)};
		
		\foreach \j in {0,2,...,\lastr} {
		    \pgfmathsetmacro\thisxa{\j*\dx}
		    \pgfmathsetmacro\thisxb{\thisxa + \dx}
		    \pgfmathsetmacro\thisxc{\thisxa + 2*\dx}
		    \pgfmathsetmacro\thisya{f(\thisxa)}
		    \pgfmathsetmacro\thisyb{f(\thisxb)}
		    \pgfmathsetmacro\thisyc{f(\thisxc)}
		
		
		    \draw[domain=\thisxa:\thisxc,smooth,variable=\x, fill=cyan!50, fill opacity=0.4, very thin]  plot ({\x},{
		      \thisya*((\x-\thisxb)*(\x-\thisxc))/((\thisxa-\thisxb)*(\thisxa-\thisxc)) +
		      \thisyb*((\x-\thisxa)*(\x-\thisxc))/((\thisxb-\thisxa)*(\thisxb-\thisxc)) +
		      \thisyc*((\x-\thisxa)*(\x-\thisxb))/((\thisxc-\thisxa)*(\thisxc-\thisxb))
		    }) -- (\thisxc,0)--(\thisxa,0)--cycle;
		    \draw[dotted,thin] (\thisxb,0)--(\thisxb,\thisyb);
		}
		
		\draw[domain=\startx:\endx,smooth,variable=\x, ultra thick, black,opacity=0.6]  plot ({\x},{f(\x)});
		\node[above right] at (0,2.7) {$n = $ \n};
		
		\draw(\startx,0)--(\startx,-0.1);
		\draw(\endx,0)--(\endx,-0.1);
		\node[below] at (\startx,-0.1) {$x_0$};
		\node[below] at (\endx,-0.1) {$x_{\n}$};
		
		\end{scope}
		}
		\end{scope}
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Quadrature Rules (author: Lauren K. Williams)}
	\end{figure}
	
	\subsubsection{Multidimensional integrals}
	Integrals of functions of several variables, over regions with dimension greater than one, are not easy. There are two reasons for this. First, the number of function evaluations needed to sample an $N$-dimensional space increases as the $N$th power of the number needed to do a one-dimensional integral. If you need $30$ function evaluations to do a one-dimensional integral crudely, then you will likely need on the order of $30\times 30\times 30=27000$ evaluations to reach the same crude level for a three-dimensional integral. Second, the region of integration in $N$-dimensional space is defined by an $N-1$ dimensional boundary which can itself be terribly complicated: It need not be convex or simply connected, for example. By contrast, the boundary of a one-dimensional integral consists of two numbers, its upper and lower limits.
	
	The first question to be asked, when faced with a multidimensional integral is if can it be reduced analytically to a lower dimensionality one?
	
	Alternatively, the function may have some special symmetry in the way it depends on its independent variables. If the boundary also has this symmetry, then the dimension can be reduced. In three dimensions, for example, the integration of a spherically symmetric function over a spherical region reduces, in polar coordinates, to a one-dimensional integral.
	
	The next questions to be asked will guide your choice between two entirely different approaches to doing the problem. The questions are: Is the shape of the boundary of the region of integration simple or complicated? Inside the region, is the integrand smooth and simple, or complicated, or locally strongly peaked? Does the problem require high accuracy, or does it require an answer accurate only to
a percent, or a few percent?

	If your answers are that the boundary is complicated, the integrand is not strongly peaked in very small regions, and relatively low accuracy is tolerable, then your problem is a good candidate for Monte Carlo integration. This method is very straightforward to program, in its cruder forms. One needs only to know a region with simple boundaries that includes the complicated region of integration, plus a method of determining whether a random point is inside or outside the region of integration. Monte Carlo integration evaluates the function at a random sample of points, and estimates its integral based on that random sample.

	If the boundary is simple, and the function is very smooth, then the remaining approaches, breaking up the problem into repeated one-dimensional integrals, or multidimensional Gaussian quadratures, will be effective and relatively fast. If you require high accuracy, these approaches are in any case the only ones available to you, since Monte Carlo methods are by nature asymptotically slow to converge.
	
	For low accuracy, use repeated one-dimensional integration or  
Gaussian quadratures when the integrand is slowly varying and smooth in the region of integration, Monte Carlo when the integrand is oscillatory or discontinuous, but not strongly peaked in small regions.

	If the integrand is strongly peaked in small regions, and you know where those regions are, break the integral up into several regions so that the integrand is smooth in each, and do each separately. If you don't know where the strongly peaked regions are, you might as well (at the level of sophistication of this book) quit: It is hopeless to expect an integration routine to search out unknown pockets of large contribution in a huge $N$-dimensional space.
	
	If, on the basis of the above guidelines, we decide to pursue the repeated one-dimensional integration approach, here is how it works (the text below about the repeated one-dimensions approach is a bit a small summary of multivariate integral that we have already study in-deep in the section of Differential and Integral Calculus page \pageref{double integral}). For definiteness, we will consider the case of a three-dimensional integral in $x$, $y$, $z$-space. Two dimensions, or more than three dimensions, are entirely analogous.

	The first step is to specify the region of integration by:
	\begin{enumerate}
		\item Its lower and upper limits in $x$, which we will denote $x_1$ and $x_2$
		
		\item Its lower and upper limits in $y$ at a specified value of $x$, denoted $y_1(x)$ and $y_2(x)$
		
		\item Its lower and upper limits in $z$ at specified $x$ and $y$, denoted $z_1(x, y)$ and $z_2(x, y)$. 
	\end{enumerate}
	In other words, find the numbers $x_1$ and $x_2$, and the functions $y_1(x)$, $y_2(x)$, $z_1(x, y)$, and $z_2(x, y)$ such that:
	
	Now we can define a function $G(x, y)$ that does the innermost integral:
	
	and a function $H(x)$ that does the integral of $G(x,y)$:
	
	and finally our answer as an integral over $H(x)$:
	
	The methods that we have introduced earlier to calculate simple integrals can be generalized to multiple integrals. Les us consider first the following integral:
	
	where $\mathcal{R}$ is a rectangular area of the plane, that is of the type:
	
	the limits of integration $a,b,c$ and $d$ being (for the moment...) constants.
	
	To determine an approximated value of this integral, let us write it under the form:
	
	and let us use the Simpson's method that we have proved earlier (see page \pageref{Simpson's rule}):
	
	for each of the integrals. Notice that every Newton-Cotes method can be used and that it is not necessary to use the same method for the two variables.
	
	Let us choose two integers $m$ and $n$ to subdivide the interval $[a,b]$ into $2m$ sub-intervals with the step:
	
	and $[c,d]$ into $2n$ sub-intervals with the step:
	
	That is $y_0=c$, $y_1=c+k$, $y_{2n}=d$.
	
	The first step has for purpose to calculate the integral:
	
	with the Simpson's method by keeping $x$ constant! We put $y_j=c+jk$, with $j=0,1,\ldots,2n$, therefore $y_0=c$ and $y_{2n}=d$. We then get (without the error term):
	
	and for the double integral:
	
	The Simpson's method can now be used to calculate each integral that leads on the variables $x$. Let us put for this purpose $x_i=a+ih$ with $i=0,1,\ldots,2m$, therefore $x_0=a$ and $x_{2m}=b$. For each value of the index $j$ ($j=0,1,2,\ldots,2n$), that is by $y=y_j$, we then get without error term:
	
	By substituting the latter in the first integral, we get:
	
	This can be written also in matrix form:
	
	The use of numerical approximation methods for double integrals is not limited to rectangular areas. The integrals of the shape:
	
	can be computed by modifying a bit the previous described method.
	
	As example, let us use the Simpson's method to determine an approximation of the following double integral:
	
	in the simplest case with $m=n=1$. Therefore the integration step of the variables $x$ is then:
	
	when that of the step of $y$ varies with $x$:
	
	The estimation of the integral then takes the following shape (without the error term):
	
	
	\pagebreak
	\subsubsection{Numerical solution of ordinary differential equations}
	Numerical integration of ordinary differential equations is a frequent task of numerical analysis. Numerical integration of differential equations is used if the equations are non-linear or if we have a large system of linear equations with constant coefficients, where the analytical solution can be found, but it is in the form of long and complicated expressions containing exponential functions. Numerical integration of such systems is more efficient both in human time and in computer time. 

	Numerical integration of linear equations with non-constant coefficients is also more efficient than the analytical solution; in the case of inner diffusion in porous catalyst with a
chemical reaction of the first order the analytical solution contains Bessel functions, which can be evaluated more conveniently when we use numerical integration of the original equations
than to evaluate Bessel functions.

	We have already seen how to use Euler's method and the method of Taylor's expansion at the page \pageref{Euler method}, let us see more elaborated techniques.
	
	 \paragraph{Runge-Kutta methods}\index{Runge-Kutta method}\label{Runge-Kutta methods}\mbox{}\\\\
	 In numerical analysis, the Runge–Kutta methods are a family of implicit and explicit iterative methods, which include the well-known "Euler's method" (we will prove that further below), used in temporal discretization for the approximate solutions of ordinary differential equations. These methods were developed around 11900 (holocene calendar) by the German mathematicians Carl Runge and Wilhelm Kutta.
	 
	  The techniques discussed in these pages approximate the solution of first order ordinary differential equations (with initial conditions) of the form ("Cauchy's problem" type):
	 
	 In the following developments you will understand why we say sometimes that Runge-Kutta methods "simulate" the Taylor's method!
	
	\subparagraph{1st order Runge-Kutta method}\mbox{}\\\\
	The first order Runge-Kutta method is already known to us. It's simply the "Euler's method" already derived earlier before.
	
	Let us review that latter introducing a notation that is really mainstream and will be useful to understand the higher Runge-Kutta order methods.
	
	For this we start again from:
	
	 and we write the approximation to the derivative as:
	 
	 We expand $y(t)$ around $t_0$ assuming a time step $h$ using Taylor expansion given for recall by:
	 
	This give us:
	 
	and drop all terms after the linear term. Because all of the dropped terms are multiplied by $h^2$ or greater, we say that the algorithm is accurate to order $h^2$ locally, or $\mathcal{O}(h^2)$ (if $h$ is small the other terms that are multiplied by $h^3$, $h^4$... which will be even smaller, and can be dropped as well):
	
	This gives us our approximate Euler's method solution at the next time step:
	
	Since the number of steps over the whole interval is proportional to $1/h$ (or $\mathcal{O}(h^{-1})$) we might expect the overall accuracy to be the $\mathcal{O}(h^2)\cdot \mathcal{O}(h^{-1})=\mathcal{O}(h)$. A rigorous analysis proves that this is true.
	
	With a little more work we can develop a method that is accurate to higher order than $\mathcal{O}(h)$. And this is the second order Runge-Kutta method.
	
	 \subparagraph{2nd order Runge-Kutta method}\mbox{}\\\\
	 In the following derivation we will use two math facts that are reviewed here. You should be familiar with this from a course in multivariate calculus.

	First, let us recall the relation we have derived in the section of Sequences and Series on bivariate Taylor series (see page \pageref{multivariate taylor series}):
	
	Hence:
	
	If $f$ is a function of two variables $f(x,y)$, where $x=r(t)$ and $y=s(t)$, then by the chain rule for partial derivatives:
	
	In particular if:
	
	then:
	
	To start, recall that we are expressing our differential equation as
	 
	 Now we define two approximations to the derivative:
	 
	 In all cases $\alpha$ and $\beta$ will represent fractional values between $0$ and $1$. These equation state that $k_1$ is the approximation to the derivative based on the estimated value of $y(t)$ at $t=t_0$ (i.e., $y^*(t_0)$) and the time at $t_0$. The value of $k_2$ is based upon the estimated value, $y^*(t_0)$, plus some fraction of the step size, $\beta h$, times the slope $k_1$, and the time at $t_0+\alpha h$ (i.e., some time between $t_0$ and $t_0+h$).
	 
	 To update our solution with the next estimate of $y(t)$ at $t_0+h$ we use then the relation:
	  
	 That latter relation can be found in some textbooks under the form:
	 
	 This equation states that we get the value of $y^*(t_0+h)$ from the value of $y^*(t_0)$ plus the time step, $h$, multiplied by a slope that is a weighted sum of $k_1$ and $k_2$. In the method described previously $a=0$ and $b=1$, so we used only the second estimate for the slope. 
	 
	 Note that Euler's Method (first order Runge-Kutta) is a special case of this method with $a=1$, $b=0$, and $\alpha$ and $\beta$ don't matter because $k_2$ is not used in the update equation!
	 \begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In general a Runge-Kutta method of order $s$ will be written as:
	
	 where:
	 
	\end{tcolorbox}
	
	Our goal now is to determine, from first principles, how to find the values $a$, $b$, $\alpha$ and $\beta$ that result in low error. Starting with the update equation above:
	 
	We can use now the bivariate-dimensional Taylor series (where the increment in the first dimension is $\beta hk_1$, and the increment in the second dimension is $\alpha h$) to expand the rightmost term:
	 
	 In the last line we used the fact the $k_1=f$. Now we substitute this in the update equation:
	 
	 To finish we compare this approximation with the expression for a Taylor expansion of the exact solution (going from the first line to the second we used the chain rule for partial derivatives):
	 
	 Comparing this expression with our final expression for the approximation:
	 
	 we see that they agree up to the error terms (third order and higher) if we define the constants, $a$, $b$, $\alpha$ and $\beta$ such that:
	 
	 Also written:
	 
	 This system is underspecified, there are four unknowns, and only three equations, so more than one solution is possible. 
	 
	 The following choices are the most common one:
	 \begin{itemize}
	 	\item $a=0$, $b=1$ and $\alpha=\beta=1/2$, we get:
	 	
	 	This is named the "\NewTerm{middle-point method}".
	 
		\item $a=b=1/2$ and $\alpha=\beta=1$, we get:
	 	
		This is named the "\NewTerm{improved Euler's method}" or "\NewTerm{modified Euler's method}". Notice that the term:
		
		is equivalent to the trapezoidal approximation of an integral of $y^*(y(t),t)$.
		
		\item $a=1/4$, $b=3/4$ and $\alpha=\beta=2/3$, we get:
		
		This is named the "\NewTerm{Heun's method}".
	 \end{itemize}
	More complicated and more accurate methods can be derived by a similar approach and this can be the subject of a whole book.
	
	\subparagraph{4th order Runge-Kutta method}\mbox{}\\\\
	It is the most widely known member of the Runge–Kutta family and generally referred to as "\NewTerm{RK4}\index{RK4}", the "\NewTerm{classic Runge–Kutta method}\index{classic Runge–Kutta method}" or simply as the "\NewTerm{Runge–Kutta method}\index{Runge-Kutta method}".
	
	To derive it, let us recall that general a Runge–Kutta (empirical) method of order $s$ can be written as:
	
	where:
	
	are increments obtained evaluating the derivatives of $y_{t}$ at the $i$-th order.
	
	We develop the derivation for the Runge–Kutta fourth-order method using the general formula with $s=4$ evaluated, at the starting point, the midpoint and the end point of any interval $(t,\ t+h)$. Thus, we choose:
	
	and $\beta _{ij}=0$ otherwise. We begin by defining the following quantities:
	
	where:
	
	If we define:
	
	and for the previous relations we can show that the following equalities hold up to $\mathcal {O}(h^{2})$:
	
	where:
	
	is the total exact differential of $f$ with respect to time.
	
	If we now express the general formula using what we just derived we get:
	
	and comparing this with the Taylor series of $y_{t+h}$ around $y_{t}$:
	
	we get a system of constraints on the coefficients:
	
	which when solved gives:
	
	as stated above.
	
	\pagebreak
	\subsection{Optimization}\label{operational research}
	In mathematics, computer science and operations research, mathematical optimization (alternatively "\NewTerm{mathematical programming}\index{mathematical programming}") is the selection of a best element (with regard to some criteria) from some set of available alternatives.
	
	In the simplest case, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function. The generalization of optimization theory and techniques to other formulations comprises a large area of applied mathematics. More generally, optimization includes finding best available values of some objective function given a defined domain (or a set of constraints), including a variety of different types of objective functions and different types of domains.
	
	In mathematics, conventional optimization problems are usually stated in terms of minimization (or changed to be as!). A large number of algorithms proposed for solving optimization problems are not capable of making a distinction between local optimal solutions and rigorous optimal solutions. The branch of applied mathematics and numerical analysis that is concerned with the development of deterministic algorithms that are capable of guaranteeing convergence in finite time to the actual optimal solution is named "\NewTerm{global optimization}\index{global optimization}".
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	We speak of "\NewTerm{convex optimization}\index{convex optimization}" when any local minimum must be a global minimum. In other word there is only one unique solution.
	\end{tcolorbox}
	In the context of problem solving which involves two variables and their products, we then speak logically "\NewTerm{quadratic programming (QP)}\index{quadratic programming}" or simply "\NewTerm{non-linear programming}\index{non-linear programming}". This is typically the case in financial engineering in portfolios modelling (\SeeChapter{see section Economy page \pageref{markowitz overall minimum variance portfolio}}) or in forecasting. We will study in details also further below simplified and particular version of the corresponding models that are the: Newton's method, quasi-Newton's method, conjugate gradient method and non-linear GRG.
	
	\pagebreak
	\subsubsection{Linear programming (Linear Optimization)}\label{linear programming}
	The objective of the linear programming (LP-programming) is to find the optimum value of a linear function subject to a system of equations consisting in inequalities constraints that are also linear. The objective function is named "\NewTerm{economic function}\index{economic function}" (because used a lot in Economy) and we solve this type of system using typically, among others, a method named "\NewTerm{simplex method}\index{simplex method}\label{simplex method}" (see below), the corresponding graph is a "\NewTerm{polygon constraints}\index{polygon constraints}" (when the number of variable is obviously equal to $2$).
	
	The reader can remember the following diagram (convex polytope) that we saw in the section Calculus:	
	\begin{center}
	\begin{tikzpicture}[scale=2]
    \draw[gray!50, thin, step=0.5] (-1,-3) grid (5,4);
    \draw[very thick,->] (-1,0) -- (5.2,0) node[right] {$x_1$};
    \draw[very thick,->] (0,-3) -- (0,4.2) node[above] {$x_2$};

    \foreach \x in {-1,...,5} \draw (\x,0.05) -- (\x,-0.05) node[below] {\tiny\x};
    \foreach \y in {-3,...,4} \draw (-0.05,\y) -- (0.05,\y) node[right] {\tiny\y};

    \fill[blue!50!cyan,opacity=0.3] (8/3,1/3) -- (1,2) -- (13/3,11/3) -- cycle;

    \draw (-1,4) -- node[below,sloped] {\tiny$x_1+x_2\geq3$} (5,-2);
    \draw (1,-3) -- (3,1) -- node[below left,sloped] {\tiny$2x_1-x_2\leq5$} (4.5,4);
    \draw (-1,1) -- node[above,sloped] {\tiny$-x_1+2x_2\leq3$} (5,4);

	\end{tikzpicture}
	\end{center}
	Corresponding to the following system of inequalities:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Linear programming is widely used (to name only the most famous case) in Supply Chain (maximal flow problem also named "\NewTerm{transport problem}\index{transport problem}"), in Corporate Finance or also in Decision Theory when we solve a mixed strategy game (see the section of Game and Decision Theory for a practical example). That's why Microsoft Excel 12.0 and earlier includes a tool named the "solver" in which there is an option named "Assume Linear Model" which then requires the use of the simplex model that we will study below:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/excel_solver_lp.jpg}
	\end{figure}
	or since the 2010 version of the software (the user interface has completely changed):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/computing/excel_solver_lp_2010.jpg}
	\end{figure}
	\end{tcolorbox}
	We will focus especially in this section on the most widely used algorithm for linear optimization named the "\NewTerm{simplex algorithm}\index{simplex algorithm}\label{simplex algorithm}".
	
	When a problem can be modelled as an economic function to be maximized with respect to certain constraints that are purely additive, we are typically in the context of linear programming!
	
	So an economic function $Z$ is defined as:
	
	where the $x_i$ are variables that affect the value of $Z$, and the $c_i$ the weights of these variables modelling the relative importance of each of them on the value of the economic function.
	
	The constraints  related to the variables are expressed by the following linear system:
	
	Under general and matrix form this problem is written as:
	
	To see the different methods of resolution let us use an example as theoretical introduction:
	
	A factory produces two types of pieces $P1$ and $P2$ machined in two workshops $A1$ and $A2$. Machining times are for $P1$ or $3$ hours in the workshop $A1$ and $6$ hours in the workshop $A2$ and of for $4$ hours for $P2$ in the workshop $A1$ and $3$ hours in the workshop $A2$.
	
	Weekly up-time of human resources (workers) of the $A1$ workshop is $160$ hours and that of the workshop $A2$ is $180$ hours.
	
	The profit margin is of $1,200.-$ for the pieces $P1$ and $1,000.-$ for pieces $P2$.

	The question is how much of each kind of piece should we make to maximize weekly margin?
	
	This will be formalized as follows (canonical formulation):
	
	
	\paragraph{Graphical LP resolution}\mbox{}\\\\
	The graphical method is will adapted for problem with $2$ or $3$ variables but not more as our perception of hyper-volume is quite limited for humans.
		
	When translate our optimization problem into graphical form, we speak also of "\NewTerm{polygon of constraints}\index{polygon of constraints}". Indeed, the economical constraints are represented by half planes. The solutions, if they exists, belongs to the intersection set name "\NewTerm{set of admissible solutions}\index{set of admissible solutions}" and is quite trivially represented in our case by:	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1017); %set diagram left start at 0, and has height of 1017
		
		%Shape: Axis 2D [id:dp7818961550684664] 
		\draw  (130,419.2) -- (583.5,419.2)(175.35,43) -- (175.35,461) (576.5,414.2) -- (583.5,419.2) -- (576.5,424.2) (170.35,50) -- (175.35,43) -- (180.35,50) (225.35,414.2) -- (225.35,424.2)(275.35,414.2) -- (275.35,424.2)(325.35,414.2) -- (325.35,424.2)(375.35,414.2) -- (375.35,424.2)(425.35,414.2) -- (425.35,424.2)(475.35,414.2) -- (475.35,424.2)(525.35,414.2) -- (525.35,424.2)(170.35,369.2) -- (180.35,369.2)(170.35,319.2) -- (180.35,319.2)(170.35,269.2) -- (180.35,269.2)(170.35,219.2) -- (180.35,219.2)(170.35,169.2) -- (180.35,169.2)(170.35,119.2) -- (180.35,119.2)(170.35,69.2) -- (180.35,69.2) ;
		\draw   ;
		%Straight Lines [id:da671642760181361] 
		\draw [line width=1.5]    (145.5,63) -- (352.5,470) ;
		%Straight Lines [id:da017696331895517003] 
		\draw [line width=1.5]    (110.5,170) -- (503.5,469) ;
		%Shape: Polygon [id:ds4840080384946932] 
		\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][fill={rgb, 255:red, 255; green, 211; blue, 217 }  ,fill opacity=1 ][line width=2.25]  (326.5,419) -- (175.35,419.2) -- (175.5,220) -- (258.5,284) -- cycle ;
		
		% Text Node
		\draw (150,30.4) node [anchor=north west][inner sep=0.75pt]    {$x_{2}$};
		% Text Node
		\draw (568,429.4) node [anchor=north west][inner sep=0.75pt]    {$x_{1}$};
		% Text Node
		\draw (145,360.4) node [anchor=north west][inner sep=0.75pt]    {$10$};
		% Text Node
		\draw (145,310.75) node [anchor=north west][inner sep=0.75pt]    {$20$};
		% Text Node
		\draw (145,261.08) node [anchor=north west][inner sep=0.75pt]    {$30$};
		% Text Node
		\draw (145,211.41) node [anchor=north west][inner sep=0.75pt]    {$40$};
		% Text Node
		\draw (145,161.74) node [anchor=north west][inner sep=0.75pt]    {$50$};
		% Text Node
		\draw (145,112.07) node [anchor=north west][inner sep=0.75pt]    {$60$};
		% Text Node
		\draw (145,62.4) node [anchor=north west][inner sep=0.75pt]    {$70$};
		% Text Node
		\draw (214,429.08) node [anchor=north west][inner sep=0.75pt]    {$10$};
		% Text Node
		\draw (264.5,429.08) node [anchor=north west][inner sep=0.75pt]    {$20$};
		% Text Node
		\draw (315,429.08) node [anchor=north west][inner sep=0.75pt]    {$30$};
		% Text Node
		\draw (365.5,429.08) node [anchor=north west][inner sep=0.75pt]    {$40$};
		% Text Node
		\draw (416,429.08) node [anchor=north west][inner sep=0.75pt]    {$50$};
		% Text Node
		\draw (466.5,429.08) node [anchor=north west][inner sep=0.75pt]    {$60$};
		% Text Node
		\draw (517,429.08) node [anchor=north west][inner sep=0.75pt]    {$70$};
		% Text Node
		\draw (213,161.4) node [anchor=north west][inner sep=0.75pt]    {$6x_{1} +3x_{2} =180$};
		% Text Node
		\draw (460,477.4) node [anchor=north west][inner sep=0.75pt]    {$3x_{1} +4x_{2} =160$};
		% Text Node
		\draw (191,322) node [anchor=north west][inner sep=0.75pt]   [align=left] {Area of\\admissible\\solutions};
		% Text Node
		\draw (155,425) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[]{Illustration of a simple operational research problem with area of feasible solutions\\(convex polytope)}
	\end{figure}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In the general case, for those who love the language of mathematicians ..., the information of a linear constraint geometrically problem corresponds to a half space of $n$-dimensional space ($n$ being the number of variables). In the elementary case, all the points in space that satisfy all constraints are limited by a convex portion of a hyperplane (see the case with $2$ variables, easy more to illustrate...), that is why this type of problems are also named "\NewTerm{convex optimization}\index{convex optimization}". If the cost function is linear, the extreme point is a vertices (easy to see in a two dimensional case). The basic algorithm simplex algorithm (see further below) start from one vertices and goes to the next vertices which locally maximizes the cost, and restarts the procedure as long as necessary.
	\end{tcolorbox}	
	To find the coordinates of the vertices, we can use the graph if the points are easy to determine.

	It is therefore to seek inside this area (connex), the pair  $(x_1,x_2)$ maximizing the economic function.

	However, the equation $Z$ is represented by a constant line of constant slope ($-1.2$) which all points $(x_1,x_2)$ provide the same $Z$ value for the objective function.
	
	In particular, the straight line $1200x_1+1000x_2$ pass trough the origin and it provides a zero value to the economic function. To increase the value of $Z$ and therefore of the economic function, we have to take away from the origin (in the quarter $x_1\geq 0,x_2\geq 0$ that is to say in the \texttt{I}st quadrant) the line of slope $-1.2$. Obviously then we see very quickly that the simplex method will not work if the constraints of the polygon does not contain the origin point!
	
	To meet the constraints, this straight line will be moved until the limit where it will not have a point of intersection anymore in common with the area of admissible solutions (eventually a segment).
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1017); %set diagram left start at 0, and has height of 1017
		
		%Shape: Axis 2D [id:dp7818961550684664] 
		\draw  (130,419.2) -- (583.5,419.2)(175.35,43) -- (175.35,461) (576.5,414.2) -- (583.5,419.2) -- (576.5,424.2) (170.35,50) -- (175.35,43) -- (180.35,50) (225.35,414.2) -- (225.35,424.2)(275.35,414.2) -- (275.35,424.2)(325.35,414.2) -- (325.35,424.2)(375.35,414.2) -- (375.35,424.2)(425.35,414.2) -- (425.35,424.2)(475.35,414.2) -- (475.35,424.2)(525.35,414.2) -- (525.35,424.2)(170.35,369.2) -- (180.35,369.2)(170.35,319.2) -- (180.35,319.2)(170.35,269.2) -- (180.35,269.2)(170.35,219.2) -- (180.35,219.2)(170.35,169.2) -- (180.35,169.2)(170.35,119.2) -- (180.35,119.2)(170.35,69.2) -- (180.35,69.2) ;
		\draw   ;
		%Straight Lines [id:da671642760181361] 
		\draw [line width=1.5]    (145.5,63) -- (352.5,470) ;
		%Straight Lines [id:da017696331895517003] 
		\draw [line width=1.5]    (131.5,187) -- (503.5,470) ;
		%Shape: Polygon [id:ds4840080384946932] 
		\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][fill={rgb, 255:red, 255; green, 211; blue, 217 }  ,fill opacity=1 ][line width=2.25]  (326.5,419) -- (175.35,419.2) -- (175.5,220) -- (258.5,284) -- cycle ;
		%Straight Lines [id:da7064072905898551] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (81.5,339) -- (282.5,507) ;
		%Straight Lines [id:da929498995764177] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (94.5,245) -- (409.5,508) ;
		%Straight Lines [id:da323096522549043] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (123.5,170) -- (451.5,443) ;
		%Straight Lines [id:da21709694610707908] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (226,458) -- (320.25,339.56) ;
		\draw [shift={(321.5,338)}, rotate = 128.51] [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da4697201993993889] 
		\draw    (346.5,222) -- (260.13,282.85) ;
		\draw [shift={(258.5,284)}, rotate = 324.83] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		
		% Text Node
		\draw (150,30.4) node [anchor=north west][inner sep=0.75pt]    {$x_{2}$};
		% Text Node
		\draw (568,429.4) node [anchor=north west][inner sep=0.75pt]    {$x_{1}$};
		% Text Node
		\draw (145,360.4) node [anchor=north west][inner sep=0.75pt]    {$10$};
		% Text Node
		\draw (145,310.75) node [anchor=north west][inner sep=0.75pt]    {$20$};
		% Text Node
		\draw (145,261.08) node [anchor=north west][inner sep=0.75pt]    {$30$};
		% Text Node
		\draw (145,211.41) node [anchor=north west][inner sep=0.75pt]    {$40$};
		% Text Node
		\draw (145,161.74) node [anchor=north west][inner sep=0.75pt]    {$50$};
		% Text Node
		\draw (145,112.07) node [anchor=north west][inner sep=0.75pt]    {$60$};
		% Text Node
		\draw (145,62.4) node [anchor=north west][inner sep=0.75pt]    {$70$};
		% Text Node
		\draw (214,429.08) node [anchor=north west][inner sep=0.75pt]    {$10$};
		% Text Node
		\draw (264.5,429.08) node [anchor=north west][inner sep=0.75pt]    {$20$};
		% Text Node
		\draw (315,429.08) node [anchor=north west][inner sep=0.75pt]    {$30$};
		% Text Node
		\draw (365.5,429.08) node [anchor=north west][inner sep=0.75pt]    {$40$};
		% Text Node
		\draw (416,429.08) node [anchor=north west][inner sep=0.75pt]    {$50$};
		% Text Node
		\draw (466.5,429.08) node [anchor=north west][inner sep=0.75pt]    {$60$};
		% Text Node
		\draw (517,429.08) node [anchor=north west][inner sep=0.75pt]    {$70$};
		% Text Node
		\draw (213,161.4) node [anchor=north west][inner sep=0.75pt]    {$6x_{1} +3x_{2} =180$};
		% Text Node
		\draw (460,477.4) node [anchor=north west][inner sep=0.75pt]    {$3x_{1} +4x_{2} =160$};
		% Text Node
		\draw (191,322) node [anchor=north west][inner sep=0.75pt]   [align=left] {Area of\\admissible\\solutions};
		% Text Node
		\draw (155,425) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
		% Text Node
		\draw (351,209) node [anchor=north west][inner sep=0.75pt]   [align=left] {optimal point};
		% Text Node
		\draw (152,507.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,opacity=1 ]  {$1200x_{1} +1000x_{2} =0$};
		% Text Node
		\draw (344,506.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,opacity=1 ]  {$1200x_{1} +1000x_{2} =z_{0}$};
		% Text Node
		\draw (329,317) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,opacity=1 ] [align=left] {direction of optimization};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Finding solutions graphically with the economic function}
	\end{figure}
	The optimal solution is therefore necessarily located on the periphery of the region of admissible solutions and the parallel formed by translating the economic function are named "\NewTerm{isoquants lines}\index{isoquants lines}" or "\NewTerm{isocost lines}\index{isocost lines}"...
		
	\paragraph{Algebraic LP resolution}\mbox{}\\\\	
	Let us now see how to solve this problem analytically before moving to the theoretical part.
	
	So we have the "\NewTerm{canonical system}\index{canonical system}":
	
	with:
	
	We first introduce the "\NewTerm{slack variables}\index{slack variables}" to transform the $2$ inequalities in equalities. The system of equations takes then "\NewTerm{standard form}\index{standard form}":
	
	Therefore, for $x_1,x_2\geq 0$ fixed, the slack variable whose coefficients are always unit, measure the distance to travel to reach the vertices.

	It goes without saying that the technique of slack variables may be used for linear (or non-linear) systems. Therefore, a constraint optimization system with inequalities, can always be reduced to an optimization system with equalities.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Obviously there is as much slack variable as inequalities.
	\end{tcolorbox}
	For the remaining part, we have noticed, after a review of this section, that the technique using tables (that we will see later) often presented in textbooks and websites finally brought nothing to a deep understanding of the resolution mechanism. Since the purpose of the present book is to always prove with a maximum of details the operating principle of a process, it goes without saying that we will opt first for a purely algebraic approach! Let us see it by returning to the system with the slack variables and the economic function but slightly rearranged:
	
	The $A1$ constraint then becomes:
	
	and the constraint $A2$ respectively becomes:
	
	Therefore, the problem consist to maximize $Z$ with the constraints:
	
	Let us start with an obvious feasible solution given the constraints that is trivially:
	
	Therefore with the system:
	
	we find immediately:
	
	The parameters in the actual state can be summarized as:
	
	To go forward, the goal will be to make $Z$ grow and for this purpose we will increase only one single variable, choosing the one with the largest coefficient (weight) in:
	
	that is to say $x_1$ (because implicitly we think this is how the $Z$ will increase the faster). We speak then of $x_1$ as the "\NewTerm{pivot direction}\index{pivot direction}". We keep then $x_2=0$ and we increase $x_1$ with the system which then reduces to:
	
	Therefore with $x_2=0$ and to begin $x_1=1$, we have:
	
	and we see that the constraints $x_1,x_2,x_3,x_4\geq 0$ are still respected, it is the same if $x_1$ is equal to $2$, $3$, $4$, $5$, ... and this until $31$, because after:
	
	and one of the slack variable has become negative, the constraints $x_1,x_2,x_3,x_4\geq 0$ are not all met and therefore this solution is not feasible.
	
	The question in the general case is to ask ourselves until what value (the most constraint value, verbatim the smallest) we can increase $x_1$ while maintaining the condition $x_1,x_2,x_3,x_4\geq 0$ when $x_2=0$? And the answer is quite simple:
	
	and therefore it is:
	
	then we speak sometimes of the "\NewTerm{pivot step}\index{pivot step}". We then have the actual solution:
	
	Which gives:
	
	Graphically, this is what we have just done:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1017); %set diagram left start at 0, and has height of 1017
		
		%Shape: Axis 2D [id:dp7818961550684664] 
		\draw  (130,419.2) -- (583.5,419.2)(175.35,43) -- (175.35,461) (576.5,414.2) -- (583.5,419.2) -- (576.5,424.2) (170.35,50) -- (175.35,43) -- (180.35,50) (225.35,414.2) -- (225.35,424.2)(275.35,414.2) -- (275.35,424.2)(325.35,414.2) -- (325.35,424.2)(375.35,414.2) -- (375.35,424.2)(425.35,414.2) -- (425.35,424.2)(475.35,414.2) -- (475.35,424.2)(525.35,414.2) -- (525.35,424.2)(170.35,369.2) -- (180.35,369.2)(170.35,319.2) -- (180.35,319.2)(170.35,269.2) -- (180.35,269.2)(170.35,219.2) -- (180.35,219.2)(170.35,169.2) -- (180.35,169.2)(170.35,119.2) -- (180.35,119.2)(170.35,69.2) -- (180.35,69.2) ;
		\draw   ;
		%Straight Lines [id:da671642760181361] 
		\draw [line width=1.5]    (145.5,63) -- (352.5,470) ;
		%Straight Lines [id:da017696331895517003] 
		\draw [line width=1.5]    (131.5,187) -- (503.5,470) ;
		%Shape: Polygon [id:ds4840080384946932] 
		\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][fill={rgb, 255:red, 255; green, 211; blue, 217 }  ,fill opacity=1 ][line width=2.25]  (326.5,419) -- (175.35,419.2) -- (175.5,220) -- (258.5,284) -- cycle ;
		%Straight Lines [id:da7064072905898551] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (81.5,339) -- (282.5,507) ;
		%Straight Lines [id:da929498995764177] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]  [dash pattern={on 5.63pt off 4.5pt}]  (100.5,230) -- (415.5,493) ;
		%Shape: Circle [id:dp6135399351697868] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (319.5,419) .. controls (319.5,415.13) and (322.63,412) .. (326.5,412) .. controls (330.37,412) and (333.5,415.13) .. (333.5,419) .. controls (333.5,422.87) and (330.37,426) .. (326.5,426) .. controls (322.63,426) and (319.5,422.87) .. (319.5,419) -- cycle ;
		%Shape: Circle [id:dp14002598025326662] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 248; green, 231; blue, 28 }  ,fill opacity=1 ] (168.35,419.2) .. controls (168.35,415.33) and (171.48,412.2) .. (175.35,412.2) .. controls (179.22,412.2) and (182.35,415.33) .. (182.35,419.2) .. controls (182.35,423.07) and (179.22,426.2) .. (175.35,426.2) .. controls (171.48,426.2) and (168.35,423.07) .. (168.35,419.2) -- cycle ;
		%Straight Lines [id:da7783133748875801] 
		\draw [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ]   (187,412) -- (307.5,412) ;
		\draw [shift={(309.5,412)}, rotate = 180] [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		
		% Text Node
		\draw (150,30.4) node [anchor=north west][inner sep=0.75pt]    {$x_{2}$};
		% Text Node
		\draw (568,429.4) node [anchor=north west][inner sep=0.75pt]    {$x_{1}$};
		% Text Node
		\draw (145,360.4) node [anchor=north west][inner sep=0.75pt]    {$10$};
		% Text Node
		\draw (145,310.75) node [anchor=north west][inner sep=0.75pt]    {$20$};
		% Text Node
		\draw (145,261.08) node [anchor=north west][inner sep=0.75pt]    {$30$};
		% Text Node
		\draw (145,211.41) node [anchor=north west][inner sep=0.75pt]    {$40$};
		% Text Node
		\draw (145,161.74) node [anchor=north west][inner sep=0.75pt]    {$50$};
		% Text Node
		\draw (145,112.07) node [anchor=north west][inner sep=0.75pt]    {$60$};
		% Text Node
		\draw (145,62.4) node [anchor=north west][inner sep=0.75pt]    {$70$};
		% Text Node
		\draw (214,429.08) node [anchor=north west][inner sep=0.75pt]    {$10$};
		% Text Node
		\draw (264.5,429.08) node [anchor=north west][inner sep=0.75pt]    {$20$};
		% Text Node
		\draw (315,429.08) node [anchor=north west][inner sep=0.75pt]    {$30$};
		% Text Node
		\draw (365.5,429.08) node [anchor=north west][inner sep=0.75pt]    {$40$};
		% Text Node
		\draw (416,429.08) node [anchor=north west][inner sep=0.75pt]    {$50$};
		% Text Node
		\draw (466.5,429.08) node [anchor=north west][inner sep=0.75pt]    {$60$};
		% Text Node
		\draw (517,429.08) node [anchor=north west][inner sep=0.75pt]    {$70$};
		% Text Node
		\draw (213,161.4) node [anchor=north west][inner sep=0.75pt]    {$6x_{1} +3x_{2} =180$};
		% Text Node
		\draw (460,477.4) node [anchor=north west][inner sep=0.75pt]    {$3x_{1} +4x_{2} =160$};
		% Text Node
		\draw (191,322) node [anchor=north west][inner sep=0.75pt]   [align=left] {Area of\\admissible\\solutions};
		% Text Node
		\draw (155,425) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
		% Text Node
		\draw (152,507.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,opacity=1 ]  {$1200x_{1} +1000x_{2} =0$};
		% Text Node
		\draw (179,398) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize,color={rgb, 255:red, 126; green, 211; blue, 33 }  ,opacity=1 ] [align=left] {direction and pivot step};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[]{Direction of the pivot head and arrival at point $(30, 0)$}
	\end{figure}
	To continue to increase $Z$ such simply (by increasing only one variable), we need a new system of equations similar to the original system:
	
	where we have expressed the variables that takes non-zero value depending on others who take a zero value! That is to say $x_3,x_4$, in function of $x_1,x_2$ since we had for recall:
	
	For the remaining part, we must express $x_1,x_3$ and also $Z$ in function of $x_2,x_4$ since we just get for reminder:
	
	Before getting the new system reaction function, making some algebraic manipulations:
	
	which gives after simplification:
	
	and therefore it comes:
	
	which gives after simplification:
	
	and we have identically:
	
	So finally the system is:
	
	from which we reiterate the process (we increase only one  variable in $Z$ keeping the other to $0$). When we can not increase $Z$ as all coefficients are negative, well it is that we are at a maximum (thank you convexity...). Let us see this...
	
	In $Z$ the biggest coefficient is now $x_2$ and so it leads us to put $x_4=0$. The most constraint value of $x_2$ that allows to always respect the constraints $x_1,x_2,x_3,x_4\geq 0$ is therefore:
	
	And for this value, we have:
	
	The original economic function then takes the value:
	
	which therefore corresponds graphically to:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1017); %set diagram left start at 0, and has height of 1017
		
		%Shape: Axis 2D [id:dp7818961550684664] 
		\draw  (130,419.2) -- (583.5,419.2)(175.35,43) -- (175.35,461) (576.5,414.2) -- (583.5,419.2) -- (576.5,424.2) (170.35,50) -- (175.35,43) -- (180.35,50) (225.35,414.2) -- (225.35,424.2)(275.35,414.2) -- (275.35,424.2)(325.35,414.2) -- (325.35,424.2)(375.35,414.2) -- (375.35,424.2)(425.35,414.2) -- (425.35,424.2)(475.35,414.2) -- (475.35,424.2)(525.35,414.2) -- (525.35,424.2)(170.35,369.2) -- (180.35,369.2)(170.35,319.2) -- (180.35,319.2)(170.35,269.2) -- (180.35,269.2)(170.35,219.2) -- (180.35,219.2)(170.35,169.2) -- (180.35,169.2)(170.35,119.2) -- (180.35,119.2)(170.35,69.2) -- (180.35,69.2) ;
		\draw   ;
		%Straight Lines [id:da671642760181361] 
		\draw [line width=1.5]    (145.5,63) -- (352.5,470) ;
		%Straight Lines [id:da017696331895517003] 
		\draw [line width=1.5]    (131.5,187) -- (503.5,470) ;
		%Shape: Polygon [id:ds4840080384946932] 
		\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][fill={rgb, 255:red, 255; green, 211; blue, 217 }  ,fill opacity=1 ][line width=2.25]  (326.5,419) -- (175.35,419.2) -- (175.5,220) -- (258.5,284) -- cycle ;
		%Straight Lines [id:da7064072905898551] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (81.5,339) -- (282.5,507) ;
		%Straight Lines [id:da929498995764177] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]  [dash pattern={on 5.63pt off 4.5pt}]  (100.5,230) -- (415.5,493) ;
		%Shape: Circle [id:dp6135399351697868] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (319.5,419) .. controls (319.5,415.13) and (322.63,412) .. (326.5,412) .. controls (330.37,412) and (333.5,415.13) .. (333.5,419) .. controls (333.5,422.87) and (330.37,426) .. (326.5,426) .. controls (322.63,426) and (319.5,422.87) .. (319.5,419) -- cycle ;
		%Shape: Circle [id:dp14002598025326662] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 248; green, 231; blue, 28 }  ,fill opacity=1 ] (168.35,419.2) .. controls (168.35,415.33) and (171.48,412.2) .. (175.35,412.2) .. controls (179.22,412.2) and (182.35,415.33) .. (182.35,419.2) .. controls (182.35,423.07) and (179.22,426.2) .. (175.35,426.2) .. controls (171.48,426.2) and (168.35,423.07) .. (168.35,419.2) -- cycle ;
		%Straight Lines [id:da7783133748875801] 
		\draw [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ]   (330,406) -- (270.38,283.8) ;
		\draw [shift={(269.5,282)}, rotate = 63.99] [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Circle [id:dp7917496317824995] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (247.5,281) .. controls (247.5,277.13) and (250.63,274) .. (254.5,274) .. controls (258.37,274) and (261.5,277.13) .. (261.5,281) .. controls (261.5,284.87) and (258.37,288) .. (254.5,288) .. controls (250.63,288) and (247.5,284.87) .. (247.5,281) -- cycle ;
		%Straight Lines [id:da5414268704346001] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (165.5,281) -- (254.5,281) ;
		%Straight Lines [id:da7604132613935108] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (254.5,281) -- (254.5,431) ;
		
		% Text Node
		\draw (150,30.4) node [anchor=north west][inner sep=0.75pt]    {$x_{2}$};
		% Text Node
		\draw (568,429.4) node [anchor=north west][inner sep=0.75pt]    {$x_{1}$};
		% Text Node
		\draw (145,360.4) node [anchor=north west][inner sep=0.75pt]    {$10$};
		% Text Node
		\draw (145,310.75) node [anchor=north west][inner sep=0.75pt]    {$20$};
		% Text Node
		\draw (145,261.08) node [anchor=north west][inner sep=0.75pt]    {$30$};
		% Text Node
		\draw (145,211.41) node [anchor=north west][inner sep=0.75pt]    {$40$};
		% Text Node
		\draw (145,161.74) node [anchor=north west][inner sep=0.75pt]    {$50$};
		% Text Node
		\draw (145,112.07) node [anchor=north west][inner sep=0.75pt]    {$60$};
		% Text Node
		\draw (145,62.4) node [anchor=north west][inner sep=0.75pt]    {$70$};
		% Text Node
		\draw (214,429.08) node [anchor=north west][inner sep=0.75pt]    {$10$};
		% Text Node
		\draw (264.5,429.08) node [anchor=north west][inner sep=0.75pt]    {$20$};
		% Text Node
		\draw (315,429.08) node [anchor=north west][inner sep=0.75pt]    {$30$};
		% Text Node
		\draw (365.5,429.08) node [anchor=north west][inner sep=0.75pt]    {$40$};
		% Text Node
		\draw (416,429.08) node [anchor=north west][inner sep=0.75pt]    {$50$};
		% Text Node
		\draw (466.5,429.08) node [anchor=north west][inner sep=0.75pt]    {$60$};
		% Text Node
		\draw (517,429.08) node [anchor=north west][inner sep=0.75pt]    {$70$};
		% Text Node
		\draw (213,161.4) node [anchor=north west][inner sep=0.75pt]    {$6x_{1} +3x_{2} =180$};
		% Text Node
		\draw (460,477.4) node [anchor=north west][inner sep=0.75pt]    {$3x_{1} +4x_{2} =160$};
		% Text Node
		\draw (191,322) node [anchor=north west][inner sep=0.75pt]   [align=left] {Area of\\admissible\\solutions};
		% Text Node
		\draw (155,425) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
		% Text Node
		\draw (152,507.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,opacity=1 ]  {$1200x_{1} +1000x_{2} =0$};
		% Text Node
		\draw (296.57,293.25) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize,color={rgb, 255:red, 126; green, 211; blue, 33 }  ,opacity=1 ,rotate=-64.61] [align=left] {direction and pivot step};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[]{Pivot direction with arrival point at $(16, 28)$ for the second iteration}
	\end{figure}
	So we see above that we arrived at the optimum value visible on the graph given at the beginning of this example. But how do we know that we arrived at the final point if we do not have plots or if we work in higher dimensions?
	
	In fact, the process is terminated either when all the coefficients of the economic function are negative or that the most constraint value that respects the constraints is equal to zero !!! Let's see if this is the case! We therefore have in our example:
	
	So we will rewrite the system:
	
	with this time $x_1,x_2$ and $Z$ dependant to $x_3,x_4$. We then have first:
	
	and we have:
	
	Therefore for $Z$ we have (the coefficients are all negative so we guess what comes ...):
	
	We then have the new system:
	
	As all coefficients of $Z$ are now negative, we're blocked because we would go in the wrong direction if we continue. We must therefore stop here and we adopt finally the solution:
	
	The method of resolution using tables that is often presented in the literature is only useful to write the coefficients of the variables of the system a table, but the changes that we made are exactly those we just made algebraically before (but a the opposite of the tables the method we used don't hide the logic of the method).
	
	\paragraph{Simplex algorithm LP resolution}\mbox{}\\\\
	To implement the  simplex algorithm, we must write the problem in a "standard" form and introduce the concept of "base program" that is the algebraic expression corresponding to the notion of "extreme point of the polyhedron of eligible programs" presented earlier above. Indeed, we will see that the solution of a problem of the linear programming type it exists, can still be obtained with a base program. The simplex method will therefore be to find a first base program and to build a following base programs constantly improving the economic function and thus leading to the optimum (this is what we name "dynamic programming").
	
	An LP problem is said to be placed in its "\NewTerm{standard form}\index{standard form}" if it involves the search for the minimum of the objective function, the latter being subject to constraints in the form of linear equations and conditions of non-negativity of the variables, that is, we can write it in the form seen earlier before:
	
	Or using matrix notation:
	
	where the matrices $C:n\times 1, A:m\times n,b:m \times 1$ respectively correspond to the activity coefficients of objective function, to the technical coefficients of activities and to second members of the constraints.

	We will now see how a LP general problem can always be reduced to a standard form. As we guess it the concept of "slack variable" will be essential to perform this "reduction".

	Find the maximum of a function $f (x)$ is equivalent to find the minimum of the opposite sign function $f (x)$. Moreover, a constraint which is presented as an inequality:
	
	can be replaced by the following system:
	
	where as we already know $s_i$ is the slack variable constraint such that $s_i\geq 0$.
	
	Of course, if the system is such that:
	
	can be replaced by the system:
	
	implying again to add a slack variable  and always with the constraint that $e_i\geq 0$.

	This work of putting in standard form work us to find a system of linear equations to solve system (we saw previously at the beginning of this section how to solve this kind of system with the pivot algorithm).

	The matrix $A$ representing the components of the system of equations can be, as we know, expressed in different ways depending on the chosen vector basis (\SeeChapter{see section Vector Calculus page \pageref{vector basis}}). We will introduce now the concept of "\NewTerm{canonical associated usable form}\index{canonical associated usable form}" by choosing a special base and show that this reformulation of constraint system will enable us to move towards the optimum.
	
	The matrix $A$ can, after introduction of the slack variables be decomposed into two sub-matrices $[D|B]$, one containing the initial variables $D$ and the other with the slack variables $B$ such that:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The slack variables are... variable and not constant!! In a system where the variables are in quantity $n$ and the equations in quantity $m$ written as:
	
	We add a slack variable to change the latter into:
	
	where $x_{n+1}=e_i$ on each row $m$. The added slack variable being different of all variables already existing in the system! This is why we can decompose the initial matrix in two sub matrices.
	\end{tcolorbox}
	The columns of the matrix $B$ are obviously, by definition of the method, units columns, linearly independent. These columns form a basis of the vector space of columns of $m$ elements (or dimensions) - the number of  lines of the system. We name $B$ the "\NewTerm{base matrix}\index{base matrix}".
	
	Right now it can seem a little bit confusing. So let us continue the theory with a companion example as we have for habit to do it in this book.
	
	The text below describing the simplex algorithm has been taken to Marcel Oliver (April 12, 12012 according to holocene calendar) and formalized by our own work.
	\begin{enumerate}
	\item Step 1: Write the linear programming problem in standard form
	
	Turning a problem into standard form involves the following steps.
	\begin{enumerate}
		\item Turn Maximization into minimization and write inequalities in
		standard order.
		
		This step is obvious.  Multiply expressions, where appropriate, by	$-1$.
		
		\item Introduce slack variables to turn inequality constraints into equality constraints with non-negative unknowns.
		
		Any inequality of the form
		
		can be replaced by:
		  
		with $s \geq 0$.
		
		\item Replace variables which are not sign-constrained by differences.
		
		Any real number $x$ can be written as the difference of non-negative
		numbers $x=u-v$ with $u,v\geq 0$.
	\end{enumerate}

	Consider the following example.
	
	subject to:
	
	Written in standard form, the problem becomes:
	
	subject to:
	

	\item Step 2: Write the coefficients of the problem into a "\NewTerm{simplex tableau}\index{simplex tableau}".
	
	The coefficients of the linear system are collected in an augmented	matrix as known from Gaussian elimination for systems of linear equations; the coefficients of the objective function are written in a separate bottom row with a zero in the right hand column.
	
	For our example, the initial tableau reads:
	\newcolumntype{B}{%
	  >{\columncolor[gray]{.8}[.5\tabcolsep]}c}
	\begin{center}
	\begin{tabular}{BcccBBB|c}
	  $x_1$ & $x_2$ & $u$ & $v$ & $s_1$ & $s_2$ & $s_3$ \\
	  \hline
	  $1$ & $1$ & $-1$ & $1$ & $0$ & $0$ & $0$ & $1$ \\
	  $2$ & $-1$ & $-2$ & $2$ & $1$ & $0$ & $0$ & $5$ \\
	  $1$ & $-1$ & $0$ & $0$ & $0$ & $1$ & $0$ & $4$ \\
	  $0$ & $1$ & $1$ & $-1$ & $0$ & $0$ & $1$ & $5$ \\
	  \hline
	  $-1$ & $-2$ & $-3$ & $3$ & $0$ & $0$ & $0$ & $0$ 
	\end{tabular}
	\end{center}
	So following what we have above, we get for:
	
	that:
	
	The variables associated to the column components of the matrix $S$ will now be named "\NewTerm{bases variables}\index{bases variables}". In our case the bases variables are then essentially the slack variables that we will write now $x_{n+1},x_{n+2},\ldots,x_{n+m}$. The variables associates to the column of the matrix $X$ are named "\NewTerm{off-base variables}\index{off-base variables}", these are the variables $x_1,x_2,\ldots,x_n$.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Let us recall that in the expression of the economic function, only the off-base variables appear.
	\end{tcolorbox}
	In the following steps, we will act on the tableau by the rules of Gaussian elimination, where the pivots are always chosen from the columns corresponding to the bases variables.
	
	Before proceeding, we need to choose an initial set of basic variables	which corresponds to a point in the feasible region of the linear programming problem.  Such a choice may be non-obvious, but we shall defer this discussion for now.  In our example, $x_1$ and $s_1, \dots, s_3$ shall be chosen as the initial bases variables, indicated by gray columns in the tableau above.
	
	\item Step 3: Gaussian elimination

	For a given set of basic variables, we use Gaussian elimination (see page \pageref{Gaussian elimination}) to reduce the corresponding columns to a permutation of the identity matrix.  This amounts to solving $A\vec{x}=\vec{b}$ in such a way that the values of the non-basic variables are zero and the values for the basic variables are explicitly given by the entries in the right hand column of the fully reduced matrix. In addition, we eliminate the 	coefficients of the objective function below each pivot.
	
	Our initial tableau is thus reduced to:
	\begin{center}
	\begin{tabular}{BcccBBB|c}
	  $x_1$ & $x_2$ & $\pmb{u}$ & $v$ & $s_1$ & $s_2$ & $s_3$ \\
	  \hline
	  $1$ & $1$ & $\pmb{-1}$ & $1$ & $0$ & $0$ & $0$ & $1$ \\
	  $0$ & $-3$ & $\pmb{0}$ & $0$ & $1$ & $0$ & $0$ & $3$ \\
	  \textit{0} & \textit{-2} & \textbf{\textit{1}} & 
	   $-$ \textit{1} & \textit{0} & \textit{1} & \textit{0} & \textit{3} \\
	  $0$ & $1$ & $\pmb{1}$ & $-1$ & $0$ & $0$ & $1$ & $5$ \\
	  \hline
	  $0$ & $-1$ & $\pmb{-4}$ & $4$ & $0$ & $0$ & $0$ & $1$ 
	\end{tabular}
	\end{center}
	The solution expressed by the tableau is only admissible if all basic variables are non-negative, i.e., if the right hand column of the reduced tableau is free of negative entries.  This is the case in this example.  At the initial stage, however, negative entries may come up; this indicates that different initial basic variables should have been chosen.  At later stages in the process, the selection rules for the	basic variables will guarantee that an initially feasible tableau will remain feasible throughout the process.

	\item Step 4: Choose new basic variables

	If, at this stage, the objective function row has at least one negative entry, the cost can be lowered by making the corresponding variable basic.  This new basic variable is named the "\NewTerm{entering variable}\index{entering variable}". Correspondingly, one formerly basic variable has then to become non-basic, this variable is named the "\NewTerm{leaving variable}\index{leaving variable}". We use the following standard selection rules.

	\begin{enumerate}
		\item The entering variable shall correspond to the column which has the most negative entry in the cost function row.  If all 	cost function coefficients are non-negative, the cost cannot be lowered and we have reached an optimum. The algorithm then terminates.
		
		\item Once the entering variable is determined, the leaving variable shall be chosen as follows.  Compute for each row the ratio of its right hand coefficient to the corresponding coefficient in the entering variable column. Select the row with the smallest finite positive ratio.  The leaving variable is then determined by the column which currently owns the pivot in this row.  If all coefficients in the entering variable column are non-positive, the cost can be lowered indefinitely, i.e., the linear programming problem does not have a finite solution. The algorithm then also terminates.
	\end{enumerate}
	If entering and leaving variable can be found, go to Step~3 and	iterate.
	
	Note that choosing the most negative coefficient in rule (i) is only a heuristic for choosing a direction of fast decrease of the objective function.  Rule (ii) ensures that the new set of basic variables remains feasible.  
	
	Let us see how this applies to our problem.  The previous tableau holds the most negative cost function coefficient in column $3$, thus $u$ shall be the entering variable (marked in boldface).  The smallest positive ratio of right hand column to entering variable column is in row $3$, as $\tfrac31<\tfrac51$.  The pivot in this row	points to $s_2$ as the leaving variable.  Thus, after going through the Gaussian elimination once more, we arrive at:
	\begin{center}
	\begin{tabular}{BcBcBcB|c}
	  $x_1$ & $\pmb{x_2}$ & $u$ & $v$ & $s_1$ & $s_2$ & $s_3$ \\
	  \hline
	  $1$ & $-$\textbf{1} & $0$ & $0$ & $0$ & $1$ & $0$ & $4$ \\
	  $0$ & $-$\textbf{3} & $0$ & $0$ & $1$ & $0$ & $0$ & $3$ \\
	  $0$ & $-$\textbf{2} & $1$ & $-1$ & $0$ & $1$ & $0$ & $3$ \\
	  \textit{0} & \textit{\textbf{3}} & \textit{\textbf{0}} & \textit{0} 
	  & \textit{0} & $-$\textit{1} & \textit{1} & \textit{2} \\
	  \hline
	  $0$ & $-$\textbf{9} & $0$ & $0$ & $0$ & $4$ & $0$ & $13$ 
	\end{tabular}
	\end{center}
	At this point, the new entering variable is $x_2$ corresponding to the 	only negative entry in the last row, the leaving variable is $s_3$. After Gaussian elimination, we find:
	\begin{center}
	\begin{tabular}{BBBcBcc|c}
	  $x_1$ & $x_2$ & $u$ & $v$ & $s_1$ & $s_2$ & $s_3$ \\
	  \hline
	  $1$ & $0$ & $0$ & $0$ & $0$ & $\tfrac23$ & $\tfrac13$ & $\tfrac{14}3$ \\
	  $0$ & $0$ & $0$ & $0$ & $1$ & $-1$ & $1$ & $5$ \\
	  $0$ & $0$ & $1$ & $-1$ & $0$ & $\tfrac13$ & $\tfrac23$ & $\tfrac{13}3$ \\
	  $0$ & $1$ & $0$ & $0$ & $0$ & $-\tfrac13$ & $\tfrac13$ & $\tfrac23$ \\
	  \hline
	  $0$ & $0$ & $0$ & $0$ & $0$ & $1$ & $3$ & $19$ 
	\end{tabular}
	\end{center}
	Since there is no more negative entry in the last row, the cost cannot
	be lowered by choosing a different set of basic variables; the
	termination condition applies.

	\item Step 5: Read off the solution

	The solution represented by the final tableau has all non-basic
	variables set to zero, while the values for the basic variables can be
	can be read off the right hand column.  The bottom right corner gives
	the negative of the objective function.
	
	In our example, the solution reads $x_1=\tfrac{14}3$, $x_2=\tfrac23$,
	$x_3=u=\tfrac{13}3$, $s_1=5$, $v=s_2=s_3=0$, which corresponds to
	$\zeta=-19$, which can be independently checked by plugging the
	solution back into the objective function.
	
	As a further check, we note that the solution must satisfy the initial equation and inequations.  This can obviously be checked by direct computation.
	\end{enumerate}
	In summary, any LP once put in standard form is such that:
	\begin{itemize}
		\item There is a square sub-matrix of matrix $A$, which is named the "base matrix" and is equal to the square unit matrix $\mathds{1}$ of size $m$ (indeed there are as many slack variables that lines in the original equations - at the number of $m$ - and as many columns as each slack variable has a different index).

		\item The basic variables involved does not appear in the expression of the economic function.

		\item The second member of the constraints consists of non-negative values.
	\end{itemize}
	We say then that the problem is put under a "a canonical form associated with the basis $B$, corresponding to the basis variables $x_{n+1},x_{n+2},\ldots,x_{n+m}$".
	
	\subsubsection{Nonlinear programming (Nonlinear optimization)}\label{nonlinear optimization}
	Nonlinear programming is the process of solving an optimization problem defined by a system of equalities and inequalities, collectively termed constraints, over a set of unknown real variables, along with an objective function to be maximized or minimized, where some of the constraints or the objective function are non-linear. 
	
	A non-linear optimization program (NLOP) is a generalization of linear programming (simplex algorithm) but about non-linear functions and can also include non-linear constraints and non-linear economic functions.

	The purpose of what follows is to understand in outline but with an acceptable level of rigour the optimization tools that offer many spreadsheets softwares like the previous versions of Microsoft Excel to the version 2007 (since the version 2007 we cannot make a fine-tuning of these options anymore):
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/excel_solver_nonlinear_optimization.jpg}
		\caption{Microsoft Excel 2003 Solver options}
	\end{figure}
	 We will especially see now in what consist the \textit{Newton} Search (meaning implicitly: "Gauss-Newton's method") with the \textit{Tangent} and \textit{Quadratic} estimates. After which we will study also the Conjugate Gradients Search also with the tangent and quadratic methods respectively.
	 \begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	We will stop at the study of the above cited models because there is an excessive quantity of empirical models such as for example the best known models (algorithms): substitution method, method of Lagrange multipliers, Nelder-Mead algorithm, Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm, algorithm  of simultaneous annihilation (SA), methods of interior points... and see Wikipedia for a more complete list (there are over a dozen of methods without taking into account the variations including empirical adjustments).
	\end{tcolorbox}
	We will see it further below, but we already guess that the choice \textit{Tangent} use a linear approximation (tangent) of the function to be optimized at the point considered when at the opposite the \textit{Quadratic} option will make an estimation of a function of the second degree at the considered point (typically a parabola). If at the considered point, the function is well modelled by a quadric, then the \textit{Quadratic} option can save time by choosing a better starting point that will require fewer steps on each additional research. If you have no idea of the behaviour of the a priori function, then the \textit{Tangent} option is slower but safer.
	
	A well known example in the literature to introduce the search of optimums of non-linear functions, before moving on to the part taking into account constraints on the system, is the "humpback whale function" of that consist to find the minimum of:
	
	With the range constraints:
	
	what we can indeed check visually:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1.4]{img/computing/hump.pdf}
		\caption[Plot of the humpback whale function with minima already visible]{Plot of the humpback whale function with minima already visible\\ made with TikZ/pgfplot/gnulplot (authors: Vincent Isoz, Jasper Habicht)}
	\end{figure}
	Either with Maple 4.00b:
	
	\texttt{>plot3d(x\string^2*(4-2.1*x\string^2+1/3*x\string^4)+x*y+y\string^2*(-4+4*y\string^2),\\
 x=-2..2,y=-1..1,contours=20,style=patchcontour,axes=boxed);}

	That gives:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.65]{img/computing/humpback_whale_function_maple.jpg}
	\end{figure}

	As we can see, this function is a great example of multiple local minimum but there is also a must more vicious one that we will refer to when we will study evolutionary algorithms, the "Rastgrini's function":
	
	\texttt{>plot3d(20+x\string^2+y\string^2-10*(cos(2*Pi*x)+cos(2*Pi*y)),\\ 	x=-5..5,y=-5..5,contours=20,style=patch,axes=boxed,numpoints=10000);}
	
	That gives:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.65]{img/computing/rastgrini_function_maple.jpg}
	\end{figure}
	
	\paragraph{Substitution Method}\mbox{}\\\\
	The least complex method for solving a non-linear programming problem is named the "\NewTerm{substitution method}\index{substitution method}".

	This method is restricted to models containing a single constraint and must be in addition to the equality type.

	Let us consider a companion example by considering the following economic function to maximize:
	
 	with the constraint (remember that it must be an equality !!!):
	
 	The first step is then to arbitrarily substitute:
	
 	into the economic function to get:
	
 	This therefore brings us back to a function to be maximized which is unconstrained, so we can differentiate it and put it as zero. Then it comes:
	
	that gives:
	
	From this we can deduce immediately that:
	
 	By injecting those two values in the economic function, we then have:
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}
		    \begin{axis}[   width=300pt,
		                    height=300pt,
		                    axis background/.style={fill=white},
		                    xtick scale label code/.code={},
		                    ytick scale label code/.code={},
		                    ztick scale label code/.code={},
		                    clip marker paths=true,
		                    domain=-40:40,
		                    view={-70}{10}
		                ]
		        \addplot3 [surf, opacity=.3, samples=50] {4*x-0.1*x^2+5*y-0.2*y^2};
		        \addplot3 [domain=-30:30, samples y=0, samples=300] ({
		                x
		            },{
		                (40-x)/2
		            },{
		                4*x-0.1*x^2+5*(40-x)/2-0.2*((40-x)/2)^2
		            });
		        \addplot3[mark=*,red,point meta=explicit symbolic,nodes near coords] 
		coordinates {(18.3,10.8,4*18.3-0.1*18.3^2+5*10.8-0.2*10.8^2)[$(18.3,10.8,5)$]};
		    \end{axis}
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[]{Substitution method example plotted}
	\end{figure}
	We see by this example very quickly the limitations of this technique. The first being that the economic function was not too complex, the final equation to solve was therefore not a problem, the second being that there were only two variables, the third being that the constraint must be an equality as we have already mentioned.
	
	
	\paragraph{Lagrange Multipliers Method}\mbox{}\\\\	
	The "\NewTerm{Lagrange multipliers method}\index{Lagrange multipliers method}\label{Lagrange multipliers method}" is a rather general technique for solving non-linear programming problems with one or more constraints with linear or non linear economic function with inequalities (rather than strict equations only) and with more than two variables (for other examples than those presented here the reader can go to the corresponding page of Wikipedia).

	We will begin to introduce this technique with a simple case which consists of taking again the example used during our study of the method of substitution:
	
	The first step is to write the constraint function in Lagrangian form as we do in Analytical Mechanics (\SeeChapter{see section  Analytical Mechanics page \pageref{euler lagrange}}) at the difference that there are no general variables depending on the time:
	
	For this, we write first:
	
	Then the idea is that since this expression is null, nothing prevents us from summing it or subtracting it from the economic function with why not an empirical multiplier that we will denote $\lambda$ and which we will name the "\NewTerm{Lagrange multiplier}".

	It comes then if we choose to subtract for example (in fact the choice of the subtraction is made by anticipation of an interpretation of the Lagrange multiplier that we will see immediately after):
	
	Which gives us our Lagrangian function. In generic form the latter is often written as following:
	
 	Now the idea is to determine the values of the variables where the partial derivatives of the Lagrangian with respect to the variables vanish at the same time (corresponding in Analytical Mechanics as the sum of all the Lagrangians relatively to one variable to be equal to zero):
	
 	What is generally written as:
	
	We therefore have a system of three equations with three unknowns which we know trivially how to solve (\SeeChapter{see section Linear Algebra page \pageref{linear systems}}). We then get as solutions:
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}
		    \begin{axis}[   width=300pt,
		                    height=300pt,
		                    axis background/.style={fill=white},
		                    xtick scale label code/.code={},
		                    ytick scale label code/.code={},
		                    ztick scale label code/.code={},
		                    clip marker paths=true,
		                    domain=-40:40,
		                    view={-70}{10}
		                ]
		        \addplot3 [surf, opacity=.3, samples=50] {4*x-0.1*x^2+5*y-0.2*y^2};
		        \addplot3 [domain=-30:30, samples y=0, samples=300] ({
		                x
		            },{
		                (40-x)/2
		            },{
		                4*x-0.1*x^2+5*(40-x)/2-0.2*((40-x)/2)^2
		            });
		        \addplot3[mark=*,red,point meta=explicit symbolic,nodes near coords] 
		coordinates {(18.3,10.8,4*18.3-0.1*18.3^2+5*10.8-0.2*10.8^2)[$(18.3,10.8,5)$]};
		    \end{axis}
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[]{Lagrange multiplier example plotted}
	\end{figure}
 	Notice that if we had not taken into consideration $\lambda$ and therefore that implicitly that latter had been equal to $1$ since the beginning, we would have had the following system of equations:
	
	and therefore we would not have obtained the results of the substitution method seen previously ... hence the multiplicative factor!

	The final value of $Z$ is then the same as for the substitution method taking into account $\lambda$!!!

	Although the Lagrange multiplier method is powerful, its increasing complexity with a large number of variables makes it a difficult tool to manipulate in practice.

	We will now focus on the interpretation of $\lambda$. As we shall see, the latter represents the local variation rate per unit of positive variation of the constant of the constraint function. Thus, if the constraint function becomes (we have changed the $40$ into a $41$):
	
	By doing again the same calculations as before, we then get:
	
 	Therefore, by having increased of one unit the constant of the constrained function, we have:
	
 	In general, if the Lagrange multiplier is positive, then the economic function will increase if the constant of the constraint is also incremented positively and vice versa.

   \label{lagrange multiplier method for markowitz portfolio}Let us now consider a case much more elaborated and useful for our study of Economics (especial Portfolio Optimization). We want to solve by the method of the Lagrange multipliers the Markowitz portfolio problem (\SeeChapter{see section Economy page \pageref{markowitz overall minimum variance portfolio}}) but without the risk free asset (we will do that later case in the section of Economy!):
	
	with for recall:
	
	To facilitate the developments that will follow we will write this system with the following quite ugly traditional notation (for the details on the notations see the section Economy):
	
	The Lagrangian function will therefore be written by adding the three constraints above:
	 
	Therefore:
	 
 	Now we calculate:
	 
	This gives us the system of three equations:
	  
	By rearranging the first equation:
	 
 	We have:
	 
 	Let us now take the two equations:
	
	Thus in an equivalent way:
	
	By injecting in it the explicit relation of the weights of the portfolio, we have:
	
	We can put this system in matrix form:
	
	Which gives us:
	
	Therefore:
	
	So once we have the values of these two Lagrange multipliers, we just have to inject them into:
	
 	to get the weights that minimize the portfolio variance:
	
	So the reader will see during our study of Modern Portfolio management that getting the optimal weights (see page \pageref{markowitz overall minimum variance portfolio}) is much more easy using the Lagrange multiplier method (and less time consuming) than using a software optimizer!
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	By the way the attentive reader will perhaps have noticed that finally making all the development with the factor $1/2$ is useless since during the final substitution, the latter is neutralized with itself since $2$ is multiplied by $1/2$.
	\end{tcolorbox}
	
	\pagebreak
	\paragraph{Newton-Raphson Method (Quadratic Newton)}\label{quadratic programming}\mbox{}\\\\
	The "\NewTerm{Newton-Raphson method}\index{Newton-Raphson method}\label{newton raphson method}", also named "\NewTerm{Newton's method for unconstrained optimization}", is a technique for searching the extremum of a function or also, as we will see it when we will compare with a special example the difference between the Gauss-Newton's method with that of Newton, for non-linear regression.

	The Newton-Raphson, who in earlier versions to Microsoft Excel 2007 was activated in the solver by selecting Newton and Quadratic option uses the second order Taylor approximation (i.e. with second order derivatives) to have a quadratic function (parabola) which converges if the origin point of the research is close to the optimum. This approximation is repeated to each iteration.

	To start the formal approach let us recall that we have proved in the section Sequences and Series that a Taylor expansion for a function of two variables could be written in quadratic approximation by:
	
	where for recall $h$ and $k$ are variables and $x_0,y_0$ are fixed and where we have the Hessian matrix (\SeeChapter{see section Sequences and Series page \pageref{hessian matrix}}):
	
	that USA experts in the field have the habit (unfortunate in my opinion ...) to denote:
	
	the latter expression being the most common can be very misleading with the notation of the Laplacian...

	In the field of numerical methods it is customary to write the Taylor series above with few notations changes by putting first:
	
	This gives us a more condensed and technical form of the Taylor series around $\vec{x}$:
	
	By changing again a little bit the notations:
	
	We thus fall back on the usual expression of a function of $\mathbb{R}^2\rightarrow \mathbb{R}$ evaluated in Taylor series centered on $\vec{x}$.

	But if we seek for a local extrema (also sometimes named "\NewTerm{critical point}\index{critical point}"), we will need in first time that the derivative of the whole Taylor series be equal to zero. That is to say:
	
	and that the determinant of the Hessian matrix is positive (\SeeChapter{see section Sequences and Series page \pageref{hessian matrix}}). And to know if we are on a local maximum or local minimum, we must look at the sign of $\partial_x^2 f(\vec{x})$.

	Let us rewrite the above relation explicitly as we proved it in the section of Sequences and Series for pedagogical reasons:
	
	And let us recall that all terms $x_0,y_0$ are constants because it is either the function $f$ evaluated the particular point $(x_0,y_0)$, or the partial derivative evaluated at the same point, either the partial second derivative always evaluated at the same point, etc.

	So finally the gradient will give:
	
	and returning traditional notations in the field of numerical methods, we have then:
	
	And so as the gradient has to be equal zero, we have:
	
	and after a first rearrangement:
	
	and a second rearrangement:
	
	which is often written:
	
	and by USA specialists in the field:
	
	Finally, before moving on to a concrete example it is important that the reader remembers the relation just seen above:
	
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	We are seeking a local extremum of the "humpback whale" function shown earlier above:
	
	with the starting point (arbitrary):
	
	To do the search, we calculate the gradient:
	
	and the Hessian matrix:
	
	We then have:
	
	and:
	
	and:
	
	and therefore:
	
	and we start again (with less detail):
	
	and:
	
	Therefore:
	
	and once again (with again less details):
	
	and therefore:
	
	and again (with even less detail):
	
	and values will not move anymore. But if we look at the original graphic where we highlighted the convergence point by a red point:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.85]{img/computing/convergence_point_humpback_whale_example.jpg}
		\caption[]{Highlight of the convergence point in the humpback whale function}
	\end{figure}
	we see that this system does not search a global extremum but a local extremum as we already specified it. In fact, as the reader may test itself, convergence is very sensitive to initial starting point.
	\end{tcolorbox}
	Keep really in mind that this method (and also the next one) works well only for smooth convex optimization problem (obviously twice differentiable and the opposite of the next method that need to be only once differentiable):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/computing/optimization_paths_convex_nonconvex.jpg}
		\caption[]{Optimization paths with different starting points are illustrated in different colours. In the case of strictly convex function (Figure a.), the paths starting from any points all lead to the global optimum. Conversely, in the case of non-convex function, different paths may end up at different local optima!}
	\end{figure}
	
	Here is the corresponding pseudo-code:
	
	\begin{algorithm}[H]
	 \KwIn{$f: \mathbb{R}^n \rightarrow \mathbb{R}$ a twice-differentiable function}
	 $\quad\quad\quad \vec{x}_0\in\mathbb{R}^n$ an initial solution\\
	 $\quad\quad\quad\varepsilon$ is a tolerance value\\
	 \For{$k$ \KwTo maxiter}{
	  Compute $\vec\nabla f\left(\vec{x}_k\right)$\;
	  Compute $H^{-1}\left(\vec{x}_k\right)$\;
	  Update $\vec{x}_{k+1} \leftarrow \vec{x}_{k}-H^{-1}\left(\vec{x}_{k}\right) \vec\nabla f\left(\vec{x}_{k}\right)$\;
	  \uIf{$\left\|f\left(\vec{x}_{k+1}\right)-f\left(\vec{x}_{k}\right)\right\|<\varepsilon\; \textbf{or} \;\left\|\vec\nabla f\left(\vec x_{k}\right)\right\|<\varepsilon$}{
     		Converged. Print $\vec x^* \leftarrow \vec x_{k+1}$ and $f\left(\vec x^*\right) \leftarrow f\left(\vec x_{k+1}\right)$\;
	 	}
	 }
	 \caption{Newton-Raphson method (quadratic Newton)}
	\end{algorithm}
	
	\paragraph{Gauss-Newton method's (Tangent Newton)}\label{gradient descent}\mbox{}\\\\
	The Gauss-Newton's method is a powerful approximation without the derivatives of the second order of the Newton-Raphson method that in the prior versions of Microsoft Excel 2007 was activated in the solver by selecting the option \textit{Newton} and \textit{Tangent}.

	To study this method, let us do use a companion concrete example! Suppose we obtained the following data:
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|p{2cm}|p{2cm}|}
			\hline
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}$t_i$} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}$y_i$}  \\ \hline
			\centering\arraybackslash\ $1$ & \centering\arraybackslash\ $3.2939$ \\ \hline	
			\centering\arraybackslash\ $2$ & \centering\arraybackslash\ $4.2699$ \\ \hline	
			\centering\arraybackslash\ $4$ & \centering\arraybackslash\ $7.1749$ \\ \hline	
			\centering\arraybackslash\ $5$ & \centering\arraybackslash\ $9.3008$ \\ \hline	
			\centering\arraybackslash\ $8$ & \centering\arraybackslash\ $20.259$ \\ \hline	
		\end{tabular}
	\end{table}
	and we suppose "a priori" that the data follow the following theoretical model (we could also try any other function):
	 
	We look then for $x_1,x_2$ that minimize the sum of squares (the square of the euclidean distance - $L^2$ - that is obviously not the only choice of distance in practice!) between the experimental and theoretical values such that:
	
	with therefore:
	
	The previous relation is also sometimes (especially in the field of Machine Learning) denoted as following (but with a notation quite generalized):
	
	and named the "\NewTerm{cost function}\index{cost function}". The $1/2$ is here by anticipation to eliminate a useless factor $2$ that will appear later below (as it change nothing to the result of minimization anyway!). The way to put a problem in this form and minimize the cost function is generally named a "least square optimization problem" or in finance and ecology a "model calibration\index{model calibration}" (that will result to a "calibrated model"...).
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	We can also use this method to just minimize a function:
	
	without using any sum of square. This depends on the problem! The methods remains exactly the same. 
	\end{tcolorbox}
	Let us write (following the tradition in the field of optimization):
	
	We then have the following common notation:
	
	Now, imagine that we found a bipoint $(x_1,x_2)=(\vec{x})$ that gives this minimum and let us denote it $(\vec{x}_{*})$ and without forgetting that it will remain a local minimum and with luck a global one...! 

	Let us consider a special case that we will name "\NewTerm{compatible solution}\index{compatible solution}" and define by the fact the bi-point that minimize the sum of squares of errors is also such that for all $i$ we have:
	
	Therefore it is immediate that:
	
	Before going further, let us notice for example that for a component $j$ (which corresponds in our case to each variable of the a priori supposed theoretical function of our model):
	
	where the last condensed equality is many times less obvious as it makes usage of the gradient of a vector field (\SeeChapter{see section Vector Calculus page \pageref{gradient of vector field}}) that we see rarely in practice (note the factor $2$ that we were speaking about earlier!). The reader that should be destabilized can refer directly to the numerical example further below to makes things more crystal clear.
 	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	In the field of Machine Learning, considering (this a liner weighted input of a neural network as we will see later during our study of these objects):
	
	where $x_0$ refers something named a "bias unit" such that $x_0=1$.\\
	
	Here, our cost function is also the following sum of squared errors (SSE) with the traditional notation of Machine Learning field that fits more when we do detailed developments:
	
	We then have for one component $j$:
	
	So we have the correspondence for this special (famous) case:
	
	\end{tcolorbox}
	So to continue... we deduce that:
	
	and the "compatible solution" brings us obviously to:
	
	Following the same step, we have:
	
	So finally we have the following two relations:
	
	Given that for the "compatible solution" we have:
	
	it follows that in this case the second relation becomes:
	
	where $H$ is the Hessian matrix (\SeeChapter{see section Sequences and Series page \pageref{hessian matrix}}) what practitioners in the USA write simply:
	
	So we can approximate in the case of the compatible solution, the Hessian that contains derivatives of the second order by derivatives of the first order and that the main difference between the Gauss-Newton and Newton-Raphson method!!!!

	So we have finally in this special case the two relations that are the pillar of the Gauss-Newton's method:
	
	Now let us recall the basic relation of the Newton-Raphson method obtained earlier above:
	
	and for information, any mathematical technique (because they are many of them!) that simplifies the Hessian matrix (\SeeChapter{see section Sequences and Series page \pageref{hessian matrix}}) to the right of equality becomes part of the family named "\NewTerm{quasi-Newton's methods}\index{quasi-Newton's methods}".
	
	Well the Gauss-Newton's method that interest us here and is therefore one of the techniques of the family of the "quasi-Newton's methods" consists simply in a first step in getting rid of the second derivatives of the Hessian (\SeeChapter{see section Sequences and Series page \pageref{hessian matrix}}) of the Newton-Raphson method at the right of the equality thanks to the previously established relations such that (attention to remember the abuse of writing!):
	
	and in a second time rewrite the gradient on the left of the equality thanks also to the previously established relation. Which gives us:
	
	The factor $2$ above being not very aesthetic, almost all textbooks simplify such optimization problem with the following type of notation:
	
	So by simply multiplying by a factor $1/2$ (which does not change the result as we already know!) we have then:
		
	Let us recall again that a spreadsheet software like Microsoft Excel can not determine the derivatives it will calculate them using the numerical methods of right or centered derivatives as we have presented a earlier above.

	Now let us come back to our example of the beginning! So we have:
	
	We start with a bipoint that seems the closest to the desired solution:
	
	Then we have:
	
	and therefore we have:
	
	Then it comes:
	
	What we can therefore rewrite as:
	
	We also have by extension:
	
	Then we apply the relation proved earlier above:
	
	Therefore:
	
	After a minor simplification:
	
	Therefore:
	
	and therefore the next bipoint for the iteration will be:
	
	Hence formally:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In the field of Machine Learning, that latter relation is written as following:
	
	but where $\eta$ is fixed (in the field of Machine Learning!) and named the "\NewTerm{learning rate}\index{learning rate}" and the whole relation (corresponding to a minimization problem for recall!) is named the "\NewTerm{gradient descent method}\index{gradient descent method}".\\
	
	Obviously we see that the weight change is then defined by:
	
	
	In component notation, for the component $j$ this gives:
	
	In numerical computing this is often denoted:
	
	where $k=1,2,\ldots$ is iteration number, $t_k$ is step size (or step length) at iteration $k$.
	\end{tcolorbox}
	Which corresponds well to the values of the first iteration:
	
	We will not do again explicitly the other iterations. So this is what we get in the end:
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|p{2cm}|p{2cm}|}
					\hline
					\multicolumn{1}{c}{\cellcolor[gray]{0.75}$i$} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}$\mathop{\min}_{x_1,x_2} f(x_1,x_2)$}  \\ \hline
					\centering\arraybackslash\ $0$ & \centering\arraybackslash\ $2\cdot 10^0$ \\ \hline	
					\centering\arraybackslash\ $1$ & \centering\arraybackslash\ $4\cdot 10^{-3}$ \\ \hline	
					\centering\arraybackslash\ $2$ & \centering\arraybackslash\ $2\cdot 10^{-8}$ \\ \hline	
					\centering\arraybackslash\ $3$ & \centering\arraybackslash\ $3\cdot 10^{-9}$ \\ \hline	
					\centering\arraybackslash\ $4$ & \centering\arraybackslash\ $3\cdot 10^{-9}$ \\ \hline	
				\end{tabular}
		\end{center}
		\caption[]{Gauss Newton iterations of our example}
	\end{table}
	with therefore for local solution at the 4th iteration:
	
	With Maple 4.00b we get:
	
	\texttt{>with(plots):}\\
	\texttt{>points:=plot([[1,3.2939],[2,4.2699],[4,7.1749],[5,9.3008],[8,20.259] ],style=point,color=blue,symbol=circle):}\\
	\texttt{>plot\_GN:=plot(2.5411*exp(0.2595*x),x=0..8):}\\	
	\texttt{>display([pict1,pict2]);}
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.85]{img/computing/gauss_newton_example_plot.jpg}
		\caption[]{Points and our interpolated Gauss-Newton function}
	\end{figure}
		
	To close this subject let us do a comparison with the Newton-Raphson method for the first iteration using the same starting bipoint. Let us recall again that for this latter method, the iterations are based on the relation:
	
	and we will write the function as following for the Newton-Raphson method:
	
	and:
	
	Then we have:
	
	that becomes:
	
	Then we have:
	
	That becomes:
	
	Thus:
	
	Therefore:
	and therefore the next bipoint for the iteration will be:
	
	which corresponds obviously to the values of the first iteration:
	
	We will not do again explicitly also other iterations. So this is what it gives finally about our function to be minimized:
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|p{2cm}|p{2cm}|}
					\hline
					\multicolumn{1}{c}{\cellcolor[gray]{0.75}$i$} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}$\mathop{\min}_{x_1,x_2} f(x_1,x_2)$}  \\ \hline
					\centering\arraybackslash\ $0$ & \centering\arraybackslash\ $2\cdot 10^0$ \\ \hline	
					\centering\arraybackslash\ $1$ & \centering\arraybackslash\ $1\cdot 10^{-1}$ \\ \hline	
					\centering\arraybackslash\ $2$ & \centering\arraybackslash\ $2\cdot 10^{-4}$ \\ \hline	
					\centering\arraybackslash\ $3$ & \centering\arraybackslash\ $5\cdot 10^{-9}$ \\ \hline	
					\centering\arraybackslash\ $4$ & \centering\arraybackslash\ $6\cdot 10^{-9}$ \\ \hline	
					\centering\arraybackslash\ $5$ & \centering\arraybackslash\ $3\cdot 10^{-9}$ \\ \hline	
				\end{tabular}
		\end{center}
		\caption[]{Gauss Newton iterations of our example}
	\end{table}
	So the Newton-Raphson method converges in this case slower than that of Gauss-Newton.
	
	Here is the corresponding pseudo-code:
	
	\begin{algorithm}[H]
	 \KwIn{$y(\vec{x},t): \mathbb{R}^n \rightarrow \mathbb{R}$ a twice-differentiable a priori function to fit the data}
	 $\quad\quad\quad \vec{y}\in\mathbb{R}^m$ the real data\\
	 $\quad\quad\quad \vec{x}_0\in\mathbb{R}^n$ an initial estimation\\
	 $\quad\quad\quad\varepsilon$ is a tolerance value\\
	 \For{$k$ \KwTo maxiter}{
	  \For{$i$ \KwTo $m$}{
	  	Assign $F_i=y(\vec{x}_k,t)-y_i$ \Comment*[r]{to get the vector $F(\vec{x})$}
	  	
	  }
	  $\triangleright$ to get the vector ${\vec{\nabla}(f(\vec{x}_k))=\vec{\nabla}\left(F(\vec{x}_k)^T\right) F(\vec{x}_k)}$\\
	  \For{$j$ \KwTo $n$}{
	  	Assign $\displaystyle\vec{\nabla}(f(\vec{x}_k))_j=\sum_{i=1}^m F_i\dfrac{\partial y(\vec{x}_k,t)}{\partial x_j}$ 
	  	
	  }
	  $\triangleright$ to get the $n\times n$ matrix: $[\vec{\nabla}\left(F(\vec{x}_k)^T\right) \vec{\nabla}(F(\vec{x}_k))]$\\
	   \For{$i$ \KwTo $n$}{ 
	     \For{$j$ \KwTo $n$}{ 
	  	   Assign $\displaystyle [\vec{\nabla}\left(F(\vec{x}_k)^T\right) \vec{\nabla}(F(\vec{x}_k))]_{ij}=\sum_{l=1}^m \dfrac{\partial y(\vec{x}_k,t)}{\partial x_i}\dfrac{\partial y(\vec{x}_k,t)}{\partial x_j}$
	      }	  	
	    }
	  Update $\vec{x}_{k+1} \leftarrow \vec{x}_{k}-\left[\vec{\nabla}\left(F\left(\vec{x}_{k}\right)^T\right)\vec{\nabla}\left(F\left(\vec{x}_{k}\right)\right)\right]^{-1}\left(\vec{\nabla}\left(F\left(\vec{x}_k\right)^T\right) F\left(\vec{x}_k\right)\right)$\;
	  \uIf{$\left\|f\left(\vec{x}_{k+1}\right)-f\left(\vec{x}_{k}\right)\right\|<\varepsilon\; \textbf{or} \;\left\|\vec\nabla f\left(\vec x_{k}\right)\right\|<\varepsilon$}{
     		Converged. Print $\vec x^* \leftarrow \vec x_{k+1}$ and $f\left(\vec x^*\right) \leftarrow f\left(\vec x_{k+1}\right)$\;
	 	}
	 }
	\caption{Gauss-Newton method (tangent Newton)}
	\end{algorithm}
	
	\begin{tcolorbox}[enhanced,title=Remark,colframe=black,arc=10pt,drop lifted shadow,after skip=15pt plus 2pt]
	For the square matrix inversion in the pseudo-code above, one can use the determinant and cofactor method (\SeeChapter{see section Linear Algebra page \pageref{determinant matrix inverse}}), QR decomposition (\SeeChapter{see section Linear Algebra page \pageref{QR decomposition}}), LU decomposition (\SeeChapter{see section Linear Algebra page \pageref{lu decomposition}}) or Gaussian Elimination with partial pivoting (GEPP).
	\end{tcolorbox}
	
	So this was a very detailed step-by-step numerical example. For people interested in Machine Learning and for a more formal example (without numerical values) with a famous case (univariate linear regression) let us consider:
	
	The cost function for any guess of $\theta_0, \theta_1$ can be computed as:
	
	where $x^{(i)}$ and $y^{(i)}$ are the $x$ and $y$ values for the $i$th component in the learning set. If we substitute for $h_\theta(x)$:
	
	Then as we know, in Machine Learning, the goal of gradient descent\index{gradient descent} can be expressed as:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	At the opposite of the "\NewTerm{gradient ascent}\index{gradient ascent}" where we seek:
	
	\end{tcolorbox}
	Finally, each step in the gradient descent can be described as we have seen earlier above for a component $j$:
	
	for $j=0$ and $j=1$ in this special case!
	
	It is straightforward that:
	
	and:
	
	Or in vector form:
	
	
	In order to minimize the above cost function $J(\theta_0,\theta_1)$, notice that the gradient  is calculated from the \underline{whole training set} (hence the sum!). This is why this approach of "gradient descent" is more specifically referred to as "\NewTerm{batch gradient descent}\index{batch gradient descent}".
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The use of the traditional complete method using the Hessian is in the early 121st century (holocene calendar) impractical for most deep learning applications because computing (and inverting) the Hessian in its explicit form is a very costly process in both space and time. For instance, a Neural Network with one million parameters would have a Hessian matrix of size $[1,000,000 \times 1,000,000]$, occupying approximately $3,725$ gigabytes of RAM. Hence, a large variety of quasi-Newton methods have been developed that seek to approximate the inverse Hessian!
	\end{tcolorbox}
	
	As we need to calculate the gradients for the whole dataset to perform just one update, batch gradient-descent can be very slow and is intractable for datasets that do not fit in memory.  Batch gradient  descent also does not allow us to update our model online, i.e. with new examples on-the-fly

	So, in "\NewTerm{stochastic gradient descent method}\index{stochastic gradient descent method}" (SGD), also known as "\NewTerm{incremental gradient descent}\index{incremental gradient descent}", instead of updating the weights $\vec{\theta}$ based on the sum of the $m$ accumulated errors over all samples $x^{(i)}$ :
	
	we can use the following update:
	
	Note that we now update the weights incrementally with a single training sample but not with the whole training set. 
	
	It is said to be "stochastic" because samples are selected randomly (or shuffled) instead of as a single group (as in standard gradient descent) or in the order they appear in the training set.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let's suppose we want to fit a straight line $y_i=\theta_0+\theta_1x_i$ to a training set with observations $(x_{1},x_{2},\ldots ,x_{n})$ and corresponding estimated responses $({\hat {y_{1}}},{\hat {y_{2}}},\ldots ,{\hat {y_{n}}})$ using least squares. The objective function to be minimized is:
	
	Then we will have using the stochastic gradient descent method:
	
	Note again that in each iteration, the gradient is evaluated at a single point $x^{(i)}$ instead of being evaluated on the set of all samples.
	\end{tcolorbox}
	The key difference compared to standard (Batch) Gradient Descent is that only one piece of data from the dataset is used to calculate the step, and the piece of data is picked randomly at each step.  SGD  does  away  with  this  redundancy  by performing one update at a time.  It is therefore usually much faster and can also be used to learn online.  SGD performs frequent updates with a high variance that cause the objective function to fluctuate heavily.
	
	While batch gradient descent converges to the minimum of the basin the parameters are placed in, SGD's fluctuation, on the one hand, enables it to jump to new and potentially better local minima. On the other hand, this ultimately complicates convergence to the exact minimum, as SGD will keep overshooting. However, it has been shown that when we slowly decrease the learning rate, SGD shows the same convergence behaviour as batch gradient descent, almost certainly converging to a local or the global minimum for non-convex and convex optimization respectively.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	"\NewTerm{mini-batch gradient descent}\index{mini-batch gradient descent}" initially takes the best of both worlds and performs an update for every mini-batch of $n$ training examples. Common mini-batch sizes range between $50$ and $256$, but can vary for different applications. Mini-batch gradient descent is typically the algorithm of choice when training a neural network and the term SGD usually is employed also when mini-batches are used
	\end{tcolorbox}
	
	\subparagraph{Gradient Descent/Ascent techniques summary}\mbox{}\\\\
	At this point it may be useful to also introduce the gradient descent\index{gradient descent} and gradient ascent\index{gradient ascent} in a simpler way (if it can help!) as it is quite and important topic.
	
	The reader may have already understand that the gradient descent/ascent is a general framework for solving optimization problems where we want to maximize or minimize functions of continuous (differentiable) parameters.

	We will illustrate the technique here for maximization (ascent) in terms of a single free variable, but the generalization is quite simple and there is already plenty of example above!
	
	Suppose we want to find a maximum of some function $y=f(x)$. The standard procedure is to find $f'(x)$, set it to zero and solve for $x$. But what if $f'(x) = 0$ is a difficult equation to solve?

	We can still find the maximum algorithmically!
	
	Let $y=f(x)$ have a maximum at $x_{\max}$. Pick an arbitrary value for $x$, say $x_1$. Compute $f '(x_1)$. If the slope of $y$ is positive at $x_1$, i.e. $f'(x_1) > 0$, then $x_{\max} > x_1$ lies to the right of $x_1$. Likewise if $f'(x_1) < 0$, then $x_{\max} < x_1$ lies to its left!

	Thus we know the direction in which $x_1$ should be updated in order to approach $x_{\max}$. In fact this direction is given by $f'(x_1)$. So we can use the update rule:
	
	where $\eta$ is a positive constant. If $\eta$ is sufficiently small, and there is indeed a maximum for $f$, the above update rule will converge to it after a finite number of iterations (when $f'(x_1) = 0$).
	
	This is the so-named "gradient ascent technique" to determine maxima when actual solution of $f' = 0$ is difficult. To find minima, the corresponding gradient descent rule can set up to update $x_1$ away from $x_{\max}$.
	
	For $y = f(x_1, \cdots, x_n)$ the partial derivatives with respect to each $x_i$ indicate how rapidly $y$ is changing along that axis and so the gradient, $\nabla y$, in fact denotes the precise direction which promises maximum increase of $y$.
	
	Collectively these, among others, are named "\NewTerm{gradient techniques}".
	
	We will summarise now the common gradient descent optimisation algorithms used in popular deep learning frameworks in the early 121st century (holocene calendar).
	
	Recall that the vanilla stochastic gradient descent (SGD) updates weights by subtracting the current weight by a factor (i.e. $\eta$, the learning rate) of its gradient:
	
	Variations in this equation are commonly known as "\NewTerm{stochastic gradient descent optimisers}\index{stochastic gradient descent optimisers}". There are 3 main ways how they differ:
	\begin{enumerate}
		\item Adapt the "gradient component" ($\frac{\partial L}{\partial w}$)

		Instead of using only one single gradient like in stochastic vanilla gradient descent to update the weight, take an aggregate of multiple gradients. Specifically, these optimisers use the exponential moving average of gradients.
		
		\item Adapt the "learning rate component" ($\eta$)

		Instead of keeping a constant learning rate, adapt the learning rate according to the magnitude of the gradient(s).
		
		\item Both (1) and (2)

		Adapt both the gradient component and the learning rate component.
	\end{enumerate}
	As you will see later, these optimisers try to improve the amount of information used to update the weights, mainly through using previous (and future) gradients, instead of only the present available gradient.

	Below is a table that summarises which components are being adapted (years are based on the holocene calendar):
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|c|c|}
		\hline
		\cellcolor[gray]{0.75}\textbf{Optimiser} & \cellcolor[gray]{0.75}\textbf{Year} & \cellcolor[gray]{0.75}\textbf{Learning Rate} & \cellcolor[gray]{0.75}\textbf{Gradient} \\ \hline
		Momentum & 11964 &  & \checkmark \\ \hline
		AdaGrad & 12011 & \checkmark  &  \\ \hline
		RMSprop & 12012 & \checkmark  &  \\ \hline
		AdaDelta & 12012 & \checkmark  &  \\ \hline
		Nesterov & 12013 &  & \checkmark  \\ \hline
		Adam & 12014 & \checkmark  & \checkmark  \\ \hline
		AdaMax & 12015 & \checkmark  & \checkmark  \\ \hline
		Nadam & 12015 & \checkmark  & \checkmark  \\ \hline
		AMSGrad & 12018 & \checkmark  & \checkmark  \\ \hline
		\end{tabular}
		\caption{Gradient descent optimisers}
	\end{table}
	Here are the detailed expression of each of the empirical method visible in the above table (the knowledgeable reader may recognize some techniques inspired by time series forecasting techniques):
	\begin{itemize}
		\item Vanilla SGD:
		
		
		\item Momentum:
		
		
		\item Adagrad:
		
		
		\item RMSprop:
		
		
		\item Adadelta:
		
		
		\item Nesterov:
		
		
		\item Adam:
		
		
		\item AdaMax:
		
		
		\item Nadam:
		
		
		\item AMSGrad:
		
	\end{itemize}
	Below the reader can see animations (credit: Alec Radford) that may help to have a better intuition about the learning process dynamics (we also strongly recommend to read the paper \textit{An overview of gradient descent optimization algorithms} to get more details on the underlying ideas \cite{ruder2016overview}).
	
	The first Flash animation below depicts the contours of a loss surface and time evolution of different optimization algorithms. Notice the "overshooting" behaviour of momentum-based methods, which make the optimization look like a ball rolling down the hill:
	\begin{center}
		\includemedia[activate=pageopen,width=\textwidth,height=380pt,
	]{}{swf/contours_evaluation_optimizers.swf}
	\end{center}
	The animation above will run for people having a PDF reader with Adobe Flash player installed and activated (otherwise see here: \url{https://vimeo.com/575749745}).
	
	The second and last Flash animation is a visualization of a saddle point in the optimization landscape, where the curvature along different dimension has different signs (one dimension curves up and another down). Notice that SGD has a very hard time breaking symmetry and gets stuck on the top. Conversely, algorithms such as RMSprop will see very low gradients in the saddle direction. Due to the denominator term in the RMSprop update, this will increase the effective learning rate along this direction, helping RMSProp proceed:
	\begin{center}
		\includemedia[activate=pageopen,width=\textwidth,height=380pt,
	]{}{swf/saddle_point_evaluation_optimizers.swf}
	\end{center}
	The animation above will run for people having a PDF reader with Adobe Flash player installed and activated (otherwise see here: \url{https://vimeo.com/575752619}).
	
	\pagebreak
	\subsubsection{Expectation-Maximization (EM) algorithm}\label{EM algorithm}
	The "\NewTerm{Expectation-Maximization algorithm}\index{expectation-maximization algorithm}" is a stochastic optimizations algorithm often used in machine learning and very quite different of the methods that we have seen before. We will use it explicitly in this book later below for various applications like Factor Analysis (see page \pageref{factor analysis}) but also for the $T$-distributed Stochastic Neighbour Embedding (see page \pageref{tsne}) and others during our study of Data Mining.
	
	The EM algorithm seems to have been introduced in the early 11950 (holocene calendar) by R. Ceppellini, M. Siniscalco and C.A. Smith in the context of gene frequency estimation. The expectation maximization algorithm was analysed more generally by H. Hartley and by L.E. Baum in the context of hidden Markov models, where it is commonly known as the "Baum-Welch algorithm".
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The text that follows is inspired of the following paper \cite{brayexpectation} but also of the following web page \url{https://indowhiz.com/articles/en/the-simple-concept-of-expectation-maximization-em-algorithm} itself inspired of \cite{do2008expectation}.
	\end{tcolorbox}
	
	 To summarize, EM algorithm is actually an iterative method, involving expectation (E-step) and maximization (M-step) to find the local maximum likelihood from the data. Commonly, EM algorithm is used on several distributions or statistical models, where there are one or more unknown variables named the "latent variables".
	 
	 To easily understand EM algorithm, we will use first a companion example of the coin tosses distribution that is widely used in quite a few textbooks, articles and blogs and afterwards we will deal with the underlying mathematical formalism.
	 
	 So let us consider that we have $2$ coins: Coin $A$ and Coin $B$, where both have a different head-up probability. We will randomly choose a coin $5$ times, whether coin $A$ or $B$. Then, each coin selection is followed by tossing it $10$ times. Therefore, we have the following outcomes:
	 \begin{itemize}
	 	\item Set 1 (\textcolor{blue}{Coin $B$}): \texttt{H T T T H H T H T H} (5H5T)
	 	\item Set 2 (\textcolor{red}{Coin $A$}): \texttt{H H H H T H H H H H} (9H1T)
	 	\item Set 3 (\textcolor{red}{Coin $A$}): \texttt{H T H H H H H T H H} (8H2T)
	 	\item Set 4 (\textcolor{blue}{Coin $B$}): \texttt{H T H T T T H H T T} (4H6T)
	 	\item Set 5 (\textcolor{red}{Coin $A$}): \texttt{T H H H T H H H T H} (7H3T)
	 \end{itemize}
	 where the image side of the coin will be denoted by \texttt{H} or Head-up, and the number side will be denoted by \texttt{T} or Tail-up. Then, the probability of a coin to land with head-up for each of these coins, will be denoted as $\theta$.
	 
	 The question is, how do we estimate $\theta$ for each coin?
	 
	 Because we already have all the data we need, we can easily calculate the probability of getting head-up for each coin. Therefore, the calculation of $\theta$ will be done through the maximum likelihood estimation (MLE) of the proportion that can naively be obtained by:
	 \begin{itemize}
	 	\item By knowing the result of coin $A = 24\texttt{H}6\texttt{T}$, in $3$ sets with a total of $30$ tosses, then:
	 	
	 	
	 	\item Similarly to the result of coin $B = 9\texttt{H}11\texttt{T}$, in $2$ sets with a total of $20$ tosses, then:
	 	
	 \end{itemize}
	 Before returning to the original case, let us explain briefly the different the stages of the EM algorithm as it will give us an overall picture of the EM algorithm flow and may help to understand what will follow:
	 \begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1215); %set diagram left start at 0, and has height of 1215
		
		%Flowchart: Terminator [id:dp08373950434149258] 
		\draw   (106.36,59) -- (163.14,59) .. controls (170.52,59) and (176.5,69.07) .. (176.5,81.5) .. controls (176.5,93.93) and (170.52,104) .. (163.14,104) -- (106.36,104) .. controls (98.98,104) and (93,93.93) .. (93,81.5) .. controls (93,69.07) and (98.98,59) .. (106.36,59) -- cycle ;
		%Shape: Rectangle [id:dp22279045778418682] 
		\draw   (236.5,59.5) -- (367,59.5) -- (367,104.5) -- (236.5,104.5) -- cycle ;
		%Shape: Rectangle [id:dp1391671247144981] 
		\draw   (236.5,146.5) -- (367,146.5) -- (367,191.5) -- (236.5,191.5) -- cycle ;
		%Straight Lines [id:da44801398728311814] 
		\draw    (176.5,81.5) -- (235.5,81.5) ;
		\draw [shift={(237.5,81.5)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da10582998563638313] 
		\draw    (301.75,104.5) -- (301.75,144) ;
		\draw [shift={(301.75,146)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da3680426994956969] 
		\draw    (301.75,191.5) -- (301.75,231) ;
		\draw [shift={(301.75,233)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Flowchart: Decision [id:dp912155101853934] 
		\draw   (301.75,233) -- (382.5,284.5) -- (301.75,336) -- (221,284.5) -- cycle ;
		%Shape: Rectangle [id:dp10614439588221525] 
		\draw   (47.5,261.5) -- (178,261.5) -- (178,306.5) -- (47.5,306.5) -- cycle ;
		%Straight Lines [id:da23496095533716566] 
		\draw    (221,284.5) -- (180.5,284.5) ;
		\draw [shift={(178.5,284.5)}, rotate = 360] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da1459076439522098] 
		\draw    (382.5,284.5) -- (421.5,284.5) ;
		\draw [shift={(423.5,284.5)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Rectangle [id:dp8079571611958085] 
		\draw   (423.5,262.5) -- (554,262.5) -- (554,307.5) -- (423.5,307.5) -- cycle ;
		%Shape: Rectangle [id:dp8098420012144425] 
		\draw   (47.5,396.5) -- (178,396.5) -- (178,441.5) -- (47.5,441.5) -- cycle ;
		%Straight Lines [id:da407313434918954] 
		\draw    (113.75,307) -- (113.75,394) ;
		\draw [shift={(113.75,396)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da6169786209955743] 
		\draw    (177.75,418) -- (301.5,418) -- (301.74,338) ;
		\draw [shift={(301.75,336)}, rotate = 90.17] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da49384748002783185] 
		\draw    (488.75,307) -- (488.75,394) ;
		\draw [shift={(488.75,396)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Flowchart: Terminator [id:dp937722397998382] 
		\draw   (460.36,395) -- (517.14,395) .. controls (524.52,395) and (530.5,405.07) .. (530.5,417.5) .. controls (530.5,429.93) and (524.52,440) .. (517.14,440) -- (460.36,440) .. controls (452.98,440) and (447,429.93) .. (447,417.5) .. controls (447,405.07) and (452.98,395) .. (460.36,395) -- cycle ;
		
		% Text Node
		\draw (118,74) node [anchor=north west][inner sep=0.75pt]   [align=left] {Start};
		% Text Node
		\draw (247.75,65) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{85pt}\setlength\topsep{0pt}
		\begin{center}
		Define\\Latent variables
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (249.25,160) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{85pt}\setlength\topsep{0pt}
		\begin{center}
		Initial Guessing
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (235.25,261) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{100pt}\setlength\topsep{0pt}
		\begin{center}
		Stop iteration?\\(stopping condition)
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (197,265) node [anchor=north west][inner sep=0.75pt]   [align=left] {No};
		% Text Node
		\draw (387,265) node [anchor=north west][inner sep=0.75pt]   [align=left] {Yes};
		% Text Node
		\draw (86,276) node [anchor=north west][inner sep=0.75pt]   [align=left] {E-step};
		% Text Node
		\draw (449,276) node [anchor=north west][inner sep=0.75pt]   [align=left] {Show result};
		% Text Node
		\draw (86,411) node [anchor=north west][inner sep=0.75pt]   [align=left] {M-step};
		% Text Node
		\draw (475,408) node [anchor=north west][inner sep=0.75pt]   [align=left] {End};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{ISO 5807:85 Flowchart of EM algorithm}
	\end{figure}
	Actually, the main point of EM is the iteration between E-step and M-step, which could be seen in the figure above. The E-step will estimate your hidden variables, and the M-step will re-update the parameters, based on the estimation of the hidden variables. In other words, this iteration aims to re-improve the estimation of current parameters.
	
	First, what we must define is what variables are required; but are not observed directly in the data.

	The goal is to estimate the probability of getting heads-up for each coin. However, it cannot be calculated directly if we don't know the identity of the coin used in each set. Therefore, it is necessary to know which coin is used in each set. In other words, this coin identity will be our latent variable.

	Right now, besides the $5$ sets of outcomes above, we only know that we used two coins; which are coin $A$ and coin $B$.
	
	As stated before, the probability of getting head-up for each of these coins is denoted as  $\theta$. Currently, there are only coin $A$ and coin $B$; with unknown parameter values of $\theta_{A}$ and $\theta_{B}$.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	By the way, please note that there is no relationship between parameters guessing; both $\theta_{A}$ and $\theta_{B}$. For example, if you think that the sum of $\theta_{A}$ and $\theta_{B}$ must be $= 1$, NO! This probability represents the individual value of getting heads-up on each coin.
	\end{tcolorbox}
	 Okay enough! Now, let's say we have randomly set both initial values, which are:
	 
	 Now we have the required variables. So, we can start estimating the identity of coin used in each set.

	Each set, which contains the outcomes of heads and tails, can be denoted by $E$ notation. Then, the probability of \textit{using coin $A$} denoted as $Z_{A}$, and the probability of \textit{using coin $B$} denoted as $Z_{B}$.
	 
	At the beginning of E-Step, we need to know the probability of a set using coins $A$ or $B$. Take an example from the set 3 earlier above, where $E = \texttt{HTHHHHHTHH}$ (8H2T). If $\hat{\theta}_{A} = 0.6$, it means the probability of getting head is $0.6$ (and tail $0.4$) uses coin $A$. Then, how much probability of coin $A$ will give $8\texttt{H}2\texttt{T}$ in $10$ tosses (a set)? This is what we need to estimate first. Then, this probability can be denoted by $P(E|Z_{A})$. Similarly, the probability of coin $B$ giving $E=8\texttt{H}2\texttt{T}$, could be denoted by $P(E|Z_{B})$. 
	
	This actually relates to the probability distribution of a binomial random variable, which definition is for recall:
	
	 Or denoted:
	 
	 Then, we can calculate the probability of getting $E$ using coins $A$ and $B$ as follows:
	 
	 Next, we can compare the probabilities of both coin $A$ and coin $B$ giving $E$. So, according to Bayes’ theorem and the law of total probability, we can determine the ratio of probability (posterior value of latent variables) using the following relation:
	 
	 where $P(Z_{x}|E)$ is the probability of coin $x$ giving $E$ (compared to coin $y$), $P(Z_{x})$ is the probability of choosing coin $x$ and $P(Z_{x})$ is the probability of choosing coin $y$.
	 
	 However, because our case is only using $2$ coins, coins $A$ and $B$, then the probability that we will choose one of them is $50/50$. Then $P(Z_{A}) = P(Z_{B}) = 0.5$.
	 
	 Accordingly, for coin $A$, we can simplify the relation above into:
	 
	 Hence:
	 
	 Similarly, for coin $B$, we have:
	 
	 Previously, the ratio of coins $A$ and $B$ in giving each $E$ needs to be calculated. In this case, it is calculated from set $1$ to $5$. Then the results could be seen in the following table:
	 \begin{table}[H]
	 	\centering
		\begin{tabular}{|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}\textbf{Coin Tosses}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{$E$}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Coin $A$ probability}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Coin $B$ probability}} \\ \hline
		\texttt{HTTTHHTHTH} & 5\texttt{H}5\texttt{T} & $0.45$ & $0.55$ \\ \hline
		\texttt{HHHHTHHHHH} & 9\texttt{H}1\texttt{T} & $0.80$ & $0.20$ \\ \hline
		\texttt{HTHHHHHTHH} & 8\texttt{H}3\texttt{T} & $0.73$ & $0.27$ \\ \hline
		\texttt{HTHTTTHHTT} & 4\texttt{H}6\texttt{T} & $0.35$ & $0.65$ \\ \hline
		\texttt{THHHTHHHTH} & 7\texttt{H}3\texttt{T} & $0.65$ & $0.35$ \\ \hline
		\end{tabular}
	\end{table}
	After that, we need to estimate the total number of \texttt{H} for each coin. It is calculated based on the coin ratio above. To calculate \textit{total heads and tails} for coin $x$, it is similar to the complete data. The method is quite easy, we just need to multiply the ratio of each coin to the number of heads in each $E$, the results are shown in the following table:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}\textbf{Coin Tosses}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{$E$}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Estimated \texttt{H} for Coin $A$}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Estimated \texttt{H} for Coin $B$}} \\ \hline
		\texttt{HTTTHHTHTH} & 5\texttt{H}5\texttt{T} & $5\cdot 0.45=2.25$ & $5\cdot 0.55 = 2.75$ \\ \hline
		\texttt{HHHHTHHHHH} & 9\texttt{H}1\texttt{T} & $9\cdot 0.8 =7.2$ & $9\cdot 0.20=1.8$ \\ \hline
		\texttt{HTHHHHHTHH} & 8\texttt{H}3\texttt{T} & $8\cdot 0.73 = 5.84$ & $8\cdot 0.27 = 2.16$ \\ \hline
		\texttt{HTHTTTHHTT} & 4\texttt{H}6\texttt{T} & $4\cdot 0.35 = 1.4$ & $4\cdot 0.65=2.6$ \\ \hline
		\texttt{THHHTHHHTH} & 7\texttt{H}3\texttt{T} & $7\cdot 0.65 = 4.55$ & $7\cdot 0.35=2.45$ \\ \hline
		\end{tabular}
	\end{table}
	If we want, we can also calculate the tails for each coin as in the table above. But that is not necessary, because we can use another straightforward approach.

	Until this step, we already have the E-Step calculations. Just a little bit more effort to finish our calculation in the M-Step.
	
	The results from the E-step can be used to improve the $\hat{\theta}_x$ parameters. Here, we can use the maximum likelihood estimation (MLE) relation similar to the completed data.

	As we said before, it is not necessary to calculate the tails for each coin; then we sum the heads and tails for each coin. Because each set contains $10$ tosses, we just need to multiply the coin ratio with $10$. That way, we could get the total estimated tosses from. Therefore (we will prove in details just further below where these both relations comes from):
	
	Finally, the parameter of $\hat{\theta}_{A}$ and $\hat{\theta}_{B}$ for the first iteration have been improved. For the next iteration, the E-Step will use this new parameter value; and re-improved at next M-step. This iteration will always repeating the E-step and M-step, until it reaches any stop condition as illustrated below:
	 \begin{figure}[H]
		\centering
		\includegraphics[scale=0.85]{img/computing/em_algorithm.jpg}
		\caption[EM parameter estimation for complete and incomplete data]{EM parameter estimation for complete and incomplete data\\ (source: Nature Biotechnology Volume 26 Number 8 August 12008)}
	\end{figure}
	The iteration of the E-Step and M-Step, will be repeated until they meet the stopping condition. Commonly, the EM algorithm has two options of stopping condition, which are:
	\begin{itemize}
		\item Maximum iteration: means that, the EM Algorithm will stop if a certain number of iterations has been reached. For example the maximum iteration is set to $10$ iterations, then the EM Algorithm will not be more than $10$ iterations
		
		\item Convergence threshold: means that, the M-step gives no significant parameter improvement; compared to the improvement in the previous iteration. The changes are very small below our threshold
	\end{itemize}
	In the EM case example above, the parameter improvement in each iteration can be seen in the following table:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}\textbf{Iteration $i$}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{$\theta_{A}^{(i)}$}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{$\theta_{B}^{(i)}$}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Differences}} \\ \hline
		$0$ & $0.6$ & $0.5$ & $0.7180$ \\ \hline		
		$1$ & $0.713$ & $0.581$ & $0.1390$ \\ \hline
		$2$ & $0.745$ & $0.569$ & $0.0342$ \\ \hline
		$3$ & $0.768$ & $0.550$ & $0.0298$ \\ \hline
		$4$ & $0.783$ & $0.535$ & $0.0212$ \\ \hline
		$5$ & $0.791$ & $0.526$ & $0.0120$ \\ \hline
		$6$ & $0.795$ & $0.522$ & $0.0057$ \\ \hline
		$7$ & $0.796$ & $0.521$ & $0.0014$ \\ \hline
		$8$ & $0.796$ & $0.520$ & $0.0010$ \\ \hline
		$9$ & $0.796$ & $0.520$ & $0.0000$ \\ \hline
		\end{tabular}
	\end{table}
	To calculate the differences or improvements in each iteration, we can use the Euclidean Distance such that:
	
	 So for example:
	 
	As many algorithm that one also has weaknesses, without too much doubts. Indeed, every iteration in the EM algorithm, in general, will always improve the parameter closer to the local maximum likelihood. In other words, the EM algorithm will guarantee convergence, but will not guarantee to give a global maximum likelihood. And also there is no guarantee that you will get the maximum likelihood estimation (MLE).

	Now let us formalize what we have seen. For this purpose we will use the personal notes of Benjamin Bray \cite{brayexpectation} who authorized us to reproduce them below (we did some minor modifications).
	
	Suppose we still have two coins, each with a different probability of heads, $\theta_A$ and $\theta_B$, unknown to us. We collect data from a series of $N$ trials in order to estimate the bias of each coin. Each trial $k$ consists of flipping the same random coin $Z_k$ a total of $M$ times and recording only the total number $X_k$ of heads.
	
	Assuming all samples in the observed dataset $X=[\vec{x}_1,\ldots,\vec{x}_N]$ are independent and identically distributed (i.i.d.), we can find the "\NewTerm{complete data likelihood}\label{complete data likelihood}\index{complete data likelihood}" function of ${\theta}$ (using the Bayes' relation established in the section of Probabilities page \pageref{bayes relation for complete data likelihood}):
	
	and the log-likelihood:
	
	Or in scalar form (for non-multivariate distributions):
	
	The second term can be dropped in our companion example as $P(z_n\vert \theta)$ is independent of the model parameters in ${\theta}$ and therefore irrelevant to the maximization of the log likelihood with respect to ${\theta}$. Indeed, the reader must remember that in our companion example, $z_n$ is the realization of the random (hidden) variable that describes the probability to deal with coin $A$ either coin $B$. We assume that this probability is constant and equal to $1/2$, hence:
	 
	The remaining term in our companion example is:
	
    Now that we have specified the probabilistic model and worked out all relevant probabilities, we are ready to derive an Expectation-Maximization algorithm.
    
	Our general approach will be to reason about the hidden variables through a proxy distribution $q$, which we use to compute a lower-bound on the log-likelihood.  This section is devoted to deriving one such bound, named the "\NewTerm{Evidence Lower Bound}\label{evidence lower bound}" (ELBO).  We can expand the data log-likelihood by marginalizing over the hidden variables:
    
	Through Jensen's inequality (\SeeChapter{see section Statistics page \pageref{jensen inequality}}), we obtain the following bound:
	
	The lower bound can be rewritten as follows:
	
    Where the last term is often a constant at each iteration that we can ignore for the maximization problem.
    
    Now that we have this result, the \textbf{E-Step} is straightforward (calculation of the lower bound on the observed log-likelihood).  The \textbf{M-Step} computes a new parameter estimate $\theta_{t+1}$ by optimizing over the lower bound found in the E-Step, as:
    
    Now, because each trial is conditionally independent of the others, given the parameters:
	
	Let $a_k = q(z_k = A)$ and $b_k = q(z_k = B)$.  Note that:
	
	 To maximize the above expression with respect to the parameters, we take derivatives with respect to $\theta_A$ and $\theta_B$ and set to zero:
	
Solving for $\theta_A$ and $\theta_B$, we get
    
    These are the both relations we used earlier above in our companion example!

    We will see more complicated and practical business applications of the EM algorithm further below like:
    \begin{itemize}
    	\item For our study of Gaussian Mixture Classification (related to $K$-mean)
    	
    	\item For our study of Factor Analysis (Exploratory Factor Analysis (EFA))
    	
    	\item For our study of $T$-distributed Stochastic Neighbour Embedding ($T$-SNE)
    \end{itemize}
    
	
	\pagebreak
	\subsection{Resampling statistics}	
	Resampling statistics refers to the use of the observed data or of a data generating mechanism (such as a die) to produce new hypothetical samples (resamples) that mimic the underlying population, the results of which can then be analysed. With numerous cross-disciplinary applications especially in the sub-disciplines of the life science, resampling methods are widely used since they are options when parametric approaches are difficult to employ or otherwise do not apply. 
	
       Resampled data is derived using a manual mechanism to simulate many pseudo-trials. These approaches were difficult to utilize prior to 11980s (holocene calendar) since these methods require many repetitions. With the incorporation of computers, the trials can be simulated in a few minutes and is why these methods have become widely used.  The methods that will be discussed are used to make many statistical inferences about the underlying population. The most practical use of resampling methods is to derive confidence intervals and test hypotheses. This is accomplished by drawing simulated samples from the data themselves (resamples) or from a reference distribution based on the data; afterwards, you are able to observe how the statistic of interest in these resamples behaves. Resampling approaches can be used to substitute for traditional statistical (formulaic) approaches or when a traditional approach is difficult to apply. These methods are widely used because their ease of use. They generally require minimal mathematical formulas, needing a small amount of mathematical (algebraic) knowledge. These methods are easy to understand and stray away from choosing an incorrect formula in your diagnostics.
      \begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,712); %set diagram left start at 0, and has height of 712
		
		%Shape: Rectangle [id:dp5872209300005498] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (73,44) -- (98.5,44) -- (98.5,62) -- (73,62) -- cycle ;
		%Shape: Rectangle [id:dp47458644534676075] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (98.5,44) -- (124,44) -- (124,62) -- (98.5,62) -- cycle ;
		%Shape: Rectangle [id:dp23920291081142087] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (73,62) -- (98.5,62) -- (98.5,78) -- (73,78) -- cycle ;
		%Shape: Rectangle [id:dp33331256059624104] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (98.5,62) -- (124,62) -- (124,78) -- (98.5,78) -- cycle ;
		%Shape: Rectangle [id:dp683903708984889] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (73,78) -- (98.5,78) -- (98.5,102) -- (73,102) -- cycle ;
		%Shape: Rectangle [id:dp42348977349618666] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (98.5,78) -- (124,78) -- (124,102) -- (98.5,102) -- cycle ;
		%Shape: Rectangle [id:dp35085149826288453] 
		\draw   (73,102) -- (98.5,102) -- (98.5,119) -- (73,119) -- cycle ;
		%Shape: Rectangle [id:dp14026376757878545] 
		\draw   (98.5,102) -- (124,102) -- (124,119) -- (98.5,119) -- cycle ;
		%Shape: Rectangle [id:dp8709973279459922] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (140,44) -- (165.5,44) -- (165.5,62) -- (140,62) -- cycle ;
		%Shape: Rectangle [id:dp935889953722125] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (165.5,44) -- (191,44) -- (191,62) -- (165.5,62) -- cycle ;
		%Shape: Rectangle [id:dp8460531132170881] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (140,62) -- (165.5,62) -- (165.5,78) -- (140,78) -- cycle ;
		%Shape: Rectangle [id:dp22005916355786037] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (165.5,62) -- (191,62) -- (191,78) -- (165.5,78) -- cycle ;
		%Shape: Rectangle [id:dp09968781158102269] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (140,78) -- (165.5,78) -- (165.5,102) -- (140,102) -- cycle ;
		%Shape: Rectangle [id:dp2447915223789836] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (165.5,78) -- (191,78) -- (191,102) -- (165.5,102) -- cycle ;
		%Shape: Rectangle [id:dp7315101159832083] 
		\draw   (140,102) -- (165.5,102) -- (165.5,119) -- (140,119) -- cycle ;
		%Shape: Rectangle [id:dp2604573700873538] 
		\draw   (165.5,102) -- (191,102) -- (191,119) -- (165.5,119) -- cycle ;
		%Shape: Rectangle [id:dp5469502329453304] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (208,44) -- (233.5,44) -- (233.5,62) -- (208,62) -- cycle ;
		%Shape: Rectangle [id:dp056334926961020715] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (233.5,44) -- (259,44) -- (259,62) -- (233.5,62) -- cycle ;
		%Shape: Rectangle [id:dp3149372129426069] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (208,62) -- (233.5,62) -- (233.5,78) -- (208,78) -- cycle ;
		%Shape: Rectangle [id:dp39678266430551545] 
		\draw   (233.5,62) -- (259,62) -- (259,78) -- (233.5,78) -- cycle ;
		%Shape: Rectangle [id:dp8128625029274765] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (208,78) -- (233.5,78) -- (233.5,102) -- (208,102) -- cycle ;
		%Shape: Rectangle [id:dp8175391428197365] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (233.5,78) -- (259,78) -- (259,102) -- (233.5,102) -- cycle ;
		%Shape: Rectangle [id:dp5121856342364495] 
		\draw   (208,102) -- (233.5,102) -- (233.5,119) -- (208,119) -- cycle ;
		%Shape: Rectangle [id:dp866937823468432] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (233.5,102) -- (259,102) -- (259,119) -- (233.5,119) -- cycle ;
		%Shape: Rectangle [id:dp06950512131996445] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (276,44) -- (301.5,44) -- (301.5,62) -- (276,62) -- cycle ;
		%Shape: Rectangle [id:dp0915188985899027] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (301.5,44) -- (327,44) -- (327,62) -- (301.5,62) -- cycle ;
		%Shape: Rectangle [id:dp8425631304651999] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (276,62) -- (301.5,62) -- (301.5,78) -- (276,78) -- cycle ;
		%Shape: Rectangle [id:dp9620545164628691] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (301.5,62) -- (327,62) -- (327,78) -- (301.5,78) -- cycle ;
		%Shape: Rectangle [id:dp5192598873202616] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (276,78) -- (301.5,78) -- (301.5,102) -- (276,102) -- cycle ;
		%Shape: Rectangle [id:dp3549713084800885] 
		\draw   (301.5,78) -- (327,78) -- (327,102) -- (301.5,102) -- cycle ;
		%Shape: Rectangle [id:dp059348084498777265] 
		\draw   (276,102) -- (301.5,102) -- (301.5,119) -- (276,119) -- cycle ;
		%Shape: Rectangle [id:dp19020478677590646] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (301.5,102) -- (327,102) -- (327,119) -- (301.5,119) -- cycle ;
		%Shape: Rectangle [id:dp8911241643012155] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (347,44) -- (372.5,44) -- (372.5,62) -- (347,62) -- cycle ;
		%Shape: Rectangle [id:dp46870817922573416] 
		\draw   (372.5,44) -- (398,44) -- (398,62) -- (372.5,62) -- cycle ;
		%Shape: Rectangle [id:dp6180344884819471] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (347,62) -- (372.5,62) -- (372.5,78) -- (347,78) -- cycle ;
		%Shape: Rectangle [id:dp9974812374282085] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (372.5,62) -- (398,62) -- (398,78) -- (372.5,78) -- cycle ;
		%Shape: Rectangle [id:dp9091078684834291] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (347,78) -- (372.5,78) -- (372.5,102) -- (347,102) -- cycle ;
		%Shape: Rectangle [id:dp815738174696161] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (372.5,78) -- (398,78) -- (398,102) -- (372.5,102) -- cycle ;
		%Shape: Rectangle [id:dp6757211184948324] 
		\draw   (347,102) -- (372.5,102) -- (372.5,119) -- (347,119) -- cycle ;
		%Shape: Rectangle [id:dp6642393870926155] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (372.5,102) -- (398,102) -- (398,119) -- (372.5,119) -- cycle ;
		%Shape: Rectangle [id:dp3574624352627789] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (74,169) -- (98.5,169) -- (98.5,193) -- (74,193) -- cycle ;
		%Shape: Rectangle [id:dp13508518390422175] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (98.5,169) -- (123,169) -- (123,193) -- (98.5,193) -- cycle ;
		%Shape: Rectangle [id:dp6830840115089409] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (74,193) -- (98.5,193) -- (98.5,209) -- (74,209) -- cycle ;
		%Shape: Rectangle [id:dp30387879312324495] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (98.5,193) -- (123,193) -- (123,209) -- (98.5,209) -- cycle ;
		%Shape: Rectangle [id:dp7790274890743532] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (74,209) -- (98.5,209) -- (98.5,226) -- (74,226) -- cycle ;
		%Shape: Rectangle [id:dp17523683412821356] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (98.5,209) -- (123,209) -- (123,226) -- (98.5,226) -- cycle ;
		%Shape: Rectangle [id:dp11451088060330972] 
		\draw   (74,226) -- (98.5,226) -- (98.5,243) -- (74,243) -- cycle ;
		%Shape: Rectangle [id:dp7602957926883729] 
		\draw   (98.5,226) -- (123,226) -- (123,243) -- (98.5,243) -- cycle ;
		%Shape: Rectangle [id:dp3946171286416913] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (141,169) -- (165.5,169) -- (165.5,193) -- (141,193) -- cycle ;
		%Shape: Rectangle [id:dp3530045052626498] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (165.5,169) -- (190,169) -- (190,193) -- (165.5,193) -- cycle ;
		%Shape: Rectangle [id:dp326542189700344] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (141,193) -- (165.5,193) -- (165.5,209) -- (141,209) -- cycle ;
		%Shape: Rectangle [id:dp6622315199615636] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (165.5,193) -- (190,193) -- (190,209) -- (165.5,209) -- cycle ;
		%Shape: Rectangle [id:dp7499568787990554] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (141,209) -- (165.5,209) -- (165.5,226) -- (141,226) -- cycle ;
		%Shape: Rectangle [id:dp6914883439608812] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (165.5,209) -- (190,209) -- (190,226) -- (165.5,226) -- cycle ;
		%Shape: Rectangle [id:dp16927434768486793] 
		\draw   (141,226) -- (165.5,226) -- (165.5,243) -- (141,243) -- cycle ;
		%Shape: Rectangle [id:dp7971741362764997] 
		\draw   (165.5,226) -- (190,226) -- (190,243) -- (165.5,243) -- cycle ;
		%Shape: Rectangle [id:dp9351721564419304] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (211,169) -- (235.5,169) -- (235.5,193) -- (211,193) -- cycle ;
		%Shape: Rectangle [id:dp47788087437534243] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (235.5,169) -- (260,169) -- (260,193) -- (235.5,193) -- cycle ;
		%Shape: Rectangle [id:dp603554732958951] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (211,193) -- (235.5,193) -- (235.5,209) -- (211,209) -- cycle ;
		%Shape: Rectangle [id:dp32061179856665634] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (235.5,193) -- (260,193) -- (260,209) -- (235.5,209) -- cycle ;
		%Shape: Rectangle [id:dp13749729638661945] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (211,209) -- (235.5,209) -- (235.5,226) -- (211,226) -- cycle ;
		%Shape: Rectangle [id:dp7621267671822614] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (235.5,209) -- (260,209) -- (260,226) -- (235.5,226) -- cycle ;
		%Shape: Rectangle [id:dp25184316947891583] 
		\draw   (211,226) -- (235.5,226) -- (235.5,243) -- (211,243) -- cycle ;
		%Shape: Rectangle [id:dp012471258791993733] 
		\draw   (235.5,226) -- (260,226) -- (260,243) -- (235.5,243) -- cycle ;
		%Shape: Rectangle [id:dp002589522937559652] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (279,169) -- (303.5,169) -- (303.5,193) -- (279,193) -- cycle ;
		%Shape: Rectangle [id:dp05445871957141568] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (303.5,169) -- (328,169) -- (328,193) -- (303.5,193) -- cycle ;
		%Shape: Rectangle [id:dp7149427804957793] 
		\draw   (279,193) -- (303.5,193) -- (303.5,209) -- (279,209) -- cycle ;
		%Shape: Rectangle [id:dp8472869852865679] 
		\draw   (303.5,193) -- (328,193) -- (328,209) -- (303.5,209) -- cycle ;
		%Shape: Rectangle [id:dp7060776777216784] 
		\draw   (279,209) -- (303.5,209) -- (303.5,226) -- (279,226) -- cycle ;
		%Shape: Rectangle [id:dp08701628184787835] 
		\draw   (303.5,209) -- (328,209) -- (328,226) -- (303.5,226) -- cycle ;
		%Shape: Rectangle [id:dp15613144892149244] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (279,226) -- (303.5,226) -- (303.5,243) -- (279,243) -- cycle ;
		%Shape: Rectangle [id:dp371517948175764] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (303.5,226) -- (328,226) -- (328,243) -- (303.5,243) -- cycle ;
		%Shape: Rectangle [id:dp6581464436389022] 
		\draw   (349,169) -- (373.5,169) -- (373.5,193) -- (349,193) -- cycle ;
		%Shape: Rectangle [id:dp40206658404164397] 
		\draw   (373.5,169) -- (398,169) -- (398,193) -- (373.5,193) -- cycle ;
		%Shape: Rectangle [id:dp3140471721792333] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (349,193) -- (373.5,193) -- (373.5,209) -- (349,209) -- cycle ;
		%Shape: Rectangle [id:dp3120996325404337] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (373.5,193) -- (398,193) -- (398,209) -- (373.5,209) -- cycle ;
		%Shape: Rectangle [id:dp3180561433192146] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (349,209) -- (373.5,209) -- (373.5,226) -- (349,226) -- cycle ;
		%Shape: Rectangle [id:dp5193981651658108] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (373.5,209) -- (398,209) -- (398,226) -- (373.5,226) -- cycle ;
		%Shape: Rectangle [id:dp9490941946512168] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (349,226) -- (373.5,226) -- (373.5,243) -- (349,243) -- cycle ;
		%Shape: Rectangle [id:dp4063635012277762] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (373.5,226) -- (398,226) -- (398,243) -- (373.5,243) -- cycle ;
		%Shape: Rectangle [id:dp9051603135037152] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (74,281) -- (98.25,281) -- (98.25,299) -- (74,299) -- cycle ;
		%Shape: Rectangle [id:dp1643220536850809] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (98.25,281) -- (122.5,281) -- (122.5,299) -- (98.25,299) -- cycle ;
		%Shape: Rectangle [id:dp6897774238026873] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (74,299) -- (98.25,299) -- (98.25,315) -- (74,315) -- cycle ;
		%Shape: Rectangle [id:dp7168861019941448] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (98.25,299) -- (122.5,299) -- (122.5,315) -- (98.25,315) -- cycle ;
		%Shape: Rectangle [id:dp7108169214127738] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (74,315) -- (98.25,315) -- (98.25,332) -- (74,332) -- cycle ;
		%Shape: Rectangle [id:dp5496156551296494] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (98.25,315) -- (122.5,315) -- (122.5,332) -- (98.25,332) -- cycle ;
		%Shape: Rectangle [id:dp8283545794371432] 
		\draw   (74,332) -- (98.25,332) -- (98.25,349) -- (74,349) -- cycle ;
		%Shape: Rectangle [id:dp5310720317979363] 
		\draw   (98.25,332) -- (122.5,332) -- (122.5,349) -- (98.25,349) -- cycle ;
		%Shape: Rectangle [id:dp6171470119435183] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (141.25,298.5) -- (165.5,298.5) -- (165.5,315.5) -- (141.25,315.5) -- cycle ;
		%Shape: Rectangle [id:dp18762028811112597] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (165.5,298.5) -- (189.75,298.5) -- (189.75,315.5) -- (165.5,315.5) -- cycle ;
		%Shape: Rectangle [id:dp5973916746534238] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (141.25,315.5) -- (165.5,315.5) -- (165.5,332.5) -- (141.25,332.5) -- cycle ;
		%Shape: Rectangle [id:dp9159713730876722] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (165.5,315.5) -- (189.75,315.5) -- (189.75,332.5) -- (165.5,332.5) -- cycle ;
		%Shape: Rectangle [id:dp8350275781896841] 
		\draw   (141.25,332.5) -- (165.5,332.5) -- (165.5,349.5) -- (141.25,349.5) -- cycle ;
		%Shape: Rectangle [id:dp018938889805229175] 
		\draw   (165.5,332.5) -- (189.75,332.5) -- (189.75,349.5) -- (165.5,349.5) -- cycle ;
		%Shape: Rectangle [id:dp1509124245414919] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (213.5,298) -- (237.75,298) -- (237.75,315) -- (213.5,315) -- cycle ;
		%Shape: Rectangle [id:dp37698588683632583] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (237.75,298) -- (262,298) -- (262,315) -- (237.75,315) -- cycle ;
		%Shape: Rectangle [id:dp5919595602005328] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (213.5,315) -- (237.75,315) -- (237.75,332) -- (213.5,332) -- cycle ;
		%Shape: Rectangle [id:dp2811357093916005] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (237.75,315) -- (262,315) -- (262,332) -- (237.75,332) -- cycle ;
		%Shape: Rectangle [id:dp6819239463583058] 
		\draw   (213.5,332) -- (237.75,332) -- (237.75,349) -- (213.5,349) -- cycle ;
		%Shape: Rectangle [id:dp5743486344668638] 
		\draw   (237.75,332) -- (262,332) -- (262,349) -- (237.75,349) -- cycle ;
		%Shape: Rectangle [id:dp7330143311580186] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (284.25,298.5) -- (308.5,298.5) -- (308.5,315.5) -- (284.25,315.5) -- cycle ;
		%Shape: Rectangle [id:dp3707543051996136] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (308.5,298.5) -- (332.75,298.5) -- (332.75,315.5) -- (308.5,315.5) -- cycle ;
		%Shape: Rectangle [id:dp04283017757655583] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (284.25,315.5) -- (308.5,315.5) -- (308.5,332.5) -- (284.25,332.5) -- cycle ;
		%Shape: Rectangle [id:dp4056193722656616] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (308.5,315.5) -- (332.75,315.5) -- (332.75,332.5) -- (308.5,332.5) -- cycle ;
		%Shape: Rectangle [id:dp21747433225983026] 
		\draw   (284.25,332.5) -- (308.5,332.5) -- (308.5,349.5) -- (284.25,349.5) -- cycle ;
		%Shape: Rectangle [id:dp15637358206458085] 
		\draw   (308.5,332.5) -- (332.75,332.5) -- (332.75,349.5) -- (308.5,349.5) -- cycle ;
		%Shape: Rectangle [id:dp20976971875311268] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (355.5,298.5) -- (379.75,298.5) -- (379.75,315.5) -- (355.5,315.5) -- cycle ;
		%Shape: Rectangle [id:dp37759797815186924] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (379.75,298.5) -- (407.75,298.5) -- (407.75,315.5) -- (379.75,315.5) -- cycle ;
		%Shape: Rectangle [id:dp9294598147121356] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (355.5,315.5) -- (379.75,315.5) -- (379.75,332.5) -- (355.5,332.5) -- cycle ;
		%Shape: Rectangle [id:dp5230970019945533] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (379.75,315.5) -- (407.75,315.5) -- (407.75,332.5) -- (379.75,332.5) -- cycle ;
		%Shape: Rectangle [id:dp8578023983509857] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (355.5,332.5) -- (379.75,332.5) -- (379.75,349.5) -- (355.5,349.5) -- cycle ;
		%Shape: Rectangle [id:dp8913928860195823] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (379.75,332.5) -- (407.75,332.5) -- (407.75,349.5) -- (379.75,349.5) -- cycle ;
		%Shape: Rectangle [id:dp34409097285891366] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (74.75,382.5) -- (98.25,382.5) -- (98.25,405.5) -- (74.75,405.5) -- cycle ;
		%Shape: Rectangle [id:dp7362626101445264] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (98.25,382.5) -- (121.75,382.5) -- (121.75,405.5) -- (98.25,405.5) -- cycle ;
		%Shape: Rectangle [id:dp6285600939820672] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (74.75,405.5) -- (98.25,405.5) -- (98.25,422.5) -- (74.75,422.5) -- cycle ;
		%Shape: Rectangle [id:dp1675679934686587] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (98.25,405.5) -- (121.75,405.5) -- (121.75,422.5) -- (98.25,422.5) -- cycle ;
		%Shape: Rectangle [id:dp09755304930587849] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (74.75,422.5) -- (98.25,422.5) -- (98.25,439.5) -- (74.75,439.5) -- cycle ;
		%Shape: Rectangle [id:dp7911621188050444] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (98.25,422.5) -- (121.75,422.5) -- (121.75,439.5) -- (98.25,439.5) -- cycle ;
		%Shape: Rectangle [id:dp1573180111425394] 
		\draw   (74.75,439.5) -- (98.25,439.5) -- (98.25,456.5) -- (74.75,456.5) -- cycle ;
		%Shape: Rectangle [id:dp5272553532230633] 
		\draw   (98.25,439.5) -- (121.75,439.5) -- (121.75,456.5) -- (98.25,456.5) -- cycle ;
		%Shape: Rectangle [id:dp7674612678913348] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (142.25,382.5) -- (165.75,382.5) -- (165.75,405) -- (142.25,405) -- cycle ;
		%Shape: Rectangle [id:dp2451215474056745] 
		\draw  [fill={rgb, 255:red, 255; green, 194; blue, 221 }  ,fill opacity=1 ] (165.75,382.5) -- (189.25,382.5) -- (189.25,405) -- (165.75,405) -- cycle ;
		%Shape: Rectangle [id:dp8438616105240277] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (142.25,405) -- (165.75,405) -- (165.75,422) -- (142.25,422) -- cycle ;
		%Shape: Rectangle [id:dp24895524897606536] 
		\draw  [fill={rgb, 255:red, 231; green, 255; blue, 208 }  ,fill opacity=1 ] (165.75,405) -- (189.25,405) -- (189.25,422) -- (165.75,422) -- cycle ;
		%Shape: Rectangle [id:dp17006609992633215] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (213.75,422.5) -- (237.25,422.5) -- (237.25,439.5) -- (213.75,439.5) -- cycle ;
		%Shape: Rectangle [id:dp6477012629119447] 
		\draw  [fill={rgb, 255:red, 255; green, 249; blue, 183 }  ,fill opacity=1 ] (237.25,422.5) -- (260.75,422.5) -- (260.75,439.5) -- (237.25,439.5) -- cycle ;
		%Shape: Rectangle [id:dp6580268503033042] 
		\draw   (213.75,439.5) -- (237.25,439.5) -- (237.25,456.5) -- (213.75,456.5) -- cycle ;
		%Shape: Rectangle [id:dp7789191308028904] 
		\draw   (237.25,439.5) -- (260.75,439.5) -- (260.75,456.5) -- (237.25,456.5) -- cycle ;
		
		% Text Node
		\draw (446,64) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{Permutation}\\Randomization test};
		% Text Node
		\draw (446,192) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{Bootstrap}};
		% Text Node
		\draw (446,316) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{Jackknife}};
		% Text Node
		\draw (446,416) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{Cross validation}};
		% Text Node
		\draw (75,122) node [anchor=north west][inner sep=0.75pt]   [align=left] {sample};
		% Text Node
		\draw (141,122.4) node [anchor=north west][inner sep=0.75pt]    {$r=1$};
		% Text Node
		\draw (209,122.4) node [anchor=north west][inner sep=0.75pt]    {$r=2$};
		% Text Node
		\draw (277,122.4) node [anchor=north west][inner sep=0.75pt]    {$r=3$};
		% Text Node
		\draw (348,122.4) node [anchor=north west][inner sep=0.75pt]    {$r=4$};
		% Text Node
		\draw (75,246) node [anchor=north west][inner sep=0.75pt]   [align=left] {sample};
		% Text Node
		\draw (142,246.4) node [anchor=north west][inner sep=0.75pt]    {$r=1$};
		% Text Node
		\draw (212,246.4) node [anchor=north west][inner sep=0.75pt]    {$r=2$};
		% Text Node
		\draw (280,246.4) node [anchor=north west][inner sep=0.75pt]    {$r=3$};
		% Text Node
		\draw (350,246.4) node [anchor=north west][inner sep=0.75pt]    {$r=4$};
		% Text Node
		\draw (75,352) node [anchor=north west][inner sep=0.75pt]   [align=left] {sample};
		% Text Node
		\draw (142.25,351.9) node [anchor=north west][inner sep=0.75pt]    {$r=1$};
		% Text Node
		\draw (214.5,352.4) node [anchor=north west][inner sep=0.75pt]    {$r=2$};
		% Text Node
		\draw (284.5,351.9) node [anchor=north west][inner sep=0.75pt]    {$r=3$};
		% Text Node
		\draw (356.5,352.9) node [anchor=north west][inner sep=0.75pt]    {$r=4$};
		% Text Node
		\draw (76.75,459.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {sample};
		% Text Node
		\draw (138.75,459.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {training};
		% Text Node
		\draw (215.75,459.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {test};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Summary of resampling in different methods}
	\end{figure}
	
	\subsubsection{Monte Carlo Simulations}\label{monte carlo simulations}
	"Monte Carlo methods", also named "Monte Carlo experiments" or "Monte Carlo simulations"\index{Monte Carlo simulations} (MCS), are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. They are often used in physical and mathematical problems (finance, supply chain, decisioneering, quality, etc.) and are most useful as workaround when it is difficult or impossible to use other mathematical methods.
	
	It finds applications in various fields including the following examples:
	\begin{itemize}
		\item Problems related to the neutron bomb (or any other problem of  the same family: neutron diffusion)

		\item Calculations of integrals or various parameters of random variables (finance, insurance, risk, forecasting)
	
		\item Resolution of elliptic or parabolic equations

		\item Solving linear systems

		\item Optimization Problem Solving (operations research, project management, supply chain)

		\item Creation of statistical tests (Anderson-Darling, Kolmogorov, Levene, Brown-Forsythe, etc.)
	\end{itemize}
	Thus there are two types of problems that can be treated by the Monte Carlo probabilistic method:
	\begin{enumerate}
		\item Problems, which have a random behaviour 

		\item Deterministic problems, which do not have a random behaviour
	\end{enumerate}
	
	About the probabilistic case, the idea is to observe the behaviour of a series of random numbers that simulates how the real problem behaves and derive statistical solutions/conclusions. We then speak of "\NewTerm{Monte Carlo estimation}\index{Monte Carlo estimation}".

	In the deterministic case, the studied problem is completely defined and we can in principle predict its evolution, but some parameters of the problem can be treated as if it were random variables (this is typically the case in "vector regression" technique in Economy). The deterministic problem then becomes probabilistic and still solvable numerically. We then speak of "\NewTerm{Monte Carlo elaborated estimation}\index{Monte Carlo elaborated estimation}".
	
	We will begin further below with the most used one in business: generating draws from a probability distribution. For this purpose we will need to introduce the "inverse transform sampling" (we will treat mostly only univariate case in this book!).
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The name of "Monte Carlo method" dates around 11944 (holocene calendar). Isolated researchers have used however long before similar statistical methods: for example, Edwin Herbert Hall for the experimental determination of the speed of light (11873 according to holocene calendar), or Kelvin in a discussion of the Boltzmann equation (11901 according to holocene calendar), but the real use of Monte Carlo methods began with research on the atomic bomb.\\

	During the immediate postwar period, John Von Neumann, Enrico Fermi and Stanislaw Ulam warned the scientific community of the applicability of the Monte Carlo methods (eg for the approximation of the eigenvalues of the Schrödinger equation). The systematic study was made by Harris and Herman Khan in 11948 (holocene calendar). After an eclipse caused by too intensive use during the 11950s (holocene calendar), the Monte Carlo method is back since almost everybody can run complex management or business strategic simulations on office computers (with simple softwares like @Risk or CrystalBall).  in short, wherever it is profitable to use simulation processes.
	\end{tcolorbox}
	
	There are ways of using probabilities that are definitely not Monte Carlo simulations – for example, deterministic modelling using single-point estimates, commonly named "\NewTerm{what if analysis}\index{what-if analysis}". Each uncertain variable within a model is assigned a "best guess" estimate. Scenarios (such as best, worst, or most likely case) for each input variable are chosen and the results recorded. By contrast, Monte Carlo simulations sample from a probability distribution for each variable to produce hundreds, thousands or millions of possible outcomes. The results are analysed to get probabilities of different outcomes occurring. But what if analysis gives equal weight to all scenarios and that's their major issue!
	
	Also the reader should know that Monte Carlo simulation (MCS) and historical simulation (HS) are both methods that can be used to determine the riskiness of a financial project. However, each method uses different assumptions and techniques to develop the probability distribution of possible outcomes!

	"\NewTerm{Historical simulation}\index{historical simulation}" (HS) involves, as indicated by its name..., the use of historical records to simulate the possible outcomes. The method assumes that past performance is an indication of future performance. Historical simulation uses actual past figures or variables that have been experienced before. Monte Carlo simulation comes with the advantage of incorporating a wider variety of scenarios than historical data, whose information scope is limited. In addition, Monte Carlo simulation answers the what if question, weighted by probabilities (!), which is not possible under historical simulation.
	
	
	\pagebreak
	\paragraph{Inverse Transform Sampling}\label{inverse transform sampling}\mbox{}\\\\
	Inverse transform sampling (also known as inversion sampling,  inverse probability integral transform, inverse transformation method, Smirnov transform, golden rule,) is a basic method for pseudo-random number sampling, i.e. for generating sample numbers at random from any probability distribution given its cumulative distribution function.
	
	Inverse transformation sampling takes uniform samples of a number $\mathcal{U}$ between $0$ and $1$, interpreted as a probability, and then return the largest number $x$ from the domain of the distribution $P(X)$ such that $P(-\infty < X < x) \le \mathcal{U}$. 
	
	Computationally, this method involves computing the quantile function of the distribution — in other words, computing the cumulative distribution function (CDF) of the distribution (which maps a number in the domain to a probability between $0$ and $1$) and then inverting that function.
	
	To use this we method we go through he "\NewTerm{probability integral transform}\index{probability integral transform}" that states that if $X$ is a continuous random variable with cumulative distribution function $F_X$, then the random variable $Y=F_X(X)$ has a uniform distribution on $[0, 1]$. The inverse probability integral transform is just the inverse of this: specifically, if $Y$ has a uniform distribution on $[0, 1]$ and if $X$ has a cumulative distribution $F_X$, then the random variable $F_X^{-1}(Y)$ has the same distribution as $X$.
	\begin{theorem}
	Suppose that a random variable $X$ has a distribution for which the cumulative distribution function (CDF) is $F_X$. Then the random variable $Y$ defined as:
	
	has a uniform distribution.
	\end{theorem}
	\begin{dem}
	Given any random variable $X$, define $Y = F_X (X)$. Then (it is not always easy to read this proof through the first time even if afterwards it is obvious):
	
	Therefore $F_Y$ is just the CDF of the uniform random variable $\mathcal{U}[0,1]$. Thus, $Y$ has a uniform distribution on the interval $[0, 1]$ (or in other words, if $X$ is a random variable with CDF $F_X(X)$ then $F_X(X)=\mathcal{U}[0,1]$).
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	In other words, the percentiles are uniformly distributed (i.e. the frequency plot of the percentiles is uniformly distributed) as illustrated below:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1,scale=0.9]
		%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
		
		%Shape: Axis 2D [id:dp07222809238177952] 
		\draw  (22.5,286.1) -- (325.5,286.1)(52.8,62) -- (52.8,311) (318.5,281.1) -- (325.5,286.1) -- (318.5,291.1) (47.8,69) -- (52.8,62) -- (57.8,69) (103.8,281.1) -- (103.8,291.1)(154.8,281.1) -- (154.8,291.1)(205.8,281.1) -- (205.8,291.1)(256.8,281.1) -- (256.8,291.1)(307.8,281.1) -- (307.8,291.1)(47.8,235.1) -- (57.8,235.1)(47.8,184.1) -- (57.8,184.1)(47.8,133.1) -- (57.8,133.1)(47.8,82.1) -- (57.8,82.1) ;
		\draw   ;
		%Straight Lines [id:da3822750364089136] 
		\draw    (49,279) -- (57.5,279) ;
		%Straight Lines [id:da639798545550728] 
		\draw    (64,281) -- (64,292) ;
		%Shape: Axis 2D [id:dp5894445321938568] 
		\draw  (343.5,286.1) -- (646.5,286.1)(373.8,62) -- (373.8,311) (639.5,281.1) -- (646.5,286.1) -- (639.5,291.1) (368.8,69) -- (373.8,62) -- (378.8,69) (424.8,281.1) -- (424.8,291.1)(475.8,281.1) -- (475.8,291.1)(526.8,281.1) -- (526.8,291.1)(577.8,281.1) -- (577.8,291.1)(628.8,281.1) -- (628.8,291.1)(368.8,235.1) -- (378.8,235.1)(368.8,184.1) -- (378.8,184.1)(368.8,133.1) -- (378.8,133.1)(368.8,82.1) -- (378.8,82.1) ;
		\draw   ;
		%Straight Lines [id:da3883185225257295] 
		\draw    (370,279) -- (378.5,279) ;
		%Straight Lines [id:da9544532307672238] 
		\draw    (385,281) -- (385,292) ;
		%Shape: Rectangle [id:dp6354957211903365] 
		\draw  [fill={rgb, 255:red, 163; green, 205; blue, 255 }  ,fill opacity=1 ] (58,272) -- (91.5,272) -- (91.5,278) -- (58,278) -- cycle ;
		%Shape: Rectangle [id:dp5852434760057732] 
		\draw  [fill={rgb, 255:red, 163; green, 205; blue, 255 }  ,fill opacity=1 ] (91.5,195) -- (120.5,195) -- (120.5,278) -- (91.5,278) -- cycle ;
		%Shape: Rectangle [id:dp4355401311606635] 
		\draw  [fill={rgb, 255:red, 163; green, 205; blue, 255 }  ,fill opacity=1 ] (121.5,143) -- (152.5,143) -- (152.5,278) -- (121.5,278) -- cycle ;
		%Shape: Rectangle [id:dp8249643629396173] 
		\draw  [fill={rgb, 255:red, 163; green, 205; blue, 255 }  ,fill opacity=1 ] (153.5,81) -- (183.5,81) -- (183.5,278) -- (153.5,278) -- cycle ;
		%Shape: Rectangle [id:dp24954066022230248] 
		\draw  [fill={rgb, 255:red, 163; green, 205; blue, 255 }  ,fill opacity=1 ] (184.5,97) -- (214.5,97) -- (214.5,278) -- (184.5,278) -- cycle ;
		%Shape: Rectangle [id:dp426749480196738] 
		\draw  [fill={rgb, 255:red, 163; green, 205; blue, 255 }  ,fill opacity=1 ] (215.5,169) -- (247.5,169) -- (247.5,278) -- (215.5,278) -- cycle ;
		%Shape: Rectangle [id:dp29514051593647483] 
		\draw  [fill={rgb, 255:red, 163; green, 205; blue, 255 }  ,fill opacity=1 ] (248.5,191) -- (280.5,191) -- (280.5,278) -- (248.5,278) -- cycle ;
		%Shape: Rectangle [id:dp5128027137940117] 
		\draw  [fill={rgb, 255:red, 163; green, 205; blue, 255 }  ,fill opacity=1 ] (281.5,240) -- (313.5,240) -- (313.5,278) -- (281.5,278) -- cycle ;
		%Curve Lines [id:da8356482652929706] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (61.5,267) .. controls (121.5,237) and (146.5,84) .. (183.5,81) .. controls (220.5,78) and (255.5,234) .. (308.5,266) ;
		%Shape: Rectangle [id:dp21395303462214366] 
		\draw  [fill={rgb, 255:red, 163; green, 205; blue, 255 }  ,fill opacity=1 ] (381.5,122) -- (411.5,122) -- (411.5,279) -- (381.5,279) -- cycle ;
		%Shape: Rectangle [id:dp47758901159526523] 
		\draw  [fill={rgb, 255:red, 163; green, 205; blue, 255 }  ,fill opacity=1 ] (412.36,95) -- (442.36,95) -- (442.36,279) -- (412.36,279) -- cycle ;
		%Shape: Rectangle [id:dp7391937853481323] 
		\draw  [fill={rgb, 255:red, 163; green, 205; blue, 255 }  ,fill opacity=1 ] (443.22,82) -- (473.22,82) -- (473.22,279) -- (443.22,279) -- cycle ;
		%Shape: Rectangle [id:dp8777578191876132] 
		\draw  [fill={rgb, 255:red, 163; green, 205; blue, 255 }  ,fill opacity=1 ] (474.08,155) -- (504.08,155) -- (504.08,279) -- (474.08,279) -- cycle ;
		%Shape: Rectangle [id:dp733679432737538] 
		\draw  [fill={rgb, 255:red, 163; green, 205; blue, 255 }  ,fill opacity=1 ] (504.94,89) -- (534.94,89) -- (534.94,279) -- (504.94,279) -- cycle ;
		%Shape: Rectangle [id:dp6031290101180578] 
		\draw  [fill={rgb, 255:red, 163; green, 205; blue, 255 }  ,fill opacity=1 ] (535.8,128) -- (565.8,128) -- (565.8,279) -- (535.8,279) -- cycle ;
		%Shape: Rectangle [id:dp018978638975898576] 
		\draw  [fill={rgb, 255:red, 163; green, 205; blue, 255 }  ,fill opacity=1 ] (566.66,169) -- (596.66,169) -- (596.66,279) -- (566.66,279) -- cycle ;
		%Shape: Rectangle [id:dp695478031722393] 
		\draw  [fill={rgb, 255:red, 163; green, 205; blue, 255 }  ,fill opacity=1 ] (597.5,83) -- (627.5,83) -- (627.5,279) -- (597.5,279) -- cycle ;
		
		% Text Node
		\draw (38,266.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (5,67.4) node [anchor=north west][inner sep=0.75pt]    {$0.235$};
		% Text Node
		\draw (58,296.4) node [anchor=north west][inner sep=0.75pt]    {$20$};
		% Text Node
		\draw (295,296.4) node [anchor=north west][inner sep=0.75pt]    {$76$};
		% Text Node
		\draw (359,266.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (331,67.4) node [anchor=north west][inner sep=0.75pt]    {$0.15$};
		% Text Node
		\draw (379,296.4) node [anchor=north west][inner sep=0.75pt]    {$0.25$};
		% Text Node
		\draw (610,296.4) node [anchor=north west][inner sep=0.75pt]    {$99.5$};
		% Text Node
		\draw (371,28) node [anchor=north west][inner sep=0.75pt]   [align=left] {Percentiles};
		% Text Node
		\draw (147,306) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {reading score};
		% Text Node
		\draw (496,306) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {pctrank};
		
		\end{tikzpicture}
	\end{figure}
	
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Caution! In NHST (Null Hypothesis Statistical Tests) the $p$-value itself is considered as a realization of random variable. Therefore if we replace the random variable $X$ above by the random variable $p$, we get the famous result that $p$-value is uniformly distributed when the null hypothesis is true (indeed under the null $H_0$ there is no reason that the value of $p$ is more located in given place than in another one \underline{by definition}!).
	\end{tcolorbox}
	
	The problem that the inverse transform sampling method solves is as follows:
	\begin{itemize}
		\item Let $X$ be a random variable whose distribution can be described by the cumulative distribution function $F_X$.
		\item We want to generate values of $X$ which are distributed according to this distribution.
	\end{itemize}
	The inverse transform sampling method works as follows:
	\begin{enumerate}
		\item Generate a random number $\mathcal{U}$ from the standard uniform distribution in the interval $[0,1]$.
		\item Compute the value $x$ such that $F_X(x) =\mathcal{U}$ (using $F_X^{-1}(\mathcal{U})$).
		\item Take $x$ to be the random number drawn from the distribution described by $F_X$.
	\end{enumerate}
	Expressed differently, given a continuous uniform variable $\mathcal{U}$ in $[0, 1]$ and an invertible cumulative distribution function $F_X$, the random variable $X = F_X^{-1}(\mathcal{U})$ has distribution $F_X$ (or, $X$ is distributed $F_X$).
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.9]{img/computing/fundamental_theorem_monte_carlo.pdf}
		\caption{Principle of the inverse transform method}
	\end{figure}
	
	\pagebreak
	\paragraph{Random number generation}\mbox{}\\\\
	The best way to understand the method of Monte Carlo is to make examples (even small one should be enough). But for this, we must first have a good random number generator (which is quite difficult depending on the job). This is a very delicate and sensitive field for which an international standards is published (ISO 28640:2010 \textit{Random variate generation methods}).
	
	A random-number generator (RNG) is a computational or physical device designed to generate a sequence of numbers or symbols that cannot be reasonably predicted better than by a random chance.
	
	Several computational methods for random-number generation exist. Many fall short of the goal of true randomness, although they may meet, with varying success, some of the statistical tests for randomness intended to measure how unpredictable their results are.
	
	There are two principal methods used to generate random numbers. The first method measures some physical phenomenon that is expected to be random and then compensates for possible biases in the measurement process. Example sources include measuring atmospheric noise, thermal noise, and other external electromagnetic and quantum phenomena. For example, cosmic background radiation or radioactive decay as measured over short time-scales represent sources of natural entropy.
	
	The second method uses computational algorithms that can produce long sequences of apparently random results, which are in fact completely determined by a shorter initial value, known as a seed value or key. As a result, the entire seemingly random sequence can be reproduced if the seed value is known. This type of random number generator is often named a "pseudo-random number generator". This type of generator typically does not rely on sources of naturally occurring entropy, though it may be periodically seeded by natural sources. This generator type is non-blocking, so they are not rate-limited by an external event, making large bulk reads a possibility.
	
	Let us take, to begin, an example with the Maple 4.00b random generator:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/maple_random_generator.jpg}
		\caption{Pseudo-random generator in Maple 4.00b}
	\end{figure}
	and:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/maple_random_generator_restart.jpg}
		\caption[]{Pseudo-random generator restart with Maple 4.00b}
	\end{figure}
	So we see that the default random number generator used by default in Maple 4.00b should be used with extreme caution since a system reset is enough to find... equal random values! This is therefore as we already said "\NewTerm{pseudo-random generator}\index{pseudo-random generator}" that gives the possibility to make what we name "\NewTerm{pseudo Monte Carlo method}\index{pseudo Monte Carlo method}".
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/maple_random_generator_special_library.jpg}
		\caption[]{Pseudo-random library with Maple 4.00b}
	\end{figure}
	The \texttt{RAND( )} and \texttt{RANDBETWEEN( )} functions of the of Microsoft Excel 14.0.6123 are also pseudo-random generators which here is a sample of 100 simulations (of course in Microsoft Excel the chart below will change each time you press on the keyboard button: F9):
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/random_generator_plot_excel.jpg}
		\caption[]{Illustration of a sequence of pseudo-random numbers with Microsoft Excel 14.0.6123}
	\end{figure}
	Unfortunately, it may happen with pseudo-random numbers that the numbers generated are presented in bunches, that is to say by sets of numbers close to each other, which reduces the effectiveness of the Monte Carlo simulation .

	An empirical technique is to use then sequences of numbers generated by algorithms that scan almost surely the range $[0,1]$. We speak then of "\NewTerm{quasi-random numbers}\index{quasi-random numbers}" to make simulations sometimes named "\NewTerm{quasi-Monte Carlo}\index{quasi-Monte Carlo}". In almost all Microsoft Excel, you can create a Visual Basic Application function that will replace the pseudo-random generators that are the \texttt{RAND( )} or \texttt{RANDBETWEEN( )}.

	Here is an example of such a V.B.A. function which generates quasi-random number named "\NewTerm{Fauré random number sequence}\index{Fauré random number sequence}":
	
	\begin{lstlisting}[language={[Visual]Basic}, caption={VBA Fauré sequence code}]
		Function SequenceFaure(n) As Double
    		Dim f As Double, sb As Double
    		Dim i As Integer, n1 As Integer, n2 As Integer
    
    		n1 = n
    		sb = 1 / 2
    		Do While n1 > 0
		        n2 = Int(n1 / 2)
		        i = n1 - n2 * 2
		        f = f + sb * i
		        sb = sb / 2
		        n1 = n2
		    Loop
		    SequenceFaure = f
		End Function
	\end{lstlisting}
	This will gives the following sequence for a sample of $100$ simulations:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/faure_pseudo_random_sequence_excel.jpg}
		\caption[]{Illustration of a Fauré sequence of pseudo-random numbers with Microsoft Excel 14.0.6123}
	\end{figure}
	where we see well that the sequence covers well the whole area between $0$ and $1$ (we say then that: it covers faster the integration surface). This technique is sometimes preferred because it has the advantage of keeping the values off the simulation every time we restart the simulation (therefore in Microsoft Excel the chart above will not change when you press the keyboard button F9).

	By cons the sequence generators have a great weakness: they are only applicable (to my knowledge at least) for problems of simulations with a single random variable (typically pricing single option strategy following Black \& Scholes model). Indeed if we have several random variables (and this is the most common case!), then the variables are artificially correlated (correlation coefficient = $1$) because they travel all the area between $0$ and $1$ in the same way! So a good simulation with several variables is a simulation including the treated variables have a correlation coefficient which approaches zero!!!!!!

	In addition, sequence generators require algorithms that are very time consuming when there are many variables relatively to a pseudo-random generator, this is why in most situations we prefer the old methods.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	As we have already mentioned it, engineers should refer to the international standard ISO 28640:2010 \textit{Random variate generation methods} when they need to implement random number generators in their softwares.
	\end{tcolorbox}
	Once the pseudo-random or random generator created and tested, we can see some application of the Monte Carlo method before continuing on performance tricks relatively to this method. Thus, in the calculation of integrals, this method is very useful and very fast in terms of convergence speed.
	
	\pagebreak
	\paragraph{Monte Carlo integration}\label{monte carlo integration}\mbox{}\\\\
	In numerical integration, methods such as the Trapezoidal rule use a deterministic approach as we already know. Monte Carlo integration, on the other hand, employs a non-deterministic approaches: each realization provides a different outcome. In Monte Carlo, the final outcome is an approximation of the correct value with respective error bars, and the correct value is within those error bars.
	
	Consider for example the calculation of the following univariate defined integral of a function $f$ and positive over the interval $[a, b]$:
	
	Given:
	
	the maximal value of the function $f$ between the bounds $[a,b]$.

	We consider the rectangle bounding function on the interval $[a, b]$ defined by vertices $\{(a,0),(b,0),(b,m),(a,m)\}$:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[
	    declare function={a(\x)=(x-3)*(x-2)*(x-1)*x*(x+1)*(x+2)*(x+3)+120;},
		]
		\begin{axis}[
			xlabel={$x$},
		    ylabel={$y$},
		    xmin=-1, xmax=4.3,
		    ymin=0, ymax=220,
		    axis lines=middle,
		    samples=100,
		    domain=-1:4.2,
		    scale=1.5 
		]
		%the function
		\addplot [very thick, blue] {a(x)};
		%the random points
		\addplot [red!50, only marks, mark=*, samples=300, mark size=0.75, samples at={0.505,0.51,...,3.0}]
		    {0.5*(a(x)+0) + 0.5*rand*(a(x)-0)};
		\addplot [gray!50, only marks, mark=*, samples=300, mark size=0.75, samples at={0.505,0.51,...,3.0}]
		    {0.5*(a(x)+145) + 0.5*rand*(a(x)-145)};
		\end{axis}
		%the vertical lines
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (2.9,0) -- (2.9,7) ;
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (7.8,0) -- (7.8,7) ;
		%horizontal dashed line
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (1.5,5.6) -- (8.5,5.6) ;
		%legends
		% Text Node
		\draw (2.6,0.3) node [anchor=north west][inner sep=0.75pt]{$a$};
		\draw (7.9,0.4) node [anchor=north west][inner sep=0.75pt]{$b$};
		\draw (8,6.5) node [anchor=north west][inner sep=0.75pt][color=blue] {$f(x)$};
		\draw (4,6.2) node [anchor=north west][inner sep=0.75pt] {$\max (f(a),f(b))$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Basic principle of the univariate integral calculation with Monte Carlo}
	\end{figure}
	We draw a large number $N$ of random points in this rectangle. For each point, $\xi_i=(x,y)$ we test if it is below the blue curve. Given $P$ the proportion of points below this curve, we have:
	
	This approach seems to be sometimes named the "\NewTerm{Monte Carlo Stochastic Point Method}" or \NewTerm{Monte Carlo hit-or-miss approach}" or "\NewTerm{Monte Carlo poor man's sampling}".
	
	The corresponding Maple 4.00b algorithm  is given by:
	
	\texttt{>intmonte:=proc(f,a,b,N)}\\
	\texttt{local i,al,bl,m,P,aleaabs,aleaord,isabove;}\\
	\texttt{m:=round(max(f(a),f(b))*10\string^4);}\\
	\texttt{al:=round(a*10\string^4);}\\
	\texttt{bl:=round(b*10\string^4);}\\
	\texttt{aleaabs:=rand(al..bl);}\\
	\texttt{aleaord:=rand(0..m);}\\
	\texttt{k:=0;}\\
	\texttt{for i from 1 to N do}\\
	\texttt{     isabove:=(f(aleaabs()/10\string^4)-aleaord()/10\string^4)>=0;}\\
	\texttt{     if isabove then}\\
	\texttt{          k:=k+1;}\\
	\texttt{     fi}\\
	\texttt{od:}\\
	\texttt{RETURN((b-a)*max(f(a),f(b))*k/N)}\\
	\texttt{end:}\\
	
	To call this procedure in Maple, just write \texttt{>intmonte(f,a,b,N)} but replacing the first argument passed as a parameter with the expression of a function and the other arguments by numerical values (obviously!).
	
	Notice that the relation above can be rewritten:
	
	where $\hat{p}$ is the estimated parameter of a Bernoulli distribution and the corresponding random variable is $\xi_i$ if we reject or accept the generated point:
	
	Then  $\{\xi_i\}$ are outcomes of independent duplicate trials and $\xi_1,\cdots,\xi_N\overset{i.i.d.}{\sim}\text{B}(1,\hat{p})$. So if $\{\xi_i\}$ follows a Bernoulli distribution, then:
	
	follows a binomial distribution $\mathcal{B}(N,k)$.
	
	We know that the mean standard deviation of a variable (without factors) following a binomial distribution is:
	
	But here we have a factor $(b-a)m/N$, so that the mean becomes:
	
	and the standard deviation will be:
	
	The both latter relation are often written in textbooks by considering (implicitly) that the range of integration has been transformed to have an interval $a=0,b=1$ and the function transformed such that maximum $m=1$. Therefore:
	
	To estimate the variance (or precision), we use the Central Limit Theorem and get\footnote{Remember that the denominator is the standard error of the mean $\sigma_\mu=\sigma/\sqrt{N}$} (don't forget that this relation is valid only if the integration has been transformed to have an interval $a=0,b=1$ and the function transformed such that maximum $m=1$):
	
	That's a useful result to build confidence interval for that Monte Carlo method.
	
	Let us consider another very common approach named "\NewTerm{Monte Carlo mean value method}" (m.v.) or "\NewTerm{Monte Carlo uniform sampling method}"!

	Let $g(x)$ be a function and suppose that we want to compute:
	
	Recall that if $X$ is a random variable with density $f(x)$, then the mathematical expectation of the random variable $Y=g(X)$ between $0$ and $1$ is given by:
	
	If a random sample is available from the distribution of $X$, an unbiased estimator of $\text{E}(g(X))$ is the sample mean:
	
	If we take a distribution $f(X^{(i)}_{[0,1]})$ that is equally likely between $0$ and $1$ like the Uniform distribution $\mathcal{U}_{[0,1]}$, then the above relation reduces to:
	
	To compute $\int_{a}^{b} g(t) \mathrm{d}t$ we make a change of variables so that the limits of integration are from $0$ to $1$. The linear transformation is $y=(t-a) /(b-a)$ and $\mathrm{d}t=(b-a)\mathrm{d}y$. Therefore:
	
	Hence:
	
	By the fundamental theorem of Monte Carlo (\SeeChapter{see section Statistics page \pageref{fundamental theorem of Monte Carlo}}) it converges with probability $1$.
	
	Notice also that by the Central Limit Theorem we have that the standard error of $I_{\text{m.v.}}$ is given by:
	
	Or using Huygens relation, given for recall by $\text{V}(X)=\text{E}(X^2)-\text{E}(X)^2$, we get:
	
	This is useful to put confidence limits or error bounds on the Monte Carlo estimate of the integral, and check for convergence.

	However, the m.v. method (Monte Carlo mean value method) has two major limitations:
	\begin{itemize}
		\item It does not apply to unbounded intervals!
		
		\item It can be inefficient to draw samples uniformly across the interval if the function $g(x)$ is not very uniform!
	\end{itemize}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The reader can refer to our \texttt{R} or MATLAB™ free companion books to see examples with the respective script language of these two softwares for the both method presented above.
	\end{tcolorbox}
	
	Now let us just compare:
	
	Moreover, many textbooks claim that:
	
	But this is neither true nor false!
	
	\paragraph{Monte Carlo estimation of $\pi$}\mbox{}\\\\
	For the calculation of $\pi$ the principle is the same and therefore consist to use the proportion of the number of points in a quarter of circle area (this simplifies the algorithm by restricting the calculations to strictly positive coordinates) inscribed in a square of side $1$ (so the radius of the circle is also equal to $1$ obviously) relatively to the total number of points (to test if a point is outside the circle, we obviously use the Pythagorean theorem) such that:
	
	Because we have for the number of points $P$ inside the quarter or circle that corresponds to a surface of $\pi R^2/4$ and the number of points $N$ inside surface of the quarter of square that corresponds to a surface $R^2$. So the ratio of both should tend to $\pi/4$ when $N\rightarrow +\infty$. We just have to multiply by $4$ to get a good approximation of $\pi$.
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/monte_carlo_pi.jpg}
		\caption{Monte Carlo $\pi$ estimate}
	\end{figure}
	
	That method is equivalent to defining an indicator variable $I$ as:
	\[I=\left\{\begin{array}{ll}
	1 & \mbox{if } X^{2}+Y^{2}\leq 1\\
	0 & \mbox{otherwise}
	\end{array}\right.\]
	where $X,Y= \mathcal{U}_{[0,1]}$. Where that:
	\begin{gather*}
		\text{E}(I)=\dfrac{\pi}{4} \quad \text{and}\quad \text{V}(I)=\frac{\pi}{4}\left(1-\frac{\pi}{4}\right)
	\end{gather*}
	
	The corresponding Maple 4.00b algorithm is given by:\\

	\texttt{>isinside:=proc(x,y) x\string^2+y\string^2<1 end:}\\
	\texttt{>calculatepi:=proc(N)}\\
	\texttt{local i,P,abs,ord,alea;}\\
	\texttt{alea:=rand(-10\string^4..10\string^4);}\\
	\texttt{P:=0;}\\
	\texttt{for i from 1 to N do}\\
	\texttt{     abs:=alea()/10\string^4;ord:=alea()/10\string^4;}\\
	\texttt{       if isinside(abs,ord) then}\\
	\texttt{            P:=P+1;}\\
	\texttt{       fi}\\
	\texttt{od;}\\
	\texttt{RETURN(4*P/N)}\\
	\texttt{end:}\\
	\texttt{>evalf(calculatepi(100));evalf(calculatepi(1000));\\evalf(calculatepi(10000));evalf(calculatepi(100000));}\\
	

	In terms of convergence it looks typically as:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/monte_carlo_pi_convergence.jpg}
	\end{figure}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The reader can refer again to our \texttt{R} or MATLAB™ free companion books to see examples with the respective script language of these two softwares for the example above.
	\end{tcolorbox}
	
	\paragraph{Monte Carlo Modelling}\mbox{}\\\\
	The most common application of the Monte Carlo method in business and industry is certainly the study of random variables. Furthermore, this method is part of the ISO 31010 Risk Management standard under the name of "\NewTerm{Monte Carlo analysis}\index{Monte Carlo analysis}". Many cutting edge tech companies make Monte Carlo modelling with a spreadsheet softwares like Microsoft Excel (even multinationals!) and with a lesser extent with professional oriented softwares or add-ins such as @RISK, CrystalBall, TreeAge, Isograph or MATLAB™ (without taking into account C, C++, Java, R, and Python obviously).
	
	The advantages of this method in modelling random variables are:
	\begin{itemize}
		\item We can integrate any distribution  including empirical one and not continuous one!

		\item Models are very easy to implement and can be expanded as needed without too much effort.

		\item All influences or relation occurring in reality (at least the identified one....) may be represented and implemented in the model.

		\item Sensitivity analysis (\SeeChapter{see section Quantitative Management Techniques page \pageref{sensitivity analysis}}) can be applied.

		\item The models are easily understandable and provide a measure of the accuracy of the result.

		\item Many inexpensive software are available (at least inexpensive in comparison to criticality of the business analysed that is most of time in the order of the billion of dollars).
	\end{itemize}
	Let us consider a simple but concrete case (widely used in business) that I like to use in my introduction training of a small project of two tasks denoted by $A$ and $B$ which follow each other without free margin (or free slack). Let us imagine that the duration of each task has been estimated in accordance with the recommendation of the Project Management Institute with a beta distribution (\SeeChapter{see section Statistics page \pageref{beta distribution}}) as learn it almost all good project managers in their training curriculum (\SeeChapter{see section Quantitative Management Techniques page \pageref{probabilitic pert}}).

	For this example, the task $A$ has an optimistic duration of $5$ days and a pessimistic duration of $8$ days. Task $B$ an optimistic duration of $1$ day and a pessimistic duration of $4$ days. We would like in the spreadsheet software Microsoft Excel using a pseudo Monte Carlo simulation (therefore necessarily based on a pseudo-random variable) introduce three traditional minimum information:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/monte_carlo_tasks.jpg}
	\end{figure}
	\begin{itemize}
		\item A table with $3$ columns (duration of $A$, $B$ and sum of both) and $10,000$ simulations (rows)
	
		\item The graphical distribution function of the sum of the two random variables 

		\item The convergence of the 95th percentile on the $100$ first simulations (useful for the subject further below).
	\end{itemize}
	We then construct the following table of $10,000$ row (the screenshot shows only the first rows...) where all cells from row $2$ to row $10,000$ of column \texttt{A} contains the following function (Microsoft Excel 14.0.7166):
	\begin{center}
	\texttt{=BETA.INV(RAND(),3+SQRT(2),3-SQRT(2),5,8)}
	\end{center}
	and for column \texttt{B} contains the following function:
	\begin{center}
	\texttt{=BETA.INV(RAND(),3+SQRT(2),3-SQRT(2),1,4)}
	\end{center}
	and finally the cell \texttt{C1} contains the following function that was pull down until cell \texttt{C10000}:
	
	\begin{center}
	\texttt{=A2+B}
	\end{center}
	
	Obviously the values in Microsoft Excel 14.0.6123 will change each time you press the \texttt{F9} key on the keyboard.

	Then this gives us the histogram still made with the same software version (we will not detail how to building such a chart it is a basic subject of Microsoft Office knowledge and has nothing to do in a scientific book) where the $x$-axes are the number of days:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/monte_carlo_histogram_tasks.jpg}
	\end{figure}
	and the convergence of the $95$th percentile of the first $100$ simulations (because as this example is simple, the system converges quickly enough so that we do not to need to take more than $100$ simulations as example) where the $y$-axes is the number of days:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/monte_carlo_convergence_tasks.jpg}
	\end{figure}
	Obviously by default, in Microsoft Excel 14.0.6123 the chart above will change each time you press the \texttt{F9} key on the keyboard.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In the case of the simulations of random variables, we can in simple situations involving only sums or subtractions of random variables, as for the example above, determine the mean and the standard deviation of the results analytically using the property of linearity of the mean and variance (because normally for the variance of two independent random variables, the covariance is zero). By analysing the difference between the analytical value and that obtained by numerical simulation, the offset can be corrected certain other statistical indicators by simply adding or subtracting the differential. This is known as the technique of "\NewTerm{control variables}\index{control variables}" that we will detail further below.
	\end{tcolorbox}
	There are other variance reduction techniques (i.e. standard deviation) than control variables techniques to reduce the variance of the Monte Carlo estimators in specific conditions:

	\begin{itemize}
		\item One of these techniques is the use of "\NewTerm{antithetic variables}\index{antithetic variables}" which consists very simply (programming this technique is at the high school level as you can see it in the MATLAB™ companion book) to decorrelate simulations to make the covariance between the variables negative  and so reduce the global variance (such as we have seen in the section Statistics, the variance of the sum of two random variables make appear a covariance term). Unfortunately, this technique works satisfactorily with symmetric distributions this is why to my knowledge it is not implemented in simulation software available on the market.

		\item There is also the technique named "\NewTerm{stratified sampling}\index{stratified sampling}" that consist to cut the  pre-image space of the random variable in regular intervals (the programming of this technique is also at the high school level as you can see in the MATLAB™ companion book). This technique works very well when the number of simulations must be small but only in the case of a single variable. This also why, as far as we I know, it is not implemented in simulation softwares available on the market.

		\item There exist is a generalization of stratified sampling (the programming of this technique is also at the high school level) for simulations with multiple variables and that is named "\NewTerm{Latin Hypercube}\index{Latin Hypercube}" (abbreviated as "LHS" for Latin Hypercube Stratification). This technique ensures that each $n$-tuple of random variables (corresponding to a space of $n$ dimensions) uses a unique pre-image at each iteration, hence the name of that technique (Latin: refers to magic squares where each value appears uniquely, Hypercube because is an $n$-dimensional generalization of a magic square). Some simulation software available on the market implement this technique (@RISK, CrystalBall).
	\end{itemize}
	To summarize, whether that it's the technique of Faure sequence generators, of antithetic variables, of control variables, of stratified sampling or Latin Hypercube even if these techniques are all easy to program, the method using the pseudo-random variables is privileged because is the most suitable for the majority of common situations in the business and in non-cutting edge scientific applications.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	For the reader interested to have a deeper overview, however with out too much mathematical details, we recommend the following actual reference: \textit{Introducing Monte Carlo Methods with R} (see \cite{robert2010introducing}).
	\end{tcolorbox}
	
	\paragraph{Monte Carlo variance reduction}\label{Monte Carlo variance reduction}\mbox{}\\\\\
	What we have seen so far is only what we may name the "\NewTerm{crude Monte Carlo simulation}". We will now introduce the problem of reducing the variance of the samples that are generated to  estimate the mean value $\lambda$ of some random variable $X$ using Monte Carlo method. The main underlying idea is to reduce the standard error and increasing the confidence in the accuracy of the estimate (very important in speculative finance and nuclear or quantum physics!). We will see further below four variance-reduction methods (among seven existing one). 
	
	Suppose one wants to compute $z:=\text{E}(Z)$ with the random variable $Z$. As we know, Monte Carlo does this by sampling i.i.d. copies $Z_{1}, \ldots, Z_{R}$ of $Z$ and then to estimate $z$ via the sample-mean estimator:
	
	Under further mild conditions such as $\text{V}(Z)<+\infty$, a central limit theorem will apply such that for large $n \rightarrow +\infty$, the distribution of $\bar{z}$ converges to a Normal distribution with mean $z$ and standard deviation $\sigma / \sqrt{n}$. Because the standard deviation only converges towards $0$ at the rate $\sqrt{n}$. implying one needs to increase the number of simulations $(n)$ by a factor of $10$ to half the standard deviation of $\bar{z}$, variance reduction methods are often useful for obtaining more precise estimates for $z$ without needing very large numbers of simulations.

	There are commonly seven variance reduction methods:
	\begin{enumerate}
		\item Antithetics  
		
		\item Stratification  
		
		\item Quasi-random method
		
		\item Conditioning (conditional sampling)
		
		\item Control variates
		
		\item Moment matching and re-weighting
		
		\item Monte Carlo Markov Chains
	\end{enumerate}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	For practical examples with explicit code (scripts) the reader can refer to our MATLAB™ or \texttt{R} free companion book (actually the MATLAB™ one has the most interesting examples).
	\end{tcolorbox}
	
	As already mentioned, will focus here on four of them:
	\begin{enumerate}
		\item \textbf{Antithetic variables}: Instead of a single sample sequence $X_{1},\ldots,X_{n}$ sampled from  $X$, this method instead uses an additional variable $Y$ for which $\text{E}(Y)=\text{E}(X)=\lambda$, but is negatively correlated with $X$. The $i$th sample now becomes $\frac{X_{i}+Y_{i}}{2}$.
		
		\item \textbf{Control variables}: A control variable is a random variable $Y$ for which $\text{E}[Y]=\mu$ is known, and for which sampling variable $X$ is replaced with $X+c(Y-\mu)$, where $c$ is chosen so as to minimize $\text{V}(X+c(Y-\mu))$.

		\item \textbf{Conditional sampling}: Conditional sampling selects a suitable variable $Y$ for which $\text{E}(X|Y)$ has a smaller variance than $X$. $Y$ is chosen so that, when it is observed, the uncertainty (i.e. entropy) of $X$ gets reduced. 
		
		\item \textbf{Stratified (Regional) sampling}: Stratified sampling is a divide-and-conquer strategy that partitions the domain of $X$ into subdomains, and then a sampling plan is devised for each subdomain. Moreover, common sense suggests that the smaller the subdomain, the less variance that should exist in that region, and so a variance reduction may be achievable in each of the subdomains that comprise the original domain.
	\end{enumerate}
	
	 And let us recall that if $X$ and $Y$ are two random variables, then we have proved in the section Statistics (see page \pageref{covariance}) that:
	 
	Ok let's start!
	
	\subparagraph{Method of Antithetic Variables}\mbox{}\\\\\
	Suppose $X$ and $Y$ are random variables for which $\text{E}(Y)=\text{E}(X)=\lambda$. Then:
	
	Moreover:
	
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In the special case of $X=Y$ we fall back obviously on:
	\begin{gather*}
		\text{E}\left(\frac{X+Y}{2}\right)=\text{E}\left(\frac{X+X}{2}\right)=\text{E}\left(X\right)=\lambda
	\end{gather*}
	and:
	\begin{gather*}
		\text{V}\left(\frac{X+X}{2}\right)=\text{V}\left(X\right)=\dfrac{1}{4}\left[\text{V}(X)+\text{V}(X)+2\text{V}(X)\right]
	\end{gather*}
	\end{tcolorbox}

	Notice that, if $X$ and $Y$ are dependent and have negative correlation, then $\text{cov}(X,Y)< 0$, which causes a reduction in variance (compared to the case where $X$ and $Y$ are uncorrelated)\footnote{Remember that by definition, the correlation between $X$ and $Y$ is defined as $\text{cor}(X,Y)=\frac{\text{cov}(X,Y)}{\sigma_{X}\sigma_{Y}}$}.
	
	A typical example given in many textbooks is to compute the mean of $\mathcal{U}_{[0,1]}$ by using an antithetic variable $1-\mathcal{U}_{[0,1]}$ (it's indeed an important example as $\mathcal{U}_{[0,1]}$ is used for the inverse transform method!).
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	The simple example is if $X=\mathcal{U}$, then $Y=1-\mathcal{U}$. We have then:
	$$\dfrac{X+Y}{2}=\dfrac{1}{2}$$
	and:
	$$\dfrac{X+Y}{2}=\dfrac{\mathcal{U}+(1-\mathcal{U})}{2}=0$$
	ie perfect negative correlation!
	\end{tcolorbox}
	
	Notice that this method can obviously be used to compute integrals by implementing it in the Monte Carlo Mean Value Method presented earlier above!

	\subparagraph{Method of Control Variables}\mbox{}\\\\\
	Suppose we are interested in estimating $\text{E}(X)=\lambda$. Then instead of exclusively sampling $X$, we may also introduce another variable $Y$, named the "\NewTerm{control variable}", that is correlated with $X$, and sample $Z=X+c(Y-\mu)$, where $c$ is a constant, and $\mu = \text{E}(Y)$. Notice that :
	
 	and so sampling $Z$ will lead to an unbiased estimator. Moreover:
 	
	which is minimized when:
	
	Therefore:
	
	(optimal $c$ sometimes denoted $c^*$) and yields a variance equal to:
	
	Hence:
	
	Therefore, the method of control variables will always reduce variance, so long as $X$ and $Y$ have non-zero correlation.

	One issue faced by this method is that usually neither $\text{cov}(X,Y)$ nor $\text{V}(X)$ are known beforehand. However, as a preprocessing step, we may 
obtain the estimators:
	
	and:
	
	to obtain the estimate:
	
	for $c$.
	
	And finally:
		
	The real challenge is to choose the control variable $Y$ that is easy to simulate and for which $\mu_Y$ is easy to find...
	
	Notice that this method can also obviously be used to compute integrals by implementing it in the Monte Carlo Mean Value Method presented earlier above!

	\subparagraph{Method of Conditioning}\mbox{}\\\\\
	There are times when knowledge of the value of a random variable $Y$ can reduce the variance of what can be observed for $X$.

	In this case $\text{E}(X|Y)$ may have less variance than $X$. Note also (\SeeChapter{see section Probabilities page \pageref{iterated conditional mean}}) the iterated conditional mean $\text{E}(\text{E}(X|Y))=\text{E}(X)$, and so $\text{E}(X|Y)$ is an unbiased estimator of $\text{E}(X)$.
	
	So in other words, the idea is that instead of computing $\mu_Y=\text{E}(X)$, for a given random variable $Z$, we put $V=\text{E}(X|Y)$ and we calculate instead $\mu_X=\text{E}(V)=\text{E}(\text{E}(X|Y))=\text{E}(X)$.

	Moreover, the amount of reduction in variance is determined by the following proposition.

	Let us prove first that:
	
	To do that we just write:
	
	We have for immediate corollary:
	\begin{corollary}
		
	\end{corollary}

	So it proves that:
	
	Very likely for many people that is very abstract and counter intuitive. So let us see an example!

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us recall the method of approximating $\pi$ using Monte Carlo that was presented earlier above with an indicator variable $I$ such that:
	\[I=\left\{\begin{array}{ll}
	1 & \mbox{if } X^{2}+Y^{2}\leq 1\\
	0 & \mbox{otherwise}
	\end{array}\right.\]
	where $X,Y= \mathcal{U}_{[0,1]}$. Recall that (remember that's for the quarter of a circle!):
	\begin{gather*}
		\text{E}(I)=\dfrac{\pi}{4} \quad \text{and}\quad \text{V}(I)=\frac{\pi}{4}\left(1-\frac{\pi}{4}\right)\cong 0.1685
	\end{gather*}
	We now show that $\text{E}(I|X)$ has a smaller variance. For that purpose we start by calculating:
	\begin{gather*}
		\begin{aligned}
		\text{E}(I|X=x)&=P(X^{2}+Y^2\leq 1|X = x) = P(Y^{2}\leq 1-x^{2}|X=x)\\
		&=P(Y^{2}\leq 1-x^{2})=P(-(1-x^2)^{1/2} \leq Y \leq (1-x^2)^{1/2})\\
		&=\int\limits_{-(1-x^2)^{1/2}}^{(1-x^2)^{1/2}}\frac{1}{2}\mathrm{d}y=(1-x^2)^{1/2}\\
		&\Rightarrow \text{E}(I|X)=(1-X^{2})^{1/2}
		\end{aligned}
	\end{gather*}
	However, since $X^{2}$ has the same distribution as $U^{2}$, $U=\mathcal{U}_{[0,1]}$, it follows that we get the following:
	\begin{gather*}
		\text{E}(I|X)=(1-U^2)^{1/2}
	\end{gather*}
	Notice that the expected means remains a random variable (without surprise as it is a conditions expected mean) that we will denote $Z:=\text{E}(I|X)=(1-U^2)^{1/2}$! Therefore using Huygens relation (\SeeChapter{see section Statistics page \pageref{huygens relation}}), $\text{V}(Z)=\text{E}(Z^2)-\text{E}(Z)^2$ we get:
	\begin{gather*}
		\begin{aligned}
		\text{V}(\text{E}(I|X))&=\text{V}\left((1-U^2)^{1/2}\right)=\text{E}\left(\left((1-U^2)^{1/2}\right)^2\right)-\text{E}\left((1-U^2)^{1/2}\right)^2\\
		&=\text{E}\left(1-U^2\right)-\text{E}\left((1-U^2)^{1/2}\right)^2=1-\text{E}\left(U^2\right)-\text{E}\left((1-U^2)^{1/2}\right)^2
		\end{aligned}
	\end{gather*}
	Using the result derived in the section Statistics page \pageref{mean squared uniform random variable}, ie $\text{E}(\mathcal{U}^2_{0,1}) =1/3$, we then have:
	\begin{gather*}
		\text{V}(\text{E}(I|X))=\text{V}\left((1-U^2)^{1/2}\right)=1-\dfrac{1}{3}-\text{E}\left((1-U^2)^{1/2}\right)^2=\dfrac{2}{3}-\text{E}\left((1-U^2)^{1/2}\right)^2
	\end{gather*}
	We just have left to compute $\text{E}(\sqrt{1-U^2})=\int_0^1 \sqrt{1-x^2}\mathrm{d}x$ and using the primitive derived in the section of Differential and Integral Calculus at page \pageref{usual primitives}, we get $\text{E}(\sqrt{1-U^2})=\pi/4$. Therefore:
	\begin{gather*}
		\text{V}(\text{E}(I|X))=\dfrac{2}{3}-\left(\dfrac{\pi}{2}\right)^2
	\end{gather*}
	Hence:
	\begin{gather*}
		\dfrac{\text{V}(\text{E}(X|Y))}{\text{V}(X)}\cong\dfrac{0.0498}{0.1685}\cong 0.295
	\end{gather*}
	yielding a $70.5\%$ reduction in variance.
	\end{tcolorbox}
	
	\subparagraph{Method of Stratified Sampling}\mbox{}\\\\\
	The reader remembers maybe that we have seen earlier above the method of Simple Random Sampling (SRS)\index{simple random sampling} in which realizations of a vector $\vec{x}$ (samples) are generated as independent and identically distributed realizations on a domain $\mathcal{S}$ with marginal cumulative distribution functions (CDFs) $F(x)$ by:
	
	where $\mathcal{U}_{i}$ are  independent and identically distributed uniformly distributed samples on $[0,1]$. The realizations $\vec{x}$ are then applied to the system $\vec{y}=\vec{F}(\vec{x})$ and $\vec{y}$ is statistically evaluated.
	
	"\NewTerm{Stratified sampling}\index{stratified sampling}" represents another conditioning approach to reduce variance. It aims to reduce the variance of the estimator by dividing the interval into strata and estimating the integral on each of the stratum with smaller variance.
	
	Again, suppose the goal is to estimate $\lambda = \text{E}(X)$. Suppose $Y$ is a finite random variable with $\text{dom}(Y)=\{y_{1},\ldots,y_{k}\}$ and $p_i=P(y_{i})$ is known for each $i=1,\ldots,k$.
	
	Then:
	
	Then $\lambda$ can be estimated by estimating each of $\text{E}(X|Y=y_{i})$ with $\hat{\lambda}_i$ and producing the estimate:
	
	where $\hat{\lambda}_i$ is obtained using $n_{i}$ samples, and $n_{1}+\cdots+n_{k}=n$ and $n_i=np_i$.
	
	Notice that:
	
	Thus:
	
	Hence:
	
	so that the reduction in variance depends on the amount of variance that occurs in $\text{E}(X|Y)$. After simplification and rearrangement we get obviously:
	
	
	The introduction above may be to abstract. So let us introduce it in another way! Consider we want to estimate an integral that consist a distribution $p$ and a function of interest $f$
	\begin{equation}
		I = \int_\chi p(x) f(x) \mathrm{d} x
	\end{equation}
	via partitioning $\chi$ into smaller groups, named "strata". It is a type of quasi-Monte Carlo (QMC) method as it introduces deterministic procedures into the MC estimation. Same as crude Monte Carlo the estimation is still unbiased; however, the variance of the estimator can be smaller than crude Monte Carlo.
	
	The (crude) Monte Carlo estimator of $I$ is:
	\begin{equation}
		\hat{I} = \frac{1}{N} \sum_{i=1}^N f(x^i)
	\end{equation}
	where $x^i \sim p(x)$ and $N$ is the number of Monte Carlo samples.
	
	This estimator is an unbiased estimator of the original target $I$:
	\begin{equation}
		\text{E}(\hat{I})= \text{E} \left(\frac{1}{N} \sum_{i=1}^N f(x^i) \right)= \frac{1}{N} \sum_{i=1}^N \text{E}(f(x^i)) = \frac{1}{N} \sum_{i=1}^N I = I
	\end{equation}
	Now let’s look at the variance of the estimator $\text{V}(\hat{I})$. We first need to define:
	\begin{equation}
		\sigma^2 := \int_\chi p(x) (f(x) - I)^2 \mathrm{d} x
	\end{equation}
	which is the variance of $f(x)$ under the distribution $p(x)$. Now the variance of interest is
	\begin{equation}
		\text{V}(\hat{I})= \text{V}  \left(\frac{1}{N} \sum_{i=1}^N f(x^i) \right)= \frac{1}{N^2} \sum_{i=1}^N \text{V}(f(x^i)) = \frac{1}{N^2} \sum_{i=1}^N \sigma^2 = \frac{\sigma^2}{N}
	\end{equation}
	As it can be seen, the variance drops with the order of $\mathcal{O}(\frac{1}{N})$ by using more samples.
	
	Now, we look at how stratified sampling works and how the variance of the corresponding estimator changes depending on different stratifications. For the sake of simplicity, we consider the case where $p(x) = \mathcal{U}^D_{[0, 1]}$ a $D$-dimensional uniform distribution for the integral. We first partition $\chi$ into $H$ strata s.t. $\chi = \chi_1 \cup \ldots \cup \chi_H$. Then, as the simplest way of performing stratified sampling, we uniformly draw independent Monte Carlo samples for each stratum $\chi_h$, and the overall estimator is given as below:
	\begin{equation}
		\hat{I}_\text{strat} = \sum_{h=1}^H \frac{|\chi_h|}{n_h}\sum_{i=1}^{n_h} f(x_h^i)
	\end{equation}
	where $\{x_h^i\}_{i=1}^{n_h}$ are samples that lie in $\chi_h$ and $n_h$ is the number of MC samples in stratum $h$.

	The mean of $f$ within each stratum $h$ is therefore:
	\begin{equation}
		\mu_h= \int_{\chi_h} \mathcal{U}(x; \chi_h) f(x) \mathrm{d} x = |\chi_h|^{-1} \int_{\chi_h} f(x) \mathrm{d} x
	\end{equation}
	and the corresponding variance is:
	\begin{equation}
		\sigma_h^2= \int_{\chi_h} \mathcal{U}(x; \chi_h) (f(x) - \mu_h)^2 \mathrm{d} x = |\chi_h|^{-1} \int_{\chi_h}  (f(x) - \mu_h)^2 \mathrm{d} x
	\end{equation}
	As it is mentioned in the beginning, $\hat{I}_\text{strat}$ is still an unbiased estimator of $I$. We can validate this by computing the mean of $\hat{I}_\text{strat}$:
	\begin{equation}
		\begin{aligned}
		\text{E}\left(\hat{I}_{\text {strat}}\right) &=\text{E}\left(\sum_{h=1}^{H} \frac{\left|\chi_{h}\right|}{n_{h}} \sum_{i=1}^{n_{h}} f\left(x_{h}^{i}\right)\right) =\sum_{h=1}^{H} \frac{\left|\chi_{h}\right|}{n_{h}} \sum_{i=1}^{n_{h}} \text{E}\left(f\left(x_{h}^{i}\right)\right) \\
		&=\sum_{h=1}^{H} \frac{\left|\chi_{h}\right|}{n_{h}} \sum_{i=1}^{n_{h}}\left|\chi_{h}\right|^{-1} \int_{\chi_{h}} f(x) \mathrm{d} x =\sum_{h=1}^{H} \frac{1}{n_{h}} \sum_{i=1}^{n_{h}} \int_{\chi_{h}} f(x) \mathrm{d} x =\sum_{h=1}^{H} \int_{\chi_{h}} f(x) \mathrm{d} x \\
		&=\int_{\chi_{1} \cup \ldots \cup \chi_{H}} f(x) \mathrm{d} x =\int_{\mathcal{X}} f(x) \mathrm{d} x =I
		\end{aligned}
	\end{equation}
	The variance of $\hat{I}_{\text{strat}}$ is then:
	\begin{equation}
		\begin{aligned}
		\text{V}\left(\hat{I}_{\text {strat}}\right) &=\text{V}\left(\sum_{h=1}^{H} \frac{\left|\chi_{h}\right|}{n_{h}} \sum_{i=1}^{n_{h}} f\left(x_{h}^{i}\right)\right) =\sum_{h=1}^{H} \text{V}\left(\frac{\left|\chi_{h}\right|}{n_{h}} \sum_{i=1}^{n_{h}} f\left(x_{h}^{i}\right)\right) =\sum_{h=1}^{H} \frac{\left|\chi_{h}\right|^{2}}{n_{h}^{2}} \text{V}\left(\sum_{i=1}^{n_{h}} f\left(x_{h}^{i}\right)\right) \\
		&=\sum_{h=1}^{H} \frac{\left|\chi_{h}\right|^{2}}{n_{h}^{2}} \sum_{i=1}^{n_{h}} \text{V}\left(f\left(x_{h}^{i}\right)\right)=\sum_{h=1}^{H} \frac{\left|\chi_{h}\right|^{2}}{n_{h}^{2}} \sum_{i=1}^{n_{h}} \sigma_{h}^{2} =\sum_{h=1}^{H} \frac{|\chi_h|^{2}}{n_{h}} \sigma_{h}^{2}
		\end{aligned}
	\end{equation}
	We are interested in comparing this estimator against (crude) Monte Carlo. In particular, we'd like to show that when $n_{h}$ is allocated proportionally to $\mid \chi_{h} \mid$ the estimator can only decrease in variance as compared to (crude) Monte Carlo.

	In order to show this, we need to rewrite the variance of crude Monte Carlo in terms of strata. Let's introduce $h(x)$ - the stratum that contains $x: x \in \mathcal{X}_{h(x)}, \forall x \in \mathcal{X}$. By using in a clever way the law of total variance (\SeeChapter{see section Statistics page \pageref{iterated condititional variance}}) we can relate $\sigma$ and $\sigma_h$ as:
	\begin{equation}
		\begin{aligned}
		\sigma^{2}=\text{V}(f(x)) &=\text{E}\left(\text{V}(f(x) \mid h(x))\right)+\text{V}\left(\text{E}(f(x) \mid h(x))\right) =\sum_{h=1}^{H}\left|\chi_{h}\right| \sigma_{h}^{2}+\sum_{h=1}^{H}\left|\chi_{h}\right|\left(\mu_{h}-I\right)^{2}
		\end{aligned}
	\end{equation}
	Therefore:
	\begin{equation}
		\text{V}(\hat{I})=\frac{\sigma^{2}}{n}=\frac{1}{n}\sum_{h=1}^{H}\left|\mathcal{X}_{h}\right| \sigma_{h}^{2}+\frac{1}{n}\sum_{h=1}^{H}\left|\mathcal{X}_{h}\right|\left(\mu_{h}-I\right)^{2}
	\end{equation}
	Note that in the case of proportional allocation, because $n_{h}=n\left|\mathcal{X}_{h}\right|$, from the above deviation we get:
	\begin{equation}
		\text{V}(\hat{I}_{\text{strat}})=\sum_{h=1}^{H} \frac{|\chi_h|^{2}}{n_{h}} \sigma_{h}^{2}=\frac{1}{n} \sum_{h=1}^{H}\left|\mathcal{X}_{h}\right| \sigma_{h}^{2}
	\end{equation}
	So comparing:
	\begin{equation}
		\text{V}(\hat{I})=\frac{1}{n}\sum_{h=1}^{H}\left|\mathcal{X}_{h}\right| \sigma_{h}^{2}+\frac{1}{n}\sum_{h=1}^{H}\left|\mathcal{X}_{h}\right|\left(\mu_{h}-I\right)^{2}\quad \text{and}\quad \text{V}(\hat{I}_{\text{strat}})=\frac{1}{n} \sum_{h=1}^{H}\left|\mathcal{X}_{h}\right| \sigma_{h}^{2}
	\end{equation}
	So we see immediately that $\text{V}(\hat{I}_{\text{strat}}) \leq \text{V}(\hat{I})$, that is:
	\begin{equation}
		\frac{1}{n} \sum_{h=1}^{H}\left|\mathcal{X}_{h}\right| \sigma_{h}^{2}\leq \frac{1}{n}\sum_{h=1}^{H}\left|\mathcal{X}_{h}\right| \sigma_{h}^{2}+\frac{1}{n}\sum_{h=1}^{H}\left|\mathcal{X}_{h}\right|\left(\mu_{h}-I\right)^{2}
	\end{equation}
	Rearranged and simplified:
	\begin{equation}
		\dfrac{\sum_{h=1}^{H}\left|\mathcal{X}_{h}\right| \sigma_{h}^{2}}{\sum_{h=1}^{H}\left|\mathcal{X}_{h}\right| \sigma_{h}^{2}+\sum_{h=1}^{H}\left|\mathcal{X}_{h}\right|\left(\mu_{h}-I\right)^{2}}\leq 1
	\end{equation}
	which is smaller than $\text{V}(\hat{I})$. Although it is not the optimal allocation, proportional allocation as applied to stratified sampling will never increase the variance over that of crude Monte Carlo. Also, note that a poor allocation can also increase the variance of the estimator in comparison to crude Monte Carlo! 
	
	So to summarize what we have seen here, in a more formal way, "stratified sampling" begins by dividing the sample domain $\mathcal{S}$ into a collection of $M$ disjoint subsets (strata) $\mathcal{S}_{k} ; k=1,2, \ldots, M$ such as $\cup_{k=1}^{M} \mathcal{S}_{k}=\mathcal{S}$ and $\mathcal{S}_{p} \cap \mathcal{S}_{q}=\varnothing ; p \neq q$. Sample realizations from a given stratum $k, \vec{x}_{k}=\left\{x_{1 k}, x_{2 k}, \ldots, x_{N k}\right\}$, are generated by randomly sampling the vector components according to:
	
	where $\mathcal{U}_{i k}$ are identically and independently uniformly distributed samples on $\left[\xi_{i k}^{l}, \xi_{i k}^{u}\right]$ with $\xi_{i k}^{l}=F_{X_{i}}\left(\zeta_{i k}^{l}\right)$ and $\xi_{i k}^{u}=F_{X_{i}}\left(\zeta_{i k}^{u}\right)$, and $\zeta_{i k}^{l}$ and $\zeta_{i k}^{u}$ denote the lower and upper bounds respectively of the $i^{\text{th}}$ vector component of stratum $\mathcal{S}{k}$. Typically the stratification is performed directly in the probability space meaning that the strata are defined directly by the bounds $\xi_{i k}^{l}$ and $\xi_{i k}^{u}$.
	
	More explicitly, given a vector of uniform random numbers $U_{1}, U_{2}, \cdots, U_{n}$ on $[0,1]$. We divide the interval $[0,1]$ into $k \leq n$ equal subintervals. We can use the given uniform random numbers to generate uniform random in each of the $k$ intervals by using the relation:
	
	Thus, $\hat{U}_{1}$ is a random number in the interval $\left(0, \frac{1}{k}\right), \hat{U}_{2}$ belongs to the interval $\left(\frac{1}{k}, \frac{2}{k}\right)$ and so on. Moreover, the numbers $\hat{U}_{1}, \hat{U}_{1+k}, \hat{U}_{1+2 k}, \cdots$ are uniformly distributed in the interval $\left(0, \frac{1}{k}\right)$, the numbers $\hat{U}_{2}, \hat{U}_{2+k}, \hat{U}_{2+2 k}, \cdots$ are uniformly distributed in $\left(\frac{1}{k}, \frac{2}{k}\right)$ and so on.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Consider the following $8$ uniform random numbers in $[0,1]$ :
	\begin{center}
	\begin{tabular}{|l|l|l|l|l|l|l|l|l|}
	\hline$i$ & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
	\hline$U_{i}$ & $0.4880$ & $0.7894$ & $0.8628$ & $0.4482$ & $0.3172$ & $0.8944$ & $0.5013$ & $0.3015$ \\
	\hline
	\end{tabular}
	\end{center}
	To generate uniform random numbers in each quartile, i.e. $k=4$ we have:
	$$
	\begin{aligned}
	&\hat{U}_{1}=\frac{1-1+U_{1}}{4}=0.122 \\
	&\hat{U}_{2}-\frac{2-1+U_{2}}{4}-0.44735 \\
	&\hat{U}_{3}=\frac{3-1+U_{3}}{4}=0.7157 \\
	&\hat{U}_{4}-\frac{4-1+U_{4}}{4}-0.86205 \\
	&\hat{U}_{5}=\frac{(5-4)-1+U_{5}}{4}=0.0793 \\
	&\hat{U}_{6}=\frac{(6-4)-1+U_{6}}{4}=0.4736 \\
	&\hat{U}_{7}=\frac{(7-4)-1+U_{7}}{4}=0.625325 \\
	&\hat{U}_{8}=\frac{(8-4)-1+U_{8}}{4}=0.825375
	\end{aligned}
	$$
	\end{tcolorbox}
	
	\subparagraph{Importance Sampling (IS)}\label{importance sampling}\mbox{}\\\\\
	In statistics, "\NewTerm{importance sampling}\index{importance sampling}" is a general technique for estimating properties of a particular distribution, while only having samples generated from a different distribution than the distribution of interest!
	
	The method is named "importance sampling" because it relies on so-named "\NewTerm{importance functions}\index{importance functions}", which are instrumental distributions, in lieu of the original distributions (the term "sampling" is arguably a misnomer since the method does not attempt to draw samples from any given distribution). In fact, an evaluation of: 
	
	based on simulations from $f$ is almost never optimal in the sense that using alternative distributions can improve the variance of the resulting estimator of $\text{E}(f(X))$. 
	
	Generally it is a very common technique used to estimate the tail of some probability distribution and is in that special case denoted:
	
	when the occurrence of $ X \leq $ threshold is very small. Such situations occurs typically in the calculation of  climatic, rail and air disasters, bankruptcies of large companies/states, major stock market cracks, portfolio risks, operational risks (legal obligation for banks or insurance companies).
	
	To explain the idea consider we want to compute for a $\mathcal{N}(0,1)$ the following:
	
	If we try to compute that with crude Monte Carlo method with $N$ varying from $1,000$ too $10,000,000$ the conclusion is almost always the same: $\theta=0$ and it's wrong as the correct value is known to be $\theta\cong 7.6199 \cdot 10^{-24}$. The necessary number of trials ($N$) should be of the order of $10^{25}$... value that is hardly accessible to most common personal computer even in the early 121st century (holocene calendar).

	The underling workaround idea of importance sampling is to use the following trick (i.e. change the law!):
	
	with:
	
	where $\phi$ is the density of $X$ and $\psi$ another density - an "importance function" - such as $\psi(x) \neq 0$ if $\phi(x) \neq 0$.
	
	Hence:
	
	To study the variance, let $X$ be a random variable with probability density function $p$. Consider evaluating the following quantity:
	
	According to what we have just seen, then the importance sampling estimator is:
	
	When $p=q$, this reduces obviously to the simple estimator that uses sample means of $f(x)$ to estimate its expectation. Does this estimator a good estimator? Let's study its bias before we focus on the variance! For the bias (see page \pageref{bias-variance tradeoff}):
	
	Thus, it is an unbiased estimator! That lead us also to the result that (we will us it just below):
	
	 And now let's see the variance, using the equality just above and Huygens relation (\SeeChapter{see section Statistics page \pageref{huygens relation}}), we get:
	
	The last equality has to be compared with:
	
	That is putting side by side:
	
	So we see that we have a variance reduction, if and only if:
	
	how to build a density $q(x)$ having the above property? It seems that this question doesn't have an easy answer other than trial and error and return on experience (REX).
	
	As it may be a quite confusing notation and concept for some readers, let us see a simple concrete example!
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Consider that we have $X =\mathcal{N}(\mu,\sigma)= \mathcal{N}(0,1)$ and we want to compute $\theta=P(X \leq-10)$. We know already that this is an issue!\\
	
	The idea is then to force the trial of a new variable $Y$ to be around $-10$. So the idea would be to choose: $Y = \mathcal{N}(-10,1)$.\\
	
	Notice that if $Y=\mathcal{N}\left(\mu, \sigma^{2}\right)$, then $P(Y \leq \mu)=1 / 2$.\\
	
	Therefore:
	\begin{gather*}
		\begin{aligned}
		\theta&=P(X \leq-10)=\int\limits\limits_{-\infty}^{-10} \frac{1}{\sqrt{2 \pi}} \exp \left(-\frac{x^{2}}{2}\right) \mathrm{d} x \\ 
		&=\int\limits_{-\infty}^{-10} \left(\frac{\frac{1}{\sqrt{2 \pi}} \exp \left(-\frac{x^{2}}{2}\right)}{\frac{1}{\sqrt{2 \pi}} \exp \left(-\frac{(x+10)^{2}}{2}\right)}\right) \frac{1}{\sqrt{2 \pi}} \exp \left(-\frac{(x+10)^{2}}{2}\right) \mathrm{d} x \\ 
		&=\int\limits_{-\infty}^{-10} \exp \left(10 x+(-10)^{2} / 2\right) \frac{1}{\sqrt{2 \pi}} \exp \left(-\frac{(x+10)^{2}}{2}\right) \mathrm{d} x \\ 
		&=\int\limits_{-\infty}^{-10} \exp \left(10 x+50\right) \frac{1}{\sqrt{2 \pi}} \exp \left(-\frac{(x+10)^{2}}{2}\right) \mathrm{d} x \\ 
		&=\text{E}\left(1_{Y \leq-10} \exp (10 Y+50)\right) 
		\end{aligned}
	\end{gather*}
	The trick works very well in this case as the reader can check it by itself in the corresponding implementation in \texttt{R} and MATLAB™ as available in our corresponding companion free books.
	\end{tcolorbox}
	
	
	
	In practice it is quite common to implement all the different methods of variance reduction seen so far and to compare which one converge the fastest after a given number of runs (or "epochs").
	
	\subsubsection{Bootstrapping}\label{bootstrap}
	In statistics, "\NewTerm{bootstrap techniques}\index{bootstrap techniques}" can refer to any test or metric that relies on random sampling with replacement of a population of data to run statistical inference on small samples. This methods is relatively intensive in computations still for expensive office computers at this beginning of the 121st century (holocene calendar).  
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
		
		%Shape: Rectangle [id:dp12466090143183495] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]  (50,39) -- (201.5,39) -- (201.5,179) -- (50,179) -- cycle ;
		%Shape: Rectangle [id:dp5392989434957713] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]  (337,107) -- (488.5,107) -- (488.5,247) -- (337,247) -- cycle ;
		%Shape: Rectangle [id:dp08320002052542774] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]  (321,289) -- (472.5,289) -- (472.5,429) -- (321,429) -- cycle ;
		%Shape: Rectangle [id:dp9658760532034605] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]  (106,246) -- (257.5,246) -- (257.5,386) -- (106,386) -- cycle ;
		%Straight Lines [id:da8425246618426836] 
		\draw    (124,181) -- (151,204.68) ;
		\draw [shift={(152.5,206)}, rotate = 221.26] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da9786924410377111] 
		\draw    (213,170) -- (321.07,275.6) ;
		\draw [shift={(322.5,277)}, rotate = 224.34] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da26529065741617885] 
		\draw    (215,126) -- (327.72,184.08) ;
		\draw [shift={(329.5,185)}, rotate = 207.26] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Circle [id:dp6477900242902217] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (59,64) .. controls (59,59.58) and (62.58,56) .. (67,56) .. controls (71.42,56) and (75,59.58) .. (75,64) .. controls (75,68.42) and (71.42,72) .. (67,72) .. controls (62.58,72) and (59,68.42) .. (59,64) -- cycle ;
		%Shape: Circle [id:dp9320904548315059] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (173,113) .. controls (173,108.58) and (176.58,105) .. (181,105) .. controls (185.42,105) and (189,108.58) .. (189,113) .. controls (189,117.42) and (185.42,121) .. (181,121) .. controls (176.58,121) and (173,117.42) .. (173,113) -- cycle ;
		%Shape: Circle [id:dp2629980787697117] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (69,146) .. controls (69,141.58) and (72.58,138) .. (77,138) .. controls (81.42,138) and (85,141.58) .. (85,146) .. controls (85,150.42) and (81.42,154) .. (77,154) .. controls (72.58,154) and (69,150.42) .. (69,146) -- cycle ;
		%Shape: Circle [id:dp7428316199202807] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (125,353) .. controls (125,348.58) and (128.58,345) .. (133,345) .. controls (137.42,345) and (141,348.58) .. (141,353) .. controls (141,357.42) and (137.42,361) .. (133,361) .. controls (128.58,361) and (125,357.42) .. (125,353) -- cycle ;
		%Shape: Circle [id:dp9782148438659826] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (230,320) .. controls (230,315.58) and (233.58,312) .. (238,312) .. controls (242.42,312) and (246,315.58) .. (246,320) .. controls (246,324.42) and (242.42,328) .. (238,328) .. controls (233.58,328) and (230,324.42) .. (230,320) -- cycle ;
		%Shape: Circle [id:dp07250597079289078] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (361,209) .. controls (361,204.58) and (364.58,201) .. (369,201) .. controls (373.42,201) and (377,204.58) .. (377,209) .. controls (377,213.42) and (373.42,217) .. (369,217) .. controls (364.58,217) and (361,213.42) .. (361,209) -- cycle ;
		%Shape: Circle [id:dp4838958969551903] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (446,199) .. controls (446,194.58) and (449.58,191) .. (454,191) .. controls (458.42,191) and (462,194.58) .. (462,199) .. controls (462,203.42) and (458.42,207) .. (454,207) .. controls (449.58,207) and (446,203.42) .. (446,199) -- cycle ;
		%Shape: Circle [id:dp7981589759204573] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (466,175) .. controls (466,170.58) and (469.58,167) .. (474,167) .. controls (478.42,167) and (482,170.58) .. (482,175) .. controls (482,179.42) and (478.42,183) .. (474,183) .. controls (469.58,183) and (466,179.42) .. (466,175) -- cycle ;
		%Shape: Circle [id:dp18863271721729258] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (440,160) .. controls (440,155.58) and (443.58,152) .. (448,152) .. controls (452.42,152) and (456,155.58) .. (456,160) .. controls (456,164.42) and (452.42,168) .. (448,168) .. controls (443.58,168) and (440,164.42) .. (440,160) -- cycle ;
		%Shape: Circle [id:dp6833703993519427] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (352,125) .. controls (352,120.58) and (355.58,117) .. (360,117) .. controls (364.42,117) and (368,120.58) .. (368,125) .. controls (368,129.42) and (364.42,133) .. (360,133) .. controls (355.58,133) and (352,129.42) .. (352,125) -- cycle ;
		%Shape: Circle [id:dp5548463997118753] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (120,57) .. controls (120,52.58) and (123.58,49) .. (128,49) .. controls (132.42,49) and (136,52.58) .. (136,57) .. controls (136,61.42) and (132.42,65) .. (128,65) .. controls (123.58,65) and (120,61.42) .. (120,57) -- cycle ;
		%Shape: Circle [id:dp9767208674289514] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (159,64) .. controls (159,59.58) and (162.58,56) .. (167,56) .. controls (171.42,56) and (175,59.58) .. (175,64) .. controls (175,68.42) and (171.42,72) .. (167,72) .. controls (162.58,72) and (159,68.42) .. (159,64) -- cycle ;
		%Shape: Circle [id:dp2052094785232743] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (77,110) .. controls (77,105.58) and (80.58,102) .. (85,102) .. controls (89.42,102) and (93,105.58) .. (93,110) .. controls (93,114.42) and (89.42,118) .. (85,118) .. controls (80.58,118) and (77,114.42) .. (77,110) -- cycle ;
		%Shape: Circle [id:dp43925176156814016] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (133,115) .. controls (133,110.58) and (136.58,107) .. (141,107) .. controls (145.42,107) and (149,110.58) .. (149,115) .. controls (149,119.42) and (145.42,123) .. (141,123) .. controls (136.58,123) and (133,119.42) .. (133,115) -- cycle ;
		%Shape: Circle [id:dp3477256453947106] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (156,164) .. controls (156,159.58) and (159.58,156) .. (164,156) .. controls (168.42,156) and (172,159.58) .. (172,164) .. controls (172,168.42) and (168.42,172) .. (164,172) .. controls (159.58,172) and (156,168.42) .. (156,164) -- cycle ;
		%Shape: Circle [id:dp4533534966192545] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (104,145) .. controls (104,140.58) and (107.58,137) .. (112,137) .. controls (116.42,137) and (120,140.58) .. (120,145) .. controls (120,149.42) and (116.42,153) .. (112,153) .. controls (107.58,153) and (104,149.42) .. (104,145) -- cycle ;
		%Shape: Circle [id:dp33311861266058607] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (426,178) .. controls (426,173.58) and (429.58,170) .. (434,170) .. controls (438.42,170) and (442,173.58) .. (442,178) .. controls (442,182.42) and (438.42,186) .. (434,186) .. controls (429.58,186) and (426,182.42) .. (426,178) -- cycle ;
		%Shape: Circle [id:dp8694178681601259] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (452,126) .. controls (452,121.58) and (455.58,118) .. (460,118) .. controls (464.42,118) and (468,121.58) .. (468,126) .. controls (468,130.42) and (464.42,134) .. (460,134) .. controls (455.58,134) and (452,130.42) .. (452,126) -- cycle ;
		%Shape: Circle [id:dp8404411151370219] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (397,208) .. controls (397,203.58) and (400.58,200) .. (405,200) .. controls (409.42,200) and (413,203.58) .. (413,208) .. controls (413,212.42) and (409.42,216) .. (405,216) .. controls (400.58,216) and (397,212.42) .. (397,208) -- cycle ;
		%Shape: Circle [id:dp42909938622104793] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (216,271) .. controls (216,266.58) and (219.58,263) .. (224,263) .. controls (228.42,263) and (232,266.58) .. (232,271) .. controls (232,275.42) and (228.42,279) .. (224,279) .. controls (219.58,279) and (216,275.42) .. (216,271) -- cycle ;
		%Shape: Circle [id:dp41491354751461285] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (114,279) .. controls (114,274.58) and (117.58,271) .. (122,271) .. controls (126.42,271) and (130,274.58) .. (130,279) .. controls (130,283.42) and (126.42,287) .. (122,287) .. controls (117.58,287) and (114,283.42) .. (114,279) -- cycle ;
		%Shape: Circle [id:dp4216945816288771] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (132,317) .. controls (132,312.58) and (135.58,309) .. (140,309) .. controls (144.42,309) and (148,312.58) .. (148,317) .. controls (148,321.42) and (144.42,325) .. (140,325) .. controls (135.58,325) and (132,321.42) .. (132,317) -- cycle ;
		%Shape: Circle [id:dp47304393296685165] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (160,353) .. controls (160,348.58) and (163.58,345) .. (168,345) .. controls (172.42,345) and (176,348.58) .. (176,353) .. controls (176,357.42) and (172.42,361) .. (168,361) .. controls (163.58,361) and (160,357.42) .. (160,353) -- cycle ;
		%Shape: Circle [id:dp08338064997965611] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (190,323) .. controls (190,318.58) and (193.58,315) .. (198,315) .. controls (202.42,315) and (206,318.58) .. (206,323) .. controls (206,327.42) and (202.42,331) .. (198,331) .. controls (193.58,331) and (190,327.42) .. (190,323) -- cycle ;
		%Shape: Circle [id:dp14801549154457816] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (212,370) .. controls (212,365.58) and (215.58,362) .. (220,362) .. controls (224.42,362) and (228,365.58) .. (228,370) .. controls (228,374.42) and (224.42,378) .. (220,378) .. controls (215.58,378) and (212,374.42) .. (212,370) -- cycle ;
		%Shape: Circle [id:dp16649862818185834] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (375,397) .. controls (375,392.58) and (378.58,389) .. (383,389) .. controls (387.42,389) and (391,392.58) .. (391,397) .. controls (391,401.42) and (387.42,405) .. (383,405) .. controls (378.58,405) and (375,401.42) .. (375,397) -- cycle ;
		%Shape: Circle [id:dp16700329159153915] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (413,403) .. controls (413,398.58) and (416.58,395) .. (421,395) .. controls (425.42,395) and (429,398.58) .. (429,403) .. controls (429,407.42) and (425.42,411) .. (421,411) .. controls (416.58,411) and (413,407.42) .. (413,403) -- cycle ;
		%Shape: Circle [id:dp8450301349552054] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (429,373) .. controls (429,368.58) and (432.58,365) .. (437,365) .. controls (441.42,365) and (445,368.58) .. (445,373) .. controls (445,377.42) and (441.42,381) .. (437,381) .. controls (432.58,381) and (429,377.42) .. (429,373) -- cycle ;
		%Shape: Circle [id:dp5037557274406035] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (404,367) .. controls (404,362.58) and (407.58,359) .. (412,359) .. controls (416.42,359) and (420,362.58) .. (420,367) .. controls (420,371.42) and (416.42,375) .. (412,375) .. controls (407.58,375) and (404,371.42) .. (404,367) -- cycle ;
		%Shape: Circle [id:dp4343705766154622] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (347,361) .. controls (347,356.58) and (350.58,353) .. (355,353) .. controls (359.42,353) and (363,356.58) .. (363,361) .. controls (363,365.42) and (359.42,369) .. (355,369) .. controls (350.58,369) and (347,365.42) .. (347,361) -- cycle ;
		%Shape: Circle [id:dp16124894815373714] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (378,349) .. controls (378,344.58) and (381.58,341) .. (386,341) .. controls (390.42,341) and (394,344.58) .. (394,349) .. controls (394,353.42) and (390.42,357) .. (386,357) .. controls (381.58,357) and (378,353.42) .. (378,349) -- cycle ;
		%Shape: Circle [id:dp2454287979161236] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (393,333) .. controls (393,328.58) and (396.58,325) .. (401,325) .. controls (405.42,325) and (409,328.58) .. (409,333) .. controls (409,337.42) and (405.42,341) .. (401,341) .. controls (396.58,341) and (393,337.42) .. (393,333) -- cycle ;
		%Shape: Circle [id:dp2567670664753179] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (430,315) .. controls (430,310.58) and (433.58,307) .. (438,307) .. controls (442.42,307) and (446,310.58) .. (446,315) .. controls (446,319.42) and (442.42,323) .. (438,323) .. controls (433.58,323) and (430,319.42) .. (430,315) -- cycle ;
		%Shape: Circle [id:dp09776935988645863] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (391,308) .. controls (391,303.58) and (394.58,300) .. (399,300) .. controls (403.42,300) and (407,303.58) .. (407,308) .. controls (407,312.42) and (403.42,316) .. (399,316) .. controls (394.58,316) and (391,312.42) .. (391,308) -- cycle ;
		%Shape: Circle [id:dp5578305670519166] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (347,321) .. controls (347,316.58) and (350.58,313) .. (355,313) .. controls (359.42,313) and (363,316.58) .. (363,321) .. controls (363,325.42) and (359.42,329) .. (355,329) .. controls (350.58,329) and (347,325.42) .. (347,321) -- cycle ;
		%Shape: Circle [id:dp3949582696173348] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (205,289) .. controls (205,284.58) and (208.58,281) .. (213,281) .. controls (217.42,281) and (221,284.58) .. (221,289) .. controls (221,293.42) and (217.42,297) .. (213,297) .. controls (208.58,297) and (205,293.42) .. (205,289) -- cycle ;
		%Shape: Circle [id:dp04958694297781352] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (156,286) .. controls (156,281.58) and (159.58,278) .. (164,278) .. controls (168.42,278) and (172,281.58) .. (172,286) .. controls (172,290.42) and (168.42,294) .. (164,294) .. controls (159.58,294) and (156,290.42) .. (156,286) -- cycle ;
		%Shape: Circle [id:dp12044492024711895] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (385,153) .. controls (385,148.58) and (388.58,145) .. (393,145) .. controls (397.42,145) and (401,148.58) .. (401,153) .. controls (401,157.42) and (397.42,161) .. (393,161) .. controls (388.58,161) and (385,157.42) .. (385,153) -- cycle ;
		%Shape: Circle [id:dp5625310168461086] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (391,178) .. controls (391,173.58) and (394.58,170) .. (399,170) .. controls (403.42,170) and (407,173.58) .. (407,178) .. controls (407,182.42) and (403.42,186) .. (399,186) .. controls (394.58,186) and (391,182.42) .. (391,178) -- cycle ;
		%Shape: Circle [id:dp7121157977994592] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (94,91) .. controls (94,86.58) and (97.58,83) .. (102,83) .. controls (106.42,83) and (110,86.58) .. (110,91) .. controls (110,95.42) and (106.42,99) .. (102,99) .. controls (97.58,99) and (94,95.42) .. (94,91) -- cycle ;
		
		% Text Node
		\draw (131,208) node [anchor=north west][inner sep=0.75pt]   [align=left] {Bootstrapped\\Sample $\displaystyle 1$};
		% Text Node
		\draw (484,301) node [anchor=north west][inner sep=0.75pt]   [align=left] {Bootstrapped\\Sample $\displaystyle 2$};
		% Text Node
		\draw (506,113) node [anchor=north west][inner sep=0.75pt]   [align=left] {Bootstrapped\\Sample $\displaystyle 3$};
		% Text Node
		\draw (467,438) node [anchor=north west][inner sep=0.75pt]   [align=left] {...Bootstrapped\\Sample $\displaystyle N$};
		% Text Node
		\draw (69,19) node [anchor=north west][inner sep=0.75pt]   [align=left] {Empirical Data};
		
		\end{tikzpicture}
	\end{figure}
	The goal of bootstrapping is to find some indication of a statistic: its estimate of course, but also its dispersion (variance, standard deviation), confidence intervals or hypothesis testing. This method is based on simulations, such as Monte Carlo methods, with the difference that the bootstrap does not require additional information than the one that is available already in the initial sample. In general, it is based on new samples obtained by sampling with replacement from the original sample (then we speak also of "\NewTerm{resampling}\index{resampling}").
	
	We distinguish generally two types of bootstrap:
	\begin{enumerate}
		\item The bootstraps which make no assumptions about the probability distribution of the data analysed. We then speak then as in statistics of "\NewTerm{nonparametric bootstrap}\index{nonparametric bootstrap}".

		\item The bootstraps replacing each data measured by those corresponding to the analytical expression of the law of probability distribution assumed. We speak then of "\NewTerm{parametric bootstrap}\index{parametric bootstrap}". Once all the original values replaced, the process is exactly that of the nonparametric bootstrap.
	\end{enumerate}
	\textbf{Definition (\#\thesection.\mydef):} Given a set $\mathcal{D}$ of $n$ observations, and a number $B$, we name  "\NewTerm{bootstrap}\index{bootstrap}" the procedure which consists in creating $B$ samples $\mathcal{D}_{1}, \mathcal {D}_{2}, \ldots, \mathcal{D}_{B}$ from $\mathcal{D}$, each obtained by drawing $n$ examples from $\mathcal{D}$ with replacement. Thus, each example may appear multiple times, or not at all, in $\mathcal{D}_{b}$.

	We will illustrate the principle of the bootstrap on the example of the confidence interval of the mean $\mu$ of a random variable (this special case is named a "\NewTerm{studentized bootstrap}\index{studentized bootstrap}". For this example, the confidence interval for the mean of a random variable is completely determined from the mean and the variance calculated on the sample (\SeeChapter{see section Statistics page \pageref{likelihood estimators}}).
	
	We consider a sample of the random variable composed of $10$ estimates:
	
	The arithmetic average of this sample is:
	
	and it standard deviation (maximum unbiased likelihood estimator of the standard deviation):
	 
	As we are in the situation of a known sample mean and an unknown sample variance, to do the calculation of a confidence interval, then we have proved in the section Statistics that we had to use:
	
	where $S$ is for recall another traditional notation in some areas of statistics for the notation of the empirical standard deviation (\SeeChapter{see section Statistics page \pageref{empirical standard deviation}}). We then have for the confidence interval at $95\%$ of the mean:
		
	Therefore:
		
	Which gives:
	
	The confidence interval can also be calculated by bootstrap (this is especially useful for complicate distributions that are not symmetric and when we focus on the median rather than on the mean). Then it is therefore obtained by the following algorithm (and abbreviated BCI for "bootstrapped confidence interval"):
	\begin{enumerate}
		\item From the initial sample, we simulate new samples of the same size, named "\NewTerm{bootstrap replicates}\index{bootstrap replicates}" of size $n$, by random draws with replacement (see figure above). For example with the previous series, we could get the following replicate:
		
		in which, by definition, some of the original sample values do not appear, and where others appear several times (yes it's a sampling with replacement therefore...). Several samples are simulated in this way. So we can form a number of replicas (arrangements with replacement) equal to (\SeeChapter{see section Probabilities page \pageref{simple arrangements with repetitions}}):
		
		Therefore with $10$ values we have $10,000,000,000$ possibilities...
		
		\item For each simulated sample, an average\footnote{in fact the process is the same for any estimate of any statistical indicator $\hat{\theta}$} is calculated (so we will have several thousand of averages!). 
	
		\item The $95\%$ confidence interval is then calculated on this set of averages (or any other estimator) by typically using the percentile calculation (through the functions of a spreadsheet software or a programming/scripting language). This grouping is named the "\NewTerm{bagging}\index{bagging}" that stands for the abbreviation of "\NewTerm{Bootstrap Aggregating}\index{bootstrap aggregating}".
	\end{enumerate}
	Obviously for each set of bootstrap, the percentiles themselves will not be the same so it is even possible to create a confidence interval for the percentiles themselves and this is named "\NewTerm{percentile bootstrap}\index{percentile bootstrap}"!  But one important assumption is that such a distribution is pivotal. This means that if the underlying parameter changes, the shape of the distribution is only shifted by a constant, and the scale does not necessarily change.
	
	It is quite easy (just like the Monte Carlo methods) to create replicas with spreadsheet software like Microsoft Excel (at least for people that know a little bit how to use a spreadsheet software) without computer programming or scripting (see below an example with Microsoft Excel)! Furthermore, the bootstrap technique is very powerful because it does not use any assumptions about the underlying statistical distribution. 

	The most common field of application of bootstrapping in "direct" business (I don't mind about Data Mining for Marketing that is not what I mean about "direct business") that I know is in project management during meetings where a dozen people estimates the duration of a project task or project phase.
	
	Bootstrapping can therefore be applied to any estimator other than the average, as the median, the correlation coefficient between two random variables or the principal eigenvalue of a variance-covariance matrix (for principal component analysis for example!), or the slope and intercept of a regression and this is its great strength!!! Indeed, for these estimators, there is no general mathematical relations that defines the standard error or confidence interval. The only methods applicable are resampling methods to which bootstrapping belongs to and this is intensively used since almost any home or office computer at the beginning of the 121st century (holocene calendar) is powerful enough to bootstrap small databases.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	As example let us first use a spreadsheet software like Microsoft Excel table 14.0.6123 and taking the theoretical companion example above as practical software example (we prohibit ourselves of doing VBA programming). We then build a small table with the previous sample:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/boostrapping_excel_initial_dataset.jpg}
	\end{figure}
	At the opposite of the companion theoretical example, we would like to be able to determine a confidence interval for the median instead of the arithmetic mean (we take on purpose a statistical indicator for which there is no simple analytical confidence interval). For this, we calculate the median of several thousand of replications in the column \texttt{F} (random choice!), where each replication corresponds to a row :
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/computing/boostrapping_excel_resampling_median.jpg}
	\end{figure}
	with the following quite long formula for Microsoft Excel Next 14.0.6123 to put in cell \texttt{F5} and then pull down to the end of the sheet:\\
	
	\texttt{=MEDIAN(INDEX($A$5:$A$14,RANDBETWEEN(1,10),1),\\
	INDEX($A$5:$A$14,RANDBETWEEN(1,10),1),\\
	INDEX($A$5:$A$14,RANDBETWEEN(1,10),1),\\
	INDEX($A$5:$A$14,RANDBETWEEN(1,10),1),\\
	INDEX($A$5:$A$14,RANDBETWEEN(1,10),1),\\
	INDEX($A$5:$A$14,RANDBETWEEN(1,10),1),\\
	INDEX($A$5:$A$14,RANDBETWEEN(1,10),1),\\
	INDEX($A$5:$A$14,RANDBETWEEN(1,10),1),\\
	INDEX($A$5:$A$14,RANDBETWEEN(1,10),1),\\
	INDEX($A$5:$A$14,RANDBETWEEN(1,10),1))
	}\\
	
	So we can not have more that $10$ billion more... corresponding to the $\bar{A}_n^n$ calculated above (Microsoft Excel 14.0.6123 and after is limited to $17,179,869,184$ cells...).\\
	
	Then simply in a cell of our choice we write:
	\begin{center}
	\texttt{=PERCENTILE(F5:F2003,0.025)}
	\end{center}
	and in another cell:
	\begin{center}
	\texttt{=PERCENTILE(F5:F2003,0.975)}
	\end{center}
	
	which will give us $2,000$ replications respectively $7$ and $29.5$.\\

	With basic knowledge of a spreadsheet software, it is possible to graphically plot the convergence of the median in function the number of replications (below we used only the first $100$ replications):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/computing/boostrapping_excel_median_convergence.jpg}
		\caption[]{Convergence of the median as function of the number of replications}
	\end{figure}
	Obviously, this chart will look different every time we restart the simulation in Microsoft Excel 14.0.6123 by pressing the F9 key.
	\end{tcolorbox}
	
	Finally let us indicate that after having studied earlier above many linear regression models this does avoid the fact that in many cases no theoretical model is adapted either to interpolate or verbatim to extrapolate some data. Therefore, if we have for each abscissa point (exogenous variable) a given quantity of values  of the output (endogenous) variable, we can therefore use the bootstrapping method which will give us the bootstrapped regression coefficients and also bootstrapped interpolated or extrapolated values! This is an extremely interesting technique  in practice of nonparametric regression! We can do such bootstrap regressions in softwares like SPSS (with additional module) or SAS but also in the free \texttt{R} software with the right packages (see the companion book on \texttt{R}).
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Bootstrapping has enormous potential in statistics education and practice, but there are subtle issues and ways to go wrong. For example, the common combination of nonparametric bootstrapping and bootstrap percentile confidence intervals seems very likely to be less accurate than using $T$-intervals for small samples, though more accurate for larger samples. We therefore strongly recommend anyone to read the following paper to avoid some traps about the usage of bootstrapping: Hesterberg TC.\textit{ What teachers should know about the bootstrap: Resampling in the undergraduate statistics curriculum.} The American Statistician. 12015 Oct 2;69(4):371-86 (holocene calendar).
	\end{tcolorbox}	
	
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Caution!!!!!!!!! Use bootstrap mainly for confidence intervals or parameters estimation. Not for testing hypotheses (under the null), unless you really know what you are doing! For NHST you should prefer permutations techniques that we will introduce further below.\\
	
	Indeed, we use bootstrap to generate data under the empirical distribution of the observed data. To get a $p$-value by bootstrap, we need to generate bootstraps under the null hypothesis!!!! So when conducting bootstrapped hypothesis tests like on the mean for example we must first subtract the empirical mean $\hat{\mu}$ of the sample and secondly add to it the mean under $H_0=\mu$. Then we compare the original observe sample mean $\hat{\mu}$ to our bootstrapped null distribution $H_0=\mu$ and see how many bootstrapped means are at least as extreme as $\hat{\mu}$ is. This gives us a $p$-value. 
	\end{tcolorbox}
	
	\pagebreak
	\subsubsection{Jackknife Resampling}\label{jackknife resampling}
	The jackknife was proposed by M.H. Quenouille in 11949 (holocene calendar) and later refined and given its current name by John Tukey in 11956 according to holocene calendar (it predates other common resampling methods such as the bootstrap). M.H. Quenouille originally developed the method as a procedure for correcting bias. Later, Tukey described its use in constructing confidence limits for a large class of estimators. It is similar to the bootstrap in that it involves resampling, but instead of sampling with replacement, the method samples without replacement.

	So in statistics, the "\NewTerm{jackknife}\index{jackknife}" is a resampling technique especially useful for variance and bias estimation at least... when these estimators converge quick enough (the number of resampling is much more limited with jackknifing than with bootstrapping). The jackknife estimator of a parameter is found by systematically leaving out each observation from a dataset and calculating the estimate and then finding the average of these calculations. Given a sample of size $N$, the jackknife estimate is found by aggregating the estimates of each $N-1$ estimate in the sample.
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
		
		%Shape: Rectangle [id:dp12466090143183495] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]  (50,39) -- (201.5,39) -- (201.5,179) -- (50,179) -- cycle ;
		%Shape: Rectangle [id:dp5392989434957713] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]  (337,107) -- (488.5,107) -- (488.5,247) -- (337,247) -- cycle ;
		%Shape: Rectangle [id:dp08320002052542774] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]  (321,289) -- (472.5,289) -- (472.5,429) -- (321,429) -- cycle ;
		%Shape: Rectangle [id:dp9658760532034605] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]  (106,246) -- (257.5,246) -- (257.5,386) -- (106,386) -- cycle ;
		%Straight Lines [id:da8425246618426836] 
		\draw    (124,181) -- (151,204.68) ;
		\draw [shift={(152.5,206)}, rotate = 221.26] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da9786924410377111] 
		\draw    (213,170) -- (321.07,275.6) ;
		\draw [shift={(322.5,277)}, rotate = 224.34] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da26529065741617885] 
		\draw    (215,126) -- (327.72,184.08) ;
		\draw [shift={(329.5,185)}, rotate = 207.26] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Circle [id:dp6477900242902217] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (59,64) .. controls (59,59.58) and (62.58,56) .. (67,56) .. controls (71.42,56) and (75,59.58) .. (75,64) .. controls (75,68.42) and (71.42,72) .. (67,72) .. controls (62.58,72) and (59,68.42) .. (59,64) -- cycle ;
		%Shape: Circle [id:dp9320904548315059] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (173,113) .. controls (173,108.58) and (176.58,105) .. (181,105) .. controls (185.42,105) and (189,108.58) .. (189,113) .. controls (189,117.42) and (185.42,121) .. (181,121) .. controls (176.58,121) and (173,117.42) .. (173,113) -- cycle ;
		%Shape: Circle [id:dp2629980787697117] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (69,146) .. controls (69,141.58) and (72.58,138) .. (77,138) .. controls (81.42,138) and (85,141.58) .. (85,146) .. controls (85,150.42) and (81.42,154) .. (77,154) .. controls (72.58,154) and (69,150.42) .. (69,146) -- cycle ;
		%Shape: Circle [id:dp5548463997118753] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (120,57) .. controls (120,52.58) and (123.58,49) .. (128,49) .. controls (132.42,49) and (136,52.58) .. (136,57) .. controls (136,61.42) and (132.42,65) .. (128,65) .. controls (123.58,65) and (120,61.42) .. (120,57) -- cycle ;
		%Shape: Circle [id:dp9767208674289514] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (159,64) .. controls (159,59.58) and (162.58,56) .. (167,56) .. controls (171.42,56) and (175,59.58) .. (175,64) .. controls (175,68.42) and (171.42,72) .. (167,72) .. controls (162.58,72) and (159,68.42) .. (159,64) -- cycle ;
		%Shape: Circle [id:dp2052094785232743] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (77,110) .. controls (77,105.58) and (80.58,102) .. (85,102) .. controls (89.42,102) and (93,105.58) .. (93,110) .. controls (93,114.42) and (89.42,118) .. (85,118) .. controls (80.58,118) and (77,114.42) .. (77,110) -- cycle ;
		%Shape: Circle [id:dp43925176156814016] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (133,115) .. controls (133,110.58) and (136.58,107) .. (141,107) .. controls (145.42,107) and (149,110.58) .. (149,115) .. controls (149,119.42) and (145.42,123) .. (141,123) .. controls (136.58,123) and (133,119.42) .. (133,115) -- cycle ;
		%Shape: Circle [id:dp3477256453947106] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (156,164) .. controls (156,159.58) and (159.58,156) .. (164,156) .. controls (168.42,156) and (172,159.58) .. (172,164) .. controls (172,168.42) and (168.42,172) .. (164,172) .. controls (159.58,172) and (156,168.42) .. (156,164) -- cycle ;
		%Shape: Circle [id:dp4533534966192545] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (104,145) .. controls (104,140.58) and (107.58,137) .. (112,137) .. controls (116.42,137) and (120,140.58) .. (120,145) .. controls (120,149.42) and (116.42,153) .. (112,153) .. controls (107.58,153) and (104,149.42) .. (104,145) -- cycle ;
		%Shape: Circle [id:dp7121157977994592] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (94,91) .. controls (94,86.58) and (97.58,83) .. (102,83) .. controls (106.42,83) and (110,86.58) .. (110,91) .. controls (110,95.42) and (106.42,99) .. (102,99) .. controls (97.58,99) and (94,95.42) .. (94,91) -- cycle ;
		%Shape: Circle [id:dp1105564690244567] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (352,127) .. controls (352,122.58) and (355.58,119) .. (360,119) .. controls (364.42,119) and (368,122.58) .. (368,127) .. controls (368,131.42) and (364.42,135) .. (360,135) .. controls (355.58,135) and (352,131.42) .. (352,127) -- cycle ;
		%Shape: Circle [id:dp3459135727308993] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (464,176) .. controls (464,171.58) and (467.58,168) .. (472,168) .. controls (476.42,168) and (480,171.58) .. (480,176) .. controls (480,180.42) and (476.42,184) .. (472,184) .. controls (467.58,184) and (464,180.42) .. (464,176) -- cycle ;
		%Shape: Circle [id:dp9098683459442347] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (360,210) .. controls (360,205.58) and (363.58,202) .. (368,202) .. controls (372.42,202) and (376,205.58) .. (376,210) .. controls (376,214.42) and (372.42,218) .. (368,218) .. controls (363.58,218) and (360,214.42) .. (360,210) -- cycle ;
		%Shape: Circle [id:dp9807031961481925] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (329,316) .. controls (329,311.58) and (332.58,308) .. (337,308) .. controls (341.42,308) and (345,311.58) .. (345,316) .. controls (345,320.42) and (341.42,324) .. (337,324) .. controls (332.58,324) and (329,320.42) .. (329,316) -- cycle ;
		%Shape: Circle [id:dp7956679266503064] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (443,365) .. controls (443,360.58) and (446.58,357) .. (451,357) .. controls (455.42,357) and (459,360.58) .. (459,365) .. controls (459,369.42) and (455.42,373) .. (451,373) .. controls (446.58,373) and (443,369.42) .. (443,365) -- cycle ;
		%Shape: Circle [id:dp2062140498403986] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (339,398) .. controls (339,393.58) and (342.58,390) .. (347,390) .. controls (351.42,390) and (355,393.58) .. (355,398) .. controls (355,402.42) and (351.42,406) .. (347,406) .. controls (342.58,406) and (339,402.42) .. (339,398) -- cycle ;
		%Shape: Circle [id:dp5503905000957026] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (229,320) .. controls (229,315.58) and (232.58,312) .. (237,312) .. controls (241.42,312) and (245,315.58) .. (245,320) .. controls (245,324.42) and (241.42,328) .. (237,328) .. controls (232.58,328) and (229,324.42) .. (229,320) -- cycle ;
		%Shape: Circle [id:dp9831551186993663] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (125,354) .. controls (125,349.58) and (128.58,346) .. (133,346) .. controls (137.42,346) and (141,349.58) .. (141,354) .. controls (141,358.42) and (137.42,362) .. (133,362) .. controls (128.58,362) and (125,358.42) .. (125,354) -- cycle ;
		%Shape: Circle [id:dp11802280918892327] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (116,272) .. controls (116,267.58) and (119.58,264) .. (124,264) .. controls (128.42,264) and (132,267.58) .. (132,272) .. controls (132,276.42) and (128.42,280) .. (124,280) .. controls (119.58,280) and (116,276.42) .. (116,272) -- cycle ;
		%Shape: Circle [id:dp4849980260562754] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (176,266) .. controls (176,261.58) and (179.58,258) .. (184,258) .. controls (188.42,258) and (192,261.58) .. (192,266) .. controls (192,270.42) and (188.42,274) .. (184,274) .. controls (179.58,274) and (176,270.42) .. (176,266) -- cycle ;
		%Shape: Circle [id:dp985238802674778] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (214,271) .. controls (214,266.58) and (217.58,263) .. (222,263) .. controls (226.42,263) and (230,266.58) .. (230,271) .. controls (230,275.42) and (226.42,279) .. (222,279) .. controls (217.58,279) and (214,275.42) .. (214,271) -- cycle ;
		%Shape: Circle [id:dp07542119335196507] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (189,324) .. controls (189,319.58) and (192.58,316) .. (197,316) .. controls (201.42,316) and (205,319.58) .. (205,324) .. controls (205,328.42) and (201.42,332) .. (197,332) .. controls (192.58,332) and (189,328.42) .. (189,324) -- cycle ;
		%Shape: Circle [id:dp8112833225544511] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (211,371) .. controls (211,366.58) and (214.58,363) .. (219,363) .. controls (223.42,363) and (227,366.58) .. (227,371) .. controls (227,375.42) and (223.42,379) .. (219,379) .. controls (214.58,379) and (211,375.42) .. (211,371) -- cycle ;
		%Shape: Circle [id:dp6208362074570379] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (160,353) .. controls (160,348.58) and (163.58,345) .. (168,345) .. controls (172.42,345) and (176,348.58) .. (176,353) .. controls (176,357.42) and (172.42,361) .. (168,361) .. controls (163.58,361) and (160,357.42) .. (160,353) -- cycle ;
		%Shape: Circle [id:dp9172634106537032] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (132,317) .. controls (132,312.58) and (135.58,309) .. (140,309) .. controls (144.42,309) and (148,312.58) .. (148,317) .. controls (148,321.42) and (144.42,325) .. (140,325) .. controls (135.58,325) and (132,321.42) .. (132,317) -- cycle ;
		%Shape: Circle [id:dp31103340181828654] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (396,209) .. controls (396,204.58) and (399.58,201) .. (404,201) .. controls (408.42,201) and (412,204.58) .. (412,209) .. controls (412,213.42) and (408.42,217) .. (404,217) .. controls (399.58,217) and (396,213.42) .. (396,209) -- cycle ;
		%Shape: Circle [id:dp9318740045242557] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (446,227) .. controls (446,222.58) and (449.58,219) .. (454,219) .. controls (458.42,219) and (462,222.58) .. (462,227) .. controls (462,231.42) and (458.42,235) .. (454,235) .. controls (449.58,235) and (446,231.42) .. (446,227) -- cycle ;
		%Shape: Circle [id:dp4259055225912396] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (425,180) .. controls (425,175.58) and (428.58,172) .. (433,172) .. controls (437.42,172) and (441,175.58) .. (441,180) .. controls (441,184.42) and (437.42,188) .. (433,188) .. controls (428.58,188) and (425,184.42) .. (425,180) -- cycle ;
		%Shape: Circle [id:dp9374302700847867] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (449,127) .. controls (449,122.58) and (452.58,119) .. (457,119) .. controls (461.42,119) and (465,122.58) .. (465,127) .. controls (465,131.42) and (461.42,135) .. (457,135) .. controls (452.58,135) and (449,131.42) .. (449,127) -- cycle ;
		%Shape: Circle [id:dp4429482562251936] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (411,120) .. controls (411,115.58) and (414.58,112) .. (419,112) .. controls (423.42,112) and (427,115.58) .. (427,120) .. controls (427,124.42) and (423.42,128) .. (419,128) .. controls (414.58,128) and (411,124.42) .. (411,120) -- cycle ;
		%Shape: Circle [id:dp4143222853656914] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (428,315) .. controls (428,310.58) and (431.58,307) .. (436,307) .. controls (440.42,307) and (444,310.58) .. (444,315) .. controls (444,319.42) and (440.42,323) .. (436,323) .. controls (431.58,323) and (428,319.42) .. (428,315) -- cycle ;
		%Shape: Circle [id:dp11510150399873575] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (390,309) .. controls (390,304.58) and (393.58,301) .. (398,301) .. controls (402.42,301) and (406,304.58) .. (406,309) .. controls (406,313.42) and (402.42,317) .. (398,317) .. controls (393.58,317) and (390,313.42) .. (390,309) -- cycle ;
		%Shape: Circle [id:dp1635631632207848] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (403,368) .. controls (403,363.58) and (406.58,360) .. (411,360) .. controls (415.42,360) and (419,363.58) .. (419,368) .. controls (419,372.42) and (415.42,376) .. (411,376) .. controls (406.58,376) and (403,372.42) .. (403,368) -- cycle ;
		%Shape: Circle [id:dp7232515048366577] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (374,397) .. controls (374,392.58) and (377.58,389) .. (382,389) .. controls (386.42,389) and (390,392.58) .. (390,397) .. controls (390,401.42) and (386.42,405) .. (382,405) .. controls (377.58,405) and (374,401.42) .. (374,397) -- cycle ;
		%Shape: Circle [id:dp8695084147105039] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (345,362) .. controls (345,357.58) and (348.58,354) .. (353,354) .. controls (357.42,354) and (361,357.58) .. (361,362) .. controls (361,366.42) and (357.42,370) .. (353,370) .. controls (348.58,370) and (345,366.42) .. (345,362) -- cycle ;
		%Shape: Circle [id:dp5287531984756051] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (384,155) .. controls (384,150.58) and (387.58,147) .. (392,147) .. controls (396.42,147) and (400,150.58) .. (400,155) .. controls (400,159.42) and (396.42,163) .. (392,163) .. controls (387.58,163) and (384,159.42) .. (384,155) -- cycle ;
		%Shape: Circle [id:dp4459700727378544] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (362,344) .. controls (362,339.58) and (365.58,336) .. (370,336) .. controls (374.42,336) and (378,339.58) .. (378,344) .. controls (378,348.42) and (374.42,352) .. (370,352) .. controls (365.58,352) and (362,348.42) .. (362,344) -- cycle ;
		
		% Text Node
		\draw (131,210) node [anchor=north west][inner sep=0.75pt]   [align=left] {Jackknife\\Sample $\displaystyle 1$};
		% Text Node
		\draw (484,301) node [anchor=north west][inner sep=0.75pt]   [align=left] {Jackknife\\Sample $\displaystyle 2$};
		% Text Node
		\draw (506,113) node [anchor=north west][inner sep=0.75pt]   [align=left] {Jackknife\\Sample $\displaystyle 3$};
		% Text Node
		\draw (468,438) node [anchor=north west][inner sep=0.75pt]   [align=left] {...Jackknife\\Sample $\displaystyle N$};
		% Text Node
		\draw (69,16) node [anchor=north west][inner sep=0.75pt]   [align=left] {Empirical Data};
		
		\end{tikzpicture}
	\end{figure}
	This method is especially useful when:
	\begin{enumerate}
		\item The computer is not powerful enough to run a bootstrap
		\item We don't trust the resampling method as not suited to the situation
	\end{enumerate}
	\textbf{Definition (\#\thesection.\mydef):} The "\NewTerm{delete-1 Jackknife samples}\index{delete-1 Jackknife samples}" are selected by taking the original data vector and deleting one observation from the set. Thus, there are $n$ unique Jackknife samples, and the $i$th Jackknife sample vector is defined as:

	This procedure is obviously generalizable to $k$ deletions.

	The $i$th Jackknife replicate is defined as the value of the estimator $s(\cdot)$ evaluated at the $i$th Jackknife sample.
	
	As we will prove it further below, the jackknife standard error of the estimator is (given typically by the bootstrap package of \texttt{R} as you can see it in the corresponding companion book):
	
	where $\hat{\theta}_{(\cdot)}$ is the empirical average of the Jackknife replicates:
	
	with:
	
	For the proof let us consider the special case where the Jackknife estimator above is an unbiased estimator of the variance of the sample mean.
	\begin{dem}
	So to prove the previous relation with the sample mean we just need to prove that (\SeeChapter{see section Statistics page \pageref{standard error}}):
	
	is equal to:
	
	And to prove that latter, we write it craftily:
   
   Once the term is squared, the equation is complete, and is identically equal to the right hand term above. Thus, in the case of the sample mean, the Jackknife estimate of the standard error reduces to the regular, unbiased estimator commonly used.\\
   
   	We say sometimes then that $n-1$ is the "\NewTerm{standard error jackknife bias inflation factor}\index{standard error jackknife bias inflation factor}".
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	As practical business oriented example of the use of Jackknife let me give the example of a customer (Fortune 500 company) that has to analyse worldwide counterfeiting of its products by sampling and controlling completely each year a given area of a given city chosen randomly in various countries and calculating the sum by country of counterfeit products as it has a major influence on the strategy of the company, national politics and borders controls (the counterfeiting being manly done by the mafia). As the geographical sampling error cannot be calculated as it is not guarantee that the chosen city has the same heterogeneity overall cities of the country it is as far as we know impossible to calculate with a closed form equation a tolerance interval for the real total counterfeit. So an easy way to get a tolerance interval is to make a Jackknife resampling as it is easily acceptable for the board committee to make an analysis of what would have been the sum if for example we removed $50\%$ of the sampling (at the condition that the a posteriori power of the test is still important enough!).
	
	\subsubsection{Permutation Tests}
	A "\NewTerm{permutation test}\index{statistical tests!permutation test}" (also named a "\NewTerm{randomization test}\index{randomization test}" or "\NewTerm{re-randomization test}") is a type of statistical significance test in which the distribution of the test statistic under the null hypothesis is obtained by calculating all possible values of the test statistic under rearrangements of the labels on the observed data points. 
	
	To illustrate the basic idea of a permutation test, suppose we have two groups (we take the special case where the size of the groups are equal!):
	 
	whose sample means are $\bar{x}_{A}$ and $\bar{x}_{B}$, and that we want to test, at $5\%$ significance level, whether they come from the same distribution. Let $n$ be the sample size corresponding to each group. The permutation test is designed to determine whether the observed difference between the sample means is large enough to reject the null hypothesis:
	
	that the two groups have identical probability distributions (i.e. identical mean).
	
	The test proceeds as follows:
	\begin{enumerate}
		\item The difference in means between the two samples is calculated as we do normally. This lead to the observed value of the test statistic $T_n^{\text{(obs)}}$.
		
		\item Then the observations of groups $\vec{A}$ and $\vec{B}$ are pooled and mixed formally using a "\NewTerm{random sampling operator}\index{random sampling operator}" $\vec{\delta}_n^R$ defined by:
		
		where $r_i$ is a binary random variable defined by $r_i\in\mathbb{Z}_2$, where $\mathbb{Z}_2=\{0,1\}$ and we also define its binary negation by:
		
		Then we create a permutation of $\vec{A}$ denoted $\vec{A}\,'$ (same respectively for $\vec{B}$) by applying the random sampling operator:
		
		
		\item Next, the difference in sample means is calculated and recorded for this new groups $\vec{A}'$ and $\vec{B}'$ and we get a new $T_{n}^{'\text{(obs)}}$.
		
		\item We repeat the above procedure for every possible way of combining the samples (in practice we do that completely randomly then some cases - pairs of samples - may repeat).
		
		The number of ways in which $m\cdot n$ different items can be divided equally into $m$ groups ($m=2$ in our example) each containing $n$ objects where the order of the groups is important is given by:
		
		And the number of ways in which $m\cdot n$ different items can be divided equally into $m$ groups, each containing $n$ objects and where the order of the groups is NOT important is:
		
		
		\item Finally the one-sided $p$-value of the test is calculated as the proportion of sampled permutations where the difference in means was greater than or equal to $T_{n}^{\text{(obs)}}$. The two-sided $p$-value of the test is calculated as the proportion of sampled permutations where the absolute difference was greater than or equal to $|T_{n}^{'\text{(obs)}}|$.
	\end{enumerate}
	
	The major down-side to permutation tests are that they:
	\begin{itemize}
		\item Can be computationally intensive and may require "custom" code for difficult-to-calculate statistics. This must be rewritten for every case.
		\item Are primarily used to provide a $p$-value. The inversion of the test to get confidence regions/intervals requires even more computation.
	\end{itemize}
	Hopefully it is quite easy to do that with modern software. You can see the \texttt{R} companion book for an example of a Student-$T$ permutation test.
	
	Keep in mind that as the assumption behind a permutation test is that the observations are exchangeable under the null hypothesis (assumption of "data exchangeability"), an important consequence of this assumption is that tests of difference in location (like a permutation $T$-test) require equal variance. 
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Permutation tests are a subset of nonparametric statistics as the basic premise is to use only the assumption that it is possible that all of the treatment groups are equivalent, and that every member of them is the same before sampling began (i.e. the slot that they fill is not differentiable from other slots before the slots are filled).
	\end{tcolorbox}
	
	\pagebreak
	\subsection{Finite difference method (F.D.M.)}\label{finite difference method}
	The "\NewTerm{finite element method}\index{finite element method}" (FEM) is a numerical technique for finding approximate solutions to boundary value problems for partial differential equations. It is also referred to as finite element analysis (FEA). FEM subdivides a large problem into smaller, simpler, parts, named "finite elements". The simple equations that model these finite elements are then assembled into a larger system of equations that models the entire problem. FEM then uses variational methods from the calculus of variations to approximate a solution by minimizing an associated error function.
	
	\subsubsection{One space dimension F.D.M.}\label{one space dimension fdm}
	Let us recall that we have proved in the section of Thermodynamics (see page \pageref{heat equation}) the following heat diffusion equation (we present here the equation reduced to only one spatial dimension):
	
	and let us notice that this equation is not very general ... (it is not relativistic and does not take into account the heat generated in the form of radiation by the concerned material concerned or many other factors ...).

	We can consider (\SeeChapter{see section Differential and Integral Calculus page \pageref{differential calculus}}) that:
	
	and:
	
	Also:
	
	Then the heat equation becomes:
	
	After rearranging, we have:
	
	If we look at this relation more closely, we see that this is a simple recursion. We just need to know the initial distribution (initial conditions) $T(x,0)$ to determine the distribution then all other values as:
	
	and:
	
	etc.
	It is possible to implement such a simulation with nothing but a small spreadsheet software and a little time as we will we see just after... (using a spreadsheet software to understand the mechanism is better than using a blackbox like Maple or MATLAB™ by my experience).

	For information $h$ and $k$ are named then the "\NewTerm{mesh step}\index{mesh step}" or "\NewTerm{space step}\index{space step}" of the model.
	
	Let us see an application example with a spreadsheet software like Microsoft Excel as it is quite a good practical exercise to understand how to implement the method.
	
	So let us consider the following worksheet\footnote{source: \url{http://excelcalculations.blogspot.co.at/2011/04/solving-1d-heat-equation-using-finite.html}} where we consider a bar that is initially at a temperature of $0$ [C]and that is heated on the left-hand side at a constant temperature of $100$ [C] and where we use the relation proved earlier above:
	
	So this gives (first rows only of $1,000$ rows): 
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.62]{img/computing/heat_equation_1d_excel_calculations.jpg}
		\caption{1D Heat Equation FDM calculations in Microsoft Excel 14.0.7172}
	\end{figure}
	Explicitly for only a few rows a and few columns visible, we have:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.42]{img/computing/heat_equation_1d_excel_formulas.jpg}
		\caption{1D Heat Equation FDM explicit formulas in Microsoft Excel 14.0.7172}
	\end{figure}
	We have above taken for boundary conditions:
	 
	
	All this with a chart view (famous figure that is painful to obtain in a spreadsheet software...):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/computing/heat_equation_1d_excel_plot.jpg}
		\caption{1D Heat Equation FDM plot in Microsoft Excel 14.0.7172}
	\end{figure}
	

	For readers wishing to practice with real values ... a longitudinal Iron bar of $1$ [kg] has a specific heat capacity of $450\;[\text{J}\cdot\text{kg}^{-1}\cdot\text{K}^{-1}]$, a density of almost $7.88\;[\text{kg}\cdot \text{m}^{-3}]$ and a thermal conductivity of $82\;[\text{J}\cdot\text{s}^{-1}\cdot\text{m}^{-1}\cdot\text{K}^{-1}]$.
	
	However with such a file a above the reader will see that the FDM is stable   after some trials and errors if and only if:
	
	This is what we will study now, first with a simple example, and afterwards with a more elaborated one:
	
	\paragraph{Numerical instability}\mbox{}\\\\
	Numerical instability is a huge issues in numerical computing that occurs in many situation (the most know one being the finite elements method). To explain this let us consider the following companion example:
	
	with $n\in\mathbb{N}$.
	
	An immediate calculation gives:
	
	This allow us to calculate $I_n$ by recurrence with:
	
	This problem apparently well posed mathematically leads digitally to catastrophic results. Indeed we have:
	
	even if we neglect the rounding error on $1/n$. The error on $I_n$ explode exponentially, the initial error on $I_0$ being multiplied by $10^n$ at the step $n$.
	
	\paragraph{von Neumann stability}\mbox{}\\\\
	In numerical analysis, "\NewTerm{von Neumann stability analysis}\index{von Neumann stability analysis}" (also known as "\NewTerm{Fourier stability analysis}\index{Fourier stability analysis}") is a procedure used to check the stability of finite difference schemes as applied to linear partial differential equations. The analysis is based on the Fourier decomposition of numerical error and was developed at Los Alamos National Laboratory after having been briefly described in a 11947 (holocene calendar) article by British researchers Crank and Nicolson. This method is an example of explicit time integration where the function that defines governing equation is evaluated at the current time. Later, the method was given a more rigorous treatment in an article co-authored by John von Neumann.
	
	The stability of numerical schemes is closely associated with numerical error. A finite difference scheme is stable if the errors made at one time step of the calculation do not cause the errors to be magnified as the computations are continued. A neutrally stable scheme is one in which errors remain constant as the computations are carried forward. If the errors decay and eventually damp out, the numerical scheme is said to be stable. If, on the contrary, the errors grow with time the numerical scheme is said to be unstable. The stability of numerical schemes can be investigated by performing von Neumann stability analysis. For time-dependent problems, stability guarantees that the numerical method produces a bounded solution whenever the solution of the exact differential equation is bounded. Stability, in general, can be difficult to investigate, especially when the equation under consideration is non-linear.
	 
	The von Neumann method is based on the decomposition of the errors into Fourier series. To illustrate the procedure, consider the one-dimensional heat equation:
	
	defined on the spatial interval $L$, which can be discretized as we have just proved as (in a very condensed form):
	
	where as we have just proved earlier:
	
	and the solution $T_j^n$ of the discrete equation approximates the analytical solution $T(x,t)$ of the PDE on the grid.
	
	Let us define the round-off error $\varepsilon_j^n$ as:
	
	where $T_j^n$ is the solution of the discretized PDE as we knot it that would be computed in the absence of round-off error, and $N_j^n$ is the numerical solution obtained in finite precision arithmetic. Since the exact solution $T_j^n$ must satisfy the discretized PDE exactly, the error $\varepsilon_j^n$ must also satisfy the discretized equation (superposition principle). Here we assumed that $N_j^n$ satisfies the PDE, too (this is only true in machine precision). Thus:
	
	is a recurrence relation for the error. 

	Equations:
	
	show that both the error and the numerical solution have the same growth or decay behaviour with respect to time. For linear differential equations with periodic boundary condition, the spatial variation of error may be expanded in a finite Fourier series, in the interval $L$, as:
	As we have proved it in the section Thermodynamics the solution of the PDE for the errors can be written in a condensed way for a given term:
	
	Since the difference equation for error is linear (the behaviour of each term of the series is the same as series itself), it is enough to consider the growth of error of a typical term:
	
	and as the reader will see with the next development we can simplify already the constant such that it remains:
	
	The stability characteristics can be studied using just this form for the error with no loss in generality. To find out how error varies in steps of time, substitute the relation above into:
	
	after noting that:
	
	to yield (after simplification):
	
	Using the identities (\SeeChapter{see section Trigonometry page \pageref{hyperbolic trigonometry}}):
	
	Then the prior previous relation can the be written as:
	
	Let us now define the "amplification factor":
	
	The necessary and sufficient condition for the error to remain bounded is that $|G|<1$. However in our case:
	
	this is the explicit condition for stability of the numerical scheme.
	
	Note that the term:
	
	is always positive. Thus, to satisfy the prior previous relation we have:
	
	For the above condition to hold at all:
	
	we have:
	
	It gives the stability requirement for the FTCS scheme as applied to one-dimensional heat equation. This is exactly the value we found when playing with the Microsoft Excel worksheet.
	
	\subsubsection{Space-time F.D.M (finite-volume method)}
	The finite-volume method (FVM) is a method for representing and evaluating partial differential equations in the form of algebraic equations. Similar to the finite difference method or finite element method, values are calculated at discrete places on a meshed geometry. 
	
	"Finite volume" refers to the small volume surrounding each node point on a mesh. In the finite volume method, volume integrals in a partial differential equation that contain a divergence term are converted to surface integrals, using the divergence theorem. These terms are then evaluated as fluxes at the surfaces of each finite volume. Because the flux entering a given volume is identical to that leaving the adjacent volume, these methods are conservative. Another advantage of the finite volume method is that it is easily formulated to allow for unstructured meshes. The method is used in many computational fluid dynamics computing packages as illustrated below:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.95]{img/computing/fdm_car.jpg}
		\caption{Space-time FDM for car $C_x$ study}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/fdm_airplane_wing.jpg}
		\caption{Space-time FDM for airplane wing profile study}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/fdm_nasa_spaceshuttle_launch.jpg}
		\caption[Space-time FDM for NASA space shuttle launch]{Space-time FDM for NASA space shuttle launch (source: NASA)}
	\end{figure}
	The F.D.M. is a therefore a veeeeery important numerical method in practice because it also gives the possibility to solve directly Maxwell's equations in the time domain and space (and also General Relativity situations). It is then the classified in the $3$D (three-dimensional quantification of space) and temporal computational methods and finds its main industrial applications in the fields of design (antennas and circuits), of electromagnetic compatibility, of the diffraction and of the propagation and electromagnetic dosimetry (living beings and waves interactions).
	
	We will discuss now the basics of the concept in a special case as in practice, programming the F.D.M. is a whole team job in itself (like the rest of this book obviously but sometimes it is useful to recall that). Indeed a loot of problems must be resolved when dealing with computer programs using F.D.M. (convergence criteria, meshing methods, boundary conditions, user input, programming language methods, etc.).

	In a electrodynamics problem treated by F.D.M., the first necessary step is to define the volume $V$ of the space and the time interval $I = [0, T]$ for which the resolution is desired (it is unrealistic at this day to hope to solve Maxwell's equations for an infinite space and for an unlimited period of time!). The volume of calculation contains the object (antenna circuit, ...) that it is desired to characterize, in response to a given excitation. Secondly, the space (meshing of $V$) and time should be discretized to allow a numerical implementation of the resolution (and in reality the meshing is not uniform...). The problem then becomes the one of determining the field at any point of the mesh for any discrete moment of the observation time interval. The spatial and temporal discretization will be specified in what will follow below and will naturally come from physics equations to solve. They obviously condition both the accuracy of the calculation results and the computing resources required to carry it out.

	The structuring of the F.D.M. mesh and the resolution method directly result of the equations to solve.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.9]{img/computing/fdm_mesh_01.jpg}
		\caption{Airplane typical FDM meshing}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/fdm_mesh_02.jpg}
		\caption{Mechanical element FDM meshing}
	\end{figure}
	In a linear, homogeneous, isotropic, non-dispersive and non-magnetic (...) material, the Maxwell equations will be written explicitly based on the third Maxwell's equation (\SeeChapter{see section Electrodynamics page \pageref{third maxwell equation}}):
	
	Either explicitly with the negative sign put at the other side of the equality:
	
	And we will also use the fourth Maxwell equation without sources:
	
	Thus explicitly and rearranged:
	
	That is to say for summary:
	
	In what follows, we will only concerned with the first equation, the other leading to similar developments.

	To allow a computer processing, the various derivatives present in the equation must be approximated numerically as we already know. To do this, we use the principle of centered finite difference which is based on the following Taylor series expansions for recall (\SeeChapter{see section Sequences and Series page \pageref{taylor series}}):
	
	We then have on the basis of this principle:
	
	If we neglect the terms of the second order, it comes by subtracting the two series:
	
	where $\varepsilon(\mathrm{d}_x^2)$ is an error or order $2$, neglected thereafter (we notice that this is the centering that, allowing compensation of second derivatives, minimizes the error in the approximation).
	
	Applying this principle to temporal and spatial derivatives of:
	
	it comes:
	
	or after rearrangement:
	
	This relation shows that if we know the components $E_y,E_z$ of electric field at time $t$ and the component $B_x$ of the magnetic field at the earlier time $t-\mathrm{d}_t/2$, it is possible to determine $B_x$ at the time $t+\mathrm{d}_t/2$. Obviously the process is exactly the same for all other components and shows the same time lag. This result suggests using an iterative numerical solution, in which the electric and magnetic fields are evaluated alternately, respectively at the discrete time $n\mathrm{d}_t$ and $(n+1/2)\mathrm{d}_t$, $\mathrm{d}_t$ being the time step (denoted $\Delta t$ by the computer scientists). It is customary in the literature to denote by $B_x^{n+\frac{1}{2}}$ the component of the magnetic field at the time $(n+1/2)\mathrm{d}_t$.
	
	The same analysis applies for the spatial distribution of the field on the observed points. Thus, evaluating $B_x$ at the point $(x,y,z)$ is based on the knowledge of $E_y$ at the points $(x,y,z+\mathrm{d}_z/2)$ and $(x,y,z-\mathrm{d}_z/2)$  and of $E_z$ at the points $(x,y+\mathrm{d}_y/2,z)$ and $(x,y-\mathrm{d}_y/2,z)$.

	So we can summarize this geometrically in the following figure named "\NewTerm{Yee cell}\index{Yee cell}":
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
		
		%Shape: Cube [id:dp3292950142922535] 
		\draw  [line width=1.5]  (204,159.6) -- (300,63.6) -- (411.5,63.6) -- (411.5,158) -- (315.5,254) -- (204,254) -- cycle ; \draw  [line width=1.5]  (411.5,63.6) -- (315.5,159.6) -- (204,159.6) ; \draw  [line width=1.5]  (315.5,159.6) -- (315.5,254) ;
		%Straight Lines [id:da3168560944377494] 
		\draw    (315.5,254) -- (315.5,88.6) ;
		\draw [shift={(315.5,86.6)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da37276208051551074] 
		\draw    (315.5,254) -- (156.5,254) ;
		\draw [shift={(154.5,254)}, rotate = 360] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da9607748349491476] 
		\draw    (315.5,254) -- (438.09,131.41) ;
		\draw [shift={(439.5,130)}, rotate = 135] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da4072565690994445] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (204,254) -- (300,158) ;
		%Straight Lines [id:da5576273128498264] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (300,158) -- (411.5,158) ;
		%Straight Lines [id:da07589062241719802] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (300,158) -- (300,63.6) ;
		%Straight Lines [id:da7011251337701188] 
		\draw    (178.5,160.6) -- (178.5,249.6) ;
		\draw [shift={(178.5,251.6)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(178.5,158.6)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da5924494043526913] 
		\draw    (205.5,286.6) -- (314.5,286.6) ;
		\draw [shift={(316.5,286.6)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(203.5,286.6)}, rotate = 0] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da41822164046810273] 
		\draw    (358.91,255.19) -- (451.09,163.01) ;
		\draw [shift={(452.5,161.6)}, rotate = 135] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(357.5,256.6)}, rotate = 315] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da18739503106299393] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (252.5,254) -- (306.5,199.6) ;
		%Straight Lines [id:da6388804021740349] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (306.5,199.6) -- (368.5,199.6) ;
		%Straight Lines [id:da6087129829825322] 
		\draw    (252,206) -- (314.5,206) ;
		%Straight Lines [id:da05601101478392145] 
		\draw    (252.5,254) -- (252.5,204.6) ;
		%Straight Lines [id:da4608986355423028] 
		\draw    (314.5,206) -- (367.5,151.6) ;
		%Straight Lines [id:da3449996585675381] 
		\draw    (368.5,199.6) -- (367.5,151.6) ;
		%Shape: Circle [id:dp7716160775075982] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (248.2,254) .. controls (248.2,251.63) and (250.13,249.7) .. (252.5,249.7) .. controls (254.87,249.7) and (256.8,251.63) .. (256.8,254) .. controls (256.8,256.37) and (254.87,258.3) .. (252.5,258.3) .. controls (250.13,258.3) and (248.2,256.37) .. (248.2,254) -- cycle ;
		%Shape: Circle [id:dp23424013832853996] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (247.7,206) .. controls (247.7,203.63) and (249.63,201.7) .. (252,201.7) .. controls (254.37,201.7) and (256.3,203.63) .. (256.3,206) .. controls (256.3,208.37) and (254.37,210.3) .. (252,210.3) .. controls (249.63,210.3) and (247.7,208.37) .. (247.7,206) -- cycle ;
		%Shape: Circle [id:dp9853391403763845] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (310.2,206) .. controls (310.2,203.63) and (312.13,201.7) .. (314.5,201.7) .. controls (316.87,201.7) and (318.8,203.63) .. (318.8,206) .. controls (318.8,208.37) and (316.87,210.3) .. (314.5,210.3) .. controls (312.13,210.3) and (310.2,208.37) .. (310.2,206) -- cycle ;
		%Shape: Circle [id:dp3996095279397658] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (302.2,199.6) .. controls (302.2,197.23) and (304.13,195.3) .. (306.5,195.3) .. controls (308.87,195.3) and (310.8,197.23) .. (310.8,199.6) .. controls (310.8,201.97) and (308.87,203.9) .. (306.5,203.9) .. controls (304.13,203.9) and (302.2,201.97) .. (302.2,199.6) -- cycle ;
		%Shape: Circle [id:dp4775379421804533] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (364.2,199.6) .. controls (364.2,197.23) and (366.13,195.3) .. (368.5,195.3) .. controls (370.87,195.3) and (372.8,197.23) .. (372.8,199.6) .. controls (372.8,201.97) and (370.87,203.9) .. (368.5,203.9) .. controls (366.13,203.9) and (364.2,201.97) .. (364.2,199.6) -- cycle ;
		%Shape: Circle [id:dp7895452356061705] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (363.2,151.6) .. controls (363.2,149.23) and (365.13,147.3) .. (367.5,147.3) .. controls (369.87,147.3) and (371.8,149.23) .. (371.8,151.6) .. controls (371.8,153.97) and (369.87,155.9) .. (367.5,155.9) .. controls (365.13,155.9) and (363.2,153.97) .. (363.2,151.6) -- cycle ;
		
		% Text Node
		\draw (441.5,133.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (324.5,82.4) node [anchor=north west][inner sep=0.75pt]    {$z$};
		% Text Node
		\draw (144.5,256.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (159,191.4) node [anchor=north west][inner sep=0.75pt]    {$d_{y}$};
		% Text Node
		\draw (419,202.4) node [anchor=north west][inner sep=0.75pt]    {$d_{x}$};
		% Text Node
		\draw (249,290.4) node [anchor=north west][inner sep=0.75pt]    {$d_{x}$};
		% Text Node
		\draw (239,182.4) node [anchor=north west][inner sep=0.75pt]    {$B_{x}$};
		% Text Node
		\draw (316.5,205.1) node [anchor=north west][inner sep=0.75pt]    {$E_{z}$};
		% Text Node
		\draw (285.5,177.1) node [anchor=north west][inner sep=0.75pt]    {$B_{z}$};
		% Text Node
		\draw (360.5,128.1) node [anchor=north west][inner sep=0.75pt]    {$B_{y}$};
		% Text Node
		\draw (370.2,203) node [anchor=north west][inner sep=0.75pt]    {$E_{x}$};
		% Text Node
		\draw (242.2,262) node [anchor=north west][inner sep=0.75pt]    {$E_{y}$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Generic Yee cell in parallelepiped mesh element}
	\end{figure}
	The electric field components are evaluated at the centers of the edges of the mesh and the components of the magnetic field at the centers of the faces so as to ensure the alternation imposed by the equations (as mentioned previously we name "Yee cell" the unit cell with this distribution of points).
	
	In the special case of a propagating electromagnetic wave, the $\vec{E}$ and $\vec{B}$ fields are always perpendicular (\SeeChapter{see section Electrodynamics page \pageref{perpendicularity electric magnetic field wave}}) in a homogeneous, linear, anisotropic medium (this type of media includes many things like air, water, glass without stress or tempering) and when the engineers prefers to represent the "real" magnetic field $\vec{H}$ instead of the magnetic excitation $\vec{H}$ we can find in the literature the following parallelepiped Yee cell figure:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
		
		%Straight Lines [id:da44872429113207346] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (216.65,234.85) -- (442.65,234.85) ;
		%Straight Lines [id:da5989514152127873] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (326.65,123.85) -- (326.65,338.85) ;
		%Straight Lines [id:da646747376010204] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (126.35,325.15) -- (352.35,325.15) ;
		%Shape: Axis 2D [id:dp34395722105477655] 
		\draw  (189.5,337) -- (505.5,337)(214.5,69) -- (214.5,368) (498.5,332) -- (505.5,337) -- (498.5,342) (209.5,76) -- (214.5,69) -- (219.5,76)  ;
		%Straight Lines [id:da5465826292297553] 
		\draw    (231.5,320) -- (94.93,453.6) ;
		\draw [shift={(93.5,455)}, rotate = 315.63] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Shape: Cube [id:dp08869067444790901] 
		\draw  [line width=1.5]  (125,214.3) -- (215.3,124) -- (445.5,124) -- (445.5,334.7) -- (355.2,425) -- (125,425) -- cycle ; \draw  [line width=1.5]  (445.5,124) -- (355.2,214.3) -- (125,214.3) ; \draw  [line width=1.5]  (355.2,214.3) -- (355.2,425) ;
		%Straight Lines [id:da9624049426007477] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (174.5,167) -- (174.5,375) ;
		%Straight Lines [id:da14480759611019556] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (174.5,167) -- (400.5,167) ;
		%Straight Lines [id:da911383264184431] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (400.5,167) -- (400.5,375) ;
		%Straight Lines [id:da4283665549790545] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (174.5,375) -- (400.5,375) ;
		%Straight Lines [id:da2663255971420808] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (241.35,425.15) -- (331.65,334.85) ;
		%Straight Lines [id:da13301962993741] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (126.35,325.15) -- (216.65,234.85) ;
		%Straight Lines [id:da017637667765791454] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (355.35,324.15) -- (445.65,233.85) ;
		%Straight Lines [id:da9516797723957766] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (242.35,212.15) -- (332.65,121.85) ;
		%Shape: Circle [id:dp5446094116322617] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (119.35,325.15) .. controls (119.35,321.28) and (122.48,318.15) .. (126.35,318.15) .. controls (130.22,318.15) and (133.35,321.28) .. (133.35,325.15) .. controls (133.35,329.02) and (130.22,332.15) .. (126.35,332.15) .. controls (122.48,332.15) and (119.35,329.02) .. (119.35,325.15) -- cycle ;
		%Shape: Circle [id:dp5882953074900017] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (167.5,167) .. controls (167.5,163.13) and (170.63,160) .. (174.5,160) .. controls (178.37,160) and (181.5,163.13) .. (181.5,167) .. controls (181.5,170.87) and (178.37,174) .. (174.5,174) .. controls (170.63,174) and (167.5,170.87) .. (167.5,167) -- cycle ;
		%Shape: Circle [id:dp2824523488060591] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (209.65,234.85) .. controls (209.65,230.98) and (212.78,227.85) .. (216.65,227.85) .. controls (220.52,227.85) and (223.65,230.98) .. (223.65,234.85) .. controls (223.65,238.72) and (220.52,241.85) .. (216.65,241.85) .. controls (212.78,241.85) and (209.65,238.72) .. (209.65,234.85) -- cycle ;
		%Shape: Circle [id:dp251741994386131] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (235.35,214.15) .. controls (235.35,210.28) and (238.48,207.15) .. (242.35,207.15) .. controls (246.22,207.15) and (249.35,210.28) .. (249.35,214.15) .. controls (249.35,218.02) and (246.22,221.15) .. (242.35,221.15) .. controls (238.48,221.15) and (235.35,218.02) .. (235.35,214.15) -- cycle ;
		%Straight Lines [id:da483425638681821] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (242.35,212.15) -- (242.35,420.15) ;
		%Shape: Circle [id:dp2102553258159765] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (319.65,123.85) .. controls (319.65,119.98) and (322.78,116.85) .. (326.65,116.85) .. controls (330.52,116.85) and (333.65,119.98) .. (333.65,123.85) .. controls (333.65,127.72) and (330.52,130.85) .. (326.65,130.85) .. controls (322.78,130.85) and (319.65,127.72) .. (319.65,123.85) -- cycle ;
		%Shape: Circle [id:dp3083248964048344] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (393.5,167) .. controls (393.5,163.13) and (396.63,160) .. (400.5,160) .. controls (404.37,160) and (407.5,163.13) .. (407.5,167) .. controls (407.5,170.87) and (404.37,174) .. (400.5,174) .. controls (396.63,174) and (393.5,170.87) .. (393.5,167) -- cycle ;
		%Shape: Circle [id:dp5765132235101109] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (438.65,234.85) .. controls (438.65,230.98) and (441.78,227.85) .. (445.65,227.85) .. controls (449.52,227.85) and (452.65,230.98) .. (452.65,234.85) .. controls (452.65,238.72) and (449.52,241.85) .. (445.65,241.85) .. controls (441.78,241.85) and (438.65,238.72) .. (438.65,234.85) -- cycle ;
		%Shape: Circle [id:dp39987153375506845] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (348.35,324.15) .. controls (348.35,320.28) and (351.48,317.15) .. (355.35,317.15) .. controls (359.22,317.15) and (362.35,320.28) .. (362.35,324.15) .. controls (362.35,328.02) and (359.22,331.15) .. (355.35,331.15) .. controls (351.48,331.15) and (348.35,328.02) .. (348.35,324.15) -- cycle ;
		%Shape: Circle [id:dp6185715471423638] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (395.5,375) .. controls (395.5,371.13) and (398.63,368) .. (402.5,368) .. controls (406.37,368) and (409.5,371.13) .. (409.5,375) .. controls (409.5,378.87) and (406.37,382) .. (402.5,382) .. controls (398.63,382) and (395.5,378.87) .. (395.5,375) -- cycle ;
		%Shape: Circle [id:dp03914665359508884] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (319.65,336.85) .. controls (319.65,332.98) and (322.78,329.85) .. (326.65,329.85) .. controls (330.52,329.85) and (333.65,332.98) .. (333.65,336.85) .. controls (333.65,340.72) and (330.52,343.85) .. (326.65,343.85) .. controls (322.78,343.85) and (319.65,340.72) .. (319.65,336.85) -- cycle ;
		%Shape: Circle [id:dp23544413709314194] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (234.35,425.15) .. controls (234.35,421.28) and (237.48,418.15) .. (241.35,418.15) .. controls (245.22,418.15) and (248.35,421.28) .. (248.35,425.15) .. controls (248.35,429.02) and (245.22,432.15) .. (241.35,432.15) .. controls (237.48,432.15) and (234.35,429.02) .. (234.35,425.15) -- cycle ;
		%Shape: Circle [id:dp6328352094643674] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (167.5,278) .. controls (167.5,274.13) and (170.63,271) .. (174.5,271) .. controls (178.37,271) and (181.5,274.13) .. (181.5,278) .. controls (181.5,281.87) and (178.37,285) .. (174.5,285) .. controls (170.63,285) and (167.5,281.87) .. (167.5,278) -- cycle ;
		%Shape: Circle [id:dp0277236727331045] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (284.5,375) .. controls (284.5,371.13) and (287.63,368) .. (291.5,368) .. controls (295.37,368) and (298.5,371.13) .. (298.5,375) .. controls (298.5,378.87) and (295.37,382) .. (291.5,382) .. controls (287.63,382) and (284.5,378.87) .. (284.5,375) -- cycle ;
		%Shape: Circle [id:dp81251009491716] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (167.5,375) .. controls (167.5,371.13) and (170.63,368) .. (174.5,368) .. controls (178.37,368) and (181.5,371.13) .. (181.5,375) .. controls (181.5,378.87) and (178.37,382) .. (174.5,382) .. controls (170.63,382) and (167.5,378.87) .. (167.5,375) -- cycle ;
		%Shape: Circle [id:dp06847823490933047] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (235.35,323.15) .. controls (235.35,319.28) and (238.48,316.15) .. (242.35,316.15) .. controls (246.22,316.15) and (249.35,319.28) .. (249.35,323.15) .. controls (249.35,327.02) and (246.22,330.15) .. (242.35,330.15) .. controls (238.48,330.15) and (235.35,327.02) .. (235.35,323.15) -- cycle ;
		%Shape: Circle [id:dp512965872314114] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (318.65,233.85) .. controls (318.65,229.98) and (321.78,226.85) .. (325.65,226.85) .. controls (329.52,226.85) and (332.65,229.98) .. (332.65,233.85) .. controls (332.65,237.72) and (329.52,240.85) .. (325.65,240.85) .. controls (321.78,240.85) and (318.65,237.72) .. (318.65,233.85) -- cycle ;
		%Shape: Circle [id:dp09877894857002767] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (280.5,167) .. controls (280.5,163.13) and (283.63,160) .. (287.5,160) .. controls (291.37,160) and (294.5,163.13) .. (294.5,167) .. controls (294.5,170.87) and (291.37,174) .. (287.5,174) .. controls (283.63,174) and (280.5,170.87) .. (280.5,167) -- cycle ;
		%Shape: Circle [id:dp048557513509696504] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (393.5,279) .. controls (393.5,275.13) and (396.63,272) .. (400.5,272) .. controls (404.37,272) and (407.5,275.13) .. (407.5,279) .. controls (407.5,282.87) and (404.37,286) .. (400.5,286) .. controls (396.63,286) and (393.5,282.87) .. (393.5,279) -- cycle ;
		%Straight Lines [id:da20319199134944332] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (174.5,167) -- (155.53,183.39) ;
		\draw [shift={(152.5,186)}, rotate = 319.18] [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (11.61,-5.58) -- (0,0) -- (11.61,5.58) -- cycle    ;
		%Straight Lines [id:da6890857780825057] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (400.5,167) -- (385.18,184.03) ;
		\draw [shift={(382.5,187)}, rotate = 311.99] [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (11.61,-5.58) -- (0,0) -- (11.61,5.58) -- cycle    ;
		%Straight Lines [id:da17548490371468506] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (326.65,123.85) -- (346.5,123.97) ;
		\draw [shift={(350.5,124)}, rotate = 180.36] [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (11.61,-5.58) -- (0,0) -- (11.61,5.58) -- cycle    ;
		%Straight Lines [id:da1909739047269443] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (242.35,214.15) -- (262.2,214.27) ;
		\draw [shift={(266.2,214.3)}, rotate = 180.36] [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (11.61,-5.58) -- (0,0) -- (11.61,5.58) -- cycle    ;
		%Straight Lines [id:da3204277030624336] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (215.65,234.85) -- (215.53,216) ;
		\draw [shift={(215.5,212)}, rotate = 89.62] [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (11.61,-5.58) -- (0,0) -- (11.61,5.58) -- cycle    ;
		%Straight Lines [id:da82945838560628] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (445.65,233.85) -- (445.53,215) ;
		\draw [shift={(445.5,211)}, rotate = 89.62] [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (11.61,-5.58) -- (0,0) -- (11.61,5.58) -- cycle    ;
		%Straight Lines [id:da3563029288425641] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (355.35,324.15) -- (355.23,305.3) ;
		\draw [shift={(355.2,301.3)}, rotate = 89.62] [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (11.61,-5.58) -- (0,0) -- (11.61,5.58) -- cycle    ;
		%Straight Lines [id:da3342334757239471] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (126.35,325.15) -- (126.23,306.3) ;
		\draw [shift={(126.2,302.3)}, rotate = 89.62] [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (11.61,-5.58) -- (0,0) -- (11.61,5.58) -- cycle    ;
		%Straight Lines [id:da5480612610988698] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (174.5,375) -- (158.25,392.1) ;
		\draw [shift={(155.5,395)}, rotate = 313.53] [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (11.61,-5.58) -- (0,0) -- (11.61,5.58) -- cycle    ;
		%Straight Lines [id:da266216324900856] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (404.5,375) -- (388.25,392.1) ;
		\draw [shift={(385.5,395)}, rotate = 313.53] [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (11.61,-5.58) -- (0,0) -- (11.61,5.58) -- cycle    ;
		%Straight Lines [id:da7911990877276434] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (326.65,337.85) -- (346.5,337.97) ;
		\draw [shift={(350.5,338)}, rotate = 180.36] [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (11.61,-5.58) -- (0,0) -- (11.61,5.58) -- cycle    ;
		%Straight Lines [id:da6844873362760133] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (241.35,425.15) -- (261.2,425.27) ;
		\draw [shift={(265.2,425.3)}, rotate = 180.36] [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (11.61,-5.58) -- (0,0) -- (11.61,5.58) -- cycle    ;
		%Straight Lines [id:da39956704666139986] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (174.5,278) -- (195.5,278) ;
		\draw [shift={(199.5,278)}, rotate = 180] [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (11.61,-5.58) -- (0,0) -- (11.61,5.58) -- cycle    ;
		%Straight Lines [id:da1814861592384418] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (400.5,279) -- (421.5,279) ;
		\draw [shift={(425.5,279)}, rotate = 180] [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (11.61,-5.58) -- (0,0) -- (11.61,5.58) -- cycle    ;
		%Straight Lines [id:da8008227137185866] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (291.5,375) -- (291.5,355) ;
		\draw [shift={(291.5,351)}, rotate = 90] [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (11.61,-5.58) -- (0,0) -- (11.61,5.58) -- cycle    ;
		%Straight Lines [id:da32999635924152937] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (287.5,167) -- (287.5,147) ;
		\draw [shift={(287.5,143)}, rotate = 90] [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (11.61,-5.58) -- (0,0) -- (11.61,5.58) -- cycle    ;
		%Straight Lines [id:da6337172602468262] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (325.65,233.85) -- (312.9,250.8) ;
		\draw [shift={(310.5,254)}, rotate = 306.94] [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (11.61,-5.58) -- (0,0) -- (11.61,5.58) -- cycle    ;
		%Straight Lines [id:da5331758735907899] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (242.35,323.15) -- (229.6,340.1) ;
		\draw [shift={(227.2,343.3)}, rotate = 306.94] [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (11.61,-5.58) -- (0,0) -- (11.61,5.58) -- cycle    ;
		
		% Text Node
		\draw (80,453.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (513,325.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (208,49.4) node [anchor=north west][inner sep=0.75pt]    {$z$};
		% Text Node
		\draw (147,141.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ]  {$E_{x}$};
		% Text Node
		\draw (338,99.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ]  {$E_{y}$};
		% Text Node
		\draw (379,137.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ]  {$E_{x}$};
		% Text Node
		\draw (146,352.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ]  {$E_{x}$};
		% Text Node
		\draw (378,348.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ]  {$E_{x}$};
		% Text Node
		\draw (260,191.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ]  {$E_{y}$};
		% Text Node
		\draw (299,312.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ]  {$E_{y}$};
		% Text Node
		\draw (253,401.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ]  {$E_{y}$};
		% Text Node
		\draw (102,300.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ]  {$E_{z}$};
		% Text Node
		\draw (191,215.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ]  {$E_{z}$};
		% Text Node
		\draw (423,205.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ]  {$E_{z}$};
		% Text Node
		\draw (332,295.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ]  {$E_{z}$};
		% Text Node
		\draw (217,296.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,opacity=1 ]  {$H_{x}$};
		% Text Node
		\draw (176.5,284.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,opacity=1 ]  {$H_{y}$};
		% Text Node
		\draw (286,236.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,opacity=1 ]  {$H_{x}$};
		% Text Node
		\draw (255,144.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,opacity=1 ]  {$H_{z}$};
		% Text Node
		\draw (402.5,282.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,opacity=1 ]  {$H_{y}$};
		% Text Node
		\draw (259,354.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,opacity=1 ]  {$H_{z}$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Yee cell in parallelepiped mesh element for EM wave propagating in vacuum}
	\end{figure}
	So globally in vacuum without the presence of any object we have a meshing of the space that can be represented as follows:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,712); %set diagram left start at 0, and has height of 712
		
		%Shape: Polygon [id:ds6127381739589697] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 226; green, 226; blue, 226 }  ,fill opacity=1 ] (321.33,282.33) -- (321.25,295.5) -- (299.5,308) -- (299.5,293) -- cycle ;
		%Shape: Rectangle [id:dp170441547909157] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 226; green, 226; blue, 226 }  ,fill opacity=1 ] (288,293) -- (299.5,293) -- (299.5,308) -- (288,308) -- cycle ;
		%Straight Lines [id:da09224864176411018] 
		\draw    (277.5,321) -- (492.74,203.96) ;
		\draw [shift={(494.5,203)}, rotate = 151.46] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da48550344894227426] 
		\draw    (169,320) -- (230.5,320) ;
		%Straight Lines [id:da8305321449379022] 
		\draw    (230.5,320) -- (206.33,320) ;
		\draw [shift={(203.33,320)}, rotate = 360] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da6377380790424261] 
		\draw    (169,320) -- (186.33,320) ;
		\draw [shift={(189.33,320)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da29452996214060234] 
		\draw    (354.5,293) -- (405.5,265) ;
		%Straight Lines [id:da8597146406326941] 
		\draw    (405.5,265) -- (393.15,271.59) ;
		\draw [shift={(390.5,273)}, rotate = 331.93] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da0657440249131025] 
		\draw    (354.5,293) -- (364.67,287.72) ;
		\draw [shift={(367.33,286.33)}, rotate = 152.55] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da30531679744268336] 
		\draw    (308,297) -- (324.5,335) -- (341.5,335) ;
		%Straight Lines [id:da0647461515887151] 
		\draw [line width=1.5]    (299.5,308) -- (425.5,241) ;
		%Straight Lines [id:da2999521897675992] 
		\draw [line width=1.5]    (153,163) -- (278,98) ;
		%Straight Lines [id:da9383078943311367] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (152.5,180) -- (299.33,180) ;
		%Straight Lines [id:da7422538443395481] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (153.5,196) -- (299.33,196) ;
		%Straight Lines [id:da7509577253863786] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (153.5,212) -- (299.33,212) ;
		%Straight Lines [id:da8091964973365475] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (153.5,228) -- (299.33,228) ;
		%Straight Lines [id:da659829446168998] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (153.5,243) -- (299.33,243) ;
		%Straight Lines [id:da6541058526853165] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (153.5,258) -- (300,258) ;
		%Straight Lines [id:da4903874047681347] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (153.5,273) -- (299.33,273) ;
		%Straight Lines [id:da6873519375069712] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (152,293) -- (299.5,293) ;
		%Straight Lines [id:da6701891551801578] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (164.5,164) -- (164.5,308.33) ;
		%Straight Lines [id:da26908671727129785] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (176.5,164) -- (176.5,307.67) ;
		%Straight Lines [id:da44658463562429507] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (189.5,164) -- (189.5,307) ;
		%Straight Lines [id:da6939494974694633] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (201.5,164) -- (201.5,307.67) ;
		%Straight Lines [id:da8925315195681411] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (213.5,164) -- (213.5,307.67) ;
		%Straight Lines [id:da16294678237613303] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (226.25,164.5) -- (226.25,307.67) ;
		%Straight Lines [id:da49916344116584743] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (237.25,164) -- (237.25,307.67) ;
		%Straight Lines [id:da6665456009485136] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (249.25,164) -- (249.25,307.67) ;
		%Straight Lines [id:da28315582550385] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (262.25,164) -- (262.25,308.33) ;
		%Straight Lines [id:da7247125567040091] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (274.25,164) -- (274.25,307.67) ;
		%Straight Lines [id:da05049457693878878] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (287.25,164) -- (287.25,307) ;
		%Straight Lines [id:da2631803281385954] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (321.25,152.33) -- (321.25,295.5) ;
		%Straight Lines [id:da19702753483556945] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (341.25,142.5) -- (341.25,284.5) ;
		%Straight Lines [id:da11186611970662974] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (360.5,132.5) -- (360.5,274.5) ;
		%Straight Lines [id:da9437272909184924] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (386,120) -- (386,262) ;
		%Straight Lines [id:da1254118149938468] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (404,110) -- (404,252) ;
		%Straight Lines [id:da7573543386509805] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (299.5,294) -- (425.5,226) ;
		%Straight Lines [id:da9844697687141717] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (299.33,273) -- (425.33,205) ;
		%Straight Lines [id:da0992013816825228] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (300,258) -- (426,190) ;
		%Straight Lines [id:da6488442015808966] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (299.33,243) -- (425.33,175) ;
		%Straight Lines [id:da35709809017061245] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (299.33,228) -- (425.33,160) ;
		%Straight Lines [id:da44804011662631793] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (299.33,212) -- (425.33,144) ;
		%Straight Lines [id:da8200532918553094] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (299.33,196) -- (425.33,128) ;
		%Straight Lines [id:da33972730936966533] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (299.33,180) -- (425.33,112) ;
		%Straight Lines [id:da5364441698291977] 
		\draw [line width=1.5]    (425.5,98) -- (425.5,241) ;
		%Straight Lines [id:da029803528134725088] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (174.42,152.33) -- (321.25,152.33) ;
		%Straight Lines [id:da6003694212066164] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (194.42,142.5) -- (341.25,142.5) ;
		%Straight Lines [id:da059787997567487094] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (213.5,132.5) -- (360.5,132.5) ;
		%Straight Lines [id:da9969248583487464] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (238,119) -- (385,119) ;
		%Straight Lines [id:da6476843054041941] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (257,108) -- (404,108) ;
		%Straight Lines [id:da7695484327776632] 
		\draw [line width=1.5]    (278,98) -- (425.5,98) ;
		%Straight Lines [id:da64815201162624] 
		\draw [line width=1.5]    (299.5,163) -- (424.5,98) ;
		%Straight Lines [id:da7095793376163602] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (287.25,164) -- (416,97.67) ;
		%Straight Lines [id:da8242772804074798] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (274.25,164) -- (403,97.67) ;
		%Straight Lines [id:da8892985212511326] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (262.25,164) -- (391,97.67) ;
		%Straight Lines [id:da019075281643895936] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (249.25,164) -- (378,97.67) ;
		%Straight Lines [id:da6251792446526392] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (237.25,164) -- (366,97.67) ;
		%Straight Lines [id:da5726368098370251] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (226.25,164.5) -- (355,98.17) ;
		%Straight Lines [id:da3895574754370539] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (213.5,164) -- (342.25,97.67) ;
		%Straight Lines [id:da9752485214565627] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (201.5,164) -- (330.25,97.67) ;
		%Straight Lines [id:da4236732656075357] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (189.5,164) -- (318.25,97.67) ;
		%Straight Lines [id:da2455144637956288] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (176.5,164) -- (305.25,97.67) ;
		%Straight Lines [id:da22554511226449758] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (164.5,164) -- (293.25,97.67) ;
		%Shape: Rectangle [id:dp9975364397981277] 
		\draw  [line width=1.5]  (153,163) -- (299.5,163) -- (299.5,308) -- (153,308) -- cycle ;
		%Straight Lines [id:da7368110052919243] 
		\draw [color={rgb, 255:red, 219; green, 219; blue, 219 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (425.5,241) -- (269.33,241) ;
		%Straight Lines [id:da52175403911714] 
		\draw [color={rgb, 255:red, 219; green, 219; blue, 219 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (153,308) -- (279,241) ;
		%Straight Lines [id:da8028348543463308] 
		\draw [color={rgb, 255:red, 219; green, 219; blue, 219 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (279,98) -- (279,241) ;
		%Straight Lines [id:da6803454705888194] 
		\draw    (437.83,135.67) -- (437.83,193.67) ;
		%Straight Lines [id:da8154996441888145] 
		\draw    (437.83,144.33) -- (437.83,154) ;
		\draw [shift={(437.83,157)}, rotate = 270] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da37035031164121923] 
		\draw    (437.83,193.67) -- (437.83,180.33) -- (437.83,176.67) ;
		\draw [shift={(437.83,173.67)}, rotate = 90] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Shape: Axis 2D [id:dp2488933916132634] 
		\draw  (331.5,308) -- (114.5,308)(299.5,64) -- (299.5,334) (121.5,303) -- (114.5,308) -- (121.5,313) (304.5,71) -- (299.5,64) -- (294.5,71)  ;
		
		% Text Node
		\draw (116,281.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (294,40.4) node [anchor=north west][inner sep=0.75pt]    {$z$};
		% Text Node
		\draw (501,189.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (188.5,323.4) node [anchor=north west][inner sep=0.75pt]    {$d_{y}$};
		% Text Node
		\draw (382,282.4) node [anchor=north west][inner sep=0.75pt]    {$d_{x}$};
		% Text Node
		\draw (347,324) node [anchor=north west][inner sep=0.75pt]   [align=left] {cell$\displaystyle ( 1,1,1)$};
		% Text Node
		\draw (440.67,154.4) node [anchor=north west][inner sep=0.75pt]    {$d_{z}$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Typical simple meshing with multiple Yee cells}
	\end{figure}
	Obviously, we can perform the calculation routines with a value of the permittivity and permeability that are not necessarily equal in all the cells. Allowing in addition to model the propagation of electromagnetic waves in heterogeneous and non isotropic media.
	
	Finally, it is important to notice that the spatial and temporal mesh step must be configured by the user running such simulations. This for computing resources reasons as well as accuracy goals. Indeed, we do not do the same simulations for  a multi-physics system in the low frequency in  vacuum than for non-isotropic material at high frequency and, either on a desktop computer or on a supercomputer.
	
	\pagebreak
	\subsection{Data Science \& Data Mining}\label{data mining}
	\textbf{Definition (\#\thesection.\mydef):} "\NewTerm{Data science}\index{data science}" is an interdisciplinary business field that uses statistics, computer science, scientific methods, processes, algorithms, domain knowledge and systems to extract or extrapolate knowledge and insights from various sources of various types of datas.
	
	\textbf{Definition (\#\thesection.\mydef):} "\NewTerm{Data Mining}\index{Data Mining}", also popularly referred to as "knowledge discovery from data" (KDD), is a subset of data science that focuses specifically on extracting patterns and knowledge from large datasets. It applies statistical and machine learning\footnote{As said by Arthur Samuel, a computer program is said to learn (i.e. machine learning) from experience $E$ with respect to some class of tasks $T$ and performance measure $P$ if its performance at tasks in $T$, as measured by $P$, improves with experience $E$.} algorithms to identify hidden relationships, trends, and anomalies in data. Data mining is often used for tasks such as customer segmentation, fraud detection, and predictive analytics.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Some statisticians argue that data science is not a new field, but rather another name for statistics. Others argue that data science is distinct from statistics because it focuses on problems and techniques unique to digital data. Some others argue that statistics emphasizes quantitative data and description. In contrast, data science deals with quantitative and qualitative data (e.g., from images, text, sensors, transactions, customer information, etc.) and emphasizes prediction and action. Some describe statistics as a non-essential part of data science and others that data science is not distinguished from statistics by the size of datasets or use of computing and that many graduate programs misleadingly advertise their analytics and statistics training as the essence of a data-science program. In this book we will consider Data Science as the set of Data investigations that don't use inferential statistics (null hypothesis testing).
	\end{tcolorbox}
	
	The following naive figure may help the reader to contextualize these both fields:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,655); %set diagram left start at 0, and has height of 655
		
		%Shape: Ellipse [id:dp7725514261323301] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 183; green, 254; blue, 232 }  ,fill opacity=0.5 ] (95,179.01) .. controls (95,98.92) and (188.88,34.01) .. (304.69,34.01) .. controls (420.49,34.01) and (514.38,98.92) .. (514.38,179.01) .. controls (514.38,259.09) and (420.49,324.01) .. (304.69,324.01) .. controls (188.88,324.01) and (95,259.09) .. (95,179.01) -- cycle ;
		%Shape: Ellipse [id:dp21501015539079615] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (232.91,408.73) .. controls (232.91,326.29) and (262.91,259.46) .. (299.91,259.46) .. controls (336.92,259.46) and (366.91,326.29) .. (366.91,408.73) .. controls (366.91,491.17) and (336.92,558.01) .. (299.91,558.01) .. controls (262.91,558.01) and (232.91,491.17) .. (232.91,408.73) -- cycle ;
		%Shape: Circle [id:dp965212931422502] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 255; green, 221; blue, 166 }  ,fill opacity=0.5 ] (181.72,203.14) .. controls (181.72,136.79) and (235.51,83) .. (301.86,83) .. controls (368.21,83) and (422,136.79) .. (422,203.14) .. controls (422,269.49) and (368.21,323.28) .. (301.86,323.28) .. controls (235.51,323.28) and (181.72,269.49) .. (181.72,203.14) -- cycle ;
		%Shape: Circle [id:dp9006206404597317] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 166; green, 207; blue, 255 }  ,fill opacity=0.5 ] (98.72,348.14) .. controls (98.72,281.79) and (152.51,228) .. (218.86,228) .. controls (285.21,228) and (339,281.79) .. (339,348.14) .. controls (339,414.49) and (285.21,468.28) .. (218.86,468.28) .. controls (152.51,468.28) and (98.72,414.49) .. (98.72,348.14) -- cycle ;
		%Shape: Circle [id:dp6321226946411471] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 173; green, 228; blue, 113 }  ,fill opacity=0.5 ] (259.72,349.14) .. controls (259.72,282.79) and (313.51,229) .. (379.86,229) .. controls (446.21,229) and (500,282.79) .. (500,349.14) .. controls (500,415.49) and (446.21,469.28) .. (379.86,469.28) .. controls (313.51,469.28) and (259.72,415.49) .. (259.72,349.14) -- cycle ;
		%Shape: Path Data [id:dp27893929933001727] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 237; green, 185; blue, 114 }  ,fill opacity=0.5 ] (301.86,323.28) .. controls (245.72,323.28) and (198.57,284.77) .. (185.39,232.72) .. controls (196.02,229.65) and (207.25,228) .. (218.86,228) .. controls (275,228) and (322.15,266.51) .. (335.33,318.55) .. controls (324.71,321.63) and (313.48,323.28) .. (301.86,323.28) -- cycle ;
		%Shape: Path Data [id:dp6650682886431436] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 216; green, 248; blue, 181 }  ,fill opacity=0.5 ] (301.86,323.28) .. controls (288.64,323.28) and (275.91,321.14) .. (264.01,317.2) .. controls (278,266.35) and (324.57,229) .. (379.86,229) .. controls (393.08,229) and (405.81,231.14) .. (417.71,235.08) .. controls (403.72,285.93) and (357.15,323.28) .. (301.86,323.28) -- cycle ;
		%Shape: Path Data [id:dp829246828182455] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 129; green, 178; blue, 237 }  ,fill opacity=0.5 ] (339,348.14) .. controls (339,383.79) and (323.47,415.82) .. (298.81,437.82) .. controls (274.79,415.85) and (259.72,384.25) .. (259.72,349.14) .. controls (259.72,313.49) and (275.25,281.46) .. (299.91,259.46) .. controls (323.93,281.43) and (339,313.02) .. (339,348.14) -- cycle ;
		%Shape: Path Data [id:dp26929693715497516] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 232; green, 242; blue, 252 }  ,fill opacity=0.5 ] (301.86,323.28) .. controls (288.64,323.28) and (275.91,321.14) .. (264.01,317.2) .. controls (270.24,294.58) and (282.9,274.64) .. (299.91,259.46) .. controls (316.95,275.04) and (329.48,295.47) .. (335.33,318.55) .. controls (324.71,321.63) and (313.48,323.28) .. (301.86,323.28) -- cycle ;
		%Shape: Ellipse [id:dp14013074637381062] 
		\draw  [fill={rgb, 255:red, 243; green, 243; blue, 243 }  ,fill opacity=0.7 ][dash pattern={on 0.84pt off 2.51pt}] (371.38,486.98) .. controls (371.38,448.86) and (402.04,417.96) .. (439.88,417.96) .. controls (477.71,417.96) and (508.38,448.86) .. (508.38,486.98) .. controls (508.38,525.1) and (477.71,556.01) .. (439.88,556.01) .. controls (402.04,556.01) and (371.38,525.1) .. (371.38,486.98) -- cycle ;
		
		% Text Node
		\draw (251,104) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{71.41pt}\setlength\topsep{0pt}
		\begin{center}
		\textbf{{\large DOMAIN}}\\\textbf{{\large EXPERTISE}}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (375,321) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{72.75pt}\setlength\topsep{0pt}
		\begin{center}
		\textbf{{\large COMPUTER}}\\\textbf{{\large SCIENCE}}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (102,321) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{91.33pt}\setlength\topsep{0pt}
		\begin{center}
		\textbf{{\large APPLIED}}\\\textbf{{\large MATHEMATICS}}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (200,241) node [anchor=north west][inner sep=0.75pt] [font=\normalsize\linespread{0.8}\selectfont]  [align=left] {\begin{minipage}[lt]{54.54pt}\setlength\topsep{0pt}
		\begin{center}
		{\footnotesize \textbf{STATISTICAL}}\\{\footnotesize \textbf{RESEARCH}}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (320,240) node [anchor=north west][inner sep=0.75pt] [font=\normalsize\linespread{0.8}\selectfont]  [align=left] {\begin{minipage}[lt]{57.12pt}\setlength\topsep{0pt}
		\begin{center}
		{\footnotesize \textbf{DATA}}\\{\footnotesize \textbf{PROCESSING}}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (263,289) node [anchor=north west][inner sep=0.75pt] [font=\normalsize\linespread{0.8}\selectfont]  [align=left] {\begin{minipage}[lt]{48.05pt}\setlength\topsep{0pt}
		\begin{center}
		\textbf{DATA}\\\textbf{SCIENCE}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (246,463) node [anchor=north west][inner sep=0.75pt] [font=\normalsize\linespread{0.8}\selectfont]  [align=center] {\begin{minipage}[lt]{77.06pt}\setlength\topsep{0pt}
		\begin{center}
		{\footnotesize \textbf{ARTIFICAL}}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (236,473) node [anchor=north west][inner sep=0.75pt] [font=\normalsize\linespread{0.8}\selectfont]  [align=center] {\begin{minipage}[lt]{77.06pt}\setlength\topsep{0pt}
		\begin{center}
		{\footnotesize \textbf{INGINTELLIGENCE}}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (268,336) node [anchor=north west][inner sep=0.75pt] [font=\normalsize\linespread{0.8}\selectfont]  [align=left] {\begin{minipage}[lt]{44.42pt}\setlength\topsep{0pt}
		\begin{center}
		{\footnotesize MACHINE}\\{\footnotesize LEARNING}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (185,63) node [anchor=north west][inner sep=0.75pt]  [font=\Large] [align=left] {\begin{minipage}[lt]{190pt}\setlength\topsep{0pt}
		\begin{center}
		\textbf{SCIENTIFIC METHOD}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (107,365) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize\linespread{0.8}\selectfont] [align=left] {\begin{minipage}[lt]{87.98pt}\setlength\topsep{0pt}
		\begin{center}
		statistics, linear algebra\\logic, calculus,\\relational algebra
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (442,149) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize\linespread{0.8}\selectfont] [align=left] {\begin{minipage}[lt]{35.37pt}\setlength\topsep{0pt}
		\begin{center}
		evidence\\based \\science
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (101,149) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize\linespread{0.8}\selectfont] [align=left] {\begin{minipage}[lt]{50.34pt}\setlength\topsep{0pt}
		\begin{center}
		evidence\\based \\management
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (367,365) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize\linespread{0.8}\selectfont] [align=left] {\begin{minipage}[lt]{87pt}\setlength\topsep{0pt}
		\begin{center}
		algorithmic\\numerical methods\\complexity\\operational research
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (337,266) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{42pt}\setlength\topsep{0pt}
		\begin{center}
		{\tiny DB, Big Data}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (348,274) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{23.4pt}\setlength\topsep{0pt}
		\begin{center}
		{\tiny Pipelines}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (343,282) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{28.21pt}\setlength\topsep{0pt}
		\begin{center}
		{\tiny Data Flows}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (228,274) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{16.59pt}\setlength\topsep{0pt}
		\begin{center}
		{\tiny NHST}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (226,266) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{20.85pt}\setlength\topsep{0pt}
		\begin{center}
		{\tiny Bayesian}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (223,286) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{26.51pt}\setlength\topsep{0pt}
		\begin{center}
		{\tiny framework}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (228,145) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {\begin{minipage}[lt]{105.88pt}\setlength\topsep{0pt}
		\begin{center}
		minimum 10 years of experience\\or PhD
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (327,272.22) node [anchor=north west][inner sep=0.75pt]  [font=\tiny,rotate=-56.66] [align=left] {data mining};
		% Text Node
		\draw (191,169) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize\linespread{0.8}\selectfont] [align=left] {\begin{minipage}[lt]{57.59pt}\setlength\topsep{0pt}
		\begin{center}
		communication\\techniques
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (344,198) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {\begin{minipage}[lt]{40pt}\setlength\topsep{0pt}
		\begin{center}
		data viz
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (214,203) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {\begin{minipage}[lt]{90pt}\setlength\topsep{0pt}
		\begin{center}
		problem solving skills
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (275,373) node [anchor=north west][inner sep=0.75pt]  [font=\tiny\linespread{0.8}\selectfont] [align=left] {\begin{minipage}[lt]{33.89pt}\setlength\topsep{0pt}
		\begin{center}
		deep learning\\reinforcement\\learning\\supervised,\\unsupervised\\learning
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (252,496) node [anchor=north west][inner sep=0.75pt]  [font=\tiny\linespread{0.8}\selectfont] [align=left] {\begin{minipage}[lt]{66.79pt}\setlength\topsep{0pt}
		\begin{center}
		computer vision\\natural language processing\\large language models
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (294,178) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {\begin{minipage}[lt]{83pt}\setlength\topsep{0pt}
		\begin{center}
		quantitative finance
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (406,517) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize\linespread{0.8}\selectfont] [align=left] {\begin{minipage}[lt]{49.41pt}\setlength\topsep{0pt}
		\begin{center}
		\textbf{Office}\\\textbf{productivity}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (388,475) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize\linespread{0.8}\selectfont] [align=left] {\begin{minipage}[lt]{86pt}\setlength\topsep{0pt}
		\begin{center}
		Excel, Word,\\PowerPoint, Calc\\LaTeX, Beamer, etc.
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (408,427) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {\begin{minipage}[lt]{45pt}\setlength\topsep{0pt}
		\begin{center}
		{\scriptsize C++, Python,}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (390,438) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {\begin{minipage}[lt]{57pt}\setlength\topsep{0pt}
		\begin{center}
		{\scriptsize Java, R, Power BI}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (383,449) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {\begin{minipage}[lt]{55pt}\setlength\topsep{0pt}
		\begin{center}
		{\scriptsize Tableau, Matlab }
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (381,459) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {\begin{minipage}[lt]{40pt}\setlength\topsep{0pt}
		\begin{center}
		{\scriptsize Maple, etc. }
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (247,220) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=center] {\begin{minipage}[lt]{88pt}\setlength\topsep{0pt}
		\begin{center}
		business intelligence
		\end{center}
		\end{minipage}};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Oversimplified overview of Data Science}
	\end{figure}
	The main purpose of Data Mining is to ascend the following analytics levels:
	\begin{figure}[H]
		\centering
		% Gradient Info
  
		\tikzset {_prk3vqvpz/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{-105 }  \pgftransformscale{2 }  }}}
		\pgfdeclarehorizontalshading{_czmv8f1hl}{150bp}{rgb(0bp)=(0.94,0.94,0.94);
		rgb(37.5bp)=(0.94,0.94,0.94);
		rgb(62.5bp)=(0.5,0.5,0.5);
		rgb(100bp)=(0.5,0.5,0.5)}
		
		% Gradient Info
		  
		\tikzset {_mrqhw1070/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{0 }  \pgftransformscale{2 }  }}}
		\pgfdeclarehorizontalshading{_edcwrc08f}{150bp}{rgb(0bp)=(0.6,0.85,1);
		rgb(37.5bp)=(0.6,0.85,1);
		rgb(62.5bp)=(0,0.5,0.5);
		rgb(100bp)=(0,0.5,0.5)}
		
		% Gradient Info
		  
		\tikzset {_kow5hx7al/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{0 }  \pgftransformscale{2 }  }}}
		\pgfdeclarehorizontalshading{_byjl9ht10}{150bp}{rgb(0bp)=(0.6,0.85,1);
		rgb(37.5bp)=(0.6,0.85,1);
		rgb(62.5bp)=(0,0.5,0.5);
		rgb(100bp)=(0,0.5,0.5)}
		
		% Gradient Info
		  
		\tikzset {_lc1j4sttd/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{0 }  \pgftransformscale{2 }  }}}
		\pgfdeclarehorizontalshading{_d3jo73ze7}{150bp}{rgb(0bp)=(0.6,0.85,1);
		rgb(37.5bp)=(0.6,0.85,1);
		rgb(62.5bp)=(0,0.5,0.5);
		rgb(100bp)=(0,0.5,0.5)}
		
		% Gradient Info
		  
		\tikzset {_u14rcgd6a/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{0 }  \pgftransformscale{2 }  }}}
		\pgfdeclarehorizontalshading{_s33jm2lj5}{150bp}{rgb(0bp)=(0.6,0.85,1);
		rgb(37.5bp)=(0.6,0.85,1);
		rgb(62.5bp)=(0,0.5,0.5);
		rgb(100bp)=(0,0.5,0.5)}
		\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        
		
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,536); %set diagram left start at 0, and has height of 536
		
		%Shape: Polygon Curved [id:ds7270603809373579] 
		\draw  [draw opacity=0][shading=_czmv8f1hl,_prk3vqvpz] (348.38,337.28) .. controls (425.3,323.86) and (572.07,298.41) .. (571.82,298.41) .. controls (571.57,298.41) and (569.41,286.46) .. (569.82,286.66) .. controls (570.24,286.87) and (598.07,303.91) .. (598.07,304.41) .. controls (598.07,304.91) and (580.82,335.41) .. (580.32,335.66) .. controls (579.82,335.91) and (577.82,323.91) .. (577.82,323.91) .. controls (577.82,323.91) and (445.54,339) .. (342.38,357.28) .. controls (239.21,375.55) and (192.19,385.25) .. (139.19,403.25) .. controls (86.19,421.25) and (42.38,452.28) .. (40.38,451.28) .. controls (38.38,450.28) and (79.19,415.91) .. (131.19,395.91) .. controls (183.19,375.91) and (271.45,350.7) .. (348.38,337.28) -- cycle ;
		%Shape: Circle [id:dp7036521322048277] 
		\draw  [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,draw opacity=1 ][fill={rgb, 255:red, 8; green, 68; blue, 141 }  ,fill opacity=1 ] (74.73,432.14) .. controls (74.73,428.75) and (77.47,426.01) .. (80.86,426.01) .. controls (84.25,426.01) and (87,428.75) .. (87,432.14) .. controls (87,435.53) and (84.25,438.28) .. (80.86,438.28) .. controls (77.47,438.28) and (74.73,435.53) .. (74.73,432.14) -- cycle ;
		%Shape: Circle [id:dp38134277926880555] 
		\draw  [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,draw opacity=1 ][fill={rgb, 255:red, 8; green, 68; blue, 141 }  ,fill opacity=1 ] (222.73,378.14) .. controls (222.73,373.65) and (226.37,370.01) .. (230.86,370.01) .. controls (235.36,370.01) and (239,373.65) .. (239,378.14) .. controls (239,382.64) and (235.36,386.28) .. (230.86,386.28) .. controls (226.37,386.28) and (222.73,382.64) .. (222.73,378.14) -- cycle ;
		%Shape: Circle [id:dp05404614834884991] 
		\draw  [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,draw opacity=1 ][fill={rgb, 255:red, 8; green, 68; blue, 141 }  ,fill opacity=1 ] (374.73,346.14) .. controls (374.73,339.99) and (379.71,335.01) .. (385.86,335.01) .. controls (392.01,335.01) and (397,339.99) .. (397,346.14) .. controls (397,352.29) and (392.01,357.28) .. (385.86,357.28) .. controls (379.71,357.28) and (374.73,352.29) .. (374.73,346.14) -- cycle ;
		%Shape: Half Frame [id:dp6316071567428823] 
		\draw  [draw opacity=0][shading=_edcwrc08f,_mrqhw1070] (41,321) -- (182.38,321) -- (161.41,335.28) -- (54.38,335.28) -- (54.38,408.17) -- (41,417.28) -- cycle ;
		%Shape: Half Frame [id:dp8447121389417505] 
		\draw  [draw opacity=0][shading=_byjl9ht10,_kow5hx7al] (187,271) -- (328.38,271) -- (307.41,285.28) -- (200.38,285.28) -- (200.38,358.17) -- (187,367.28) -- cycle ;
		%Shape: Half Frame [id:dp7292821257357112] 
		\draw  [draw opacity=0][shading=_d3jo73ze7,_lc1j4sttd] (334,237) -- (475.38,237) -- (454.41,251.28) -- (347.38,251.28) -- (347.38,324.17) -- (334,333.28) -- cycle ;
		%Shape: Half Frame [id:dp8714201842454472] 
		\draw  [draw opacity=0][shading=_s33jm2lj5,_u14rcgd6a] (479,214) -- (620.38,214) -- (599.41,228.28) -- (492.38,228.28) -- (492.38,301.17) -- (479,310.28) -- cycle ;
		
		% Text Node
		\draw (93,430.01) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {\textbf{\textcolor[rgb]{0.04,0.39,0.82}{{\small Hindsight}}}};
		% Text Node
		\draw (241,373.14) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {\textbf{\textcolor[rgb]{0.04,0.39,0.82}{Indsight}}};
		% Text Node
		\draw (399,341.14) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {\textbf{\textcolor[rgb]{0.04,0.39,0.82}{{\large Foresight}}}};
		% Text Node
		\draw (54,321) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textcolor[rgb]{1,1,1}{INFORMATION}};
		% Text Node
		\draw (492,214) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textcolor[rgb]{1,1,1}{OPTIMIZATION}};
		% Text Node
		\draw (43,303) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {What happened?};
		% Text Node
		\draw (189,256) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {Why did it happen?};
		% Text Node
		\draw (334,219) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {What will happen?};
		% Text Node
		\draw (481,180) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {How can we\\make it happen?};
		% Text Node
		\draw (57.38,340.28) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{{\Large Descriptive}}\\\textbf{{\Large Analytics}}};
		% Text Node
		\draw (204.38,291.28) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{{\Large Diagnostic}}\\\textbf{{\Large Analytics}}};
		% Text Node
		\draw (352.38,257.28) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{{\Large Predictive}}\\\textbf{{\Large Analytics}}};
		% Text Node
		\draw (496.38,235.28) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{{\Large Prescriptive}}\\\textbf{{\Large Analytics}}};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Descriptive, Diagnostic, Predictive and Prospective analytics}
	\end{figure}
	Or an extended and detailed version of the above figure but instead as a table and with a slightly different vocabulary:
	\begin{table}[H]
		\begin{tabular}{l|l}
		\rowcolor[gray]{0.75}
		\textbf{Analytics Steps} & \textbf{Typical Questions Addressed} \\ \hline
		\begin{tabular}[c]{@{}l@{}}Descriptive\\ analytics\end{tabular} & \begin{tabular}[c]{@{}l@{}}
		\textbullet What is the current situation? What's happening?\\
		\textbullet What has change? What's new?\\
		\textbullet What should we focus on? What should we worry about?\\
		\textbullet Is this the right data to use for making the prediction?\\
		\textbullet How reliable is the data we are using here?
		\end{tabular} \\ \hline
		\begin{tabular}[c]{@{}l@{}}Causal\\ analytics\end{tabular} & \begin{tabular}[c]{@{}l@{}}
		\textbullet Diagnosis, explanation, and attribution: What explains the current\\
		situation?\\
		\textbullet What can we do about it?\\
		\textbullet How would different actions change the probabilities of\\
		 different future outcomes?\\
		\textbullet What decisions will this be used to make?\\
		\textbullet What's the cost of getting those decisions wrong?\\
		\textbullet How much will decisions affect the targeted people\\ 
		(especially the vulnerable one!)?
		\end{tabular} \\ \hline
		\begin{tabular}[c]{@{}l@{}}Predictive\\ analytics\end{tabular} & \begin{tabular}[c]{@{}l@{}}
		\textbullet If we do not change what we are doing, what will (probably)\\
		happen next? When? How likely are the different possibilities?\\
		\textbullet Given observed (or assumed) values for some variables, what are\\
		the probabilities for values of other variables?\\
		\textbullet  How well can some variables be predicted from others?\\
		\textbullet How well can future outcomes be predicted now?\\
		\textbullet  Are we certain that's what we want to predict?\\
		\textbullet  Are we really aware of the limits of the predictions?
		\end{tabular} \\ \hline
		\begin{tabular}[c]{@{}l@{}}Prescriptive\\ analytics\end{tabular} & \begin{tabular}[c]{@{}l@{}}
		\textbullet What should we do next?\\
		\textbullet What decisions and policies implemented	now will most improve \\ probabilities of future outcomes?
		\end{tabular} \\ \hline
		\begin{tabular}[c]{@{}l@{}}Evaluation \\ analytics\end{tabular} & \begin{tabular}[c]{@{}l@{}}
		\textbullet How much will it cost to deploy and maintain the models?\\
		\textbullet How well are our current decisions and policies working?\\
		\textbullet What effects have our decisions and policies actually caused?\\
		\textbullet How do different policies affect behaviour and outcomes for different\\
		people?
		\end{tabular} \\ \hline
		\begin{tabular}[c]{@{}l@{}}Learning \\ analytics\end{tabular}& \begin{tabular}[c]{@{}l@{}}
		\textbullet What decisions or policies might work better than our current ones?\\
		\textbullet How can we use data and experimentation to find out?\\
		\textbullet By how much do different items of information improve decisions?\\
		\textbullet What is the value of information for different measurements?
		\end{tabular} \\ \hline
		\begin{tabular}[c]{@{}l@{}}Collaborative \\ analytics\end{tabular} & \begin{tabular}[c]{@{}l@{}}
		\textbullet How can we best work together to improve probabilities of future\\
		outcomes?\\
		\textbullet Who should share what information with whom, how and when?\\
		\textbullet What actions should each division of an organization or each member\\
		of a team take?		
		\end{tabular}
		\end{tabular}
		\caption{Component of risk analytics}
	\end{table}
	A second level of granularity, with their relations but poorly readable and not error proof (...), would be (we will come back on a more precise definition further below)\label{mindmap of data science}:
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.39\textwidth]{img/computing/data_science_map.pdf}
		\caption{Data Mining/Machine Learning non-exhaustive Mind Map techniques and tools}
	\end{figure}
	and a third level of granularity would give\footnote{this list is strongly inspired by Wikipedia, SPSS, SAS, \texttt{R}, Rapid Miner and Tanagra softwares options. For a neural networks list see the figure further below of Fjodor van Veen where $27$ different neural networks type are given.} (sorry it's quite long but "data scientists", managers and IT staff in my teachings ask me many times to have an exhaustive one-place list):
	\begin{itemize}
		\item \textbf{Variables transformations:}
			\begin{small}
			\begin{multicols}{2}
			\begin{enumerate}
				\item Percentage ("normalization")
				\item Logarithmic transformation
				\item Center transformation (i.e. mean normalization)
				\item $Z$-transformation (centered-reduced / standardized)
				\item Rank transformation
				\item Percentile transformation (scaling)
				\item Range transformation (i.e. Min-Max transformation)
				\item Robust scaling
				\item Box-Cox transformation
				\item Modified Box-Cox transformation
				\item Johnson transformation (Yeo-Johnson)
				\item Variance stabilizing transformation
				\item Fisher transformation
				\item Lambert W x F transformation
				\item ORQ normalization
				\item Manly's exponential transformation
				\item John/Draper’s modulus
				\item Bickel/Doksum's modified Box-Cox
				\item Whitening
				\item ...
			\end{enumerate}
			\end{multicols}
			\end{small}
		\item \textbf{Distance metrics:}
			\begin{small}
			\begin{multicols}{2}
			\begin{enumerate}
				\item Euclidean distance (special case of Hölder's distance)
				\item Mahalanobis distance
				\item Manhattan distance
				\item Canberra distance
				\item Chebyshev distance
				\item Earth mover's distance
				\item Hölder distance
				\item Cosine similarity
				\item Minkowski distance
				\item Haversine distance
				\item Hamming distance
				\item Jaccard distance
				\item Chi-squared distance
				\item Kendall rank correlation distance
				\item Heterogeneous euclidean overlap Metric (HEOM)
				\item...
			\end{enumerate}
			\end{multicols}
			\end{small}
		\item \textbf{Sampling and validation techniques:}
			\begin{small}
			\begin{multicols}{2}
			\begin{enumerate}
				\item Simple random sampling with/without replacement (SRWR)
				\item Cluster sampling
				\item Systematic sampling
				\item Stratified random sampling
				\item Bootstrap sampling
				\item Conveniance sampling
				\item Judgemental or purposive sampling
				\item Snowball sampling
				\item Quota sampling
				\item Jackknife sampling
				\item Latin Hypercube sampling
				\item V-Fold cross-validation
				\item Leave-one-out cross-validation
				\item Monte Carlo cross-validation
				\item Group V-fold cross-validation
				\item Rolling origin forecast resampling
				\item Nested or double resampling
				\item ...
			\end{enumerate}
			\end{multicols}
			\end{small}
		\item \textbf{Dimensionality reduction (feature extraction+feature selection):}
			\begin{small}
			\begin{multicols}{2}
			\begin{enumerate}
				\item Correlation coefficients (Pearson's, Kendall, MIC, etc.)
				\item Stepwise regression
				\item Forward/Backward-logit
				\item CFS (Correlation Feature Selection) filtering (Hall \& Smith CFS)
				\item FCBF filtering (Yiu \& Liu Fast Correlation Based Filter)
				\item mRMR (Minimum-redundancy-maximum-relevance) feature selection
				\item Chi-squared feature ranking
				\item Fisher Criterion Scoring
				\item Battiti's MIFS (Mutual Information-based Feature Selection Method) feature filtering
				\item NIMFS (Normalized mutual information feature) selection
				\item MODTree (Multivalued Oblivious Decision) filtering (Lallich \& Rakotomalala MODTree)
				\item Non-negative matrix factorization dimension reduction
				\item ReliefF (Kira \& Rendell)
				\item Runs filtering
				\item Stepdisc (Wilk's partial lambda)
				\item Oracle Minimum Description Length (MDL)
				\item PCA (Principal Component Analysis)
				\item SVD (Singular Value Decomposition)
				\item CCA (Curvilinear Component Analysis)
				\item ICA (Independent Component Analysis)/Kernel ICA			
				\item Parallel Factor Analysis (PARAFAC)
				\item Tucker Decomposition (PARAFAC generalization)
				\item Sammon mapping
				\item Kernel PCA / SVD
				\item Projection Pursuit (PP) 
				\item Laplacian eigenmaps
				\item Mixture Discriminant Analysis (MDA)
				\item Regularized Discriminant Analysis (RDA)
				\item Flexible Discriminant Analysis (FDA)
				\item Latent Dirichlet allocation
				\item Isomap
				\item Locally linear embedding
				\item Maximum variance unfolding
				\item Synthetic Minority Over-sampling Technique (SMOTE)
				\item Adaptive Synthetic Sampling Approach (ADASYN)
				\item Autoencoder
				\item $T$-SNE ($T$-distributed stochastic neighbour embedding) 
				\item Generalized Low Rank Models (GLRM)
				\item Multivariate (Soft) Self Modelling Curve Resolution (MCR)
				\item Recursive Feature Selection
				\item Uniform Manifold Approximation and Projection (UMAP)
				\item ...
			\end{enumerate}
			\end{multicols}
			\end{small}
		
		\item \textbf{Statistical indicators (parametric and nonparametric\footnote{indicators with the exponent $^\text{np}$ are nonparametric}):}
			\begin{small}
			\begin{multicols}{2}
			\begin{enumerate}
				\item Mean (arithmetic, geometric, harmonic, weighted, LS...)
				\item Bias
				\item M-Estimators$^\text{np}$ / L-Estimators$^\text{np}$ / W-estimators / S-estimators
				\item Maximum, Minimum, Range$^\text{np}$, Midhinge, Trimean
				\item Median$^\text{np}$, Pseudomedian$^\text{np}$ (Hodges-Lehmann estimator)
				\item Median absolute deviation (MAD)
				\item Quantiles, Upper and Lower Hinge, Interquartile range$^\text{np}$			
				\item Mode$^\text{np}$
				\item Variance, semivariance, standard deviation/MSE (biased or unbiased), circular variance
				\item Fluctuation interval
				\item Skewness (Pearson's, Bowleys's, Kelly's, Groeneveld \& Meeden's, Medcouple)
				\item Kurtosis
				\item Bimodal coefficient
				\item Pearson correlation, adjusted correlation, partial correlation, semi-partial correlation, biweight correlation
				\item Blomqvist’s correlation$^\text{np}$
				\item Spearman$^\text{np}$, Kendall$^\text{np}$ correlations
				\item McFadden, Cox \& Snell's, Nagelkerke, Efron, McKelvey \& Zavoina, Count, Adjusted count pseudo-correlations\footnote{Pseudo-correlations are used in logistic regression}
				\item Lin' correlation
				\item Shepherd’s Pi correlation
				\item d-variable Hilbert-Schmidt independence criterion (dHSIC)
				\item $p$-value
				\item $\beta$ of a NHST (power of test)
				\item Pitman's ARE (asymptotic relative efficiency)
				\item Coefficient of variation
				\item Effect size ($\omega^2$, partial $\omega^2$,$\eta^2$, Cohen's $f^2$, Cohen's $q$, Cohen's $w$, Cohen's $h$, Glass' $\Delta$, $\Psi$, Cliff's $\Delta$, Risk ratio, odds ratio)
				\item Cohen's $\kappa$
				\item Yule Q coefficient
				\item Gini impurity index (i.e.  Somers' D)
				\item Area Under the Curve (AUC)
				\item Harrell’s $C$-index
				\item $F_1$ score, $F_\beta$ score
				\item Piotroski score
				\item Intraclass correlation coefficient
				\item Bangdiwala's B
				\item Hartley entropy
				\item Rényi entropy
				\item Moran's I correlation
				\item Fleiss Kappa
				\item Pearson's measure of mean square contingency
				\item Cramér'V ($\phi_c$)
				\item Tschuprow's T 
				\item Scott's $\pi$
				\item $\phi$-coefficient (Matthews correlation coefficient)
				\item Point-Biserial correlation coefficient
				\item Confusion matrix $\mathcal{C}$
				\item Mallow's $C_p$
				\item Akaike information criterion (AIC)
				\item Corrected Akaike information criterion (AICc)
				\item Bayesian information criterion (BIC)
				\item Bayes factor (BF)
				\item Deviance information criterion (DIC)
				\item Hannan–Quinn information criterion (HQIC)
				\item Focused information criterion (FIC)
				\item Mutual information criterion (MIC)
				\item Bayesian predictive information criterion (BPIC)
				\item Covariance inflation criterion (CIC)
				\item Risk inflation criterion (RIC)
				\item Relative error, Absolute error, Final prediction error (FPE)
				\item Widely applicable information criterion (WAIC)
				\item Quasilikelihood information criteria (QIC)
				\item Shannon entropy (+perplexity)
				\item Specificity
				\item Log-loss (LL)
				\item Sensitivity
				\item Jensen–Shannon divergence
				\item Kullback–Leibler divergence
				\item Aggregation indices (econometric)
				\item Helsel's coefficient
				\item Minimum message length (MML)
				\item Freeman's $\theta$
				\item $\varepsilon$-squared
				\item Pearson contingency coefficient
				\item Goodman's Gamma
				\item Guttman monotonicity coefficient$^\text{np}$
				\item Rand index
				\item Concentration coefficient
				\item Uncertainty coefficient
				\item Brier score loss
				\item NNS (Nonlinear Nonparametric Statistic) dependence
				\item ...
			\end{enumerate}
			\end{multicols}
			\end{small}
		
		\item \textbf{Statistical tests\footnote{Some of these tests are proved in the section Statistics page \pageref{statistics}. Some items in the lists below are not really tests but more "procedures" or "methods". And obviously some tests are just special cases of others tests!!!} (parametric, semi-parametric and nonparametric\footnote{Tests with the exponent $^\text{sp}$ are semi-parametric, those with $^\text{np}$ are nonparametric}):}\label{list of statistical tests}
			\begin{small}
			\begin{multicols}{3}
			\begin{enumerate}
				\item (Brown-)Mood's test$^\text{np}$
				\item (Siegel-)Tukeys HSD test$^\text{np}$
				\item Abelson-Tukey score test
				\item Ahsanullah test$^\text{np}$
				\item Adjacency test$^\text{np}$
				\item ADF-GLS test
				\item 2-AFC/3-AFC (alternative force choice) tests
				\item Agostino test of skewness
				\item Agresti–Pendergrast test$^\text{np}$
				\item (Hodges)-Ajne's test$^\text{np}$
				\item Anscombe-Glynn test of kurtosis
				\item Anderson-Darling's adequation test$^\text{np}$
				\item ANCOVA test
				\item ANOVA/MANOVA tests (+Welch's ANOVA, Friedman's ANOVA$^\text{np}$, Kruskal-Wallis ANOVA$^\text{np}$, Scheirer-Ray-Hare ANOVA$^\text{np}$, Van der Waerden ANOVA$^\text{np}$)
				\item Ansari-Bradley's test$^\text{np}$
				\item Anscombe-Glynn test
				\item Aroian test
				\item (Aspin-)Welch test
				\item Auto-Regressive Conditional Heteroskedasticity (ARCH) effect test$^\text{np}$
				\item Baarda's w-test
				\item Bai and Ng’s test
				\item Banerjee-Lumsdaine-Stock test
				\item Baumgartner-Weiss-Schindler test$^\text{np}$
				\item Baringhaus-Henze-Epps-Pulley (BHEP) test
				\item Bartholomew's likelihood ratio test
				\item Barnard's test$^\text{np}$
				\item Bartels rank test$^\text{np}$ of randomness
				\item Bartlett-(Kendall) test for variances
				\item Bartlett's test for sphericity
				\item Bartlett-Diananda test
				\item Bayesian A/B tests
				\item Beale's $F$ test
				\item Behrens-Fisher test
				\item Bentler and Yuan’s scree test
				\item Beran's tests
				\item Begg test
				\item Berk-Jones tests$^\text{np}$
				\item Besag-Newell's R test
				\item BDS (Brock-Dechert-Scheinkman) test for non-linear serial dependence in time series
				\item Bhapkar's test$^\text{np}$
				\item Bieren's test
				\item Bimodality test
				\item (Binomial) sign test$^\text{np}$
				\item Binomial exact test$^\text{np}$ 
				\item Bithell’s linear rank score test$^\text{np}$ 
				\item Birch test$^\text{np}$ 
				\item Bivariate sign test$^\text{np}$ 
				\item Blumen's test$^\text{np}$
				\item Blum-Kiefer-Rosenblatt independence test
				\item BMP (Boehmer, Musumeci and Poulsen) cross-sectional test
				\item Bonett-Seier test of kurtosis
				\item Bonferroni outlier test
				\item Boschloo's test$^\text{np}$
				\item Bowker's test for symmetry$^\text{np}$
				\item Bowman and Shenton combination tests
				\item Box's (M) test
				\item Box–Pierce test
				\item Brant test$^\text{np}$ (proportional odds test)
				\item Breitung test$^\text{np}$
				\item Breslow-Day test$^\text{np}$
				\item Breusch-Pagan-Godfrey's test for homogeneity of variances
				\item Breusch-Godfrey stationarity test
				\item Breusch-Pagan stationarity test
				\item Brown–Forsythe's test
				\item Brunk's$^\text{np}$ test
				\item Brunner-Munzel generalized Wilcoxon test$^\text{np}$
				\item Buishand range and U test$^\text{np}$
				\item Busetti-Taylor test
				\item Buy-and-hold abnormal return test (BHAR)
				\item Butler-Smirnov test
				\item Canova and Hansen test$^\text{np}$
				\item Capon test$^\text{np}$
				\item CAR (cumulative abnormal return) test
				\item Cattell's scree test
				\item Chacko's test$^\text{np}$
				\item Change point mean stationarity test
				\item Change point variance changes stationarity test
				\item Chen-Deo test
				\item Chen-Wolfe test$^\text{np}$
				\item Cheng-Spiring test$^\text{np}$
				\item Chi-squared test for association / independence or homogeneity$^\text{np}$
				\item Chi-squared test for independence with/without Yates correction
				\item Chi-squared test for outlier
				\item Chi-squared test for adequation$^\text{np}$
				\item Chow's test$^\text{np}$
				\item Cho–Gaines's test$^\text{np}$
				\item Chow-Denning test
				\item Chakraborti-Desu test
				\item Chakraborti-Desu test$^\text{np}$
				\item Chebyshev distance test$^\text{np}$
				\item Circular-cone test
				\item Clark-Evans' test
				\item Cliff-Ord tests
				\item Cochran's C-test
				\item Cochran's Q-test$^\text{np}$
				\item Cochran-Armitage's test$^\text{np}$ (aka Chi square test for trend)
				\item Cochran-Mantel-Haenszel's test$^\text{np}$
				\item Cohen’s d test
				\item Conover-Iman squared ranks pairwise comparison test$^\text{np}$
				\item Corner test for association$^\text{np}$
				\item Comparing correlation coefficients of overlapping (CCO) samples
				\item Corrado rank test$^\text{np}$	
				\item Correlation for categorical variables test
				\item Cowan generalized sign test$^\text{np}$
				\item Cox's F-test$^\text{np}$	
				\item Cox and Oakes test$^\text{np}$
				\item Cox–Small test		
				\item Cox-Stuart trend test$^\text{np}$
				\item Craddock test$^\text{np}$
				\item Cramer–von Mises' test$^\text{np}$
				\item Cressie tests$^\text{np}$
				\item Cross sectional $T$-test
				\item Cusum test for structural change
				\item Cuzick–Edwards test
				\item Cuzick's trend test$^\text{np}$
				\item d-variable Hilbert Schmidt independence criterion test$^\text{np}$
				\item Daniel's test for trend$^\text{np}$
				\item Darroch interaction test$^\text{np}$
				\item Davidson-MacKinnon test for comparing non-nested models
				\item David-Barton test
				\item David-Hellwig test
				\item Davis-Quade's test$^\text{np}$
				\item Demsar test$^\text{np}$
				\item Depaulis test$^\text{np}$
				\item Deshpande test$^\text{np}$
				\item Dickey–Fuller's test$^\text{np}$
				\item Difference between two non-overlapping dependent correlation coefficients test
				\item Diggle and Chetwynd’s test
				\item Dixon's test
				\item Doornik-Hansen test
				\item Duckworth's test$^\text{np}$
				\item Dudewicz-van der Meulen test$^\text{np}$
				\item Duncan's multiple range comparison (MRT) test
				\item Dunn's test$^\text{np}$
				\item Dunnett's $T$, C or $T3$ test
				\item Duo-Trio test
				\item Durbin's rank test$^\text{np}$
				\item Durbin-Watson autocorrelation test
				\item Durbin-Wu-Hausman test
				\item Dwass-Steele-Critchlow paired test$^\text{np}$
				\item Dzhaparidze-Nikulin test$^\text{np}$
				\item Egger's test
				\item Euclidean distance test$^\text{np}$
				\item Elliott-Rothenberg stock test
				\item Enders sequential test procedure for unit roots
				\item Engelman-Hartigan test
				\item Engle test
				\item Engle-Granger cointegration test
				\item Epstein's test
				\item Epps and Pulley test
				\item Epps-Singleton (ES) test
				\item Epsilon squared test
				\item Eta squared test
				\item Ewens-Watterson test$^\text{np}$
				\item Excess mass test$^\text{np}$
				\item Extreme rank sum test$^\text{np}$
				\item Fagerland-Hosmer-Bofin test$^\text{np}$
				\item Fay tests$^\text{np}$
				\item Ferguson test
				\item Fieller's test (A/B test)
				\item Filliben's probability plot correlation test
				\item Finkelstein and Schoenfeld test$^\text{np}$
				\item Fisher-ADF test$^\text{np}$
				\item Fisher-PP$^\text{np}$
				\item Fisher-Behrens test
				\item Fisher's Exact (Fisher-Irwin) test$^\text{np}$
				\item Fisher-Freeman-Halton Exact test$^\text{np}$
				\item Fisher's LSD (Least Significant Difference) test
				\item Fisher-Hayter's LSD test
				\item Fisher-Pitman's permutation test$^\text{np}$
				\item Fisher's $G$-test for periodicity
				\item Fisher's variance test
				\item Fisher's cumulant test
				\item Fisher-Yates test
				\item Fisher-Yates-Terry-Hoeffding test$^\text{np}$
				\item Fligner(-Killeen)'s test for homogeneity of variances$^\text{np}$
				\item Fligner–Wolfe's test$^\text{np}$
				\item Fligner–Policello's test$^\text{np}$
				\item Foutz' $F_n$ test
				\item Fung-Paul test
				\item Freeman–Tukey's test
				\item Freund-Ansari-Bradley test for scale$^\text{np}$
				\item Friedman correlation test$^\text{np}$
				\item Friedman–Rafsky test$^\text{np}$
				\item Frosini test
				\item Fu and Li's test$^\text{np}$
				\item $G$-test
				\item Gabriel's pairwise test
				\item Galton's rank order test$^\text{np}$
				\item Gart's test
				\item Garud tests$^\text{np}$
				\item Gel-Gastwirth test
				\item Generalized extreme studentized deviate test$^\text{np}$ (GESD)
				\item Generalized rank $T$-test$^\text{np}$
				\item Generalized rank $Z$-test$^\text{np}$
				\item Generalized sequential probability ratio test$^\text{np}$ (GSPR)
				\item Generalised sign test$^\text{np}$
				\item Games-Howell test$^\text{np}$
				\item Geary's test
				\item Gehan's generalized Wilcoxon test$^\text{np}$
				\item Gini test (Gail and Gastwirth)
				\item Glejser's test
				\item Gnedenko $F$-test$^\text{np}$
				\item Goldfeld–Quandt's test
				\item Goodman-Kruskal's Gamma test$^\text{np}$
				\item Goodman-Kruskal's Lambda test$^\text{np}$
				\item Goodman-Kruskal's tau test$^\text{np}$
				\item Goodman's interaction test$^\text{np}$
				\item Gore's test$^\text{np}$
				\item Grambsch-Therneau test of proportionality
				\item Granger causality test
				\item Grazzini stationarity test$^\text{np}$
				\item Greenhouse-Geisser test
				\item Greenwood's test$^\text{np}$
				\item Greenwood-Quesenberry-Miller test$^\text{np}$
				\item Gregory–Hansen test$^\text{np}$
				\item Grubbs' test
				\item Gupta's test$^\text{np}$
				\item Hahn-Shapiro test
				\item Hadri test
				\item Harrington and Fleming's Gp tests
				\item Harris-Gnedenko test$^\text{np}$
				\item Harrison-McCabe test for heteroskedasticity
				\item Harrison-Kanji-Gadsden test
				\item Hartigan's Dip (HDS) test
				\item Hartley's (F-max) test
				\item Harbord's test
				\item Harvey-Collier test for linearity
				\item Hatemi–J test$^\text{np}$
				\item Haugh's test
				\item Hayter-Stone test$^\text{np}$
				\item Hayter-Nasimoto-Wright test
				\item Hegazy-Green test$^\text{np}$
				\item Henze-Zirkler test
				\item Hermans–Rasson test 
				\item Hettmansperger–McKean test$^\text{np}$
				\item Hirsch-Slack correlation test$^\text{np}$
				\item Hochberg's GT2 pairwise test$^\text{np}$
				\item Hodges' bivariate sign test$^\text{np}$
				\item Hodges-Lehmann test$^\text{np}$
				\item Hoeffding's independence test$^\text{np}$
				\item Hollander's bivariate symmetry test$^\text{np}$
				\item Hollander's parallelism test$^\text{np}$
				\item Hollander-Proshan test$^\text{np}$
				\item Hopkins-Skellam test
				\item Hosmer–Lemeshow test$^\text{np}$
				\item Hosmer-Le Cessie test$^\text{np}$
				\item Hosking test
				\item Hotelling's T2-test 
				\item Hsieh test$^\text{np}$
				\item Hsu's MCB (multiple comparisons with the best) test
				\item Hudson-Martin-Arguadé test$^\text{np}$ (HKA)
				\item Huynh-Feldt test
				\item Imam test$^\text{np}$
				\item Im, Pesaran and Shin test
				\item J-tests
				\item Jacquez’s $k$-nearest neighbours test
				\item Jalal and Jamshidian test
				\item Jarque-Bera test
				\item Jennrich test of the equality of two matrices\footnote{Correlation matrices typically!}
				\item Johansen cointegration test
				\item Joenssen’s JP-square test$^\text{np}$
				\item Jonckheere-Terpstra (JT) trend test$^\text{np}$
				\item Johnson-Mehrotra test$^\text{np}$
				\item Joint digit Benford test$^\text{np}$
				\item Judge-Schechter Mean Deviation test$^\text{np}$
				\item Inannan HCT test$^\text{np}$
				\item Inner-Wedge test
				\item K test (Kleibergen)
				\item Kallenberg-Ledwina test$^\text{np}$
				\item Kao's test$^\text{np}$
				\item Kattumannil-Sreedevi test$^\text{np}$
				\item Kaplan-Meier test$^\text{np}$
				\item Kaiser-Meyer-Olkin test
				\item Keenan's test$^\text{np}$
				\item Kendall rank correlation test$^\text{np}$
				\item Kendall's tau-b test$^\text{np}$
				\item Kendall's tau-c test$^\text{np}$
				\item Kendall's concordance W test$^\text{np}$
				\item Kendall's tau test$^\text{np}$
				\item Kenward-Roger tests
				\item Kim's ratio test
				\item Kimball test$^\text{np}$
				\item Kimber-Michael test
				\item KPSS (Kwiatkowski, Phillips, Schmidt and Shin) test$^\text{np}$
				\item Klotz scale test$^\text{np}$
				\item Knox's tests
				\item Kochar test$^\text{np}$
				\item Kochar and Gupta tests$^\text{np}$
				\item Kolmogorov-Smirnov test$^\text{np}$
				\item Kowalski's bivariate line test
				\item Koziol-Nemec test$^\text{np}$
				\item Kruskal-Wallace test 
				\item Kuiper's test$^\text{np}$
				\item Kurtosis test
				\item LaBreque's tests
				\item Lagrange multiplier (LM) score test
				\item Lanzante's test$^\text{np}$
				\item Larocque and Labarre sign test$^\text{np}$
				\item Lawless and Landau recurrence Mean Cumulated Function (MCF) test$^\text{np}$
				\item Le's test$^\text{np}$
				\item Lee-Wolfe test$^\text{np}$
				\item Lehmacher test$^\text{np}$
				\item Lehmann's test
				\item Leybourne-McCabe test
				\item Leybourne-Kim-Smith-Newbold test
				\item Lepage test
				\item Levene's test
				\item Liddell's exact test$^\text{np}$
				\item Li-Mak test 
				\item Likelihood ratio test (LRT)
				\item Lilliefors test
				\item Lim–Wolfe test$^\text{np}$
				\item Linear Time MMD$^\text{np}$
				\item Link–Wallace test
				\item Lin-Mudholkar test
				\item Lin-Wang test
				\item Lin-Xu test
				\item Lipsitz goodness of fit test$^\text{np}$
				\item Little’s test for data MCAR
				\item Liu and Singh rank sum test$^\text{np}$
				\item Ljung-Box test
				\item Locally most powerful rank order test$^\text{np}$
				\item Locally best invariant (LBI) test
				\item Log-rank (Mantel-Cox) test$^\text{np}$
				\item Locke and Spurrier tests
				\item Loh test$^\text{np}$
				\item Lo-MacKinlay test
				\item Lumsdaine and Papell (LP) unit root test
				\item Lorenz test$^\text{np}$
				\item Lu-Smith Normal score test
				\item Luukkonen test$^\text{np}$
				\item Macaskill test
				\item Mack–Wolfe's test$^\text{np}$
				\item Maddala-Wu panel unit root test$^\text{np}$
				\item Madhava Rao-Raghunath test$^\text{np}$
				\item Mann-Kendall trend test$^\text{np}$
				\item Mann-Whitney's test$^\text{np}$
				\item Mantel-Haenszel log-rank test$^\text{np}$
				\item Mantel's test
				\item Mardia's test of multivariate normality
				\item Mardia–Watson–Wheeler's test
				\item Martinez-Iglewicz test
				\item Mathisen's test$^\text{np}$
				\item Mauchly's test
				\item max-combo test
				\item McLeod-Li test
				\item McCabe–Tremayne's test
				\item McDonald-Kreitman test$^\text{np}$
				\item McNemar's test$^\text{np}$
				\item McCulloch test$^\text{np}$
				\item Mean slippage test (Schwager-Margolin)
				\item Median test$^\text{np}$
				\item Median crossing test$^\text{np}$
				\item Miller test$^\text{np}$
				\item Michael's test
				\item Mood's scale test$^\text{np}$
				\item Moon and Perron
				\item Mojena's test
				\item Moran test
				\item Moran's I  and II tests$^\text{np}$
				\item Moses test$^\text{np}$
				\item Mosteller's $k$-sample slippage test
				\item Mudlarks-McDermott test of ordered variance
				\item Mukerjee-Robertson-Wright multiple-contrast test
				\item Multi-binomial test
				\item Murphy's test
				\item Murakami $k$-Sample BWS normal test$^\text{np}$
				\item MZ test$^\text{np}$
				\item Nagarwalla’s scan statistic
				\item Nearest distance test$^\text{np}$ (Andrews-Bickel-Hampel-Huber-Rogers-Tukey)
				\item Nemenyi-Damico-Wolfe test$^\text{np}$
				\item (Student)-Newman–Keuls (SNK) test (contested)
				\item Neuhauser's test (Murakami's B2 test)
				\item Neumann trend test\footnote{Also named "adjacency test" or "mean successive difference test".}
				\item Neyman's smooth goodness of fit test
				\item Ng and Perron (NP) tests$^\text{np}$
				\item Nikulin-Rao-Robson test$^\text{np}$
				\item Noether's test for cyclical trend
				\item Norton's test$^\text{np}$
				\item O'Brien-Flemings test
				\item O'Brien's test for homogeneity of variance$^\text{np}$
				\item O'Brien's test for association of a single variable with survival$^\text{np}$
				\item Oja's tests
				\item Omega square test
				\item Orthogonal $T$-test
				\item Osius-Rojek test$^\text{np}$
				\item Page's test$^\text{np}$ (paired and not paired versions)
				\item Pardo test$^\text{np}$
				\item Park test
				\item Partial Pearson correlation trend test
				\item Partial Spearman correlation trend test$^\text{np}$ 
				\item Patell test
				\item Partial Theil U$^\text{np}$
				\item Pearson correlation $T$-test
				\item Pearson's chi-squared test
				\item Pearson-D'Agostino-Bowman test
				\item Pedroni's test
				\item Permutation tests
				\item Perron-Vogelsang test
				\item Pesaran’s test
				\item Peters test
				\item Pettitt’s test  
				\item Peto and Peto test (aka Peto-Prentice test)
				\item Phillips-Perron (PP) test$^\text{np}$
				\item Phillips-Ouliaris test$^\text{np}$
				\item Pillai's trace test
				\item Pitman-Morgan test
				\item Pietra test
				\item Pocock's test
				\item Plackett's interaction test 
				\item Poisson tests (E-test, C-test)
				\item Poisson recurrence Mean Cumulated Function (MCF) test$^\text{np}$
				\item Pope's Tau-test
				\item Pulkstenis-Robinson test$^\text{np}$
				\item Potthoff and Whitlinghill's test
				\item Potthoff's test$^\text{np}$
				\item Precedence tests$^\text{np}$
				\item Prentice test$^\text{np}$
				\item Prentice-Wilcoxon test$^\text{np}$
				\item Priestley's tests
				\item Priestley-Subba Rao (PSR) test
				\item Proportional Mass test$^\text{np}$
				\item Puri's expected Normal scores test$^\text{np}$
				\item Pustejovsky test
				\item $p$ (proportion/binomial)-test$^\text{np}$
				\item Quade test$^\text{np}$
				\item Quadratic Time MMD$^\text{np}$
				\item Positive quadrant dependence test$^\text{np}$
				\item Quenouille's test
				\item Rainbow test for linearity
				\item Ramsey Regression Equation Specification Error test (RESET)
				\item Rank product test$^\text{np}$
				\item Ranked von Neumann ratio test$^\text{np}$
				\item Rao's score test 
				\item Rao's spacing test of uniformity$^\text{np}$
				\item Rao's spacing test of homogoneity$^\text{np}$
				\item Rayleigh's test of uniformity
				\item Renyi test$^\text{np}$
				\item RESET test (REgression Specification Error)
				\item Robust rank-order test$^\text{np}$
				\item Rohlf generalized gap test
				\item Rosenbaum's test of uniformity$^\text{np}$
				\item Rosenbaum's cross matching test$^\text{np}$
				\item Rosner outlier test
				\item Rossberg test$^\text{np}$
				\item Royston's test
				\item Roy’s largest root test statistic
				\item Roy and Kastenbaum test$^\text{np}$
				\item Ryan-Einot-Gabriel-Welsch range (REGWQ) test
				\item Ryan-Einot-Gabriel-Welsch $F$ (REGWF) test
				\item Runs test$^\text{np}$
				\item Rücker's test$^\text{np}$
				\item Sandvik-Olsson test
				\item Sargan–Hansen test
				\item Sarkadi-Kosik test
				\item Savage's test
				\item Schach's two-sample tests$^\text{np}$
				\item Scheffé's test
				\item Schuster's test$^\text{np}$
				\item Scott-Knott test
				\item Schmidt-Phillips test
				\item Schwarzer's test$^\text{np}$
				\item Seasonal Hybrid ESD test
				\item Sen's slope test$^\text{np}$
				\item Semi-partial correlation tests
				\item Sequential test$^\text{np}$
				\item Serial correlation test$^\text{np}$
				\item Shapiro-Wilk adequation test
				\item Shapiro–Francia test
				\item Sherman's test$^\text{np}$
				\item Shirley-Williams test$^\text{np}$
				\item Siegel test$^\text{np}$
				\item Silverman’s bandwidth test$^\text{np}$
				\item Skewness test
				\item Skillings-Mack test$^\text{np}$
				\item Skillings-Wolfe expected Normal scores test$^\text{np}$
				\item Slatkin's exact test$^\text{np}$
				\item Slivka's test
				\item Smith quartile means test
				\item Smith and Jain's test
				\item Smith and Taylor ratio test$^\text{np}$
				\item Spatial scan statistic$^\text{np}$
				\item Spearman correlation test$^\text{np}$
				\item Sobel test
				\item Sobolev test$^\text{np}$
				\item Somers's d test$^\text{np}$
				\item Standard normal homogeneity test (SNHT)
				\item Steel test$^\text{np}$
				\item Stone’s test$^\text{np}$
				\item Stuart–Maxwell test$^\text{np}$
				\item Stukel test$^\text{np}$
				\item Subba Rao-Gabr's test$^\text{np}$
				\item Sukhatme's test$^\text{np}$
				\item sup-Wald test$^\text{np}$, sup-LM test$^\text{np}$, sup-LR test$^\text{np}$, sup-MZ test$^\text{np}$
				\item Swartz' entropy test$^\text{np}$
				\item Tarone's $Z$ test
				\item Taha's test$^\text{np}$
				\item Tajima’s D test$^\text{np}$
				\item Tamhane's T2 test
				\item Tamhane-Dunnett test
				\item Tango's maximized excess events test$^\text{np}$ (MEET)
				\item Tango’s score test
				\item Tarone-Ware test
				\item TAR-LR test
				\item Taylor robust stationarity test
				\item Taylor variance ratio test
				\item Tetrad test
				\item Tetrachoric correlation coefficient test$^\text{np}$
				\item Terasvirta neural network test$^\text{np}$
				\item Theil's $U$ test$^\text{np}$
				\item Tietjen-Moore test
				\item Tiku's test$^\text{np}$
				\item Toda-Yamamoto test
				\item Traditional HEGY (Hylleberg, Engle, Granger and Yoo) test$^\text{np}$
				\item Triangle test
				\item Triples test
				\item $T$-test for coefficient slope
				\item $T$-test (heteroscedastic or homoscedastic version - paired or unpaired version)
				\item Trim \& Fill test$^\text{np}$
				\item Trimmed means $T$-test
				\item Tryon $T$-test
				\item Tsay's test$^\text{np}$
				\item Two out of five test
				\item Tsai-Koziol correlation test
				\item Tukey's B test
				\item Tukey's quick test$^\text{np}$
				\item Tukey's test for non-additivity
				\item Tukey-Kramer test
				\item Turning point test$^\text{np}$
				\item Ury-Wiggins-Hochberg test
				\item Vasicek's sample entropy test
				\item V-test (modified Rayleigh)
				\item Von Neumann ratio test
				\item Vuong's test$^\text{np}$
				\item W/S-test
				\item Waerden Normal-Scores test
				\item Wald Chi-square test
				\item Wald's $F$-test
				\item Wald–Wolfowitz test$^\text{np}$ for continuous/dichotomous data
				\item Walsh's outlier test$^\text{np}$
				\item Walker's test$^\text{np}$
				\item Wall test$^\text{np}$
				\item Waller-Duncan $k$-ratio $T$-test
				\item Waller and Lawson’s score test
				\item Wallis-Moore phase frequency test$^\text{np}$
				\item Wang and Tsiatis' test$^\text{np}$
				\item Wavelet stationarity test$^\text{np}$
				\item Watson's $U^2$-test$^\text{np}$
				\item Watson–Williams test$^\text{np}$
				\item Wei–Lachin test
				\item Weisberg–Bingham test
				\item Welch(-Satterthwaite) $T$-test
				\item Westerlund cointegration test$^\text{np}$
				\item Jager-Wellner test$^\text{np}$
				\item Westenberg's interquartile range test
				\item Wheeler-Watson test$^\text{np}$
				\item White neural network test for non-linearity$^\text{np}$
				\item White test for homogeneity of variances 
				\item Whittemore's test
				\item Wilcoxon's rank sum test$^\text{np}$
				\item Wilcoxon's signed rank test$^\text{np}$
				\item Wilk's multivariate outlier test
				\item Wilk's lambda test
				\item William's trend test
				\item Woolf logit test$^\text{np}$
				\item Worsley's likelihood ratio test
				\item Wright tests
				\item Wu test$^\text{np}$
				\item Wu's H test$^\text{np}$
				\item Yang test$^\text{np}$
				\item Yuen's $T$-test
				\item Yuen-Welch $T$-test
				\item $Z$-test for the arithmetic or geometric mean
				\item $Z$-test for the difference between independent correlations
				\item Zelen's test$^\text{np}$
				\item Zhang's test$^\text{np}$
				\item Zivot-Andrews stationarity test
				\item ...
			\end{enumerate}
			\end{multicols}
			\end{small}
		
		\item \textbf{Regression and regularization techniques\footnote{see section Numerical Methods page \pageref{regression techniques}}:}
			\begin{small}
			\begin{multicols}{2}
			\begin{enumerate}
				\item Linear regression model (LSM)
				\item Gaussian linear regression model		
				\item Non-linear regression models with binary or continuous variables 
				\item Polynomial regression model 
				\item Weighted least squares regression model
				\item B-spline or of collocation polynomial regression		
				\item Logistic regression models (binomial, multinomial, ordinal)
				\item Counting Poisson regression (Poisson MLE, PMLE, GLM, Poison-quasi-Lindley) 
				\item Negative binomial (binomial MLE and QGPMLE) regression model		
				\item Beta regression
				\item Cox proportional hazard regression
				\item Threshold regression models
				\item Total linear regression
				\item Orthogonal linear regression model
				\item Deming regression
				\item Quantile regression model	
				\item Partial least squares regression (PLS)
				\item Structural equation modelling (Two-Stage Least Squares/2SLS)
				\item Moderated multiple regression
				\item Mixed linear/non-linear or generalized models (nested or not, splitted or not)
				\item Panel regression
				\item LAD (Least Absolute Deviation) regression 		
				\item LOESS (LOcally Estimated Scatterplot Smoothing) and LOWESS (LOcally Weighted Scatterplot Smoothing)		
				\item Least absolute shrinkage and selection operator (LASSO)
				\item Multivariate adaptive regression splines (MARS)		
				\item Bayesian linear regression model
				\item Logic regression
				\item Ridge regression model 		
				\item Bootstrap or Jackknife regression model
				\item Backward elimination regressions
				\item Forward entry regression
				\item C-RT regression tree
				\item DfBetas
				\item Epsilon SVR (support vector regression)
				\item Nu SVR
				\item Least-angle regression (LARS)
				\item Elastic net regularization 
				\item Simplex regression
				\item Independent component regression
				\item Theil-Sen estimator regression method
				\item Isotonic regression
				\item ...
			\end{enumerate}
			\end{multicols}
			\end{small}
		
		\item \textbf{Factor Analysis (FA):}
			\begin{small}
			\begin{multicols}{2}
			\begin{enumerate}
				\item FADM (Mixed data factor analysis)
				\item Bootstrap eigenvalues
				\item Linear discriminant analysis (canonical LDA)/Kernel LDA
				\item Correspondence analysis
				\item Discriminant correspondence analysis
				\item Principal Component Analysis with/without Factor rotation (VariMax)
				\item Harris component analysis
				\item Multiple correspondence analysis
				\item NIPALS (Non-linear Iterative Partial Least Squares)
				\item Parallel analysis
				\item Principal factor analysis
				\item ...
			\end{enumerate}
			\end{multicols}
			\end{small}
				
		\item \textbf{Clustering (unsupervised):}
			\begin{small}
			\begin{multicols}{2}
			\begin{enumerate}
				\item CatVARCHA (categorical variable hierarchical agglomerative clustering)
				\item Clustering Tree (CT) with/without post-pruning
				\item Fuzzy clustering tree
				\item Kernel Density estimation (density based clustering)
				\item Kernel PCA (Principal Component Analysis)
				\item Singular Value Decomposition (SVD)
				\item EM-Clustering (Expectation-Maximization)
				\item HAC (Hierarchical Agglomerative Clustering)
				\item $k$-means, $k$-Medians or $k$-Medoids clustering, Fuzzy clustering, $k$-Modes, $k$-Prototypes
				\item Kernel $k$-means
				\item Mean shift clustering
				\item Spectral clustering
				\item Multidimensional scaling (MDS)
				\item Kohonen-SOM (Self Organization Map)
				\item Kohonen-LVQ (Learning Vector Quantizer)
				\item GMM (Gaussian Mixture Model)
				\item VARCLUS (top down approach)
				\item VARHCA (clustering variables using Hierarchical Cluster Analysis)
				\item VARKMeans (clustering variable using $k$-means)
				\item Density-based spatial clustering of applications with noise (DBSCAN)
				\item Soft independent modelling of class analogies (SIMCA)
				\item ...
			\end{enumerate}
			\end{multicols}
			\end{small}
		
		\item \textbf{Supervised (Spv) Learning:}
			\begin{small}
			\begin{multicols}{2}
			\begin{enumerate}
				\item Binary logistic regression
				\item BVM (Ball Vector Machine)
				\item Iterative Dichotomiser 3 (ID3 Quinla algorithm)
				\item C4.5 (Quinlan algorithm extension of ID3)
				\item M5 (Quinlan algorithm)
				\item Cubist (extension of M5)
				\item C-PLS (PLS for classification)
				\item C-RT (Regression Tree for classification)
				\item CS-CRT (Cost Sensitive Classification RT)
				\item C-MC4 (M-Estimates based and Laplace smoothed CS-CRT)
				\item C-SVC (Continuous Supervised Classification)
				\item CVM (Core Vector Machine)
				\item Decision List (One-Rule, Zero-Rule, CN2)
				\item Repeated Incremental Pruning to Produce Error Reduction (RIPPER) 
				\item CHAID (Chi-squared Automatic Interaction Detection)
				\item Gradient Boosting
				\item Gaussian processes
				\item $k$-nn (Nearest Neigbhors)
				\item Linear discriminant analysis (canonical LDA)/Kernel LDA
				\item QDA (Quadratic Discriminant Analysis)
				\item Log-Reg TRIRLS 
				\item Multilayer perceptron (MLP neural network)
				\item Multinomial Logistic Regression
				\item Naive bayes categorical variables classification
				\item Naive bayes continuous variables classification
				\item PLS-DA (PLS Discriminant Analysis)
				\item PLS-LDS (PLS Linear DA)
				\item Prototype-NN (Nearest Neighbours)
				\item Radial basis function (RBF neural network)
				\item Random Tree
				\item Rule Induction, Fuzzy rule induction
				\item Support Vector Machine (SVM)
				\item Genetic classification
				\item Locally Weighted Learning (LWL)
				\item ...
			\end{enumerate}
			\end{multicols}
			\end{small}
		
		\item \textbf{Semi-Supervised learning:}
			\begin{small}
			\begin{enumerate}
				\item Contrastive Pessimistic Likelihood Estimation (CPLE)
				\item semi-supervised vector machine (S3VM)
				\item Transductive Support Vector Machines (TSVM)
				\item Label propagation
				\item Label spreading
				\item ...
			\end{enumerate}
			\end{small}
			
		\item \textbf{Meta Supervised learning (ensemble learning):}
			\begin{small}
			\begin{multicols}{2}
			\begin{enumerate}
				\item Random forests
				\item Extremely Randomized trees
				\item Rotation forests
				\item Mondrian forests
				\item Gradient Boosting Machines (GMB)
				\item Boosting
				\item Bootstrapped Aggregation (bagging)
				\item CatBoost, AdaBoost, XGBoost
				\item Light GBM
				\item Stacked Generalization (blending)
				\item Gradient Boosted Regression Trees (GBRT)
				\item Arcing (Arc-x4) bagging with weights
				\item Bagging with/without cost sensitivity
				\item Boosting with/without cost sensitivity
				\item MultiCost sensitive supervised learning
				\item Stacked generalization
				\item ...
			\end{enumerate}
			\end{multicols}
			\end{small}
			
		\item \textbf{\text{Supervised learning assessment:}}
			\begin{small}
			\begin{multicols}{2}
			\begin{enumerate}
				\item Bias-variance decomposition (Wolpert \& Kohavi)
				\item Bootstrap
				\item Cross-validation
				\item Leave-One-Out
				\item Test set assessment
				\item Learning/Train assessment test
				\item ...
			\end{enumerate}
			\end{multicols}
			\end{small}
		
		\item \textbf{Scoring:}
			\begin{small}
			\begin{multicols}{2}
			\begin{enumerate}
				\item Lift curve
				\item ROC curve
				\item Precision-Recall curve
				\item Log-loss (and ie log likelihood)
				\item Log-pointwise predictive density (LPPD)
				\item Reliability diagram
				\item ...
			\end{enumerate}
			\end{multicols}
			\end{small}
			
		\item \textbf{Association:}
			\begin{small}
			\begin{multicols}{2}
			\begin{enumerate}
				\item A priori (MR version or not)
				\item A priori PT (Borgelt's algorithm)
				\item Assoc outlier (association rule mining principle)
				\item Frequent itemsets (Borgelt's algorithm)
				\item ECLAT (equivalence class transformation algorithm)
				\item FP-growth (frequent pattern growth)
				\item RElim (recursive elimination)
				\item SaM (Split and Merge)
				\item JIM (Jaccard Itemset Mining)
				\item Spv association rule 
				\item Spv association tree
				\item ...
			\end{enumerate}
			\end{multicols}
			\end{small}
			
		\item \textbf{Anomaly detection:}
			\begin{small}
			\begin{multicols}{2}
			\begin{enumerate}
				\item Chauvenet's criterion
				\item Cochran C test
				\item Dixon's test
				\item Grubb test
				\item Cook's distance
				\item Peirce's criterion
				\item Studentized residual
				\item Isolation forest
				\item Control Charts\footnote{see section Industrial Engineering (page \pageref{quality control charts}) for the details on control charts.} (P, NP, C, U, R-R, X-R, etc.)
				\item Markov modulated Poisson process (MMPP)
				\item Local outlier factor (FOL)
				\item Online Over-Sampling Principal Component Analysis
				\item ...
			\end{enumerate}
			\end{multicols}
			\end{small}
		
		\item \textbf{Reinforcement learning:}
			\begin{small}
			\begin{multicols}{2}
			\begin{enumerate}
				\item Markov Decisions Processes
				\item Case-Based Reasoning 
				\item Genetic algorithm
				\item Asynchronous Advantage Actor Critic (A3C) 
				\item State–Action–Reward–State–Action (SARSA)
				\item Q-learning 
				\item TD($\lambda$) algorithm
				\item TD($0$) Actor-Critic
				\item ...
			\end{enumerate}
			\end{multicols}
			\end{small}
			
		\item \textbf{Imputations methods\footnote{Be always careful when using imputation methods. For example impute missing values with mean or zero alters summary statistics significantly, changes the distribution significantly and inflate the presence of a specific value and therefore leads to inaccurate modelling and unlikely conclusions.}:}
			\begin{small}
			\begin{multicols}{2}
			\begin{enumerate}
				\item Predictive mean matching
				\item Weighted predictive mean matching 
				\item Random sample from observed values
				\item Classification and regression trees 
				\item Hot or Cold deck imputation
				\item Random forest imputations
				\item Unconditional mean imputation
				\item Bayesian linear regression
				\item Linear regression ignoring model error
				\item Linear regression using bootstrap
				\item Linear regression, predicted values
				\item Imputation of quadratic terms
				\item Laplace smoothing
				\item Random indicator for non-ignorable data
				\item Logistic regression
				\item Logistic regression with bootstrap
				\item Proportional odds model
				\item Polytomous logistic regression
				\item Linear discriminant analysis (LDA)
				\item Level-1 normal heteroscedastic
				\item Level-1 logistic
				\item Level-2 class mean/normal
				\item Level-2 class predictive mean matching
				\item Multivariate imputation by Chained Equations 
				\item $k$-NN imputer
				\item ...
			\end{enumerate}
			\end{multicols}
			\end{small}
			
		\item \textbf{Forecastings:}
			\begin{small}
			\begin{multicols}{2}
			\begin{enumerate}
				\item Linear and polynomial regressions (see above)
				\item Moving Average
				\item Simple exponential smoothing
				\item Double exponential smoothing (Brown)
				\item Triple exponential smoothing (Holt \& Winters)
				\item ETS (Error Trend Seasonal) models
				\item Logistic forecasts
				\item Croston's method / Adjusted Croston's method
				\item ARIMA\footnote{AutoRegressive Integrated Moving Average} processes (AR, ARMA DARMA, INARMA, ARIMA, SARIMA, ARFIMA, SARFIMA, ARIMAX)
				\item GARCH (General-ARCH\footnote{AutoRegressive Conditional Heteroskedasticity}) processes (GARCH-X, EGARCH, ARCH, sGARCH, RUGARCH, iGARCH, MGARCH)
				\item NARCH (Non-linear-ARCH) processes
				\item TARCH (Threshold-ARCH) processes
				\item Independent Component Analysis (ICA)
				\item ...
			\end{enumerate}
			\end{multicols}
			\end{small}
			
		\item \textbf{Sequence mining:}
			\begin{small}
			\begin{enumerate}
				\item GSP (Generalized Sequential Patterns)
				\item SPADE (Sequential Pattern Discovery)
				\item FreeSpan
				\item HMM (hidden Markov models)
				\item ...
			\end{enumerate}
			\end{small}
			
		\item \textbf{Neural Networks/Deep learning:}
			\begin{small}
			\begin{multicols}{2}
			\begin{enumerate}
				\item Perceptron
				\item Feed-Forward Neural Network (FFNN)
				\item Radial Basis Network (RBF)
				\item Recurrent Neural Network (RNN)
				\item Gated Recurrent Unit (GRU)
				\item Long Short-Term Memory (LSTM)
				\item Deep Boltzmann Machine (DBM)
				\item Deep Feed Forward Networks (DBN)
				\item Deep Belief Networks (DBN)
				\item Deep Belief Convolutional Network (DCN)
				\item Deep Residual Network (DRN)
				\item Differentiable Neural Computer (DNC)
				\item Neural Turing Machine (NTM)
				\item Capsule Network (CN)
				\item Kohonen Network (KN)
				\item Attention Network (AN)
				\item Deconvolutional Network (DN)
				\item Deep Convolutional Inverse Graphics Network (DCIGN)
				\item Liquid State Machine (LSM)
				\item Extreme Learning Machine (ELM)
				\item Echo State Network (ESN)
				\item Convolutional Neural Networks (CNN)
				\item Fuzzy Neural Network (FNN)
				\item Spectral-Residual CNN (SR-CNN)
				\item Super Resolution CNN (SRCNN)
				\item Stacked Auto-Encoders (SAE)
				\item Variational Auto-Encoder (VAE)
				\item Denoising Auto-Encoder (DAE)
				\item Sparse Auto-Encoder (DAE)
				\item Hopfield Network (HN)
				\item Restricted Boltzmann Machine Network (RBM)
				\item Adaptative Neuro-Fuzzy Inference System (ANFIS)
				\item Generative Adversarial Networks (GAN)
				\item Wasserstein Generative Adversarial Networks (WGAN)
				\item Spectral Normalization Generative Adversarial Networks (SN-GAN)
				\item Self-attention Generative Adversarial Networks (S-GAN)
				\item Progressive Generative Adversarial Networks (PROGAN)
				\item Contextual RNN-GANs (Context-RNN-GAN)
				\item Continuous recurrent neural networks With Generative Adversarial Networks (C-RNN-GAN)
				\item Conditional Sequence Generative Adversarial Networks (CS-GAN)
				\item Fine-Grained Image Generation through Asymmetric Training (CVAE-GAN)
				\item Cycle-consistent Adversarial Networks (CycleGAN)
				\item Unsupervised Cross-Domain Image (DTN)
				\item Unsupervised Learning With Deep Convolutional Generative Adversarial Networks (DCGAN)
				\item Discover Cross-Domain Relations Generative Adversarial Networks (DiscoGAN)
				\item Disentangled Representation Learning GAN for pose-lnvariant Recognition (DR-GAN)
				\item Unsupervised Dual Learning for Translation (DualGAN)
				\item Energy-based Generative Adversarial Network (EBCAN)
				\item Training Generative Neural Samplers Variational Divergence Minimization (f-GAN)
				\item Face Frontalization Generative Adversarial Network (FF-GAN)
				\item Generative Adversarial What-Where Network (GAWWN)
				\item Learning Object Transfiguration and Attribute Subspacefrom unpaired Data (GeneGAN)
				\item Geometric Generative Adversarial Networks (GGAN)
				\item Generative Adversarial Networks With Maximum Margin Ranking (Hogan)
				\item Towards Realistic Blending (GP-GAN)
				\item Introspective Adversarial Networks (IAN)
				\item Natural Image Manifold Generative Adversarial Networks (iGAN)
				\item Invertible Conditional Generative Adversarial Networks (ICGAN)
				\item Image De-raining using a Conditional Generative Adversarial Network (ID-CGAN)
				\item Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets (InfoGAN)
				\item Location-Aware Generative Adversarial Networkss (LAGAN) 
				\item Laplacian Pyramid Of Adversarial Networks (LAPGAN)
				\item 3D Generative-Adversarial Modelling (3D-GAN)
				\item Face Aging With Conditional Generative Adversarial Networks (acGAN)
				\item Auxiliary Classifier Generative Adversarial Networks (AC-GAN )
				\item Boosting Generative Models (AdaGAN)
				\item Autoencoder based Generative Adversarial Networks (AEGAN)
				\item Amortised MAP Inference for Image Super-resolution (AffGAN)
				\item Attributes and Semantic Layouts Conditional Generative Adversarial Networks (AL-CGAN)
				\item Adversarial Learned Inference (ALI)
				\item Activation Maximization Generative Adversarial Networks (AM-GAN)
				\item Anomaly Detection Generative Adversarial Networks (AnoGAN)
				\item Artwork Synthesis Generative Adversarial Networks (ArtGAN)
				\item Brainstorming Generative Adversarial Networks (b-GAN)
				\item Bayesian GAN (BGAN)
				\item Boundary Equilibrium Generative Adversarial Networks (BEGAN)
				\item Bidirectional (Generative Adversarial Networks)
				\item Boundary-Seeking Generative Adversarial Networks (BS-GAN)
				\item Conditional Generative Adversarial Nets (CGAN)
				\item Calorimeters With Generative Adversarial Networks (CaloGAN)
				\item Context-Conditional Generative Adversarial Networks (CCCAN)
				\item Categorical Generative Adversarial Networks (CatGAN)
				\item Coupled Generative Adversarial Networks (COGAN)
				\item ...
			\end{enumerate}
			\end{multicols}
			\end{small}
			
		\item \textbf{Cost functions:}
			\begin{small}
			\begin{multicols}{2}
			\begin{enumerate}
				\item Quadratic cost
				\item Cross-entropy cost
				\item Exponential cost
				\item Hellinger cost
				\item Kullback-Leibler cost
				\item Generalized Kullback-Leibler cost
				\item Bregman cost
				\item Jensen-Shannon cost
				\item Itakura-Saito cost
				\item ...
			\end{enumerate}
			\end{multicols}
			\end{small}
			
		\item \textbf{Calibration methods:}
			\begin{small}
			\begin{multicols}{2}
			\begin{enumerate}
				\item Histogram binning
				\item Isotonic regression
				\item Bayesian Binning into Quantiles (BBQ)
				\item Platt scaling
				\item Matrix and vector scaling
				\item ...
			\end{enumerate}
			\end{multicols}
			\end{small}
		
		\item \textbf{Text Mining:}
			\begin{small}
			\begin{multicols}{2}
			\begin{enumerate}
				\item Latent Semantic Analysis (LSA)
				\item Latent Semantic Indexing (LSI)
				\item Latent Dirichlet allocation (LDA)
				\item Probabilistic Latent Semantic Indexing (PLSI)
				\item Vector space model (term vector model)
				\item Sentiment Analysis
				\item Language detection
				\item Pattern detection/correlation
				\item ...
			\end{enumerate}
			\end{multicols}
			\end{small}
	\end{itemize}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Keep in mind that Machine Learning cannot be used for everything related to statistics. Indeed, in drug assessment and approval we are limited by a set of statistical methods forming an industry standard and approved by regulatory agencies (FDA, EMA, etc.), by the accepted approach named "confirmatory data analysis" via inferential statistics, both frequentist and bayesian, by small and extremely small data (starting from $5$ observations in early trial phases) by demand for interpretability (not rarely we prefer here simpler but more "transparent" methods rather than efficient ones). In clinical research, every single possible aspect is regulated by guidelines, controlled by validation, every step of statistical analysis must be well enough justified under the threat of being rejected by regulatory or statistical reviewers and lead the whole trial to fail.
	\end{tcolorbox}
	
	The reader must also keep in mind some possible limitations of predictive model based on data fitting:
	\begin{itemize}
		\item History cannot always predict future
		\item The issue of unknown unknowns
		\item Bad model choice
		\item Over-fitting model
		\item Overconfidence of humans on algorithms results
		\item ...
	\end{itemize}
	
	So we see through these lists that Data Mining and Machine Learning are child of statistics, computer science, and mathematical optimization. Along the way, it took inspiration from information theory, neural science, theoretical physics, and many other fields.
	
	This was the scientist point of view of Data Mining... From the point of view of business, Data Mining is more considered as the following data life cycle:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/computing/data_mining_life_cycle.jpg}
		\caption[Data Mining life cycle as seen by SAS™]{Data Mining life cycle as seen by SAS™ (source: SlideShare SAS)}
	\end{figure}
	
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Remember, as we already said it in the introduction chapter of this book, that statisticians do not in general exactly agree on how to analyse anything but the simplest of problems! The fact that statistical inference uses mathematics does not imply there is only one reasonable or useful way to conduct an analysis. Engineering uses math as well, but there are many ways to build a bridge. So trying multiple analyses given to multiple independent teams of researchers may be considered scientifically prudent to test the robustness of findings.
	\end{tcolorbox}
	
	And also let us introduce the 4V's of Data Mining/Machine Learning that resume quite well the most common cases of data to which the Data Scientist is faced to:
	\begin{figure}[H]
		\centering
		% Gradient Info
		  
		\tikzset {_55vjy2jkw/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{-90 }  \pgftransformscale{2 }  }}}
		\pgfdeclarehorizontalshading{_701lezanm}{150bp}{rgb(0bp)=(0.6,0.85,1);
		rgb(37.5bp)=(0.6,0.85,1);
		rgb(62.5bp)=(0,0.5,0.5);
		rgb(100bp)=(0,0.5,0.5)}
		
		% Gradient Info
		  
		\tikzset {_i9bbqc416/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{-90 }  \pgftransformscale{2 }  }}}
		\pgfdeclarehorizontalshading{_pphz17vo1}{150bp}{rgb(0bp)=(0.6,0.85,1);
		rgb(37.5bp)=(0.6,0.85,1);
		rgb(62.5bp)=(0,0.5,0.5);
		rgb(100bp)=(0,0.5,0.5)}
		
		% Gradient Info
		  
		\tikzset {_plofqte8g/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{-90 }  \pgftransformscale{2 }  }}}
		\pgfdeclarehorizontalshading{_g12uf5uzq}{150bp}{rgb(0bp)=(0.6,0.85,1);
		rgb(37.5bp)=(0.6,0.85,1);
		rgb(62.5bp)=(0,0.5,0.5);
		rgb(100bp)=(0,0.5,0.5)}
		
		% Gradient Info
		  
		\tikzset {_pdwt6hv0d/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{-90 }  \pgftransformscale{2 }  }}}
		\pgfdeclarehorizontalshading{_vxk5koa9e}{150bp}{rgb(0bp)=(0.6,0.85,1);
		rgb(37.5bp)=(0.6,0.85,1);
		rgb(62.5bp)=(0,0.5,0.5);
		rgb(100bp)=(0,0.5,0.5)}
		
		% Gradient Info
		  
		\tikzset {_qeigkqps2/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformscale{1 }  }}}
		\pgfdeclareradialshading{_1911hdlsl}{\pgfpoint{0bp}{0bp}}{rgb(0bp)=(0,0,0);
		rgb(0.1618298462459019bp)=(0,0,0);
		rgb(25bp)=(1,1,1);
		rgb(400bp)=(1,1,1)}
		
		% Gradient Info
		  
		\tikzset {_emxnt22fc/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformscale{1 }  }}}
		\pgfdeclareradialshading{_q9byl8127}{\pgfpoint{0bp}{0bp}}{rgb(0bp)=(0,0,0);
		rgb(0.1618298462459019bp)=(0,0,0);
		rgb(25bp)=(1,1,1);
		rgb(400bp)=(1,1,1)}
		
		% Gradient Info
		  
		\tikzset {_qqfpazku0/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformscale{1 }  }}}
		\pgfdeclareradialshading{_2608flvwz}{\pgfpoint{0bp}{0bp}}{rgb(0bp)=(0,0,0);
		rgb(0.1618298462459019bp)=(0,0,0);
		rgb(25bp)=(1,1,1);
		rgb(400bp)=(1,1,1)}
		
		% Gradient Info
		  
		\tikzset {_7bydj8qvz/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformscale{1 }  }}}
		\pgfdeclareradialshading{_q7hg0oot3}{\pgfpoint{0bp}{0bp}}{rgb(0bp)=(0,0,0);
		rgb(0.1618298462459019bp)=(0,0,0);
		rgb(25bp)=(1,1,1);
		rgb(400bp)=(1,1,1)}
		
		% Gradient Info
		  
		\tikzset {_x5nb9nd2c/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformscale{1 }  }}}
		\pgfdeclareradialshading{_vxndtc1gv}{\pgfpoint{0bp}{0bp}}{rgb(0bp)=(0,0,0);
		rgb(0.1618298462459019bp)=(0,0,0);
		rgb(25bp)=(1,1,1);
		rgb(400bp)=(1,1,1)}
		
		% Gradient Info
		  
		\tikzset {_r5knpdskb/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformscale{1 }  }}}
		\pgfdeclareradialshading{_b9vf5xtgg}{\pgfpoint{0bp}{0bp}}{rgb(0bp)=(0,0,0);
		rgb(0.1618298462459019bp)=(0,0,0);
		rgb(25bp)=(1,1,1);
		rgb(400bp)=(1,1,1)}
		
		% Gradient Info
		  
		\tikzset {_s09zw8gxa/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformscale{1 }  }}}
		\pgfdeclareradialshading{_t7myh7j85}{\pgfpoint{0bp}{0bp}}{rgb(0bp)=(0,0,0);
		rgb(0.1618298462459019bp)=(0,0,0);
		rgb(25bp)=(1,1,1);
		rgb(400bp)=(1,1,1)}
		
		% Gradient Info
		  
		\tikzset {_8k0o2w4mt/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformscale{1 }  }}}
		\pgfdeclareradialshading{_bwixx49iu}{\pgfpoint{0bp}{0bp}}{rgb(0bp)=(0,0,0);
		rgb(0.1618298462459019bp)=(0,0,0);
		rgb(25bp)=(1,1,1);
		rgb(400bp)=(1,1,1)}
		
		% Gradient Info
		  
		\tikzset {_63i4wnz42/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformscale{1 }  }}}
		\pgfdeclareradialshading{_qsw7d4w9r}{\pgfpoint{0bp}{0bp}}{rgb(0bp)=(0,0,0);
		rgb(0.1618298462459019bp)=(0,0,0);
		rgb(25bp)=(1,1,1);
		rgb(400bp)=(1,1,1)}
		
		% Gradient Info
		  
		\tikzset {_53o0er9ot/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformscale{1 }  }}}
		\pgfdeclareradialshading{_ao9016aew}{\pgfpoint{0bp}{0bp}}{rgb(0bp)=(0,0,0);
		rgb(0.1618298462459019bp)=(0,0,0);
		rgb(25bp)=(1,1,1);
		rgb(400bp)=(1,1,1)}
		
		% Gradient Info
		  
		\tikzset {_oscnr0vn0/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformscale{1 }  }}}
		\pgfdeclareradialshading{_r50ysld39}{\pgfpoint{0bp}{0bp}}{rgb(0bp)=(0,0,0);
		rgb(0.1618298462459019bp)=(0,0,0);
		rgb(25bp)=(1,1,1);
		rgb(400bp)=(1,1,1)}
		
		% Gradient Info
		  
		\tikzset {_lycmv2vwb/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformscale{1 }  }}}
		\pgfdeclareradialshading{_fymvlh0xo}{\pgfpoint{0bp}{0bp}}{rgb(0bp)=(0,0,0);
		rgb(0.1618298462459019bp)=(0,0,0);
		rgb(25bp)=(1,1,1);
		rgb(400bp)=(1,1,1)}
		\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        
		
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,653); %set diagram left start at 0, and has height of 653
		
		%Rounded Same Side Corner Rect [id:dp14222550262894873] 
		\draw  [draw opacity=0][shading=_701lezanm,_55vjy2jkw] (60,47.36) .. controls (60,43.5) and (63.13,40.38) .. (66.98,40.38) -- (172.39,40.38) .. controls (176.25,40.38) and (179.38,43.5) .. (179.38,47.36) -- (179.38,75.28) .. controls (179.38,75.28) and (179.38,75.28) .. (179.38,75.28) -- (60,75.28) .. controls (60,75.28) and (60,75.28) .. (60,75.28) -- cycle ;
		%Rounded Rect [id:dp821618536305257] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] (60,47.36) .. controls (60,43.83) and (62.85,40.98) .. (66.38,40.98) -- (173,40.98) .. controls (176.52,40.98) and (179.38,43.83) .. (179.38,47.36) -- (179.38,288.88) .. controls (179.38,292.4) and (176.52,295.26) .. (173,295.26) -- (66.38,295.26) .. controls (62.85,295.26) and (60,292.4) .. (60,288.88) -- cycle ;
		%Rounded Same Side Corner Rect [id:dp2850733864306412] 
		\draw  [draw opacity=0][shading=_pphz17vo1,_i9bbqc416] (191,47.36) .. controls (191,43.5) and (194.13,40.38) .. (197.98,40.38) -- (303.39,40.38) .. controls (307.25,40.38) and (310.38,43.5) .. (310.38,47.36) -- (310.38,75.28) .. controls (310.38,75.28) and (310.38,75.28) .. (310.38,75.28) -- (191,75.28) .. controls (191,75.28) and (191,75.28) .. (191,75.28) -- cycle ;
		%Rounded Rect [id:dp8776904980438334] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] (191,47.36) .. controls (191,43.83) and (193.85,40.98) .. (197.38,40.98) -- (304,40.98) .. controls (307.52,40.98) and (310.38,43.83) .. (310.38,47.36) -- (310.38,288.88) .. controls (310.38,292.4) and (307.52,295.26) .. (304,295.26) -- (197.38,295.26) .. controls (193.85,295.26) and (191,292.4) .. (191,288.88) -- cycle ;
		%Rounded Same Side Corner Rect [id:dp34831297809549255] 
		\draw  [draw opacity=0][shading=_g12uf5uzq,_plofqte8g] (322,46.36) .. controls (322,42.5) and (325.13,39.38) .. (328.98,39.38) -- (434.39,39.38) .. controls (438.25,39.38) and (441.38,42.5) .. (441.38,46.36) -- (441.38,74.28) .. controls (441.38,74.28) and (441.38,74.28) .. (441.38,74.28) -- (322,74.28) .. controls (322,74.28) and (322,74.28) .. (322,74.28) -- cycle ;
		%Rounded Rect [id:dp7847413322157322] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] (322,46.36) .. controls (322,42.83) and (324.85,39.98) .. (328.38,39.98) -- (435,39.98) .. controls (438.52,39.98) and (441.38,42.83) .. (441.38,46.36) -- (441.38,287.88) .. controls (441.38,291.4) and (438.52,294.26) .. (435,294.26) -- (328.38,294.26) .. controls (324.85,294.26) and (322,291.4) .. (322,287.88) -- cycle ;
		%Rounded Same Side Corner Rect [id:dp635962286937078] 
		\draw  [draw opacity=0][shading=_vxk5koa9e,_pdwt6hv0d] (452,46.36) .. controls (452,42.5) and (455.13,39.38) .. (458.98,39.38) -- (564.39,39.38) .. controls (568.25,39.38) and (571.38,42.5) .. (571.38,46.36) -- (571.38,74.28) .. controls (571.38,74.28) and (571.38,74.28) .. (571.38,74.28) -- (452,74.28) .. controls (452,74.28) and (452,74.28) .. (452,74.28) -- cycle ;
		%Rounded Rect [id:dp6077124276396781] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] (452,46.36) .. controls (452,42.83) and (454.85,39.98) .. (458.38,39.98) -- (565,39.98) .. controls (568.52,39.98) and (571.38,42.83) .. (571.38,46.36) -- (571.38,287.88) .. controls (571.38,291.4) and (568.52,294.26) .. (565,294.26) -- (458.38,294.26) .. controls (454.85,294.26) and (452,291.4) .. (452,287.88) -- cycle ;
		%Shape: Circle [id:dp24402377782839757] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (72.72,93.64) .. controls (72.72,91.63) and (74.35,90) .. (76.36,90) .. controls (78.37,90) and (80,91.63) .. (80,93.64) .. controls (80,95.65) and (78.37,97.28) .. (76.36,97.28) .. controls (74.35,97.28) and (72.72,95.65) .. (72.72,93.64) -- cycle ;
		%Shape: Circle [id:dp5930803334526251] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (91.47,93.64) .. controls (91.47,91.63) and (93.1,90) .. (95.11,90) .. controls (97.12,90) and (98.75,91.63) .. (98.75,93.64) .. controls (98.75,95.65) and (97.12,97.28) .. (95.11,97.28) .. controls (93.1,97.28) and (91.47,95.65) .. (91.47,93.64) -- cycle ;
		%Shape: Circle [id:dp656671167219468] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (110.22,93.64) .. controls (110.22,91.63) and (111.85,90) .. (113.86,90) .. controls (115.87,90) and (117.5,91.63) .. (117.5,93.64) .. controls (117.5,95.65) and (115.87,97.28) .. (113.86,97.28) .. controls (111.85,97.28) and (110.22,95.65) .. (110.22,93.64) -- cycle ;
		%Shape: Circle [id:dp9590662467722133] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (128.97,93.64) .. controls (128.97,91.63) and (130.6,90) .. (132.61,90) .. controls (134.62,90) and (136.25,91.63) .. (136.25,93.64) .. controls (136.25,95.65) and (134.62,97.28) .. (132.61,97.28) .. controls (130.6,97.28) and (128.97,95.65) .. (128.97,93.64) -- cycle ;
		%Shape: Circle [id:dp05472342093108118] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (147.72,93.64) .. controls (147.72,91.63) and (149.35,90) .. (151.36,90) .. controls (153.37,90) and (155,91.63) .. (155,93.64) .. controls (155,95.65) and (153.37,97.28) .. (151.36,97.28) .. controls (149.35,97.28) and (147.72,95.65) .. (147.72,93.64) -- cycle ;
		
		%Shape: Circle [id:dp06117121348202681] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (82.72,106.36) .. controls (82.72,104.35) and (84.35,102.72) .. (86.36,102.72) .. controls (88.37,102.72) and (90,104.35) .. (90,106.36) .. controls (90,108.37) and (88.37,110) .. (86.36,110) .. controls (84.35,110) and (82.72,108.37) .. (82.72,106.36) -- cycle ;
		%Shape: Circle [id:dp6819826088269698] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (101.47,106.36) .. controls (101.47,104.35) and (103.1,102.72) .. (105.11,102.72) .. controls (107.12,102.72) and (108.75,104.35) .. (108.75,106.36) .. controls (108.75,108.37) and (107.12,110) .. (105.11,110) .. controls (103.1,110) and (101.47,108.37) .. (101.47,106.36) -- cycle ;
		%Shape: Circle [id:dp6103409812815095] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (120.22,106.36) .. controls (120.22,104.35) and (121.85,102.72) .. (123.86,102.72) .. controls (125.87,102.72) and (127.5,104.35) .. (127.5,106.36) .. controls (127.5,108.37) and (125.87,110) .. (123.86,110) .. controls (121.85,110) and (120.22,108.37) .. (120.22,106.36) -- cycle ;
		%Shape: Circle [id:dp5384375730031219] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (138.97,106.36) .. controls (138.97,104.35) and (140.6,102.72) .. (142.61,102.72) .. controls (144.62,102.72) and (146.25,104.35) .. (146.25,106.36) .. controls (146.25,108.37) and (144.62,110) .. (142.61,110) .. controls (140.6,110) and (138.97,108.37) .. (138.97,106.36) -- cycle ;
		%Shape: Circle [id:dp08595180693509863] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (157.72,106.36) .. controls (157.72,104.35) and (159.35,102.72) .. (161.36,102.72) .. controls (163.37,102.72) and (165,104.35) .. (165,106.36) .. controls (165,108.37) and (163.37,110) .. (161.36,110) .. controls (159.35,110) and (157.72,108.37) .. (157.72,106.36) -- cycle ;
		
		%Shape: Circle [id:dp44825606430826626] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (73.72,119.08) .. controls (73.72,117.07) and (75.35,115.44) .. (77.36,115.44) .. controls (79.37,115.44) and (81,117.07) .. (81,119.08) .. controls (81,121.09) and (79.37,122.72) .. (77.36,122.72) .. controls (75.35,122.72) and (73.72,121.09) .. (73.72,119.08) -- cycle ;
		%Shape: Circle [id:dp6117767572598438] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (92.47,119.08) .. controls (92.47,117.07) and (94.1,115.44) .. (96.11,115.44) .. controls (98.12,115.44) and (99.75,117.07) .. (99.75,119.08) .. controls (99.75,121.09) and (98.12,122.72) .. (96.11,122.72) .. controls (94.1,122.72) and (92.47,121.09) .. (92.47,119.08) -- cycle ;
		%Shape: Circle [id:dp5337820532586892] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (111.22,119.08) .. controls (111.22,117.07) and (112.85,115.44) .. (114.86,115.44) .. controls (116.87,115.44) and (118.5,117.07) .. (118.5,119.08) .. controls (118.5,121.09) and (116.87,122.72) .. (114.86,122.72) .. controls (112.85,122.72) and (111.22,121.09) .. (111.22,119.08) -- cycle ;
		%Shape: Circle [id:dp7864457207835862] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (129.97,119.08) .. controls (129.97,117.07) and (131.6,115.44) .. (133.61,115.44) .. controls (135.62,115.44) and (137.25,117.07) .. (137.25,119.08) .. controls (137.25,121.09) and (135.62,122.72) .. (133.61,122.72) .. controls (131.6,122.72) and (129.97,121.09) .. (129.97,119.08) -- cycle ;
		%Shape: Circle [id:dp06455580213499768] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (148.72,119.08) .. controls (148.72,117.07) and (150.35,115.44) .. (152.36,115.44) .. controls (154.37,115.44) and (156,117.07) .. (156,119.08) .. controls (156,121.09) and (154.37,122.72) .. (152.36,122.72) .. controls (150.35,122.72) and (148.72,121.09) .. (148.72,119.08) -- cycle ;
		
		%Shape: Circle [id:dp135073247938293] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (82.72,131.79) .. controls (82.72,129.78) and (84.35,128.16) .. (86.36,128.16) .. controls (88.37,128.16) and (90,129.78) .. (90,131.79) .. controls (90,133.8) and (88.37,135.43) .. (86.36,135.43) .. controls (84.35,135.43) and (82.72,133.8) .. (82.72,131.79) -- cycle ;
		%Shape: Circle [id:dp4881867525759809] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (101.47,131.79) .. controls (101.47,129.78) and (103.1,128.16) .. (105.11,128.16) .. controls (107.12,128.16) and (108.75,129.78) .. (108.75,131.79) .. controls (108.75,133.8) and (107.12,135.43) .. (105.11,135.43) .. controls (103.1,135.43) and (101.47,133.8) .. (101.47,131.79) -- cycle ;
		%Shape: Circle [id:dp14408772930464608] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (120.22,131.79) .. controls (120.22,129.78) and (121.85,128.16) .. (123.86,128.16) .. controls (125.87,128.16) and (127.5,129.78) .. (127.5,131.79) .. controls (127.5,133.8) and (125.87,135.43) .. (123.86,135.43) .. controls (121.85,135.43) and (120.22,133.8) .. (120.22,131.79) -- cycle ;
		%Shape: Circle [id:dp2788478282629847] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (138.97,131.79) .. controls (138.97,129.78) and (140.6,128.16) .. (142.61,128.16) .. controls (144.62,128.16) and (146.25,129.78) .. (146.25,131.79) .. controls (146.25,133.8) and (144.62,135.43) .. (142.61,135.43) .. controls (140.6,135.43) and (138.97,133.8) .. (138.97,131.79) -- cycle ;
		%Shape: Circle [id:dp5077649217504134] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (157.72,131.79) .. controls (157.72,129.78) and (159.35,128.16) .. (161.36,128.16) .. controls (163.37,128.16) and (165,129.78) .. (165,131.79) .. controls (165,133.8) and (163.37,135.43) .. (161.36,135.43) .. controls (159.35,135.43) and (157.72,133.8) .. (157.72,131.79) -- cycle ;
		
		%Shape: Circle [id:dp9696319115024321] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (73.72,144.51) .. controls (73.72,142.5) and (75.35,140.87) .. (77.36,140.87) .. controls (79.37,140.87) and (81,142.5) .. (81,144.51) .. controls (81,146.52) and (79.37,148.15) .. (77.36,148.15) .. controls (75.35,148.15) and (73.72,146.52) .. (73.72,144.51) -- cycle ;
		%Shape: Circle [id:dp5681123817717466] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (92.47,144.51) .. controls (92.47,142.5) and (94.1,140.87) .. (96.11,140.87) .. controls (98.12,140.87) and (99.75,142.5) .. (99.75,144.51) .. controls (99.75,146.52) and (98.12,148.15) .. (96.11,148.15) .. controls (94.1,148.15) and (92.47,146.52) .. (92.47,144.51) -- cycle ;
		%Shape: Circle [id:dp37628484191757194] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (111.22,144.51) .. controls (111.22,142.5) and (112.85,140.87) .. (114.86,140.87) .. controls (116.87,140.87) and (118.5,142.5) .. (118.5,144.51) .. controls (118.5,146.52) and (116.87,148.15) .. (114.86,148.15) .. controls (112.85,148.15) and (111.22,146.52) .. (111.22,144.51) -- cycle ;
		%Shape: Circle [id:dp6465976632842974] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (129.97,144.51) .. controls (129.97,142.5) and (131.6,140.87) .. (133.61,140.87) .. controls (135.62,140.87) and (137.25,142.5) .. (137.25,144.51) .. controls (137.25,146.52) and (135.62,148.15) .. (133.61,148.15) .. controls (131.6,148.15) and (129.97,146.52) .. (129.97,144.51) -- cycle ;
		%Shape: Circle [id:dp0696204695880156] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (148.72,144.51) .. controls (148.72,142.5) and (150.35,140.87) .. (152.36,140.87) .. controls (154.37,140.87) and (156,142.5) .. (156,144.51) .. controls (156,146.52) and (154.37,148.15) .. (152.36,148.15) .. controls (150.35,148.15) and (148.72,146.52) .. (148.72,144.51) -- cycle ;
		
		%Shape: Circle [id:dp2663817632417549] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (81.72,157.23) .. controls (81.72,155.22) and (83.35,153.59) .. (85.36,153.59) .. controls (87.37,153.59) and (89,155.22) .. (89,157.23) .. controls (89,159.24) and (87.37,160.87) .. (85.36,160.87) .. controls (83.35,160.87) and (81.72,159.24) .. (81.72,157.23) -- cycle ;
		%Shape: Circle [id:dp8277411398516714] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (100.47,157.23) .. controls (100.47,155.22) and (102.1,153.59) .. (104.11,153.59) .. controls (106.12,153.59) and (107.75,155.22) .. (107.75,157.23) .. controls (107.75,159.24) and (106.12,160.87) .. (104.11,160.87) .. controls (102.1,160.87) and (100.47,159.24) .. (100.47,157.23) -- cycle ;
		%Shape: Circle [id:dp9847498442096916] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (119.22,157.23) .. controls (119.22,155.22) and (120.85,153.59) .. (122.86,153.59) .. controls (124.87,153.59) and (126.5,155.22) .. (126.5,157.23) .. controls (126.5,159.24) and (124.87,160.87) .. (122.86,160.87) .. controls (120.85,160.87) and (119.22,159.24) .. (119.22,157.23) -- cycle ;
		%Shape: Circle [id:dp03788508768143628] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (137.97,157.23) .. controls (137.97,155.22) and (139.6,153.59) .. (141.61,153.59) .. controls (143.62,153.59) and (145.25,155.22) .. (145.25,157.23) .. controls (145.25,159.24) and (143.62,160.87) .. (141.61,160.87) .. controls (139.6,160.87) and (137.97,159.24) .. (137.97,157.23) -- cycle ;
		%Shape: Circle [id:dp127054671372405] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (156.72,157.23) .. controls (156.72,155.22) and (158.35,153.59) .. (160.36,153.59) .. controls (162.37,153.59) and (164,155.22) .. (164,157.23) .. controls (164,159.24) and (162.37,160.87) .. (160.36,160.87) .. controls (158.35,160.87) and (156.72,159.24) .. (156.72,157.23) -- cycle ;
		
		%Shape: Circle [id:dp39036593245790097] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (73.72,169.95) .. controls (73.72,167.94) and (75.35,166.31) .. (77.36,166.31) .. controls (79.37,166.31) and (81,167.94) .. (81,169.95) .. controls (81,171.96) and (79.37,173.59) .. (77.36,173.59) .. controls (75.35,173.59) and (73.72,171.96) .. (73.72,169.95) -- cycle ;
		%Shape: Circle [id:dp5290920578868197] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (92.47,169.95) .. controls (92.47,167.94) and (94.1,166.31) .. (96.11,166.31) .. controls (98.12,166.31) and (99.75,167.94) .. (99.75,169.95) .. controls (99.75,171.96) and (98.12,173.59) .. (96.11,173.59) .. controls (94.1,173.59) and (92.47,171.96) .. (92.47,169.95) -- cycle ;
		%Shape: Circle [id:dp7120410589663413] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (111.22,169.95) .. controls (111.22,167.94) and (112.85,166.31) .. (114.86,166.31) .. controls (116.87,166.31) and (118.5,167.94) .. (118.5,169.95) .. controls (118.5,171.96) and (116.87,173.59) .. (114.86,173.59) .. controls (112.85,173.59) and (111.22,171.96) .. (111.22,169.95) -- cycle ;
		%Shape: Circle [id:dp8531447552912288] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (129.97,169.95) .. controls (129.97,167.94) and (131.6,166.31) .. (133.61,166.31) .. controls (135.62,166.31) and (137.25,167.94) .. (137.25,169.95) .. controls (137.25,171.96) and (135.62,173.59) .. (133.61,173.59) .. controls (131.6,173.59) and (129.97,171.96) .. (129.97,169.95) -- cycle ;
		%Shape: Circle [id:dp844481075182985] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (148.72,169.95) .. controls (148.72,167.94) and (150.35,166.31) .. (152.36,166.31) .. controls (154.37,166.31) and (156,167.94) .. (156,169.95) .. controls (156,171.96) and (154.37,173.59) .. (152.36,173.59) .. controls (150.35,173.59) and (148.72,171.96) .. (148.72,169.95) -- cycle ;
		
		%Shape: Circle [id:dp5807099044018316] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (81.72,182.64) .. controls (81.72,180.63) and (83.35,179) .. (85.36,179) .. controls (87.37,179) and (89,180.63) .. (89,182.64) .. controls (89,184.65) and (87.37,186.28) .. (85.36,186.28) .. controls (83.35,186.28) and (81.72,184.65) .. (81.72,182.64) -- cycle ;
		%Shape: Circle [id:dp6877169421908955] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (100.47,182.64) .. controls (100.47,180.63) and (102.1,179) .. (104.11,179) .. controls (106.12,179) and (107.75,180.63) .. (107.75,182.64) .. controls (107.75,184.65) and (106.12,186.28) .. (104.11,186.28) .. controls (102.1,186.28) and (100.47,184.65) .. (100.47,182.64) -- cycle ;
		%Shape: Circle [id:dp730832531903769] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (119.22,182.64) .. controls (119.22,180.63) and (120.85,179) .. (122.86,179) .. controls (124.87,179) and (126.5,180.63) .. (126.5,182.64) .. controls (126.5,184.65) and (124.87,186.28) .. (122.86,186.28) .. controls (120.85,186.28) and (119.22,184.65) .. (119.22,182.64) -- cycle ;
		%Shape: Circle [id:dp04336917401061546] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (137.97,182.64) .. controls (137.97,180.63) and (139.6,179) .. (141.61,179) .. controls (143.62,179) and (145.25,180.63) .. (145.25,182.64) .. controls (145.25,184.65) and (143.62,186.28) .. (141.61,186.28) .. controls (139.6,186.28) and (137.97,184.65) .. (137.97,182.64) -- cycle ;
		%Shape: Circle [id:dp951266677838303] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (156.72,182.64) .. controls (156.72,180.63) and (158.35,179) .. (160.36,179) .. controls (162.37,179) and (164,180.63) .. (164,182.64) .. controls (164,184.65) and (162.37,186.28) .. (160.36,186.28) .. controls (158.35,186.28) and (156.72,184.65) .. (156.72,182.64) -- cycle ;
		
		%Straight Lines [id:da4826164648365361] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (211.8,93.02) -- (226.6,93.02) ;
		%Straight Lines [id:da2514525089936621] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (202.2,94.62) -- (217,94.62) ;
		%Shape: Circle [id:dp47515609973965556] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (225.32,95.44) .. controls (225.32,93.43) and (226.95,91.8) .. (228.96,91.8) .. controls (230.97,91.8) and (232.6,93.43) .. (232.6,95.44) .. controls (232.6,97.45) and (230.97,99.08) .. (228.96,99.08) .. controls (226.95,99.08) and (225.32,97.45) .. (225.32,95.44) -- cycle ;
		%Straight Lines [id:da13270355389276278] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (199,96.62) -- (223.4,96.62) ;
		%Straight Lines [id:da19461100247487373] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (209.8,99.02) -- (224.6,99.02) ;
		
		%Straight Lines [id:da1451318036571052] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (246.8,98.82) -- (261.6,98.82) ;
		%Straight Lines [id:da5785775159587745] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (237.2,100.42) -- (252,100.42) ;
		%Shape: Circle [id:dp36532365379296317] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (260.32,101.24) .. controls (260.32,99.23) and (261.95,97.6) .. (263.96,97.6) .. controls (265.97,97.6) and (267.6,99.23) .. (267.6,101.24) .. controls (267.6,103.25) and (265.97,104.88) .. (263.96,104.88) .. controls (261.95,104.88) and (260.32,103.25) .. (260.32,101.24) -- cycle ;
		%Straight Lines [id:da1240973278705535] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (234,102.42) -- (258.4,102.42) ;
		%Straight Lines [id:da9259806495615381] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (244.8,104.82) -- (259.6,104.82) ;
		
		%Straight Lines [id:da5568952348667904] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (274.8,90.42) -- (283.4,90.42) ;
		%Straight Lines [id:da6333320771367383] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (267,93.02) -- (281.8,93.02) ;
		%Shape: Circle [id:dp17306745098280518] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (285.12,92.84) .. controls (285.12,90.83) and (286.75,89.2) .. (288.76,89.2) .. controls (290.77,89.2) and (292.4,90.83) .. (292.4,92.84) .. controls (292.4,94.85) and (290.77,96.48) .. (288.76,96.48) .. controls (286.75,96.48) and (285.12,94.85) .. (285.12,92.84) -- cycle ;
		%Straight Lines [id:da43027268363623206] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (275.6,95.22) -- (283,95.22) ;
		%Straight Lines [id:da8936467946884337] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (267.6,92.42) -- (276.6,92.42) ;
		
		%Straight Lines [id:da9235801877833816] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (278.4,106.42) -- (287,106.42) ;
		%Straight Lines [id:da08685325914396991] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (270.6,109.02) -- (285.4,109.02) ;
		%Shape: Circle [id:dp05690705252853978] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (288.72,108.84) .. controls (288.72,106.83) and (290.35,105.2) .. (292.36,105.2) .. controls (294.37,105.2) and (296,106.83) .. (296,108.84) .. controls (296,110.85) and (294.37,112.48) .. (292.36,112.48) .. controls (290.35,112.48) and (288.72,110.85) .. (288.72,108.84) -- cycle ;
		%Straight Lines [id:da36397627252329423] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (279.2,111.22) -- (286.6,111.22) ;
		%Straight Lines [id:da3982423796840546] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (271.2,108.42) -- (280.2,108.42) ;
		
		%Straight Lines [id:da81660700656655] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (213.4,113.42) -- (222,113.42) ;
		%Straight Lines [id:da42268406699688943] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (205.6,116.02) -- (220.4,116.02) ;
		%Shape: Circle [id:dp2901533802488434] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (223.72,115.84) .. controls (223.72,113.83) and (225.35,112.2) .. (227.36,112.2) .. controls (229.37,112.2) and (231,113.83) .. (231,115.84) .. controls (231,117.85) and (229.37,119.48) .. (227.36,119.48) .. controls (225.35,119.48) and (223.72,117.85) .. (223.72,115.84) -- cycle ;
		%Straight Lines [id:da6710452501355648] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (214.2,118.22) -- (221.6,118.22) ;
		%Straight Lines [id:da3161829344459155] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (206.2,115.42) -- (215.2,115.42) ;
		
		%Straight Lines [id:da6296078289746134] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (261.8,123.82) -- (276.6,123.82) ;
		%Straight Lines [id:da31779172805602784] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (252.2,125.42) -- (267,125.42) ;
		%Shape: Circle [id:dp2560082310784515] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (275.32,126.24) .. controls (275.32,124.23) and (276.95,122.6) .. (278.96,122.6) .. controls (280.97,122.6) and (282.6,124.23) .. (282.6,126.24) .. controls (282.6,128.25) and (280.97,129.88) .. (278.96,129.88) .. controls (276.95,129.88) and (275.32,128.25) .. (275.32,126.24) -- cycle ;
		%Straight Lines [id:da5724673878253015] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (249,127.42) -- (273.4,127.42) ;
		%Straight Lines [id:da48031171129176586] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (259.8,129.82) -- (274.6,129.82) ;
		
		%Straight Lines [id:da2872044032160741] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (226.8,134.42) -- (235.4,134.42) ;
		%Straight Lines [id:da08375556644374571] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (219,137.02) -- (233.8,137.02) ;
		%Shape: Circle [id:dp4734170612769706] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (237.12,136.84) .. controls (237.12,134.83) and (238.75,133.2) .. (240.76,133.2) .. controls (242.77,133.2) and (244.4,134.83) .. (244.4,136.84) .. controls (244.4,138.85) and (242.77,140.48) .. (240.76,140.48) .. controls (238.75,140.48) and (237.12,138.85) .. (237.12,136.84) -- cycle ;
		%Straight Lines [id:da017240811214034535] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (227.6,139.22) -- (235,139.22) ;
		%Straight Lines [id:da610221014095202] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (219.6,136.42) -- (228.6,136.42) ;
		
		%Straight Lines [id:da9904053168015228] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (217.8,147.82) -- (232.6,147.82) ;
		%Straight Lines [id:da037880619907905366] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (208.2,149.42) -- (223,149.42) ;
		%Shape: Circle [id:dp5005997960080057] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (231.32,150.24) .. controls (231.32,148.23) and (232.95,146.6) .. (234.96,146.6) .. controls (236.97,146.6) and (238.6,148.23) .. (238.6,150.24) .. controls (238.6,152.25) and (236.97,153.88) .. (234.96,153.88) .. controls (232.95,153.88) and (231.32,152.25) .. (231.32,150.24) -- cycle ;
		%Straight Lines [id:da9766119064711183] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (205,151.42) -- (229.4,151.42) ;
		%Straight Lines [id:da6734515234186242] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (215.8,153.82) -- (230.6,153.82) ;
		
		%Straight Lines [id:da6920611771402216] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (265.4,139.42) -- (274,139.42) ;
		%Straight Lines [id:da0875013310617696] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (257.6,142.02) -- (272.4,142.02) ;
		%Shape: Circle [id:dp17565028121669402] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (275.72,141.84) .. controls (275.72,139.83) and (277.35,138.2) .. (279.36,138.2) .. controls (281.37,138.2) and (283,139.83) .. (283,141.84) .. controls (283,143.85) and (281.37,145.48) .. (279.36,145.48) .. controls (277.35,145.48) and (275.72,143.85) .. (275.72,141.84) -- cycle ;
		%Straight Lines [id:da038080239287716866] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (266.2,144.22) -- (273.6,144.22) ;
		%Straight Lines [id:da40563836815854115] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (258.2,141.42) -- (267.2,141.42) ;
		
		%Straight Lines [id:da5135872961056611] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (256.4,153.42) -- (265,153.42) ;
		%Straight Lines [id:da6939227836230635] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (248.6,156.02) -- (263.4,156.02) ;
		%Shape: Circle [id:dp5713130983623629] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (266.72,155.84) .. controls (266.72,153.83) and (268.35,152.2) .. (270.36,152.2) .. controls (272.37,152.2) and (274,153.83) .. (274,155.84) .. controls (274,157.85) and (272.37,159.48) .. (270.36,159.48) .. controls (268.35,159.48) and (266.72,157.85) .. (266.72,155.84) -- cycle ;
		%Straight Lines [id:da3428448979425416] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (257.2,158.22) -- (264.6,158.22) ;
		%Straight Lines [id:da9331859552186839] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (249.2,155.42) -- (258.2,155.42) ;
		
		%Straight Lines [id:da009768426597732693] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (276.8,166.82) -- (291.6,166.82) ;
		%Straight Lines [id:da9075322811281381] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (267.2,168.42) -- (282,168.42) ;
		%Shape: Circle [id:dp35457035157250627] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (290.32,169.24) .. controls (290.32,167.23) and (291.95,165.6) .. (293.96,165.6) .. controls (295.97,165.6) and (297.6,167.23) .. (297.6,169.24) .. controls (297.6,171.25) and (295.97,172.88) .. (293.96,172.88) .. controls (291.95,172.88) and (290.32,171.25) .. (290.32,169.24) -- cycle ;
		%Straight Lines [id:da7284213346003781] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (264,170.42) -- (288.4,170.42) ;
		%Straight Lines [id:da9093908110833913] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (274.8,172.82) -- (289.6,172.82) ;
		
		%Straight Lines [id:da04807297699139412] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (242.8,179.82) -- (257.6,179.82) ;
		%Straight Lines [id:da9329559690549742] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (233.2,181.42) -- (248,181.42) ;
		%Shape: Circle [id:dp08572518425869102] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (256.32,182.24) .. controls (256.32,180.23) and (257.95,178.6) .. (259.96,178.6) .. controls (261.97,178.6) and (263.6,180.23) .. (263.6,182.24) .. controls (263.6,184.25) and (261.97,185.88) .. (259.96,185.88) .. controls (257.95,185.88) and (256.32,184.25) .. (256.32,182.24) -- cycle ;
		%Straight Lines [id:da313878854500266] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (230,183.42) -- (254.4,183.42) ;
		%Straight Lines [id:da17676811335109233] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (240.8,185.82) -- (255.6,185.82) ;
		
		%Straight Lines [id:da508610735232742] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (223.4,164.42) -- (232,164.42) ;
		%Straight Lines [id:da4311627502508122] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (215.6,167.02) -- (230.4,167.02) ;
		%Shape: Circle [id:dp8208217911704376] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (233.72,166.84) .. controls (233.72,164.83) and (235.35,163.2) .. (237.36,163.2) .. controls (239.37,163.2) and (241,164.83) .. (241,166.84) .. controls (241,168.85) and (239.37,170.48) .. (237.36,170.48) .. controls (235.35,170.48) and (233.72,168.85) .. (233.72,166.84) -- cycle ;
		%Straight Lines [id:da1708064786951824] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (224.2,169.22) -- (231.6,169.22) ;
		%Straight Lines [id:da5057486858574647] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (216.2,166.42) -- (225.2,166.42) ;
		
		%Straight Lines [id:da8433352670744587] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (209.4,180.42) -- (218,180.42) ;
		%Straight Lines [id:da44759421136088107] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (201.6,183.02) -- (216.4,183.02) ;
		%Shape: Circle [id:dp49415038337184725] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (219.72,182.84) .. controls (219.72,180.83) and (221.35,179.2) .. (223.36,179.2) .. controls (225.37,179.2) and (227,180.83) .. (227,182.84) .. controls (227,184.85) and (225.37,186.48) .. (223.36,186.48) .. controls (221.35,186.48) and (219.72,184.85) .. (219.72,182.84) -- cycle ;
		%Straight Lines [id:da6644675312848431] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (210.2,185.22) -- (217.6,185.22) ;
		%Straight Lines [id:da6733128394780039] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (202.2,182.42) -- (211.2,182.42) ;
		
		%Straight Lines [id:da9402316154530297] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (278.4,181.42) -- (287,181.42) ;
		%Straight Lines [id:da820715635547268] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (270.6,184.02) -- (285.4,184.02) ;
		%Shape: Circle [id:dp07517554685144456] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (288.72,183.84) .. controls (288.72,181.83) and (290.35,180.2) .. (292.36,180.2) .. controls (294.37,180.2) and (296,181.83) .. (296,183.84) .. controls (296,185.85) and (294.37,187.48) .. (292.36,187.48) .. controls (290.35,187.48) and (288.72,185.85) .. (288.72,183.84) -- cycle ;
		%Straight Lines [id:da6447470654577163] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (279.2,186.22) -- (286.6,186.22) ;
		%Straight Lines [id:da5663752736531067] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (271.2,183.42) -- (280.2,183.42) ;
		
		%Shape: Circle [id:dp9653653673826883] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 248; green, 231; blue, 28 }  ,fill opacity=1 ] (352.06,92.13) .. controls (352.06,89.38) and (354.28,87.16) .. (357.03,87.16) .. controls (359.77,87.16) and (362,89.38) .. (362,92.13) .. controls (362,94.87) and (359.77,97.1) .. (357.03,97.1) .. controls (354.28,97.1) and (352.06,94.87) .. (352.06,92.13) -- cycle ;
		%Shape: Circle [id:dp277480998958781] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 248; green, 231; blue, 28 }  ,fill opacity=1 ] (381.06,124.13) .. controls (381.06,121.38) and (383.28,119.16) .. (386.03,119.16) .. controls (388.77,119.16) and (391,121.38) .. (391,124.13) .. controls (391,126.87) and (388.77,129.1) .. (386.03,129.1) .. controls (383.28,129.1) and (381.06,126.87) .. (381.06,124.13) -- cycle ;
		%Shape: Circle [id:dp03203028301824018] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (370.06,103.13) .. controls (370.06,100.38) and (372.28,98.16) .. (375.03,98.16) .. controls (377.77,98.16) and (380,100.38) .. (380,103.13) .. controls (380,105.87) and (377.77,108.1) .. (375.03,108.1) .. controls (372.28,108.1) and (370.06,105.87) .. (370.06,103.13) -- cycle ;
		%Shape: Circle [id:dp5635737196983179] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 210; blue, 226 }  ,fill opacity=1 ] (392.06,105.13) .. controls (392.06,102.38) and (394.28,100.16) .. (397.03,100.16) .. controls (399.77,100.16) and (402,102.38) .. (402,105.13) .. controls (402,107.87) and (399.77,110.1) .. (397.03,110.1) .. controls (394.28,110.1) and (392.06,107.87) .. (392.06,105.13) -- cycle ;
		%Shape: Circle [id:dp024356078025129957] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (412.06,117.13) .. controls (412.06,114.38) and (414.28,112.16) .. (417.03,112.16) .. controls (419.77,112.16) and (422,114.38) .. (422,117.13) .. controls (422,119.87) and (419.77,122.1) .. (417.03,122.1) .. controls (414.28,122.1) and (412.06,119.87) .. (412.06,117.13) -- cycle ;
		%Shape: Circle [id:dp8639609995839614] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (353.06,113.13) .. controls (353.06,110.38) and (355.28,108.16) .. (358.03,108.16) .. controls (360.77,108.16) and (363,110.38) .. (363,113.13) .. controls (363,115.87) and (360.77,118.1) .. (358.03,118.1) .. controls (355.28,118.1) and (353.06,115.87) .. (353.06,113.13) -- cycle ;
		%Shape: Circle [id:dp9725841299267026] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (337.06,122.13) .. controls (337.06,119.38) and (339.28,117.16) .. (342.03,117.16) .. controls (344.77,117.16) and (347,119.38) .. (347,122.13) .. controls (347,124.87) and (344.77,127.1) .. (342.03,127.1) .. controls (339.28,127.1) and (337.06,124.87) .. (337.06,122.13) -- cycle ;
		%Shape: Circle [id:dp06298831729101106] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 203; green, 196; blue, 196 }  ,fill opacity=1 ] (363.06,129.13) .. controls (363.06,126.38) and (365.28,124.16) .. (368.03,124.16) .. controls (370.77,124.16) and (373,126.38) .. (373,129.13) .. controls (373,131.87) and (370.77,134.1) .. (368.03,134.1) .. controls (365.28,134.1) and (363.06,131.87) .. (363.06,129.13) -- cycle ;
		%Shape: Circle [id:dp7025801093068234] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (349.06,136.13) .. controls (349.06,133.38) and (351.28,131.16) .. (354.03,131.16) .. controls (356.77,131.16) and (359,133.38) .. (359,136.13) .. controls (359,138.87) and (356.77,141.1) .. (354.03,141.1) .. controls (351.28,141.1) and (349.06,138.87) .. (349.06,136.13) -- cycle ;
		%Shape: Circle [id:dp2951678116149026] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 203; green, 196; blue, 196 }  ,fill opacity=1 ] (324.06,147.13) .. controls (324.06,144.38) and (326.28,142.16) .. (329.03,142.16) .. controls (331.77,142.16) and (334,144.38) .. (334,147.13) .. controls (334,149.87) and (331.77,152.1) .. (329.03,152.1) .. controls (326.28,152.1) and (324.06,149.87) .. (324.06,147.13) -- cycle ;
		%Shape: Circle [id:dp6191534359946411] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (355.06,157.13) .. controls (355.06,154.38) and (357.28,152.16) .. (360.03,152.16) .. controls (362.77,152.16) and (365,154.38) .. (365,157.13) .. controls (365,159.87) and (362.77,162.1) .. (360.03,162.1) .. controls (357.28,162.1) and (355.06,159.87) .. (355.06,157.13) -- cycle ;
		%Shape: Circle [id:dp47591752602235604] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 75; green, 142; blue, 213 }  ,fill opacity=1 ] (337,140.13) .. controls (337,137.38) and (339.23,135.16) .. (341.97,135.16) .. controls (344.72,135.16) and (346.94,137.38) .. (346.94,140.13) .. controls (346.94,142.87) and (344.72,145.1) .. (341.97,145.1) .. controls (339.23,145.1) and (337,142.87) .. (337,140.13) -- cycle ;
		%Shape: Circle [id:dp1460443026338727] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (369.06,152.13) .. controls (369.06,149.38) and (371.28,147.16) .. (374.03,147.16) .. controls (376.77,147.16) and (379,149.38) .. (379,152.13) .. controls (379,154.87) and (376.77,157.1) .. (374.03,157.1) .. controls (371.28,157.1) and (369.06,154.87) .. (369.06,152.13) -- cycle ;
		%Shape: Circle [id:dp7636750710558717] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 80; green, 227; blue, 194 }  ,fill opacity=1 ] (336.06,163.13) .. controls (336.06,160.38) and (338.28,158.16) .. (341.03,158.16) .. controls (343.77,158.16) and (346,160.38) .. (346,163.13) .. controls (346,165.87) and (343.77,168.1) .. (341.03,168.1) .. controls (338.28,168.1) and (336.06,165.87) .. (336.06,163.13) -- cycle ;
		%Shape: Circle [id:dp8607085924900473] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (387.06,161.13) .. controls (387.06,158.38) and (389.28,156.16) .. (392.03,156.16) .. controls (394.77,156.16) and (397,158.38) .. (397,161.13) .. controls (397,163.87) and (394.77,166.1) .. (392.03,166.1) .. controls (389.28,166.1) and (387.06,163.87) .. (387.06,161.13) -- cycle ;
		%Shape: Circle [id:dp7905620149146337] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (397.06,141.13) .. controls (397.06,138.38) and (399.28,136.16) .. (402.03,136.16) .. controls (404.77,136.16) and (407,138.38) .. (407,141.13) .. controls (407,143.87) and (404.77,146.1) .. (402.03,146.1) .. controls (399.28,146.1) and (397.06,143.87) .. (397.06,141.13) -- cycle ;
		%Shape: Circle [id:dp6445669585270466] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (357.06,173.13) .. controls (357.06,170.38) and (359.28,168.16) .. (362.03,168.16) .. controls (364.77,168.16) and (367,170.38) .. (367,173.13) .. controls (367,175.87) and (364.77,178.1) .. (362.03,178.1) .. controls (359.28,178.1) and (357.06,175.87) .. (357.06,173.13) -- cycle ;
		%Shape: Circle [id:dp6401740727917904] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (334.06,187.13) .. controls (334.06,184.38) and (336.28,182.16) .. (339.03,182.16) .. controls (341.77,182.16) and (344,184.38) .. (344,187.13) .. controls (344,189.87) and (341.77,192.1) .. (339.03,192.1) .. controls (336.28,192.1) and (334.06,189.87) .. (334.06,187.13) -- cycle ;
		%Shape: Circle [id:dp72696812565524] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 75; green, 142; blue, 213 }  ,fill opacity=1 ] (374,182.13) .. controls (374,179.38) and (376.23,177.16) .. (378.97,177.16) .. controls (381.72,177.16) and (383.94,179.38) .. (383.94,182.13) .. controls (383.94,184.87) and (381.72,187.1) .. (378.97,187.1) .. controls (376.23,187.1) and (374,184.87) .. (374,182.13) -- cycle ;
		%Shape: Circle [id:dp9932584107888862] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 210; blue, 226 }  ,fill opacity=1 ] (393.06,179.13) .. controls (393.06,176.38) and (395.28,174.16) .. (398.03,174.16) .. controls (400.77,174.16) and (403,176.38) .. (403,179.13) .. controls (403,181.87) and (400.77,184.1) .. (398.03,184.1) .. controls (395.28,184.1) and (393.06,181.87) .. (393.06,179.13) -- cycle ;
		%Shape: Circle [id:dp905807688200593] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 203; green, 196; blue, 196 }  ,fill opacity=1 ] (385.06,196.13) .. controls (385.06,193.38) and (387.28,191.16) .. (390.03,191.16) .. controls (392.77,191.16) and (395,193.38) .. (395,196.13) .. controls (395,198.87) and (392.77,201.1) .. (390.03,201.1) .. controls (387.28,201.1) and (385.06,198.87) .. (385.06,196.13) -- cycle ;
		%Shape: Circle [id:dp5172023439728446] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 80; green, 227; blue, 194 }  ,fill opacity=1 ] (409.06,184.13) .. controls (409.06,181.38) and (411.28,179.16) .. (414.03,179.16) .. controls (416.77,179.16) and (419,181.38) .. (419,184.13) .. controls (419,186.87) and (416.77,189.1) .. (414.03,189.1) .. controls (411.28,189.1) and (409.06,186.87) .. (409.06,184.13) -- cycle ;
		%Shape: Circle [id:dp5891623731948574] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (408.06,162.13) .. controls (408.06,159.38) and (410.28,157.16) .. (413.03,157.16) .. controls (415.77,157.16) and (418,159.38) .. (418,162.13) .. controls (418,164.87) and (415.77,167.1) .. (413.03,167.1) .. controls (410.28,167.1) and (408.06,164.87) .. (408.06,162.13) -- cycle ;
		%Shape: Circle [id:dp4448029775054243] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (426.06,159.13) .. controls (426.06,156.38) and (428.28,154.16) .. (431.03,154.16) .. controls (433.77,154.16) and (436,156.38) .. (436,159.13) .. controls (436,161.87) and (433.77,164.1) .. (431.03,164.1) .. controls (428.28,164.1) and (426.06,161.87) .. (426.06,159.13) -- cycle ;
		%Shape: Circle [id:dp13242109512586908] 
		\draw  [draw opacity=0][shading=_1911hdlsl,_qeigkqps2] (486.9,106.55) .. controls (486.9,93.54) and (497.45,83) .. (510.45,83) .. controls (523.46,83) and (534,93.54) .. (534,106.55) .. controls (534,119.55) and (523.46,130.1) .. (510.45,130.1) .. controls (497.45,130.1) and (486.9,119.55) .. (486.9,106.55) -- cycle ;
		%Shape: Circle [id:dp14525629489542502] 
		\draw  [draw opacity=0][shading=_q9byl8127,_emxnt22fc] (470.9,87.55) .. controls (470.9,82.83) and (474.73,79) .. (479.45,79) .. controls (484.17,79) and (488,82.83) .. (488,87.55) .. controls (488,92.27) and (484.17,96.1) .. (479.45,96.1) .. controls (474.73,96.1) and (470.9,92.27) .. (470.9,87.55) -- cycle ;
		%Shape: Circle [id:dp3562714578690507] 
		\draw  [draw opacity=0][shading=_2608flvwz,_qqfpazku0] (456.9,111.55) .. controls (456.9,104.07) and (462.97,98) .. (470.45,98) .. controls (477.93,98) and (484,104.07) .. (484,111.55) .. controls (484,119.03) and (477.93,125.1) .. (470.45,125.1) .. controls (462.97,125.1) and (456.9,119.03) .. (456.9,111.55) -- cycle ;
		%Shape: Circle [id:dp9143283716721493] 
		\draw  [draw opacity=0][shading=_q7hg0oot3,_7bydj8qvz] (540.9,97.55) .. controls (540.9,93.93) and (543.84,91) .. (547.45,91) .. controls (551.07,91) and (554,93.93) .. (554,97.55) .. controls (554,101.16) and (551.07,104.1) .. (547.45,104.1) .. controls (543.84,104.1) and (540.9,101.16) .. (540.9,97.55) -- cycle ;
		%Shape: Circle [id:dp937903828907785] 
		\draw  [draw opacity=0][shading=_vxndtc1gv,_x5nb9nd2c] (536.9,120.05) .. controls (536.9,115.6) and (540.51,112) .. (544.95,112) .. controls (549.4,112) and (553,115.6) .. (553,120.05) .. controls (553,124.49) and (549.4,128.1) .. (544.95,128.1) .. controls (540.51,128.1) and (536.9,124.49) .. (536.9,120.05) -- cycle ;
		%Shape: Circle [id:dp22581059055509267] 
		\draw  [draw opacity=0][shading=_b9vf5xtgg,_r5knpdskb] (538.9,141.05) .. controls (538.9,134.39) and (544.3,129) .. (550.95,129) .. controls (557.61,129) and (563,134.39) .. (563,141.05) .. controls (563,147.7) and (557.61,153.1) .. (550.95,153.1) .. controls (544.3,153.1) and (538.9,147.7) .. (538.9,141.05) -- cycle ;
		%Shape: Circle [id:dp8071201075802168] 
		\draw  [draw opacity=0][shading=_t7myh7j85,_s09zw8gxa] (517.9,143.05) .. controls (517.9,139.16) and (521.06,136) .. (524.95,136) .. controls (528.84,136) and (532,139.16) .. (532,143.05) .. controls (532,146.94) and (528.84,150.1) .. (524.95,150.1) .. controls (521.06,150.1) and (517.9,146.94) .. (517.9,143.05) -- cycle ;
		%Shape: Circle [id:dp0012194611720854898] 
		\draw  [draw opacity=0][shading=_bwixx49iu,_8k0o2w4mt] (528.9,161.05) .. controls (528.9,157.16) and (532.06,154) .. (535.95,154) .. controls (539.84,154) and (543,157.16) .. (543,161.05) .. controls (543,164.94) and (539.84,168.1) .. (535.95,168.1) .. controls (532.06,168.1) and (528.9,164.94) .. (528.9,161.05) -- cycle ;
		%Shape: Circle [id:dp49459383603371543] 
		\draw  [draw opacity=0][shading=_qsw7d4w9r,_63i4wnz42] (483.9,137.55) .. controls (483.9,134.48) and (486.39,132) .. (489.45,132) .. controls (492.52,132) and (495,134.48) .. (495,137.55) .. controls (495,140.61) and (492.52,143.1) .. (489.45,143.1) .. controls (486.39,143.1) and (483.9,140.61) .. (483.9,137.55) -- cycle ;
		%Shape: Circle [id:dp13848976564313698] 
		\draw  [draw opacity=0][shading=_ao9016aew,_53o0er9ot] (463.9,145.05) .. controls (463.9,140.6) and (467.51,137) .. (471.95,137) .. controls (476.4,137) and (480,140.6) .. (480,145.05) .. controls (480,149.49) and (476.4,153.1) .. (471.95,153.1) .. controls (467.51,153.1) and (463.9,149.49) .. (463.9,145.05) -- cycle ;
		%Shape: Circle [id:dp552368190611241] 
		\draw  [draw opacity=0][shading=_r50ysld39,_oscnr0vn0] (469.9,172.05) .. controls (469.9,163.74) and (476.64,157) .. (484.95,157) .. controls (493.26,157) and (500,163.74) .. (500,172.05) .. controls (500,180.36) and (493.26,187.1) .. (484.95,187.1) .. controls (476.64,187.1) and (469.9,180.36) .. (469.9,172.05) -- cycle ;
		%Shape: Circle [id:dp06634514234557409] 
		\draw  [draw opacity=0][shading=_fymvlh0xo,_lycmv2vwb] (506.76,171.11) .. controls (506.76,166.14) and (510.78,162.12) .. (515.75,162.12) .. controls (520.71,162.12) and (524.74,166.14) .. (524.74,171.11) .. controls (524.74,176.07) and (520.71,180.1) .. (515.75,180.1) .. controls (510.78,180.1) and (506.76,176.07) .. (506.76,171.11) -- cycle ;
		
		% Text Node
		\draw (92,50) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,opacity=1 ] [align=left] {Volume};
		% Text Node
		\draw (76,212) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{0.49,0.83,0.13}{Data at Rest}}};
		% Text Node
		\draw (66,236) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize\linespread{0.8}\selectfont] [align=left] {\begin{minipage}[lt]{80pt}\setlength\topsep{0pt}
		\begin{center}
		Terabytes to\\exabytes of existing\\data to process
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (223,50) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,opacity=1 ] [align=left] {Velocity};
		% Text Node
		\draw (200,210) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{0.49,0.83,0.13}{Data in Motion}}};
		% Text Node
		\draw (197,236) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize\linespread{0.8}\selectfont] [align=left] {\begin{minipage}[lt]{80pt}\setlength\topsep{0pt}
		\begin{center}
		Streaming data\\milliseconds to\\seconds to response
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (361,49) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,opacity=1 ] [align=left] {Variety};
		% Text Node
		\draw (330,211) node [anchor=north west][inner sep=0.75pt] [font=\normalsize\linespread{0.8}\selectfont]  [align=left] {\begin{minipage}[lt]{80pt}\setlength\topsep{0pt}
		\textbf{\textcolor[rgb]{0.49,0.83,0.13}{Data in Many }}
		\begin{center}
		\textbf{\textcolor[rgb]{0.49,0.83,0.13}{Forms}}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (325,241) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize\linespread{0.8}\selectfont] [align=left] {\begin{minipage}[lt]{85pt}\setlength\topsep{0pt}
		\begin{center}
		Structured, \\Unstructured, Quasi-\\Structured, Semi-\\Structured
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (484,49) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,opacity=1 ] [align=left] {Veracity};
		% Text Node
		\draw (465,197) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{0.49,0.83,0.13}{Data in Doubt}}};
		% Text Node
		\draw (456,213) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize\linespread{0.8}\selectfont] [align=left] {\begin{minipage}[lt]{85pt}\setlength\topsep{0pt}
		\begin{center}
		Uncertainty due to\\data incosistency \&\\incompleteness,\\ambiguities, latency,\\deception, model\\approximations
		\end{center}
		\end{minipage}};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Data Mining/Machine Learning 4V's}
	\end{figure}
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} The data scientist must take care of not becoming a "Data Pusher" and not to do just "infotainement" (almost no-sense statistics) in order to remain employed. This is ethically non-scientific and as the reader of this book already knows it, as going at the opposite of the Archimedes Oath!\\
	
	\textbf{R2.} Corporations and managers have to understand that a data steward cannot be a data analyst, that a data analyst that cannot be a data scientist and that latter is also most of time not at the level of statistician. Think the opposite show an evident lack of technical and scientific knowledge from the management, especially when the data scientist is supposed alone to install Big Data servers, clean data, develop new mathematical models, do programming for implementing the models and put them in production.\\
	
	\textbf{R3.} Data is not a currency, most data is garbage. Actionable information is currency. Extracting information form modern big data sets requires the equivalent processing infrastructure and time of extracting a nugget of gold from a mountain of dirt.
	\end{tcolorbox}

	\begin{table}[H]
		\resizebox{\textwidth}{!}{
		\begin{tabular}{|l|l|l|}
		\hline
		\rowcolor[HTML]{C0C0C0} 
		\textbf{Type of Data} & \textbf{Description} & \textbf{Examples} \\ \hline
		\textbf{Unstructured} & Data that has no inherent structure & \begin{tabular}[c]{@{}l@{}}PDF, Text documents\\ (txt, doc, docx), Images,\\ Videos, Slideshows\end{tabular} \\ \hline
		\textbf{Quasi-structured} & \begin{tabular}[c]{@{}l@{}}Textual data with erratic formats that\\ can be formatted with effort and\\ software tools\end{tabular} & \begin{tabular}[c]{@{}l@{}}website pages, log files,\\ social networks post,\\ configuration files\end{tabular} \\ \hline
		\textbf{Semi-structured} & \begin{tabular}[c]{@{}l@{}}Textual data files with an apparent pattern\\ enabling analysis (but with some pain)\end{tabular} & \begin{tabular}[c]{@{}l@{}}spreadhsheet (xls, xlsx), \\ Comma separated value (csv), \\ JSON, XML, emails\end{tabular} \\ \hline
		\textbf{Structured} & \begin{tabular}[c]{@{}l@{}}Data having a defined data model, format,\\ structure,\end{tabular} & \begin{tabular}[c]{@{}l@{}}Parquet files, Relational \\ Databases (i.e. semantic model)\end{tabular} \\ \hline
		\end{tabular}
		}
		\caption{Types of data in Data Science}
	\end{table}
	
	Small, medium and international companies must not be surprised to be disappointed after having hired a Data Scientist that theoretically (on the paper...) should "master" various programming languages (R, Python, C++, Java), various tools (Microsoft Excel, Salesforce Tableau, Microsoft Power BI) data retrieval, data wrangling, data archiving, data governance, scientific methodology and advanced statistics as it is impossible for one person to master all these subjects at the same time (or even to have an undergraduate level at the same time of each of these topics)! Furthermore when considering the four types of data below where each one required very specific high level mathematical skills (at PhD level) to be able to correctly and efficiently deal with:
	
	So don't be surprised if by hiring only one person in your company that should do all the following jobs:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/computing/data_science_roles.jpg}
		\caption[Data science roles]{Data science roles (source: ?)}
	\end{figure}
	that you get garbage (unreliable) results without any strong scientific methodology.
	
	It should be noticed that business intelligence (BI) is quite different from Data Science as illustrated below:
	\begin{figure}[H]
		\centering
		% Gradient Info
		  
		\tikzset {_i1jakdzo6/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{-278 }  \pgftransformscale{2 }  }}}
		\pgfdeclarehorizontalshading{_qldegd16y}{150bp}{rgb(0bp)=(1,1,0);
		rgb(37.5bp)=(1,1,0);
		rgb(62.5bp)=(0,0.5,0.5);
		rgb(100bp)=(0,0.5,0.5)}
		
		% Gradient Info
		  
		\tikzset {_gfd41pe98/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{-121 }  \pgftransformscale{2 }  }}}
		\pgfdeclarehorizontalshading{_9r7cumy82}{150bp}{rgb(0bp)=(1,1,0);
		rgb(37.5bp)=(1,1,0);
		rgb(62.5bp)=(1,0,0);
		rgb(100bp)=(1,0,0)}
		
		% Gradient Info
		  
		\tikzset {_m1sbu9a62/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{-142 }  \pgftransformscale{2 }  }}}
		\pgfdeclarehorizontalshading{_1zbh08m29}{150bp}{rgb(0bp)=(1,1,1);
		rgb(37.5bp)=(1,1,1);
		rgb(62.5bp)=(0,0,0);
		rgb(100bp)=(0,0,0)}
		\tikzset{_ktch56qs1/.code = {\pgfsetadditionalshadetransform{\pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{-142 }  \pgftransformscale{2 } }}}
		\pgfdeclarehorizontalshading{_j9zq90b2n} {150bp} {color(0bp)=(transparent!0);
		color(37.5bp)=(transparent!0);
		color(62.5bp)=(transparent!10);
		color(100bp)=(transparent!10) } 
		\pgfdeclarefading{_4c2knk014}{\tikz \fill[shading=_j9zq90b2n,_ktch56qs1] (0,0) rectangle (50bp,50bp); } 
		\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        
		
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,712); %set diagram left start at 0, and has height of 712
		
		%Shape: Pie [id:dp23871820644225816] 
		\path  [shading=_qldegd16y,_i1jakdzo6] (30.65,105.73) .. controls (30.65,105.73) and (30.65,105.73) .. (30.65,105.73) .. controls (216.64,105.73) and (367.4,251.93) .. (367.4,432.28) -- (30.65,432.28) -- cycle ; % for fading 
		 \draw   (30.65,105.73) .. controls (30.65,105.73) and (30.65,105.73) .. (30.65,105.73) .. controls (216.64,105.73) and (367.4,251.93) .. (367.4,432.28) -- (30.65,432.28) -- cycle ; % for border 
		
		%Shape: Axis 2D [id:dp28257936062394373] 
		\draw  (12.3,432.59) -- (406.68,432.59)(30.65,81.19) -- (30.65,453.19) (399.68,427.59) -- (406.68,432.59) -- (399.68,437.59) (25.65,88.19) -- (30.65,81.19) -- (35.65,88.19)  ;
		%Shape: Pie [id:dp9377520156696793] 
		\path  [shading=_9r7cumy82,_gfd41pe98] (30.65,170.99) .. controls (30.65,170.99) and (30.65,170.99) .. (30.65,170.99) .. controls (179.47,170.99) and (300.11,287.97) .. (300.11,432.28) -- (30.65,432.28) -- cycle ; % for fading 
		 \draw   (30.65,170.99) .. controls (30.65,170.99) and (30.65,170.99) .. (30.65,170.99) .. controls (179.47,170.99) and (300.11,287.97) .. (300.11,432.28) -- (30.65,432.28) -- cycle ; % for border 
		
		%Shape: Pie [id:dp7261694431872061] 
		\path  [shading=_1zbh08m29,_m1sbu9a62,path fading= _4c2knk014 ,fading transform={xshift=2}] (30.65,261.43) .. controls (30.65,261.43) and (30.65,261.43) .. (30.65,261.43) .. controls (127.96,261.43) and (206.84,337.92) .. (206.84,432.28) -- (30.65,432.28) -- cycle ; % for fading 
		 \draw   (30.65,261.43) .. controls (30.65,261.43) and (30.65,261.43) .. (30.65,261.43) .. controls (127.96,261.43) and (206.84,337.92) .. (206.84,432.28) -- (30.65,432.28) -- cycle ; % for border 
		
		%Straight Lines [id:da8145817154248232] 
		\draw    (218.38,479.19) -- (173.51,479.11) -- (115.35,374.93) ;
		\draw [shift={(114.38,373.19)}, rotate = 60.82] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Rounded Rect [id:dp22158150043606661] 
		\draw   (218.38,479.19) .. controls (218.38,469.59) and (226.15,461.81) .. (235.75,461.81) -- (473,461.81) .. controls (482.6,461.81) and (490.38,469.59) .. (490.38,479.19) -- (490.38,589.71) .. controls (490.38,599.31) and (482.6,607.09) .. (473,607.09) -- (235.75,607.09) .. controls (226.15,607.09) and (218.38,599.31) .. (218.38,589.71) -- cycle ;
		%Straight Lines [id:da49348130575827653] 
		\draw    (218,486.91) -- (489.38,486.91) ;
		%Straight Lines [id:da27791886963503076] 
		\draw    (219,546.91) -- (490.38,546.91) ;
		%Straight Lines [id:da6082041820952799] 
		\draw    (274.38,487.19) -- (274.38,608.19) ;
		%Straight Lines [id:da6939584808405581] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ]   (376.38,253.19) -- (326.38,253.19) -- (223.34,271.83) ;
		\draw [shift={(221.38,272.19)}, rotate = 349.74] [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Rounded Rect [id:dp5705275294608576] 
		\draw  [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] (377,247.3) .. controls (377,235.22) and (386.79,225.44) .. (398.86,225.44) -- (616.14,225.44) .. controls (628.21,225.44) and (638,235.22) .. (638,247.3) -- (638,386.33) .. controls (638,398.4) and (628.21,408.19) .. (616.14,408.19) -- (398.86,408.19) .. controls (386.79,408.19) and (377,398.4) .. (377,386.33) -- cycle ;
		%Straight Lines [id:da38487743177681644] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ]   (377,245.91) -- (639.38,245.91) ;
		%Straight Lines [id:da4523697510735263] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ]   (378,330.91) -- (638.38,330.91) ;
		%Straight Lines [id:da5080831737002947] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ]   (433.38,246.19) -- (433.38,408.19) ;
		%Straight Lines [id:da039611832030854055] 
		\draw [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ]   (286.38,52.19) -- (267.38,52.19) -- (237.77,200.23) ;
		\draw [shift={(237.38,202.19)}, rotate = 281.31] [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Rounded Rect [id:dp6154744745916445] 
		\draw  [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ] (287,27.96) .. controls (287,15.87) and (296.81,6.06) .. (308.9,6.06) -- (539.1,6.06) .. controls (551.19,6.06) and (561,15.87) .. (561,27.96) -- (561,167.28) .. controls (561,179.38) and (551.19,189.19) .. (539.1,189.19) -- (308.9,189.19) .. controls (296.81,189.19) and (287,179.38) .. (287,167.28) -- cycle ;
		%Straight Lines [id:da5197522727058557] 
		\draw [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ]   (287,26.91) -- (560.38,26.91) ;
		%Straight Lines [id:da2951239734192286] 
		\draw [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ]   (288,99.91) -- (560.38,99.91) ;
		%Straight Lines [id:da4512643502187832] 
		\draw [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ]   (343.38,27.19) -- (343.38,188.19) ;
		
		% Text Node
		\draw (412,423) node [anchor=north west][inner sep=0.75pt]   [align=left] {Time};
		% Text Node
		\draw (15,60) node [anchor=north west][inner sep=0.75pt]   [align=left] {Exploratory};
		% Text Node
		\draw (34.38,435.28) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\scriptsize Past}};
		% Text Node
		\draw (371.38,441.28) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\scriptsize Future}};
		% Text Node
		\draw (65,348.91) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textcolor[rgb]{1,1,1}{\textbf{Reporting}}};
		% Text Node
		\draw (143,263.91) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textcolor[rgb]{1,1,1}{\textbf{Business }}\\\textcolor[rgb]{1,1,1}{\textbf{Intelligence}}};
		% Text Node
		\draw (226,214.91) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textcolor[rgb]{1,1,1}{\textbf{Data}}\\\textcolor[rgb]{1,1,1}{\textbf{Science}}};
		% Text Node
		\draw (310,465.91) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{Reporting}};
		% Text Node
		\draw (220,489.91) node [anchor=north west][inner sep=0.75pt] [font=\footnotesize\linespread{0.8}\selectfont]  [align=left] {\begin{minipage}[lt]{37.25pt}\setlength\topsep{0pt}
		\begin{center}
		{\scriptsize \textit{Typical}}\\{\scriptsize \textit{techniques}}\\{\scriptsize \textit{and}}\\{\scriptsize \textit{data types}}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (223,562.91) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{33.28pt}\setlength\topsep{0pt}
		\begin{center}
		{\scriptsize \textit{Common}}\\{\scriptsize \textit{questions}}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (276.38,490.19) node [anchor=north west][inner sep=0.75pt] [font=\footnotesize\linespread{0.8}\selectfont]  [align=left] {{\scriptsize * Punctual values (sums, averages,}\\{\scriptsize standard deviation, total year to date), KPIs...}};
		% Text Node
		\draw (276.38,520.19) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize\linespread{0.8}\selectfont] [align=left] {{\scriptsize * Structured and semi-structured data}\\{\scriptsize (spreadsheet, csv, databases, xml, ...)}};
		% Text Node
		\draw (276.38,550.69) node [anchor=north west][inner sep=0.75pt] [font=\footnotesize\linespread{0.8}\selectfont]  [align=left] {{\scriptsize * What happened last quart?}\\{\scriptsize * How many units sold?}\\{\scriptsize * What are the problems?}\\{\scriptsize * Where do customers comes from?}};
		% Text Node
		\draw (434,226.91) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{0.96,0.65,0.14}{Business Intelligence}}};
		% Text Node
		\draw (379,248.91) node [anchor=north west][inner sep=0.75pt] [font=\footnotesize\linespread{0.8}\selectfont]  [align=left] {\begin{minipage}[lt]{37.25pt}\setlength\topsep{0pt}
		\begin{center}
		{\scriptsize \textit{Typical}}\\{\scriptsize \textit{techniques}}\\{\scriptsize \textit{and}}\\{\scriptsize \textit{data types}}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (380,341.91) node [anchor=north west][inner sep=0.75pt] [font=\footnotesize\linespread{0.8}\selectfont]  [align=left] {\begin{minipage}[lt]{33.28pt}\setlength\topsep{0pt}
		\begin{center}
		{\scriptsize \textit{Common}}\\{\scriptsize \textit{questions}}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (435.38,249.19) node [anchor=north west][inner sep=0.75pt] [font=\footnotesize\linespread{0.8}\selectfont]  [align=left] {{\scriptsize * Forecasting (linear, ETS, SARIMA),}\\{\scriptsize operational reasearch (simplex, conjugate }\\{\scriptsize gradients...), key influencers, dashobards, }\\{\scriptsize business scorecards, automated alerts, ...}};
		% Text Node
		\draw (434.38,303.19) node [anchor=north west][inner sep=0.75pt] [font=\footnotesize\linespread{0.8}\selectfont]  [align=left] {{\scriptsize * Structured, semi-structured, }\\{\scriptsize quasi-structured data (xlsx, db, csv, jpg,...)}};
		% Text Node
		\draw (435.38,336.69) node [anchor=north west][inner sep=0.75pt] [font=\footnotesize\linespread{0.8}\selectfont]  [align=left] {{\scriptsize * What if?}\\{\scriptsize * What will happen likely in the future ?}\\{\scriptsize * What are the influencers, the factors?}\\{\scriptsize * What are the probabilities of an event?}\\{\scriptsize * What are the correlations?}};
		% Text Node
		\draw (375,8.91) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize\linespread{0.8}\selectfont] [align=left] {\textbf{\textcolor[rgb]{0.49,0.83,0.13}{Data Science}}};
		% Text Node
		\draw (289,29.91) node [anchor=north west][inner sep=0.75pt] [font=\footnotesize\linespread{0.8}\selectfont]  [align=left] {\begin{minipage}[lt]{37.25pt}\setlength\topsep{0pt}
		\begin{center}
		{\scriptsize \textit{Typical}}\\{\scriptsize \textit{techniques}}\\{\scriptsize \textit{and}}\\{\scriptsize \textit{data types}}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (290,106.91) node [anchor=north west][inner sep=0.75pt] [font=\footnotesize\linespread{0.8}\selectfont]  [align=left] {\begin{minipage}[lt]{33.28pt}\setlength\topsep{0pt}
		\begin{center}
		{\scriptsize \textit{Common}}\\{\scriptsize \textit{questions}}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (345.38,30.19) node [anchor=north west][inner sep=0.75pt] [font=\footnotesize\linespread{0.8}\selectfont]  [align=left] {{\scriptsize * Clustering, classification, dimension}\\{\scriptsize reduction, neural networks forecasting, NLP,}\\{\scriptsize data imputation, NHST, Bayesian analysis, ...}};
		% Text Node
		\draw (345.38,72.19) node [anchor=north west][inner sep=0.75pt] [font=\footnotesize\linespread{0.8}\selectfont]  [align=left] {{\scriptsize * Structured, semi-structured, }\\{\scriptsize quasi-structured and unstructured data}};
		% Text Node
		\draw (345.38,104.69) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize\linespread{0.8}\selectfont] [align=left] {{\scriptsize * What's the optimal configuration/scenario?}\\{\scriptsize * What's the statistical significance?}\\{\scriptsize * What will happened next?}\\{\scriptsize * What is the probability range?}\\{\scriptsize * What is the similarity between?}\\{\scriptsize * To what group belongs something a priori?}};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Comparing Reporting and Business Intelligence with Data Science}
	\end{figure}
	Finally, as requested by a customer, here is also a list of visuals (charts) used in Business Intelligence (for a more complete list including high level scientists and engineers charts, see page \pageref{complete mosaic of charts type}):
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/graphic_continuum.jpg}
		\caption[Graphic Continuum]{Graphic Continuum (authors: Jon Schwabish, Severino Ribecca)}
	\end{figure}
	And keep in mind to avoid as Data Scientist, or... as a manager of a Data Scientist (!), to be careful with most common data fallacies (\SeeChapter{see section Statistics page \pageref{data fallacies}}) and especially with the fact that an engineer is by default not a scientist (same applies for physicians: they are by default  not scientists)!
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.4]{img/computing/path_to_data_science_adventure.jpg}
		\vspace*{3mm}
		\caption[]{source: \url{www.xkcd.com} (author: Randall Munroe)}
	\end{figure}
	
	\subsubsection{Training vs Test Data}
	First, to define a problem of machine learning, we have to decide what type of modelization function we want to consider.
	
	\textbf{Definition (\#\thesection.\mydef):} We name "\NewTerm{hypothesis space}\index{hypothesis space}", the set of functions $\mathcal{F} \subseteq \mathcal{Y}^{\mathcal{X}}$ describing the modelization functions that we will consider. This space is chosen depending on our convictions (return on experience) relatively to the problem we are faced to.

	We typically split the input data into learning and testing datasets. We then run the Machine Learning algorithm on the learning dataset to generate the prediction model. Later, we use the test dataset to evaluate our model.
	
	\textbf{Definition (\#\thesection.\mydef):} Given a dataset $\mathcal{D}=\left\{\left(\vec{x}^{i}, y^{i}\right)\right\}_{i=1, \ldots , n}$, partitioned into two sets $\mathcal{D}_{\text {tr }}$ and $\mathcal{D}_{\text {te }}$, we name "\NewTerm{training set}\index{training set}" the set $\mathcal{D}_{\text{tr}}$ used to train a predictive model, and test set the set $\mathcal{D}_{\text{te}}$ used for its evaluation.

	It is important that the test data is separate from the one used in training otherwise we will be kind of cheating because may for example the generated model memorizes the data and hence if the test data is also part of the training data then our evaluation scores of the model will be higher than they actually are.
	
	The data is usually split $75\%$ training and $25\%$ data or $2/3$ training and $1/3$ testing. It is important to note that: the smaller the training set the more challenging it is for the algorithm to discover the rules.
	
	In addition, when splitting the dataset, we need to maintaining class proportions and population statistics otherwise we will have some classes that are under represented in the training dataset and over represented in the test dataset.
	
	A crucial step when building our Machine Learning model is to estimate its performance on that the model hadn't seen before. We want to make sure that the model generalizes well to new unseen data.
	
	Now consider the situation where we want to choose between $K$ models. We can then train each of the models on the training dataset, thus obtaining $K$ decision functions $f_{1}, f_{2}, \ldots, f_{K}$, then calculate the error of each of these models on the test set. We can then choose as a model the one that has the smallest error on the test set:
	

	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/computing/overfitting.jpg}
		\caption{Underfitting and overfitting errors}
	\end{figure}
	In statistics and Machine Learning, the "\NewTerm{bias–variance tradeoff}\index{bias–variance tradeoff}" (or dilemma) is the problem of simultaneously minimizing two sources of error that prevent supervised learning algorithms from generalizing beyond their training set:
	 \begin{itemize}
	 	\item The bias is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).

		\item The variance is an error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting).
	\end{itemize}
	This tradeoff applies to all forms of supervised learning: classification, regression (function fitting), and structured output learning. It may has also been invoked to explain the effectiveness of heuristics in human learning.
	
	The bias-variance tradeoff is a central problem in supervised learning. Ideally, one wants to choose a model that both accurately capture the regularities in its training data, but also generalizes well to unseen data. Unfortunately, it is typically impossible to do both simultaneously. High-variance learning methods may be able to represent their training set well but are at risk of overfitting to noisy or unrepresentative training data. In contrast, algorithms with high bias typically produce simpler models that don't tend to overfit but may underfit their training data, failing to capture important regularities.
	
	Models with low bias are usually more complex (e.g. higher-order regression polynomials), enabling them to represent the training set more accurately. In the process, however, they may also represent a large noise component in the training set, making their predictions less accurate - despite their added complexity. In contrast, models with higher bias tend to be relatively simple (low-order or even linear regression polynomials) but may produce lower variance predictions when applied beyond the training set.
	
	Suppose that we have a training set consisting of a set of points $x_{1},\dots ,x_{n}$ and real values $y_{i}$ associated with each point $x_{i}$. We assume that there is a function with noise $y=f(x)+\varepsilon$, where the noise, $\varepsilon$, has zero mean and variance $\sigma^{2}_\varepsilon$.
	
	We want to find a function $\hat{f}$, that approximates the true function $f(x)$ as well as possible, by means of some learning algorithm. We make "as well as possible" precise by measuring the mean squared error between $y$ and $\hat{f}(x)$: we want $(y-{\hat {f}}(x))^{2}$ to be minimal, both for $x_{1},\dots ,x_{n}$ and for points outside of our sample. Of course, we cannot hope to do so perfectly, since the $y_{i}$ contain noise $\varepsilon$; this means we must be prepared to accept an irreducible error in any function we come up with.
	
	\begin{theorem}
	Finding an $\hat{f}$ that generalizes to points outside of the training set can be done with any of the countless algorithms used for supervised learning. It turns out that whichever function $\hat{f}$ we select, we can decompose its expected error on an unseen sample $x$ as follows:
	
	named "\NewTerm{bias-variance tradeoff}\index{bias-variance tradeoff}", where:
	
	and:
	
	\end{theorem}
	and where PE denotes the "\NewTerm{prediction error}\index{prediction error}" also sometimes named the "\NewTerm{expected loss}\index{expected loss}".
	\begin{dem}
	The derivation of the bias-variance decomposition for squared error proceeds as follows. For notational convenience, abbreviate $f = f(x)$ and $\hat{f}=\hat{f}(x)$. First, recall that, by definition, for any random variable $X$, we have:
	
	Rearranging, we get:
	
	If we assume a model where $f$ is deterministic:
	
	This, given $y=f+\varepsilon$ and $\text{E}(\varepsilon)=0$, implies:
	
	Also, since $\text{V}(\varepsilon)=\sigma_\varepsilon^2$:
	
	Thus, since $\varepsilon$ and $\hat{f}$ are independent, we can write:
	
	So we get finally the bias-variance tradeoff relation\label{bias-variance tradeoff}:
	
	That latter is sometimes denoted as following in some textbooks (the letter $D$ denotes the known data):
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	The trade-off between bias and variance is that:
	\begin{itemize}
		\item Simple Models: High Bias, Low Variance (over-fitting)
		\item Complex Models: Low Bias, High Variance (under-fitting)
	\end{itemize}
	
	We will derive later the bias and variance for the (vanilla) $k$ nearest neighbours clustering but also for the bagging and bootstrap of random forests (when we will introduce these topics!). Let us however see right now as an example how to derive the prediction error for the linear regression!
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Our goal\label{bias-variance tradeoff ols} is first to derive the bias of the ordinary least square regression:
	
	For that we assume obviously the true model to be:
	
	Therefore:
	
	Hence $\hat{\theta}_{\text{MLE}}$ is unbiased! Let's compute the variance now:
	
	\end{tcolorbox}
	With the above example we see therefore that:
	
	
	\newcommand{\target}[1]{%
	  \foreach \r in {2.5, 2, 1.5, 1, 0.5, 0.05} {
	    \draw [
	      fill = black,
	      fill opacity = 0.05
	    ] (0, 0) circle (\r cm);
	  }
	}%
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[
		  every path/.style = {draw, > = stealth'},
		  nodetext/.style = {
		    draw, rounded corners = 2pt, fill = white,
		  },
		  labeltext/.style = {
		    draw, minimum width = 5.25cm, minimum height = 1.5em, fill = gray!20,
		  },scale=0.8]
		
		\def\dist{5.5};
		
		\begin{scope}[shift = {(0, 0)}, scale = 1.25]
		
		  \target
		
		  \foreach \x/\y in {0.825/1.414, 1.827/0.407, 1.068/0.855, 2.016/0.645, 2.131/-0.294, 0.341/1.3, 1.306/-0.008, 2.035/-0.416, 1.353/0.156, 1.163/1.409} {
		    \draw[fill = orange] (\x, \y) circle (2pt);
		  }
		
		  \node[text = cyan] (mean) at (1.4065, 0.5468) {$\times$};
		  \node[text = magenta] (theta) at (0, 0) {$\times$};
		
		  \path[->] (mean) to[out = 90, in = 180] ++(1, 1) node[nodetext, right] {$\text{E}(\hat{\theta)}$};
		  \path[->] (theta) to[out = 90, in = 0] ++(-1, 1) node[nodetext, left] {$\theta$};
		  \path[->] (1.163, 1.409) to[out = 90, in = 180] ++(1, 1) node[nodetext, right] {$\hat{\theta}_i$};
		
		  \node[nodetext] at (0, -3) {$\textcolor{magenta}{\text{PE}(\hat{\theta})} = \textcolor{cyan}{\text{V}(\hat{\theta})} - \textcolor{red}{\text{Bias}(\hat{\theta})}^2$};
		
		\end{scope}
		
		\begin{scope}[shift = {(7, 0)}]
		
		  \target
		
		  \foreach \x/\y in {0.825/1.414, 1.827/0.407, 1.068/0.855, 2.016/0.645, 2.131/-0.294, 0.341/1.3, 1.306/-0.008, 2.035/-0.416, 1.353/0.156, 1.163/1.409} {
		    \draw[fill = orange] (\x, \y) circle (2pt);
		    \draw[dashed, magenta] (\x, \y) -- (0, 0);
		  }
		
		  \node[nodetext] at (0, -1) {$\textcolor{magenta}{\text{PE}(\hat{\theta})} = 1/n\sum (\hat{\theta}_i - \theta)^2$};
		
		  \node[text = cyan] at (1.4065, 0.5468) {$\times$};
		  \node[text = magenta] at (0, 0) {$\times$};
		
		\end{scope}
		
		\begin{scope}[shift = {(12, 2.75)}]
		
		  \target
		
		  \foreach \x/\y in {0.825/1.414, 1.827/0.407, 1.068/0.855, 2.016/0.645, 2.131/-0.294, 0.341/1.3, 1.306/-0.008, 2.035/-0.416, 1.353/0.156, 1.163/1.409} {
		    \draw[fill = orange] (\x, \y) circle (2pt);
		    \draw[dashed, cyan] (\x, \y) -- (1.4065, 0.5468);
		  }
		
		  \node[nodetext] at (0, -1) {$\textcolor{cyan}{\text{V}(\hat{\theta})} = 1/n\sum (\hat{\theta}_i - \text{E}(\hat{\theta}))^2$};
		
		  \node[text = cyan] at (1.4065, 0.5468) {$\times$};
		  \node[text = magenta] at (0, 0) {$\times$};
		
		\end{scope}
		
		\begin{scope}[shift = {(12, -2.75)}]
		
		  \target
		
		  \foreach \x/\y in {0.825/1.414, 1.827/0.407, 1.068/0.855, 2.016/0.645, 2.131/-0.294, 0.341/1.3, 1.306/-0.008, 2.035/-0.416, 1.353/0.156, 1.163/1.409} {
		    \draw[fill = orange] (\x, \y) circle (2pt);
		  }
		
		  \node[nodetext] at (0, -1) {$\textcolor{red}{\text{Bias}(\hat{\theta})} = \text{E}(\hat{\theta}) - \theta$};
		
		  \node[text = cyan] at (1.4065, 0.5468) {$\times$};
		  \node[text = magenta] at (0, 0) {$\times$};
		
		  \draw[dashed, red] (0, 0) -- (1.4065, 0.5468);
		
		\end{scope}
		
		\end{tikzpicture}
	\end{figure}
	
	One case, the Machine Learning algorithm has different parameters and we want to tune these parameters to achieve the best performance (the parameters of the Machine Learning algorithm are named "\NewTerm{hyperparameters}" as we will see further below\footnote{The double problem in Data Mining to find the best algorithm + its better hyperparameters is named the "CASH problem", where CASH stands for "Combined Algorithm Selection and Hyperparameters"}). Another case, sometimes we want to try out different algorithms and choose the best performing one. Here is a typical technique:
	
	We simply split the data into training and testing datasets. Then, the training data is further split into training and validation sets.
	
	The training data is used to train different models. Then the validation data is used to compute performance of each of them and we select the best one. Finally, the model is then used for the test set to evaluate performance. The next figure illustrates this idea:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.9]{img/computing/machine_learning_validation.jpg}
		\caption{Holdout Validation in Machine learning}
	\end{figure}
	However, because we use the validation set multiple times, Holdout validation is sensitive to how we partition the data and that is what $K$-fold cross validation tries to solve.
	
	Initially, we split the data into training and testing dataset. Furthermore, the training dataset is split into $K$ chunks.

	Suppose we will use 5-fold cross validation, the training data set is split into 5 chunks and the training phase will take place over 5 iterations. In each iteration we use one chunk as the validation dataset while the rest of the chunk are grouped together to form the training dataset.

	This is very similar to Holdout validation except in each iteration the validation data is different and this removed the bias. Each iteration generates a score and the final score is the average score of all iteration. As before we select the best model and use the test data for the final performance evaluation.
	
	It is interesting to know that this practice of splitting data seems to come first form an article of David Freedman \cite{freedman1983note} published in 11983 according to holocene calendar (hence sometimes the name "\NewTerm{Freedman paradox}"). Freedman shows in an impressive way the dangers of data reuse in statistical analysis. The potentially dangerous scenarios include those where the results of one statistical procedure performed on the data are fed into another procedure performed on the same data. As a concrete example Freedman considers the practice of performing variable selection first, and then fitting another model using only the identified variables on the same data that was used to identify them in the first place. 
	
	The figure below shows the distributions of the considered model statistics (variable selection and afterwards a regression) by David Freedman across the $1000$ repetitions for model fits with and without data reuse (the code producing this figure is given at the bottom of this post):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/computing/freedman_paradox.jpg}
		\caption{Freedman paradox illustration}
	\end{figure}
	Well, the $R$ squared statistic shows a moderate change between models with or without data reuse (average of $0.3093018$ vs. $0.5001641$). The $F$ test statistic however grows immensely to an average of $3.2806118$ (from $1.0480097$), and the $p$-values fall after data reuse to an average of $0.0112216$ (from $0.5017696$), below the widely used (but arbitrary) $5\%$ significance level.

	Obviously the model with data reuse is highly misleading here, because in fact there are absolutely no relationships between the predictor variables and the response (as per the data generation procedure).
	
	OK! This done let us now begin first with the basics of clustering methods! 
	
	\pagebreak
	\subsubsection{Hyperparameters vs Parameters}
	You may have heard of terms like "\NewTerm{hyperparameter}\index{hyperparameter}" search, autotuning (which is just a shorter way of saying hyperparameter search), or grid search (a possible method for hyperparameter search). Where do those terms fit in? To understand hyperparameter search, we have to talk about the difference between a model parameter and a hyperparameter. In brief, model parameters are the knobs that the training algorithm knows how to tweak; they are learned from data. Hyperparameters, on the other hand, are not learned by the training method, but they also need to be tuned.
	
	A famous example are neural networks (see further below) where weights and bias are autotuned parameters, but the number of layers, the type of activation function of the bias values are hyperparameters:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.63]{img/computing/hyperparameters.jpg}
		\vspace*{3mm}
		\caption{Parameters vs Hyperparameters}
	\end{figure}
	"\NewTerm{Bagging}\index{bagging}" and "\NewTerm{Boosting}\index{boosting}" are both ensemble methods in Machine Learning that also use hyperparameters (number of iterations/trees, sampling with or without replacement, minimum leaf size, minim parent size, maximum number of decision splits, learning rate for shrinkage, etc).
	
	Bagging and Boosting are similar in that they are both ensemble techniques, where a set of weak learners are combined to create a strong learner that obtains better performance than a single one. So, let's start from the beginning\footnote{The whole text introducing the difference between bagging and boosting in this book is taken form the QuantDare blog article \url{https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/} written by Ana Porras Garrido who works for the company ETS Asset Management Factory (\url{https://www.etsfactory.com/})}.
	
	"\NewTerm{Ensemble learning}\index{ensemble learning}" is a Machine Learning concept in which the idea is to train multiple models using the same learning algorithm. The ensembles take part in a bigger group of methods, named "\NewTerm{multiclassifiers}\index{multiclassifiers}", where a set of hundreds or thousands of learners with a common objective are fused together to solve the problem.

	The second group of multiclassifiers contain the "\NewTerm{hybrid methods}\index{hybrid methods}
". They use a set of learners too, but they can be trained using different learning techniques. 

	The main causes of error in learning are due to noise, bias and variance. Ensemble helps to minimize these factors. These methods are designed to improve the stability and the accuracy of Machine Learning algorithms. Combinations of multiple classifiers decrease variance, especially in the case of unstable classifiers, and may produce a more reliable classification than a single classifier.

	To use Bagging or Boosting you must select a base learner algorithm. For example, if we choose a classification tree, Bagging and Boosting would consist of a pool of trees as big as we want. 
	
	Bagging and Boosting get $N$ learners by generating additional data in the training stage. $N$ new training data sets are produced by random sampling with replacement from the original set. By sampling with replacement some observations may be repeated in each new training data set.
	
	In the case of Bagging, any element has the same probability to appear in a new data set. However, for Boosting the observations are weighted and therefore some of them will take part in the new sets more often: 
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/bagging_and_boosting_01.jpg}
	\end{figure}
	These multiple sets are used to train the same learner algorithm and therefore different classifiers are produced.
	
	At this point, we begin to deal with the main difference between the two methods. While the training stage is parallel for Bagging (i.e., each model is built independently), Boosting builds the new learner in a sequential way:
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/bagging_and_boosting_02.jpg}
	\end{figure}
	To predict the class of new data we only need to apply the $N$ learners to the new observations. In Bagging the result is obtained by averaging the responses of the $N$ learners (or majority vote). However, Boosting assigns a second set of weights, this time for the $N$ classifiers, in order to take a weighted average of their estimates.
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/bagging_and_boosting_03.jpg}
	\end{figure}
	In the Boosting training stage, the algorithm allocates weights to each resulting model. A learner with good a classification result on the training data will be assigned a higher weight than a poor one. So when evaluating a new learner, Boosting needs to keep track of learners’ errors, too. Let's see the differences in the procedures:
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/bagging_and_boosting_04.jpg}
	\end{figure}
	Some of the Boosting techniques include an extra-condition (hyperparamater) to keep or discard a single learner. For example, in AdaBoost, the most renowned, an error less than $50\%$ is required to maintain the model; otherwise, the iteration is repeated until achieving a learner better than a random guess.

	The previous image shows the general process of a Boosting method, but several alternatives exist with different ways to determine the weights to use in the next training step and in the classification stage (AdaBoost, LPBoost, XGBoost, GradientBoost, BrownBoost).
	
	We may ask ourselves what is the best, Bagging or Boosting?
	
	There's not an outright winner; it depends on the data, the simulation and the circumstances. Bagging and Boosting decrease the variance of your single estimate as they combine several estimates from different models. So the result may be a model with higher stability.
	
	Indeed, let us recall that the bias-variance tradeoff relation that we have derived at page \pageref{bias-variance tradeoff}:
	
	As bagging consist to average the different models, we know that averaging reduces the variance according:
	
	So bagging $B$ times leads to:
	
	As we know decreasing the variance therefore increase the bias (as the sum of the both must always be equal). In practice the bagging models are correlated, and as we know it, correlation makes variance even smaller!

	If the problem is that the single model gets a very low performance, Bagging will rarely get a better bias. However, Boosting could generate a combined model with lower errors as it optimises the advantages and reduces pitfalls of the single model.

	By contrast, if the difficulty of the single model is over-fitting, then Bagging is the best option. Boosting for its part doesn't help to avoid over-fitting; in fact, this technique is faced with this problem itself. For this reason, Bagging is effective more often than Boosting.
	
	Here is a very nice summary of the most three common ensemble learning method in one picture:
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/ensemble_learning.pdf}
		\caption[Three most common ensemble learning methods]{Three most common ensemble learning methods\\ (source: \url{https://www.cheatsheets.aqeel-anwar.com}, author: Aqeel Anwar)}
	\end{figure}
	
	\pagebreak
	\subsubsection{Association Rules}
	A classic story tells about the increase in beer sales after a store decided to place the beer next to the diapers. What is the relationship between baby diapers and beers? We would be wondering...
	
	Several explanations follow to understand this observation, but that's about the purchases made by the male parents. The latter, when they go to take diapers profit to buy a few beers. This little anecdote actually reveals the existence of a rule of association between these two products which at first sight have nothing in common.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/computing/diapers_beer.jpg}
	\end{figure}
	Association rule learning is a unsupervised rule-based Machine Learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness.
	
	 For example, the rule  ${\displaystyle \{\mathrm {diapeer} \}\Rightarrow \{\mathrm {beer} \}}$ or ${\displaystyle \{\mathrm {onions,potatoes} \}\Rightarrow \{\mathrm {burger} \}}$  found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as, e.g., promotional pricing or product placements. In addition to the above example from "market basket analysis\index{market basket analysis}" association rules are employed today in many application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.
	
	Here are some possible questions that association rules can answer:
	\begin{itemize}
		\item Which products tend to be purchased together?
		\item Of those customers who are similar to this person, what products do they tend to buy?
		\item Of those customers who have purchased this product, what other similar products do they tend to
		view or purchase?
	\end{itemize}
	Some well-known algorithms are OneR (OneRule), Apriori, ECLAT (Equivalence Class Transformation) and FP-Growth (first pass), but they only do half the job, since they are algorithms for mining frequent itemsets. Another step needs to be done after to generate rules from frequent itemsets found in a database.
	
	\paragraph{ZeroR}\mbox{}\\\\
	"\NewTerm{ZeroR}\index{ZeroR}", short for "Zero Rule", is the simplest supervised classification method which relies on the target and ignores all predictors. ZeroR classifier simply predicts the majority category (class). Although there is no predictability power in ZeroR, it is useful for determining a baseline performance as a benchmark for other classification methods.
	
	A nice story about ZeroR algorithm is that we use it unconsciously everyday when our brain chooses the most frequent seen item as the most probable event (ZeroR classification) or when we talk about averages (ZeroR Regression).
	
	The ZeroR algorithm is (it's not a joke!): Construct a frequency table for the target and select its most frequent value.
	
	The "\textit{Play Golf} $=$ \textit{Yes}" in the example below\footnote{Thanks to Dr. Saed Sayad for having authorized us to reproduce its web page: \url{http://www.saedsayad.com/zeror.htm}} is the ZeroR rule for the learning table ($14$ rows) with an accuracy of $9/14\cong 0.64$.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/computing/zeror.jpg}
		\caption{ZeroR Data Mining (association rule) example}
	\end{figure}
	There is nothing to be said about the predictors contribution to the model because ZeroR does not use any of them.
	
	The following confusion matrix $\mathcal{C}$ (more complete than the one we have introduce during our study of the binomial logistic regression):
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|c|c|l|l|}
		\hline
		\multicolumn{2}{|l|}{\cellcolor[HTML]{FFFFFF}} & \multicolumn{2}{c|}{\cellcolor[gray]{0.75}\textbf{Play Golf}} & \multicolumn{2}{l|}{} \\ \cline{3-4}
		\multicolumn{2}{|l|}{\multirow{-2}{*}{\cellcolor[HTML]{FFFFFF}Confusion matrix}} & \cellcolor[gray]{0.75}Yes & \cellcolor[gray]{0.75}No & \multicolumn{2}{l|}{\multirow{-2}{*}{}} \\ \hline
		\cellcolor[gray]{0.75} & \cellcolor[gray]{0.75}Yes & $a$ & $b$ & \cellcolor[HTML]{FFFFC7} Positive Predictive Value & \cellcolor[HTML]{FFFFC7}$\dfrac{a}{a+b}$ \\ \cline{2-6} 
		\multirow{-2}{*}{\cellcolor[gray]{0.75}\textbf{ZeroR}} & \cellcolor[gray]{0.75}No & $c$ & $d$ & \cellcolor[HTML]{FFFFC7} Negative Predictive Value & \cellcolor[HTML]{FFFFC7}$\dfrac{d}{c+d}$ \\ \hline
		\multicolumn{2}{|l|}{} & \cellcolor[HTML]{FFFFC7} Sensitivity & \cellcolor[HTML]{FFFFC7} Specificity & \multicolumn{2}{l|}{} \\ \cline{3-4}
		\multicolumn{2}{|l|}{\multirow{-2}{*}{}} & \cellcolor[HTML]{FFFFC7}$\dfrac{a}{a+c}$ & \cellcolor[HTML]{FFFFC7}$\dfrac{d}{b+d}$ & \multicolumn{2}{l|}{\multirow{-2}{*}{\textbf{Accuracy =}\index{accuracy}$\dfrac{a+d}{a+b+c+d}$}} \\ \hline
		\end{tabular}
		\caption{Full Confusion matrix example}
	\end{table}
	gives in the case of our example:
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|c|c|l|l|}
		\hline
		\multicolumn{2}{|l|}{\cellcolor[HTML]{FFFFFF}} & \multicolumn{2}{c|}{\cellcolor[gray]{0.75}\textbf{Play Golf}} & \multicolumn{2}{l|}{} \\ \cline{3-4}
		\multicolumn{2}{|l|}{\multirow{-2}{*}{\cellcolor[HTML]{FFFFFF}Confusion matrix}} & \cellcolor[gray]{0.75}Yes & \cellcolor[gray]{0.75}No & \multicolumn{2}{l|}{\multirow{-2}{*}{}} \\ \hline
		\cellcolor[gray]{0.75} & \cellcolor[gray]{0.75}Yes & $9$ & $5$ & \cellcolor[HTML]{FFFFC7} Positive Predictive Value & \cellcolor[HTML]{FFFFC7}$0.64$ \\ \cline{2-6} 
		\multirow{-2}{*}{\cellcolor[gray]{0.75}\textbf{ZeroR}} & \cellcolor[gray]{0.75}No & $0$ & $0$ & \cellcolor[HTML]{FFFFC7} Negative Predictive Value & \cellcolor[HTML]{FFFFC7}$0.00$ \\ \hline
		\multicolumn{2}{|l|}{} & \cellcolor[HTML]{FFFFC7} Sensitivity & \cellcolor[HTML]{FFFFC7} Specificity & \multicolumn{2}{l|}{} \\ \cline{3-4}
		\multicolumn{2}{|l|}{\multirow{-2}{*}{}} & \cellcolor[HTML]{FFFFC7}$1.00$ & \cellcolor[HTML]{FFFFC7}$0.00$ & \multicolumn{2}{l|}{\multirow{-2}{*}{\textbf{Accuracy =}$0.65$}} \\ \hline
		\end{tabular}
	\end{table}
	that ZeroR only predicts the majority class correctly. As mentioned before, ZeroR is only useful for determining a baseline performance for other classification methods.
	
	\paragraph{OneR (classification by induction)}\mbox{}\\\\
	"\NewTerm{OneR}\index{OneR}", short for "One Rule", is a simple, yet accurate, classification algorithm that generates one rule for each predictor in the data, then selects the rule with the smallest total error as its "one rule".  To create a rule for a predictor $p_i$ among the set of all predictor $\mathbb{P}$, we construct a frequency table for each predictor against each possible value $t_i$ of the target attribute set $\mathbb{T}$.
	
	The OneR algorithm is the following\footnote{Thanks again to Dr. Saed Sayad for having authorized us to reproduce its web page: \url{http://www.saedsayad.com/oner.htm}}:

	\begin{algorithm}[H]
	 \KwData{$p_i$ $a_{i,j}$,$r$, $T$ }
	 \KwResult{$\min\{(p_i,\varepsilon_i\}$}
	 initialization\;
	  \ForEach{predictor $p_i \in \mathcal{P}$}{%
	  	\ForEach{attribute value $a_{i,j}$ in $p_i$}{
	  	Count how often each value of target (class) $t_k\in\mathcal{T}$ appears in all rows $r$ for $p_i$\;
	  	Find the most frequent class $t_k$ for $a_{i,j}$ \;
	  	Make the rule assign that class to this value of the predictor;
	  	}
		Calculate the total error of the rules of each predictor $\varepsilon_{i}$
      }
	 \caption{OneR (one-Rule) algorithm}
	\end{algorithm}
	As companion example let us consider again the following learning table ($14$ rows):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/computing/oner.jpg}
		\caption{OneR Data Mining (association rule) example}
	\end{figure}
	Now we create the following frequency tables for each predictor and each attribute inside each predictor:
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|c|c|}
		\hline
		\multicolumn{2}{|l|}{} & \multicolumn{2}{l|}{\cellcolor[HTML]{FFCCC9}\textbf{Play Golf}} \\ \cline{3-4} 
		\multicolumn{2}{|l|}{\multirow{-2}{*}{}} & \cellcolor[HTML]{FFCCC9}\textit{Yes} & \cellcolor[HTML]{FFCCC9}\textit{No} \\ \hline
		\cellcolor[HTML]{ECF4FF} & \cellcolor[HTML]{ECF4FF}\textit{Sunny} & $3$ & $2$ \\ \cline{2-4} 
		\cellcolor[HTML]{ECF4FF} & \cellcolor[HTML]{ECF4FF}\textit{Overcast} & $4$ & $0$ \\ \cline{2-4} 
		\multirow{-3}{*}{\cellcolor[HTML]{ECF4FF}\textbf{\phantom{x} Outlook \phantom{xx}}} & \cellcolor[HTML]{ECF4FF}\textit{Rainy} & $2$ & $3$ \\ \hline
		\end{tabular}
	\end{table}
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|c|c|}
		\hline
		\multicolumn{2}{|l|}{} & \multicolumn{2}{l|}{\cellcolor[HTML]{FFCCC9}\textbf{Play Golf}} \\ \cline{3-4} 
		\multicolumn{2}{|l|}{\multirow{-2}{*}{}} & \cellcolor[HTML]{FFCCC9}\textit{Yes} & \cellcolor[HTML]{FFCCC9}\textit{No} \\ \hline
		\cellcolor[HTML]{ECF4FF} & \cellcolor[HTML]{ECF4FF}\textit{Hot} & $2$ & $2$ \\ \cline{2-4} 
		\cellcolor[HTML]{ECF4FF} & \cellcolor[HTML]{ECF4FF}\textit{Mild\phantom{xxxx}} & $4$ & $2$ \\ \cline{2-4} 
		\multirow{-3}{*}{\cellcolor[HTML]{ECF4FF}\textbf{Temperature}} & \cellcolor[HTML]{ECF4FF}\textit{Cold} & $3$ & $1$ \\ \hline
		\end{tabular}
	\end{table}
	
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|c|c|}
		\hline
		\multicolumn{2}{|l|}{} & \multicolumn{2}{l|}{\cellcolor[HTML]{FFCCC9}\textbf{Play Golf}} \\ \cline{3-4} 
		\multicolumn{2}{|l|}{\multirow{-2}{*}{}} & \cellcolor[HTML]{FFCCC9}\textit{Yes} & \cellcolor[HTML]{FFCCC9}\textit{No} \\ \hline
		\cellcolor[HTML]{ECF4FF} & \cellcolor[HTML]{ECF4FF}\textit{High} & $3$ & $4$ \\ \cline{2-4} 
		\multirow{-2}{*}{\cellcolor[HTML]{ECF4FF}\textbf{\phantom{x}Humidity\phantom{xx}}} & \cellcolor[HTML]{ECF4FF}\textit{Normal\phantom{xx}} & $6$ & $1$ \\ \hline
		\end{tabular}
	\end{table}
	
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|c|c|}
		\hline
		\multicolumn{2}{|l|}{} & \multicolumn{2}{l|}{\cellcolor[HTML]{FFCCC9}\textbf{Play Golf}} \\ \cline{3-4} 
		\multicolumn{2}{|l|}{\multirow{-2}{*}{}} & \cellcolor[HTML]{FFCCC9}\textit{Yes} & \cellcolor[HTML]{FFCCC9}\textit{No} \\ \hline
		\cellcolor[HTML]{ECF4FF} & \cellcolor[HTML]{ECF4FF}\textit{False} & $6$ & $2$ \\ \cline{2-4} 
		\multirow{-2}{*}{\cellcolor[HTML]{ECF4FF}\textbf{\phantom{xxx}Windy\phantom{xxx}}} & \cellcolor[HTML]{ECF4FF}\textit{True\phantom{xxxx}} & $3$ & $3$ \\ \hline
		\end{tabular}
	\end{table}
	Now according to the algorithm, for each predictor we keep only the attribute that has the biggest frequency (in red below\footnote{We can notice that in case of equality we take the target that occurs the more for the other attributes.}):
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|c|c|}
		\hline
		\multicolumn{2}{|l|}{} & \multicolumn{2}{l|}{\cellcolor[HTML]{FFCCC9}\textbf{Play Golf}} \\ \cline{3-4} 
		\multicolumn{2}{|l|}{\multirow{-2}{*}{}} & \cellcolor[HTML]{FFCCC9}\textit{Yes} & \cellcolor[HTML]{FFCCC9}\textit{No} \\ \hline
		\cellcolor[HTML]{ECF4FF} & \cellcolor[HTML]{ECF4FF}\textit{Sunny} & $\color{red}{3}$ & $2$ \\ \cline{2-4} 
		\cellcolor[HTML]{ECF4FF} & \cellcolor[HTML]{ECF4FF}\textit{Overcast} & $\color{red}{4}$ & $0$ \\ \cline{2-4} 
		\multirow{-3}{*}{\cellcolor[HTML]{ECF4FF}\textbf{\phantom{x} Outlook \phantom{xx}}} & \cellcolor[HTML]{ECF4FF}\textit{Rainy} & $2$ & $\color{red}{3}$ \\ \hline
		\end{tabular}
	\end{table}
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|c|c|}
		\hline
		\multicolumn{2}{|l|}{} & \multicolumn{2}{l|}{\cellcolor[HTML]{FFCCC9}\textbf{Play Golf}} \\ \cline{3-4} 
		\multicolumn{2}{|l|}{\multirow{-2}{*}{}} & \cellcolor[HTML]{FFCCC9}\textit{Yes} & \cellcolor[HTML]{FFCCC9}\textit{No} \\ \hline
		\cellcolor[HTML]{ECF4FF} & \cellcolor[HTML]{ECF4FF}\textit{Hot} & $\color{red}{2}$ & $2$ \\ \cline{2-4} 
		\cellcolor[HTML]{ECF4FF} & \cellcolor[HTML]{ECF4FF}\textit{Mild\phantom{xxxx}} & $\color{red}{4}$ & $2$ \\ \cline{2-4} 
		\multirow{-3}{*}{\cellcolor[HTML]{ECF4FF}\textbf{Temperature}} & \cellcolor[HTML]{ECF4FF}\textit{Cold} & $\color{red}{3}$ & $1$ \\ \hline
		\end{tabular}
	\end{table}
	
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|c|c|}
		\hline
		\multicolumn{2}{|l|}{} & \multicolumn{2}{l|}{\cellcolor[HTML]{FFCCC9}\textbf{Play Golf}} \\ \cline{3-4} 
		\multicolumn{2}{|l|}{\multirow{-2}{*}{}} & \cellcolor[HTML]{FFCCC9}\textit{Yes} & \cellcolor[HTML]{FFCCC9}\textit{No} \\ \hline
		\cellcolor[HTML]{ECF4FF} & \cellcolor[HTML]{ECF4FF}\textit{High} & $3$ & $\color{red}{4}$ \\ \cline{2-4} 
		\multirow{-2}{*}{\cellcolor[HTML]{ECF4FF}\textbf{\phantom{x}Humidity\phantom{xx}}} & \cellcolor[HTML]{ECF4FF}\textit{Normal\phantom{xx}} & $\color{red}{6}$ & $1$ \\ \hline
		\end{tabular}
	\end{table}
	
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|c|c|}
		\hline
		\multicolumn{2}{|l|}{} & \multicolumn{2}{l|}{\cellcolor[HTML]{FFCCC9}\textbf{Play Golf}} \\ \cline{3-4} 
		\multicolumn{2}{|l|}{\multirow{-2}{*}{}} & \cellcolor[HTML]{FFCCC9}\textit{Yes} & \cellcolor[HTML]{FFCCC9}\textit{No} \\ \hline
		\cellcolor[HTML]{ECF4FF} & \cellcolor[HTML]{ECF4FF}\textit{False} & $\color{red}{6}$ & $2$ \\ \cline{2-4} 
		\multirow{-2}{*}{\cellcolor[HTML]{ECF4FF}\textbf{\phantom{xxx}Windy\phantom{xxx}}} & \cellcolor[HTML]{ECF4FF}\textit{True\phantom{xxxx}} & $\color{red}{3}$ & $3$ \\ \hline
		\end{tabular}
	\end{table}
	This means that so far we would have the rules:
	\begin{itemize}
		\item For \textit{Outlook}:
		\begin{table}[H]
			\centering
			\begin{tabular}{l}
			\texttt{IF Outlook = Sunny THEN Play Golf = Yes}\\
			\texttt{IF Outlook = Overcast THEN Play Golf = Yes}\\
			\texttt{IF Outlook = Rainy THEN Play Golf = No}\\
			\end{tabular}
		\end{table}
	
		\item For \textit{Temperature}:
		\begin{table}[H]
			\centering
			\begin{tabular}{l}
			\texttt{IF Temperature = Hot THEN Play Golf = Yes}\\
			\texttt{IF Temperature = Mild THEN Play Golf = Yes}\\
			\texttt{IF Temperature = Cool THEN Play Golf = Yes}\\
			\end{tabular}
		\end{table}
	
		\item For \textit{Humidity}:
		\begin{table}[H]
			\centering
			\begin{tabular}{l}
			\texttt{IF Humidity = High THEN Play Golf = No}\\
			\texttt{IF Humidity = Normal THEN Play Golf = Yes}\\
			\end{tabular}
		\end{table}
	
		\item For \textit{Windy}:
		\begin{table}[H]
			\centering
			\begin{tabular}{l}
			\texttt{IF Windy = False THEN Play Golf = Yes}\\
			\texttt{IF Windy = True THEN Play Golf = Yes}\\
			\end{tabular}
		\end{table}
	\end{itemize}
	Now if we count for each attribute according to the above rules the number of error of wrongly (false) classified items, we get:
	\begin{itemize}
		\item For \textit{Outlook}: $4$ wrongly classified items ($10$ well classified)
	
		\item For \textit{Temperature}: $5$ wrongly classified items ($9$ well classified)
	
		\item For \textit{Humidity}: $4$ wrongly classified items ($10$ well classified)
	
		\item For \textit{Windy}: $5$ wrongly classified items ($9$ well classified)
	\end{itemize}
	We see that only like this it is quite not easy to select the best rule. Therefore a possible solution is to use again a confusion matrix $\mathcal{C}$ for each attribute:
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|c|c|l|l|}
		\hline
		\multicolumn{2}{|l|}{\cellcolor[HTML]{FFFFFF}} & \multicolumn{2}{c|}{\cellcolor[gray]{0.75}\textbf{Play Golf}} & \multicolumn{2}{l|}{} \\ \cline{3-4}
		\multicolumn{2}{|l|}{\multirow{-2}{*}{\cellcolor[HTML]{FFFFFF}Confusion matrix}} & \cellcolor[gray]{0.75}Yes & \cellcolor[gray]{0.75}No & \multicolumn{2}{l|}{\multirow{-2}{*}{}} \\ \hline
		\cellcolor[gray]{0.75} & \cellcolor[gray]{0.75}Yes & $a$ & $b$ & \cellcolor[HTML]{FFFFC7} Positive Predictive Value & \cellcolor[HTML]{FFFFC7}$\dfrac{a}{a+b}$ \\ \cline{2-6} 
		\multirow{-2}{*}{\cellcolor[gray]{0.75}\textbf{OneR}} & \cellcolor[gray]{0.75}No & $c$ & $d$ & \cellcolor[HTML]{FFFFC7} Negative Predictive Value & \cellcolor[HTML]{FFFFC7}$\dfrac{d}{c+d}$ \\ \hline
		\multicolumn{2}{|l|}{} & \cellcolor[HTML]{FFFFC7} Sensitivity & \cellcolor[HTML]{FFFFC7} Specificity & \multicolumn{2}{l|}{} \\ \cline{3-4}
		\multicolumn{2}{|l|}{\multirow{-2}{*}{}} & \cellcolor[HTML]{FFFFC7}$\dfrac{a}{a+c}$ & \cellcolor[HTML]{FFFFC7}$\dfrac{d}{b+d}$ & \multicolumn{2}{l|}{\multirow{-2}{*}{\textbf{Accuracy =}$\dfrac{a+d}{a+b+c+d}$}} \\ \hline
		\end{tabular}
	\end{table}
	Therefore after building all the confusion matrices we notice that the one with the best accuracy if for the \textit{Outlook} attribute:
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|c|c|l|l|}
		\hline
		\multicolumn{2}{|l|}{\cellcolor[HTML]{FFFFFF}} & \multicolumn{2}{c|}{\cellcolor[gray]{0.75}\textbf{Play Golf}} & \multicolumn{2}{l|}{} \\ \cline{3-4}
		\multicolumn{2}{|l|}{\multirow{-2}{*}{\cellcolor[HTML]{FFFFFF}Confusion matrix}} & \cellcolor[gray]{0.75}Yes & \cellcolor[gray]{0.75}No & \multicolumn{2}{l|}{\multirow{-2}{*}{}} \\ \hline
		\cellcolor[gray]{0.75} & \cellcolor[gray]{0.75}Yes & $7$ & $2$ & \cellcolor[HTML]{FFFFC7} Positive Predictive Value & \cellcolor[HTML]{FFFFC7}$0.78$ \\ \cline{2-6} 
		\multirow{-2}{*}{\cellcolor[gray]{0.75}\textbf{OneR}} & \cellcolor[gray]{0.75}No & $2$ & $3$ & \cellcolor[HTML]{FFFFC7} Negative Predictive Value & \cellcolor[HTML]{FFFFC7}$0.60$ \\ \hline
		\multicolumn{2}{|l|}{} & \cellcolor[HTML]{FFFFC7} Sensitivity & \cellcolor[HTML]{FFFFC7} Specificity & \multicolumn{2}{l|}{} \\ \cline{3-4}
		\multicolumn{2}{|l|}{\multirow{-2}{*}{}} & \cellcolor[HTML]{FFFFC7}$0.78$ & \cellcolor[HTML]{FFFFC7}$0.60$ & \multicolumn{2}{l|}{\multirow{-2}{*}{\textbf{Accuracy =}$0.71$}} \\ \hline
		\end{tabular}
	\end{table}
	Hence the final chosen association rule:
	\begin{table}[H]
		\centering
		\begin{tabular}{l}
		\texttt{IF Outlook = Sunny THEN Play Golf = Yes}\\
		\texttt{IF Outlook = Overcast THEN Play Golf = Yes}\\
		\texttt{IF Outlook = Rainy THEN Play Golf = No}\\
		\end{tabular}
	\end{table}
	\StickyNote[2.5cm]{\LARGE ToDo: Create the confusion matrices for the other attributes}[6.5cm]
	
	\paragraph{Apriori}\mbox{}\\\\
	Before introducing the Apriori algorithm  which is the second most popular algorithm of the Association Rules, it is important to define some key concepts specific to these particular methods of learning.
	
	Here are some vocabulary definitions:
	\begin{itemize}
		\item A "\NewTerm{transaction}" is a basket or cart of one or more items purchased by consumers. 
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		Here is a base of five transactions or baskets:
		\begin{table}[H]
			\centering
			\begin{tabular}{|c|c|c|c|c|}
			\hline
			\rowcolor[gray]{0.75} 
			\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}{\textbf{Transaction Number}}} & \multicolumn{4}{c|}{\cellcolor[gray]{0.75}{\textbf{Basket composition}}} \\ \hline
			$1$ & bread & juice & jam & sugar \\ \hline
			$2$ & milk & bread & sugar &  \\ \hline
			$3$ & cheese & bread & butter & milk \\ \hline
			$4$ & cheese & milk & sugar & flour \\ \hline
			$5$ & chips & bread & jam & yoghurt \\ \hline
			\end{tabular}
		\end{table}
		\end{tcolorbox}
		
		\item An "\NewTerm{item}" designates an article. Example: juice, bread, milk ...
		
		\item An "\NewTerm{itemset}" is a collection of items or articles. A itemset containing $k$ items or a "$k$-itemset" is written between brackets $\{\mathrm{item\; 1}, \mathrm{item\; 2}, \mathrm{item\; 3}, \ldots, \mathrm{item}\; k\}$.
		
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Examples:}\\
		\begin{itemize}
			\item $1\text{-}\mathrm{itemset}$: $\{\mathrm{milk}\}$ or $\{\mathrm{bread}\}$
		
			\item $2\text{-}\mathrm{itemset}$: $\{\mathrm{milk},\mathrm{bread}\}$ or $\{\mathrm{bread},\mathrm{butter}\}$
		
			\item $3\text{-}\mathrm{itemset}$: $\{\mathrm{bread},\mathrm{butter},\mathrm{sugar}\}$
		
			\item ...
		\end{itemize}
		\end{tcolorbox}
		
		\item Given a $k$-itemset $P$, the "\NewTerm{support}" of $P$, is the percentage of transactions containing $P$. This is an index of reliability.
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Examples:}\\\\
		E1. For $S = \{\mathrm{bread}, \mathrm{milk}\}$ then the support according to the previous set of baskets is equal to $P=2/5 = 0.4$, hence $40\%$:
		\begin{table}[H]
			\centering
			\begin{tabular}{|c|c|c|c|c|}
			\hline
			\rowcolor[gray]{0.75} 
			\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}{\textbf{Transaction Number}}} & \multicolumn{4}{c|}{\cellcolor[gray]{0.75}{\textbf{Basket composition}}} \\ \hline
			$1$ & bread & juice & jam & sugar \\ \hline
			\cellcolor[HTML]{9AFF99}$2$ & \cellcolor[HTML]{9AFF99}milk & \cellcolor[HTML]{9AFF99}bread & sugar &  \\ \hline
			\cellcolor[HTML]{9AFF99}$3$ & cheese & \cellcolor[HTML]{9AFF99}bread & butter & \cellcolor[HTML]{9AFF99}milk \\ \hline
			$4$ & cheese & milk & sugar & flour \\ \hline
			$5$ & chips & bread & jam & yoghurt \\ \hline
			\end{tabular}
		\end{table}
		
		E2. For  $L = \{\mathrm{bread}\}$ then the support of $L$ is $P=4/5 = 0.8$, hence $80\%$ of the transactions contains $L$:
		\begin{table}[H]
			\centering
			\begin{tabular}{|c|c|c|c|c|}
			\hline
			\rowcolor[gray]{0.75} 
			\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}{\textbf{Transaction Number}}} & \multicolumn{4}{c|}{\cellcolor[gray]{0.75}{\textbf{Basket composition}}} \\ \hline
			\cellcolor[HTML]{9AFF99}$1$ & \cellcolor[HTML]{9AFF99}bread & juice & jam & sugar \\ \hline 
			\cellcolor[HTML]{9AFF99}$2$ & milk & \cellcolor[HTML]{9AFF99}bread & sugar &  \\ \hline
			\cellcolor[HTML]{9AFF99}$3$ & cheese & \cellcolor[HTML]{9AFF99}bread & butter & milk \\ \hline
			$4$ & cheese & milk & sugar & flour \\ \hline
			\cellcolor[HTML]{9AFF99}$5$ & chips & \cellcolor[HTML]{9AFF99}bread & jam & yoghurt \\ \hline
			\end{tabular}
		\end{table}
		\end{tcolorbox}
		
		\item The "\NewTerm{minimum support}" is the minimum threshold of from which one qualifies a $k$-itemset as frequent.
		
		\item One will say of a itemset that it is a "\NewTerm{frequent $k$-itemset}" if its support is greater or equal to the minimum support.
		
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Examples:}\\\\
		If the minimum support is $0.5$, we can say that $L$, from the previous example, is a frequent itemset but not $S$ whose support is only $0.4$.
		\end{tcolorbox} 
	\end{itemize}
	
	"\NewTerm{Apriori}\index{apriori}" is an algorithm that was proposed by Agrawal and Srikant in 11994 (holocene calendar) for frequent item set mining and association rule learning over transactional databases. It proceeds by identifying the frequent individual items in the database and extending them to larger and larger item sets as long as those item sets appear sufficiently often in the database. The frequent item sets determined by Apriori can be used to determine association rules which highlight general trends in the database: this has applications in domains such as market basket analysis.
	
	The "a priori" algorithm is based on the so-named a priori property which is stated as follows: «\textit{If one itemset is considered as frequent then any other sub itemset arising from the items of $P$ is also frequent}».
	
	Taking again the following table as companion example:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}{\textbf{Transaction Number}}} & \multicolumn{4}{c|}{\cellcolor[gray]{0.75}{\textbf{Basket composition}}} \\ \hline
		$1$ & bread & juice & jam & sugar \\ \hline
		$2$ & milk & bread & sugar &  \\ \hline
		$3$ & cheese & bread & butter & milk \\ \hline
		$4$ & cheese & milk & sugar & flour \\ \hline
		$5$ & chips & bread & jam & yoghurt \\ \hline
		\end{tabular}
	\end{table}
	By setting the minimum support to $P_{\min}$ to $0.35$, the previous itemset $S = \{\mathrm{bread}, \mathrm{milk}\}$ with a support of $0.4$ is therefore referred to as frequent. The previous itemset $L = \{\mathrm{bread}\}$ is a sub-item of $L\subset S$, it is according to the apriori property also frequent (support of $L = 0.8>0.35$) and if one considers another new sub-itemset $D = \{\mathrm{milk}\}$ of $S$, one sees that the support the 1-itemset of $D$ is $3/5 = 0.6> 0.35$.
	
	If we have $n$ different items, then we have $2^n-1$ possible different itemsets (the $-1$ is to remove the empty itemset). Therefore in our case, we could create $2^{10}-1=1023$ different possible itemsets with:
	
	Thus, the number of itemsets to be analysed can be considerably reduced following the pruning of low-level non-frequent itemsets.
	
	Given a minimal support that we will denote $\delta$, the Apriori algorithm adopts an ascending iterative approach that determines all possible $1$-itemset first and then those whose support is less than $\delta$ are subject to pruning and the rest, those that are frequent are stored in an $L_1$ itemset collections.
	
	For the $5$ transactions of the previous illustration in the $10$ possible itemsets of type $1$-itemset, for a minimum support of $\delta= 0.35$, only the following itemsets will be part of $L_1$:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\textbf{$\pmb{1}$-itemset} & \textbf{Support} \\ \hline
		$\{\mathrm{bread}\}$ & $0.8$ \\ \hline
		$\{\mathrm{sugar}\}$ & $0.6$ \\ \hline
		$\{\mathrm{milk}\}$ & $0.6$ \\ \hline
		$\{\mathrm{jam}\}$ & $0.4$ \\ \hline
		$\{\mathrm{cheese}\}$ & $0.4$ \\ \hline
		\end{tabular}
	\end{table}
	The next step consists of all possible $2$-itemsets from the items contained in $L_1$. Again the $2$-itemsets are tested, those whose support is less than $\delta$ are pruned and only the frequent $2$-itemsets are kept in a collection of $L_2$ itemsets.
	
	On the basis of the $L_1$ previously constituted, it is always possible with $\delta = 0.35$ to obtain a collection $L_2$ consisting of:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\textbf{$\pmb{2}$-itemset} & \textbf{Support} \\ \hline
		$\{\mathrm{jam},\mathrm{bread}\}$ & $0.4$ \\ \hline
		$\{\mathrm{cheese},\mathrm{milk}\}$ & $0.6$ \\ \hline
		$\{\mathrm{sugar},\mathrm{milk}\}$ & $0.6$ \\ \hline
		$\{\mathrm{bread},\mathrm{sugar}\}$ & $0.4$ \\ \hline
		$\{\mathrm{milk},\mathrm{bread}\}$ & $0.4$ \\ \hline
		\end{tabular}
	\end{table}
	This aggregation and pruning process is repeated for a collection $L_k$ containing the possible itemsets of $k$-itemsets having validated the test of the minimum support $\delta$.
	
	So figuring the concept with transactions of a maximum of $4$-itemsets $\{\mathrm{A},\mathrm{B},\mathrm{C},\mathrm{D}\}$ then with a total of $2^4-1=15$ possible combinations, this procedure would lead for example to:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/computing/apriori.jpg}
	\end{figure}
	The algorithm stops if for a new collection $L_{k+1}$, no $(k + 1)$-itemsets validates the test of the minimum support $\delta$ (as is the case for the previous figure where one can not obtain $3$-itemsets on the basis of $L_2$). Optionally, we can define an integer $K$ which specifies either the maximum number of items that an itemset can contain or the maximum number of iterations to be performed by the algorithm.
	
	Once the collections of itemsets are formed, it is possible to transcribe this in the form of candidate rules (for example $X\Rightarrow Y$ or: $X$ implies $Y$). However, some criteria are defined to assess the relevance of an association rule:
	\begin{itemize}
		\item "Confidence" or "trust of a rule": It's the measure of certainty or confidence associated with an association rule. Formally, for a rule $X \Rightarrow Y$, it is the percentage of transaction containing both $X$ and $Y$ on the total of transactions containing $X$:
		
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
		\textbf{{\Large \ding{45}}Example:}\\\\
		What is the confidence index of the rule:
		\begin{gather*}
			\{\mathrm{milk}, \mathrm{sugar}\}\Rightarrow\mathrm{flour}
		\end{gather*}
		As:
		\begin{gather*}
			\text{Support}({\mathrm{milk}, \mathrm{sugar}}) = 0.4
		\end{gather*}
		and:
		\begin{gather*}
			\text{Support}({\mathrm{farine}, \mathrm{milk}, \mathrm{sugar}}) = 0.2
		\end{gather*}
		Then:
		\begin{gather*}
			\text{Conf}(\{\mathrm{milk}, \mathrm{sugar}\}\Rightarrow\mathrm{flour}) = \dfrac{\{\mathrm{milk}, \mathrm{sugar}\}}{\mathrm{flour}}=\dfrac{0.2}{0.4}=0.5
		\end{gather*}
		thus a confidence index of $50\%$. 
		\end{tcolorbox}
		In practice, an acceptable threshold of confidence is defined. We are confident with regard to a rule when the index of confidence is large and moves away from the minimum.
		
		\item "Lift of an association rule\index{lift}\label{lift association rule}": Assuming that $X$ and $Y$ are statistically independent (\SeeChapter{see section Probabilities page \pageref{joint probability}}), the lift measures the percentage of the number of times it is more frequent to obtain $X$ and $Y$:
		
		Thus, if the lift is equal to $1$ then $X$ and $Y$ are  independent, which means that the rule is not at all relevant: a simple coincidence! While a lift greater than $1$, shows how useful the rule is and a higher value shows how strong the rule is and therefore relevant, it is then a kind of correlation measure. A lift smaller than $1$ indicates that $X$ and $Y$ appear less often together than expected, this means that the occurrence of $X$ has a negative effect on the occurrence of $Y$ or that $X$ is negatively correlated with $Y$. Thus, lift is a value between $0$ and $+\infty$.
		
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		We could also run (obviously) a complementary $\chi^2$-test of independence of a $2\times 2$ contingency table (having for entries $X$, $Y$, $\neg X$, $\neg Y$) to test if there is statistically or not independence (\SeeChapter{see section Statistics page \pageref{chi-square test of independence}}).
		\end{tcolorbox}	
	
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
		\textbf{{\Large \ding{45}}Example:}\\\\
		What is the lift of the rule:
		\begin{gather*}
			\{\mathrm{milk}, \mathrm{sugar}\}\Rightarrow \{\mathrm{flour}\}
		\end{gather*}
		As:
		\begin{gather*}
			\text{Support}(\{\mathrm{milk},\mathrm{sugar}\})=0.4
		\end{gather*}
		and:
		\begin{gather*}
			\text{Support}(\{\mathrm{flour},\mathrm{milk},\mathrm{sugar}\})=0.2
		\end{gather*}
		and:
		\begin{gather*}
			\text{Support}(\{\mathrm{flour}\})=0.2
		\end{gather*}
		Then:
		\begin{gather*}
			\text{Lift}(\{\mathrm{milk}, \mathrm{sugar}\}\Rightarrow \{\mathrm{flour}\})=\dfrac{\{\mathrm{flour},\mathrm{milk},\mathrm{sugar}\}}{\{\mathrm{milk},\mathrm{sugar}\}\cdot\{\mathrm{flour}\}}=\dfrac{0.2}{0.4\cdot 0.2}=2.5
		\end{gather*}
		Of course, it is always necessary to compare these results with the values obtained with the other association rules before judging the relevance.
		\end{tcolorbox}
	\end{itemize}
	Lift and $\chi^2$ are not "null-invariant". That means that they are not so good at evaluating data that contain too many null transactions. 
	
	We limited ourselves here to the two most widely used criteria for evaluating association rules, but there are other criteria (some of them are null-invariant) such as Leverage, Conviction index, Coverage, Jaccard, Cosine, Kulcynski, etc.
	
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|c|}{\cellcolor[gray]{0.75}\textbf{Measure}} & \textbf{Definition} & \textbf{Range} & \textbf{Null-Invariant} \\ \hline
		$\chi^2(X,Y)$ & $\displaystyle \sum_{i,j}\dfrac{(O_{ij}-E_{ij})^2}{E_{ij}}$ & $[0,+\infty[$ & No \\ \hline
		$\text{Lift}(X,Y)$ & $\displaystyle \dfrac{S(X\cap Y)}{S(X)\cdot S(Y)}$ & $[0,+\infty[$ & No \\ \hline
		$\text{AllConf}(X,Y)$ & $\displaystyle \dfrac{S(X\cap Y)}{\max \left(S(X), S(Y)\right)}$ & $[0,1]$ & Yes \\ \hline
		$\text{Jaccard}(X,Y)$ & $\displaystyle \dfrac{S(X\cap Y)}{S(X)+S(Y)-S(X\cap Y)}$ & $[0,1]$ & Yes \\ \hline
		$\text{Cosine}(X,Y)$ & $\displaystyle \dfrac{S(X\cap Y)}{\sqrt{S(X)\cdot S(Y)}}$ & $[0,1]$ & Yes \\ \hline
		$\text{Kulczinsky}(X,Y)$ & $\displaystyle \dfrac{1}{2}\left(\dfrac{S(X\cap Y)}{S(X)}+\dfrac{S(X\cap Y)}{S(Y)}\right)$ & $[0,1]$ & Yes \\ \hline
		$\text{MaxConf}(X,Y)$ & $\displaystyle  \max\left(\dfrac{S(X)}{S(X\cap Y)},\dfrac{S(Y)}{S(X\cap Y)}\right)$ & $[0,1]$ & Yes \\ \hline
		$\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ \\ \hline
		\end{tabular}
		\caption{Some support-confidence interesting measures}
	\end{table}
	
	Finally, we can say that the association rules are very easily interpretable and their implementation simple and easy to transcribe into operational rules. However, a number of points are quite problematic:
	\begin{itemize}
		\item Runtime cost: Even if the Apriori algorithm eliminates several occurrences with the minimum support rule, it is easy to imagine that with a large database, occurrences that exceed the minimum support can be large.

		\item Fallacious or parasitic associations: This effect depends in particular on the minimum support defined, but in front of a large database, to obtain a sufficient quantity of rules it is sometimes useful to consider a low minimum support.
	\end{itemize}
	
	\subsubsection{Equivalence CLAss Transformation (ECLAT)}
	The "\NewTerm{ECLAT}\index{ECLAT}" algorithm, acronym for Equivalence CLAss Transformation, is an alternative way to the previous method to build the item-set that performs better in huge databases.
	
	Let us use the companion example as before to introduce detail how ECLAT works:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}{\textbf{Transaction Number}}} & \multicolumn{4}{c|}{\cellcolor[gray]{0.75}{ \textbf{Basket composition}}} \\ \hline
		$1$ & bread & juice & jam & sugar \\ \hline
		$2$ & milk & bread & sugar &  \\ \hline
		$3$ & cheese & bread & butter & milk \\ \hline
		$4$ & cheese & milk & sugar & flour \\ \hline
		$5$ & chips & bread & jam & yoghurt \\ \hline
		\end{tabular}
	\end{table}
	The idea of ECLAT is for each item, to store a list of transaction ID's (abbreviated "tids" as acronym from "transaction ID's" or sometimes "tidlist"). It as vertical data layout at the opposite of the above one. Indeed, the transaction table above transformed into a tids will look like following for the $1$-itemset:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		{\cellcolor[gray]{0.75}{ \textbf{Item Set}}} & {\cellcolor[gray]{0.75}{\textbf{TID Set}}} \\ \hline
		bread & $1,2,3,5$ \\ \hline
		jam & $1,5$  \\ \hline
		cheese & $3,4$  \\ \hline
		juice & $1$ \\ \hline
		milk & $2,3,4$ \\ \hline
		butter & $3$ \\ \hline
		sugar & $1,2,4$ \\ \hline
		flour & $4$ \\ \hline
		chips & $5$ \\ \hline
		\end{tabular}
		\caption[]{ECLAT $1$-itemset tidlist}
	\end{table}	
	And the corresponding tidlist for $2$-itemset:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		{\cellcolor[gray]{0.75}{\textbf{Item Set}}} & {\cellcolor[gray]{0.75}{\textbf{TID Set}}} \\ \hline
		\{bread,jam\} & $1,5$ \\ \hline
		\{bread,cheese\} & $3$ \\ \hline
		\{bread,juice\} & $1$ \\ \hline
		\{bread,milk\} & $2,3$ \\ \hline
		\{bread,butter\} & $2,3$ \\ \hline
		\{bread,sugar\} & $1,2$ \\ \hline
		\{cheese,flour\} & $4$ \\ \hline
		\{breach, chips\} & $5$ \\ \hline
		\{jam, juice\} & $1$ \\ \hline
		\{jam, chips\} & $5$ \\ \hline
		\{cheese, milk\} & $4$ \\ \hline
		\{cheese, butter\} & $4$ \\ \hline
		$\ldots$ & $\ldots$ \\ \hline
		\end{tabular}
		\caption[]{ECLAT $2$-itemset tidlist}
	\end{table}	
	And the corresponding tidlist for $3$-itemset:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		{\cellcolor[gray]{0.75}{\textbf{Item Set}}} & {\cellcolor[gray]{0.75}{\textbf{TID Set}}} \\ \hline
		\{bread,jam,juice\} & $1$ \\ \hline
		\{bread,jam,chips\} & $5$ \\ \hline
		\{bread,milk,sugar\} & $2$ \\ \hline
		\{bread,cheese,milk\} & $3$ \\ \hline
		\{bread,cheese,butter\} & $3$ \\ \hline
		\{jam,juice,sugar\} & $1$ \\ \hline
		\{cheese,milk,butter\} & $3$ \\ \hline
		\{cheese,milk,sugar\} & $4$ \\ \hline
		$\ldots$ & $\ldots$ \\ \hline
		\end{tabular}
		\caption[]{ECLAT $3$-itemset tidlist}
	\end{table}
	This process repeats, with $k$ incremented by $1$ each time, until no frequent items or no candidate itemsets can be found. 

	The pros are:
	\begin{itemize}
		\item Depth-first search reduces memory requirements
		\item Usually (considerably) faster than Apriori
		\item No need to scan the database to find the support of $(k+1)$-itemsets, for $k>\geq 1$
	\end{itemize}
	and the cons:
	\begin{itemize}
		\item The TID-sets can be quite long, hence expensive to manipulate
	\end{itemize}

	
	\pagebreak
	\subsubsection{Clustering and Classification}
	In statistical data analysis, "clustering" describes empirical methods of data classification (hierarchical clustering method or data partitioning method). In other words clustering and classification are tasks for predicting the value of a categorical variable (target or class) by building a model based on one or more numerical and/or categorical variables (predictors or attributes).

	These techniques typically enable the segmentation of all customers of a company based on their demography or buying patterns, to group documents for presentations, identify new animal or plant species, to group information by individuals or by interests.
	
	\begin{table}[H]
		\begin{tabular}{|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\textbf{Classification} & \textbf{Clustering} \\ \hline
		Use labelled as the input & Use unlabelled as the input \\ \hline
		The output is known & The output is unknown \\ \hline
		Uses supervised machine learning & Uses unsupervised machine learning \\ \hline
		\begin{tabular}[c]{@{}c@{}}A training data set is provided and used\\ to produce classifications\end{tabular} & \begin{tabular}[c]{@{}c@{}}A training data set is not provided and used\\ to produce clusters\end{tabular} \\ \hline
		\begin{tabular}[c]{@{}c@{}}Examples of algorithms: Decision-trees,\\ Bayesian classifiers and Support Vector\\ Machines (SVM)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Examples of algorithms: Partition-based\\ clustering ($k$-means), Hierarchical clustering\\ (agglomerative / divisive) and DBSCAN\end{tabular} \\ \hline
		Can be more complex than clustering & Can be less complex than clustering \\ \hline
		Does not specify areas for improvement & Specify areas for improvement \\ \hline
		Known number of classes & Unknown number of classes \\ \hline
		Interpretability High & Interpretability Low \\ \hline
		\end{tabular}
		\caption{Typical Classification vs Clustering differences}
	\end{table}
	By using some mathematical tricks, some classification techniques can be transformed into clustering techniques. That's why sometimes all these different techniques are confusing!

	We see in practice two major families of \underline{clustering} techniques (careful even experts in the field are unable to agree on a common classification ...!):
	\begin{enumerate}
		\item The "\NewTerm{non-hierarchical techniques}\index{non-hierarchical techniques}": that is to say where the final number of classes (groups) is chosen in advance.

		\item The "\NewTerm{hierarchical techniques}\index{hierarchical techniques}": that is to say where it leads to a classification by successive aggregations or divisions.
	\end{enumerate}
	\begin{figure}[H]
    	\centering
    	 \begin{forest}
                for tree={
                    forked edges,
                    grow'=0,
                    draw,
                    rounded corners,
                    node options={align=center,},
                    text width=3cm,
                    inner sep=2pt
                },
                [\textbf{Clustering}, fill=gray!45, parent
                %
                    [Non-hierarchical, for tree={fill=brown!45, child}
                        [Mixture Model, fill=brown!30, grandchild
							[Expectation Maximization, fill=brown!30, greatgrandchild]                        
                        ]
                        [Density Based, fill=brown!30, grandchild
                        	[PROCLUS, fill=brown!30, greatgrandchild]                        
                        	[CLIQUE, fill=brown!30, greatgrandchild]
                        	[OPTICS, fill=brown!30, greatgrandchild]
                        	[DBSCAN, fill=brown!30, greatgrandchild]
                        ]
                        [Nearest Neighbour, fill=brown!30, grandchild
                            [Jarvis-Patrick (S-NN), fill=brown!30, greatgrandchild]
                        ]
                        [Relocation, fill=brown!30, grandchild
                            [CLARANS, fill=brown!30, greatgrandchild]
                            [CLARA, fill=brown!30, greatgrandchild]
                            [PAM, fill=brown!30, greatgrandchild]
                            [$k$-means, fill=brown!30, greatgrandchild]
                        ]
                        [Single Pass, fill=brown!30, grandchild
                            [Leader algorithm, fill=brown!30, greatgrandchild]
                        ]
                        [Others..., fill=brown!30, grandchild]
                    ]
                    %
                    [Hierarchical, for tree={fill=red!45,child}
                        [Divise \\(HDC/DIANA), fill=red!30, grandchild
							[Polythetic, fill=red!20, greatgrandchild
								[Bisective $k$-means, fill=red!10, referenceblock]
								[Minimum Diameter, fill=red!10, referenceblock]	
							]
							[Monothetic, fill=red!20, greatgrandchild]
                        ]
                        [Agglomerative (HAC/AGNES), fill=red!30, grandchild
							[CURE, fill=red!20, greatgrandchild]
							[CHAMELEON, fill=red!20, greatgrandchild] 
							[Weighted Average, fill=red!20, greatgrandchild]
							[Group Average, fill=red!20, greatgrandchild] 
							[Centroid, fill=red!20, greatgrandchild] 
							[Medoids, fill=red!20, greatgrandchild] 
							[Ward, fill=red!20, greatgrandchild] 
							[Complete link, fill=red!20, greatgrandchild] 
							[Single link, fill=red!20, greatgrandchild] 
                        ]
                    ]
                ]
            \end{forest}
   		\vspace*{3mm}
		\caption{Non-exhaustive orgchart of clustering techniques}
    \end{figure}
	Among these two families we distinguish three sub-families not highlighted in the orgchart above (this is the definition for THIS book - and that correspond almost to that of the SAS company - as they are no common accepted definitions until now between specialists working in this field):
	\begin{enumerate}
		\item The techniques that make use of data whose nominal classification property is known in advance to train a prediction model: "\NewTerm{Machine Learning\footnote{Oxford Dictionary definition: The capacity of a computer to learn from experience, i.e. to modify its processing on the basis of newly acquired information.}\index{machine learning}}" or "\NewTerm{supervised learning}\index{supervised learning}". We find in this category the logistic regression techniques, CRT, ID3, discriminant analysis, Bayesian networks, decision lists, $k$-NN, neural networks, etc. 

		As there are not today a consensus in the definitions, it is important that the reader also knows that "supervised" algorithms by are algorithms that have a correction mechanism of the model parameters based on generated errors.
	
		\item The techniques that make use of data whose no nominal classification is known in advance to suggest a classification and seek a possible classification (which at the business level is often more interesting): "\NewTerm{data mining}\index{unsupervised learning}". We find in this category the, HAC, $k$-means, kohonen maps, $K$-modes, $K$-medoids, etc.
		
		\item The techniques that make use of a mixture of classified and unclassified data and named "\NewTerm{semi-supervised learning}\index{semi-supervised learning}".
	\end{enumerate}
	The first two families are quite well summarized by the following figure:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
		
		%Shape: Axis 2D [id:dp7910817713265392] 
		\draw  (101,167.6) -- (255.5,167.6)(111.5,37) -- (111.5,180.6) (248.5,162.6) -- (255.5,167.6) -- (248.5,172.6) (106.5,44) -- (111.5,37) -- (116.5,44)  ;
		%Shape: Axis 2D [id:dp7600485982463021] 
		\draw  (316,167.6) -- (470.5,167.6)(326.5,37) -- (326.5,180.6) (463.5,162.6) -- (470.5,167.6) -- (463.5,172.6) (321.5,44) -- (326.5,37) -- (331.5,44)  ;
		%Shape: Star [id:dp12951832113897654] 
		\draw   (136.19,137.6) -- (136.19,140.65) -- (139.22,139.71) -- (136.19,140.65) -- (138.06,143.12) -- (136.19,140.65) -- (134.31,143.12) -- (136.19,140.65) -- (133.16,139.71) -- (136.19,140.65) -- cycle ;
		%Shape: Star [id:dp05008324005138043] 
		\draw   (126.19,133.6) -- (126.19,136.65) -- (129.22,135.71) -- (126.19,136.65) -- (128.06,139.12) -- (126.19,136.65) -- (124.31,139.12) -- (126.19,136.65) -- (123.16,135.71) -- (126.19,136.65) -- cycle ;
		%Shape: Star [id:dp2165238302120478] 
		\draw   (149.69,124.1) -- (149.69,127.15) -- (152.72,126.21) -- (149.69,127.15) -- (151.56,129.62) -- (149.69,127.15) -- (147.81,129.62) -- (149.69,127.15) -- (146.66,126.21) -- (149.69,127.15) -- cycle ;
		%Shape: Star [id:dp04437939070269881] 
		\draw   (162.69,110.6) -- (162.69,113.65) -- (165.72,112.71) -- (162.69,113.65) -- (164.56,116.12) -- (162.69,113.65) -- (160.81,116.12) -- (162.69,113.65) -- (159.66,112.71) -- (162.69,113.65) -- cycle ;
		%Shape: Star [id:dp13734981592242645] 
		\draw   (142.44,115.6) -- (142.44,118.65) -- (145.47,117.71) -- (142.44,118.65) -- (144.31,121.12) -- (142.44,118.65) -- (140.56,121.12) -- (142.44,118.65) -- (139.41,117.71) -- (142.44,118.65) -- cycle ;
		%Shape: Star [id:dp09054992448604882] 
		\draw   (145.44,106.1) -- (145.44,109.15) -- (148.47,108.21) -- (145.44,109.15) -- (147.31,111.62) -- (145.44,109.15) -- (143.56,111.62) -- (145.44,109.15) -- (142.41,108.21) -- (145.44,109.15) -- cycle ;
		%Shape: Star [id:dp45241022514569873] 
		\draw   (132.19,109.85) -- (132.19,112.9) -- (135.22,111.96) -- (132.19,112.9) -- (134.06,115.37) -- (132.19,112.9) -- (130.31,115.37) -- (132.19,112.9) -- (129.16,111.96) -- (132.19,112.9) -- cycle ;
		%Shape: Star [id:dp9190524154287498] 
		\draw   (125.94,98.1) -- (125.94,101.15) -- (128.97,100.21) -- (125.94,101.15) -- (127.81,103.62) -- (125.94,101.15) -- (124.06,103.62) -- (125.94,101.15) -- (122.91,100.21) -- (125.94,101.15) -- cycle ;
		%Shape: Star [id:dp6426074893494167] 
		\draw   (142.44,88.85) -- (142.44,91.9) -- (145.47,90.96) -- (142.44,91.9) -- (144.31,94.37) -- (142.44,91.9) -- (140.56,94.37) -- (142.44,91.9) -- (139.41,90.96) -- (142.44,91.9) -- cycle ;
		%Shape: Star [id:dp4693216445946007] 
		\draw   (132.94,80.85) -- (132.94,83.9) -- (135.97,82.96) -- (132.94,83.9) -- (134.81,86.37) -- (132.94,83.9) -- (131.06,86.37) -- (132.94,83.9) -- (129.91,82.96) -- (132.94,83.9) -- cycle ;
		%Shape: Star [id:dp5902612421146274] 
		\draw   (145.69,70.85) -- (145.69,73.9) -- (148.72,72.96) -- (145.69,73.9) -- (147.56,76.37) -- (145.69,73.9) -- (143.81,76.37) -- (145.69,73.9) -- (142.66,72.96) -- (145.69,73.9) -- cycle ;
		%Shape: Circle [id:dp44996437310943427] 
		\draw   (154.72,90.92) .. controls (154.72,89.67) and (155.74,88.65) .. (157,88.65) .. controls (158.26,88.65) and (159.27,89.67) .. (159.27,90.92) .. controls (159.27,92.18) and (158.26,93.2) .. (157,93.2) .. controls (155.74,93.2) and (154.72,92.18) .. (154.72,90.92) -- cycle ;
		%Shape: Circle [id:dp7171968751286055] 
		\draw   (164.72,91.17) .. controls (164.72,89.92) and (165.74,88.9) .. (167,88.9) .. controls (168.26,88.9) and (169.27,89.92) .. (169.27,91.17) .. controls (169.27,92.43) and (168.26,93.45) .. (167,93.45) .. controls (165.74,93.45) and (164.72,92.43) .. (164.72,91.17) -- cycle ;
		%Shape: Circle [id:dp49613744432943396] 
		\draw   (188.47,95.92) .. controls (188.47,94.67) and (189.49,93.65) .. (190.75,93.65) .. controls (192.01,93.65) and (193.02,94.67) .. (193.02,95.92) .. controls (193.02,97.18) and (192.01,98.2) .. (190.75,98.2) .. controls (189.49,98.2) and (188.47,97.18) .. (188.47,95.92) -- cycle ;
		%Shape: Circle [id:dp9091141984853768] 
		\draw   (171.47,108.92) .. controls (171.47,107.67) and (172.49,106.65) .. (173.75,106.65) .. controls (175.01,106.65) and (176.02,107.67) .. (176.02,108.92) .. controls (176.02,110.18) and (175.01,111.2) .. (173.75,111.2) .. controls (172.49,111.2) and (171.47,110.18) .. (171.47,108.92) -- cycle ;
		%Shape: Circle [id:dp5798764228740139] 
		\draw   (205.22,108.92) .. controls (205.22,107.67) and (206.24,106.65) .. (207.5,106.65) .. controls (208.76,106.65) and (209.77,107.67) .. (209.77,108.92) .. controls (209.77,110.18) and (208.76,111.2) .. (207.5,111.2) .. controls (206.24,111.2) and (205.22,110.18) .. (205.22,108.92) -- cycle ;
		%Shape: Circle [id:dp9061509568667869] 
		\draw   (184.72,113.67) .. controls (184.72,112.42) and (185.74,111.4) .. (187,111.4) .. controls (188.26,111.4) and (189.27,112.42) .. (189.27,113.67) .. controls (189.27,114.93) and (188.26,115.95) .. (187,115.95) .. controls (185.74,115.95) and (184.72,114.93) .. (184.72,113.67) -- cycle ;
		%Shape: Circle [id:dp13334976118177222] 
		\draw   (144.22,135.17) .. controls (144.22,133.92) and (145.24,132.9) .. (146.5,132.9) .. controls (147.76,132.9) and (148.77,133.92) .. (148.77,135.17) .. controls (148.77,136.43) and (147.76,137.45) .. (146.5,137.45) .. controls (145.24,137.45) and (144.22,136.43) .. (144.22,135.17) -- cycle ;
		%Shape: Circle [id:dp655505566280026] 
		\draw   (157.97,140.17) .. controls (157.97,138.92) and (158.99,137.9) .. (160.25,137.9) .. controls (161.51,137.9) and (162.52,138.92) .. (162.52,140.17) .. controls (162.52,141.43) and (161.51,142.45) .. (160.25,142.45) .. controls (158.99,142.45) and (157.97,141.43) .. (157.97,140.17) -- cycle ;
		%Shape: Circle [id:dp4483514656733574] 
		\draw   (171.22,131.42) .. controls (171.22,130.17) and (172.24,129.15) .. (173.5,129.15) .. controls (174.76,129.15) and (175.77,130.17) .. (175.77,131.42) .. controls (175.77,132.68) and (174.76,133.7) .. (173.5,133.7) .. controls (172.24,133.7) and (171.22,132.68) .. (171.22,131.42) -- cycle ;
		%Shape: Circle [id:dp7759559900440633] 
		\draw   (141.22,152.92) .. controls (141.22,151.67) and (142.24,150.65) .. (143.5,150.65) .. controls (144.76,150.65) and (145.77,151.67) .. (145.77,152.92) .. controls (145.77,154.18) and (144.76,155.2) .. (143.5,155.2) .. controls (142.24,155.2) and (141.22,154.18) .. (141.22,152.92) -- cycle ;
		%Shape: Star [id:dp2069598030939197] 
		\draw   (366.25,78.6) -- (366.25,82.48) -- (369.63,82.48) -- (366.25,82.48) -- (366.25,86.35) -- (366.25,82.48) -- (362.88,82.48) -- (366.25,82.48) -- cycle ;
		%Shape: Star [id:dp7658593274828285] 
		\draw   (353.25,86.6) -- (353.25,90.85) -- (356.88,90.85) -- (353.25,90.85) -- (353.25,95.1) -- (353.25,90.85) -- (349.63,90.85) -- (353.25,90.85) -- cycle ;
		%Shape: Star [id:dp8266026469679524] 
		\draw   (362.75,95.35) -- (362.75,99.6) -- (366.38,99.6) -- (362.75,99.6) -- (362.75,103.85) -- (362.75,99.6) -- (359.13,99.6) -- (362.75,99.6) -- cycle ;
		%Shape: Star [id:dp2927972642612551] 
		\draw   (346.38,104.35) -- (346.38,108.35) -- (349.38,108.35) -- (346.38,108.35) -- (346.38,112.35) -- (346.38,108.35) -- (343.38,108.35) -- (346.38,108.35) -- cycle ;
		%Shape: Star [id:dp2105407147892222] 
		\draw   (352.88,117.35) -- (352.88,121.35) -- (355.88,121.35) -- (352.88,121.35) -- (352.88,125.35) -- (352.88,121.35) -- (349.88,121.35) -- (352.88,121.35) -- cycle ;
		%Shape: Star [id:dp4451215302189018] 
		\draw   (366,114.35) -- (366,117.85) -- (368.63,117.85) -- (366,117.85) -- (366,121.35) -- (366,117.85) -- (363.38,117.85) -- (366,117.85) -- cycle ;
		%Shape: Star [id:dp6897111355561172] 
		\draw   (362.75,122.6) -- (362.75,126.1) -- (365.38,126.1) -- (362.75,126.1) -- (362.75,129.6) -- (362.75,126.1) -- (360.13,126.1) -- (362.75,126.1) -- cycle ;
		%Shape: Star [id:dp869388612700208] 
		\draw   (345.88,139.85) -- (345.88,143.85) -- (348.88,143.85) -- (345.88,143.85) -- (345.88,147.85) -- (345.88,143.85) -- (342.88,143.85) -- (345.88,143.85) -- cycle ;
		%Shape: Star [id:dp40554820645018186] 
		\draw   (356,144.1) -- (356,148.6) -- (359.13,148.6) -- (356,148.6) -- (356,153.1) -- (356,148.6) -- (352.88,148.6) -- (356,148.6) -- cycle ;
		%Shape: Star [id:dp4095541551307851] 
		\draw   (362.13,155.85) -- (362.13,158.73) -- (364.13,158.73) -- (362.13,158.73) -- (362.13,161.61) -- (362.13,158.73) -- (360.13,158.73) -- (362.13,158.73) -- cycle ;
		%Shape: Star [id:dp5393823960102055] 
		\draw   (365.36,137.6) -- (365.36,140.1) -- (367.1,140.1) -- (365.36,140.1) -- (365.36,142.6) -- (365.36,140.1) -- (363.63,140.1) -- (365.36,140.1) -- cycle ;
		%Shape: Star [id:dp661158359876866] 
		\draw   (370,131.35) -- (370,134.43) -- (372.63,134.43) -- (370,134.43) -- (370,137.5) -- (370,134.43) -- (367.38,134.43) -- (370,134.43) -- cycle ;
		%Shape: Star [id:dp953957956494238] 
		\draw   (379.38,141.85) -- (379.38,144.18) -- (381.38,144.18) -- (379.38,144.18) -- (379.38,146.5) -- (379.38,144.18) -- (377.38,144.18) -- (379.38,144.18) -- cycle ;
		%Shape: Star [id:dp9605805097537827] 
		\draw   (392.56,132.35) -- (392.56,135.18) -- (394.99,135.18) -- (392.56,135.18) -- (392.56,138) -- (392.56,135.18) -- (390.13,135.18) -- (392.56,135.18) -- cycle ;
		%Shape: Star [id:dp48798648173494885] 
		\draw   (382.63,118.85) -- (382.63,122.05) -- (385.38,122.05) -- (382.63,122.05) -- (382.63,125.25) -- (382.63,122.05) -- (379.88,122.05) -- (382.63,122.05) -- cycle ;
		%Shape: Star [id:dp8444541442877695] 
		\draw   (392.48,110.6) -- (392.48,113.05) -- (394.59,113.05) -- (392.48,113.05) -- (392.48,115.5) -- (392.48,113.05) -- (390.38,113.05) -- (392.48,113.05) -- cycle ;
		%Shape: Star [id:dp8530183209728455] 
		\draw   (375.98,93.1) -- (375.98,95.55) -- (378.09,95.55) -- (375.98,95.55) -- (375.98,98) -- (375.98,95.55) -- (373.88,95.55) -- (375.98,95.55) -- cycle ;
		%Shape: Star [id:dp8980281534632217] 
		\draw   (385.98,93.35) -- (385.98,95.8) -- (388.09,95.8) -- (385.98,95.8) -- (385.98,98.25) -- (385.98,95.8) -- (383.88,95.8) -- (385.98,95.8) -- cycle ;
		%Shape: Star [id:dp7021600785967645] 
		\draw   (406.38,114.6) -- (406.38,117.51) -- (408.88,117.51) -- (406.38,117.51) -- (406.38,120.41) -- (406.38,117.51) -- (403.88,117.51) -- (406.38,117.51) -- cycle ;
		%Shape: Star [id:dp8565997468485342] 
		\draw   (409.84,97.35) -- (409.84,99.93) -- (412.06,99.93) -- (409.84,99.93) -- (409.84,102.5) -- (409.84,99.93) -- (407.63,99.93) -- (409.84,99.93) -- cycle ;
		%Shape: Star [id:dp3426709819147873] 
		\draw   (426,110.6) -- (426,113.36) -- (428.38,113.36) -- (426,113.36) -- (426,116.12) -- (426,113.36) -- (423.63,113.36) -- (426,113.36) -- cycle ;
		%Shape: Axis 2D [id:dp9178622316814513] 
		\draw  (101,403.6) -- (255.5,403.6)(111.5,273) -- (111.5,416.6) (248.5,398.6) -- (255.5,403.6) -- (248.5,408.6) (106.5,280) -- (111.5,273) -- (116.5,280)  ;
		%Shape: Star [id:dp8846253740330465] 
		\draw   (136.19,373.6) -- (136.19,376.65) -- (139.22,375.71) -- (136.19,376.65) -- (138.06,379.12) -- (136.19,376.65) -- (134.31,379.12) -- (136.19,376.65) -- (133.16,375.71) -- (136.19,376.65) -- cycle ;
		%Shape: Star [id:dp09900427695463265] 
		\draw   (126.19,369.6) -- (126.19,372.65) -- (129.22,371.71) -- (126.19,372.65) -- (128.06,375.12) -- (126.19,372.65) -- (124.31,375.12) -- (126.19,372.65) -- (123.16,371.71) -- (126.19,372.65) -- cycle ;
		%Shape: Star [id:dp2072090870447938] 
		\draw   (149.69,360.1) -- (149.69,363.15) -- (152.72,362.21) -- (149.69,363.15) -- (151.56,365.62) -- (149.69,363.15) -- (147.81,365.62) -- (149.69,363.15) -- (146.66,362.21) -- (149.69,363.15) -- cycle ;
		%Shape: Star [id:dp745641776683504] 
		\draw   (162.69,346.6) -- (162.69,349.65) -- (165.72,348.71) -- (162.69,349.65) -- (164.56,352.12) -- (162.69,349.65) -- (160.81,352.12) -- (162.69,349.65) -- (159.66,348.71) -- (162.69,349.65) -- cycle ;
		%Shape: Star [id:dp3425152135433851] 
		\draw   (142.44,351.6) -- (142.44,354.65) -- (145.47,353.71) -- (142.44,354.65) -- (144.31,357.12) -- (142.44,354.65) -- (140.56,357.12) -- (142.44,354.65) -- (139.41,353.71) -- (142.44,354.65) -- cycle ;
		%Shape: Star [id:dp15302226617030845] 
		\draw   (145.44,342.1) -- (145.44,345.15) -- (148.47,344.21) -- (145.44,345.15) -- (147.31,347.62) -- (145.44,345.15) -- (143.56,347.62) -- (145.44,345.15) -- (142.41,344.21) -- (145.44,345.15) -- cycle ;
		%Shape: Star [id:dp055845796495678446] 
		\draw   (132.19,345.85) -- (132.19,348.9) -- (135.22,347.96) -- (132.19,348.9) -- (134.06,351.37) -- (132.19,348.9) -- (130.31,351.37) -- (132.19,348.9) -- (129.16,347.96) -- (132.19,348.9) -- cycle ;
		%Shape: Star [id:dp9973149105054266] 
		\draw   (125.94,334.1) -- (125.94,337.15) -- (128.97,336.21) -- (125.94,337.15) -- (127.81,339.62) -- (125.94,337.15) -- (124.06,339.62) -- (125.94,337.15) -- (122.91,336.21) -- (125.94,337.15) -- cycle ;
		%Shape: Star [id:dp4392942792874299] 
		\draw   (142.44,324.85) -- (142.44,327.9) -- (145.47,326.96) -- (142.44,327.9) -- (144.31,330.37) -- (142.44,327.9) -- (140.56,330.37) -- (142.44,327.9) -- (139.41,326.96) -- (142.44,327.9) -- cycle ;
		%Shape: Star [id:dp16303070196902358] 
		\draw   (132.94,316.85) -- (132.94,319.9) -- (135.97,318.96) -- (132.94,319.9) -- (134.81,322.37) -- (132.94,319.9) -- (131.06,322.37) -- (132.94,319.9) -- (129.91,318.96) -- (132.94,319.9) -- cycle ;
		%Shape: Star [id:dp4616284593440907] 
		\draw   (145.69,306.85) -- (145.69,309.9) -- (148.72,308.96) -- (145.69,309.9) -- (147.56,312.37) -- (145.69,309.9) -- (143.81,312.37) -- (145.69,309.9) -- (142.66,308.96) -- (145.69,309.9) -- cycle ;
		%Shape: Circle [id:dp055465378631171625] 
		\draw   (154.72,326.92) .. controls (154.72,325.67) and (155.74,324.65) .. (157,324.65) .. controls (158.26,324.65) and (159.27,325.67) .. (159.27,326.92) .. controls (159.27,328.18) and (158.26,329.2) .. (157,329.2) .. controls (155.74,329.2) and (154.72,328.18) .. (154.72,326.92) -- cycle ;
		%Shape: Circle [id:dp9995538629542435] 
		\draw   (164.72,327.17) .. controls (164.72,325.92) and (165.74,324.9) .. (167,324.9) .. controls (168.26,324.9) and (169.27,325.92) .. (169.27,327.17) .. controls (169.27,328.43) and (168.26,329.45) .. (167,329.45) .. controls (165.74,329.45) and (164.72,328.43) .. (164.72,327.17) -- cycle ;
		%Shape: Circle [id:dp8670305219238168] 
		\draw   (188.47,331.92) .. controls (188.47,330.67) and (189.49,329.65) .. (190.75,329.65) .. controls (192.01,329.65) and (193.02,330.67) .. (193.02,331.92) .. controls (193.02,333.18) and (192.01,334.2) .. (190.75,334.2) .. controls (189.49,334.2) and (188.47,333.18) .. (188.47,331.92) -- cycle ;
		%Shape: Circle [id:dp4526664400302052] 
		\draw   (171.47,344.92) .. controls (171.47,343.67) and (172.49,342.65) .. (173.75,342.65) .. controls (175.01,342.65) and (176.02,343.67) .. (176.02,344.92) .. controls (176.02,346.18) and (175.01,347.2) .. (173.75,347.2) .. controls (172.49,347.2) and (171.47,346.18) .. (171.47,344.92) -- cycle ;
		%Shape: Circle [id:dp17856925895220077] 
		\draw   (205.22,344.92) .. controls (205.22,343.67) and (206.24,342.65) .. (207.5,342.65) .. controls (208.76,342.65) and (209.77,343.67) .. (209.77,344.92) .. controls (209.77,346.18) and (208.76,347.2) .. (207.5,347.2) .. controls (206.24,347.2) and (205.22,346.18) .. (205.22,344.92) -- cycle ;
		%Shape: Circle [id:dp9503583522123751] 
		\draw   (184.72,349.67) .. controls (184.72,348.42) and (185.74,347.4) .. (187,347.4) .. controls (188.26,347.4) and (189.27,348.42) .. (189.27,349.67) .. controls (189.27,350.93) and (188.26,351.95) .. (187,351.95) .. controls (185.74,351.95) and (184.72,350.93) .. (184.72,349.67) -- cycle ;
		%Shape: Circle [id:dp7979166594890559] 
		\draw   (144.22,371.17) .. controls (144.22,369.92) and (145.24,368.9) .. (146.5,368.9) .. controls (147.76,368.9) and (148.77,369.92) .. (148.77,371.17) .. controls (148.77,372.43) and (147.76,373.45) .. (146.5,373.45) .. controls (145.24,373.45) and (144.22,372.43) .. (144.22,371.17) -- cycle ;
		%Shape: Circle [id:dp056838535474615925] 
		\draw   (157.97,376.17) .. controls (157.97,374.92) and (158.99,373.9) .. (160.25,373.9) .. controls (161.51,373.9) and (162.52,374.92) .. (162.52,376.17) .. controls (162.52,377.43) and (161.51,378.45) .. (160.25,378.45) .. controls (158.99,378.45) and (157.97,377.43) .. (157.97,376.17) -- cycle ;
		%Shape: Circle [id:dp1660150181108473] 
		\draw   (171.22,367.42) .. controls (171.22,366.17) and (172.24,365.15) .. (173.5,365.15) .. controls (174.76,365.15) and (175.77,366.17) .. (175.77,367.42) .. controls (175.77,368.68) and (174.76,369.7) .. (173.5,369.7) .. controls (172.24,369.7) and (171.22,368.68) .. (171.22,367.42) -- cycle ;
		%Shape: Circle [id:dp6032098227168115] 
		\draw   (141.22,388.92) .. controls (141.22,387.67) and (142.24,386.65) .. (143.5,386.65) .. controls (144.76,386.65) and (145.77,387.67) .. (145.77,388.92) .. controls (145.77,390.18) and (144.76,391.2) .. (143.5,391.2) .. controls (142.24,391.2) and (141.22,390.18) .. (141.22,388.92) -- cycle ;
		%Shape: Axis 2D [id:dp5710029014880469] 
		\draw  (318,403.6) -- (472.5,403.6)(328.5,273) -- (328.5,416.6) (465.5,398.6) -- (472.5,403.6) -- (465.5,408.6) (323.5,280) -- (328.5,273) -- (333.5,280)  ;
		%Shape: Star [id:dp001073192882631302] 
		\draw   (368.25,314.6) -- (368.25,318.48) -- (371.63,318.48) -- (368.25,318.48) -- (368.25,322.35) -- (368.25,318.48) -- (364.88,318.48) -- (368.25,318.48) -- cycle ;
		%Shape: Star [id:dp9348673211504062] 
		\draw   (355.25,322.6) -- (355.25,326.85) -- (358.88,326.85) -- (355.25,326.85) -- (355.25,331.1) -- (355.25,326.85) -- (351.63,326.85) -- (355.25,326.85) -- cycle ;
		%Shape: Star [id:dp02193211509441295] 
		\draw   (364.75,331.35) -- (364.75,335.6) -- (368.38,335.6) -- (364.75,335.6) -- (364.75,339.85) -- (364.75,335.6) -- (361.13,335.6) -- (364.75,335.6) -- cycle ;
		%Shape: Star [id:dp7485768000714108] 
		\draw   (348.38,340.35) -- (348.38,344.35) -- (351.38,344.35) -- (348.38,344.35) -- (348.38,348.35) -- (348.38,344.35) -- (345.38,344.35) -- (348.38,344.35) -- cycle ;
		%Shape: Star [id:dp4320579565335967] 
		\draw   (354.88,353.35) -- (354.88,357.35) -- (357.88,357.35) -- (354.88,357.35) -- (354.88,361.35) -- (354.88,357.35) -- (351.88,357.35) -- (354.88,357.35) -- cycle ;
		%Shape: Star [id:dp5823597033378081] 
		\draw   (368,350.35) -- (368,353.85) -- (370.63,353.85) -- (368,353.85) -- (368,357.35) -- (368,353.85) -- (365.38,353.85) -- (368,353.85) -- cycle ;
		%Shape: Star [id:dp9032749071259685] 
		\draw   (364.75,358.6) -- (364.75,362.1) -- (367.38,362.1) -- (364.75,362.1) -- (364.75,365.6) -- (364.75,362.1) -- (362.13,362.1) -- (364.75,362.1) -- cycle ;
		%Shape: Star [id:dp691770048461749] 
		\draw   (347.88,375.85) -- (347.88,379.85) -- (350.88,379.85) -- (347.88,379.85) -- (347.88,383.85) -- (347.88,379.85) -- (344.88,379.85) -- (347.88,379.85) -- cycle ;
		%Shape: Star [id:dp848169989438827] 
		\draw   (358,380.1) -- (358,384.6) -- (361.13,384.6) -- (358,384.6) -- (358,389.1) -- (358,384.6) -- (354.88,384.6) -- (358,384.6) -- cycle ;
		%Shape: Star [id:dp885470335649015] 
		\draw   (364.13,391.85) -- (364.13,394.73) -- (366.13,394.73) -- (364.13,394.73) -- (364.13,397.61) -- (364.13,394.73) -- (362.13,394.73) -- (364.13,394.73) -- cycle ;
		%Shape: Star [id:dp6440138366625574] 
		\draw   (367.36,373.6) -- (367.36,376.1) -- (369.1,376.1) -- (367.36,376.1) -- (367.36,378.6) -- (367.36,376.1) -- (365.63,376.1) -- (367.36,376.1) -- cycle ;
		%Shape: Star [id:dp2836455176236905] 
		\draw   (372,367.35) -- (372,370.43) -- (374.63,370.43) -- (372,370.43) -- (372,373.5) -- (372,370.43) -- (369.38,370.43) -- (372,370.43) -- cycle ;
		%Shape: Star [id:dp4416989820632591] 
		\draw   (381.38,377.85) -- (381.38,380.18) -- (383.38,380.18) -- (381.38,380.18) -- (381.38,382.5) -- (381.38,380.18) -- (379.38,380.18) -- (381.38,380.18) -- cycle ;
		%Shape: Star [id:dp16300595561381104] 
		\draw   (394.56,368.35) -- (394.56,371.18) -- (396.99,371.18) -- (394.56,371.18) -- (394.56,374) -- (394.56,371.18) -- (392.13,371.18) -- (394.56,371.18) -- cycle ;
		%Shape: Star [id:dp6694445553175017] 
		\draw   (384.63,354.85) -- (384.63,358.05) -- (387.38,358.05) -- (384.63,358.05) -- (384.63,361.25) -- (384.63,358.05) -- (381.88,358.05) -- (384.63,358.05) -- cycle ;
		%Shape: Star [id:dp01706284588010143] 
		\draw   (394.48,346.6) -- (394.48,349.05) -- (396.59,349.05) -- (394.48,349.05) -- (394.48,351.5) -- (394.48,349.05) -- (392.38,349.05) -- (394.48,349.05) -- cycle ;
		%Shape: Star [id:dp683433056713763] 
		\draw   (377.98,329.1) -- (377.98,331.55) -- (380.09,331.55) -- (377.98,331.55) -- (377.98,334) -- (377.98,331.55) -- (375.88,331.55) -- (377.98,331.55) -- cycle ;
		%Shape: Star [id:dp47954616714673515] 
		\draw   (387.98,329.35) -- (387.98,331.8) -- (390.09,331.8) -- (387.98,331.8) -- (387.98,334.25) -- (387.98,331.8) -- (385.88,331.8) -- (387.98,331.8) -- cycle ;
		%Shape: Star [id:dp09974668069915693] 
		\draw   (408.38,350.6) -- (408.38,353.51) -- (410.88,353.51) -- (408.38,353.51) -- (408.38,356.41) -- (408.38,353.51) -- (405.88,353.51) -- (408.38,353.51) -- cycle ;
		%Shape: Star [id:dp678066588411653] 
		\draw   (411.84,333.35) -- (411.84,335.93) -- (414.06,335.93) -- (411.84,335.93) -- (411.84,338.5) -- (411.84,335.93) -- (409.63,335.93) -- (411.84,335.93) -- cycle ;
		%Shape: Star [id:dp5110200898767963] 
		\draw   (428,346.6) -- (428,349.36) -- (430.38,349.36) -- (428,349.36) -- (428,352.12) -- (428,349.36) -- (425.63,349.36) -- (428,349.36) -- cycle ;
		%Notched Right Arrow [id:dp6408482917828369] 
		\draw   (169.62,192.6) -- (169.62,244.2) -- (179.5,244.2) -- (166.75,262.6) -- (154,244.2) -- (163.88,244.2) -- (163.88,192.6) -- (166.75,198.97) -- cycle ;
		%Notched Right Arrow [id:dp8995936401665743] 
		\draw   (379.62,192.6) -- (379.62,244.2) -- (389.5,244.2) -- (376.75,262.6) -- (364,244.2) -- (373.88,244.2) -- (373.88,192.6) -- (376.75,198.97) -- cycle ;
		%Straight Lines [id:da5858230952760488] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (117.5,178.6) -- (218.5,43.6) ;
		%Shape: Rectangle [id:dp47751640362293957] 
		\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ] (341.84,308) -- (373.5,308) -- (373.5,365) -- (341.84,365) -- cycle ;
		%Shape: Rectangle [id:dp3510557057431962] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] (373.5,308) -- (431.5,308) -- (431.5,365) -- (373.5,365) -- cycle ;
		%Shape: Rectangle [id:dp7237574472671633] 
		\draw  [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ] (341.84,365) -- (401.5,365) -- (401.5,399) -- (341.84,399) -- cycle ;
		
		% Text Node
		\draw (73.5,44) node [anchor=north west][inner sep=0.75pt]   [align=left] {debt};
		% Text Node
		\draw (229,171) node [anchor=north west][inner sep=0.75pt]   [align=left] {income};
		% Text Node
		\draw (288,42) node [anchor=north west][inner sep=0.75pt]   [align=left] {debt};
		% Text Node
		\draw (444,171) node [anchor=north west][inner sep=0.75pt]   [align=left] {income};
		% Text Node
		\draw (73.5,280) node [anchor=north west][inner sep=0.75pt]   [align=left] {debt};
		% Text Node
		\draw (229,407) node [anchor=north west][inner sep=0.75pt]   [align=left] {income};
		% Text Node
		\draw (290,278) node [anchor=north west][inner sep=0.75pt]   [align=left] {debt};
		% Text Node
		\draw (446,407) node [anchor=north west][inner sep=0.75pt]   [align=left] {income};
		% Text Node
		\draw (182,204.6) node [anchor=north west][inner sep=0.75pt]   [align=left] {supervised\\learning};
		% Text Node
		\draw (392,204.6) node [anchor=north west][inner sep=0.75pt]   [align=left] {unspervised\\learning};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Supervised vs Unsupervised learning idea}
	\end{figure}
	are themselves divided into two sub-families: "\NewTerm{kernel-based}" (named also "\NewTerm{soft clustering}\index{soft clustering}" or "\NewTerm{fuzzy clustering}\index{fuzzy clustering}") or non-kernel based (named also "\NewTerm{hard clustering}\index{hard clustering}" )... (that means for the latter we don't need to make any assumptions on the statistical distribution shape of the variable of interest and that an individual can only belong to one category\footnote{Indeed, in fuzzy clustering technique like GMM (Gaussian Mixture Models) an individual has classification weights for each possible class!}).
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.48]{img/computing/data_mining_vs_machine_learning_following_sas.jpg}
		\vspace*{3mm}
		\caption[Data Mining as seen by SAS™]{Data Mining as seen by SAS™ (source: SlideShare SAS)}
	\end{figure}
	The distinction between Machine Learning and data mining has become more and more blurred, and there is a great deal of "cross-fertilization".
	
	The industrial or operational use of this knowledge in the professional world can solve very different problems, ranging from customer relationship management to preventive maintenance, through fraud detection and the optimization of websites, the supervision of financial markets, pro-active prospecting (customer consumption preferences), optimization of paths (analysis of road caps and left turns), or target selection (probability of acquiring a new given prospects) anticipate the identification of terrorists, pricing of products in comparison to equivalent one on the mark, etc.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Be careful to not to confuse strictly speaking the concepts of classification, segmentation and association! Although the first two are often confused (classification / segmentation) because many algorithms do both at once, classification is the prediction of one or more discrete variables based on the value of other fields in the data set as the segmentation divides the data into groups of items having the most identical properties as possible. About association it is clear that many segmentation algorithms shows what is associated with what but the idea strictly speaking of association is to quantified by a scalar the degree of association between two data sets.
	\end{tcolorbox}	
	We will see here some trivial techniques that we will complete with time. We tried to order them in the ascending order of pedagogical complexity (but that concept is quite subjective obviously that's why many people would argue that $k$-nn should be presented first)...
	
	\paragraph{Naive Bayes classifiers}\mbox{}\\\\
	In Machine Learning, "\NewTerm{naive Bayes classifiers}\index{naive Bayes classifiers}" are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features (properties).

	Naive Bayes has been studied extensively since the 11950s (holocene calendar). It was introduced under a different name into the text retrieval community in the early 11960s (holocene calendar), and remains a popular (baseline) method for text categorization, the problem of judging documents as belonging to one category or the other (such as spam or legitimate, sports or politics, etc.) with word frequencies as the features. With appropriate pre-processing, it is competitive in this domain with more advanced methods including support vector machines. It also finds application in automatic medical diagnosis.
	
	\subparagraph{Binomial Naive Bayes classifier}\mbox{}\\\\
	The "\NewTerm{binomial naive Bayes classifier}\index{binomial naive Bayes classifier}" consists by using Bayes probabilities to classify an item in two possible outcomes.
	
	For example, the table below consisting of $4$ e-mail, having $2$ attributes and two possible classification outcomes that are $\{\mathrm{No-Spam},\mathrm{Spam}\}$:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}\textbf{Email}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{$100\%$ Guaranteed}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Meeting}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Classification}} \\ \hline
		$1$ & $\mathrm{No}$ & $\mathrm{Yes}$ & $\mathrm{No-Spam}$ \\ \hline
		$2$ & $\mathrm{No}$ & $\mathrm{Yes}$ & $\mathrm{No-Spam}$ \\ \hline
		$3$ & $\mathrm{Yes}$ & $\mathrm{Yes}$ & $\mathrm{Spam}$ \\ \hline
		$4$ & $\mathrm{Yes}$ & $\mathrm{No}$ & $\mathrm{No-Spam}$ \\ \hline
		\end{tabular}
		\caption{Naive Bayes spam-email companion example}
	\end{table}
	Abstractly, as already mentioned naive Bayes is a conditional probability model: given a problem instance to be classified, represented by a vector $\vec{x} =(x_{1},\dots ,x_{n})$ representing some $n$ features (independent variables), it assigns a posteriori with probabilities $P(C_k/x_1,\ldots,x_n)$ to the instance $C_k$, or also written $P(C_k/\vec{x})$, for each of $k$ possible outcomes or classes $C_k$.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Don't forget as we have seen in the section Probabilities (\SeeChapter{see section Probabilities page \pageref{chain rule}}) that $P(C_k/x_1,\ldots,x_n)$ is just a convenient notation to write $P(C_k/x_1\cap\ldots\cap x_n)$. And therefore $P(\vec{x})$ is also a convenient notation for $P(x_1\cap\ldots\cap x_n)$.
	\end{tcolorbox}
	With the example above it would look like this for the first e-mail (thus $n=2$):
	
	The problem with the above formulation is that if the number of features $n$ is large or if a feature can take on a large number of values, then basing such a model on probability tables is very hard. We therefore reformulate the model to make it more tractable. Using Bayes' theorem (\SeeChapter{see section Probabilities page \pageref{bayes formula}}), the conditional probability can be decomposed as:
	
	With for recall:
	\begin{itemize}
		\item $P(C_k/\vec{x})$ is the a posteriori probability to belong the class $C_k$ knowing the features $x_i$ (probability of the hypothetical class given the evidence ie. the features)
		\item $P(C_k)$ is the a priori probability (or "marginal probability") of the class $C_k$
		\item $P(\vec{x})$, the a priori probability of the feature (it is also a marginal probability)
		\item $P(\vec{x}/C_k)$, the a posteriori probability of the features $x_i$ knowing the class $C_k$, it is also the likelihood of $Y_k$ for $\vec{x}_i$ known (probability of evidence, ie. the feature, given that the hypothetical class holds).
	\end{itemize}
	In plain English, using Bayesian probability terminology, the above relation can be written for recall as:
	
	In practice, there is interest only in the numerator of that fraction (equivalent to the joint probability), because the denominator does not depend on $C_k$ and the values of the features $x_i$ are given, so that the denominator is effectively constant whatever the $k$.

	Hence the fact that we often can see in textbooks the following relation:	
	
	\begin{theorem}
	A key idea in the NB model is the following assumption:
	
	\end{theorem}
	\begin{dem}
	This equality is derived using iteratively (don't forget that we don't care about the numerator!) the relation proved in the section of Probabilities but without the denominator (\SeeChapter{see section Probabilities page \pageref{bayes formula}}):
	
	Indeed, let us rewrite this relation more explicitly:
	
	Next we rewrite the second product term using the assumption of conditional independence:
	
	Hence reinjecting in the prior-previous relation:
	
	In a condensed form the posterior probability of belonging to $C_k$ knowing $\vec{x}$ is then given by:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	The idea now, when we have a training set of size $N$, is just to take for estimators:
	
	where obviously we define $C=C_k$ to be $1$ if $C=C_k$, $0$ otherwise. This estimator is simple the number of times that the label $C_k$ is seen in the training set!
	
	Similarly:
	
	This is a very natural estimate: we simply count the number of times label $C_k$ is seen in conjunction with $\vec{x}_i$ taking value $\vec{x}$ that we divide by the number of times the label $C_k$ is seen in total.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	We consider the following training set:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}\textbf{Email}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{$100\%$ Guaranteed}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Meeting}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Classification}} \\ \hline
		$1$ & $\mathrm{No}$ & $\mathrm{Yes}$ & $\mathrm{No-Spam}$ \\ \hline
		$2$ & $\mathrm{No}$ & $\mathrm{Yes}$ & $\mathrm{No-Spam}$ \\ \hline
		$3$ & $\mathrm{Yes}$ & $\mathrm{Yes}$ & $\mathrm{Spam}$ \\ \hline
		$4$ & $\mathrm{Yes}$ & $\mathrm{No}$ & $\mathrm{No-Spam}$ \\ \hline
		\end{tabular}
	\end{table}
	A new e-mail arrives and contains the expression "$100\%$ \textit{Guaranteed}" and the expression "\textit{Meeting}". We want to predict if it is a Spam or not?
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}\textbf{Email}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{$100\%$ Guaranteed}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Meeting}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Classification}} \\ \hline
		$5$ & $\mathrm{Yes}$ & $\mathrm{Yes}$ & ? \\ \hline
		\end{tabular}
	\end{table}
	For this purpose we will first simplify the notation by denoting the expression "$100\%$ \textit{Guaranteed}" by the letter $G$ and the expression "\textit{Meeting}" by the letter $M$.\\
	
	The posterior probabilities are in this simple case given by Bayes theorem:
	
	Hence the ratio, that eliminates (as we know) the denominator that is then useless, that must be greater that $1$ if the e-mail is a Spam:
	
	According to the assumption of the independence of conditional probabilities we can write for the first factor of the numerator and respectively for that of the denominator:
	
	So we can rewrite the ratio as:
	
	And as we have in our training set:
	
	Injecting in the previous ratio, we get:
	
	That's all for this naive Bayesian example for document classification problem.
	\end{tcolorbox}
	Statisticians are somewhat disturbed by use of the NBC (which they dub Idiot's Bayes) because the naive assumption of independence is almost always invalid in the real world.
	
	However, the method has been shown to perform surprisingly well in a wide variety of contexts.
	
	\subparagraph{Gaussian Naive Bayes classifier}\mbox{}\\\\
	When dealing with continuous data, a typical assumption is that the continuous values associated with each class are distributed according to a Gaussian distribution. For example, suppose the training data contains a continuous attribute, $x$. We first segment the data by the class, and then compute the mean $\hat{\mu}$ and standard deviation $\hat{\sigma}$ of $x$ for each class.
	
	 Let $\hat{\mu} _{k}$ be the mean of the values in $x$ associated with class $C_k$, and let $\hat{\sigma}_{k}$ be the standard deviation of the values in $x$ associated with class $C_k$.
	 
	 Suppose we have collected some new observation value $x'$. Then, the probability density of $x'$ given a class $C_{k}$, $P(x=x'/C_{k})$ , can be computed by plugging $x'$ into the equation for a Normal distribution parametrized by $\hat{\mu}_{k}$ and $\hat{\sigma}_{k}^{2}$. That is:
	 
	 And after we apply again:
	 
	 
	 \begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	We consider the following training set:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}\textbf{Person}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Height [cm]}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Weight [kg]}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Foot size [cm]}} \\ \hline
		male & $182.88$ & $81.64$ & $30.48$ \\ \hline
		male & $180.44$ & $86.18$ & $27.94$ \\ \hline
		male & $170.07$ & $77.11$ & $30.48$ \\ \hline
		male & $180.44$ & $74.84$ & $25.4$ \\ \hline
		female & $152.4$ & $45.35$ & $15.24$ \\ \hline
		female & $167.64$ & $68.03$ & $20.32$ \\ \hline
		female & $165.20$ & $58.96$ & $17.78$ \\ \hline
		female & $175.26$ & $68.03$ & $22.86$ \\ \hline
		\end{tabular}
	\end{table}
	The classifier created from the training set using a Gaussian distribution assumption would be (given variances are unbiased sample variances):
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}\textbf{Person}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\parbox{1cm}{\textbf{Mean \\Height}}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\parbox{1.3cm}{\textbf{Variance \\Height}}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\parbox{1.3cm}{\textbf{Mean\\ Weight}}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\parbox{1.3cm}{\textbf{Variance\\ Weight}}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\parbox{1.4cm}{\textbf{Mean\\ Foot size}}} &\multicolumn{1}{l|}{\cellcolor[gray]{0.75}\parbox{1.4cm}{\textbf{Variance\\ Foot size}}} \\ \hline
		male & $178.45$ & $32.59$ & $79.94$ & $25.28$ & $28.58$ & $5.91$ \\ \hline
		female & $165.13$ & $90.32$ & $60.09$ & $114.88$ & $19.05$ & $10.75$ \\ \hline
		\end{tabular}
	\end{table}
	We also know from the training set that we have equiprobable classes so:
	
	Now consider a new sample to be classified as male or female:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}\textbf{Person}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Height [cm]}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Weight [kg]}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Foot size [cm]}} \\ \hline
		? & $182.88$ & $58.96$ & $20.32$ \\ \hline
		\end{tabular}
	\end{table}
	We wish to determine which posterior is greater, male or female. For the classification as male the posterior is given according to our boxed relation above:
	
	For the classification as female the posterior is given by:
	
	 We now determine the probabilities distribution for the sex of the sample:
	 \begin{itemize}
	 	\item $P(\mathrm{male})=0.5$
	 	\item $P(\mathrm{female})=0.5$
	 	\item $P(\mathrm{height}/ {\mathrm{male}})={\dfrac {1}{\sqrt {2\pi \hat{\sigma}_{H,M}^{2}}}}\exp \left({\frac {-(182.88-\hat{\mu}_{H,M} )^{2}}{2 \hat{\sigma}^{2}_{H,M}}}\right)\cong 0.0517$
	 	\item $P(\mathrm{height}/ {\mathrm{female}})={\dfrac {1}{\sqrt {2\pi \hat{\sigma}_{H,F}^{2}}}}\exp \left({\frac {-(182.88-\hat{\mu}_{H,F} )^{2}}{2 \hat{\sigma}^{2}_{H,F}}}\right)\cong 0.0073$
	 	\item $P(\mathrm{weight}/ {\mathrm{male}})={\dfrac {1}{\sqrt {2\pi \hat{\sigma}_{W,M}^{2}}}}\exp \left({\frac {-(58.96-\hat{\mu}_{W,M} )^{2}}{2 \hat{\sigma}^{2}_{W,M}}}\right)\cong 1.3142\cdot 10^{-05}$
		\item $P(\mathrm{weight}/ {\mathrm{female}})={\dfrac {1}{\sqrt {2\pi \hat{\sigma}_{W,M}^{2}}}}\exp \left({\frac {-(58.96-\hat{\mu}_{W,M} )^{2}}{2 \hat{\sigma}^{2}_{W,M}}}\right)\cong 0.0370$
		\item $P(\mathrm{foot\,size}/ {\mathrm{male}})={\dfrac {1}{\sqrt {2\pi \hat{\sigma}_{FS,M}^{2}}}}\exp \left({\frac {-(20.32-\hat{\mu}_{FS,M} )^{2}}{2 \hat{\sigma}^{2}_{FS,M}}}\right)\cong 0.0005$
		\item $P(\mathrm{foot\,size}/ {\mathrm{female}})={\dfrac {1}{\sqrt {2\pi \hat{\sigma}_{FS,F}^{2}}}}\exp \left({\frac {-(20.32-\hat{\mu}_{FS,F} )^{2}}{2 \hat{\sigma}^{2}_{FS,F}}}\right)\cong 0.1128$
	 \end{itemize}
	 Hence:
	 
	Since posterior numerator is greater in the female case, we predict the new individual is female.\\
	
	Most software multiplyes these two probabilities by a factor $f$ such that:
	$$f\cdot (1.6986\cdot 10^{-10}+1.5233\cdot 10^{-5})=1$$
	It is easy to find that in this special case $f\cong 65646.21869$ and therefore we get:
	$$P(C_k=\mathrm{male}/\vec{x})\cong 1.11507\cdot 10^{-5}\qquad\text{and}\qquad P(C_k=\mathrm{female}/\vec{x})\cong 0.999988849$$
	\end{tcolorbox}
	
	
	\paragraph{$k$-nearest neighbours classifier ($k$-nn)}\mbox{}\\\\
	The "\NewTerm{$k$-nn}\index{$k$-nearest neighbours}" is a quite simple supervised classification technique (do not confuse with the $k$-means that is a unsupervised clustering technique and that we will see later!). It takes a bunch of labelled points and uses them to learn how to label other points.  To label a new point, it looks at the labelled points closest to that new point (those are its nearest neighbours), and has those neighbours vote, so whichever label the most of the neighbours have is the label for the new point (the "$k$" is the number of neighbours it checks).
	
	In this framework, we have a learning database consisting of $N$ input-output pairs. To estimate the output associated with a new input $x$, the $k$ nearest neighbours method consists in taking into account (identically) the $k$ learning samples whose input is closest to the new input $x$, according to a distance to be defined.

For example, in a classification problem, we will retain the class most represented among the $k$ outputs associated with the $k$ inputs closest to the new input $x$.

	Given $D$ the dataset composed of $n$ pairs $(\vec{x}, y)$, with $\vec{x}$ the description of an observation according to $d$ quantitative variables, in the form of a real vector of dimensions $d$, and $y$ the class of this observation. Consider the case where the descriptive variables are all quantitative and where the class variable have $q$ modalities:
	
	The space of representation of the observations must be provided with a distance necessary for the construction of the neighbourhood of a new observation. The distance is a function $d:\mathbb{R}^d\times\mathbb{R}^d\mapsto\mathbb{R}^+$, respecting the three axioms of the distance that we know well (\SeeChapter{see section Topology page \pageref{distance}}).
	
	The most common distances for vectors of dimension $n$ (corresponding therefore to the number the input attributes that must all belong to $\mathbb{R}$) in a table of size $N$ (be careful! we know that computer scientists use some distances that doesn't respect the four axioms of distances and especially the triangular inequality as already discussed in the section Analysis at page \pageref{common types of distances}):
	\begin{itemize}
		\item Minkowski distance:
		
		
		\item Euclidean distance:
		
	
		\item Manhattan distance (city-block):
		
	
		\item Quadratic distance ($\sigma$ is as we know a correlation matrix):
		
		as we can see it corresponds to the square of the Mahalanobis distance.
	
		\item Correlation distance:
		
	
		\item Chi-$2$ distance:
		
	
		\item Kendall rank correlation distance:
		
		with for recall:
		
	
		\item Heterogeneous Euclidean Overlap Metric (HEOM) by focusing only on continuous attributes (case of the Tanagra software):
		
		where $\max_{i,a}$ and $\min_{i,a}$ are respectively the largest and smallest value of the attribute $i$ throughout the training sample so that the term in the parenthesis is always between $0$ and $1$ (normalized in other words...).
	\end{itemize}
	The figures below illustrate how the $k$ nearest neighbours solves a binary supervised classification task ($y_i\in \{0,1\}$) in $\mathbb{R}^2$, depending on the Euclidean distance and for $k = 3$. The first figure show (in three steps, from left to right) a new observation is attributed to class $1$, its neighbour being composed of three observations belonging to class $1$. The second figure shows how a new observation is attributed to class $1$, its vicinity being composed of two observations belonging to the class $1$ and of one observation belonging to the class $0$.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/computing/knn_01.jpg}
		\caption[Classification of a new observation with $k-nn$]{Classification of a new observation with $k-nn$: the three nearest neighbours belong to class $1$}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/computing/knn_02.jpg}
		\caption[]{Classification of a new observation with $k-nn$: $2$ of the $3$ nearest neighbours belong to class $1$}
	\end{figure}
	The distance to the $k$th nearest neighbour can also be seen as a local density estimate and thus is also a popular outlier score in anomaly detection. The larger the distance to the $k$-nn, the lower the local density, the more likely the query point is an outlier. Although quite simple, this outlier model, along with another classic data mining method, local outlier factor, works quite well also in comparison to more recent and more complex approaches, according to a large scale experimental analysis.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Obviously we may run the $k$-nn on the factorial space obtained after having run a PCA (Principal Component Analysis as seen on page \pageref{principal component analysis}) on our data. We can also even reduce the dimensions using that latter if necessary!
	\end{tcolorbox}
	
	\paragraph{Regression and classification trees}\mbox{}\\\\
	"\NewTerm{Classification and Regression Tree}\index{classification and regression tree}\index{CART}" (CART) are a set of heuristic supervised algorithms introduced in 11984 (holocene calendar) by Leo Breiman, Jerome Friedman, Richard Olshen and Charles Stone as an umbrella term to refer to the following types of \NewTerm{decision trees}\index{decision trees}\footnote{A "\NewTerm{decision stump}\index{decision stump}" is a Machine Learning model consisting of a one-level decision tree}": 
	\begin{itemize}
		\item "\NewTerm{Classification Trees}\index{classification trees}": where the target variable is categorical and the tree is used to identify the "class" within which a target variable would likely fall into.

		\item "\NewTerm{Regression Trees}\index{regression trees}": where the target variable is continuous and tree is used to predict it's value.
	\end{itemize}
	Decision tree are non-linear, nonparametric and nonmetric classifier. Concretely, decision trees are predictive models that proceed through a tree structure of rules or a hierarchy of tests extracted from learning or training data, to partition the basic data into homogeneous subgroups from the point of view of the variable to predict. The generated rules tree easily lends itself to the human reasoning mode and it becomes more intuitive to interpret the tree and make predictions.
	
	These trees are widely used in advanced marketing or social science to discriminate (categorize) a very large population. Obviously, these algorithms (that are part of hierarchical techniques) will never do better than a human being (at least in this first decade of the 121st century according to holocene calendar)... but also ask an employee to create groups in a population of $5$ million customers on the basis of ten explanatory variables. You will have to wait for the response a quite long time...
	
	While these automated  classification techniques are very useful in the above situations, they nevertheless have a major problem that makes we will not focus too much on this subject:
	\begin{itemize}
		\item These techniques are very sensitive to the analysed population and give very different results.

		\item The various existing  techniques of classifications give results that are completely different for the same population.
		
		\item Decision trees can also never predict values out of the range of the targets seen in the training dataset (i.e. classical decision trees don't extrapolate, they only retrodict).
	\end{itemize}
	It is better to be cautious about the conclusions that we can draw from these models and compare the results of several methods depending on the return on experience choose the one that seems the best suited.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In good practice situations, the construction of a tree is, as always in the field of Machine Learning (...), made from a set of data named the "learning sample". Once the tree is built, it is tested for its predictive power usually on a set of data named the "test sample".
	\end{tcolorbox}

	\subparagraph{Binary classification trees with quantitative variables}\mbox{}\\\\
	For companion example, let us consider a set of categorical variables $x_1,\ldots,x_p$. The recursive partitioning has for purpose to divide the $p$ variables of the space into rectangles which do not overlap.
	
	For example, consider the variable $x_i$ and a value $s_i$ of this variable, we find that the partitioning $x_i<s_i$ and $s_i<x_i$ separates well the data into two disjoint sets. Then one of the parts is in turn divided by a value $x_i$ or by the value of another variable. We end then with three rectangles and so on...
	
	The idea is to create $n$ rectangles such that all data contained in a rectangle are homogeneous (that is to say contains only one family of points).

	To address this issue, consider the following first practical case with only quantitative variables (it is therefore a binary regression tree):
	
	A dealer would like for his city to find a way to classify the families that are able to buy a car (owners) and those that are not ready to buy a car (non-owners). A sample of $12$ owners ("$1$" in the figure below) and $12$ non-owners ("$2$" in the figure below) is selected. The two independent variables are $x_1$ (wage in kilo-dollars) and $x_2$ (area of their home). In other words, we have here a case where $x_i\in\mathbb{R}^+$ and $y_i\{1,2\}$.
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.9]{img/computing/cart_data_list_sample_excel.jpg}
		\caption[]{Original data list of CART application with Microsoft Excel 14.0.7172}
	\end{figure}
	We see that we have as many owners than non-owners (assumed equal appearance frequency in the whole population). Therefore the probability of belonging to a class is $50\%$.

	Or graphically:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/computing/cart_data_list_sample_excel_chart_0_iteration.jpg}
		\caption[]{Original  plot CART application with Microsoft Excel 14.0.7172}
	\end{figure}
	If we apply the CART algorithm on this data, we see that we must choose $x_2$ (Area) as the first choice of division with the division value of $19$ (we will justify why!). The space $(x_1,x_2)$ is now divided into two rectangles (it was easy to guess that discriminating step without even using mathematics):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/computing/cart_data_list_sample_excel_chart_1_iteration.jpg}
		\caption[]{First iteration of CART application with Microsoft Excel 14.0.7172}
	\end{figure}
	Notice how the division into two rectangles created two zones (splits) which are more homogeneous than the original graph! The upper rectangle contains points which are more Owners while the lower rectangle contains more Non-owners.

	To determine this division, the CART algorithm examines each variable and all possible values for each variable division in order to find the best division.

	Thus, the possible points of division for $x_1$ are (notice that this is every time the average of the two related values in the table):
	\begin{gather*}
		x_1:\{38.1,45.3,50.1,\ldots,109.5\}
	\end{gather*}
	and that for $x_2$ are:
	\begin{gather*}
		x_2:\{14.4,15.4,16.2,\ldots,23\}
	\end{gather*}
	These points are ordered by the algorithm according to the way they reduce "impurity" (heterogeneous of composition) in the rectangle that generates the "split".

	There are a large number of empirical ways to measure the impurity. But to start with our first example, let us use the easiest and most common "\NewTerm{impurity measurement criterion}\index{impurity measurement criterion}" or "\NewTerm{segmentation criteria}\index{segmentation criteria}" so far that is the use of an indicator inspired by the Gini coefficient (\SeeChapter{see section Quantitative Management Techniques page \pageref{gini index}}). Thus, if we denote the classes by $k=1,2,3,\ldots,C$ where $C$ is the total number of classes to be predicted, the "\NewTerm{Gini impurity index}\index{Gini impurity index}\label{Gini impurity index}" for the $A$ rectangle is defined by:
	
	where $p_k$ is the fraction of observations in the rectangle $A$ which belong to the class $k$. 

	In our example, we always have only two classes: Owners / Non-Owners.

	Next, the global Gini index is defined as the weighted average of the Gini indices.

	So in our example, we have two classes, therefore $C=2$. Before the separation, we have:
	
	The separation found in $19$ (see the second figure) gives for example for the top rectangle of the first subdivision:
	
	For the inferior part:
	
	By the hazard of the choice of this example, the impurity is the same for both rectangles (top and bottom).

	The overall Gini index is then given by:
	
	Notice before continuing that if the subdivision is perfect (only have one family of points in one of the boxes), then we have:
	
	So the impurity is zero ... And if all the points appear in equal proportions in each of the rectangles (worst situation we could say), the value is then:
	
	If we generalize to $C$ classes ($C$ spatial dimensions), it comes immediately:
	
	which is the maximum impurity.

	So the impurity is always defined by a value in the range:
	
	Now, to continue with our example, even without using a computer algorithm, without even calculating the impurity, it is relatively easy to guess which will be the next discrimination: it will be $x_1=84.75$ (Income). Which will give:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/computing/cart_data_list_sample_excel_chart_2_iteration.jpg}
		\caption[]{Second iteration of CART application with Microsoft Excel 14.0.7172}
	\end{figure}
	What was also easy to guess even without using the calculations (try your friends, you will see that very often they can find the first two discrimination).

	The impurity will be calculated in the new discriminated area by:
	
	The overall Gini index is given by:
	
	We continue, but be aware that the result is less easy to guess. 

	The majority of individuals interviewed are wrong without using the mathematical definition of impurity and intuitively wrong to propose one or more of the following discrimination:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/computing/cart_data_list_sample_excel_chart_3_iteration.jpg}
		\caption[]{Third iteration of CART application with Microsoft Excel 14.0.7172}
	\end{figure}
	with (you can do the math) a total impurity of $0.2727$. When in reality, the optimum discrimination is:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/computing/cart_data_list_sample_excel_chart_3_iteration_real.jpg}
		\caption[]{Third real iteration of CART application with Microsoft Excel 14.0.7172}
	\end{figure}
	with a total impurity of $0.2592$. Indeed:
	
	The overall Gini index is:
	
	In the next step, we have:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/computing/cart_data_list_sample_excel_chart_4_iteration.jpg}
		\caption[]{Fourth iteration of CART application with Microsoft Excel 14.0.7172}
	\end{figure}
	At the next step:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/computing/cart_data_list_sample_excel_chart_5_iteration.jpg}
		\caption[]{Fifth iteration of CART application with Microsoft Excel 14.0.7172}
	\end{figure}
	etc. until the end:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/computing/cart_data_list_sample_excel_chart_final_iteration.jpg}
		\caption[]{Final iteration of CART application with Microsoft Excel 14.0.7172}
	\end{figure}
	where each rectangle is pure (only contains data that one of the two classes).

	The reason why the method is named "Classification And Regression Tree" algorithm is that each division can be represented as the division of a node into two successor nodes. The first division is shown as a branch of the tree root node. Here, for example are the first six iterations of the algorithm (only the first six one as the page size is to small to get them all):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/computing/cart.jpg}
		\caption{CART Final result in the traditional tree form}
	\end{figure}
	If you follow the detailed steps given in our \texttt{R} companion book you will see the corresponding result that is:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.9]{img/computing/cart_r.jpg}
		\caption[]{CART Final result in the traditional tree form with \texttt{R} 3.0.2}
	\end{figure}
	And if you follow the detailed steps given in our MATLAB™ companion book you will see the corresponding result that is:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.85]{img/computing/cart_matlab.jpg}
		\caption[]{CART Final result in the traditional tree form with MATLAB™ 2013a}
	\end{figure}
	We need to be careful to pick an appropriate tree depth! Indeed:
	\begin{itemize}
		\item If the tree is too deep, we may overfit
		\item If the tree is too shallow, we may underfit
	\end{itemize}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{img/computing/decision_tree_vocabulary.jpg}
		\caption{Main vocabulary of decision trees}
	\end{figure}
	Alternative strategy is to create a very deep tree, and then to prune it.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	A common question for binary decision trees is: why not use logistic regression instead? The answer is quite simple! Decision trees helps you developing decision rules but in logistic regression you can not visualize a rule even though you can know which factor are more important! We will also use binary decision trees when there are too many variables, especially categorical ones and you have outliers in predictor variables and relationships are non-linear. And if we want probabilities for each case, we should go for logistic regression obviously!
	\end{tcolorbox}	
	
	\subparagraph{Detection of significant splits}\mbox{}\\\\
	Quite often it is necessary to measure the significance of a split in a decision tree, especially when the information gain (see further below page \pageref{information gain}) is small.

	Let $N_{A}$ and $N_{B}$ be the number of items of class $A$ and class $B$ in the parent node. Let $N_{A L}$ represent the number of class $A$ going to the left child node, $N_{B L}$ represent the number of class $B$ going to the left child node, $N_{A R}$ represent the number of class $B$ going to the right child node, and $N_{B R}$ represent the number of class $B$ going to the right child node.

	Let $p_{L}$ and $p_{B}$ denote the proportion of data going to the left and right node, respectively:
	
	The following measure computes the significance of a split. In other words, it measures how much the split deviates from what would be expected in the random data:
	
	where:
	
	We recognise here the shape of a $\chi^2$ random variable with $4$ degrees of freedom ($\chi^2_4$).
	
	If $K$ is small, the information gain from the split is not significant. If $K$ is big, it would suggest the information gain from the split is significant.
	
	\subparagraph{Classification (and clustering) validation}\mbox{}\\\\
	There exist many different clustering/classification methods, depending on the type of clusters/class sought and on the inherent data characteristics. Given the diversity of clustering/classification algorithms and their parameters it is important to develop objective approaches to assess clustering/classification results. Cluster/class validation and assessment encompasses three main tasks: "\NewTerm{clustering/class evaluation}" seeks to assess the goodness or quality of the clustering/classification, "\NewTerm{clustering/class stability}" seeks to understand the sensitivity of the clustering/class result to various algorithmic parameters, for example, the number of clusters/classes, and "\NewTerm{clustering/class tendency}" assesses the suitability of applying clustering/class in the first place, that is, whether the data has any inherent grouping structure. There are a number of validity measures and statistics that have been proposed for each of the aforementioned tasks, which can be divided into three main types:

	\begin{itemize}
		\item \textbf{External validation} measures employ criteria that are not inherent to the dataset. This can be in form of prior or expert-specified knowledge about the clusters/classes.
	
		\item \textbf{Internal validation} measures employ criteria that are derived from the data itself. For instance, we can use intracluster and intercluster distances to obtain measures of cluster compactness (e.g., how similar are the points in the same cluster?) and separation (e.g., how far apart are the points in different clusters?).
	
		\item \textbf{Relative validation} measures aim to directly compare different clusterings/classes, usually those obtained via different parameter settings for the same algorithm.
	\end{itemize}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	For the reader interested to further reading on the topic of clustering validation, we strongly recommend: \textit{DATA MINING AND ANALYSIS: Fundamental Concepts and Algorithms} (see \cite{zaki2014data}), that has excellent detailed definitions with step by step manually calculated examples from page 425 to page 462 (all chapter 17).
	\end{tcolorbox}

	To see now some other segmentation external criteria but now with qualitative variables let us consider the following first companion example based on the table below:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|}
			\hline
			\rowcolor[gray]{0.75}\textbf{Color} & \textbf{Shape} & \textbf{Size} & \textbf{Class} \\ \hline
			red (R) & square (S) & big (B) & plus  \\ \hline
			blue (B) & square (S) & big (B) & plus  \\ \hline
			red (R) & round (R) & small (S) & minus \\ \hline
			green (G) & square (S) & small (S) & minus \\ \hline
			red (R) & round (R) & big (B) & plus  \\ \hline
			green (G) & square (S) & big (B) & minus \\ \hline
		\end{tabular}
	\end{table}

	First let us do again the calculation with the Gini impurity index:
	
	where $p(i/t)$ still represents the frequency or the probability of the $i$ relatively to the partition $t$ (again, a partition is "pure" if it's Gini impurity index is equal to zero).

	As in the previous example we must afterwards calculate the overall Gini impurity index as the weighted average of the Gini impurity index of each partition following:
	
	Therefore the overall Gini index for the \textit{Color} attribute is given by (keep in mind that our dataset has $6$ rows and the numerator of each fraction represent the number of occurrences of the concerned partition!):
	\begin{gather*}
		I(\text{Color})=\dfrac{1}{6}I(B) + \dfrac{3}{6} I(R) + \dfrac{2}{6}I(G) = 0.22
	\end{gather*}
	with (keep in mind that the denominator is always the number of rows of the corresponding partition, and the numerator correspond to how many of a given \textit{Class} we have among all these rows):
	\begin{itemize}
		\item $I(B)= 1 - (0/1)^2 - (1/1)^2 = 0.0$
	    \item $I(R)= 1 - (1/3)^2 - (2/3)^2 = 0.44$
	    \item $I(G)= 1 - (2/2)^2 - (0/2)^2 = 0.0$
	\end{itemize}
	And the overall Gini index for the \textit{Shape} attribute is then given by:
	\begin{gather*}
		I(\text{Shape})=\dfrac{4}{6}I(C) + \dfrac{2}{6}I(R) = 0.5
	\end{gather*}
	with:
	\begin{itemize}
		\item $I(S)= 1 - (2/4)^2 - (2/4)^2 = 0.50$
		\item $I(C)= 1 - (1/2)^2 - (1/2)^2 = 0.50$
	\end{itemize}
	And finally the overall Gini index for the \textit{Size} attribute:
	\begin{gather*}
		I(\text{Size})=\dfrac{4}{6} I(B) + \dfrac{2}{6}I(S) = 0.25
	\end{gather*}
	with:
	\begin{itemize}
		\item $I(B)= 1 - (1/4)^2 - (3/4)^2 = 0.375$
		\item $I(S)= 1 - (2/2)^2 - (0/2)^2 = 0.0$
	\end{itemize}
	As the variables with lower Gini impurity index are the one to split, then we will chose the \textit{Color} attribute as first split in our example! So we see that in classification trees, the Gini Index can be used as "\NewTerm{variable importance}\index{variable importance}" metric for classification trees!
	
	Now let us see a new segmentation criteria named the "information gain", used by the ID3\index{ID3}, C4.5 and C5.0 tree-generation algorithms, and based on the measurement of information's entropy\footnote{ID3, C4.5 and C5.0 uses entropy information gain (IG) when CART uses the Gini impurity (see page \pageref{Gini impurity index})} from Statistical Mechanics (see page \pageref{information entropy}).
	
	For this purpose let us define the entropy for the partition $t$ by the relation (named for recall "Shannon formula"):
	
	Also denoted in some textbooks as:
	
	where $p_{C_i}$ is the probability of cluster $C_i$.
	
	Thus, similarly to the Gini impurity index, the conditional entropy for a given attribute is given by the weighted information entropies:
	
	Candidate splits are determined by looking at each variable that makes up an object belonging in a given class. In the example above all objects can either be \textit{plus} or \textit{minus}.
	
	Now for all possible split, we determine fist the entropy before the split which is found using the classification of each object (keep in mind that our dataset has $6$ rows and the numerator of each fraction represent the number of occurrences of the concerned partition!):
	\begin{gather*}
		S(\text{Class})=\dfrac{3}{6}S(\text{plus})+\dfrac{3}{6} S(\text{minus})=-\dfrac{3}{6}\log\left(\dfrac{3}{6}\right)-\dfrac{3}{6}\log\left(\dfrac{3}{6}\right)\cong 0.301
	\end{gather*}
	Now the conditional entropy for the \textit{Class} given the \textit{Color} attribute is given by : 
	\begin{gather*}
		S(C=\text{Class},a=\text{Color})=-\left(\dfrac{1}{6}S(B) +\dfrac{3}{6} S(R) +\dfrac{2}{6} S(G)\right) \cong 0.138
	\end{gather*}
	with (keep in mind that the denominator is always the number of rows of the corresponding partition, and the numerator correspond to how many of a given \textit{Class} we have among all these rows):
	\begin{itemize}
		\item $S(B) = - (1/1)\cdot\log(1/1)=0$
		\item $S(R) = - (1/3)\cdot\log(1/3) - (2/3)\cdot\log(2/3)\cong 0.276
$
		\item $S(G) = - (2/2)\cdot\log(2/2)=0$
	\end{itemize}
	The above definition of $S(C,a)$ is also sometimes defined is some textbooks with the following notation (where $\mathcal{T}$ is the partitioning):
	
	where $p_{i j}=\frac{n_{i j}}{n}$ is the probability that a point in cluster $i$ also belongs to partition $j$. The more a cluster's members are split into different partitions, the higher the conditional entropy. For a perfect clustering, the conditional entropy value is zero, whereas the worst possible conditional entropy value is $\log (k)$. Further, expanding the previous relation, we can see that:
	
	where $H(\mathcal{C}, \mathcal{T})=-\sum_{i=1}^{r} \sum_{j=1}^{k} p_{i j} \log (p_{i j})$ is the joint entropy of $\mathcal{C}$ and $\mathcal{T}$. The conditional entropy $H(\mathcal{T} \mid \mathcal{C})$ thus measures the remaining entropy of $\mathcal{T}$ given the clustering $\mathcal{C}$. In particular, $H(\mathcal{T} \mid \mathcal{C})=0$ if and only if $\mathcal{T}$ is completely determined by $\mathcal{C}$, corresponding to the ideal clustering. On the other hand, if $\mathcal{C}$ and $\mathcal{T}$ are independent of each other, then $H(\mathcal{T} \mid \mathcal{C})=H(\mathcal{T})$, which means that $\mathcal{C}$ provides no information about $\mathcal{T}$. 
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The "\NewTerm{mutual information}\index{mutual information}" tries to quantify the amount of shared information between the clustering $\mathcal{C}$ and partitioning $\mathcal{T}$, and it is defined as:
	
	It measures the dependence between the observed joint probability $p_{i j}$ of $\mathcal{C}$ and $\mathcal{T}$, and the expected joint probability $p_{C_{i}} \cdot p_{T_{j}}$ under the independence assumption. When $\mathcal{C}$ and $\mathcal{T}$ are independent then $p_{i j}=p_{C_{i}} \cdot p_{T_{j}}$, and thus $I(\mathcal{C}, \mathcal{T})=0$. However, there is no upper bound on the mutual information.\\
	
	Expanding this relation we observe that $I(\mathcal{C}, \mathcal{T})=H(\mathcal{C})+H(\mathcal{T})-H(\mathcal{C}, \mathcal{T})$. Using $H(\mathcal{T} \mid \mathcal{C})=H(\mathcal{C}, \mathcal{T})-H(\mathcal{C})$ prove just earlier, we get the two equivalent expressions:
	
	\end{tcolorbox}
	
	The "\NewTerm{information gain}\index{information gain}"\label{information gain}  (IG) is then be defined by finding the difference in the prior entropy and the conditional entropy for the class $C$ given a specific attribute variable $a$:
	
	We also recognize here that the information gain is simply the relation derived in the remark box above:
	
	Therefore in our example here the information gain for colors is:
	\begin{gather*}
		\text{IG}(\text{Class},\text{Color})=S(\text{Class})-S(\text{Class},\text{Color})\cong 0.301-0.459=-0.158
	\end{gather*}
	And now the conditional entropy for the \textit{Class} given the \textit{Shape} attribute is then given by (we change to notation for the entropy from $H$ to $S$ in the second term to avoid having to write an ugly $S(S)$...):
	\begin{gather*}
		S(\text{Class},\text{Shape})=\dfrac{4}{6} S(R) + \dfrac{2}{6} H(S) = 1
	\end{gather*}
	with:
	\begin{itemize}
		\item $S(R) = - (2/4)\cdot\log(2/4) - (2/4)\cdot\log(2/4)$
		\item $S(S)= - (1/2)\cdot\log(1/2) - (1/2)\cdot\log(1/2)$
	\end{itemize}
	Therefore the information gain for \textit{Shape} is:
	\begin{gather*}
		\text{IG}(\text{Class},\text{Shape})=S(\text{Class})-S(\text{Class},\text{Shape})\cong 0.301-1=-0.699
	\end{gather*}
	And finally the conditional entropy for the \textit{Class} given the \textit{Size} attribute is then given by:
	\begin{gather*}
		S(\text{Class},\text{Size})=\dfrac{4}{6} S(B) + \dfrac{2}{6} H(S) = 0.541
	\end{gather*}
	with:
	\begin{itemize}
		\item $S(B) = - (1/4)\cdot\log(1/4) - (3/4)\cdot\log(3/4)$
		\item $S(S) = - (2/2)\cdot\log(2/2)$
	\end{itemize}
	Therefore the information gain for sizes is:
	\begin{gather*}
		\text{IG}(\text{Class},\text{Size})=S(\text{Class})-S(\text{Class},\text{Size})\cong 0.301-0.541=-0.24
	\end{gather*}
	As the variables with lower conditional entropy (respectively highest information gain) are the one to split, then we will chose the \textit{Color} attribute as first split in our example! So here the split choice is the same as with the Gini impurity index. So we see that in classification trees, the overall information entropy can be used as "\NewTerm{variable importance}\index{variable importance}" metric for classification trees!
	
	The problem with information gain as a measure to select the attribute for partition is that in the quest of pure partition, it may select attributes that are meaningless from the Machine Learning point of view.  This drawback is due to the inherent deficiency in the measure information gain that gives preference to the attribute that can divide the parent dataset to datasets with the least amount of entropy.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Notice that there are a lot of other segmentation criterias as the likelihood-ratio chi-squared statistic, the DKM criterion, the Twoing criterion, the Orthogonal criterion (ORT), the Kolmogorov-Smirnoff criterion, the AUC-Splitting criterion, ...
	\end{tcolorbox}

	And as mentioned above, we will stop here about our study of CART because the study of the corresponding empirical variations techniques are a full time job as they are numerous (and almost none gives the same results...).
	
	\subparagraph{Random Forests}\mbox{}\\\\
	The random forest (see figure below) takes the notion of binary decision tree to the next level by combining trees with the notion of an ensemble. Thus, in ensemble terms, the standard decision trees are weak learners and the random forest is a strong learner (in the sense of a statistical crows sourcing).
	
	In other words "\NewTerm{random forests}\index{random forests}" or "\NewTerm{random decision forests}\index{random decision forests}" are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/random_forest.jpg}
		\caption[Random forest illustration]{Random Forster illustration (author: ?)}
	\end{figure}
	Here is how such a system is trained; for some number of trees:
	\begin{enumerate}
		\item Sample $N$ cases at random with replacement to create a subset of the data (see top layer of figure above). The subset should be about $66\%$ of the total set.
	
		\item At each node:
		\begin{enumerate}
			\item For some number $m$, $m$ predictor variables are selected at random from all the predictor variables.
	
			\item The predictor variable that provides the best split, according to some objective function, is used to do a binary split on that node.
	
			\item At the next node, choose another $m$ variables at random from all predictor variables and do the same.
		\end{enumerate}
	\end{enumerate}
	For prediction with a new entered input into the system, it is run down all of the trees. The result may either be an average or weighted average of all of the terminal nodes that are reached, or, in the case of categorical variables, a voting majority.
	
	Note that:
	\begin{itemize}
		\item With a large number of predictors, the eligible predictor set will be quite different from node to node.
		\item The greater the inter-tree correlation, the greater the random forest error rate, so one pressure on the model is to have the trees as uncorrelated as possible.
		\item As $m$ goes down, both inter-tree correlation and the strength of individual trees go down. So some optimal value of $m$ must be discovered.
	\end{itemize}
	Random forest runtimes are quite fast, and they are able to deal with unbalanced and missing data. Random Forest weaknesses are that when used for regression they cannot predict beyond the range in the training data, and that they may over-fit data sets that are particularly noisy. Of course, the best test of any algorithm is how well it works upon your own data set.
	
	To understand why random forest works quite good you must first remember the bias-variance tradeoff relation (see earlier above page \pageref{bias-variance tradeoff}) that is given by:
	
	So first, if the trees are sufficiently deep, they have very small bias!
	
	Now let us recall the variance of the mean of identically correlated i.i.d variables  (\SeeChapter{see section Statistics \pageref{variance mean correlated variables}}):
	
	So to improve the variance over that of a single tree, we see that de-correlation gives better accuracy as a small $\rho$ decrease the first term, but we also see that the second term decrease if the number of trees $B$ increase (independently of $\rho$)! So the idea in random forests is to improve the variance reduction of bagging by reducing the correlation between the trees, without increasing the variance too much. This is achieved in the tree-growing process through random selection of the input variables.

	\paragraph{$k$-means clustering}\label{K-means}\mbox{}\\\\
	The "\NewTerm{$k$-means}\index{$k$-means}" algorithm or "\NewTerm{mobile centers}", also sometimes named "\NewTerm{classification method with dynamic clouds}" is in statistics and Machine Learning (specifically unsupervised learning), a data partitioning algorithm, ie a method that aims to divide observations in $K$ clusters in which each observation belongs to the partition with the nearest average (mean).

	The basic steps of the algorithm are as follows:
	\begin{enumerate}
		\item We choose a partitioning into $K$ groups

		\item We generate $K$ averages (centers $c_i$) randomly

		\item The data are assigned to the group whose center is closest to them

		\item We calculated the average of each group using the affected data (new centers)

		\item We return to step 3
	\end{enumerate}	
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/k_means_iterative_algorithm.jpg}
		\caption[$k$-means iterative algorithm]{$k$-means iterative algorithm (authors: Afshine Amidi, Shervine Amidi)}
	\end{figure}
	To make more explicit the algorithm each step will be illustrated by means of the following small dataset:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}\textbf{Object}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Weight}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Length}} \\ \hline
		$\mathrm{a}$ & $12$ & $10$ \\ \hline
		$\mathrm{b}$ & $15$ & $25$ \\ \hline
		$\mathrm{c}$ & $30$ & $55$ \\ \hline
		$\mathrm{d}$ & $50$ & $100$ \\ \hline
		$\mathrm{e}$ & $35$ & $70$ \\ \hline
		$\mathrm{f}$ & $45$ & $70$ \\ \hline
		$\mathrm{g}$ & $35$ & $60$ \\ \hline
		\end{tabular}
	\end{table}
	we determine $k$ temporary centers of $k$ clusters generally by randomly dragging $k$ individuals from the database. These $k$ individuals form the center of the $k$ clusters. To achieve the first clustering, we assign each individual to the cluster of which it is closest. Here the notion of proximity is understood in a measure of the Euclidean distance:
	
	For $k = 2$, we will randomly select $k$ individuals, here the object $a$ and the object $d$ for example:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\textbf{Clusters} & \textbf{Objects} & \textbf{Centers} \\ \hline
		Cluster $c_1$ & $c_1\{\mathrm{a}\}$ & $(12,10)$ \\ \hline
		Cluster $c_2$ & $c_2\{\mathrm{d}\}$ & $(50,100)$ \\ \hline
		\end{tabular}
	\end{table}
	Now we must calculate the Euclidean distance of each individual with respect to the centers and assign each to the cluster that is closer to it:
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|c|}{\cellcolor[gray]{0.75}\textbf{Objects}} & \textbf{Distance Cluster 1} & \textbf{Distance Cluster 2} & \textbf{Affectation To} \\ \hline
		$\mathrm{a}$ & $0.00$ & $97.69$ & Cluster $c_1$ \\ \hline
		$\mathrm{b}$ & $15.30$ & $82.76$ & Cluster $c_1$ \\ \hline
		$\mathrm{c}$ & $48.47$ & $49.24$ & Cluster $c_1$ \\ \hline
		$\mathrm{d}$ & $97.69$ & $0.00$ & Cluster $c_2$ \\ \hline
		$\mathrm{e}$ & $64.26$ & $33.54$ & Cluster $c_2$ \\ \hline
		$\mathrm{f}$ & $68.48$ & $30.41$ & Cluster $c_2$ \\ \hline
		$\mathrm{g}$ & $55.04$ & $42.72$ & Cluster $c_2$ \\ \hline
		\end{tabular}
	\end{table}
	Then, we again determine the $k$ centers of the $k$ last partitions performed. The distances of each individual with respect to these centers are recalculated and each is reallocated according to the cluster that is close to it.
	
	Following the last clustering we obtain the center (point of gravity or barycenter) of each partitions like this:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\textbf{Clusters} & \textbf{Objects} & \textbf{Centers} \\ \hline
		Cluster $c_1$ & $c_1\{\mathrm{a},\mathrm{b},\mathrm{c}\}$ & $(19,30)$ \\ \hline
		Cluster $c_2$ & $c_2\{\mathrm{d},\mathrm{e},\mathrm{f},\mathrm{g}\}$ & $(41.25,75)$ \\ \hline
		\end{tabular}
	\end{table}
	Again, it is necessary to calculate the Euclidean distance of each individual with respect to the centers and reassign each to the cluster that is closer to it:
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|c|}{\cellcolor[gray]{0.75}\textbf{Objects}} & \textbf{Distance Cluster 1} & \textbf{Distance Cluster 2} & \textbf{Affectation To} \\ \hline
		$\mathrm{a}$ & $21.19$ & $71.28$ & Cluster $c_1$ \\ \hline
		$\mathrm{b}$ & $6.40$ & $56.47$ & Cluster $c_1$ \\ \hline
		$\mathrm{c}$ & $27.31$ & $22.95$ & Cluster $c_2$ \\ \hline
		$\mathrm{d}$ & $76.56$ & $26.49$ & Cluster $c_2$ \\ \hline
		$\mathrm{e}$ & $43.08$ & $8.00$ & Cluster $c_2$ \\ \hline
		$\mathrm{f}$ & $47.71$ & $6.25$ & Cluster $c_2$ \\ \hline
		$\mathrm{g}$ & $34.00$ & $16.25$ & Cluster $c_2$ \\ \hline
		\end{tabular}
	\end{table}
	We see at this level that the object $\mathrm{c}$ change the group (centroid) he belongs to before.
	
	This last step is repeated as many times as necessary until the process is stabilized. The algorithm can be stopped either:
	\begin{itemize}
		\item Because we do not observe changes anymore in the composition of each cluster after two successive iterations
		
		\item Because an empirical control criterion is verified (number of iteration fixed for example)
	\end{itemize}

	In the previous illustration, it is rather the first reason that will decide the end of the clustering. Indeed, at the 3rd iteration there is no change in the composition of the clusters.
	
	New centers following last assignment:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\textbf{Clusters} & \textbf{Objects} & \textbf{Centers} \\ \hline
		Cluster $c_1$ & $c_1\{\mathrm{a},\mathrm{b}\}$ & $(13.5,17.5)$ \\ \hline
		Cluster $c_2$ & $c_2\{\mathrm{d},\mathrm{e},\mathrm{f},\mathrm{g}\}$ & $(39,71)$ \\ \hline
		\end{tabular}
	\end{table}
	Distance calculation and reallocation:
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|c|}{\cellcolor[gray]{0.75}\textbf{Objects}} & \textbf{Distance Cluster 1} & \textbf{Distance Cluster 2} & \textbf{Affectation To} \\ \hline
		$\mathrm{a}$ & $7.65$ & $66.71$ & Cluster $c_1$ \\ \hline
		$\mathrm{b}$ & $7.65$ & $51.88$ & Cluster $c_1$ \\ \hline
		$\mathrm{c}$ & $40.97$ & $18.36$ & Cluster $c_2$ \\ \hline
		$\mathrm{d}$ & $90.21$ & $31.02$ & Cluster $c_2$ \\ \hline
		$\mathrm{e}$ & $56.73$ & $4.12$ & Cluster $c_2$ \\ \hline
		$\mathrm{f}$ & $61.22$ & $6.08$ & Cluster $c_2$ \\ \hline
		$\mathrm{g}$ & $47.63$ & $11.70$ & Cluster $c_2$ \\ \hline
		\end{tabular}
	\end{table}
	It is clear that the composition of the clusters does not change anymore and a projection of the clusters on a chart gives:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,896); %set diagram left start at 0, and has height of 896
		
		%Shape: Axis 2D [id:dp7007476827976784] 
		\draw  (86.5,361.33) -- (516.5,361.33)(101.75,105) -- (101.75,382) (509.5,356.33) -- (516.5,361.33) -- (509.5,366.33) (96.75,112) -- (101.75,105) -- (106.75,112)  ;
		%Straight Lines [id:da28124136177812753] 
		\draw    (96,322) -- (108.5,322) ;
		%Straight Lines [id:da9097311604079323] 
		\draw    (96,281.6) -- (108.5,281.6) ;
		%Straight Lines [id:da9465595479975548] 
		\draw    (96,241.2) -- (108.5,241.2) ;
		%Straight Lines [id:da09059434884124107] 
		\draw    (96,200.8) -- (108.5,200.8) ;
		%Straight Lines [id:da8619662633836778] 
		\draw    (95,160.4) -- (107.5,160.4) ;
		%Straight Lines [id:da4339177832075807] 
		\draw    (96,120) -- (108.5,120) ;
		%Straight Lines [id:da8283880843041833] 
		\draw    (168,355) -- (168,368) ;
		%Straight Lines [id:da47838272777794333] 
		\draw    (234.8,355) -- (234.8,368) ;
		%Straight Lines [id:da03383434494438875] 
		\draw    (301.6,355) -- (301.6,368) ;
		%Straight Lines [id:da5031344693818682] 
		\draw    (368.4,355) -- (368.4,368) ;
		%Straight Lines [id:da8377506981455423] 
		\draw    (435.2,355) -- (435.2,368) ;
		%Straight Lines [id:da39127334933978264] 
		\draw    (502,355) -- (502,368) ;
		%Shape: Ellipse [id:dp9856301496652415] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] (160.75,357.24) .. controls (153.15,349.65) and (160.66,329.84) .. (177.51,312.99) .. controls (194.36,296.15) and (214.17,288.65) .. (221.76,296.25) .. controls (229.36,303.84) and (221.85,323.65) .. (205,340.5) .. controls (188.15,357.34) and (168.34,364.84) .. (160.75,357.24) -- cycle ;
		%Shape: Ellipse [id:dp787023037208725] 
		\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ] (298.13,255.42) .. controls (286.92,238.78) and (310.7,203.15) .. (351.25,175.84) .. controls (391.8,148.53) and (433.75,139.89) .. (444.96,156.53) .. controls (456.16,173.17) and (432.38,208.79) .. (391.83,236.1) .. controls (351.29,263.41) and (309.33,272.06) .. (298.13,255.42) -- cycle ;
		%Shape: Circle [id:dp42741930893726643] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (197,312.5) .. controls (197,310.01) and (199.01,308) .. (201.5,308) .. controls (203.99,308) and (206,310.01) .. (206,312.5) .. controls (206,314.99) and (203.99,317) .. (201.5,317) .. controls (199.01,317) and (197,314.99) .. (197,312.5) -- cycle ;
		%Shape: Circle [id:dp06037039747588846] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (176,341.5) .. controls (176,339.01) and (178.01,337) .. (180.5,337) .. controls (182.99,337) and (185,339.01) .. (185,341.5) .. controls (185,343.99) and (182.99,346) .. (180.5,346) .. controls (178.01,346) and (176,343.99) .. (176,341.5) -- cycle ;
		%Shape: Circle [id:dp4465604769802156] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (297,250.5) .. controls (297,248.01) and (299.01,246) .. (301.5,246) .. controls (303.99,246) and (306,248.01) .. (306,250.5) .. controls (306,252.99) and (303.99,255) .. (301.5,255) .. controls (299.01,255) and (297,252.99) .. (297,250.5) -- cycle ;
		%Shape: Circle [id:dp26469219089954654] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (330,240.5) .. controls (330,238.01) and (332.01,236) .. (334.5,236) .. controls (336.99,236) and (339,238.01) .. (339,240.5) .. controls (339,242.99) and (336.99,245) .. (334.5,245) .. controls (332.01,245) and (330,242.99) .. (330,240.5) -- cycle ;
		%Shape: Circle [id:dp32917830958531624] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (331,219.5) .. controls (331,217.01) and (333.01,215) .. (335.5,215) .. controls (337.99,215) and (340,217.01) .. (340,219.5) .. controls (340,221.99) and (337.99,224) .. (335.5,224) .. controls (333.01,224) and (331,221.99) .. (331,219.5) -- cycle ;
		%Shape: Circle [id:dp7949649812819299] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (397,220.5) .. controls (397,218.01) and (399.01,216) .. (401.5,216) .. controls (403.99,216) and (406,218.01) .. (406,220.5) .. controls (406,222.99) and (403.99,225) .. (401.5,225) .. controls (399.01,225) and (397,222.99) .. (397,220.5) -- cycle ;
		%Shape: Circle [id:dp4291973224738277] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (430,160.5) .. controls (430,158.01) and (432.01,156) .. (434.5,156) .. controls (436.99,156) and (439,158.01) .. (439,160.5) .. controls (439,162.99) and (436.99,165) .. (434.5,165) .. controls (432.01,165) and (430,162.99) .. (430,160.5) -- cycle ;
		%Shape: Cross [id:dp014106860134407873] 
		\draw   (191.25,320) -- (191.25,320) -- (191.25,326.25) -- (197.5,326.25) -- (197.5,326.75) -- (191.25,326.75) -- (191.25,333) -- (191.25,333) -- (191.25,326.75) -- (185,326.75) -- (185,326.25) -- (191.25,326.25) -- cycle ;
		%Shape: Cross [id:dp602536104485037] 
		\draw   (362.25,211) -- (362.25,211) -- (362.25,217.25) -- (368.5,217.25) -- (368.5,217.75) -- (362.25,217.75) -- (362.25,224) -- (362.25,224) -- (362.25,217.75) -- (356,217.75) -- (356,217.25) -- (362.25,217.25) -- cycle ;
		%Straight Lines [id:da1949571894910691] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ]   (130,112) -- (146.5,112) ;
		%Straight Lines [id:da27914772102941] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (130,140) -- (146.5,140) ;
		
		% Text Node
		\draw (86,367.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (278,393) node [anchor=north west][inner sep=0.75pt]   [align=left] {Weight};
		% Text Node
		\draw (47.5,269.5) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Length};
		% Text Node
		\draw (75,311.4) node [anchor=north west][inner sep=0.75pt]    {$20$};
		% Text Node
		\draw (75,272.4) node [anchor=north west][inner sep=0.75pt]    {$40$};
		% Text Node
		\draw (75,231.4) node [anchor=north west][inner sep=0.75pt]    {$60$};
		% Text Node
		\draw (75,190.4) node [anchor=north west][inner sep=0.75pt]    {$80$};
		% Text Node
		\draw (67,152.4) node [anchor=north west][inner sep=0.75pt]    {$100$};
		% Text Node
		\draw (67,109.4) node [anchor=north west][inner sep=0.75pt]    {$120$};
		% Text Node
		\draw (159,373.9) node [anchor=north west][inner sep=0.75pt]    {$10$};
		% Text Node
		\draw (225,373.9) node [anchor=north west][inner sep=0.75pt]    {$20$};
		% Text Node
		\draw (293,373.9) node [anchor=north west][inner sep=0.75pt]    {$30$};
		% Text Node
		\draw (360,373.9) node [anchor=north west][inner sep=0.75pt]    {$40$};
		% Text Node
		\draw (427,373.9) node [anchor=north west][inner sep=0.75pt]    {$50$};
		% Text Node
		\draw (491,373.9) node [anchor=north west][inner sep=0.75pt]    {$60$};
		% Text Node
		\draw (152,104) node [anchor=north west][inner sep=0.75pt]   [align=left] {Cluster 1};
		% Text Node
		\draw (152,132) node [anchor=north west][inner sep=0.75pt]   [align=left] {Cluster 2};
		\end{tikzpicture}
	\end{figure}
	
	So we see with this small previous example that our problem is the same as minimizing the enlarged criterion:
	
	over bother clusterings $C$ and $c_1,\ldots,c_K\in\mathbb{R}^p$. That is to say, minimize the "within-cluster sum of squares" (WCSS).
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	$k$-means is as we have already mentioned it a hard assignment version of the soft assignment variant of EM (see above page \pageref{EM algorithm}), with the assumptions that clusters are spherical (plus all variables must have same variance and the prior probability for all clusters are the same, ie each cluster has roughly equal number of observations). Here "spherical" means identical variance-covariance matrices for each cluster (assuming gaussian distribution), which is also known as "model-based clustering". Furthermore notice that there is no "$k$-means algorithm". There is MacQueens algorithm for $k$-means, the Lloyd/Forgy algorithm for $k$-means, the Hartigan-Wong method, ...\\
	
	The procedure describe above is the Lloyds algorithm (the one assumed by default in most cheap books on Data Science, it is sometimes also referred to as "naive $k$-means", because there exist much faster alternatives) that consists of two step given a set of $k$-means $m_1^{(1)},\ldots,m_k^{(1)}$ and therefore of $S = \{S_1, S_2, \ldots, S_k\}$ clusters:
	\begin{itemize}
		\item the E-step (assignment step), where each object is assigned to the centroid such that it is assigned to the most likely cluster using the shortest euclidean distance):
		
		
		\item the M-step (update step), where the model (=centroids) are recomputed (= least squares optimization):
		
	\end{itemize}
	... iterating these two steps, as done by Lloyd, makes this effectively an instance of the general EM scheme.
	\end{tcolorbox}
	
	We see also well through the previous step-by-step example that the $k$-means algorithm (which therefore belongs to non-hierarchical clustering techniques) however does not necessarily converge to an optimal solution. Let us recall that a global optimization calculation is inconsistent with the data volumes used, regardless of the power of computers. Thus, the $k$-means will use iterative algorithms to reach a local optimum. It is also an algorithm that will try to find the best $K$ initial points.

	Some software gives you the ability to set the initial values and this will affect the final quality of the typology, knowing that there is ONE no good initial choice. This varies depending on the configuration of data, on the return of experience (REX) and even the chance...
	
	There are three common solutions:
	\begin{enumerate}
		\item The software determines the $K$ initial points randomly. It can perform a number of tests and he will choose the most conclusive one.

		\item We use the expert opinion that suppose someone has a fairly good knowledge of the study population for attaching to each class an ideal type. This may or not be a real individual.

		\item The software distribute the $K$ initial points not randomly but according to some empirical algorithms.
	\end{enumerate}

	Let us see a further step that is instructive to show how to implement this technique in a spreadsheet software like Microsoft Excel 14.0.6123 with genetic algorithms (see further below).
	
	For this, we will first consider the following structure for which the idea is to establish three centers (so it is a $3$-means):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.74]{img/computing/kmeans_initial_sheet_values_and_chart_excel.jpg}
		\caption[]{Basic starting $k$-means in Microsoft Excel 14.0.6123}
	\end{figure}
	where we have on the left data from a population-based on the characteristics $X$ and $Y$ with a small table that will display the coordinates of the three centroid. We create on the same sheet the following table:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.74]{img/computing/kmeans_initial_sheet_point_cluster_association_excel.jpg}
		\caption[]{Initial points-centroids $k$-means association in Microsoft Excel 14.0.6123}
	\end{figure}
	with trivial formulas for the three columns \texttt{N}, \texttt{O}, \texttt{P}, where we used the standard Euclidean distance:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{img/computing/kmeans_initial_sheet_point_cluster_association_excel_explicit_formulas.jpg}
		\caption[]{Initial points-centroids $k$-means association explicit formulas in Microsoft Excel 14.0.6123}
	\end{figure}
	Then we launch the Microsoft Excel 14.0.6123 Solver  with the following parameters being careful to take the Evolutionary algorithm option. Therefore we assume that at every run we could have a different results:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/computing/kmeans_solver_settings_excel.jpg}
		\caption[]{$k$-means solver settings in Microsoft Excel 14.0.6123}
	\end{figure}
	and therefore we get for results:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.57]{img/computing/kmeans_final_sheet_point_cluster_association_excel.jpg}
		\caption[]{$k$-means final associations in Microsoft Excel 14.0.6123}
	\end{figure}
	A software like Minitab 15.1.1 gives us other values for the centroids. As the latter don't give any plots let us see how the associations looks like if we write manually the centroid values given by Minitab in our Microsoft Excel sheet:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.55]{img/computing/kmeans_final_point_cluster_association_minitab.jpg}
		\caption[]{$k$-means final associations with Minitab 15.1.1 values}
	\end{figure}
	The huge difference between Microsoft Excel and any other statistical software is quite simple to explain! Software such as Microsoft Excel 14.0.6123 minimizes the distance points to the centers but is unable simultaneously to maximize the distance between the centers. Again the statistical software have algorithms implemented for this purpose. More in details the idea is the following:
	
	The quality/assessment of a clustering\footnote{In an unsupervised learning setting, it is often hard to assess the performance of a model since we don't have the ground truth labels as was the case in the supervised learning setting.} is many times measured as following:
	\begin{itemize}
		\item We use the "\NewTerm{cluster cohesion}", also named "with sum of squares", already introduced earlier (we just generalize the notation without expliciting the type of distance/metric):
		
		In practice the purpose is to minimize it.
	
		\item We introduce the "\NewTerm{cluster separation}", also named "between sum of squares", defined by:
		
		where $n_k$ is the number of points in cluster number $k$ and $C$ is the barycenter of all clusters. In practice the purpose is to maximize it.
	\end{itemize}
	Using the properties of the barycenter (\SeeChapter{see section Geometry page \pageref{barycenter}}), we can then write the total sum of squares:
	
	That latter relation is also often written as:
	
	with the following companion illustration:
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/kmean_wss_bss_v2.jpg}
		\caption[]{$k$-means BSS and WSS}
	\end{figure}
	So in practice, the $k$-means method is fully encompassed based on the above fundamental relation. It is then quite easy to understand that if Intra-cluster inertia decreases the inter-cluster inertia increases and vice versa.
	
	Therefore with Tanagra 1.4.44 we get for example:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/kmeans_final_point_cluster_association_tanagra.jpg}
		\caption[]{$k$-means final associations in Tanagra 1.4.44}
	\end{figure}
	and using the detailed steps given in our \texttt{R} companion book we get:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/kmeans_final_point_cluster_association_plot1_r.jpg}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/kmeans_final_point_cluster_association_plot2_r.jpg}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/kmeans_final_point_cluster_association_plot3_r.jpg}
		\caption[]{$k$-means final associations in \texttt{R} 3.0.2}
	\end{figure}
	A major difficulty with the $k$-means is to choose the number of clusters. For this let us recall the fundamental relation of $k$-means:
	
	The strategy commonly chosen to choose the right $K$ is to make vary WSS as a function of $k$ and to take the last of $k$ which induces a considerable reduction of WSS. Geometrically speaking, at this optimal point, we observe a kind of "elbow" (or "knee") forming on the plot of $\text{WSS}=f(K)$:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/computing/kmeans_clusters_dependance_wss.jpg}
	\end{figure}
	In the case of our first $k$-means clustering, the partition in $K=2$ is more interesting, because not only does it induce a considerable gain in information (according to the figure above), thus a significant loss or reduction of WSS, but also it offers clusters having more than $2$ individuals. Beyond this, for example, for $K=3$, the gain in information is no longer significant and we will obtain a cluster with only $1$ individual (not very relevant for a group analysis...).
	
	In addition to homogeneity of the observations within each group, we also seek heterogeneity of the groups; hence, intuitively, an $F$ -like statistic as a ratio of the between-group dissimilarity to the within-group dissimilarity could be used to indicate the goodness of a given clustering. We define a "\NewTerm{pseudo $F$}":
	
	This measure is also named the "\NewTerm{Calinski-Harabasz index}\index{Calinski-Harabasz index}". The larger that ratio, the better the clustering at any fixed number of clusters.

	A practical approach to the problem is to vary $K$, as $K_{0}, K_{0}+1, \ldots$, computing $\widetilde{F}_{K}$, which will initially increase fairly rapidly, and to choose the value of $K$ as the point at which $\widetilde{F}_{K+1}-\widetilde{F}_{K}$ is relatively small. This is similar to the type of approach in a different context; that is, choosing the number of principal components.

	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} In some applications we want each center to be one of the point itself. This is where "\NewTerm{$K$-medoids}\index{$K$-medoids}" (partition around medoids) comes in an algorithm similar to the $k$-means algorithm, except when fitting the centers $c_1,\ldots,c_K$, we restrict our attention to the point themselves. Generally $K$-medoids obviously return a higher value of $K_{\text{means}}$. There is also on example in our \texttt{R} companion book.\\
	
	\textbf{R2.} Obviously as for the $k$-nn, we may run the $k$-means on the factorial space obtained after having run a PCA (Principal Component Analysis as seen on page \pageref{principal component analysis}) on our data. We can also even reduce the dimensions using that latter if necessary!
	\end{tcolorbox}
	A very common simple assessment metric is the "\NewTerm{silhouette coefficient}\index{silhouette coefficient}" given as graphical output in most statistical software and defined as given an observation $\vec{x} \in \mathcal{D}$ by:
	
	where $a(\vec{x})$ is the average distance of $\vec{x}$ to all other elements in the cluster it belongs to and $b(\vec{x})$ is the smallest value that could take $a(\vec{x})$, if $a$ belonged to another cluster:
	
	The silhouette coefficient of $\vec{x}$ is all the closer to $1$ as its assignment to the cluster $\mathcal{C}_{k(\vec{x})}$ is satisfactory.
	
	The "global silhouette coefficient" of the clustering is the average of all silhouette coefficients:
	
	
	\paragraph{Support Vector Machines (SVM) classifier}\label{support vector machines}\mbox{}\\\\
	"\NewTerm{Support vector machines}\index{support vector machines}" (SVM) is learning method used for binary classification\footnote{In its most simple type, SVM doesn't support multiclass classification natively. It supports binary classification and separating data points into two classes. For multiclass classification, the same principle is utilized after breaking down the multiclassification problem into multiple binary classification problems.} that searches for so-named "\NewTerm{support vectors}" which are observations that are found to lie at the edge of an area in space which presents a boundary between one of these classes of observations (e.g., the squares) and another class of observations (e.g., the circles). SVM seems to have been introduced by Vladimir Vapnik and colleagues. The earliest mention was in (Vapnik, 11979 according to holocene calendar), but the first main paper seems to be (Vapnik, 11995 according to holocene calendar). In the terminology of SVM we talk about the space between these two regions as the "margin" between the classes. Each region contains observations with the same value for the target variable (i.e., the class). The support vectors, and only the support vectors, are used to identify a hyperplane (a straight line in two dimensions) that separates the classes. The maximum margin between the separable classes is sought. This then represents the model. 
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,896); %set diagram left start at 0, and has height of 896
		
		%Shape: Rectangle [id:dp624062917004164] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 249; blue, 212 }  ,fill opacity=1 ] (102,355.92) -- (441.15,97.94) -- (532.05,159.55) -- (192.9,417.54) -- cycle ;
		%Shape: Axis 2D [id:dp29267008097142466] 
		\draw  (120.5,355) -- (541.5,355)(138.5,67) -- (138.5,366) (534.5,350) -- (541.5,355) -- (534.5,360) (133.5,74) -- (138.5,67) -- (143.5,74)  ;
		%Straight Lines [id:da044102010466228636] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (104.5,353) -- (439.5,98) ;
		%Straight Lines [id:da8131913061356342] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (194.5,416) -- (528.5,162) ;
		%Straight Lines [id:da6469082805732667] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (150,386) -- (485.5,129) ;
		%Straight Lines [id:da005921304455923648] 
		\draw    (261,300) -- (240.65,280.1) ;
		\draw [shift={(238.5,278)}, rotate = 44.36] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Shape: Circle [id:dp8289929976053334] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (181,134.5) .. controls (181,129.25) and (185.25,125) .. (190.5,125) .. controls (195.75,125) and (200,129.25) .. (200,134.5) .. controls (200,139.75) and (195.75,144) .. (190.5,144) .. controls (185.25,144) and (181,139.75) .. (181,134.5) -- cycle ;
		%Shape: Circle [id:dp01303649227546999] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (176,178.5) .. controls (176,173.25) and (180.25,169) .. (185.5,169) .. controls (190.75,169) and (195,173.25) .. (195,178.5) .. controls (195,183.75) and (190.75,188) .. (185.5,188) .. controls (180.25,188) and (176,183.75) .. (176,178.5) -- cycle ;
		%Shape: Circle [id:dp8402135879105455] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (177,235.5) .. controls (177,230.25) and (181.25,226) .. (186.5,226) .. controls (191.75,226) and (196,230.25) .. (196,235.5) .. controls (196,240.75) and (191.75,245) .. (186.5,245) .. controls (181.25,245) and (177,240.75) .. (177,235.5) -- cycle ;
		%Shape: Circle [id:dp6361291151540152] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (243,243.5) .. controls (243,238.25) and (247.25,234) .. (252.5,234) .. controls (257.75,234) and (262,238.25) .. (262,243.5) .. controls (262,248.75) and (257.75,253) .. (252.5,253) .. controls (247.25,253) and (243,248.75) .. (243,243.5) -- cycle ;
		%Shape: Circle [id:dp7777037107371234] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (225,202.5) .. controls (225,197.25) and (229.25,193) .. (234.5,193) .. controls (239.75,193) and (244,197.25) .. (244,202.5) .. controls (244,207.75) and (239.75,212) .. (234.5,212) .. controls (229.25,212) and (225,207.75) .. (225,202.5) -- cycle ;
		%Shape: Circle [id:dp8969364623389571] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (222,171.5) .. controls (222,166.25) and (226.25,162) .. (231.5,162) .. controls (236.75,162) and (241,166.25) .. (241,171.5) .. controls (241,176.75) and (236.75,181) .. (231.5,181) .. controls (226.25,181) and (222,176.75) .. (222,171.5) -- cycle ;
		%Shape: Circle [id:dp5584171672792166] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (267,165.5) .. controls (267,160.25) and (271.25,156) .. (276.5,156) .. controls (281.75,156) and (286,160.25) .. (286,165.5) .. controls (286,170.75) and (281.75,175) .. (276.5,175) .. controls (271.25,175) and (267,170.75) .. (267,165.5) -- cycle ;
		%Shape: Circle [id:dp7949139720001899] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (311,188.5) .. controls (311,183.25) and (315.25,179) .. (320.5,179) .. controls (325.75,179) and (330,183.25) .. (330,188.5) .. controls (330,193.75) and (325.75,198) .. (320.5,198) .. controls (315.25,198) and (311,193.75) .. (311,188.5) -- cycle ;
		%Shape: Circle [id:dp37725223572677047] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (303,133.5) .. controls (303,128.25) and (307.25,124) .. (312.5,124) .. controls (317.75,124) and (322,128.25) .. (322,133.5) .. controls (322,138.75) and (317.75,143) .. (312.5,143) .. controls (307.25,143) and (303,138.75) .. (303,133.5) -- cycle ;
		%Shape: Circle [id:dp21702322884720004] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (266,115.5) .. controls (266,110.25) and (270.25,106) .. (275.5,106) .. controls (280.75,106) and (285,110.25) .. (285,115.5) .. controls (285,120.75) and (280.75,125) .. (275.5,125) .. controls (270.25,125) and (266,120.75) .. (266,115.5) -- cycle ;
		%Shape: Circle [id:dp5000898839307757] 
		\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (382,265.5) .. controls (382,260.25) and (386.25,256) .. (391.5,256) .. controls (396.75,256) and (401,260.25) .. (401,265.5) .. controls (401,270.75) and (396.75,275) .. (391.5,275) .. controls (386.25,275) and (382,270.75) .. (382,265.5) -- cycle ;
		%Shape: Circle [id:dp17026597655093179] 
		\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (419,283.5) .. controls (419,278.25) and (423.25,274) .. (428.5,274) .. controls (433.75,274) and (438,278.25) .. (438,283.5) .. controls (438,288.75) and (433.75,293) .. (428.5,293) .. controls (423.25,293) and (419,288.75) .. (419,283.5) -- cycle ;
		%Shape: Circle [id:dp04172125482188882] 
		\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (464,262.5) .. controls (464,257.25) and (468.25,253) .. (473.5,253) .. controls (478.75,253) and (483,257.25) .. (483,262.5) .. controls (483,267.75) and (478.75,272) .. (473.5,272) .. controls (468.25,272) and (464,267.75) .. (464,262.5) -- cycle ;
		%Shape: Circle [id:dp7921527739808276] 
		\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (512,266.5) .. controls (512,261.25) and (516.25,257) .. (521.5,257) .. controls (526.75,257) and (531,261.25) .. (531,266.5) .. controls (531,271.75) and (526.75,276) .. (521.5,276) .. controls (516.25,276) and (512,271.75) .. (512,266.5) -- cycle ;
		%Shape: Circle [id:dp24812453185456085] 
		\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (493,329.5) .. controls (493,324.25) and (497.25,320) .. (502.5,320) .. controls (507.75,320) and (512,324.25) .. (512,329.5) .. controls (512,334.75) and (507.75,339) .. (502.5,339) .. controls (497.25,339) and (493,334.75) .. (493,329.5) -- cycle ;
		%Shape: Circle [id:dp6252031106754774] 
		\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (464,304.5) .. controls (464,299.25) and (468.25,295) .. (473.5,295) .. controls (478.75,295) and (483,299.25) .. (483,304.5) .. controls (483,309.75) and (478.75,314) .. (473.5,314) .. controls (468.25,314) and (464,309.75) .. (464,304.5) -- cycle ;
		%Shape: Circle [id:dp3555023884560702] 
		\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (414,329.5) .. controls (414,324.25) and (418.25,320) .. (423.5,320) .. controls (428.75,320) and (433,324.25) .. (433,329.5) .. controls (433,334.75) and (428.75,339) .. (423.5,339) .. controls (418.25,339) and (414,334.75) .. (414,329.5) -- cycle ;
		%Shape: Circle [id:dp19982074009386963] 
		\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (382,309.5) .. controls (382,304.25) and (386.25,300) .. (391.5,300) .. controls (396.75,300) and (401,304.25) .. (401,309.5) .. controls (401,314.75) and (396.75,319) .. (391.5,319) .. controls (386.25,319) and (382,314.75) .. (382,309.5) -- cycle ;
		%Shape: Circle [id:dp27315140162154905] 
		\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (340,336.5) .. controls (340,331.25) and (344.25,327) .. (349.5,327) .. controls (354.75,327) and (359,331.25) .. (359,336.5) .. controls (359,341.75) and (354.75,346) .. (349.5,346) .. controls (344.25,346) and (340,341.75) .. (340,336.5) -- cycle ;
		%Straight Lines [id:da12756645058768368] 
		\draw    (140,354) -- (122.5,367) ;
		%Straight Lines [id:da7569141822499712] 
		\draw    (127.62,368.78) -- (146.38,382.42) ;
		\draw [shift={(148,383.6)}, rotate = 216.03] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(126,367.6)}, rotate = 36.03] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da8952579739299011] 
		\draw    (528.5,120) -- (546.9,133.8) ;
		\draw [shift={(548.5,135)}, rotate = 216.87] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da545768963388144] 
		\draw    (492.5,94) -- (470.13,78.16) ;
		\draw [shift={(468.5,77)}, rotate = 35.31] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		
		% Text Node
		\draw (112,58.4) node [anchor=north west][inner sep=0.75pt]    {$x_{2}$};
		% Text Node
		\draw (538,364.4) node [anchor=north west][inner sep=0.75pt]    {$x_{1}$};
		% Text Node
		\draw (249.1,282.99) node [anchor=north west][inner sep=0.75pt]  [rotate=-319.59]  {$\vec{w}$};
		% Text Node
		\draw (381.1,168.68) node [anchor=north west][inner sep=0.75pt]  [rotate=-321.95]  {$\textcolor[rgb]{0.82,0.01,0.11}{\vec{w}}\textcolor[rgb]{0.82,0.01,0.11}{^{T}}\textcolor[rgb]{0.82,0.01,0.11}{\vec{x}}\textcolor[rgb]{0.82,0.01,0.11}{-b=0}$};
		% Text Node
		\draw (333.1,139.68) node [anchor=north west][inner sep=0.75pt]  [rotate=-321.95]  {$\textcolor[rgb]{0.29,0.56,0.89}{\vec{w}}\textcolor[rgb]{0.29,0.56,0.89}{^{T}}\textcolor[rgb]{0.29,0.56,0.89}{\vec{x}}\textcolor[rgb]{0.29,0.56,0.89}{-b=+1}$};
		% Text Node
		\draw (447.1,226.68) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,opacity=1 ,rotate=-321.95]  {$\textcolor[rgb]{0.49,0.83,0.13}{\vec{w}}\textcolor[rgb]{0.49,0.83,0.13}{^{T}}\textcolor[rgb]{0.49,0.83,0.13}{\vec{x}}\textcolor[rgb]{0.49,0.83,0.13}{-b=-1}$};
		% Text Node
		\draw (118.06,371.56) node [anchor=north west][inner sep=0.75pt]  [rotate=-35.79]  {$\dfrac{b}{\| \vec{w} \| }$};
		% Text Node
		\draw (510.46,80.16) node [anchor=north west][inner sep=0.75pt]  [rotate=-36.22]  {$\dfrac{2}{\| \vec{w} \| }$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[Support Vector Machine (SVM) idea]{SVM idea (source: Wikipedia)}
	\end{figure}
	It is usually quite rare that we can separate the data with a straight line (or a hyperplane when we have more than two input variables). That is, the data is not usually distributed in such a way that it is "\NewTerm{linearly separable}". When this is the case, a technique is used to combine (or remap) the data in different ways, creating new variables so that the classes are then more likely to become linearly separable by a hyperplane (i.e., so that with the new dimensional data there is a gap between observations in the two classes). We can use the model we have built to score new observations by mapping the data in the same way as when the model was built, and then decide on which side of the hyperplane the observation lies and hence the decision associated with it.
	
	Support vector machines have been found to perform well on problems that are non-linear, sparse, and high-dimensional. A disadvantage is that the algorithm is sensitive to the choice of tuning option (e.g., the type of transformations to perform), making it harder to use and time consuming to identify the best model. Another disadvantage is that the transformations performed can be computationally expensive and are performed both whilst building the model and when scoring new data.
	
	An advantage of the method is that the modelling only deals with
these support vectors rather than the whole training dataset, and so the size of the training set is not usually an issue. Also, as a consequence of only using the support vectors to build a model, the model is less affected by outliers.

	Ok let us first introduce the idea in a gently way...:

	Let $\mathcal{D}=\left\{\left(\vec{x}_{i}, y_{i}\right)\right\}_{i=1}^{n}$ be a classification dataset, with $n$ points in a $d$-dimensional space. Further, let us assume that there are only two class labels, that is, $y_{i} \in\{+1,-1\}$ denoting the positive and negative classes.
	
	A hyperplane in $d$ dimensions is given as the set of all points $\vec{x} \in \mathbb{R}^{d}$ that satisfy the equation $h(\vec{x})=0$, where $h(\vec{x})$ is the hyperplane function, defined as follows (\SeeChapter{see section Analytical Geometry page \pageref{equation of the plane}}):
	
	Here, $\vec{w}$ is a dimensional weight vector and $b$ is a scalar, named  without surprise the "\NewTerm{bias}\index{bias}". For points that lie on the hyperplane, we have:
	
	The hyperplane is thus defined as the set of all points such that $\vec{w}^{T} \vec x=-b$. To see the role played by $b$, assuming that $w_{1} \neq 0$, and setting $x_{i}=0$ for all $i>1$, we can obtain the offset where the hyperplane intersects the first axis. Therefore we have obviously:
	
	In other words, the point:
	
	lies on the hyperplane (and it's module is equal to the perpendicular distance from the hyperplane to the origin!). In a similar manner, we can obtain the offset where the hyperplane intersects each of the axes, which is given as $-b/w_{i}$ (provided $\left.w_{i} \neq 0\right)$.
	
	A hyperplane splits the original $d$ -dimensional space into two half-spaces (remember the first figure above depicting the idea!). A dataset is said to be "\NewTerm{linearly separable}" if each half-space has points only from a single class. If the input dataset is linearly separable, then we can find a separating hyperplane $h(\vec{x})=0$, such that for all points labelled $y_{i}=-1$, we have $h\left(\vec{x}_{i}\right)<0$, and for all points labelled $y_{i}=+1$, we have $h\left(\vec{x}_{i}\right)>0$. In fact, the hyperplane function $h(\vec{x})$ serves as a linear classifier or a linear discriminant, which predicts the class $y$ for any given point $x$, according to the decision rule:
	
	Let $\vec{a}_{1}$ and $\vec{a}_{2}$ be two arbitrary points that lie on the hyperplane. From the hyperplane equation we have:
	
	Subtracting one from the other we obtain:
	
	This means without surprise (as we also proved it in the section of Analytical Geometry) that the weight vector $\vec{w}$ is orthogonal to the hyperplane because it is orthogonal to any arbitrary vector $\left(\vec{a}_{1}-\vec{a}_{2}\right)$ on the hyperplane. In other words, the weight vector $\vec{w}$ specifies the direction that is normal to the hyperplane, which fixes the orientation of the hyperplane, whereas the bias $b$ fixes the offset of the hyperplane in the $d$-dimensional space. Because both $\vec{w}$ and $-\vec{w}$ are normal to the hyperplane, we remove this ambiguity by requiring that $h\left(\vec{x}_{i}\right)>0$ when $y_{i}=1$, and $h\left(\vec{x}_{i}\right)<0$ when $y_{i}=-1$.
	
	Ok now that we have finished with the gentle introduction let's go more in the maths with a text completely inspired from \cite{fletcher2009support}!
	
	We are given $l$ training examples $\{ \vec{x} _i , y_i\}, \ i = 1,\ldots, L \ $, where each example has $d$ inputs ($\vec{x}_i \in \mathbb{R}^d$), and a class label with one of two values $y_i \in \{-1,1\}$.
	
	Now, all hyperplanes in $\mathbb{R}^d$ are parametrized by a vector ($\vec{w}$), and a constant ($b$), expressed in the equation:
	
	(recall again that $\vec{w}$ is in fact the vector orthogonal to the hyperplane). Given such a hyperplane ($\vec{w}$,$b$) that separates the data, this gives the
function:
	
	which correctly classifies the training data (and hopefully other "testing" data it hasn't seen yet).
	
	However, a given hyperplane represented by ($\vec{w}$,$b$) is equally expressed by all pairs $\{\lambda \vec{w}, \lambda b \}$ for $\lambda \in \mathbb{R}^+$.  So we define the "\NewTerm{canonical hyperplane}\index{canonical hyperplane}" to be that which separates the data from the hyperplane by a "distance" of at least\footnote{In fact, we require that at least one example on both sides has a distance of \textit{exactly} $1$.  Thus, for a given hyperplane, the scaling (the $\lambda$) is implicitly set.} $1$.  That is, we consider those that satisfy:
	
	or more compactly:
	
	That we often write as:
	
	All such hyperplanes have a "functional distance" $\ge 1$ (quite literally, the function's value is $\ge 1$).  This shouldn't be confused with the "geometric" or "Euclidean distance" (also known as the "\text{margin}").  For a given hyperplane
($\vec{w}$,$b$), all pairs $\{\lambda \vec{w}, \lambda b \}$ define the exact same hyperplane, but each has a different functional distance to a given data point.

	To obtain the geometric distance from the hyperplane to a data point, we must normalize by the magnitude of $\vec{w}$.  This distance is simply:
	
	Intuitively, we want the hyperplane that maximizes the geometric distance to the closest data points.
	
	As simple vector geometry shows that the margin is equal to $1/\|\vec{w}\|$ and maximizing it subject to the constraint in:
	
	is equivalent to find:
	
	Minimize $\|\vec{w}\|$ is equivalent to minimize $\frac{1}{2}\|\vec{w}\|^{2}$ and the use of this term makes it possible to perform Nonlinear Programming (see page \pageref{nonlinear optimization}) optimization later on. We therefore need to find:
	
	In order to cater for the constraints in this minimization, we need to allocate them Lagrange multipliers (see page \pageref{Lagrange multipliers method}) $\alpha$, where\footnote{These conditions are necessary for the primal optimization problem $L_P$ to be strictly equal to the dual problem $L_D$, such that $L_P=L_D$ and not just equal or less such that $L_D\leq L_P$.} $\alpha_{i} \geq 0\;\forall_{i}$:
	
	We wish to find the $\vec{w}$ and $b$ which minimizes, and the $\vec \alpha$ which maximizes the above relation (whilst keeping $\alpha_{i} \geq 0\; \forall_{i}$). We can do this by differentiating $L_{P}$ with respect to $\vec{w}$ and $b$ and setting the derivatives to zero:
	
	Substituting these two relations into:
	
	 gives a new formulation which, being dependent on $\vec \alpha,$ we need to maximize:
	
	This new formulation $L_{D}$ is referred to as we know to as the "Dual form" of the Primary $L_{P}$. And the corresponding constraint (conditions) are named the "\NewTerm{Karush-Kuhn-Tucker conditions}\index{Karush-Kuhn-Tucker conditions}" (there exist other possible conditions but there are out of the scope of this book). It is worth noting that the Dual form requires only the dot product of each input vector $\vec x_{i}$ to be calculated, this is important for the Kernel Trick described further below.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The matrix $H$ above is often written $H_{i j} \equiv y_{i} y_{j} k(\vec{x}_{i} ,\vec{x}_{j})$ where $k(\vec{x}_{i} ,\vec{x}_{j})$ is an example of a family of functions named "\NewTerm{kernel functions}\index{kernel functions}". The special case $k(\vec{x}_{i} ,\vec{x}_{j})=\vec{x}_{i} \circ \vec{x}_{j}$ is known as a the "\NewTerm{linear kernel}".\\
	
	The reason that this kernel trick is useful is that there are many classification/regression problems that are not linearly separable/regressable in the space of the inputs $\vec{x}$, which might be in a higher dimensionality feature space given a suitable mapping $\vec{x}\mapsto \Phi(\vec{x})$. For example the "radial kernel":
	$$k\left(\vec{x}_{i}, \vec{x}_{j}\right)=e^{-\left(\frac{\left\|\vec{x}_{i}-\vec{x}_{j}\right\|^{2}}{2 \sigma^{2}}\right)}$$
	Other popular kernels (among many others) for classification and regression are the "polynomial kernel":
	$$k\left(\vec{x}_{i}, \vec{x}_{j}\right)=\left(\vec{x}_{i} \circ \vec{x}_{j}+a\right)^{b}$$
	and the "sigmoidal kernel":
	$$k\left(\vec{x}_{i}, \vec{x}_{j}\right)=\tanh \left(a \vec{x}_{i} \circ \vec{x}_{j}-b\right)$$
	where $a$ and $b$ are parameters defining the kernel's behaviour.
	\end{tcolorbox}
	
	Having moved from minimizing $L_{P}$ to maximizing $L_{D},$ we need to find:
	
	This is a convex quadratic optimization problem, and we run a solver which will return $\vec \alpha$ and from:
	
	will give us $\vec{w}$. What remains is to calculate $b$.
	
	Any data point satisfying:
	
	which is a support vector denoted $\vec x_{s}$ (samples that are on the gutter) will have the form:
	
	If we inject into it the previous derived relation:
	
	we get:
	
	Where $S$ denotes the set of indices of the support vectors. $S$ is determined by finding the indices $i$ where $\alpha_{i}>0$. Multiplying through by $y_{s}$ and then using $y_{s}^{2}=1$ we have :
	
	Therefore:
	
	Instead of using an arbitrary support vector $\mathrm{x}_{s}$, it is better to take an average over all of the support vectors in $S$ :
	
	We now have the variables $\vec{w}$ and $b$ that define our separating hyperplane's optimal orientation and hence our Support Vector Machines.
	
	So to summarize...  In order to use a SVM to solve a linearly separable, binary classification problem we need to:
	\begin{itemize}
		\item Create the matrix ${H}$, where $H_{i j}=y_{i} y_{j} \vec{x}_{i} \circ \vec{x}_{j}$

		\item Find $\vec \alpha$ so that:
		
		is maximized, subject to the constraints:
		
		This is done using a nonlinear programming solver.
		
		\item Calculate:
		

		\item Determine the set of support vectors $S$ by finding the indices such that $\alpha_{i}>0$
		
		\item Calculate:
		

		\item Each new point $\vec{x}^{\prime}$ is classified by evaluating:
		
	\end{itemize}

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	We recommend the reader interested in delving deeper into the subject to read the article \textit{Support Vector Machines Explained} from Tristan Fletcher \cite{fletcher2009support} which was used to write the above presentation. You will then see how we can extend the SVM methodology to handle data that is not fully
linearly separable or what are the maths behind regression SVM.
	\end{tcolorbox}
	
	
	\paragraph{Gaussian Mixture Model (GMM) clustering}\label{Gaussian mixture model}\mbox{}\\\\
	A more formal approach to clustering can be achieved if we assume underlying distributions of the observations. This is also known as "\NewTerm{finite mixture model clustering}\index{finite mixture model clustering}". Observations arise from a distribution that is a mixture of two or more components, or clusters. Each cluster is described by a density and has an associated probability or weight in the mixture.
	
	We can think of building a "\NewTerm{Gaussian Mixture Model}\index{Gaussian Mixture Model}" as a type of clustering algorithm. Using again the expectation-maximization algorithm seen earlier above (see page \pageref{EM algorithm}), the process and result is very similar to $k$-means clustering. The difference is that the clusters are assumed to each have an independent Gaussian distribution, each with their own mean and covariance matrix.
	
	When performing $k$-means clustering, we assign points to clusters using the straight Euclidean distance. The Euclidean distance is a poor metric, however, when the cluster contains significant covariance! The Gaussian Mixture Models approach will take cluster covariance into account when forming the clusters.

	Another important difference with $k$-means is that standard $k$-means performs a hard assignment of data points to clusters, ie each point is assigned to the closest cluster. With Gaussian Mixture Models, what we will end up is a collection of independent multivariate Gaussian distributions, and so for each data point, we will have a probability that it belongs to each of these distributions / clusters.
	
	Suppose we are given a data set $D = \{\vec{x}_1,\ldots,\vec{x}_N\}$ where $\vec{x}_i$ is a $d$-dimensional vector measurement. Let us assume that the points are generated in an independent and identically distributed way from an underlying density $P(\vec{x})$.
	
	We further assume that $P(\vec{x})$ is defined as a finite mixture model with $K$ components:
	
	where:
	\begin{itemize}
		\item The $P_k(\vec{x} | z_k, \theta_k)$ are mixture components, $1 \le k \le K$. Each is a density or distribution defined over $P(\vec{x})$, with parameters $\theta_k$. 
	
		\item $\vec{z} = (z_1,\ldots,z_K)$ is a vector of $K$ binary indicator variables that are mutually exclusive and exhaustive (i.e., one and only one of the $z_k$'s is equal to $1$, and the others are $0$). $\vec{z}$ is a $K$-ary random variable representing the identity of the mixture component that generated $\vec{x}$. It is convenient for mixture models to represent $z$ as a vector of $K$ indicator variables.
		
		\item The $\alpha_k = P(z_k)$ are the mixture weights, representing the probability that a randomly selected $\vec{x}$ was generated by component $k$, where $\sum_{k=1}^K \alpha_k = 1$.
	\end{itemize}   
	The complete set of parameters for a mixture model with $K$ components is:
	
	 We can compute the "membership weight" (i.e. hidden posterior) of data point $\vec{x}_i$ in cluster $k$, given  parameters $\Theta$ as:
	
	This follows from a direct application of Bayes rule often written as:
	
	The membership weights above reflect our uncertainty, given $\vec{x}_i$ and $\Theta$, about which of the $K$ components generated vector $\vec{x}_i$. Note that we are assuming in our generative mixture model that each $\vec{x}_i$ was generated by a single component so these probabilities reflect our uncertainty about which component $\vec{x}_i$ came from, not any "mixing" in the generative process.

	For $\vec{x} \in {\mathbb{R}}^d$ we can define a Gaussian mixture model by making each of the $K$ components  a Gaussian density with parameters $\vec{\mu}_k$ and $\Sigma_k$.
	
	Each component (cluster) is a multivariate Gaussian density given for recall by:
	
	with its own parameters $\theta_k = \{\vec{\mu}_k, \Sigma_k\}$.

	We use the Expectation-Maximization algorithm for Gaussian mixtures as follows: The algorithm is an iterative algorithm that starts from some initial estimate of $\Theta$ (e.g., random), and then proceeds to iteratively update $\Theta$ until convergence is detected. Each iteration consists of an E-step and a M-step.
	
	To kickstart the EM algorithm, we randomly select data points to use as the initial means, and we set the covariance matrix for each cluster to be equal to the covariance of the full training set. Also, we give each cluster equal prior probability. A cluster's prior probability is just the fraction of  the dataset that belongs to each cluster. We start by assuming the dataset is equally divided between the clusters.
	
	In other words:
	\begin{itemize}
		\item In the "Expectation" step (E-step), we will calculate the (posterior) probability that each data point $\vec{x}^{i}$ belongs to each cluster $k$ (using our current estimated mean vectors and covariance matrices) following:
				
		This seems analogous to the cluster assignment step in $k$-means. Technically this step denote the current parameter values as $\Theta$.  It computes the $w_{i,k}$ (using the equation above for membership weights) for all data points $\vec{x}_i, 1 \le i \le N$ and all mixture components $1 \le k \le K$. Note that for each data point $\vec{x}_i$ the membership weights are defined such that $\sum_{k=1}^K w_{i,k} = 1$. This yields an $N \times K$ matrix of membership weights, where each of the rows sum to $1$.

		\item In the "Maximization" step (M-step), we will re-calculate the cluster means and covariances based on the probabilities calculated in the expectation step (i.e. we use the membership weights and the data to calculate new parameter values). This seems analogous to the cluster movement step in $k$-means.
	\end{itemize}

	Let $N_k = \sum_{i=1}^N w_{i,k}$, i.e., the sum of the membership weights for the $k$th component---this is the effective number of data points assigned to component $k$.

	Specifically:
	
	These are the new mixture weights:
	
	The updated mean is calculated in a manner similar to how we could compute a standard empirical average, except that the $i$th data vector $\vec{x}_i$ has a fractional weight $w_{i,k}$.

	Note that this is a vector equation since $\vec{\mu}_k^{\text{new}}$ and $\vec{x}_i$ are both $d$-dimensional vectors:
	
	Again we get an equation that is similar in shape to how we would normally compute an empirical covariance matrix, except that the contribution of each data point is weighted by $w_{i,k}$. Note that this is a matrix equation of dimensionality $d \times d$ on each side.

	The  equations in the M-step need to be computed in this order, i.e., first compute the $K$ new $\alpha$'s, then the $K$ new $\vec{\mu}_k$'s, and finally the $K$ new $\Sigma_k$'s.

	After we have computed all of the new parameters, the M-step is complete and we can now go back and recompute the membership weights in the E-step, then recompute the parameters again in the E-step, and continue updating the parameters in this manner. Each pair of E and M steps is considered to be one iteration.

	The EM algorithm can be started by either initializing the algorithm with a set of initial parameters and then conducting an E-step, or by starting with a set of initial weights and then doing a first M-step. The initial parameters or weights can be chosen randomly (e.g. select $K$ random data points as initial means and select the covariance matrix of the whole data set for each of the initial $K$ covariance matrices) or could be chosen via some heuristic method (such as by using the $k$-means algorithm to cluster the data first and then defining weights based on $k$-means memberships).
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/expectation_maximization_algorithm.jpg}
		\caption[Expectation-Maximization iterative algorithm]{Expectation-Maximization iterative algorithm (authors: Afshine Amidi, Shervine Amidi)}
	\end{figure} 
	Convergence is generally detected by computing the value of the log-likelihood after each iteration and halting when it appears not to be changing in a significant manner from one iteration to the next. Note that the log-likelihood (under the IID assumption) is defined as follows for this case:
	
	where $P_k(\vec{x}_i | z_k, \theta_k)$ is the Gaussian density for the $k$th mixture component. 
	
	Explicitly the complete data log-likelihood over all points $\{ ( \vec{x}_n, \vec{z}_n) \}_{n=1}^N$ is:
	
	We can now detail the Expectation-Maximization algorithm for Gaussian mixtures:
	\begin{itemize}
		\item \textbf{E-Step:}
		
		Before the E-step, we have an estimate $\theta_t$ of the parameters, and seek to compute a new lower bound on the observed log-likelihood.  Earlier, we showed that the optimal lower bound is (see page \pageref{evidence lower bound}):
	    \begin{equation}
	    	L(q_{\theta_t}, \theta) = \text{E}_q[ \ln\left( p(\vec{x},\vec{z}|\theta)\right)] + c^{te}
	    \end{equation}
		where $q_{\theta_t}(z) \equiv p(\vec{z}|\vec{x},\theta_t)$ and the second term is constant with respect to $\theta$. The E-Step requires us to derive an expression for the first term. Using the relation above:
		
		the expected complete data log-likelihood is given by:
		
		where $r_{nk}:= P(z_n = k \mid x_n, \theta_t)$ is named the "responsability" that cluster $k$ takes for data point $x_n$ after step $t$.  During the E-Step, we compute these values explicitly with the relation seen earlier above:
		
		
		\item \textbf{ M-Step:}
		
		During the M-Step, we optimize our lower bound with respect to the parameters $\theta = (\vec\alpha, \vec\mu, \vec\Sigma)$.  For the mixing weights $\vec\pi$, we use Lagrange multipliers to maximize the evidence lower bound (see page \pageref{evidence lower bound}) subject to the constraint $\sum_{k=1}^K \pi_k = 1$.  The Lagrangian (see page  \pageref{Lagrange multipliers method}) is:
	    
	    with for recall:
	    
	    So let us derive the four corresponding results... In order to resolve this problem, we have to find the parameters for which the partial derivatives are null:
	    \begin{equation}
			\begin{aligned}
			\max _{\Theta, \lambda} L(\theta, r_{nk}, \lambda) & \Leftrightarrow \nabla_{\theta, \lambda} L(\theta_t, r_{nk}, \lambda)=0  \Leftrightarrow\left\{\begin{array}{l}
			\dfrac{\partial L(\theta_t, r_{nk}, \lambda)}{\partial \alpha_{k}}=0 \\
			\dfrac{\partial L(\theta_t, r_{nk}, \lambda)}{\partial \mu_{k}}=0 \\
			\dfrac{\partial L(\theta_t, r_{nk}, \lambda)}{\partial \Sigma_{k}}=0 \\
			\dfrac{\partial L(\theta_t, r_{nk}, \lambda)}{\partial \lambda}=0
			\end{array}\right.
			\end{aligned}
		\end{equation}
		First, let's start with the equation with respect to $\lambda$:
		\begin{equation}
			\begin{aligned}
			\frac{\partial \mathcal{L}(\theta, r_{nk}, \lambda)}{\partial \lambda}=0 \Leftrightarrow & \frac{\partial}{\partial \lambda}\left[\left(\sum_{n=1}^N \sum_{k=1}^{K} r_{nk} \ln\left(\alpha_{k} \mathcal{N}\left(x_{n} \mid \mu_{k}, \Sigma_{k}\right)\right)\right)-\lambda\left(\sum_{k=1}^{K} \alpha_{k}-1\right)\right]=0 \\
			& \frac{\partial}{\partial \lambda}\left[-\lambda \sum_{k=1}^{K} \alpha_{k}+\lambda\right]=0 \\
			&-\sum_{k=1}^{K} \alpha_{k}+1=0 \\
			& \sum_{k=1}^{K} \alpha_{k}=1
			\end{aligned}
		\end{equation}
		Ok, so we are back to the definition of the constraint. No surprise here but not very useful.
		
		Now let's move on with the equation with respect to $\alpha_k$. Note that in the following derivations, we use the fact that deriving an $\alpha_j$ with $j$ different than $j$ results in a constant (this basically means that the summation over the $K$ clusters can be ignored):
		\begin{equation}
			\begin{aligned}
			\frac{\partial L(\theta_t, r_{nk}, \lambda)}{\partial \alpha_{k}}=0 \quad \Leftrightarrow & \frac{\partial}{\partial \alpha_{k}}\left[\left(\sum_{n=1}^N \sum_{k=1}^{K}r_{nk} \ln \left(\alpha_{k} \mathcal{N}\left(x_{n} \mid \mu_{k}, \Sigma_{k}\right)\right)\right)-\lambda\left(\sum_{k=1}^{K} \alpha_{k}-1\right)\right]=0 \\
			& \frac{\partial}{\partial \alpha_{k}}\left[\left(\sum_{n=1}^K r_{nk}\ln \left(\alpha_{k} \mathcal{N}\left(x_{n} \mid \mu_{k}, \Sigma_{k}\right)\right)\right)-\lambda \alpha_{k}\right]=0 \\
			& \frac{\partial}{\partial \alpha_{k}}\left[\left(\sum_{n=1}^N r_{nk} \ln \left(\alpha_{k}\right)+\sum_{n=1}^N r_{nk} \ln \left(\mathcal{N}\left(x_{n} \mid \mu_{k}, \Sigma_{k}\right)\right)\right]-\lambda \alpha_{k}\right]=0 \\
			& \frac{\partial}{\partial \alpha_{k}}\left[\sum_{n=1}^N r_{nk} \ln \left(\alpha_{k}\right)-\lambda \alpha_{k}\right]=0 \\
			& \frac{\sum_{n=1}^N r_{nk}}{\alpha_{k}}-\lambda=0 \\
			& \alpha_k=\frac{\sum_{n=1}^N  r_{nk}}{\lambda}
			\end{aligned}
		\end{equation}
		Now in order to get rid of the $\lambda$, we use the definition of the constraint regarding the mixtures weights:
		\begin{equation}
			\begin{aligned}
			\sum_{k=1}^K\alpha_k=1 \Leftrightarrow &\sum_{k=1}^K\dfrac{\sum_{n=1}^N  r_{nk}}{\lambda}=1\\
			&\dfrac{\sum_{k=1}^K\sum_{n=1}^N  r_{nk}}{\lambda}=1\\
			&\lambda=\sum_{k=1}^K\sum_{n=1}^N  r_{nk}
			\end{aligned}
		\end{equation} 
		And this gives us the final closed-form expressions for the mixture weights:
		\begin{equation}
			 \alpha_k=\frac{\sum_{n=1}^N  r_{nk}}{\sum_{k=1}^K\sum_{n=1}^N  r_{nk}}=\frac{\sum_{n=1}^N  r_{nk}}{N}
		\end{equation} 
		This tells us that for each class, after the M-Step, the mixture weight will be the sum of all the individual weights for that class normalized by the sum of all the individual weights for all the classes. And this makes perfect sense, if all the observations put a little weight on a specific class compared to the other classes than this class will have a small overall weight, and vice-versa.
		
		Now let's move on the derivation of the mean vector $\mu$. This one is a little trickier because, this time, we are computing the partial derivative for vectors and matrices:
		\begin{equation}
			\begin{aligned}
			\frac{\partial L(\theta_t, r_{nk}, \lambda)}{\partial \mu_{k}}=0 \Leftrightarrow & \frac{\partial}{\partial \mu_{k}}\left[\left(\sum_{n=1}^N \sum_{k=1}^{K} r_{nk} \ln \left(\alpha_{k} \mathcal{N}\left(x_{n} \mid \mu_{k}, \Sigma_{k}\right)\right)\right)-\lambda\left(\sum_{k=1}^{K} \alpha_{k}-1\right)\right]=0 \\
			& \frac{\partial}{\partial \mu_{k}}\left[\sum_{n=1}^N r_{nk} \ln \left(\alpha_{k}\right)+\sum_{n=1}^N r_{nk} \ln \left(\mathcal{N}\left(x_{n} \mid \mu_{k}, \Sigma_{k}\right)\right)\right]=0 \\
			& \frac{\partial}{\partial \mu_{k}}\left[\sum_{n=1}^N r_{nk} \ln \left(\frac{1}{(2 \pi)^{(N / 2)}\left|\Sigma_{k}\right|^{1 / 2}} \exp \left(-\frac{1}{2}\left(x_{n}-\mu_{k}\right)^{T} \Sigma_{k}^{-1}\left(x_{n}-\mu_{k}\right)\right)\right]\right]=0 \\
			& \frac{\partial}{\partial \mu_{k}}	\sum_{n= 1}^N r_{nk} \ln \left(\frac{1}{(2 \pi)^{(N / 2)}\left|\Sigma_{k}\right|^{1 / 2}}\right)+\sum_{n=1}^N r_{nk}\ln \left(\exp \left(-\frac{1}{2}\left(x_{n}-\mu_{k}\right)^{T} \Sigma_{k}^{-1}\left(x_{n}-\mu_{k}\right)\right)\right)]=0 \\
			&\frac{\partial}{\partial \mu_{k}}\left[\sum_{n=1}^N r_{nk}\left(-\frac{1}{2}\left(x_{n}-\mu_{k}\right)^{T} \Sigma_{k}^{-1}\left(x_{n}-\mu_{k}\right)\right)\right]=0
			\end{aligned}
		\end{equation}
		Hence:
		\begin{equation}
			\begin{aligned}
			&\sum_{n=1}^N r_{nk}\left(-\frac{1}{2}\left(-2 \Sigma_{k}^{-1}\left(x_{n}-\mu_{k}\right)\right)\right)=0 \\
			&\sum_{n=1}^N r_{nk} \Sigma_{k}^{-1}\left(x_{n}-\mu_{k}\right)=0 \\
			&\sum_{n=1}^N r_{nk} \Sigma_{k}^{-1} x_{n}-\sum_{n=1}^N r_{nk} \Sigma_{k}^{-1} \mu_{k}=0
			\end{aligned}
		\end{equation}
		So finally:
		\begin{equation}
			\mu_{k}=\frac{\sum_{n=1}^N r_{nk} \Sigma_{k}^{-1} x_{n}}{\sum_{i} r_{nk} \Sigma_{jk}^{-1}} =\frac{\sum_{n=1}^N r_{nk} x_{n}}{\sum_{n=1}^N r_{nk}}
		\end{equation}
		This looks perfectly right. Out of the M-Step, the mean value for a specific Gaussian distribution will be the expected value of the data points with respect to the variational distribution $r_{nk}$ normalized by the sum of all the weights.
		
		Finally, let's derive the analytical update expression for the covariance matrix $\Sigma$ (we use in this development derivatives of matrices as seen in the section of Linear Algebra page \pageref{derivative of logarithm of a determinant}):
		\begin{equation}
			\begin{aligned}
			\frac{\partial L(\theta_t, r_{nk}, \lambda)}{\partial \Sigma_{k}}=0 \quad \Leftrightarrow & \frac{\partial}{\partial \Sigma_{k}}\left[\left(\sum_{n=1}^N \sum_{k=1}^{K} r_{nk} \ln \left(\alpha_{k} \mathcal{N}\left(x_{n} \mid \mu_{k}, \Sigma_{k}\right)\right)\right)-\lambda\left(\sum_{k=1}^{K} \alpha_{k}-1\right)\right]=\mathds{O} \\
			& \frac{\partial}{\partial \Sigma_{k}}\left[\sum_{n=1}^N r_{nk} \ln \left(\mathcal{N}\left(x_{n} \mid \mu_{k}, \Sigma_{k}\right)\right)\right]=\mathds{O} \\
			& \frac{\partial}{\partial \Sigma_{k}}\left[\sum_{n=1}^N r_{nk} \ln \left(\frac{1}{(2 \pi)^{(N / 2)}\left|\Sigma_{k}\right|^{1 / 2}} \exp \left(-\frac{1}{2}\left(x_{n}-\mu_{k}\right)^{T} \Sigma_{k}^{-1}\left(x_{n}-\mu_{k}\right)\right)\right]\right]=\mathds{O}\\
			& \frac{\partial}{\partial \Sigma_{k}}\left[\sum_{n=1}^N r_{nk}\left(\ln \left(\frac{1}{(2 \pi)^{(N / 2)}}\right)+\frac{1}{2} \ln \left(\frac{1}{\left|\Sigma_{k}\right|}\right)+\ln\left(\exp \left(-\frac{1}{2}\left(x_{n}-\mu_{k}\right)^{T} \Sigma_{k}^{-1}\left(x_{n}-\mu_{k}\right)\right)\right)\right)\right]=\mathds{O}\\
			& \frac{\partial}{\partial \Sigma_{k}}\left[\sum_{n=1}^N r_{nk}\left(-\frac{1}{2} \ln \left(\left|\Sigma_{k}\right|\right)-\frac{1}{2}\left(x_{n}-\mu_{k}\right)^{T} \Sigma_{k}^{-1}\left(x_{n}-\mu_{k}\right)\right)\right]=\mathds{O}\\
			& \frac{\partial}{\partial \Sigma_{k}}\left[\sum_{n=1}^N r_{nk}\left(\ln \left(\left|\Sigma_{k}\right|\right)+\left(x_{n}-\mu_{k}\right)^{T} \Sigma_{k}^{-1}\left(x_{n}-\mu_{k}\right)\right)\right]=\mathds{O}
			\end{aligned}
		\end{equation}
		Therefore:
		
		Hence:
		
		Finally:
		
		And this looks pretty dam right! The covariance matrix after the M-Step is a re-weighted version of the previous covariance matrix.
	\end{itemize}

	Most algorithms searches over a range of different types of Gaussians and $n$ (named sometimes the "number of components") to find the best model by the Bayesian information criterion (BIC). The model is often referred to a three letter code nomenclature from the variance-covariance matrix describing the "shape" (relative magnitude of eigenvalues categorized into $2$ different possibilities), "volume" (absolute magnitude of eigenvalues categorized into $3$ different possibilities) and "orientation" (orientation of the eigenvectors categorized into $4$ different possibilities) of the clusters. So there is a total of $2\cdot 3\cdot 4=24$ possibilities, here is a non-exhaustive list:
	\begin{itemize}
		\item \texttt{EII}: equal volume, round shape (spherical covariance)
		\item \texttt{VII}: varying volume, round shape (spherical covariance)
		\item \texttt{EEI}: equal volume, equal shape, axis parallel orientation (diagonal covariance)
		\item \texttt{VEI}: varying volume, equal shape, axis parallel orientation (diagonal covariance)
		\item \texttt{EVI}: equal volume, varying shape, axis parallel orientation (diagonal covariance)
		\item \texttt{VVI}: varying volume, varying shape, equal orientation (diagonal covariance)
		\item \texttt{EEE}: equal volume, equal shape, equal orientation (ellipsoidal covariance)
		\item \texttt{EEV}: equal volume, equal shape, varying orientation (ellipsoidal covariance)
		\item \texttt{VEV}: varying volume, equal shape, varying orientation (ellipsoidal covariance)
		\item \texttt{VVV}: varying volume, varying shape, varying orientation (ellipsoidal covariance)
	\end{itemize}
	Some of the above combinations are illustrated below: 
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1215); %set diagram left start at 0, and has height of 1215
		
		%Shape: Rectangle [id:dp5350058854822106] 
		\draw   (48,75) -- (616.5,75) -- (616.5,318) -- (48,318) -- cycle ;
		%Straight Lines [id:da029718887492870882] 
		\draw    (162,75) -- (162,318) ;
		%Straight Lines [id:da05344874547129863] 
		\draw    (275,75) -- (275,318) ;
		%Straight Lines [id:da21463378291275204] 
		\draw    (389,76) -- (389,319) ;
		%Straight Lines [id:da5474414620078112] 
		\draw    (502,76) -- (502,319) ;
		%Straight Lines [id:da5345944555007334] 
		\draw    (48.5,197) -- (616.5,197) ;
		%Shape: Circle [id:dp48023676077954547] 
		\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ] (52,173.75) .. controls (52,163.95) and (59.95,156) .. (69.75,156) .. controls (79.55,156) and (87.5,163.95) .. (87.5,173.75) .. controls (87.5,183.55) and (79.55,191.5) .. (69.75,191.5) .. controls (59.95,191.5) and (52,183.55) .. (52,173.75) -- cycle ;
		%Shape: Circle [id:dp7798883018170188] 
		\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ] (166,175.75) .. controls (166,165.95) and (173.95,158) .. (183.75,158) .. controls (193.55,158) and (201.5,165.95) .. (201.5,175.75) .. controls (201.5,185.55) and (193.55,193.5) .. (183.75,193.5) .. controls (173.95,193.5) and (166,185.55) .. (166,175.75) -- cycle ;
		%Shape: Ellipse [id:dp3961235884469514] 
		\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ] (281,174) .. controls (281,163.51) and (296.67,155) .. (316,155) .. controls (335.33,155) and (351,163.51) .. (351,174) .. controls (351,184.49) and (335.33,193) .. (316,193) .. controls (296.67,193) and (281,184.49) .. (281,174) -- cycle ;
		%Shape: Ellipse [id:dp3522517716500919] 
		\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ] (393,172.5) .. controls (393,161.73) and (406.32,153) .. (422.75,153) .. controls (439.18,153) and (452.5,161.73) .. (452.5,172.5) .. controls (452.5,183.27) and (439.18,192) .. (422.75,192) .. controls (406.32,192) and (393,183.27) .. (393,172.5) -- cycle ;
		%Shape: Ellipse [id:dp11583954464680146] 
		\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ] (512.44,130.92) .. controls (516.32,130.78) and (519.95,143.98) .. (520.55,160.4) .. controls (521.15,176.82) and (518.49,190.24) .. (514.61,190.38) .. controls (510.73,190.53) and (507.1,177.33) .. (506.5,160.91) .. controls (505.9,144.49) and (508.56,131.07) .. (512.44,130.92) -- cycle ;
		%Shape: Ellipse [id:dp5610127232013735] 
		\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ] (65.71,216.93) .. controls (73.18,217.05) and (78.9,238.55) .. (78.49,264.95) .. controls (78.07,291.36) and (71.68,312.67) .. (64.21,312.55) .. controls (56.74,312.44) and (51.02,290.93) .. (51.43,264.53) .. controls (51.85,238.12) and (58.24,216.81) .. (65.71,216.93) -- cycle ;
		%Shape: Ellipse [id:dp3386662099980404] 
		\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ] (227.31,244.41) .. controls (232.99,249.26) and (224.07,269.02) .. (207.38,288.54) .. controls (190.69,308.06) and (172.56,319.94) .. (166.88,315.09) .. controls (161.2,310.23) and (170.13,290.47) .. (186.82,270.95) .. controls (203.5,251.44) and (221.64,239.55) .. (227.31,244.41) -- cycle ;
		%Shape: Ellipse [id:dp7810037909916034] 
		\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ] (343.34,257.98) .. controls (347.88,263.2) and (337.37,279.8) .. (319.86,295.04) .. controls (302.35,310.29) and (284.46,318.41) .. (279.92,313.18) .. controls (275.37,307.96) and (285.88,291.37) .. (303.39,276.12) .. controls (320.9,260.88) and (338.79,252.75) .. (343.34,257.98) -- cycle ;
		%Shape: Ellipse [id:dp5101251882915419] 
		\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ] (461.24,268.38) .. controls (464.42,273.01) and (456.13,284.2) .. (442.74,293.38) .. controls (429.35,302.56) and (415.93,306.25) .. (412.76,301.62) .. controls (409.58,296.99) and (417.87,285.8) .. (431.26,276.62) .. controls (444.65,267.44) and (458.07,263.75) .. (461.24,268.38) -- cycle ;
		%Shape: Ellipse [id:dp4901421301595086] 
		\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ] (573.72,255.26) .. controls (582.57,265.99) and (575.6,286.36) .. (558.16,300.76) .. controls (540.71,315.16) and (519.39,318.15) .. (510.53,307.42) .. controls (501.68,296.69) and (508.64,276.32) .. (526.09,261.92) .. controls (543.54,247.52) and (564.86,244.54) .. (573.72,255.26) -- cycle ;
		%Shape: Circle [id:dp28943882804629184] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] (88,98.75) .. controls (88,88.95) and (95.95,81) .. (105.75,81) .. controls (115.55,81) and (123.5,88.95) .. (123.5,98.75) .. controls (123.5,108.55) and (115.55,116.5) .. (105.75,116.5) .. controls (95.95,116.5) and (88,108.55) .. (88,98.75) -- cycle ;
		%Shape: Circle [id:dp4935279164420545] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] (195.5,106.5) .. controls (195.5,91.86) and (207.36,80) .. (222,80) .. controls (236.64,80) and (248.5,91.86) .. (248.5,106.5) .. controls (248.5,121.14) and (236.64,133) .. (222,133) .. controls (207.36,133) and (195.5,121.14) .. (195.5,106.5) -- cycle ;
		%Shape: Ellipse [id:dp12754332466278595] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] (297,99) .. controls (297,88.51) and (312.67,80) .. (332,80) .. controls (351.33,80) and (367,88.51) .. (367,99) .. controls (367,109.49) and (351.33,118) .. (332,118) .. controls (312.67,118) and (297,109.49) .. (297,99) -- cycle ;
		%Shape: Ellipse [id:dp7583435107314875] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] (422.5,91) .. controls (422.5,84.92) and (429.89,80) .. (439,80) .. controls (448.11,80) and (455.5,84.92) .. (455.5,91) .. controls (455.5,97.08) and (448.11,102) .. (439,102) .. controls (429.89,102) and (422.5,97.08) .. (422.5,91) -- cycle ;
		%Shape: Ellipse [id:dp6404221488807598] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] (542.5,80.5) .. controls (549.68,80.5) and (555.5,87.89) .. (555.5,97) .. controls (555.5,106.11) and (549.68,113.5) .. (542.5,113.5) .. controls (535.32,113.5) and (529.5,106.11) .. (529.5,97) .. controls (529.5,87.89) and (535.32,80.5) .. (542.5,80.5) -- cycle ;
		%Shape: Ellipse [id:dp23035781945374345] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] (91.5,200) .. controls (98.68,200) and (104.5,207.28) .. (104.5,216.25) .. controls (104.5,225.22) and (98.68,232.5) .. (91.5,232.5) .. controls (84.32,232.5) and (78.5,225.22) .. (78.5,216.25) .. controls (78.5,207.28) and (84.32,200) .. (91.5,200) -- cycle ;
		%Shape: Ellipse [id:dp6355206575078223] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] (241.91,205.46) .. controls (247.44,210.52) and (241.64,225.84) .. (228.97,239.67) .. controls (216.29,253.51) and (201.54,260.61) .. (196.01,255.55) .. controls (190.48,250.49) and (196.28,235.17) .. (208.95,221.34) .. controls (221.63,207.5) and (236.38,200.39) .. (241.91,205.46) -- cycle ;
		%Shape: Ellipse [id:dp573346885577968] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] (311.44,203.34) .. controls (317.04,197.23) and (334.92,204.51) .. (351.37,219.6) .. controls (367.82,234.7) and (376.61,251.89) .. (371,258) .. controls (365.39,264.11) and (347.51,256.83) .. (331.06,241.74) .. controls (314.61,226.64) and (305.83,209.45) .. (311.44,203.34) -- cycle ;
		%Shape: Ellipse [id:dp4184469869248577] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] (540.85,202.53) .. controls (545.76,197.18) and (562.61,204.67) .. (578.5,219.25) .. controls (594.38,233.82) and (603.29,249.97) .. (598.38,255.32) .. controls (593.48,260.67) and (576.62,253.18) .. (560.73,238.6) .. controls (544.85,224.03) and (535.94,207.87) .. (540.85,202.53) -- cycle ;
		%Shape: Ellipse [id:dp3967960144851028] 
		\draw  [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ] (475.21,208.12) .. controls (487.73,208.25) and (497.64,230.99) .. (497.36,258.91) .. controls (497.07,286.83) and (486.69,309.36) .. (474.17,309.23) .. controls (461.65,309.1) and (451.73,286.36) .. (452.02,258.44) .. controls (452.31,230.52) and (462.69,207.99) .. (475.21,208.12) -- cycle ;
		%Shape: Circle [id:dp7745298897635893] 
		\draw  [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ] (123,136.75) .. controls (123,126.95) and (130.95,119) .. (140.75,119) .. controls (150.55,119) and (158.5,126.95) .. (158.5,136.75) .. controls (158.5,146.55) and (150.55,154.5) .. (140.75,154.5) .. controls (130.95,154.5) and (123,146.55) .. (123,136.75) -- cycle ;
		%Shape: Circle [id:dp0877483337882159] 
		\draw  [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ] (245.5,140.5) .. controls (245.5,133.6) and (251.1,128) .. (258,128) .. controls (264.9,128) and (270.5,133.6) .. (270.5,140.5) .. controls (270.5,147.4) and (264.9,153) .. (258,153) .. controls (251.1,153) and (245.5,147.4) .. (245.5,140.5) -- cycle ;
		%Shape: Ellipse [id:dp042600074427249] 
		\draw  [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ] (315,136) .. controls (315,125.51) and (330.67,117) .. (350,117) .. controls (369.33,117) and (385,125.51) .. (385,136) .. controls (385,146.49) and (369.33,155) .. (350,155) .. controls (330.67,155) and (315,146.49) .. (315,136) -- cycle ;
		%Shape: Ellipse [id:dp24719127986980816] 
		\draw  [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ] (406.5,132) .. controls (406.5,114.88) and (427.09,101) .. (452.5,101) .. controls (477.91,101) and (498.5,114.88) .. (498.5,132) .. controls (498.5,149.12) and (477.91,163) .. (452.5,163) .. controls (427.09,163) and (406.5,149.12) .. (406.5,132) -- cycle ;
		%Shape: Ellipse [id:dp1948938830227529] 
		\draw  [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ] (526.5,128) .. controls (526.5,124.69) and (546.2,122) .. (570.5,122) .. controls (594.8,122) and (614.5,124.69) .. (614.5,128) .. controls (614.5,131.31) and (594.8,134) .. (570.5,134) .. controls (546.2,134) and (526.5,131.31) .. (526.5,128) -- cycle ;
		%Shape: Ellipse [id:dp7575706749442266] 
		\draw  [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ] (76.5,240) .. controls (76.5,233.92) and (94.86,229) .. (117.5,229) .. controls (140.14,229) and (158.5,233.92) .. (158.5,240) .. controls (158.5,246.08) and (140.14,251) .. (117.5,251) .. controls (94.86,251) and (76.5,246.08) .. (76.5,240) -- cycle ;
		%Shape: Ellipse [id:dp7674657139996115] 
		\draw  [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ] (216.02,288.49) .. controls (211.8,284.12) and (221.57,267.83) .. (237.86,252.09) .. controls (254.14,236.35) and (270.76,227.14) .. (274.98,231.51) .. controls (279.2,235.88) and (269.43,252.17) .. (253.14,267.91) .. controls (236.86,283.65) and (220.24,292.86) .. (216.02,288.49) -- cycle ;
		%Shape: Ellipse [id:dp27461786191045046] 
		\draw  [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ] (371,299) .. controls (364.1,299) and (358.5,280.64) .. (358.5,258) .. controls (358.5,235.36) and (364.1,217) .. (371,217) .. controls (377.9,217) and (383.5,235.36) .. (383.5,258) .. controls (383.5,280.64) and (377.9,299) .. (371,299) -- cycle ;
		%Shape: Ellipse [id:dp6550116952226948] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] (440.16,223.2) .. controls (443.45,219.61) and (451.56,221.69) .. (458.28,227.85) .. controls (464.99,234.01) and (467.76,241.91) .. (464.46,245.5) .. controls (461.17,249.09) and (453.06,247.01) .. (446.35,240.85) .. controls (439.63,234.69) and (436.86,226.79) .. (440.16,223.2) -- cycle ;
		%Shape: Ellipse [id:dp8188446693809268] 
		\draw  [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ] (586.5,256.46) .. controls (586.5,250.13) and (592.77,245) .. (600.5,245) .. controls (608.23,245) and (614.5,250.13) .. (614.5,256.46) .. controls (614.5,262.79) and (608.23,267.92) .. (600.5,267.92) .. controls (592.77,267.92) and (586.5,262.79) .. (586.5,256.46) -- cycle ;
		
		% Text Node
		\draw (89,118.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{{\huge EII}}};
		% Text Node
		\draw (202,118.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{{\huge VII}}};
		% Text Node
		\draw (305,118.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{{\huge EEI}}};
		% Text Node
		\draw (425,118.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{{\huge VEI}}};
		% Text Node
		\draw (536,118.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{{\huge EVI}}};
		% Text Node
		\draw (83,238.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{{\huge VVI}}};
		% Text Node
		\draw (190,238.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{{\huge EEE}}};
		% Text Node
		\draw (299,238.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{{\huge EEV}}};
		% Text Node
		\draw (411,238.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{{\huge VEV}}};
		% Text Node
		\draw (530,238.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{{\huge VVV}}};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Variance-Covariance matrix typical structures}
	\end{figure}  
	
	\paragraph{Density-Based Spatial Clustering of Applications with Noise (DBSCAN)}\label{DBSCAN}\mbox{}\\\\
	"\NewTerm{Density-based spatial clustering of applications with noise}\index{Density-based spatial clustering of applications with noise}" (DBSCAN) is a data clustering algorithm proposed by Martin Ester, Hans-Peter Kriegel, Jörg Sander and Xiaowei Xu in 11996 (holocene calendar). It is a density-based clustering non-parametric (unsupervised) algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away). DBSCAN is one of the most common, and most commonly cited, clustering algorithms iN the early 121st century (holocene calendar).
	
	Consider a set of points in some space to be clustered. Let $\varepsilon$ be a parameter specifying the radius of a neighborhood with respect to some point. For the purpose of DBSCAN clustering, the points are classified as:
	\begin{itemize}
		\item core points
		\item (directly-) reachable points
		\item outliers
	\end{itemize}	
	 as follows:
	 
	 \textbf{Definitions (\#\thesection.\mydef):}
	 \begin{itemize}
	 	\item[D1.] A point $p$ is a "\NewTerm{core point}" if at least $\mathrm{minPts}$\footnote{While $\mathrm{minPts}$ intuitively is the minimum cluster size, in some cases DBSCAN can produce smaller clusters. A DBSCAN cluster consists of at least one core point. As other points may be border points to more than one cluster, there is no guarantee that at least $\mathrm{minPts}$ points are included in every cluster.} points are within distance $\varepsilon$ of it (including $p$)
	 	
	 	\item[D2.] A point $q$ is "\NewTerm{directly reachable}" from $p$ if point $q$ is within distance $\varepsilon$ from core point $p$. Points are only said to be directly reachable from core points.
	 	
	 	An object is said to be in a "$\varepsilon$-neighbourhood" if the object is within a radius of $\varepsilon$ from an object as:
	 	$$N_{\varepsilon}(p):\{q \mid d(p, q) \leq \varepsilon\}$$
	 	
	 	\item[D3.] A point $q$ is "\NewTerm{reachable}" from $p$ if there is a path $p_1, \ldots, p_n$ with $p_1=p$ and $p_n=q$, where each $p_{i+1}$ is directly reachable from $p_i$. Note that this implies that the initial point and all points on the path must be core points, with the possible exception of $q$.
	 	
	 	\item[D4.] All points not reachable from any other point are "\NewTerm{outliers}" or "\NewTerm{noise}".
	 \end{itemize}
	 Now if $p$ is a core point, then it forms a cluster together with all points (core or non-core) that are reachable from it. Each cluster contains at least one core point; non-core points can be part of a cluster, but they form its "edge", since they cannot be used to reach more points.
	 
	 \begin{tcolorbox}[enhanced,title=Remark,colframe=black,arc=10pt,drop lifted shadow,after skip=15pt plus 2pt]
		Reachability is not a symmetric relation: by definition, only core points can reach non-core points. The opposite is not true, so a non-core point may be reachable, but nothing can be reached from it. 
	\end{tcolorbox}
	
	In the figure below $\mathrm{minPts} = 4$ the point $A$ and the other red points are "core points", because the area surrounding these points in an $\varepsilon$ radius contain at least $4$ points (including the point itself). Because they are all reachable from one another, they form a single cluster. Points $B$ and $C$ are not core points, but are reachable from $A$ (via other core points) and thus belong to the cluster as well. Point $N$ is a noise point that is neither a core point nor directly-reachable:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,396); %set diagram left start at 0, and has height of 396
		
		%Shape: Circle [id:dp30580130322558574] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (142.72,127.64) .. controls (142.72,122.87) and (146.59,119) .. (151.36,119) .. controls (156.13,119) and (160,122.87) .. (160,127.64) .. controls (160,132.41) and (156.13,136.28) .. (151.36,136.28) .. controls (146.59,136.28) and (142.72,132.41) .. (142.72,127.64) -- cycle ;
		%Shape: Circle [id:dp9143135347250106] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] (83.22,127.64) .. controls (83.22,90.01) and (113.73,59.5) .. (151.36,59.5) .. controls (188.99,59.5) and (219.5,90.01) .. (219.5,127.64) .. controls (219.5,165.27) and (188.99,195.78) .. (151.36,195.78) .. controls (113.73,195.78) and (83.22,165.27) .. (83.22,127.64) -- cycle ;
		%Shape: Circle [id:dp5641381410055248] 
		\draw  [fill={rgb, 255:red, 248; green, 231; blue, 28 }  ,fill opacity=1 ] (142.72,238.64) .. controls (142.72,233.87) and (146.59,230) .. (151.36,230) .. controls (156.13,230) and (160,233.87) .. (160,238.64) .. controls (160,243.41) and (156.13,247.28) .. (151.36,247.28) .. controls (146.59,247.28) and (142.72,243.41) .. (142.72,238.64) -- cycle ;
		%Shape: Circle [id:dp7195828913317155] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (193.72,219.64) .. controls (193.72,214.87) and (197.59,211) .. (202.36,211) .. controls (207.13,211) and (211,214.87) .. (211,219.64) .. controls (211,224.41) and (207.13,228.28) .. (202.36,228.28) .. controls (197.59,228.28) and (193.72,224.41) .. (193.72,219.64) -- cycle ;
		%Shape: Circle [id:dp7078420424454785] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (233.72,204.64) .. controls (233.72,199.87) and (237.59,196) .. (242.36,196) .. controls (247.13,196) and (251,199.87) .. (251,204.64) .. controls (251,209.41) and (247.13,213.28) .. (242.36,213.28) .. controls (237.59,213.28) and (233.72,209.41) .. (233.72,204.64) -- cycle ;
		%Shape: Circle [id:dp31950065614584133] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (279.72,182.64) .. controls (279.72,177.87) and (283.59,174) .. (288.36,174) .. controls (293.13,174) and (297,177.87) .. (297,182.64) .. controls (297,187.41) and (293.13,191.28) .. (288.36,191.28) .. controls (283.59,191.28) and (279.72,187.41) .. (279.72,182.64) -- cycle ;
		%Shape: Circle [id:dp5554188196658671] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (231.72,261.64) .. controls (231.72,256.87) and (235.59,253) .. (240.36,253) .. controls (245.13,253) and (249,256.87) .. (249,261.64) .. controls (249,266.41) and (245.13,270.28) .. (240.36,270.28) .. controls (235.59,270.28) and (231.72,266.41) .. (231.72,261.64) -- cycle ;
		%Shape: Circle [id:dp18712634305474163] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (286.72,232.64) .. controls (286.72,227.87) and (290.59,224) .. (295.36,224) .. controls (300.13,224) and (304,227.87) .. (304,232.64) .. controls (304,237.41) and (300.13,241.28) .. (295.36,241.28) .. controls (290.59,241.28) and (286.72,237.41) .. (286.72,232.64) -- cycle ;
		%Shape: Circle [id:dp5272477681145324] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (337.72,203.64) .. controls (337.72,198.87) and (341.59,195) .. (346.36,195) .. controls (351.13,195) and (355,198.87) .. (355,203.64) .. controls (355,208.41) and (351.13,212.28) .. (346.36,212.28) .. controls (341.59,212.28) and (337.72,208.41) .. (337.72,203.64) -- cycle ;
		%Shape: Circle [id:dp2860534811489188] 
		\draw  [fill={rgb, 255:red, 248; green, 231; blue, 28 }  ,fill opacity=1 ] (392.72,193.64) .. controls (392.72,188.87) and (396.59,185) .. (401.36,185) .. controls (406.13,185) and (410,188.87) .. (410,193.64) .. controls (410,198.41) and (406.13,202.28) .. (401.36,202.28) .. controls (396.59,202.28) and (392.72,198.41) .. (392.72,193.64) -- cycle ;
		%Shape: Circle [id:dp5421315128067317] 
		\draw  [color={rgb, 255:red, 248; green, 231; blue, 28 }  ,draw opacity=1 ] (83.22,238.64) .. controls (83.22,201.01) and (113.73,170.5) .. (151.36,170.5) .. controls (188.99,170.5) and (219.5,201.01) .. (219.5,238.64) .. controls (219.5,276.27) and (188.99,306.78) .. (151.36,306.78) .. controls (113.73,306.78) and (83.22,276.27) .. (83.22,238.64) -- cycle ;
		%Shape: Circle [id:dp6882314997073806] 
		\draw  [color={rgb, 255:red, 248; green, 231; blue, 28 }  ,draw opacity=1 ] (333.22,193.64) .. controls (333.22,156.01) and (363.73,125.5) .. (401.36,125.5) .. controls (438.99,125.5) and (469.5,156.01) .. (469.5,193.64) .. controls (469.5,231.27) and (438.99,261.78) .. (401.36,261.78) .. controls (363.73,261.78) and (333.22,231.27) .. (333.22,193.64) -- cycle ;
		%Shape: Circle [id:dp7351120727947769] 
		\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ] (134.22,219.64) .. controls (134.22,182.01) and (164.73,151.5) .. (202.36,151.5) .. controls (239.99,151.5) and (270.5,182.01) .. (270.5,219.64) .. controls (270.5,257.27) and (239.99,287.78) .. (202.36,287.78) .. controls (164.73,287.78) and (134.22,257.27) .. (134.22,219.64) -- cycle ;
		%Shape: Circle [id:dp694380755998036] 
		\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ] (174.22,204.64) .. controls (174.22,167.01) and (204.73,136.5) .. (242.36,136.5) .. controls (279.99,136.5) and (310.5,167.01) .. (310.5,204.64) .. controls (310.5,242.27) and (279.99,272.78) .. (242.36,272.78) .. controls (204.73,272.78) and (174.22,242.27) .. (174.22,204.64) -- cycle ;
		%Shape: Circle [id:dp6149956471433973] 
		\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ] (172.22,261.64) .. controls (172.22,224.01) and (202.73,193.5) .. (240.36,193.5) .. controls (277.99,193.5) and (308.5,224.01) .. (308.5,261.64) .. controls (308.5,299.27) and (277.99,329.78) .. (240.36,329.78) .. controls (202.73,329.78) and (172.22,299.27) .. (172.22,261.64) -- cycle ;
		%Shape: Circle [id:dp3011158351272085] 
		\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ] (220.22,182.64) .. controls (220.22,145.01) and (250.73,114.5) .. (288.36,114.5) .. controls (325.99,114.5) and (356.5,145.01) .. (356.5,182.64) .. controls (356.5,220.27) and (325.99,250.78) .. (288.36,250.78) .. controls (250.73,250.78) and (220.22,220.27) .. (220.22,182.64) -- cycle ;
		%Shape: Circle [id:dp8503919664984145] 
		\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ] (227.22,241.28) .. controls (227.22,203.65) and (257.73,173.14) .. (295.36,173.14) .. controls (332.99,173.14) and (363.5,203.65) .. (363.5,241.28) .. controls (363.5,278.91) and (332.99,309.42) .. (295.36,309.42) .. controls (257.73,309.42) and (227.22,278.91) .. (227.22,241.28) -- cycle ;
		%Straight Lines [id:da47848692371284285] 
		\draw    (204.23,218.94) -- (240.49,205.34) ;
		\draw [shift={(242.36,204.64)}, rotate = 159.44] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(202.36,219.64)}, rotate = 339.44] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da9966831876862141] 
		\draw    (244.17,203.78) -- (286.56,183.5) ;
		\draw [shift={(288.36,182.64)}, rotate = 154.44] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(242.36,204.64)}, rotate = 334.44] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da2812568727814839] 
		\draw    (290.24,183.32) -- (344.48,202.96) ;
		\draw [shift={(346.36,203.64)}, rotate = 199.9] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(288.36,182.64)}, rotate = 19.9] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da7133273257649368] 
		\draw    (297.1,231.65) -- (344.62,204.63) ;
		\draw [shift={(346.36,203.64)}, rotate = 150.38] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(295.36,232.64)}, rotate = 330.38] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da9569559140780775] 
		\draw    (295.08,230.66) -- (288.64,184.62) ;
		\draw [shift={(288.36,182.64)}, rotate = 82.03] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(295.36,232.64)}, rotate = 262.03] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da8342973717385458] 
		\draw    (293.59,231.7) -- (244.13,205.57) ;
		\draw [shift={(242.36,204.64)}, rotate = 27.85] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(295.36,232.64)}, rotate = 207.85] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da2691676434289121] 
		\draw    (239.02,260.16) -- (203.7,221.12) ;
		\draw [shift={(202.36,219.64)}, rotate = 47.86] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(240.36,261.64)}, rotate = 227.86] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da23569698574725573] 
		\draw    (240.43,259.64) -- (242.29,206.64) ;
		\draw [shift={(242.36,204.64)}, rotate = 92.01] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(240.36,261.64)}, rotate = 272.01] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da9193008789241903] 
		\draw    (242.13,260.71) -- (293.59,233.57) ;
		\draw [shift={(295.36,232.64)}, rotate = 152.2] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(240.36,261.64)}, rotate = 332.2] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da9463097748570775] 
		\draw    (346.36,203.64) -- (399.39,194) ;
		\draw [shift={(401.36,193.64)}, rotate = 169.7] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da8326570387498593] 
		\draw    (202.36,219.64) -- (153.23,237.94) ;
		\draw [shift={(151.36,238.64)}, rotate = 339.57] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da40076234973731184] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (401.36,193.64) -- (432.37,247.68) ;
		\draw [shift={(433.86,250.28)}, rotate = 240.15] [fill={rgb, 255:red, 128; green, 128; blue, 128 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		
		% Text Node
		\draw (146,98.4) node [anchor=north west][inner sep=0.75pt]    {$D$};
		% Text Node
		\draw (142,213.4) node [anchor=north west][inner sep=0.75pt]    {$B$};
		% Text Node
		\draw (411,170.4) node [anchor=north west][inner sep=0.75pt]    {$C$};
		% Text Node
		\draw (280,154.4) node [anchor=north west][inner sep=0.75pt]    {$A$};
		% Text Node
		\draw (305,40) node [anchor=north west][inner sep=0.75pt]   [align=left] {minPts=4\\eps= 1 unit};
		% Text Node
		\draw (421,207.4) node [anchor=north west][inner sep=0.75pt]    {$\varepsilon $};
		% Text Node
		\draw (122,135) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {\textcolor[rgb]{0.29,0.56,0.89}{noise}};
		% Text Node
		\draw (179,195) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize,color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ] [align=left] {core};
		% Text Node
		\draw (229,273) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize,color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ] [align=left] {core};
		% Text Node
		\draw (284,243) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize,color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ] [align=left] {core};
		% Text Node
		\draw (223,180) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize,color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ] [align=left] {core};
		% Text Node
		\draw (294,157) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize,color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ] [align=left] {core};
		% Text Node
		\draw (372,173) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize,color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ] [align=left] {\textcolor[rgb]{0.97,0.91,0.11}{border}};
		% Text Node
		\draw (135,250) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize,color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ] [align=left] {\textcolor[rgb]{0.97,0.91,0.11}{border}};
		\end{tikzpicture}
	\end{figure}
	
	DBSCAN requires two parameters: $\varepsilon$ (eps) and the minimum number of points required to form a dense region ($\mathrm{minPts}$). It starts with an arbitrary starting point that has not been visited. This point's $\varepsilon$-neighborhood is retrieved, and if it contains sufficiently many points, a cluster is started. Otherwise, the point is labeled as noise. Note that this point might later be found in a sufficiently sized $\varepsilon$-environment of a different point and hence be made part of a cluster.
	
	If the number of points is more than $\mathrm{minPts}$, the current point and its nearby points from a cluster, and the starting point is marked as visited. Then, all the points in the cluster are not marked as visited that are processed in the same way recursively, to expand the cluster. Otherwise, the point is temporarily marked as a noise point. If the cluster is fully expanded, that is, all points in the cluster are marked as visited, and then the same algorithm is used to process the non-visited points. Until all objects are marked as a certain cluster or noise, the clustering process ends.
	
	DBSCAN can be used with any distance function (as well as similarity functions or other predicates). The distance function (dist) can therefore be seen as an additional parameter.
	
	Here is a general idea of the DBSCAN algorithm:
	
	\begin{algorithm}[H]
        \KwIn
		{%
		 DB (database), distFunc,
		$\varepsilon$,
		minPts
		}
		\ForEach{ $\mathrm{point}\;(P) \in \mathrm{database}\;\mathrm{DB}$}{%
			\uIf{$\mathrm{point}\;(P)\;\mathrm{is}\;\mathrm{not}\;\mathrm{yet}\;\mathrm{classified}$}{
                $\mathrm{collect}\;\mathrm{all}\;\mathrm{objects}\;\mathrm{density-reachable}\;\mathrm{from}\;(P)$
        		$\mathrm{and}\;\mathrm{assign}\;\mathrm{them}\;\mathrm{to}\;\mathrm{a}\;\mathrm{new}\;\mathrm{cluster}$\;
            }
            \uElse{
                $\mathrm{assign}\;(P)\;\mathrm{to}\;\mathrm{noise}$\;
            }
      	}
    	\Return cluster, label
	\end{algorithm}
	
	 Or with more details:
	 
	 \begin{algorithm}[H]
		\caption{DBSCAN Algorithm}\label{DBSCAN algorithm}
		\SetKw{Next}{Next}
		\SetKwComment{Comment}{$\triangleright$\ }{}\
		 \SetKwFunction{DBSCAN}{DBSCAN}
        \SetKwFunction{RangeQuery}{RangeQuery}
        \SetKwProg{Fn}{Function}{:}{}
		\KwIn
		{%
		 DB (database), distFunc,
		$\varepsilon$,
		minPts
		}
		\Fn{\DBSCAN{DB,distFunc,$\varepsilon$,minPts}}{
		$C:=0$ \Comment*[r]{we initialize the number of clusters}
		\ForEach{ $\mathrm{point}\;p \in \mathrm{database}\;\mathrm{DB}$}{%
      		\If{$\mathrm{label}(P)\neq \mathrm{undefined}$}{%
        		\Next \Comment*[r]{we jump to the next iteration}
      		}
      		$\mathrm{Neighbors}\;N:=\mathrm{RangeQuery(\mathrm{DB},\mathrm{distFunc},P,\varepsilon)}$\;
      		\If{$\mathrm{Card}(N)< \mathrm{minPts}$}{%
	        	$\mathrm{label}(P)=\mathrm{noise}$\;
    	    	\Next \Comment*[r]{we jump to the next iteration}
      		}
      		$C:=C+1$\;
      		$\mathrm{label}(P):=C$\;
      		$\mathrm{SeedSet}\;S:=N \backslash \left\lbrace P \right\rbrace$\Comment*[r]{random point of $C$ excepted the core one}
      		\ForEach{ $\mathrm{point} \;Q \in S$}{
	      		\If{$\mathrm{label}(Q)= \mathrm{noise}$}{%
	        		$\mathrm{label}(Q):=C$\;
	      		}
	      		\If{$\mathrm{label}(Q)\neq \mathrm{undefined}$}{%
	    	    	\Next \Comment*[r]{we jump to the next iteration}
	      		}
	      		$\mathrm{label}(Q):=C$\;
	      		$\mathrm{Neighbors}\;N:=\mathrm{RangeQuery(\mathrm{DB},\mathrm{distFunc},Q,\varepsilon)}$\;
	      		\If{$\mathrm{Card}(N)\geq \mathrm{minPts}$}{%
	    	    	$S:=S \cup N$
	      		}
    		}
    	}
    	\Return cluster, label
		}
		\Fn{\RangeQuery{DB,distFunc,$Q$,$\varepsilon$}}{
			$\mathrm{Neighbors}\;N:=\mathrm{empty}\;\mathrm{list}$\;
			\ForEach{ $\mathrm{point} \;P \in\mathrm{database}\; \mathrm{DB}$}{
	      		\If{$\mathrm{distFun}(Q,P)\leq \varepsilon$}{%
	        		$N:=N\cup \left\lbrace P\right\rbrace$\;
	      		}
    		}
    		\Return $N$
		}
	\end{algorithm}
	As we can see from the above pseudo-code DBSCAN has a worst-case of $\mathcal{O}(n^2)$.
	
	DBSCAN has the following advantages:
	\begin{itemize}
		\item DBSCAN does not require one to specify the number of clusters in the data a priori, as opposed to $k$-means.
		
		\item DBSCAN can find arbitrarily-shaped clusters. It can even find a cluster completely surrounded by (but not connected to) a different cluster. Due to the MinPts parameter, the so-called single-link effect (different clusters being connected by a thin line of points) is reduced.
		
		DBSCAN can find non-linearly separable clusters. The dataset below cannot be adequately clustered with $k$-means or Gaussian Mixture EM clustering:
		\begin{figure}[H]
			\centering
			\includegraphics[scale=0.7]{img/computing/dbscan_nonlinear.jpg}
		\end{figure}
	
		\item DBSCAN has a notion of noise, and is robust to outliers.
		
		\item DBSCAN requires just two parameters and is mostly insensitive to the ordering of the points in the database.
	\end{itemize}
	and the following disadvantages:
	\begin{itemize}
		\item DBSCAN is not entirely deterministic: border points that are reachable from more than one cluster can be part of either cluster, depending on the order the data are processed. 
		
		\item The quality of DBSCAN depends on the distance measure used in the function $\mathrm{regionQuery}(P,\varepsilon)$. The most common distance metric used is Euclidean distance. 
		
		\item DBSCAN cannot cluster data sets well with large differences in densities, since the $\mathrm{minPts}-\varepsilon$ combination cannot then be chosen appropriately for all clusters.
		
		\item If the data and scale are not well understood, choosing a meaningful distance threshold $\varepsilon$ can be difficult.
	\end{itemize}
	
	\begin{tcolorbox}[enhanced,title=Remark,colframe=black,arc=10pt,drop lifted shadow,after skip=15pt plus 2pt]
	Generalized DBSCAN (GDBSCAN) is a generalization by the same authors to arbitrary "neighborhood" and "dense" predicates. HDBSCAN is a hierarchical version of DBSCAN.
	\end{tcolorbox}

	
	\paragraph{Mean shift clustering}\mbox{}\\\\
	"\NewTerm{Mean shift}\index{mean shift}" is a powerful and versatile non parametric iterative algorithm that can be used for lot of purposes like finding modes, clustering, tracking images etc. Mean Shift was introduced by Keinosuke Fukunaga and Larry D. Hostetler in 11975 (holocene calendar) and has been extended to be applicable in other fields like Computer Vision. The text that follows will provide a discussion of Mean Shift , prove its convergence and slightly discuss its important applications.
	
	This text provides an intuitive idea of Mean shift and the later sections will expand the idea. Mean shift considers feature space as a empirical probability density function. If the input is a set of points then Mean shift considers them as sampled from the underlying probability density function. If dense regions (or clusters) are present in the feature space , then they correspond to the mode (or local maxima) of the probability density function. We can also identify clusters associated with the given mode using Mean Shift.

	For each data point, Mean shift associates it with the nearby peak of the dataset’s probability density function. For each data point, Mean shift defines a window around it and computes the mean of the data point. Then it shifts the center of the window to the mean and repeats the algorithm till it converges. After each iteration, we can consider that the window shifts to a more denser region of the dataset.
	
	At the high level, we can specify Mean Shift as follows :
	\begin{enumerate}
		\item Choose a kernel $K$
		\item Fix a window size $h$\footnote{Some algorithms have a dynamic window}
		\item Randomly segment the domain with the kernel $K$ (of size $h$)
		\item Compute the mean of data (barycenter) weighted within the window $h$ of each kernel
		\item Shift the window $h$ of each kernel $K$ to the mean (barycenter) and repeat till convergence
		\item Calculate the probability each point has to belong to one of the kernel $K$
		\item Cluster each point to the kernel $K$ have the highest probability
	\end{enumerate}
	
	Before dealing with the mathematical aspect, let us consider the graphic and playful aspect! The purpose of the Mean Shift algorithm is to find the maximum density region(s) per iteration. So consider for example the following bi-varied case with some points:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{img/computing/mean_shift_initial_points.jpg}
	\end{figure} 
	where we place randomly one or many circles (in $d$-dimensional case it will be $d$-dimensions sphere instead of simple circles) of a given radius:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.9]{img/computing/mean_shift_initial_configuration.jpg}
		\caption[]{Initial configuration of a bi-dimensional mean shift application}
	\end{figure} 
	Then more or less empirically (there are different techniques!) we calculate the center of mass inside this circle:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/computing/mean_shift_barycenter_calculation.jpg}
	\end{figure} 
	Then we move the circle on the center of gravity (barycenter) to get:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/computing/mean_shift_barycenter_move.jpg}
	\end{figure}
	and we repeat the procedure:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.85]{img/computing/mean_shift_barycenter_move_again.jpg}
	\end{figure}
	And so on:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/computing/mean_shift_barycenter_move_again_and_again.jpg}
	\end{figure}
	That is to say that if we have for example a distribution of points such as below:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/mean_shift_3d_perspective.jpg}
	\end{figure}
	In reality we will randomly segment the domain and run the algorithm iteratively in parallel for each sphere (circle in the case of dimension 2):
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/mean_shift_random_segmentation.jpg}
	\end{figure}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The reader will be able to find a practical example in our \texttt{R} companion book!
	\end{tcolorbox}
	As the reader can guess intuitively, this method converges securely (it can be proved algebraically but it's quite boring!), on the other hand it converges infinitely so we must set a convergence terminal for the algorithm to stop its iterations.

	Let's move on to the computational aspect! We will introduce two different and very common approaches.
	
	For the both approach, the first idea of the Mean Shift algorithm is to consider the workspace as a probability density (or multiple one!). Then each point of the workspace is identified by a coordinate vector $\vec{p}_i$ with a "mass" (corresponding mathematically to it's probability density function: the kernel!) given by the kernel:
	
	centered on $\vec{p}_0$. We have already meet such Kernels\footnote{Several types of kernel functions are commonly used: uniform, triangle, Epanechnikov, quartic (biweight), tricube, triweight, Gaussian, quadratic and cosine.} and we know what should be their properties. We won't come back here on these aspects! If the workspace contains a set of points, the latter are considered as a sample of an underlying probability density function and the dense regions (local maximum) as the modal value of the function of density of interest!
	
	Obvious a very common Kernel is the multivariate Gaussian one given for recall by (where the lowercase $k$ means that at each iteration $k$ the variance-covariance matrix is recomputed):
	
	In practice some algorithms introduce a constant $h$ such that the latter is written for a $d$-dimensional case:
	
	The first typical implementation of the Mean Shift clustering algorithm is based on the barycenter in Geometry (or center of gravity in physics). For this let us recall that we have proved in the section of Euclidean Geometry (page \pageref{barycenter}), that the barycenter was given by:
	
	This will be written in the context of Mean Shift with Gaussian kernel:
	
	The difference:
	
	is named the "\NewTerm{mean shift vector}" in the paper of Fukunaga and Hostetler. The mean-shift algorithm sets afterwards $\vec{p}_0^{\,[k]}\leftarrow m(\vec{p}_0)$, and repeats the estimation $k$ time $m(\vec{p}_0)^{[k]}$ converges.
	
	
	For the second typical possible implementation, consider again that $\{\vec{p}_i\}_{i=1\ldots N}$ is the position of the points and $K$ is a given kernel function (that weights the points) for a given iteration, then for each of the $N$ measurement points we have, we calculate the mean relatively to a reference point $\vec{p}_0$ (the center point of the kernel):
	
	The first step in this second method with this underlying density $f(\vec{p}_0, h)$ is to find the mode of this density! The mode is located at the zero of the gradient
$\vec{\nabla} f(\vec{p}_0, h)=\vec{0}$ and this second method is an elegant way to locate this zero!
	
	Indeed, let us compute the gradient:
	
	Now a traditional idea is to replace $\vec{p}_0$ in the last parenthesis thanks to gradient itself. Indeed, that latter leads us to:
	
	Hence after rearranging:
	
	So we get a fixed-point iteration, that brings us to write:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	As in the case of Gaussian Kernel we have $K'=K$ we then fall back on the result of the first method:
	
	\end{tcolorbox}
	What interest us is again the "\NewTerm{mean shift vector}" defined this time by:
	
	
	Whatever the method, the choice of bandwidth parameter $h$ or Kernel are critical. A large $h$ might result in incorrect clustering and might merge distinct clusters. A very small $h$ might result in too many clusters.

	Mean-shift looks very similar to $k$-means, they both move the point closer to the cluster centroids. One may wonder: How is this different from $k$-means? The key difference is that Mean shift does not require the user to specify the number of clusters. In some cases, it is not straightforward to guess the right number of clusters to use. In $k$-means, the output may end up having too few clusters or too many clusters to be useful. At the cost of larger time complexity, Mean shift determines the number of clusters suitable to the dataset provided.

	Another commonly cited difference is that $k$-means can only learn circle or ellipsoidal clusters. However, this is not true. The reason that Mean shift can learn arbitrary shapes is because the features are mapped to another higher dimensional feature space through the kernel. The arbitrary shapes are due to the algorithm finding circle or ellipsoidal clusters in higher dimensional feature space. When the features are mapped back to $1$D/$2$D/$3$D, the resulting clusters look like strange shapes. This is also the trick as used in Support Vector Machines.

	A traditional $k$-means does not use kernels, but Kernel $k$-means is available. Kernel $k$-means is useful if:
	\begin{enumerate}
		\item The number of clusters is known or can be reasonably estimated
		
		\item Dataset needs learning non-ellipsoidal cluster shapes
	\end{enumerate}
	So, you can enjoy the better runtime complexity of $k$-means and learn arbitrary clusters if you can determine the number of clusters to use.
	
	\paragraph{Hierarchical Ascendant Clustering (HAC)}\mbox{}\\\\
	In data mining and statistics, "\NewTerm{hierarchical clustering}\index{hierarchical clustering}", also named "\NewTerm{hierarchical cluster analysis}\index{hierarchical cluster analysis}" (HCA) or "\NewTerm{hierarchical agglomerative clustering}\index{hierarchical agglomerative clustering}" is an unsupervised method of cluster analysis which seeks to build a hierarchy of clusters in a predefined order. Let us remind that strategies for hierarchical clustering generally fall into two types:
	\begin{itemize}
		\item \textbf{Agglomerative/Ascendant:} This is a "bottom-up" approach! each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy. We speak then of "\NewTerm{hierarchical ascendant clustering}\index{hierarchical ascendant clustering}" (HAC), also known as "\NewTerm{Hierarchical Agglomerative Clustering}\index{Hierarchical Agglomerative Clustering}" (HAC) or "\NewTerm{AGNES}\index{AGNES}" (acronym for "AGglomerative NESting")
	
		\item \textbf{Divisive/Descendant:} This is a "top-down" approach! all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy. We speak then of "\NewTerm{hierarchical descendant clustering}\index{hierarchical descendant clustering}" (HDC), also known as "\NewTerm{DIANA}\index{DIANA}" (acronym for "DivisiveANAly").
	\end{itemize}
	In order to decide which clusters should be combined (for agglomerative), or where a cluster should be split (for divisive), a measure of dissimilarity between sets of observations is required. In most methods of hierarchical clustering, this is achieved by use of an appropriate metric (a measure of distance between pairs of observations), and a linkage criterion which specifies the dissimilarity of sets as a function of the pairwise distances of observations in the sets.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	"\NewTerm{Non hierarchical clustering}\index{non hierarchical clustering}" involves formation of new clusters by merging or splitting the clusters. It does not follow a tree like structure like hierarchical clustering. This technique groups the data in order to maximize or minimize some evaluation criteria. $k$-means clustering is an effective way of non hierarchical clustering (see page \pageref{K-means}). In this method the partitions are made such that non-overlapping groups having no hierarchical relationships between themselves. 
	\end{tcolorbox}
	
	To introduce this clustering technique let us consider the following companion example based on the following list of data in Microsoft Excel:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/dendrogram_excel_list.jpg}
		\caption[]{List of data for our study of HAC in Microsoft Excel 14.0.6123}
	\end{figure}
	We wish to have a hierarchical organization of likeness of individuals based on their income (\textit{Wage}) and their living space (\textit{Area}). 

	One possible technique is to define for this measurement a distance of similarity. For example, the Euclidean distance:
	
	is a special choice that will associate two individuals whose distance is minimal. We speak then in the area of clustering "\NewTerm{single linkage}\index{simple linkage}".
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,445); %set diagram left start at 0, and has height of 445
		
		%Shape: Ellipse [id:dp08905452292737914] 
		\draw   (227,82.05) .. controls (227,64.9) and (253.06,51) .. (285.2,51) .. controls (317.34,51) and (343.4,64.9) .. (343.4,82.05) .. controls (343.4,99.2) and (317.34,113.1) .. (285.2,113.1) .. controls (253.06,113.1) and (227,99.2) .. (227,82.05) -- cycle ;
		%Shape: Ellipse [id:dp006377044026549594] 
		\draw   (227,181.05) .. controls (227,163.9) and (253.06,150) .. (285.2,150) .. controls (317.34,150) and (343.4,163.9) .. (343.4,181.05) .. controls (343.4,198.2) and (317.34,212.1) .. (285.2,212.1) .. controls (253.06,212.1) and (227,198.2) .. (227,181.05) -- cycle ;
		%Shape: Ellipse [id:dp5592581186588139] 
		\draw   (227,283.05) .. controls (227,265.9) and (253.06,252) .. (285.2,252) .. controls (317.34,252) and (343.4,265.9) .. (343.4,283.05) .. controls (343.4,300.2) and (317.34,314.1) .. (285.2,314.1) .. controls (253.06,314.1) and (227,300.2) .. (227,283.05) -- cycle ;
		%Shape: Ellipse [id:dp652322790967363] 
		\draw   (389,81.5) .. controls (389,70.45) and (404.67,61.5) .. (424,61.5) .. controls (443.33,61.5) and (459,70.45) .. (459,81.5) .. controls (459,92.55) and (443.33,101.5) .. (424,101.5) .. controls (404.67,101.5) and (389,92.55) .. (389,81.5) -- cycle ;
		%Shape: Ellipse [id:dp2707297711990473] 
		\draw   (392,178.5) .. controls (392,167.45) and (407.67,158.5) .. (427,158.5) .. controls (446.33,158.5) and (462,167.45) .. (462,178.5) .. controls (462,189.55) and (446.33,198.5) .. (427,198.5) .. controls (407.67,198.5) and (392,189.55) .. (392,178.5) -- cycle ;
		%Shape: Ellipse [id:dp1521416136575482] 
		\draw   (394,282.5) .. controls (394,271.45) and (409.67,262.5) .. (429,262.5) .. controls (448.33,262.5) and (464,271.45) .. (464,282.5) .. controls (464,293.55) and (448.33,302.5) .. (429,302.5) .. controls (409.67,302.5) and (394,293.55) .. (394,282.5) -- cycle ;
		%Shape: Circle [id:dp7959814316822784] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (260.9,64.55) .. controls (260.9,62.59) and (262.49,61) .. (264.45,61) .. controls (266.41,61) and (268,62.59) .. (268,64.55) .. controls (268,66.51) and (266.41,68.1) .. (264.45,68.1) .. controls (262.49,68.1) and (260.9,66.51) .. (260.9,64.55) -- cycle ;
		%Shape: Circle [id:dp032439751264510486] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (242.9,77.95) .. controls (242.9,75.99) and (244.49,74.4) .. (246.45,74.4) .. controls (248.41,74.4) and (250,75.99) .. (250,77.95) .. controls (250,79.91) and (248.41,81.5) .. (246.45,81.5) .. controls (244.49,81.5) and (242.9,79.91) .. (242.9,77.95) -- cycle ;
		%Shape: Circle [id:dp9911307218076917] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (307.9,97.95) .. controls (307.9,95.99) and (309.49,94.4) .. (311.45,94.4) .. controls (313.41,94.4) and (315,95.99) .. (315,97.95) .. controls (315,99.91) and (313.41,101.5) .. (311.45,101.5) .. controls (309.49,101.5) and (307.9,99.91) .. (307.9,97.95) -- cycle ;
		%Shape: Circle [id:dp37081563946171703] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (264.9,159.55) .. controls (264.9,157.59) and (266.49,156) .. (268.45,156) .. controls (270.41,156) and (272,157.59) .. (272,159.55) .. controls (272,161.51) and (270.41,163.1) .. (268.45,163.1) .. controls (266.49,163.1) and (264.9,161.51) .. (264.9,159.55) -- cycle ;
		%Shape: Circle [id:dp3375839911948433] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (246.9,172.95) .. controls (246.9,170.99) and (248.49,169.4) .. (250.45,169.4) .. controls (252.41,169.4) and (254,170.99) .. (254,172.95) .. controls (254,174.91) and (252.41,176.5) .. (250.45,176.5) .. controls (248.49,176.5) and (246.9,174.91) .. (246.9,172.95) -- cycle ;
		%Shape: Circle [id:dp20812836184952976] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (311.9,192.95) .. controls (311.9,190.99) and (313.49,189.4) .. (315.45,189.4) .. controls (317.41,189.4) and (319,190.99) .. (319,192.95) .. controls (319,194.91) and (317.41,196.5) .. (315.45,196.5) .. controls (313.49,196.5) and (311.9,194.91) .. (311.9,192.95) -- cycle ;
		%Shape: Circle [id:dp6418134319587308] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (263.9,266.55) .. controls (263.9,264.59) and (265.49,263) .. (267.45,263) .. controls (269.41,263) and (271,264.59) .. (271,266.55) .. controls (271,268.51) and (269.41,270.1) .. (267.45,270.1) .. controls (265.49,270.1) and (263.9,268.51) .. (263.9,266.55) -- cycle ;
		%Shape: Circle [id:dp7810812033776524] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (245.9,279.95) .. controls (245.9,277.99) and (247.49,276.4) .. (249.45,276.4) .. controls (251.41,276.4) and (253,277.99) .. (253,279.95) .. controls (253,281.91) and (251.41,283.5) .. (249.45,283.5) .. controls (247.49,283.5) and (245.9,281.91) .. (245.9,279.95) -- cycle ;
		%Shape: Circle [id:dp44261742521168856] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (310.9,299.95) .. controls (310.9,297.99) and (312.49,296.4) .. (314.45,296.4) .. controls (316.41,296.4) and (318,297.99) .. (318,299.95) .. controls (318,301.91) and (316.41,303.5) .. (314.45,303.5) .. controls (312.49,303.5) and (310.9,301.91) .. (310.9,299.95) -- cycle ;
		%Shape: Circle [id:dp2997651796913714] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (419.9,70.55) .. controls (419.9,68.59) and (421.49,67) .. (423.45,67) .. controls (425.41,67) and (427,68.59) .. (427,70.55) .. controls (427,72.51) and (425.41,74.1) .. (423.45,74.1) .. controls (421.49,74.1) and (419.9,72.51) .. (419.9,70.55) -- cycle ;
		%Shape: Circle [id:dp2248195529871031] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (439.9,90.55) .. controls (439.9,88.59) and (441.49,87) .. (443.45,87) .. controls (445.41,87) and (447,88.59) .. (447,90.55) .. controls (447,92.51) and (445.41,94.1) .. (443.45,94.1) .. controls (441.49,94.1) and (439.9,92.51) .. (439.9,90.55) -- cycle ;
		%Shape: Circle [id:dp5641866459061935] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (422.9,166.55) .. controls (422.9,164.59) and (424.49,163) .. (426.45,163) .. controls (428.41,163) and (430,164.59) .. (430,166.55) .. controls (430,168.51) and (428.41,170.1) .. (426.45,170.1) .. controls (424.49,170.1) and (422.9,168.51) .. (422.9,166.55) -- cycle ;
		%Shape: Circle [id:dp36269833576190846] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (442.9,186.55) .. controls (442.9,184.59) and (444.49,183) .. (446.45,183) .. controls (448.41,183) and (450,184.59) .. (450,186.55) .. controls (450,188.51) and (448.41,190.1) .. (446.45,190.1) .. controls (444.49,190.1) and (442.9,188.51) .. (442.9,186.55) -- cycle ;
		%Shape: Circle [id:dp9627387780713244] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (426.9,268.55) .. controls (426.9,266.59) and (428.49,265) .. (430.45,265) .. controls (432.41,265) and (434,266.59) .. (434,268.55) .. controls (434,270.51) and (432.41,272.1) .. (430.45,272.1) .. controls (428.49,272.1) and (426.9,270.51) .. (426.9,268.55) -- cycle ;
		%Shape: Circle [id:dp20323019811768872] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (446.9,288.55) .. controls (446.9,286.59) and (448.49,285) .. (450.45,285) .. controls (452.41,285) and (454,286.59) .. (454,288.55) .. controls (454,290.51) and (452.41,292.1) .. (450.45,292.1) .. controls (448.49,292.1) and (446.9,290.51) .. (446.9,288.55) -- cycle ;
		%Straight Lines [id:da5281636649401522] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (264.45,64.55) -- (423.45,70.55) ;
		%Straight Lines [id:da6667086434819329] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (264.45,64.55) -- (443.45,90.55) ;
		%Straight Lines [id:da02017305676872372] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (246.45,75.89) -- (443.45,90.55) ;
		%Straight Lines [id:da6807797063743541] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (246.45,77.95) -- (423.45,70.55) ;
		%Straight Lines [id:da8789472123520299] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (311.45,97.95) -- (423.45,70.55) ;
		%Straight Lines [id:da478336509391069] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (311.45,97.95) -- (443.45,90.55) ;
		%Straight Lines [id:da8197393334717378] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (250.45,171.7) -- (446.45,185.3) ;
		%Straight Lines [id:da29891559227658315] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (314.45,299.95) -- (430.45,268.55) ;
		
		% Text Node
		\draw (100,73) node [anchor=north west][inner sep=0.75pt]   [align=left] {Average linkage};
		% Text Node
		\draw (100,170) node [anchor=north west][inner sep=0.75pt]   [align=left] {Complete linkage};
		% Text Node
		\draw (100,274) node [anchor=north west][inner sep=0.75pt]   [align=left] {Single linkage};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Type of links in Hierarchical Clustering}
	\end{figure} 
	Formally:
	\begin{itemize}
		\item Maximum or complete linkage clustering:
		
	
		\item Minimum or single linkage clustering:
		
	
		\item Average linkage:
		
	\end{itemize}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	How to measure the distance between two individuals considering:
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|c|c|c|c|}
		\rowcolor[gray]{0.75}\hline \textbf{Id} & \textbf{Firstname} & \textbf{Age} & \textbf{Children} & \textbf{Shoe size} & \textbf{Height} \\
		\hline 1 & Alain & 45 & 3 & 45 & 182 \\
		\hline 2 & Martine & 28 & 1 & 36 & 165 \\
		\hline 3 & Pierre & 22 & 0 & 43 & 172 \\
		\hline
		\end{tabular}
	\end{table}
	Who is Martine closest to? We will use the Euclidean distance!
	\begin{itemize}
		\item Distance between Alain and Martine:
		$$
		d_{1 * 2}=\sqrt{(45-28)^{2}+(3-1)^{2}+(45-36)^{2}+(182-165)^{2}}
		$$
		
		\item Distance between Martine and Pierre:\\
		$$
		d_{2 * 3}=\sqrt{(28-22)^{2}+(1-0)^{2}+(36-43)^{2}+(165-172)^{2}}
		$$
		
		\item Distance between Alain and Pierre :
		$$
		d_{1 * 3}=\sqrt{(45-22)^{2}+(3-0)^{2}+(45-43)^{2}+(182-172)^{2}}
		$$
	\end{itemize}
	Table of distances:
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|c|c|c|}
		\hline & \cellcolor[gray]{0.75} Alain & \cellcolor[gray]{0.75} Martine & \cellcolor[gray]{0.75} Pierre \\
		\hline \cellcolor[gray]{0.75} Alain & 0 & & \\
		\hline \cellcolor[gray]{0.75} Martine & 25.74 & 0 & \\
		\hline \cellcolor[gray]{0.75} Pierre & 25.33 & 11.61 & 0 \\
		\hline
		\end{tabular}
	\end{table}
	We notice that the two variables \textit{Size} and \textit{Children} have little weight in the distance calculation. To remedy this, we must center and reduce the data normally before computing distances!	
	\end{tcolorbox}
	So we can easily, using a spreadsheet software like Microsoft Excel 14.0.6123, create a "\NewTerm{distance matrix}\index{distance matrix}" or "\NewTerm{proximity matrix}\index{proximity matrix}" which is a symmetric matrix with zero in diagonal and that relatively to the list above will give us:
	\begin{figure}[H]
		\centering
		\includegraphics[width=1\textwidth]{img/computing/dendrogram_excel_distance_matrix.jpg}
		\caption[]{Distance matrix in Microsoft Excel 14.0.6123}
	\end{figure}
	where we put in the cell \texttt{D4} the following Euclidean distance formula:
	\begin{center}
		\texttt{=SQRT((B4-D2)\string^2+(C4-D3)\string^2)}
	\end{center}
	we then drag this formula for the rest of the matrix to the cell \texttt{AA27}.

	Then we use the bottom-up method (ascendant agglomeration) where we combine the groups until there is only one group remaining (containing all data) and this is a very boring work to describe and to do manually in spreadsheet software with a table of the size given above. We will give a step by step small example later below (see page \pageref{detailed example HAC}).
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Let us remind that in their implementation, hierarchical clustering algorithms can adopt one of two following methods:
	\begin{itemize}
		\item The "\NewTerm{top-down}" ("\NewTerm{descending}") or "\NewTerm{dividing method}" consists in starting with a large cluster consisting of the whole data set and then in each step, the successive splitting of the generated clusters is carried out until each cluster consists of only one individual.
	
		\item The "\NewTerm{bottom-up}" ("\NewTerm{ascending}") or "\NewTerm{agglomerative method}" reverses the previous logic, it starts with clusters each composed of an individual and then at each step, the clusters are associated in pairs to form other clusters which are in turn joined two to two until a cluster is obtained.
	\end{itemize}
	\end{tcolorbox}
	This work will give us in a tabular form following the detailed steps given in our Minitab 15.1.1.0 companion book (values are not rounded to hundredths unlike the small matrix given in the figure above):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/computing/dendrogram_minitab_summary_list.jpg}
		\caption[]{HAC distance summary list in Minitab 15.1.1.0}
	\end{figure}
	where the level of similarity of the linked group $i,j$ is defined empirically by:
	
	Thus, for the first row we have for example:
	
	The preceding list is more pleasant to analyse if, as it is customary, we represent it is a "\NewTerm{dendrogram}\index{dendrogram}" (from Greek dendro "tree" and gramma "drawing"), ie a tree diagram frequently used to illustrate the arrangement of the clusters produced by hierarchical clustering, as given by Minitab 15.1.1.0 below:	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/computing/dendrogram_minitab_plot.jpg}
		\caption{Dendrograms plot in Minitab 15.1.1.0}
	\end{figure}
	If you follow the detailed steps given in our MATLAB™ 2013a companion book you will get:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/computing/dendrogram_matlab_plot.jpg}
		\caption{Dendrograms plot in MATLAB™ 2013a }
	\end{figure}
	If you follow the detailed steps given in our \texttt{R} companion book you will get:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/computing/dendrogram_r_plot.jpg}
		\caption{Dendrograms plot in \texttt{R} 3.0.2}
	\end{figure}
	and still with \texttt{R} (see the corresponding companion book) for the same data:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.85]{img/computing/dendrogram_r_plot_heatmap.jpg}
		\caption{Dendrograms heatmap plot in \texttt{R} 3.0.2}
	\end{figure}
	Dendrograms and HAC are also used in biostatistics as shown below (still with \texttt{R} but not described in the companion book yet):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.65]{img/computing/dendrogram_r_plot_heatmap_microarray.jpg}
		\caption{Dendrograms heatmap microarray plot in \texttt{R} 3.0.2}
	\end{figure}
	Or in financial engineering to group similar times series (still with \texttt{R} and detailed steps given in the companion book):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.65]{img/computing/dendrogram_r_plot_tsa.jpg}
		\caption{Dendrograms TSA microarray plot in \texttt{R} 3.0.2}
	\end{figure}
	\label{detailed example HAC}Let's go now for a detailed step by step example! For this consider the following table:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}\textbf{Object}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Weight}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Length}} \\ \hline
		$\mathrm{a}$ & $12$ & $10$ \\ \hline
		$\mathrm{b}$ & $15$ & $25$ \\ \hline
		$\mathrm{c}$ & $30$ & $55$ \\ \hline
		$\mathrm{d}$ & $50$ & $100$ \\ \hline
		$\mathrm{e}$ & $35$ & $70$ \\ \hline
		$\mathrm{f}$ & $45$ & $70$ \\ \hline
		$\mathrm{g}$ & $35$ & $60$ \\ \hline
		\end{tabular}
	\end{table}
	All that remains is to determine the matrix of Euclidean distances. The following distance matrix is then obtained with the data set presented above:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|c|c|c|c|}
		\hline
		 &\cellcolor[gray]{0.75} $\mathrm{a}$ &\cellcolor[gray]{0.75} $\mathrm{b}$ &\cellcolor[gray]{0.75} $\mathrm{c}$ &\cellcolor[gray]{0.75} $\mathrm{d}$ &\cellcolor[gray]{0.75} $\mathrm{e}$ &\cellcolor[gray]{0.75} $\mathrm{f}$ &\cellcolor[gray]{0.75} $\mathrm{g}$ \\ \hline
		\cellcolor[gray]{0.75}$\mathrm{a}$ & - & $15.30$ & $48.47$ & $97.69$ & $64.26$ & $68.48$ & $55.04$ \\ \hline
		\cellcolor[gray]{0.75}$\mathrm{b}$ & $15.30$ & - & $33.54$ & $82.76$ & $49.24$ & $54.08$ & $40.31$ \\ \hline
		\cellcolor[gray]{0.75}$\mathrm{c}$ & $48.47$ & $33.54$ & - & $49.24$ & $15.81$ & $21.21$ & $7.07$ \\ \hline
		\cellcolor[gray]{0.75}$\mathrm{d}$ & $97.69$ & $82.76$ & $49.24$ & - & $33.54$ & $30.41$ & $42.72$ \\ \hline
		\cellcolor[gray]{0.75}$\mathrm{e}$ & $64.26$ & $49.24$ & $15.81$ & $33.54$ & - & $10.00$ & $10.00$ \\ \hline
		\cellcolor[gray]{0.75}$\mathrm{f}$ & $68.48$ & $54.08$ & $21.21$ & $30.41$ & $10.00$ & - & $14.14$ \\ \hline
		\cellcolor[gray]{0.75}$\mathrm{g}$ & $55.04$ & $40.31$ & $7.07$ & $42.72$ & $10.00$ & $14.14$ & - \\ \hline
		\end{tabular}
	\end{table}
	Obviously, the distance between a point and itself is $0$ for example $d(a, a)=0$ and the distance:
	
	For the next step, the closest individuals are detected and aggregated into a new cluster and the distance matrix is again calculated. This last one is based on the old one except that the distance between this new cluster formed and the other objects or individuals (or clusters since after $2$ iterations, it is possible that clusters already aggregate between them) are recalculated according to the defined method (Single linkage or Complete linkage, ... etc).
	
	On the basis of the previous distance matrix, it can be seen that $\mathrm{c}$ and $\mathrm{g}$ are the closest $(7.07)$, then they are aggregated. The distance between the cluster $\{\mathrm{c}, \mathrm{g}\}$ and the other objects is updated according to the Complete Linkage method (another method could have been used as we have already mention it):
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		 &\cellcolor[gray]{0.75} $\mathrm{a}$ &\cellcolor[gray]{0.75} $\mathrm{b}$ &\cellcolor[gray]{0.75} $\{\mathrm{c},\mathrm{g}\}$ &\cellcolor[gray]{0.75} $\mathrm{d}$ &\cellcolor[gray]{0.75} $\mathrm{e}$ &\cellcolor[gray]{0.75} $\mathrm{f}$ \\ \hline
		\cellcolor[gray]{0.75}$\mathrm{a}$ & - & $15.30$ & $\color{red}{55.04}$ & $97.69$ & $64.26$ & $68.48$ \\ \hline
		\cellcolor[gray]{0.75}$\mathrm{b}$ & $15.30$ & - & $\color{red}{40.31}$ & $82.76$ & $49.24$ & $54.08$ \\ \hline
		\cellcolor[gray]{0.75}$\{\mathrm{c},\mathrm{g}\}$ & $\color{red}{55.04}$ & $\color{red}{40.31}$ & - & $\color{red}{49.24}$ & $\color{red}{15.81}$ & $\color{red}{21.21}$ \\ \hline
		\cellcolor[gray]{0.75}$\mathrm{d}$ & $97.69$ & $82.76$ & $\color{red}{49.24}$ & - & $33.54$ & $30.41$ \\ \hline
		\cellcolor[gray]{0.75}$\mathrm{e}$ & $64.26$ & $49.24$ & $\color{red}{15.81}$ & $33.54$ & - & $10.00$ \\ \hline
		\cellcolor[gray]{0.75}$\mathrm{f}$ & $68.48$ & $54.08$ & $\color{red}{21.21}$ & $30.41$ & $10.00$ & - \\ \hline
		\end{tabular}
	\end{table}
	As the method of the complete linkage or the maximum linkage defines the distance between cluster based on the most distant points, then one can calculate:
	
	On the basis of the updated matrix above, a close distance between $\mathrm{f}$ and $\mathrm{e}$ is detected since their distance is the smallest and equal to $10$. Again, a cluster $\{\mathrm{e}, \mathrm{f}\}$ is constructed and their distance from the others will also be readjusted according to maximum linkage:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		 &\cellcolor[gray]{0.75} $\mathrm{a}$ &\cellcolor[gray]{0.75} $\mathrm{b}$ &\cellcolor[gray]{0.75} $\{\mathrm{c},\mathrm{g}\}$ &\cellcolor[gray]{0.75} $\mathrm{d}$ &\cellcolor[gray]{0.75} $\{\mathrm{e},\mathrm{f}\}$ \\ \hline
		\cellcolor[gray]{0.75}$\mathrm{a}$ & - & $15.30$ & $55.04$ & $97.69$ & $\color{red}{68.48}$ \\ \hline
		\cellcolor[gray]{0.75}$\mathrm{b}$ & $15.30$ & - & $40.31$ & $82.76$ & $\color{red}{54.08}$ \\ \hline
		\cellcolor[gray]{0.75}$\{\mathrm{c},\mathrm{g}\}$ & $55.04$ & $40.31$ & - & $49.24$ & $\color{red}{21.21}$ \\ \hline
		\cellcolor[gray]{0.75}$\mathrm{d}$ & $97.69$ & $82.76$ & $49.24$ & - & $\color{red}{33.54}$ \\ \hline
		\cellcolor[gray]{0.75}$\{\mathrm{e},\mathrm{f}\}$ & $\color{red}{68.48}$ & $\color{red}{54.08}$ & $\color{red}{21.21}$ & $\color{red}{33.54}$ & - \\ \hline
		\end{tabular}
	\end{table}
	As the method of the Complete linkage or the maximum linkage defines the distance between cluster based on the most distant points, then one can calculate:
	
	This time it is $\mathrm{a}$ and $\mathrm{b}$ that will be clustered at a distance of $15.3$ and the matrix after updating the distances becomes this:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|c|}
		\hline
		 &\cellcolor[gray]{0.75} $\{\mathrm{a},\mathrm{b}\}$ &\cellcolor[gray]{0.75} $\{\mathrm{c},\mathrm{g}\}$ &\cellcolor[gray]{0.75} $\mathrm{d}$ &\cellcolor[gray]{0.75} $\{\mathrm{e},\mathrm{f}\}$ \\ \hline
		\cellcolor[gray]{0.75}$\{\mathrm{a},\mathrm{b}\}$ & - & $\color{red}{55.04}$ & $\color{red}{97.69}$ & $\color{red}{68.48}$ \\ \hline
		\cellcolor[gray]{0.75}$\{\mathrm{c},\mathrm{g}\}$ & $\color{red}{55.04}$ & - & $49.24$ & $21.21$ \\ \hline
		\cellcolor[gray]{0.75}$\mathrm{d}$ & $\color{red}{97.69}$ & $49.24$ & - & $33.54$ \\ \hline
		\cellcolor[gray]{0.75}$\{\mathrm{e},\mathrm{f}\}$ & $\color{red}{68.48}$ & $21.21$ & $33.54$ & - \\ \hline
		\end{tabular}
	\end{table}
	The following matrix will give what we have for distances after that  $\{\mathrm{c}, \mathrm{g}\}$ and $\{\mathrm{e}, \mathrm{f}\}$ are grouped to another cluster at a distance of $21.21$:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline
		 &\cellcolor[gray]{0.75} $\{\mathrm{a},\mathrm{b}\}$ &\cellcolor[gray]{0.75} $\left\lbrace\{\mathrm{c},\mathrm{g}\},\{\mathrm{e},\mathrm{f}\}\right\rbrace$ &\cellcolor[gray]{0.75} $\mathrm{d}$ \\ \hline
		\cellcolor[gray]{0.75}$\{\mathrm{a},\mathrm{b}\}$ & - & $\color{red}{55.04}$ & $97.69$ \\ \hline
		\cellcolor[gray]{0.75}$\left\lbrace\{\mathrm{c},\mathrm{g}\},\{\mathrm{e},\mathrm{f}\}\right\rbrace$ & $\color{red}{55.04}$ & - & $\color{red}{49.24}$ \\ \hline
		\cellcolor[gray]{0.75}$\mathrm{d}$ & $97.69$ & $\color{red}{49.24}$ & - \\ \hline
		\end{tabular}
	\end{table}
	And then it is the cluster group $\left\lbrace \{\mathrm{c}, \mathrm{g}\}, \{\mathrm{e}, \mathrm{f}\}\right\rbrace$ and $\mathrm{d}$ that will be grouped at a distance of $49.24$, updating the new distances gives:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|}
		\hline
		 &\cellcolor[gray]{0.75} $\{\mathrm{a},\mathrm{b}\}$ &\cellcolor[gray]{0.75} $\left\lbrace\{\mathrm{c},\mathrm{g}\},\{\mathrm{e},\mathrm{f}\},\mathrm{d}\right\rbrace$ \\ \hline
		\cellcolor[gray]{0.75}$\{\mathrm{a},\mathrm{b}\}$ & - & $\color{red}{97.69}$ \\ \hline
		\cellcolor[gray]{0.75}$\left\lbrace\{\mathrm{c},\mathrm{g}\},\{\mathrm{e},\mathrm{f}\},\mathrm{d}\right\rbrace$ & $\color{red}{97.69}$ & - \\ \hline
		\end{tabular}
	\end{table}
	The algorithm ends at the next iteration when $\{\mathrm{a}, \mathrm{b}\}$ and the cluster group $\left\lbrace \{\mathrm{c}, \mathrm{g}\}, \{\mathrm{e}, \mathrm{f}\},\mathrm{d}\right\rbrace$ will form the root cluster at a distance of $97.69$.
	
	This gives us finally the following hierarchical agglomerative ascendant clustering:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/computing/cluster_dendrogram.jpg}
	\end{figure}
	The vertical axis \textit{Height} above corresponds to the following calculations we did above: 
	\begin{itemize}
		\item $\{\mathrm{c},\mathrm{g}\}$: $7.07$
		\item $\{\mathrm{e},\mathrm{f}\}$: $10.00$
		\item $\{\mathrm{a},\mathrm{b}\}$: $15.3$		
		\item $\{\{\mathrm{c},\mathrm{g}\},\{\mathrm{e},\mathrm{f}\}\}$: $21.1$
		\item $\{\{\mathrm{c},\mathrm{g}\},\{\mathrm{e},\mathrm{f}\},d\}$: $97.69$
	\end{itemize}
	Hierarchical clustering offers the advantage of presenting hierarchical groups so that the resulting ramifications are easy to read. In contrast to the $k$-means, it does not require the definition of the number of clusters in advance and it is reproducible in the sense that the choice of the initial cluster centers is not random. But in return it is more expensive in time when it comes to implementing it for a large database.
	
	This is why in practice, when dealing with huge databases we use mixed (or hybrid) methods, ie before doing a HAC, we first run a $k$-means and after we do a HAC on the resulting $k$-means (therefore the bottom of the dendrogram tree will be obtain with a $k$-means and the upper part with the HAC). This process is named "\NewTerm{hybrid hierarchical k-means clustering}\index{hybrid hierarchical k-means clustering}"  (hkmeans) and the reader can see a practical example in our \texttt{R} companion book.
	
	 Obviously as for the $k$-nn and the $k$-means, we may run the HAC on the factorial space obtained after having run a PCA (Principal Component Analysis as seen on page \pageref{principal component analysis}) on our data (see our \texttt{R} companion book for an application example). We can also even reduce the dimensions using that latter if necessary!
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/computing/hac_factormap.jpg}
		\caption[HAC on a factor map]{HAC on a factor map with R}
	\end{figure}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	To do a classification on qualitative data we will use typically a Multiple Correspondence Analysis (MCA).
	\end{tcolorbox}
	
	\paragraph{Linear Discriminant Analysis}\label{linear discriminant analysis}\index{linear discriminant analysis}\mbox{}\\\\
	"\NewTerm{Fisher Linear Discriminant Analysis}\index{Fisher Linear Discriminant Analysis}" is a method used in statistics, pattern recognition and machine learning to find a linear combination of features which characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.
	
	There are commonly three linear discriminant methods for this technique:
	\begin{itemize}
		\item Maximum likelihood: Assigns $\vec x$ to the group that maximizes population (group) density.
	
		\item Bayes Discriminant Rule: Assigns $\vec x$ to the group that maximizes $\pi_i f_i(\vec x)$ where $\pi_i$  represents the prior probability of that classification, and $f_i(x)$ represents the population density.
	
		\item Fisher's linear discriminant rule. We will focus here on that method which is the easiest (so see details below!)
	\end{itemize}

	LDA is closely related to PCA, for both of them are based on linear, i.e. matrix multiplication, transformations. For the case of PCA, the transformation is based on minimizing mean square error between original data vectors and data vectors that can be estimated fro the reduced dimensionality data vectors. And the PCA does not take into account any difference in class. But for the case of LDA, the transformation is based on maximizing a ratio of ``between-class variance" to "within-class variance" with the goal of reducing data variation in the same class and increasing the separation between classes. Let's see an example of LDA.
	
	The idea is seeking to obtain a scalar $y$ (in the special two classes case it's a scalar otherwise it's a vector!) by projecting the samples $X$ onto a line:
	 
	Then we try to find the $\vec\theta^\ast$ to maximize the ratio of "between-class variance" to "within-class variance". Next, we will introduce how to use mathematics way to present this problem. 
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1215); %set diagram left start at 0, and has height of 1215
		
		%Straight Lines [id:da12391086741967272] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (158.58,150.58) -- (166,295) ;
		%Straight Lines [id:da5240536024385432] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (148.92,113.58) -- (158.6,296.59) ;
		%Straight Lines [id:da7534478916854368] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (164.92,125.58) -- (174.2,296.19) ;
		%Straight Lines [id:da3673935002344386] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (175.72,137.58) -- (184.2,294.59) ;
		%Straight Lines [id:da5256800382641562] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (188.52,157.58) -- (195.8,293.79) ;
		%Straight Lines [id:da6309954234413193] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (177.72,114.78) -- (187.4,293.79) ;
		%Straight Lines [id:da6905337777857932] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (194.92,128.38) -- (203.4,292.59) ;
		%Straight Lines [id:da7877731164441566] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (446.92,151.58) -- (395.66,122.19) ;
		%Straight Lines [id:da7193309628849498] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (476.25,158.25) -- (399.26,115.39) ;
		%Straight Lines [id:da35171729042384703] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (463.58,139.25) -- (402.06,105.39) ;
		%Straight Lines [id:da028609007260602892] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (452.92,126.58) -- (403.66,99.79) ;
		%Straight Lines [id:da4057739645111036] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (436.92,114.58) -- (405.26,97.39) ;
		%Straight Lines [id:da7006995213627216] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (465.58,115.58) -- (410.46,86.99) ;
		%Straight Lines [id:da28422086060118534] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (483.25,129.92) -- (408.86,91.39) ;
		%Straight Lines [id:da2618859972051957] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (437,170.33) -- (388,142.33) ;
		%Straight Lines [id:da08230509298660826] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (452.33,188.67) -- (384.33,150.67) ;
		%Straight Lines [id:da7309968235551203] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (463,202.33) -- (381.33,157.67) ;
		%Straight Lines [id:da44543800013256307] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (448,204) -- (378.67,165.33) ;
		%Straight Lines [id:da6247786394967294] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (444.33,224.33) -- (371.67,184) ;
		%Straight Lines [id:da9620265436186595] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (426,201.33) -- (375.67,173) ;
		%Straight Lines [id:da8963019511518409] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (419.67,215.67) -- (369.5,188.5) ;
		%Straight Lines [id:da7112889978495032] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (174.67,201.33) -- (179.67,294) ;
		%Straight Lines [id:da3166413519608524] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (164,187.67) -- (169.33,295) ;
		%Straight Lines [id:da011263987540257148] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (159.67,203) -- (164.67,295) ;
		%Straight Lines [id:da8034990344968698] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (156,223.33) -- (160.33,296) ;
		%Straight Lines [id:da9274811527283033] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (138.67,200.33) -- (143.33,297.33) ;
		%Straight Lines [id:da2683177268464003] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (149.67,169.33) -- (156.67,296) ;
		%Straight Lines [id:da08329105299523398] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (131.33,214.67) -- (135.67,298.33) ;
		%Shape: Axis 2D [id:dp7540240006493004] 
		\draw  (23.5,308) -- (291.5,308)(34.5,47) -- (34.5,322) (284.5,303) -- (291.5,308) -- (284.5,313) (29.5,54) -- (34.5,47) -- (39.5,54)  ;
		%Shape: Axis 2D [id:dp21201651892632256] 
		\draw  (310.5,308) -- (578.5,308)(321.5,47) -- (321.5,322) (571.5,303) -- (578.5,308) -- (571.5,313) (316.5,54) -- (321.5,47) -- (326.5,54)  ;
		%Straight Lines [id:da7878520910190974] 
		\draw [line width=2.25]    (34.5,308) -- (226.5,290) ;
		%Straight Lines [id:da07442897611489352] 
		\draw [line width=2.25]    (321.5,308) -- (417.5,67) ;
		%Shape: Circle [id:dp024584412498512176] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (128,214.67) .. controls (128,212.83) and (129.49,211.33) .. (131.33,211.33) .. controls (133.17,211.33) and (134.67,212.83) .. (134.67,214.67) .. controls (134.67,216.51) and (133.17,218) .. (131.33,218) .. controls (129.49,218) and (128,216.51) .. (128,214.67) -- cycle ;
		%Shape: Circle [id:dp11482249307187531] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (135.33,200.33) .. controls (135.33,198.49) and (136.83,197) .. (138.67,197) .. controls (140.51,197) and (142,198.49) .. (142,200.33) .. controls (142,202.17) and (140.51,203.67) .. (138.67,203.67) .. controls (136.83,203.67) and (135.33,202.17) .. (135.33,200.33) -- cycle ;
		%Shape: Circle [id:dp3699867012628133] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (152.67,223.33) .. controls (152.67,221.49) and (154.16,220) .. (156,220) .. controls (157.84,220) and (159.33,221.49) .. (159.33,223.33) .. controls (159.33,225.17) and (157.84,226.67) .. (156,226.67) .. controls (154.16,226.67) and (152.67,225.17) .. (152.67,223.33) -- cycle ;
		%Shape: Circle [id:dp7130570843687383] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (156.33,203) .. controls (156.33,201.16) and (157.83,199.67) .. (159.67,199.67) .. controls (161.51,199.67) and (163,201.16) .. (163,203) .. controls (163,204.84) and (161.51,206.33) .. (159.67,206.33) .. controls (157.83,206.33) and (156.33,204.84) .. (156.33,203) -- cycle ;
		%Shape: Circle [id:dp7159500548435878] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (160.67,187.67) .. controls (160.67,185.83) and (162.16,184.33) .. (164,184.33) .. controls (165.84,184.33) and (167.33,185.83) .. (167.33,187.67) .. controls (167.33,189.51) and (165.84,191) .. (164,191) .. controls (162.16,191) and (160.67,189.51) .. (160.67,187.67) -- cycle ;
		%Shape: Circle [id:dp26630555364232644] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (171.33,201.33) .. controls (171.33,199.49) and (172.83,198) .. (174.67,198) .. controls (176.51,198) and (178,199.49) .. (178,201.33) .. controls (178,203.17) and (176.51,204.67) .. (174.67,204.67) .. controls (172.83,204.67) and (171.33,203.17) .. (171.33,201.33) -- cycle ;
		%Shape: Circle [id:dp75083550262056] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (146.33,169.33) .. controls (146.33,167.49) and (147.83,166) .. (149.67,166) .. controls (151.51,166) and (153,167.49) .. (153,169.33) .. controls (153,171.17) and (151.51,172.67) .. (149.67,172.67) .. controls (147.83,172.67) and (146.33,171.17) .. (146.33,169.33) -- cycle ;
		%Shape: Circle [id:dp030178359183811843] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (132.33,298.33) .. controls (132.33,296.49) and (133.83,295) .. (135.67,295) .. controls (137.51,295) and (139,296.49) .. (139,298.33) .. controls (139,300.17) and (137.51,301.67) .. (135.67,301.67) .. controls (133.83,301.67) and (132.33,300.17) .. (132.33,298.33) -- cycle ;
		%Shape: Circle [id:dp44647483325363413] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (140,297.33) .. controls (140,295.49) and (141.49,294) .. (143.33,294) .. controls (145.17,294) and (146.67,295.49) .. (146.67,297.33) .. controls (146.67,299.17) and (145.17,300.67) .. (143.33,300.67) .. controls (141.49,300.67) and (140,299.17) .. (140,297.33) -- cycle ;
		%Shape: Circle [id:dp49680754455241005] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (153.33,296) .. controls (153.33,294.16) and (154.83,292.67) .. (156.67,292.67) .. controls (158.51,292.67) and (160,294.16) .. (160,296) .. controls (160,297.84) and (158.51,299.33) .. (156.67,299.33) .. controls (154.83,299.33) and (153.33,297.84) .. (153.33,296) -- cycle ;
		%Shape: Circle [id:dp2453413399224207] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (157,296) .. controls (157,294.16) and (158.49,292.67) .. (160.33,292.67) .. controls (162.17,292.67) and (163.67,294.16) .. (163.67,296) .. controls (163.67,297.84) and (162.17,299.33) .. (160.33,299.33) .. controls (158.49,299.33) and (157,297.84) .. (157,296) -- cycle ;
		%Shape: Circle [id:dp5300140024751117] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (161.33,295) .. controls (161.33,293.16) and (162.83,291.67) .. (164.67,291.67) .. controls (166.51,291.67) and (168,293.16) .. (168,295) .. controls (168,296.84) and (166.51,298.33) .. (164.67,298.33) .. controls (162.83,298.33) and (161.33,296.84) .. (161.33,295) -- cycle ;
		%Shape: Circle [id:dp3943247556030238] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (166,295) .. controls (166,293.16) and (167.49,291.67) .. (169.33,291.67) .. controls (171.17,291.67) and (172.67,293.16) .. (172.67,295) .. controls (172.67,296.84) and (171.17,298.33) .. (169.33,298.33) .. controls (167.49,298.33) and (166,296.84) .. (166,295) -- cycle ;
		%Shape: Circle [id:dp8500348057922544] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (176.33,294) .. controls (176.33,292.16) and (177.83,290.67) .. (179.67,290.67) .. controls (181.51,290.67) and (183,292.16) .. (183,294) .. controls (183,295.84) and (181.51,297.33) .. (179.67,297.33) .. controls (177.83,297.33) and (176.33,295.84) .. (176.33,294) -- cycle ;
		%Shape: Circle [id:dp664828617494019] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (384.67,142.33) .. controls (384.67,140.49) and (386.16,139) .. (388,139) .. controls (389.84,139) and (391.33,140.49) .. (391.33,142.33) .. controls (391.33,144.17) and (389.84,145.67) .. (388,145.67) .. controls (386.16,145.67) and (384.67,144.17) .. (384.67,142.33) -- cycle ;
		%Shape: Circle [id:dp6152346953121477] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (381,150.67) .. controls (381,148.83) and (382.49,147.33) .. (384.33,147.33) .. controls (386.17,147.33) and (387.67,148.83) .. (387.67,150.67) .. controls (387.67,152.51) and (386.17,154) .. (384.33,154) .. controls (382.49,154) and (381,152.51) .. (381,150.67) -- cycle ;
		%Shape: Circle [id:dp20564166185429555] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (378,157.67) .. controls (378,155.83) and (379.49,154.33) .. (381.33,154.33) .. controls (383.17,154.33) and (384.67,155.83) .. (384.67,157.67) .. controls (384.67,159.51) and (383.17,161) .. (381.33,161) .. controls (379.49,161) and (378,159.51) .. (378,157.67) -- cycle ;
		%Shape: Circle [id:dp953944618797661] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (375.33,165.33) .. controls (375.33,163.49) and (376.83,162) .. (378.67,162) .. controls (380.51,162) and (382,163.49) .. (382,165.33) .. controls (382,167.17) and (380.51,168.67) .. (378.67,168.67) .. controls (376.83,168.67) and (375.33,167.17) .. (375.33,165.33) -- cycle ;
		%Shape: Circle [id:dp5989101965777568] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (372.33,173) .. controls (372.33,171.16) and (373.83,169.67) .. (375.67,169.67) .. controls (377.51,169.67) and (379,171.16) .. (379,173) .. controls (379,174.84) and (377.51,176.33) .. (375.67,176.33) .. controls (373.83,176.33) and (372.33,174.84) .. (372.33,173) -- cycle ;
		%Shape: Circle [id:dp8040855906166324] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (368.33,184) .. controls (368.33,182.16) and (369.83,180.67) .. (371.67,180.67) .. controls (373.51,180.67) and (375,182.16) .. (375,184) .. controls (375,185.84) and (373.51,187.33) .. (371.67,187.33) .. controls (369.83,187.33) and (368.33,185.84) .. (368.33,184) -- cycle ;
		%Shape: Circle [id:dp2755984846203843] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (366.17,188.5) .. controls (366.17,186.66) and (367.66,185.17) .. (369.5,185.17) .. controls (371.34,185.17) and (372.83,186.66) .. (372.83,188.5) .. controls (372.83,190.34) and (371.34,191.83) .. (369.5,191.83) .. controls (367.66,191.83) and (366.17,190.34) .. (366.17,188.5) -- cycle ;
		%Shape: Square [id:dp891020141809107] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (155.5,147.5) -- (161.67,147.5) -- (161.67,153.67) -- (155.5,153.67) -- cycle ;
		%Shape: Square [id:dp013403961744838266] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (161.5,122.5) -- (167.67,122.5) -- (167.67,128.67) -- (161.5,128.67) -- cycle ;
		%Shape: Square [id:dp2125347802574975] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (172.17,135.17) -- (178.33,135.17) -- (178.33,141.33) -- (172.17,141.33) -- cycle ;
		%Shape: Square [id:dp440771746748188] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (184.83,154.17) -- (191,154.17) -- (191,160.33) -- (184.83,160.33) -- cycle ;
		%Shape: Square [id:dp32067438229140555] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (191.83,125.83) -- (198,125.83) -- (198,132) -- (191.83,132) -- cycle ;
		%Shape: Square [id:dp3253255273794122] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (145.83,110.5) -- (152,110.5) -- (152,116.67) -- (145.83,116.67) -- cycle ;
		%Shape: Square [id:dp08916600739977065] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (174.17,111.5) -- (180.33,111.5) -- (180.33,117.67) -- (174.17,117.67) -- cycle ;
		%Shape: Circle [id:dp5299219650821838] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (416.33,215.67) .. controls (416.33,213.83) and (417.83,212.33) .. (419.67,212.33) .. controls (421.51,212.33) and (423,213.83) .. (423,215.67) .. controls (423,217.51) and (421.51,219) .. (419.67,219) .. controls (417.83,219) and (416.33,217.51) .. (416.33,215.67) -- cycle ;
		%Shape: Circle [id:dp9959123115245367] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (422.67,201.33) .. controls (422.67,199.49) and (424.16,198) .. (426,198) .. controls (427.84,198) and (429.33,199.49) .. (429.33,201.33) .. controls (429.33,203.17) and (427.84,204.67) .. (426,204.67) .. controls (424.16,204.67) and (422.67,203.17) .. (422.67,201.33) -- cycle ;
		%Shape: Circle [id:dp6305500083679587] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (441,224.33) .. controls (441,222.49) and (442.49,221) .. (444.33,221) .. controls (446.17,221) and (447.67,222.49) .. (447.67,224.33) .. controls (447.67,226.17) and (446.17,227.67) .. (444.33,227.67) .. controls (442.49,227.67) and (441,226.17) .. (441,224.33) -- cycle ;
		%Shape: Circle [id:dp9477879615691831] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (444.67,204) .. controls (444.67,202.16) and (446.16,200.67) .. (448,200.67) .. controls (449.84,200.67) and (451.33,202.16) .. (451.33,204) .. controls (451.33,205.84) and (449.84,207.33) .. (448,207.33) .. controls (446.16,207.33) and (444.67,205.84) .. (444.67,204) -- cycle ;
		%Shape: Circle [id:dp5270499983545605] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (449,188.67) .. controls (449,186.83) and (450.49,185.33) .. (452.33,185.33) .. controls (454.17,185.33) and (455.67,186.83) .. (455.67,188.67) .. controls (455.67,190.51) and (454.17,192) .. (452.33,192) .. controls (450.49,192) and (449,190.51) .. (449,188.67) -- cycle ;
		%Shape: Circle [id:dp11768286415969076] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (459.67,202.33) .. controls (459.67,200.49) and (461.16,199) .. (463,199) .. controls (464.84,199) and (466.33,200.49) .. (466.33,202.33) .. controls (466.33,204.17) and (464.84,205.67) .. (463,205.67) .. controls (461.16,205.67) and (459.67,204.17) .. (459.67,202.33) -- cycle ;
		%Shape: Circle [id:dp7993097697964635] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (433.67,170.33) .. controls (433.67,168.49) and (435.16,167) .. (437,167) .. controls (438.84,167) and (440.33,168.49) .. (440.33,170.33) .. controls (440.33,172.17) and (438.84,173.67) .. (437,173.67) .. controls (435.16,173.67) and (433.67,172.17) .. (433.67,170.33) -- cycle ;
		%Shape: Square [id:dp5389883943219873] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (443.83,148.5) -- (450,148.5) -- (450,154.67) -- (443.83,154.67) -- cycle ;
		%Shape: Square [id:dp5104360318791308] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (449.83,123.5) -- (456,123.5) -- (456,129.67) -- (449.83,129.67) -- cycle ;
		%Shape: Square [id:dp7648857574081598] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (460.5,136.17) -- (466.67,136.17) -- (466.67,142.33) -- (460.5,142.33) -- cycle ;
		%Shape: Square [id:dp2837780759472006] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (473.17,155.17) -- (479.33,155.17) -- (479.33,161.33) -- (473.17,161.33) -- cycle ;
		%Shape: Square [id:dp10913021141745816] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (480.17,126.83) -- (486.33,126.83) -- (486.33,133) -- (480.17,133) -- cycle ;
		%Shape: Square [id:dp6448988869422398] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (433.83,111.5) -- (440,111.5) -- (440,117.67) -- (433.83,117.67) -- cycle ;
		%Shape: Square [id:dp8355092308853733] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (462.5,112.5) -- (468.67,112.5) -- (468.67,118.67) -- (462.5,118.67) -- cycle ;
		%Shape: Square [id:dp017997264307569427] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (392.57,119.1) -- (398.74,119.1) -- (398.74,125.27) -- (392.57,125.27) -- cycle ;
		%Shape: Square [id:dp6547587375526274] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (395.17,112.3) -- (401.34,112.3) -- (401.34,118.47) -- (395.17,118.47) -- cycle ;
		%Shape: Square [id:dp5114515323514481] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (398.97,102.3) -- (405.14,102.3) -- (405.14,108.47) -- (398.97,108.47) -- cycle ;
		%Shape: Square [id:dp6674803121096264] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (401.57,96.7) -- (407.74,96.7) -- (407.74,102.87) -- (401.57,102.87) -- cycle ;
		%Shape: Square [id:dp9887651799443435] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (403.66,93.62) -- (409.82,93.62) -- (409.82,99.79) -- (403.66,99.79) -- cycle ;
		%Shape: Square [id:dp7916946415382173] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (404.77,88.3) -- (410.94,88.3) -- (410.94,94.47) -- (404.77,94.47) -- cycle ;
		%Shape: Square [id:dp8211995133158274] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (407.37,83.9) -- (413.54,83.9) -- (413.54,90.07) -- (407.37,90.07) -- cycle ;
		%Shape: Square [id:dp5811088945168952] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (155.83,292.5) -- (162,292.5) -- (162,298.67) -- (155.83,298.67) -- cycle ;
		%Shape: Square [id:dp8444027598656572] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (162.83,291.83) -- (169,291.83) -- (169,298) -- (162.83,298) -- cycle ;
		%Shape: Square [id:dp4010545495716853] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (171.17,290.83) -- (177.33,290.83) -- (177.33,297) -- (171.17,297) -- cycle ;
		%Shape: Square [id:dp47692100179481445] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (181.17,290.5) -- (187.33,290.5) -- (187.33,296.67) -- (181.17,296.67) -- cycle ;
		%Shape: Square [id:dp323076903544921] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (184.83,290.17) -- (191,290.17) -- (191,296.33) -- (184.83,296.33) -- cycle ;
		%Shape: Square [id:dp7833264044502133] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (192.5,289.5) -- (198.67,289.5) -- (198.67,295.67) -- (192.5,295.67) -- cycle ;
		%Shape: Square [id:dp9185093239957174] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (200.5,288.5) -- (206.67,288.5) -- (206.67,294.67) -- (200.5,294.67) -- cycle ;
		
		% Text Node
		\draw (11,33.4) node [anchor=north west][inner sep=0.75pt]    {$x_{2}$};
		% Text Node
		\draw (264,315.4) node [anchor=north west][inner sep=0.75pt]    {$x_{1}$};
		% Text Node
		\draw (298,33.4) node [anchor=north west][inner sep=0.75pt]    {$x_{2}$};
		% Text Node
		\draw (551,315.4) node [anchor=north west][inner sep=0.75pt]    {$x_{1}$};
		% Text Node
		\draw (47,51) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {the two classes are not\\well separated when projected\\onto this line};
		% Text Node
		\draw (351.5,238) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {this line succeeded in separating the two \\classes and in the meantime reducing \\the dimensionality of our problem from \\two features $\displaystyle ( x_{1} ,x_{2})$ to only a scalar value $\displaystyle y$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Underlying idea of linear discriminant analysis}
	\end{figure}

	To figure out the LDA, first we need know how to translate "between-class variance" and "within-class variance" to mathematics language. Then we try to maximize the ratio between these two. To simplify the problem, we start with two classes problem.
	
	\subparagraph{Two classes problem}\mbox{}\\\\
	Assume we have a set of $D$-dimensional samples $X = \{\vec x^{(1)}, 
	\vec x^{(2)}, \ldots  ,\vec x^{(m)} \}$, $n_1$ of which belong to class $\mathcal{C}_1$, and $n_2$ of which belong to class $\mathcal{C}_2$.
	
	We also assume the mean vector of two classes in $X$-space (if needed, there is a practical detailed numerical example just a little bit further below that can help the reader to understand what follows!):
	
	and in $y$-space:
		
	One way to define a measure of separation between two classes is to choose the distance between the projected means, which is in $y$-space, so the "between-class variance" is:
	
	Also, we can define the "within-class variance" for each class $\mathcal{C}_k$ is:
	
	Then, we get the between-class variance and within-class variance, we can define our objective function $J(\theta)$ as:
	
	In fact, if maximizing the objective function $J$, we are looking for a projection where examples from the class are projected very close to each other and at the same time, the projected means are as farther apart as possible. 
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1215); %set diagram left start at 0, and has height of 1215
		
		%Shape: Axis 2D [id:dp6206060725834754] 
		\draw  (197.5,368.7) -- (475.5,368.7)(225.3,51) -- (225.3,404) (468.5,363.7) -- (475.5,368.7) -- (468.5,373.7) (220.3,58) -- (225.3,51) -- (230.3,58)  ;
		%Straight Lines [id:da29159497186149497] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (195.5,61) -- (225.3,368.7) ;
		%Shape: Circle [id:dp9522288529893315] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (311,140) .. controls (311,137.24) and (313.24,135) .. (316,135) .. controls (318.76,135) and (321,137.24) .. (321,140) .. controls (321,142.76) and (318.76,145) .. (316,145) .. controls (313.24,145) and (311,142.76) .. (311,140) -- cycle ;
		%Shape: Circle [id:dp7868712314973729] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (419,194) .. controls (419,191.24) and (421.24,189) .. (424,189) .. controls (426.76,189) and (429,191.24) .. (429,194) .. controls (429,196.76) and (426.76,199) .. (424,199) .. controls (421.24,199) and (419,196.76) .. (419,194) -- cycle ;
		%Straight Lines [id:da8639513723135916] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (316,140) -- (512.5,98) ;
		%Straight Lines [id:da4298132688110665] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (424,194) -- (521.5,173) ;
		%Straight Lines [id:da9045596310278559] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (182.5,143) -- (309.5,116) ;
		%Straight Lines [id:da2718893751104259] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (190.5,196) -- (317.5,169) ;
		%Straight Lines [id:da17992242138646497] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (163.5,223) -- (415.5,169) ;
		%Straight Lines [id:da6243997410409647] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (173.5,275) -- (425.5,221) ;
		%Straight Lines [id:da508802097528885] 
		\draw    (500.34,104.97) -- (512.16,173.03) ;
		\draw [shift={(512.5,175)}, rotate = 260.15] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(500,103)}, rotate = 80.15] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da5461814556917335] 
		\draw    (169.88,224.96) -- (179.12,272.04) ;
		\draw [shift={(179.5,274)}, rotate = 258.91] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(169.5,223)}, rotate = 78.91] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da07759115933296679] 
		\draw    (189.85,144.97) -- (198.15,192.03) ;
		\draw [shift={(198.5,194)}, rotate = 259.99] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(189.5,143)}, rotate = 79.99] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Ellipse [id:dp803827441047793] 
		\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]  (243.65,157.62) .. controls (240.35,143.08) and (269.83,124) .. (309.5,115) .. controls (349.17,106) and (384,110.5) .. (387.3,125.04) .. controls (390.59,139.58) and (361.11,158.67) .. (321.45,167.67) .. controls (281.78,176.66) and (246.95,172.17) .. (243.65,157.62) -- cycle ;
		%Shape: Ellipse [id:dp1052896953403224] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]  (352.18,210.29) .. controls (348.88,195.75) and (378.36,176.66) .. (418.03,167.67) .. controls (457.69,158.67) and (492.52,163.17) .. (495.82,177.71) .. controls (499.12,192.25) and (469.64,211.34) .. (429.97,220.33) .. controls (390.31,229.33) and (355.48,224.83) .. (352.18,210.29) -- cycle ;
		
		% Text Node
		\draw (208,372) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
		% Text Node
		\draw (204,33.4) node [anchor=north west][inner sep=0.75pt]    {$x_{2}$};
		% Text Node
		\draw (465,378) node [anchor=north west][inner sep=0.75pt]    {$x_{1}$};
		% Text Node
		\draw (288,131.4) node [anchor=north west][inner sep=0.75pt]    {$\vec{u}_{1}$};
		% Text Node
		\draw (399,184.4) node [anchor=north west][inner sep=0.75pt]    {$\vec{u}_{2}$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Expected configuration for linear Discriminant analysis}
	\end{figure}

	To find the optimum $\theta^\ast$, we must express $J(\theta)$ as a function of $\theta$. Before the optimum,we need introduce the "scatter" matrix instead of variance. 

	We define some measures of the scatter as following:
	\begin{itemize}
	    \item The scatter matrix in feature space-$x$:
	    
	    \item The within-class scatter matrix:
	    
	    \item The between-class scatter matrix: 
	    
	\end{itemize}
	Let's see $J(\theta)$ again:
	
	The scatter of the projection $y$ can then be expressed as a function of the scatter matrix in feature space $X$:
	
	
	So we can get:
	
	
	Similarly, the difference between the projected means can be expressed in terms 
	of the means in the original feature space:
	
	We can finally express the "\NewTerm{Fisher criterion}\index{Fisher criterion}\label{Fisher criterion}" in terms of $S_W$ and $S_B$ as:
	
	Next, we will maximize this objective function. 
	
	The easiest way to maximize the object function $J$ is to derive it and set it to zero:
	
	
	Divided by $\vec\theta^{\,T} S_W \vec\theta$:
	 
	Notice that using the third row above, we have a classical generalized eigenvalue problem:
	
	Often denoted as (\SeeChapter(see section Linear Algebra page \pageref{characteristic polynomial determinant}):
	
	For now, the problem has been solved and we just want to get the direction of the $\vec \theta$, which is the optimum $\vec \theta^\ast$:
	
	This is known as "\NewTerm{Fisher's linear discriminant}\index{Fisher's linear discriminant}" (11936 according to holocene calendar), although it is not a discriminant but rather a specific choice of direction for the projection of the data down to one dimension, which is $ y = \vec\theta^{\ast T}X$. 
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	We want to compute the Linear Discriminant projection for the following two-dimensional dataset:
	\begin{itemize}
		\item Samples for class $\mathcal{C}_{1}: X_{1}=\left(x_{1}, x_{2}\right)=\{(4,2),(2,4),(2,3),(3,6),(4,4)\}$
		\item Sample for class $\mathcal{C}_{2}: X_{2}=\left(x_{1}, x_{2}\right)=\{(9,10),(6,8),(9,5),(8,7),(10,8)\}$
	\end{itemize}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\textwidth]{img/computing/lda_example_01.jpg}
	\end{figure}
	The classes means are :
	\begin{gather*}
		\begin{aligned}
		&\vec u_{1}=\frac{1}{n_{1}} \sum_{\vec x \in \mathcal{C}_{1}} \vec x^{(i)}=\frac{1}{5}\left[\left(\begin{array}{l}
		4 \\
		2
		\end{array}\right)+\left(\begin{array}{l}
		2 \\
		4
		\end{array}\right)+\left(\begin{array}{l}
		2 \\
		3
		\end{array}\right)+\left(\begin{array}{l}
		3 \\
		6
		\end{array}\right)+\left(\begin{array}{l}
		4 \\
		4
		\end{array}\right)\right]=\left(\begin{array}{c}
		3 \\
		3.8
		\end{array}\right) \\
		&\vec u_{2}=\frac{1}{n_{2}} \sum_{\vec x \in \mathcal{C}_{2}} \vec x^{(i)}=\frac{1}{5}\left[\left(\begin{array}{c}
		9 \\
		10
		\end{array}\right)+\left(\begin{array}{l}
		6 \\
		8
		\end{array}\right)+\left(\begin{array}{l}
		9 \\
		5
		\end{array}\right)+\left(\begin{array}{l}
		8 \\
		7
		\end{array}\right)+\left(\begin{array}{c}
		10 \\
		8
		\end{array}\right)\right]=\left(\begin{array}{l}
		8.4 \\
		7.6
		\end{array}\right)
		\end{aligned}
	\end{gather*}
	The covariance matrix of the first class is:
	\begin{gather*}
		\begin{aligned}
		S_{1}=\sum_{\vec x \in \mathcal{C}_{1}}(\vec x^{(i)}-&\left.\vec u_{1}\right)\left(\vec x^{(i)}-\vec u_{1}\right)^{T}=\left[\left(\begin{array}{c}
		4 \\
		2
		\end{array}\right)-\left(\begin{array}{c}
		3 \\
		3.8
		\end{array}\right)\right]^{2}+\left[\left(\begin{array}{l}
		2 \\
		4
		\end{array}\right)-\left(\begin{array}{c}
		3 \\
		3.8
		\end{array}\right)\right]^{2} \\
		&+\left[\left(\begin{array}{l}
		2 \\
		3
		\end{array}\right)-\left(\begin{array}{c}
		3 \\
		3.8
		\end{array}\right)\right]^{2}+\left[\left(\begin{array}{l}
		3 \\
		6
		\end{array}\right)-\left(\begin{array}{c}
		3 \\
		3.8
		\end{array}\right)\right]^{2}+\left[\left(\begin{array}{l}
		4 \\
		4
		\end{array}\right)-\left(\begin{array}{c}
		3 \\
		3.8
		\end{array}\right)\right]^{2} \\
		&=\left(\begin{array}{cc}
		1 & -0.25 \\
		-0.25 & 2.2
		\end{array}\right)
		\end{aligned}
	\end{gather*}
	Covariance matrix of the second class:
	\begin{gather*}
		\begin{aligned}
		S_{2}=\sum_{\vec x \in \mathcal{C}_{2}}(\vec x^{(i)}-&\left.\vec u_{2}\right)\left(\vec x^{(i)}-\vec u_{2}\right)^{T}=\left[\left(\begin{array}{c}
		9 \\
		10
		\end{array}\right)-\left(\begin{array}{l}
		8.4 \\
		7.6
		\end{array}\right)\right]^{2}+\left[\left(\begin{array}{l}
		6 \\
		8
		\end{array}\right)-\left(\begin{array}{l}
		8.4 \\
		7.6
		\end{array}\right)\right]^{2} \\
		&+\left[\left(\begin{array}{l}
		9 \\
		5
		\end{array}\right)-\left(\begin{array}{l}
		8.4 \\
		7.6
		\end{array}\right)\right]^{2}+\left[\left(\begin{array}{l}
		8 \\
		7
		\end{array}\right)-\left(\begin{array}{l}
		8.4 \\
		7.6
		\end{array}\right)\right]^{2}+\left[\left(\begin{array}{c}
		10 \\
		8
		\end{array}\right)-\left(\begin{array}{l}
		8.4 \\
		7.6
		\end{array}\right)\right]^{2} \\
		&=\left(\begin{array}{cc}
		2.3 & -0.05 \\
		-0.05 & 3.3
		\end{array}\right)
		\end{aligned}
	\end{gather*}
	Therefore we have the within-class scatter matrix:
	\begin{gather*}
		\begin{aligned}
		S_{W}=S_{1}+S_{2} &=\left(\begin{array}{cc}
		1 & -0.25 \\
		-0.25 & 2.2
		\end{array}\right)+\left(\begin{array}{cc}
		2.3 & -0.05 \\
		-0.05 & 3.3
		\end{array}\right)=\left(\begin{array}{cc}
		3.3 & -0.3 \\
		-0.3 & 5.5
		\end{array}\right)
		\end{aligned}
	\end{gather*}
	Between-class scatter matrix:
	\begin{gather*}
		\begin{aligned}
		S_{B}=&\left(\vec u_{1}-\vec u_{2}\right)\left(\vec u_{1}-\vec u_{2}\right)^{T} =\left[\left(\begin{array}{c}
		3 \\
		3.8
		\end{array}\right)-\left(\begin{array}{c}
		8.4 \\
		7.6
		\end{array}\right)\right]\left[\left(\begin{array}{c}
		3 \\
		3.8
		\end{array}\right)-\left(\begin{array}{c}
		8.4 \\
		7.6
		\end{array}\right)\right]^{T} \\
		&=\left(\begin{array}{c}
		-5.4 \\
		-3.8
		\end{array}\right)\left(\begin{array}{cc}
		-5.4 & -3.8
		\end{array}\right)=\left(\begin{array}{ll}
		29.16 & 20.52 \\
		20.52 & 14.44
		\end{array}\right)
		\end{aligned}
	\end{gather*}
	The LDA projection is then obtained as the solution of the generalized eigen value problem:
	\begin{gather*}
		\begin{aligned}
		S_{W}^{-1} S_{B} \vec\theta &=\lambda \vec\theta  \\
		& \det\left(S_{W}^{-1} S_{B}-\lambda I\right)=0 \\
		& \Rightarrow\left|\left(\begin{array}{cc}
		3.3 & -0.3 \\
		-0.3 & 5.5
		\end{array}\right)^{-1}\left(\begin{array}{cc}
		29.16 & 20.52 \\
		20.52 & 14.44
		\end{array}\right)-\lambda\left(\begin{array}{ll}
		1 & 0 \\
		0 & 1
		\end{array}\right)\right|=0 \\
		& \Rightarrow\left|\left(\begin{array}{ll}
		0.3045 & 0.0166 \\
		0.0166 & 0.1827
		\end{array}\right)\left(\begin{array}{cc}
		29.16 & 20.52 \\
		20.52 & 14.44
		\end{array}\right)-\lambda\left(\begin{array}{ll}
		1 & 0 \\
		0 & 1
		\end{array}\right)\right|=0 \\
		& \Rightarrow\left|\left(\begin{array}{cc}
		9.2213-\lambda & 6.489 \\
		4.2339 & 2.9794-\lambda
		\end{array}\right)\right| \\
		&=\left(\begin{array}{ll}
		9.2213-\lambda
		\end{array}\right)(2.9794-\lambda)-6.489 \times 4.2339=0 \\
		& \Rightarrow \lambda^{2}-12.2007 \lambda=0 \Rightarrow \lambda(\lambda-12.2007)=0 \\
		& \Rightarrow \lambda_{1}=0, \lambda_{2}=12.2007
		\end{aligned}
	\end{gather*}
	We have therefore also two eigenvectors. One is the zero vector so it has no interest. It remains only the second that takes us to write:
	\begin{gather*}
		S_{W}^{-1} S_{B} \vec\theta_2 =\lambda_2 \vec\theta_2 \Leftrightarrow \left(\begin{array}{cc}
		9.2213 & 6.489 \\
		4.2339 & 2.9794
		\end{array}\right) \left(\begin{array}{l}
		\theta^{(2)}_{1} \\
		\theta^{(2)}_{2}
		\end{array}\right)=\underbrace{12.2007}_{\lambda_{2}}\left(\begin{array}{l}
		\theta^{(2)}_{1} \\
		\theta^{(2)}_{2}
		\end{array}\right)
	\end{gather*}
	Therefore we just have to solve the following system:
	\begin{gather*}
		\begin{aligned}
		9.2213\theta^{(2)}_{1}+6.489\theta^{(2)}_{2}=12.2007\theta^{(2)}_{1}\\
		4.2339\theta^{(2)}_{1}+2.979\theta^{(2)}_{2}=12.2007\theta^{(2)}_{2}
		\end{aligned}
	\end{gather*}	
	Hence:
	\begin{gather*}
		\vec\theta^\ast=\vec\theta_2=\left(\begin{array}{l}
		\theta^{(2)}_{1} \\
		\theta^{(2)}_{2}
		\end{array}\right)=\left(\begin{array}{l}
		0.9088 \\
		0.4173
		\end{array}\right)
	\end{gather*}
	We can get the same result more directly using the relation derived also earlier above:
	\begin{gather*}
		\vec\theta^{\ast}  \propto S^{-1}_W(\vec u_2 - \vec u_1)
	\end{gather*}
	Hence:
	\begin{gather*}
	\begin{aligned}
		\vec \theta^{*}&=S_{W}^{-1}\left(\mu_{1}-\mu_{2}\right)=\left(\begin{array}{cc}
		3.3 & -0.3 \\
		-0.3 & 5.5
		\end{array}\right)^{-1}\left[\left(\begin{array}{c}
		3 \\
		3.8
		\end{array}\right)-\left(\begin{array}{c}
		8.4 \\
		7.6
		\end{array}\right)\right] \\
		&=\left(\begin{array}{cc}
		0.3045 & 0.0166 \\
		0.0166 & 0.1827
		\end{array}\right)\left(\begin{array}{c}
		-5.4 \\
		-3.8
		\end{array}\right)=\left(\begin{array}{l}
		0.9088 \\
		0.4173
		\end{array}\right)
		\end{aligned}
	\end{gather*}
	\begin{figure}[H]
		\centering
		\includegraphics[width=1\textwidth]{img/computing/lda_example_02.jpg}
	\end{figure}
	\end{tcolorbox}
	Notice that LDF will fail when the discriminatory information is not in the mean but rather in the variance of the data as illustrated below:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1215); %set diagram left start at 0, and has height of 1215
		
		%Shape: Axis 2D [id:dp9846152844920288] 
		\draw  (127.5,464.9) -- (527.5,464.9)(167.5,86) -- (167.5,507) (520.5,459.9) -- (527.5,464.9) -- (520.5,469.9) (162.5,93) -- (167.5,86) -- (172.5,93)  ;
		%Straight Lines [id:da04982013357789716] 
		\draw    (76,131) -- (413.5,432) ;
		%Straight Lines [id:da8610286509768736] 
		\draw    (539.5,220) -- (413.5,432) ;
		%Straight Lines [id:da6104625566694908] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (254,288) -- (342.5,135) ;
		%Straight Lines [id:da619569818726758] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (504.5,277) -- (342.5,135) ;
		%Straight Lines [id:da14925243702293134] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (499.5,286) -- (337.5,144) ;
		%Shape: Circle [id:dp38515384668977415] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (331.5,144) .. controls (331.22,141.24) and (333.24,139) .. (336,139) .. controls (338.76,139) and (341.22,141.24) .. (341.5,144) .. controls (341.78,146.76) and (339.76,149) .. (337,149) .. controls (334.24,149) and (331.78,146.76) .. (331.5,144) -- cycle ;
		%Shape: Circle [id:dp18307766000639902] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (338,133) .. controls (337.72,130.24) and (339.74,128) .. (342.5,128) .. controls (345.26,128) and (347.72,130.24) .. (348,133) .. controls (348.28,135.76) and (346.26,138) .. (343.5,138) .. controls (340.74,138) and (338.28,135.76) .. (338,133) -- cycle ;
		%Shape: Ellipse [id:dp18416180652097047] 
		\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]  (317,133) .. controls (317,114.5) and (328.64,99.5) .. (343,99.5) .. controls (357.36,99.5) and (369,114.5) .. (369,133) .. controls (369,151.5) and (357.36,166.5) .. (343,166.5) .. controls (328.64,166.5) and (317,151.5) .. (317,133) -- cycle ;
		%Shape: Ellipse [id:dp8808036116568427] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]  (359.02,120.33) .. controls (419.46,175.26) and (458.82,230.4) .. (446.94,243.48) .. controls (435.05,256.55) and (376.42,222.61) .. (315.98,167.67) .. controls (255.54,112.74) and (216.18,57.6) .. (228.06,44.52) .. controls (239.95,31.45) and (298.58,65.39) .. (359.02,120.33) -- cycle ;
		%Curve Lines [id:da3939959817626595] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (76,131) .. controls (90.8,143.8) and (209.6,247.8) .. (228,262.6) .. controls (246.4,277.4) and (301.25,180.6) .. (312,187.8) .. controls (322.75,195) and (264.85,292) .. (279.25,308) .. controls (293.65,324) and (385.9,408) .. (413.5,432) ;
		%Curve Lines [id:da5886141788090522] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (76,131) .. controls (228.8,265.4) and (232.85,217.9) .. (275.25,251.5) .. controls (317.65,285.1) and (280.7,314.4) .. (413.5,432) ;
		%Curve Lines [id:da5968908134959541] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (413.5,432) .. controls (430.8,401) and (481.6,323.8) .. (486,302.2) .. controls (490.4,280.6) and (453.6,244.6) .. (460.4,237.4) .. controls (467.2,230.2) and (504,260.2) .. (518.4,248.6) .. controls (532.8,237) and (536.4,225.4) .. (539.5,220) ;
		%Curve Lines [id:da6695786097789556] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (413.5,432) .. controls (430.8,401) and (476,333) .. (480.4,311.4) .. controls (484.8,289.8) and (448,253.8) .. (454.8,246.6) .. controls (461.6,239.4) and (498.4,269.4) .. (512.8,257.8) .. controls (527.2,246.2) and (537.2,223.8) .. (539.5,220) ;
		
		% Text Node
		\draw (145,473) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
		% Text Node
		\draw (144,67.4) node [anchor=north west][inner sep=0.75pt]    {$x_{2}$};
		% Text Node
		\draw (526,472.4) node [anchor=north west][inner sep=0.75pt]    {$x_{1}$};
		% Text Node
		\draw (188.73,242.22) node [anchor=north west][inner sep=0.75pt]  [rotate=-41.35] [align=left] {Principal Component Analysis (PCA)};
		% Text Node
		\draw (422.72,434.29) node [anchor=north west][inner sep=0.75pt]  [rotate=-301] [align=left] {Linear Discriminant Analysis (LDA)};
		
		\end{tikzpicture}
		\vspace*{3mm} 
		\caption{Typical case where LDA fail and PCA don't }
	\end{figure}
	Through this example with see better why LDA is not only a classification technique. It is a composite, two-stage technique: first it reduce dimensionality, then it classifies.
	
	Classification of new data that weren't in the training set is essentially done with the class that belongs the closest class mean\footnote{And in the case of the Bayesian approach of LDA a prior term is added to shift the mean closer or further from the new point.}.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The reader can refer to our \texttt{R} or MATLAB™ free companion books to see examples with the respective script language of these two softwares for the same set of numerical data.
	\end{tcolorbox}

	\subparagraph{Multiclass problem}\mbox{}\\\\
	Based on two classes problem, we can see that the Fisher's LDA generalizes gracefully for multiple classes problem (we speak then of "Multiple Discriminant Analysis"). Assume we still have a set of $D$-dimensional samples $X = \{\vec x^{(1)}, \vec x^{(2)}, \ldots  , \vec x^{(m)} \}$, and there are totally $C$ classes. Instead of one projection $y$, mentioned above, we now will seek $(\mathcal{C}-1)$ projections $[y_1, y_2, \dots y_{\mathcal{C}-1}]$ by means of $(\mathcal{C}-1)$ projection vectors $\vec\theta_i$ arranged by columns into a projection matrix $\Theta = [\vec\theta_1 | \vec\theta_2 | \ldots | \vec\theta_{C-1}]$, where: 
	
	First we will use the scatters in space-$x$ as following:
	\begin{itemize}
		\item Within-class scatter matrix:
		
	
		\item Between-class scatter matrix:
		
		\item Total scatter matrix:
		
	\end{itemize}
	Similarly, we define the mean vector and scatter matrices for the projected samples as:
	\begin{itemize}
		\item $\vec{\hat{u}}_i = \frac{1}{n_i} \sum_{i \in \mathcal{C}_i} \vec y^{\,(i)}$ 
		\item $\vec{\hat{u}} = \frac{1}{N} \sum_{i=1}^{m}\vec y^{\,(i)}$
		\item $ \hat{S}_W = \sum_{i=1}^{\mathcal{C}}\sum_{\vec y \in \mathcal{C}_i} (\vec y - \vec{\hat{u}}_i)(\vec y - \vec{\hat{u}}_i)^T $
		\item $\hat{S}_B = \sum_{i=1}^{\mathcal{C}} n_i (\vec{\hat{u}}_i - \vec{\hat{u}})(\vec{\hat{u}}_i - \vec{\hat{u}})^T$
	\end{itemize}
	From our derivation for the two-class problem, we can get:
	
	Recall that we are looking for a projection that maximizes the ratio of between-class to within-class scatter. Since the projection is no longer a scalar (it has $\mathcal{C}-1$ dimensions), we use the determinant of the scatter matrices to obtain a scalar objective function:
	
	And now, our job is to seek the projection matrix $\Theta^{\ast}$ that maximize this ratio. We will not give the derivation process. But we know that the optimal projection matrix $\Theta^{\ast}$ is the one whose columns are the eigenvectors corresponding to the largest eigenvalues of the following generalized eigenvalue problem:
	
	Hence:
	
	Thus, if $S_W$ is a non-singular matrix, and can be inverted, then the Fisher's criterion is maximized when the projection matrix $\Theta^{\ast}$ is composed of the eigenvectors of:
	
	Noticed that, there will be at most $\mathcal{C}-1$ eigenvectors with non-zero real corresponding eigenvalues $\lambda_i$. This is because $S_B$ is of rank $(\mathcal{C}-1)$ or less. So we can see that LDA can represent a massive reduction in the dimensionality of the problem. In face recognition for example there may be several thousand variables, but only a few hundred classes.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The reader must keep in mind that what we have seen here is a naive approach of LDA. Investigating further and using Bayesian analysis, we can prove that the above results are just a special case of that latter. Maybe (or maybe not) we will provide the detailed bayesian derivation in the 5th edition of the present book.
	\end{tcolorbox} 
	
	\pagebreak
	\subsubsection{Dimensionality Reduction}
	"\NewTerm{Dimensionality reduction}\index{dimensionality reduction}" or "\NewTerm{dimension reduction}\index{dimension reduction}" is the process of reducing the number of random variables under consideration.
	
	More technically, dimensionality reduction is typically choosing a basis or mathematical representation within which you can describe most but not all of the variance within your data, thereby retaining the relevant information, while reducing the amount of information necessary to represent it.
	
	Approaches can be divided into "feature selection" and "feature extraction".
	
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{itemize}
		\item[D1.] "\NewTerm{Feature selection}\index{feature selection}" approaches try to find a subset of the known input variables (also naed "features" or "attributes"). The three strategies are: the filter strategy (e.g. information gain), the wrapper strategy (e.g. search guided by accuracy), and the embedded strategy (selected features are added or are removed while building the model based on prediction errors).
		
		Typical feature selection methods are:
		\begin{itemize}
			\item Missing Value Ratio
			\item Low Variance Filter
			\item High Correlation Filter
			\item Random Forests
			\item Backward Feature Selection
			\item Forward Feature Selection
			\item ...
		\end{itemize}
		
		\item[D2.] "\NewTerm{Feature extraction}\index{feature extraction}"  approaches transform the data from a well known interpretable high-dimensional space to a space of smaller abstract dimensional space. The data transformation may be linear, as in Principal Component Analysis (we have already study quite in detail the naive version of Principal Component Analysis in the section Statistics page \pageref{principal component analysis}), but many non-linear dimensionality reduction techniques also exist.
		
		Typical feature extraction methods are:
		\begin{itemize}
			\item Components/Factor based:
			\begin{itemize}
				\item Factor Analysis
				\item Principal Component Analysis
				\item Independent Component Analysis
				\item ...
			\end{itemize}
			\item Projection based:
			\begin{itemize}
				\item ISOMAP
				\item $T$-SNE
				\item UMAP
				\item Linear discriminant analysis (LDA)
				\item ...
			\end{itemize}
		\end{itemize}
	\end{itemize}
	For a more exhaustive list of techniques the reader can refer as always to the Data Science Mind Map at page \pageref{mindmap of data science}.
	
	\paragraph{Principal Component Analysis}\mbox{}\\\\
	As already mentioned earlier, PCA unsupervised feature selection dimensionality reduction is already treated quite with details in its naive version in the section of Statistics at page \pageref{principal component analysis}. 
	
	\paragraph{Linear Discriminant Analysis (LDA)}\mbox{}\\\\
	The LDA supervised projection dimensionality reduction method has also been treated earlier quite in details in it's naive version at page \pageref{linear discriminant analysis} because it is also a classification technique as we saw it! 
	
	So let us focus on the second most common component/factor based dimensionality reduction technique that is MDS:
	
	\pagebreak
	\paragraph{MultiDimensional Scaling (MDS)}\mbox{}\\\\
	The goal of "\NewTerm{multidimensional scaling}\index{multidimensional scaling}\label{multidimensional scaling}" (MDS) is to visualize a set of $N$ objects based on their similarities measured in $n$ different aspects, and the "\NewTerm{classical MDS}\index{classical MDS}" (cMDS), also known as "\NewTerm{principal coordinates analysis}\index{principal coordinates analysis}" (PCoA), is one of the methods for MDS but that adds the possibility to reduce the space dimension of the original set.
	
	Let us introduce the first aspect of MDS. Being known the level of similarity of individual cases of a dataset (without knowing their intrinsic characteristics!), the purpose of MDS is to find a way of visualizing the original intrinsic unknown characteristics. In other words, MDS is used to translate information about the pairwise distances among a set of $N$ objects or individuals into a configuration of $N$ points mapped into an abstract Cartesian space.
	
	Formally, given a pairwise similarity matrix $D_x=[d^x_{ij}]_{N\times N}$ (\SeeChapter{see section Linear Algebra page \pageref{distance matrix}}) of a set of $N$ objects ($N$ data points in an $n$-dimensional space) $X=[\vec{x}_1,\ldots,\vec{x}_N]$ but of unknown components, where the $ij$-th component $d^x_{ij}$ is some measurement of the similarity between $\vec{x}_i$ and $\vec{x}_j$ such as the Euclidean distance $d^x_{ij}=\vert\vert\vec{x}_i-\vec{x}_j\vert\vert _2$ between $\vec{x}_i$ and $\vec{x}_j$, we want to be able to construct a map $Y=[\vec{y}_1,\ldots,\vec{y}_N]$ in an $n$ dimensional space that explains well enough the original $D_x$ by reversing the original unknown components of the $X=[\vec{x}_1,\ldots,\vec{x}_N]$. When $n=2$ or $3$, the resulting dataset $Y$ can be visualized and the corresponding chart is then name a "\NewTerm{perceptual plot}".
	
	So the idea is given the only known dissimilarity matrix $D_x=[d^x_{ij}]_{N\times N}$ to find a best estimate of the original unknown $X=[\vec{x}_1,\ldots,\vec{x}_N]$, that we will denote $Y=[\vec{y}_1,\ldots,\vec{y}_N]$. These estimates will obviously lead to a dissimilarity matrix that we will denote $D_y=[d^y_{ij}]_{N\times N}$.
	
	Our goal again is then to find $Y=\left[\vec{y}_{1}, \ldots, \vec{y}_{N}\right]$ so that its pairwise similarity matrix $D_{y}$ matches $D_{x}$ optimally, in the sense that the following objective function, named sometimes "strain" in the MDS study field, is minimized:
	
	with obviously all $d^x_{ij}$ who are given, but where the given estimated pairwise similarity of Euclidean distance:
	
	have to be determined.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	This optimization problem is also often denoted:
	
	with $D_{x}^{2}$ given and $D_{\hat{x}}$ to be found when calculated as:
	
	\end{tcolorbox}
	The optimisation problem above is often written explicitly as (but there are other empirical choices as we already know that leads to different final results):
	
	otherwise it doesn't make really sense.
	
	This relation is often normalized (to have values that have non physical units) and root squared (it's more convenient to deal with distance)and a special name is given to it, the "\NewTerm{Kruskal's Stress coefficient}\index{Kruskal's Stress coefficient}" and denoted:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Some computer softwares don't normalize, nor take the square root, that's why the objective final value function may differ between softwares.
	\end{tcolorbox}
	
	\subparagraph{Classical MDS (cMDS)}\mbox{}\\\\
	Given again a pairwise similarity matrix $D_x=[d^x_{ij}]_{N\times N}$ of a set of $N$ objects ($N$ data points in an $n$ dimensional space) $X=[\vec{x}_1,\ldots,\vec{x}_N]$, where the $ij$-th component $d^x_{ij}$ is some measurement of the similarity between $\vec{x}_i$ and $\vec{x}_j$ such as the Euclidean distance $d^x_{ij}=\vert\vert\vec{x}_i-\vec{x}_j\vert\vert _2$ between $\vec{x}_i$ and $\vec{x}_j$, we can construct a map $Y=[\vec{y}_1,\ldots,\vec{y}_N]$ in an $m$ dimensional space ($m<n$) so that its pairwise distance matrix $D_y=[d^y_{ij}]_{N\times N}=\vert\vert\vec{y}_i-\vec{y}_j\vert\vert _2$ explains well enough the original $D_x$. As $m<n$, cMDS can be considered as a method for dimension reduction. 
	
	In some cases only the similarities $d^x_{ij}\;(i,j=1,\ldots,N)$ are given, while the $N$ objects in $X$ are not explicitly given, and the dimensionality $n$ may not even be known!

	We first assume the given pairwise similarity is the Euclidean distance $d_{ij}=\vert\vert\vec{x}_i-\vec{x}_j\vert\vert _2$:
	
	and we build the following similarity matrix:
	
	where we have defined the following three matrices to simplify the notations:
	
	We then introduce a centering matrix (for recall $\mathds{J}$ is a matrix of ones as already defined in the section of Linear Algebra):
	
	 which is an $N \times N$ and symmetric $\mathrm{C}_{N}^{T}=\mathrm{C}_{N}$.
	 
	 Pre-multiplying $C_{N}$ by a column vector $\vec{a}=\left[a_{1}, \ldots, a_{N}\right]^{T}$ removes the mean $\bar{a}=\sum_{i=1}^{N} a_{i} / N$ of from each component of $\vec{a}$ :
	
	Taking transpose on both sides, we see that post-multiplying $\mathrm{C}_{N}$ by a row vector $\vec{a}^{T}$ removes the mean of $\vec{a}^{T}$ from each component:
	
	We now apply double centering to $D_{x}^{2}$ by pre and post multiplying $C_{N}$ to $D_{x}^{2}$:	
	
	where $X_{r} C_{N}=\mathds{O}$ as all components of each row of $X_{r}$ are the same as their mean, and removing the mean results in a zero row vector. Similarly, $C_{N} X_{c}=\mathds{O}$, and we have also defined just above:
		
	with the mean of each row of $X$ removed, i.e., the $i$th column is:
	

	We therefore see that $\sum_{i=1}^{N} \bar{x}_{i}=0$, ie., all vectors in $\bar{X}=\left[\bar{x}_{1}, \ldots, \bar{x}_{N}\right]$ are those in $X=\left[\vec{x}_{1}, \ldots, \vec{x}_{N}\right]$ shifted by the mean vector $\bar{X}$. Now the points in $\bar{X}$ are centralized, but their pairwise similarities remain the same as the given $d_{i j}^{x}$.
	
	The relation:
	
	that we have just proved earlier can be rearranged to get:
	
	that is Gram matrix (\SeeChapter{see section Linear Algebra page \pageref{Gram matrix}}) of $\overline{X}$.
	
	Our goal is to find $Y=\left[y_{1}, \cdots, y_{N}\right]$ so that its pairwise similarity matrix $D_{y}$ matches $D_{x}$ optimally, in the sense that
the following objective function is minimized:
	
	Assuming $Y$ is also a double centered matrix as well as $X$, we get its Gram matrix:
	
	and the objective function above can be redefined as:
	
	We now carry out eigenvalue decomposition (\SeeChapter{see section Linear Algebra page \pageref{orthogonal decomposition}}) of the $N \times N$ symmetric matrix $B_{x}$ to find its real eigenvalue matrix $\Lambda$ and the orthogonal eigenvector matrix $V$ satisfying $B_{x} V= \Lambda V$, ie:
	
	To minimize the objective function $o(Y)=\left\|B_{y}-B_{x}\right\|$, we let:	
	
	Each column $\vec{y}_i$ in $Y=[\vec{y}_1,\ldots,\vec{y}_N]$ is an $n$ dimension vector. To reduce the dimensionality from $n$ to a given chosen $m$, we take the first $m$ rows of $Y$ corresponding to the $m$ greatest eigenvalues of $\lambda_1\ge\ldots\ge\lambda_m\ge\ldots\ge\lambda_N$ and their corresponding eigenvectors $\vec{v}_1,\ldots,\vec{v}_m$ as we do in Principal Component Analysis (\SeeChapter{see section Statistics page \pageref{principal component analysis}}).
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In practice it happens sometimes that only $D_x^2$ is given without the original coordinates of the $N$ data points $X=[\vec{x}_1,\ldots,\vec{x}_N]$. Also the objective function may have different empirical expression and the distance may be another one than the classic Euclidean.
	\end{tcolorbox}
	
	In summary, here are the different steps:
	\begin{itemize}
		\item Given the pairwise similarity, an $N\times N$ matrix $D_x=[d_{ij}]$, construct the squared proximity matrix $D^2_x=[d^2_{ij}]$
		
		\item Apply double centering to get $B_x=-C_ND^2_xC_N$;
		
		\item Find the $m$ greatest eigenvalues $\lambda_1,\ldots,\lambda_m$ of $B_x$, and the corresponding eigenvectors $\vec{v}_1,\ldots,\vec{v}_m$;
		
		\item Get the map:

		

	\end{itemize}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	For a practical application the reader can take a look to our \texttt{R} or MATLAB™ companion books.
	\end{tcolorbox}
	
	So let us focus on the third most common component/factor based dimensionality reduction technique:
	
	\pagebreak
	\paragraph{Factor Analysis (Exploratory Factor Analysis (EFA))}\mbox{}\\\\
	The method of "\NewTerm{Factor Analysis}\index{factor analysis}\label{factor analysis}" (FA), also named "\NewTerm{Exploratory Factor Analysis}\index{exploratory factor analysis}\label{exploratory factor analysis}" (EFA), not to be confuse with the method of Chi-square Correspondence Factor Analysis (\SeeChapter{see section Statistics page \pageref{chi-square correspondence factor analysis}}), models a set of $D$ observed manifest variables in $\vec{x}=[x_1,\ldots,x_D]^T$ as a linear combination of a set of $d<D$ unobserved hidden "\NewTerm{latent variables}" or "\NewTerm{common factors}" in $\vec{z}=[z_1,\ldots,z_d]^T$, to explain and reveal the variability and dependency among the $D$ observed variables, typically correlated, in terms of the latent variables, assumed to be independent and therefore uncorrelated. The method of FA is therefore considered as a means for dimensionality reduction\footnote{The text that follows is completely inspired from the lecture notes of the professor Ruye Wang (\url{http://fourier.eng.hmc.edu/e176/lectures/index_e176.html})}!
	
	Specifically, we assume each of the observed variables in $\vec{x}$ is a linear combination of the $d$ factors in $\vec{z}$:
	
	with $i=1,\ldots,D$ or in matrix form:
	
	where:	
	
	is a $D\times d$ factor loading matrix, and $\varepsilon_i$ is the noise associated with $x_i$.  
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In many textbooks the reader can find the following notation:
	
	where the $l_{ij}$ are named the "loadings" and the $F_j$ the "latent factors".
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let's approach this technique by going directly with an example. For this, consider that $5$ candidates have passed $3$ exams in the field of Finance, Statistics and Standards (Norms) to validate their entry into a MBA course. Each of the corrected exams is marked on a scale ranging from $0$ to $10$ points corresponding to the variables to be explained. The data is summarized in the table below:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline 
		\rowcolor[gray]{0.75}Candidate n$^{\circ}$ & Finance ${y_{1}}$ & Statistics ${y_{2}}$ & Norms $y_{3}$ \\
		\hline $1$ & $3$ & $6$ & $5$ \\
		\hline $2$ & $7$ & $3$ & $3$ \\
		\hline $3$ & $10$ & $9$ & $8$ \\
		\hline $4$ & $3$ & $9$ & $7$ \\
		\hline $5$ & $10$ & $6$ & $5$ \\
		\hline
		\end{tabular}
	\end{table}
	It was suggested by an expert committee that the results obtained are dependent on two explanatory variables not directly observable, $f_1$ and $f_2$, which are respectively judged as being the \textit{Analytical Capacity} and the \textit{Memorization Capacity} of the candidates. It is accepted that each of the explained variables depends linearly on these two factors such as (this is a notation common in french textbooks for EFA):
	\begin{equation*}
		\begin{array}{l}
		y_{1}=\beta_{10}+\beta_{11} f_{1}+\beta_{12} f_{2}+\varepsilon_{1} \\
		y_{2}=\beta_{20}+\beta_{21} f_{1}+\beta_{22} f_{2}+\varepsilon_{2} \\
		y_{3}=\beta_{30}+\beta_{31} f_{1}+\beta_{32} f_{2}+\varepsilon_{3}
		\end{array}
	\end{equation*}
	For information, this type of model is represented in the framework of the Structural Equation Modelling (we will introduce this type of model later) in the following way (disregarding the constant and the error terms):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1215); %set diagram left start at 0, and has height of 1215
		
		%Shape: Circle [id:dp8120986822487546] 
		\draw  [line width=1.5]  (231,107) .. controls (231,93.19) and (242.19,82) .. (256,82) .. controls (269.81,82) and (281,93.19) .. (281,107) .. controls (281,120.81) and (269.81,132) .. (256,132) .. controls (242.19,132) and (231,120.81) .. (231,107) -- cycle ;
		%Shape: Circle [id:dp38962538266229174] 
		\draw  [line width=1.5]  (316,106) .. controls (316,92.19) and (327.19,81) .. (341,81) .. controls (354.81,81) and (366,92.19) .. (366,106) .. controls (366,119.81) and (354.81,131) .. (341,131) .. controls (327.19,131) and (316,119.81) .. (316,106) -- cycle ;
		%Shape: Rectangle [id:dp32658838637278853] 
		\draw  [line width=1.5]  (179,221) -- (258.5,221) -- (258.5,279.89) -- (179,279.89) -- cycle ;
		%Shape: Rectangle [id:dp8143663972307038] 
		\draw  [line width=1.5]  (340,221) -- (419.5,221) -- (419.5,279.89) -- (340,279.89) -- cycle ;
		%Straight Lines [id:da15025335812170715] 
		\draw    (256,132) -- (222.23,218.14) ;
		\draw [shift={(221.5,220)}, rotate = 291.41] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da5731767900585865] 
		\draw    (256,132) -- (374.89,219.81) ;
		\draw [shift={(376.5,221)}, rotate = 216.45] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da15026449707805245] 
		\draw    (341,131) -- (223.1,218.81) ;
		\draw [shift={(221.5,220)}, rotate = 323.32] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da34747971737736427] 
		\draw    (341,131) -- (375.77,219.14) ;
		\draw [shift={(376.5,221)}, rotate = 248.47] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		
		% Text Node
		\draw (246.5,98.4) node [anchor=north west][inner sep=0.75pt]    {$f_{1}$};
		% Text Node
		\draw (331.5,97.4) node [anchor=north west][inner sep=0.75pt]    {$f_{2}$};
		% Text Node
		\draw (211,245) node [anchor=north west][inner sep=0.75pt]    {$y_{1}$};
		% Text Node
		\draw (373,245) node [anchor=north west][inner sep=0.75pt]    {$y_{2}$};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (224,142) -- (254,142) -- (254,168) -- (224,168) -- cycle  ;
		\draw (227,146.4) node [anchor=north west][inner sep=0.75pt]    {$\beta _{11}$};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (255,171.75) -- (285,171.75) -- (285,197.75) -- (255,197.75) -- cycle  ;
		\draw (258,176.15) node [anchor=north west][inner sep=0.75pt]    {$\beta _{12}$};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (315.25,171.75) -- (345.25,171.75) -- (345.25,197.75) -- (315.25,197.75) -- cycle  ;
		\draw (318.25,176.15) node [anchor=north west][inner sep=0.75pt]    {$\beta _{21}$};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (344,142) -- (374,142) -- (374,168) -- (344,168) -- cycle  ;
		\draw (347,146.4) node [anchor=north west][inner sep=0.75pt]    {$\beta _{22}$};
		\end{tikzpicture}
	\end{figure}
	With the following assumptions:
$$\text{E}(\varepsilon_i)=0,\text{V}(\varepsilon_i)=\sigma^2_{\varepsilon_i},\text{cov}(\varepsilon_i,\varepsilon_j)=0\; \forall i\neq j$$
	and:
	$$\text{E}(f_i)=0,\text{V}(f_i)=1,\text{cov}(f_i,f_j)=0\;  \forall i\neq j$$
	The model is generally written in french textbooks in the following matrix form, using our particular example:
	\begin{equation*}
		\bar{Y}=\bar{\beta}_{0}+L \bar{f}+\bar{\varepsilon}=\left(\begin{array}{l}
		\beta_{10} \\
		\beta_{20} \\
		\beta_{30}
		\end{array}\right)+\left[\begin{array}{lll}
		\beta_{11} & \beta_{12} & \beta_{13} \\
		\beta_{21} & \beta_{22} & \beta_{23} \\
		\beta_{31} & \beta_{32} & \beta_{33}
		\end{array}\right]\left(\begin{array}{c}
		f_{1} \\
		f_{2} \\
		f_{3}=0
		\end{array}\right)+\left(\begin{array}{l}
		\varepsilon_{1} \\
		\varepsilon_{2} \\
		\varepsilon_{3}
		\end{array}\right)
	\end{equation*}
	\end{tcolorbox}
	Also, for simplicity and without loss of generality, we assume the dataset has a zero mean. If $W$ were available, the $d$ factors in $\vec{z}$ can be found by solving this over-determined linear equation system of $D$ equations but $d<D$ unknowns by the least-square method as seen at page \pageref{information matrix} (with minimum squared error $\vert\vert\vec{\varepsilon}\vert\vert^2$):
	
	where for recall $W^-=(W^TW)^{-1}W^T$ is the left pseudo-inverse of $W$.
	
	However, as neither $W$ nor $\vec{z}$ is available, we need to estimate both of them at the same time, based on a set of $N$ observed data points $X=[\vec{x}_1,\ldots,\vec{x}_N]$, typically $N\gg d$, by the method of expectation-maximization (EM), which in general is an iterative algorithm to find the maximum likelihood (ML) or maximum a posteriori (MAP) estimates of some model parameters.
	
	We treat both $\vec{z}$ and $\vec{\varepsilon}$ as random vectors, and make the following assumptions:
	\begin{itemize}
		\item The latent variables in $\vec{z}$ are of zero mean, independent of each other, and with unity variance:
		
		and they are Normally distributed:
		
		
		\item The noise components in $\vec{\varepsilon}$ are of zero mean, independent of each other, and with a diagonal covariance matrix:
		
		and they are also normally distributed:
		
		
		\item The latent variables and the noise are independent of each other:
		
	\end{itemize}
	The two matrices $W$ and $\Psi$ defined above are the parameters of the FA model, generally denoted by $\theta=\{W,\Psi\}$.
	
	As a linear combination of the two Gaussian vectors $\vec{z}$ and $\vec{\varepsilon}$ then:
	
	 also has a Gaussian distribution. We first find its mean $\text{E}(\vec{x})$:
	
	and variance-covariance matrix $\Sigma_x$ (see page \pageref{normal multivariate distribution}):
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	That last relation is often denoted in textbooks as:
	
	with therefore:
	
	The portion of the variance of the $i$th variable contributed by the $m$ common factors is named the "\NewTerm{$i$th communality}\index{communality}". That portion of $\text{V}(x_i)=\sigma_{ii}$ due to the specific factor is often named the "\NewTerm{uniqueness}", or "\NewTerm{specific variance}\index{specific variance}". Denoting the $i$th communality by $h^2_i$, we see that:
	
	where:
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us observe what are the implications of the aforementioned assumptions. To do this, let's write the general form of each of the linear relations from our previous example:
	\begin{equation}
		y_{i}=\beta_{i 0}+\beta_{i 1} f_{1}+\beta_{i 2} f_{2}+\varepsilon_{i}
	\end{equation}
	If we calculate the variance, then we have using its properties:
	\begin{equation}
		\begin{array}{l}
		\text{V}\left(y_{i}\right)=\text{V}\left(\beta_{i 0}\right)+\text{V}\left(\beta_{11} f_{1}\right)+\text{V}\left(\beta_{i 2} f_{2}\right)+\text{V}\left(\varepsilon_{i}\right) \\
		=\text{V}\left(\beta_{i 1} f_{1}\right)+\text{V}\left(\beta_{i 2} f_{2}\right)+\sigma_{i}^{2}=\beta_{i 1}^{2} \text{V}\left(f_{1}\right)+\beta_{i 2}^{2} V\left(f_{2}\right)+\sigma_{\varepsilon_{i}}^{2} \\
		=\underbrace{\beta_{i 1}^{2}+\beta_{i 2}^{2}}_{\text {communality}}+\sigma_{\varepsilon_{i}}^{2}
		\end{array}
	\end{equation}
	Hence in the general case:
	\begin{equation}
		\text{V}\left(y_{i}\right)=\underbrace{\sum_{j=1} \beta_{i 1}^{2}}_{\text {communality}}+\sigma_{\varepsilon_{i}}^{2}
	\end{equation}
	The "communality" is then the part of the variance of the variable which is explained purely by the factors of the explanatory variables of the model. The greater the communality, the better the predictive power of the factors !!\\

	Obviously, if the factors are perfect predictors, we have the specific variance $\sigma^2_{\varepsilon_i}$ which would then be zero.
	\end{tcolorbox}
	
	And then get the Normal distribution of $\vec{x}$ given $\vec{\theta}$:
	
	We can further find the joint distribution $P(\vec{z},\vec{x})$, also a Gaussian, which has a zero mean:
	
	and a covariance matrix $\Sigma$ composed of four submatrices:
	
	where he have immediately:
	
	Now the Normal distribution $P(\vec{z},\vec{x}\vert\vec{\theta})$ given model parameter $\vec{\theta}=\{W,\Psi\}$ can be expressed as:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us now calculate the covariance of a couple of variables $y_i,y_j$ from our previous example by first writing:
	\begin{equation}
		\begin{array}{l}
		y_{i}=\beta_{i 0}+\beta_{i 1} f_{1}+\beta_{i 2} f_{2}+\varepsilon_{i} \\
		y_{j}=\beta_{j 0}+\beta_{j 1} f_{1}+\beta_{j 2} f_{2}+\varepsilon_{j}
		\end{array}
	\end{equation}
	We then have:
	
	From the definition of covariance we know that the constants will vanish. So we can focus on:
	
	We saw just previously that in the simple case where $ i = j $, we have in our particular case:
	
	Now let's deal with the case where $ i \neq j $. We then have:
	\begin{equation}
	\begin{array}{l}
		\operatorname{cov}\left(y_{i}, y_{j}\right)=\operatorname{cov}\left(\beta_{i 1} f_{1}+\beta_{i 2} f_{2}+\varepsilon_{i}, \beta_{j 1} f_{1}+\beta_{j 2} f_{2}+\varepsilon_{j}\right) \\
		=\beta_{j 1} \operatorname{cov}\left(\beta_{i 1} f_{1}+\beta_{i 2} f_{2}+\varepsilon_{i}, f_{1}\right)+\beta_{j 2} \operatorname{cov}\left(\beta_{i 1} f_{1}+\beta_{i 2} f_{2}+\varepsilon_{i}, f_{2}\right) \\
		+\operatorname{cov}\left(\beta_{i 1} f_{1}+\beta_{i 2} f_{2}+\varepsilon_{i}, \varepsilon_{j}\right) \\
		=\beta_{j 1}\left(\beta_{i 1} \operatorname{cov}\left(f_{1}, f_{1}\right)+\beta_{i 2} \operatorname{cov}\left(f_{2}, f_{1}\right)+\operatorname{cov}\left(\varepsilon_{i}, f_{1}\right)\right) \\
		+\beta_{j 2}\left(\beta_{i 1} \operatorname{cov}\left(f_{1}, f_{2}\right)+\beta_{i 2} \operatorname{cov}\left(f_{2}, f_{2}\right)+\operatorname{cov}\left(\varepsilon_{i}, f_{2}\right)\right) \\
		+1 \cdot\left(\left(\beta_{i 1} \operatorname{cov}\left(f_{1}, \varepsilon_{j}\right)+\beta_{i 2} \operatorname{cov}\left(f_{2}, \varepsilon_{j}\right)+\operatorname{cov}\left(\varepsilon_{i}, \varepsilon_{j}\right)\right)\right) \\
		=\beta_{j 1}\left(\beta_{i 1} \text{V}\left(f_{1}\right)+\beta_{i 2} \operatorname{cov}\left(f_{2}, f_{1}\right)+\operatorname{cov}\left(\varepsilon_{i}, f_{1}\right)\right) \\
		+\beta_{j 2}\left(\beta_{i 1} \operatorname{cov}\left(f_{1}, f_{2}\right)+\beta_{i 2} \text{V}\left(f_{2}\right)+\operatorname{cov}\left(\varepsilon_{i}, f_{2}\right)\right) \\
		+1 \cdot\left(\left(\beta_{i 1} \operatorname{cov}\left(f_{1}, \varepsilon_{j}\right)+\beta_{i 2} \operatorname{cov}\left(f_{2}, \varepsilon_{j}\right)+\operatorname{cov}\left(\varepsilon_{i}, \varepsilon_{j}\right)\right)\right) \\
		=\beta_{j 1}\left(\beta_{i 1}+\beta_{i 2} \operatorname{cov}\left(f_{2}, f_{1}\right)+\operatorname{cov}\left(\varepsilon_{i}, f_{1}\right)\right) \\
		+\beta_{j 2}\left(\beta_{i 1} \operatorname{cov}\left(f_{1}, f_{2}\right)+\beta_{i 2}+\operatorname{cov}\left(\varepsilon_{i}, f_{2}\right)\right) \\
		+1 \cdot\left(\left(\beta_{i 1} \operatorname{cov}\left(f_{1}, \varepsilon_{j}\right)+\beta_{i 2} \operatorname{cov}\left(f_{2}, \varepsilon_{j}\right)+\operatorname{cov}\left(\varepsilon_{i}, \varepsilon_{j}\right)\right)\right)
		\end{array}
	\end{equation}
	Since the factors are assumed to be independent random variables by assumption in the model, their covariance is zero. It is the same for the terms of errors. It remains then:
	\begin{equation}
		\begin{array}{l}
		\operatorname{cov}\left(y_{i}, y_{j}\right)=\beta_{j 1}\left(\beta_{11}+\operatorname{cov}\left(\varepsilon_{i}, f_{1}\right)\right)+\beta_{j 2}\left(\beta_{i 2}+\operatorname{cov}\left(\varepsilon_{i}, f_{2}\right)\right) \\
		+1 \cdot\left(\left(\beta_{i 1} \operatorname{cov}\left(f_{1}, \varepsilon_{j}\right)+\beta_{i 2} \operatorname{cov}\left(f_{2}, \varepsilon_{j}\right)\right)\right)
		\end{array}
	\end{equation}
	We will make the assumption that the errors are random variables independent of the factors. So at the level of the covariances for the moment all the assumptions are then:
	\begin{equation}
		\begin{array}{l}
		\operatorname{cov}\left(f_{i}, f_{j}\right)_{i \neq j}=0 \\
		\operatorname{cov}\left(\varepsilon_{i}, f_{j}\right)_{i=j}=0 \\
		\operatorname{cov}\left(\varepsilon_{i}, \varepsilon_{j}\right)_{i \neq j}=0
		\end{array}
	\end{equation}
	So their covariance also becomes zero and it remains:
	\begin{equation}
		\text{cov}\left(y_{i}, y_{j}\right)_{i \neq j}=\beta_{j 1} \beta_{i 1}+\beta_{j 2} \beta_{i 2}
	\end{equation}
	So in the general case where $i\neq j$ we finally have:
	\begin{equation}
		\text{cov}\left(y_{i}, y_{j}\right)_{i \neq j}=\sum_{k=1}^{m} \beta_{j k} \beta_{i k}
	\end{equation}
	We can then build the "theoretical matrix of the variances-covariances of the factorial model" in the particular case which serves as an example, we have:
	\begin{equation}
		\begin{array}{l}
		C=\left[\begin{array}{ccc}
		\beta_{11}^{2}+\beta_{12}^{2}+\sigma_{\varepsilon_{1}}^{2} & \beta_{21} \beta_{11}+\beta_{22} \beta_{12} & \beta_{31} \beta_{11}+\beta_{32} \beta_{21} \\
		\beta_{11} \beta_{21}+\beta_{12} \beta_{22} & \beta_{11}^{2}+\beta_{22}^{2}+\sigma_{\varepsilon_{2}}^{2} & \beta_{21} \beta_{31}+\beta_{22} \beta_{32} \\
		\beta_{11} \beta_{31}+\beta_{12} \beta_{32} & \beta_{21} \beta_{31}+\beta_{22} \beta_{32} & \beta_{31}^{2}+\beta_{32}^{2}+\sigma_{\varepsilon_{3}}^{2}
		\end{array}\right] \\
		= L \cdot L^{T}+\left[\begin{array}{ccc}
		\sigma_{\varepsilon_{1}}^{2} & 0 & 0 \\
		0 & \sigma_{\varepsilon_{2}}^{2} & 0 \\
		0 & 0 & \sigma_{\varepsilon_{3}}^{2}
		\end{array}\right]=L \cdot L^{T}+\Psi
		\end{array}
	\end{equation}
	Notice that if we disregard the error term, it comes in our particular case:
	\begin{equation}
		\begin{aligned}
		C& =\left[\begin{array}{ccc}
		\beta_{11}^{2}+\beta_{12}^{2} & \beta_{21} \beta_{11}+\beta_{22} \beta_{12} & \beta_{31} \beta_{11}+\beta_{32} \beta_{21} \\
		\beta_{11} \beta_{21}+\beta_{12} \beta_{22} & \beta_{11}^{2}+\beta_{22}^{2} & \beta_{21} \beta_{31}+\beta_{22} \beta_{32} \\
		\beta_{11} \beta_{31}+\beta_{12} \beta_{32} & \beta_{21} \beta_{31}+\beta_{22} \beta_{32} & \beta_{31}^{2}+\beta_{32}^{2}
		\end{array}\right] \\
		&=\left[\begin{array}{llc}
		\beta_{11} & \beta_{12} & \beta_{13} \\
		\beta_{21} & \beta_{22} & 0 \\
		\beta_{31} & \beta_{32} & 0
		\end{array}\right]\left[\begin{array}{ccc}
		\beta_{11} & \beta_{12} & \beta_{13} \\
		\beta_{21} & \beta_{22} & 0 \\
		\beta_{31} & \beta_{32} & 0
		\end{array}\right]^{T} \\
		&=\left[\begin{array}{lll}
		\beta_{11} & \beta_{12} & \beta_{13} \\
		\beta_{21} & \beta_{22} & 0 \\
		\beta_{31} & \beta_{32} & 0
		\end{array}\right]\left[\begin{array}{ccc}
		\beta_{11} & \beta_{21} & \beta_{31} \\
		\beta_{12} & \beta_{22} & \beta_{32} \\
		\beta_{13} & 0 & 0
		\end{array}\right]
		\end{aligned}
	\end{equation}
	In practice, we can easily calculate the "experimental variance-covariance matrix" of the measured data. We usually note it in the case of three variables to be explained:
	\begin{equation}
		\hat{C}=\left[\begin{array}{ccc}
		S_{1}^{2} & S_{12} & S_{13} \\
		S_{21} & S_{2}^{2} & S_{23} \\
		S_{31} & S_{32} & S_{3}^{2}
		\end{array}\right]=L \cdot L^{T}+\Psi
	\end{equation}
	Then we just have to put the two matrices in correspondence (well normally it is customary to denote by $b_{ij}$ the estimated values of the $\beta_{ij}$ but we will suppose that the reader will be able to easily differentiate the moment when we work with the estimators or the actual theoretical values):
	\begin{equation}
		\left[\begin{array}{ccc}
		S_{1}^{2} & S_{12} & S_{13} \\
		S_{21} & S_{2}^{2} & S_{23} \\
		S_{31} & S_{32} & S_{3}^{2}
		\end{array}\right]=\left[\begin{array}{ccc}
		\beta_{11}^{2}+\beta_{12}^{2}+\sigma_{\varepsilon_{1}}^{2} & \beta_{21} \beta_{11}+\beta_{22} \beta_{12} & \beta_{31} \beta_{11}+\beta_{32} \beta_{21} \\
		\beta_{11} \beta_{21}+\beta_{12} \beta_{22} & \beta_{11}^{2}+\beta_{22}^{2}+\sigma_{\varepsilon_{2}}^{2} & \beta_{21} \beta_{31}+\beta_{22} \beta_{32} \\
		\beta_{11} \beta_{31}+\beta_{12} \beta_{32} & \beta_{21} \beta_{31}+\beta_{22} \beta_{32} & \beta_{31}^{2}+\beta_{32}^{2}+\sigma_{\varepsilon_{3}}^{2}
		\end{array}\right]
	\end{equation}
	If we go back to our original example, then we have:
	\begin{equation}
		{\left[\begin{array}{ccc}
		1 & -0.051 & 0.804 \\
		-0.051 & 1 & 0.981 \\
		0.804 & 0.981 & 1
		\end{array}\right]} =\left[\begin{array}{ccc}
		\beta_{11}^{2}+\beta_{12}^{2}+\sigma_{\varepsilon_{1}}^{2} & \beta_{21} \beta_{11}+\beta_{22} \beta_{12} & \beta_{31} \beta_{11}+\beta_{32} \beta_{21} \\
		\beta_{11} \beta_{21}+\beta_{12} \beta_{22} & \beta_{11}^{2}+\beta_{22}^{2}+\sigma_{\varepsilon_{2}}^{2} & \beta_{21} \beta_{31}+\beta_{22} \beta_{32} \\
		\beta_{11} \beta_{31}+\beta_{12} \beta_{32} & \beta_{21} \beta_{31}+\beta_{22} \beta_{32} & \beta_{31}^{2}+\beta_{32}^{2}+\sigma_{\varepsilon_{3}}^{2}
		\end{array}\right]
	\end{equation}
	We will then see a little further that we can deduce all the saturations (coefficients of the model) from this positive definite matrix!
	\end{tcolorbox}

	Now we have two typical ways to determine the parameters of interest! First an iterative way based on the Expectation-Maximization (EM) algorithm that gives us the possibility to determine $W$ and $\Psi$ and another one, named "Principal Factor Analysis Extraction", based on the eigenvalue spectral decomposition (same idea as Principal Component Analysis) to determine only $W$.
	
	\subparagraph{Expectation-Maximization (EM) algorithm solution}\mbox{}\\\\
	Based on this joint Normal distribution, we can further find the following marginal distributions (see properties of Normal distributions at page \pageref{marginal and conditional distributions of multivariate Normal distributions}):
	\begin{itemize}
		\item First:
		
		with:
		
		
		\item Secondly:
		
		with:
		
		where we have defined:
		
	\end{itemize}
	
	Note that while $P(\vec{z})={\cal N}({\vec{0}},\mathds{1})$ has zero mean and diagonal covariance, the conditional distribution $P(\vec{z}\vert\vec{x},\vec{\theta})={\cal N}(\vec{\mu}_{z\vert x},\Sigma_{z\vert x})$ has non-zero mean $\vec{\mu}_{z\vert x}$ and non-diagonal covariance $\Sigma_{z\vert x}$!
	
	The computational complexity for the inversion of the $N\times N$ matrix $WW^T+\Psi$ is $\mathcal{O}(N^3)$. However, by applying the Woodbury matrix identity (\SeeChapter{see section Linear Algebra page \pageref{Woodbury matrix identity}}):
	
	where $\Psi$ as a diagonal matrix can be easily inverted, and $\mathds{1}+W^T\Psi^{-1}W$ is an $d \times d$ matrix that can be inverted with complexity $\mathcal{O}(d^3) \ll \mathcal{O}(N^3)$.
	
	The model parameters in $\theta=\{W,\Psi\}$ can be estimated based on an observed dataset $X$ by the EM algorithm (see page \pageref{EM algorithm}), an iterative process of the following two steps:
	\begin{itemize}
		\item The E-step:
		
		Find the expectation of the log likelihood function of the model parameters in ${\theta}$, to be maximized in the following M-step.

		Assuming all samples in the observed dataset $X=[\vec{x}_1,\ldots,\vec{x}_N]$ are independent and identically distributed (i.i.d.), we can find the "complete data likelihood\index{complete data likelihood}" function of ${\theta}$ (using the Bayes' relation established in the section of Probabilities page \pageref{bayes relation for complete data likelihood}):
		
		and the log-likelihood:
		
		The second term can be dropped as $P(\vec{z}| \theta)={\cal N}({\vec 0},\mathds{1})$ is independent of the model parameters in ${\theta}$ and therefore irrelevant to the maximization of the log likelihood with respect to ${\theta}$.

		The expectation of the log-likelihood function above (aka "expected complete data log-likelihood"), denoted by $Q$, with respect to the latent variable $\vec{z}$ is:
		
		Note that $\Psi$ is diagonal and therefore also symmetric, so is its own inverse $\Psi^{-1}$, we have:
		
		Also, note that $C=-DN\ln(2\pi)/2$ is a constant independent of $\vec{\theta}$ and is therefore dropped.
		
		\item The M-step:
		
		Find the optimal model parameters in $\theta=\{W,\Psi\}$ that maximize the expectation of the log-likelihood $Q$ obtained in the E-step.

		This is done by setting to zero the derivative of $Q$ with respective each of the two parameters in $\theta=\{W,\Psi\}$:
		\begin{itemize}
			\item The first derivative is:
			
			Solving for $W$ we get:
			
			where $\text{E}_{z\vert x_n}(\vec{z})$ and $\text{E}_{z\vert x_n}(\vec{z}\vec{z}^T)$ can be found in (see previously):
			
			Therefore:
			
			The second equation is due to the fact that $\Sigma_z=\text{E}(\vec{z}\vec{z}^T)-\vec{\mu}_z\vec{\mu}_z^T$. Here $\text{E}_{z\vert x_n}(\vec{z})$ can be considered as the estimation of the $\vec{z}$, while $\text{E}_{z\vert x_n}(\vec{z}\vec{z}^T)$ the uncertainty of the estimation.
			
			\item The second derivative (we use in this development derivatives of matrices as seen in the section of Linear Algebra page \pageref{derivative of logarithm of a determinant}):
			
		Solving for $\Psi$ we get:
		
		where $\text{diag}$ represents the operation that sets all off-diagonal elements of the right-hand side to zero, so that the resulting matrix $\Psi$ is guaranteed to be diagonal as required. We further replace $W$ in front of the last term above by that in:
		
		and get
		
		\end{itemize}
	\end{itemize}
	In summary, here is the list of steps in the EM algorithm for FA:
	\begin{enumerate}
		\item Initialize parameters ${\theta}_{\text{old}}=\{W_{\text{old}},\Psi_{\text{old}}\}$
		
		\item E-step: Based on ${\theta}_{\text{old}}$, find $\vec{\mu}_{z\vert x_n}$ and $\Sigma_{z\vert x_n}$ in:
		
		and then $\text{E}_{z\vert x}(\vec{z})$ and $\text{E}_{z\vert x}(\vec{z}\vec{z}^T)$ in:
		
		
		\item M-step: Find ${\theta}_{\text{new}}=\{W_{\text{new}},\Psi_{\text{new}}\}$ by evaluating:
		
		and:
		
		
		\item Terminate if convergence criterion is satisfied, otherwise replace ${\theta}_{\text{old}}$ by ${\theta}_{\text{new}}$ and return to step 2.
	\end{enumerate}
	
	\subparagraph{Principal Factor Analysis Extraction solution}\mbox{}\\\\
	Now, if we center and reduce the starting vectors containing the measured values, the variance-covariance matrix becomes the correlation matrix as we have proved it during our study of Principal Component Analysis (PCA)! This is why the PCA is the basis of the Factorial Analysis (exploratory) and we then say that we are doing a "\NewTerm{Principal Factor Analysis Extraction}" (PFAE).
	
	To understand the idea let us recall that we can write the Factor Analysis model in the following matrix form:
	
	If the data are centered and reduced, we will write:
	
	Now let's play cleverly with an orthogonal matrix $S$:
	
	It is thus an alternative notation for the initial model but as there is an infinity of orthogonal matrices, there is an infinity of alternative models.
	
	We denoted during our study of the Principal Component Analysis the matrix of the eigenvalues arranged in decreasing order in the diagonal by $\Lambda$ and by $S$ the matrix of the eigenvectors. This then gives us using the results of our study of the Principal Component Analysis and from what has just been seen:
	
	Using now the property of the transpose seen during our study of Linear Algebra, we have:
	
	Therefore:
	
	The matrix:
	
	therefore does the trick and it is customary to name the components of the matrix $L$ the "\NewTerm{factor saturations}" and it therefore contains the coefficients of the factors of the implicit linear model when the variables to be explained are centered and reduced. This is enough to give a qualitative idea of the weight of each of the factors.
	
	Some statistical software return what is are named the "\NewTerm{loadings}" or "\NewTerm{factor score}" and which corresponds to the expression:
	
	Whatever the chosen above solution, the factor model assumes that the $D(D+1)/2$ components of its covariance-variance matrix for $\vec{x}$ can be reproduced from the $D\cdot d$ factor loading (weights) $w_{ij}$ and the $D$ specific variances $\psi_D$. Because we have proved earlier above that:
	
	then when $D=d$ any covariance matrix $\Sigma_x$ can be reproduced exactly as $WW^T$, so $\Psi$ can be the zero matrix. It is when $d$ is small relative to $D$ that factor analysis is more useful. In this case, the factor model provides a simple explanation of the covariation in $\vec{x}$ with fewer parameters than $D(D+1)/2$ in $\Sigma_x$.
	
	Unfortunately for the factor analyst it is not easy to choose the number of latent variables. If the number of common factors is not determined by a priori considerations, such as by theory or the work of other researchers, the choice of $d$ can be based on the estimated eigenvalues of the principal component analysis.
	
	Given observations $x_1, x_2, \ldots , x_D$ on $d$ generally correlated variables, factor analysis seeks to answer the question: Does the factor model (linear in our case), with a small number of factors, adequately represent the data? In essence, we tackle this statistical model-building problem by trying to verify the covariance relations:
	
	If the off-diagonal elements of the estimator of $\Sigma_x$ are small, the variables are not related, and a factor analysis will not provide any useful evidence. If $\Sigma_x$ appears to deviate significantly from a diagonal matrix, then a factor model can be entertained, and the initial problem is one of estimating the factor loadings $w_{ij}$ and specific variables $\psi_i$.
	
	It should be also noticed that there is an infinity of solutions. To understand why consider the following model (we use here again the traditional notation in french textbooks):
	\begin{equation}
		\begin{array}{l}
		y_{1}=\beta_{11} f_{1}+\beta_{12} f_{2}+\varepsilon_{1}=0.5 f_{1}+0.5 f_{2}+\varepsilon_{1} \\
		y_{2}=\beta_{21} f_{1}+\beta_{22} f_{2}+\varepsilon_{2}=0.3 f_{1}+0.3 f_{2}+\varepsilon_{2} \\
		y_{3}=\beta_{31} f_{1}+\beta_{32} f_{2}+\varepsilon_{3}=0.5 f_{1}-0.5 f_{2}+\varepsilon_{3}
		\end{array}
	\end{equation}
	The variance-covariance matrix of the above factorial model is then:
	\begin{equation}
		\text{cov}\left(y_{i}, y_{j}\right)=\left[\begin{array}{ccc}
		0.5+\sigma_{\varepsilon_{1}}^{2} & 0.3 & 0 \\
		0.3 & 0.18+\sigma_{\varepsilon_{2}}^{2} & 0 \\
		0 & 0 & 0.5+\sigma_{\varepsilon_{3}}^{2}
		\end{array}\right]
	\end{equation}
	But the following model:
	\begin{equation}
		\begin{array}{l}
		y_{1}=\frac{\sqrt{2}}{2} f_{1}+0 \cdot f_{2}+\varepsilon_{1} \\
		y_{2}=0.3 \sqrt{2} f_{1}+0 \cdot f_{2}+\varepsilon_{2} \\
		y_{3}=0 \cdot f_{1}-\frac{\sqrt{2}}{2} f_{2}+\varepsilon_{3}
		\end{array}
	\end{equation}
	also has exactly the same variance-covariance matrix of the factor model as the reader can quickly verify and is therefore also a solution! The reader will be able to verify that the application:
	
	for any angle always gives the same variance-covariance matrix (reason for which there is an infinity of possible models for the same theoretical matrix of variances-covariance of the factorial model). So, in the example above, we simply took an angle of $\theta = -\pi/4$ which gives for the first line of the model:
	
	and so on line by line with each pair of coefficients and with the same rotation matrix.
	
	Although factor analysis and principal component analysis are typically labelled as data-reduction techniques, there are significant differences between these two. The objective of principal component analysis is to reduce the number of variables to a few components such that each component forms a new variable and the retained components explain the maximum amount of variance in the data. The objective of factor analysis, on the other hand, is to search or identify the underlying factor(s) or latent constructs that can explain the intercorrelation among the variables. 
	
	\pagebreak
	\paragraph{$T$-distributed Stochastic Neighbor Embedding ($T$-SNE)}\mbox{}\\\\
	"\NewTerm{$T$-distributed Stochastic Neighbor Embedding}\index{T-distributed Stochastic Neighbor Embedding}\label{tsne}" ($T$-SNE) is a machine learning algorithm for visualization developed by Laurens van der Maaten and Geoffrey Hinton (betwen 12002 and 12008 according to holocene calendar). It is a non-linear dimensionality reduction technique well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modelled by nearby points and dissimilar objects are modelled by distant points with high probability.

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	It is very important that the reader keep in mind that is designed only for visualizing a dataset in a low ($2$ or $3$) dimension space. We give it all the data we want to visualize all at once. It is not a general purpose dimensionality reduction tool!
	\end{tcolorbox}
	
	The $T$-SNE algorithm comprises two main stages. First, $T$-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects have a high probability of being picked while dissimilar points have an extremely small probability of being picked. Second, $T$-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the Kullback–Leibler divergence (\SeeChapter{see section Statistical Mechanics page \pageref{kullback-leibler divergence}}) between the two distributions with respect to the locations of the points in the map. Note that while the original algorithm uses the Euclidean distance between objects as the base of its similarity metric, this should be changed as appropriate.

	$T$-SNE has been used for visualization in a wide range of applications, including computer security research, music analysis, cancer research, bioinformatics, and biomedical signal processing. It is often used to visualize high-level representations learned by an artificial neural network. Indeed, as the reader may already know, deep CNN  (convolutional neural networks) are basically black boxes. There is no way to really interpret what's on deeper levels in the network. A common explanation is that deeper levels contain information about more complex objects. But that's not completely true, you can interpret it like that but data itself is just a high-dimensional noise for humans. But, with the help of $T$-SNE you can create maps to display which input data seams similar for the network.

	While $T$-SNE plots often seem to display clusters, the visual clusters can be influenced strongly by the chosen parametrization and therefore a good understanding of the parameters for $T$-SNE is necessary. Such "clusters" can be shown to even appear in non-clustered data, and thus may be false findings! Interactive exploration may thus be necessary to choose parameters and validate results. It has been demonstrated that $T$-SNE is often able to recover well-separated clusters, and with special parameter choices, approximates a simple form of spectral clustering.

	Given a set of $N$ high-dimensional objects $\vec{x} _{1},\ldots ,\vec{x} _{N}$, $T$-SNE first computes probabilities $p_{ij}$ that are proportional to the similarity of objects $\vec{x}_{i}$ and $\vec{x}_{j}$, defined as follows:
	
	Note that we have obviously $\sum _{j}p_{j\mid i}=1$ for all $i$.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	It seems that in general all SNE models are written in the form:
	
	where $g$ is a given function.
	\end{tcolorbox}
	
	As Van der Maaten and Hinton explained: "The similarity of datapoint $\vec x_{j}$ to datapoint $\vec x_{i}$ is the conditional probability, $p_{j|i}$, that $\vec x_{i}$ would pick $\vec x_{j}$ as its neighbour if neighbours were picked in proportion to their probability density under a Gaussian centered at $\vec x_{i}$."
	
	Now we define:
	
	and note that $p_{ij}=p_{ji}$, $p_{ii}=0$ and $\sum _{i,j}p_{ij}=1$.
	
	The bandwidth of the Gaussian kernels $\sigma_{i}$ is set in such a way that the perplexity of the conditional distribution equals a predefined perplexity using the bisection method to match a pre-specified perplexity value (Perp). The perplexity is $\text{Perp}(P_j)=2^{H(p_j)}$, where $H(P_j)=-\sum_j p_{i|j}\log(p_{i|j})$, and $\sigma_j$ is selected so that $\text{Perp}(P_j)=\text{Perp}$. As a result, the bandwidth is adapted to the density of the data: smaller values of $\sigma_{i}$ are used in denser parts of the data space.
	
	$T$-SNE is actually very sensitive to the Perplexity value. Let us see the following value:
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/tsne_perplexity_sensitivity.jpg}
		\caption{$T$-SNE perplexity sensitivity}
	\end{figure} 
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	There seems to be no standard way to choose the correct perplexity aside looking at the produced reduced dimension dataset and then assessing if it is meaningful. There are some general facts, eg. distances between clusters are mostly meaningless, small perplexity values encourage small clot-like structures but that's about it. A rule of thumb is to set the perplexity to $5\%$ of the dataset size.
	\end{tcolorbox}
	
	Since the Gaussian kernel uses the Euclidean distance $\lVert \vec x_{i}-\vec x_{j}\rVert$, it is affected by the curse of dimensionality, and in high dimensional data when distances lose the ability to discriminate, the $p_{ij}$ become too similar (asymptotically, they would converge to a constant). It has been proposed to adjust the distances with a power transform, based on the intrinsic dimension of each point, to alleviate this.
	
	In other words, in many of the dimensionality reduction techniques (e.g. $T$-SNE), we try to map the data points from a higher dimension to a lower dimension. While doing so a fundamental problem occurs, the distance between the points at a higher dimension can't be preserved at a lower dimension. This results in a overlapping or erroneous distance between the points at a lower dimension. This problem is named the "Crowding Problem". The $T$-distribution being heavier tailed helps in this regard. 
	
	$T$-SNE aims to learn a $d$-dimensional map $\vec{y}_{1},\ldots ,\vec{y}_{N}$ (with $\vec{y} _{i}\in \mathbb{R}^{d}$ that reflects the similarities $p_{i\mid j}$ as well as possible. To this end, it measures similarities $q_{ij}$ between two points in the map {$\vec{y}_{i}$ and $\vec{y}_{j}$, using a very similar approach. Specifically, $q_{j\mid i}$ is defined as:
	
	Herein a heavy-tailed Student $T$-distribution (with one-degree of freedom, which is the same as a Cauchy distribution) is used to measure similarities between low-dimensional points in order to allow dissimilar objects to be modelled far apart in the map. Note that also in this case we set $q_{ii}=0$.
	
	The locations of the points $\vec{y} _{i}$ in the map are determined by minimizing the (non-symmetric) Kullback–Leibler divergence (\SeeChapter{see section Statistical Mechanics page \pageref{kullback-leibler divergence}}) of the distribution $Q$ from the distribution $P$, that is:
	
	in which $P_i$ is the conditional probability distribution over all other datapoints given data-point $\vec x_i$, and $Q_i$ represents the conditional probability distribution over all other map points given map point $\vec y_i$. 
	
	The minimization of the Kullback–Leibler divergence with respect to the points $\vec{y}_{i}$ is performed using gradient descent. The result of this optimization should be a map that reflects the similarities between the high-dimensional inputs well.

	Let us see now how to derive the gradient of the KL divergence Loss function for the standard SNE and the $T$-SNE:

	\subparagraph{KL gradient of Stochastic Neighbour Embedding (SNE)}\mbox{}\\\\
	The definition of $q_{j | i}$ for SNE is different from that of the $T$-SNE (thanks to Federico Errica for providing the detailed developments):
	
	For what follows we will treat the one-dimensional case to simplify the notations (and we will also omit the SNE exponent).
	
	Notice that $E_{ij} = E_{ji}$. The loss function is defined as:
	
	We derive with respect to a $y_i$ (notice that the first term vanish a it depends only on $x_i$ or $x_j$ but not on any $y_i$ or $y_j$. To make the derivation less cluttered, we will omit the $\partial y_i$ term at the denominator:
	
	We start with the first term, noting that the derivative are obviously non-zero if and only if each term of the sums contains at least a $i$. Therefore we can reduce the two sums to only one with two terms (by symmetry):
	
	Since $\partial E_{ij}=E_{ij}(-2(y_i-y_j))$ we have:
	
	We conclude with the second term:
	
	 Since $\sum_{l \neq j} p_{l | j}=1$ and $Z_{j}$ does not
depend on $k,$ we can write (changing variable from $l$ to $j$ to make it more similar to the already computed terms):
	
	The derivative is non-zero only when $k=i$ or $j=i$ (also, in the latter case we can move $Z_{i}$ inside the summation because constant) therefore:
	
	Combining the previous and prior-previous relations we arrive at the final result:
	
	
	\subparagraph{KL gradient of $T$-distributed Stochastic Neighbour Embedding ($T$-SNE)}\mbox{}\\\\
	Let us do now the same job for $T$-SNE and for that the reader must remember:
	
	For what follows we will treat again the one-dimensional case to simplify the notations (and we will also omit the TSNE exponent).
	
	Notice that $E_{i j}=E_{j i}$. The loss function is defined as:
	
	We derive with respect to a $y_{i}$. To make the derivation less cluttered, we will again omit the $\partial y_{i}$ term at the denominator:
	
	We start with the first term, noting that the derivative is non-zero only when $\forall j$ , $k=i$ or $l=i,$ and also remembering that $p_{j\mid i}=p_{i\mid j}$ and $E_{j i}=E_{i j}$:
	
	since $\partial E_{i j}^{-1}=E_{i j}^{-2}\left(-2\left(y_{i}-y_{j}\right)\right)$ we have:
	
	We conclude with the second term. Using the fact that $\sum_{k, l \neq k} p_{k l}=1$ and that $Z$ does not depend on $k$ or $l$:
	
	Combining we arrive at the final result:
	
	Keep in mind that $T$-SNE learns a nonparametric mapping, which means that it does not learn an explicit function that maps data from the input space to the map!!! Therefore, it is not possible to embed test points in an existing map (although we could re-run $T$-SNE on the full dataset). A potential approach to deal with this would be to train a multivariate regressor to predict the map location from the input data. But if we are trying to apply $T$-SNE to "new" data, we are probably not thinking about our problem correctly, or perhaps simply we did not understand the purpose of $T$-SNE.
	
	\pagebreak
	\subsubsection{Gradient Boosting}
	In many supervised learning problems one has an output variable $y$ and a vector of input variables $\vec{x}$ described via a joint probability distribution $P(\vec{x},y)$ (at least for the purposes of theoretical analysis!). Using a training set $\{ (x_1,y_1), \dots , (x_n,y_n) \}$ of known values of $\vec{x}$ and corresponding values of $y$, the goal is to find an approximation $\hat{F}(x)$ to a function $F(x)$ that minimizes the expected value of some specified loss function $L(y, F(x))$:
	
	The "\NewTerm{gradient boosting method}\index{gradient boosting}\label{gradient boosting}" (that personally the author of these lines like better to name "\NewTerm{incremental iterative learning}") assumes a real-valued $y$ and seeks an approximation $\hat{F}(x)$ in the form of a weighted sum of functions $h_i (x)$ from some class $\mathcal{H}$ of weak learner:
	
	Keep in mind that the underlying idea is that instead of creating a single model, boosting combines multiple simple models into a single composite model. The idea is that, as we introduce more and more simple models, the overall model becomes stronger and stronger. And for reminder, in boosting terminology, the simple models are named "weak models" or "weak learners".
	
	In accordance with the empirical risk minimization principle, the method tries to find an approximation $\hat{F}(x)$ that minimizes the average value of the loss function on the training set, i.e., minimizes the empirical risk. It does so by starting with a model, consisting of a constant function $F_0(x)$, and incrementally expands it in a greedy fashion:
	
	where $h_m \in \mathcal{H}$ is a base learner function.
	
	Unfortunately, choosing the best function $h_i$ at each step for an arbitrary loss function $L$ is a computationally infeasible optimization problem in general (as least as far as we know for the moment...). Therefore, we  restrict our approach to a simplified version of the problem.
	
	The idea is to apply a steepest descent step to this minimization problem. If we consider the continuous case, i.e. where $\mathcal{H}$ is the set of arbitrary differentiable functions on $\mathbb{R}$, we would update the model in accordance with the following equations:	
	
	where the derivatives are taken with respect to the functions $F_i$ for $i \in \{ 1,..,m \}$.  In the discrete case however, i.e. when the set $\mathcal{H}$ is finite, we choose the candidate function $h$ closest to the gradient of $L$ for which the coefficient $\gamma$ may then be calculated with the aid of line search on the above equations. Note that this approach is a heuristic and therefore doesn't yield an exact solution to the given problem, but rather an approximation.
	
	In pseudocode, the generic gradient boosting method is:
	\begin{enumerate}[label*=\arabic*.]
		\item Training set $\{(x_{i},y_{i})\}_{i=1}^{n}$, a differentiable loss function $L(y,F(x))$, number of iterations $M$.
	
		\item For $m=1$ to $M$
		\begin{enumerate}[label*=\arabic*.]
			\item Compute so-named pseudo-residuals:
			
			
			\item Fit a base learner (e.g. tree) $h_{m}(x)$ to pseudo-residuals, i.e. train it using the training set $\{(x_{i},r_{im})\}_{i=1}^{n}$.
			
			\item Compute multiplier $\gamma _{m}$ by solving the following one-dimensional optimization problem:
			
			
			\item Update the model:
			
			
			\item Output $F_M(x)$
		\end{enumerate}
	\end{enumerate}
	In pseudocode (non-unique and not optimized):
	
	\begin{algorithm}[H]
	 \KwData{$\left\lbrace \vec{x}_i,y_i\right\rbrace_{i=1}^n$}
	 \KwResult{$F_M(\vec{x})$}
	 initialization\;
	$F_{0}(\vec{x})=\arg \min _{\gamma} \sum_{i=1}^{n} L\left(y_{i}, \gamma\right)$\;
	 \For{$m=1$ \KwTo $M$}{
	  \For{$i=1$ \KwTo $n$}{
	    $r_{im} = -\left[\frac{\partial L(y_i, F(\vec{x}_i))}{\partial F(\vec{x}_i)}\right]_{F(x)=F_{m-1}(\vec{x}_i)}$\;
	   }
	   tree$\left(h_m(\left\lbrace \vec{x}_i,r_{im}\right\rbrace_{i=1}^n\right)$\;
	   $\gamma_m = \underset{\gamma}{\min} \sum_{i=1}^n L\left(y_i, F_{m-1}(\vec{x}_i) + \gamma h_m(\vec{x}_i)\right)$\;
	   $F_{m}(\vec{x}_i)=F_{{m-1}}(\vec{x}_i)+\gamma _{m}h_{m}(\vec{x}_i)$\;
	   }
	 \caption{Generic gradient boosting tree pseudocode}
	\end{algorithm}
	
	Here is a very nice detailed example developed by antonioACR1 (we only have his pseudonyme sadly) on \href{https://datascience.stackexchange.com/questions/9134/gradient-boosting-algorithm-example}{Stackexchange} that is categorized as a special case of gradient boosting named: "\NewTerm{least-squares boosting algorithm}\footnote{As it is based on the usual squared error loss function!}\index{least-squares boosting algorithm}" (LS Boost).

	The example aims to predict salary per month (in dollars) based on whether or not the observation has own house, own car and own family/children. Suppose we have a dataset of three observations where the first variable is \textit{have own house}, the second is \textit{have own car} and the third variable is \textit{have family/children}, and target is \textit{salary per month}. The observations are:
	\begin{itemize}
		\item \{Yes,Yes,Yes,$1000$\}
		\item \{No,No,No,$25$\}
		\item \{Yes,No,No,$5000$\}
	\end{itemize}
	Choose a number $M$ of boosting stages, say $M=1$. The first step of gradient boosting algorithm is to start with an initial model $F_0$. This model is a constant defined by $\underset{\gamma}{\min}\sum_{i=1}^3 L(y_i,\lambda)$ in our case, where $L$ is the loss function. Suppose that we are working with the usual loss function:
	
	When this is the case, this constant is equal to the mean of the outputs $y_i$, so in our case:
	
	So our initial model is $F_0(x)=5008.3$ (which maps every observation $x$ (e.g. \{No,Yes,No\}) to $5008.3$.
	
	Next we should create a new dataset, which is the previous dataset but instead of $y_i$ we take the residuals:
	
	In our case, we have for the "\NewTerm{alignment accuracy residuals}\index{alignment accuracy residuals}":
	
	So our dataset becomes:
	\begin{itemize}
		\item \{Yes,Yes,Yes,$4991.6$\}
		\item \{No,No,No,$-4983.3$\}
		\item \{Yes,No,No,$-8.3$\}
	\end{itemize}
	The next step is to fit a base learner $h$ to this new dataset. Usually the base learner is a decision tree, so we use this.
	
	Now assume that we constructed the following decision tree $h$. We constructed this tree using entropy and information gain formulas (but maybe there is some mistake as the calculations were done by hand, however for our purposes we can assume it's correct!):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
		
		%Straight Lines [id:da4995476176612539] 
		\draw    (195.5,75) -- (195.5,117) ;
		\draw [shift={(195.5,119)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da830070609502433] 
		\draw    (195.5,150) -- (195.5,168) -- (195.5,193) ;
		\draw [shift={(195.5,195)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da6083793514374372] 
		\draw    (195.5,227) -- (195.5,270) ;
		\draw [shift={(195.5,272)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da12336924349005352] 
		\draw    (195.5,304) -- (195.5,347) ;
		\draw [shift={(195.5,349)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da7545081802895535] 
		\draw    (221.5,231) -- (385.57,276.47) ;
		\draw [shift={(387.5,277)}, rotate = 195.49] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da4587061393128502] 
		\draw    (393.5,306) -- (393.5,349) ;
		\draw [shift={(393.5,351)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da22287050830332222] 
		\draw    (221.5,75) -- (385.57,120.47) ;
		\draw [shift={(387.5,121)}, rotate = 195.49] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da5773255490798794] 
		\draw    (393.5,150) -- (393.5,193) ;
		\draw [shift={(393.5,195)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		
		% Text Node
		\draw (140,52) node [anchor=north west][inner sep=0.75pt]   [align=left] {have own house};
		% Text Node
		\draw (182.5,124) node [anchor=north west][inner sep=0.75pt]   [align=left] {Yes};
		% Text Node
		\draw (110,359) node [anchor=north west][inner sep=0.75pt]   [align=left] {salary residual$\displaystyle =4991.6$};
		% Text Node
		\draw (150,202) node [anchor=north west][inner sep=0.75pt]   [align=left] {have own car};
		% Text Node
		\draw (182.5,281) node [anchor=north west][inner sep=0.75pt]   [align=left] {Yes};
		% Text Node
		\draw (382.5,280) node [anchor=north west][inner sep=0.75pt]   [align=left] {No};
		% Text Node
		\draw (306,359) node [anchor=north west][inner sep=0.75pt]   [align=left] {salary residual$\displaystyle =-8.3$};
		% Text Node
		\draw (382.5,124) node [anchor=north west][inner sep=0.75pt]   [align=left] {No};
		% Text Node
		\draw (306,203) node [anchor=north west][inner sep=0.75pt]   [align=left] {salary residual$\displaystyle =-4983.3$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[]{Decision tree base learn for gradient boosting}
	\end{figure} 
	Let us denote this decision tree $h_0$. The next step is to find a constant:
	
	Therefore, we want a constant $\lambda$ minimizing:
	
	This is where gradient descent comes in handy!
	
	Suppose that we start at $P_0=0$. We choose the learning rate equal to $\eta=0.01$. We have:
	
	Then our next value $P_1$ is given by:
	
	We repeat this step $N$ times, and suppose that the last value is $P_N$. If $N$ is sufficiently large and $\eta$ is sufficiently small then $\lambda:=P_N$ should be the value where:
	
	is minimized. If this is the case, then our $\lambda_0$ will be equal to $P_N$. Just for the sake of it, suppose that $P_N=0.5$ (so that $\sum_{i=1}^{3}L(y_{i},F_{0}(x_{i})+\lambda{h_{0}(x_{i})})$) is minimized at $\lambda:=0.5$). Therefore, $\lambda_0=0.5$.
	
	The next step is to update our initial model $F_0$ by $F_1(x):=F_0(x)+\lambda_0h_0(x)$. Since our number of boosting stages is just one, then this is our final model $F_1$.
	
	Now suppose that we want to predict a new observation $x=\{\text{Yes},\text{Yes},\text{No}\}$ (so this person does have own house and own car but no children). What is the salary per month of this person? We just compute $F_1(x)=F_0(x)+\lambda_0h_0(x)=5008.3+0.5\cdot 4991.6=7504.1$. So this person earns $7504.1$ per month according to our model.
	
	Here is an interesting abstract visualization that may help to understand this incremental iterative learning process (we don't know who is the author of this figure for sure but maybe it's Terence Parr):
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/gradient_boosting_illustrated_visualization.jpg}
		\caption{Illustrate visualization of the iterative concept of Gradient Boosting}
	\end{figure}
	Or a more explicit one would be:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1,scale=0.9]
		%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
		
		%Shape: Axis 2D [id:dp8075294037144167] 
		\draw  (37.55,210) -- (223.5,210)(56.15,39) -- (56.15,229) (216.5,205) -- (223.5,210) -- (216.5,215) (51.15,46) -- (56.15,39) -- (61.15,46) (80.15,205) -- (80.15,215)(104.15,205) -- (104.15,215)(128.14,205) -- (128.14,215)(152.14,205) -- (152.14,215)(176.15,205) -- (176.15,215)(200.15,205) -- (200.15,215)(51.15,186) -- (61.15,186)(51.15,162) -- (61.15,162)(51.15,138) -- (61.15,138)(51.15,114) -- (61.15,114)(51.15,90) -- (61.15,90)(51.15,66) -- (61.15,66) ;
		\draw   (87.15,222) node[anchor=east, scale=0.75]{1} (111.15,222) node[anchor=east, scale=0.75]{2} (135.14,222) node[anchor=east, scale=0.75]{3} (159.14,222) node[anchor=east, scale=0.75]{4} (183.15,222) node[anchor=east, scale=0.75]{5} (207.15,222) node[anchor=east, scale=0.75]{6} (53.15,186) node[anchor=east, scale=0.75]{1} (53.15,162) node[anchor=east, scale=0.75]{2} (53.15,138) node[anchor=east, scale=0.75]{3} (53.15,114) node[anchor=east, scale=0.75]{4} (53.15,90) node[anchor=east, scale=0.75]{5} (53.15,66) node[anchor=east, scale=0.75]{6} ;
		%Shape: Rectangle [id:dp3615104485898446] 
		\draw   (56.15,66) -- (200.5,66) -- (200.5,210) -- (56.15,210) -- cycle ;
		%Shape: Circle [id:dp25276361858588925] 
		\draw  [line width=1.5]  (65,144.5) .. controls (65,140.91) and (67.91,138) .. (71.5,138) .. controls (75.09,138) and (78,140.91) .. (78,144.5) .. controls (78,148.09) and (75.09,151) .. (71.5,151) .. controls (67.91,151) and (65,148.09) .. (65,144.5) -- cycle ;
		%Shape: Circle [id:dp8091416405494123] 
		\draw  [line width=1.5]  (59,196) .. controls (59,192.13) and (62.13,189) .. (66,189) .. controls (69.87,189) and (73,192.13) .. (73,196) .. controls (73,199.87) and (69.87,203) .. (66,203) .. controls (62.13,203) and (59,199.87) .. (59,196) -- cycle ;
		%Shape: Circle [id:dp8016761124973397] 
		\draw  [line width=1.5]  (96,91.5) .. controls (96,87.91) and (98.91,85) .. (102.5,85) .. controls (106.09,85) and (109,87.91) .. (109,91.5) .. controls (109,95.09) and (106.09,98) .. (102.5,98) .. controls (98.91,98) and (96,95.09) .. (96,91.5) -- cycle ;
		%Shape: Circle [id:dp36102396536924974] 
		\draw  [line width=1.5]  (118,79.5) .. controls (118,75.91) and (120.91,73) .. (124.5,73) .. controls (128.09,73) and (131,75.91) .. (131,79.5) .. controls (131,83.09) and (128.09,86) .. (124.5,86) .. controls (120.91,86) and (118,83.09) .. (118,79.5) -- cycle ;
		%Shape: Circle [id:dp06337228166322428] 
		\draw  [line width=1.5]  (141,86.5) .. controls (141,82.91) and (143.91,80) .. (147.5,80) .. controls (151.09,80) and (154,82.91) .. (154,86.5) .. controls (154,90.09) and (151.09,93) .. (147.5,93) .. controls (143.91,93) and (141,90.09) .. (141,86.5) -- cycle ;
		%Shape: Square [id:dp11368942757567058] 
		\draw  [line width=1.5]  (178.5,82) -- (192.5,82) -- (192.5,96) -- (178.5,96) -- cycle ;
		%Shape: Square [id:dp30539957549685504] 
		\draw  [line width=1.5]  (179.5,183) -- (193.5,183) -- (193.5,197) -- (179.5,197) -- cycle ;
		%Shape: Square [id:dp6189064999985778] 
		\draw  [line width=1.5]  (105.5,117) -- (119.5,117) -- (119.5,131) -- (105.5,131) -- cycle ;
		%Shape: Square [id:dp11224045503665225] 
		\draw  [line width=1.5]  (95.5,143) -- (109.5,143) -- (109.5,157) -- (95.5,157) -- cycle ;
		%Shape: Square [id:dp22818615376665963] 
		\draw  [line width=1.5]  (131.32,130) -- (145.32,130) -- (145.32,144) -- (131.32,144) -- cycle ;
		%Shape: Square [id:dp0019835893733135546] 
		\draw  [line width=1.5]  (120.32,176) -- (134.32,176) -- (134.32,190) -- (120.32,190) -- cycle ;
		%Shape: Rectangle [id:dp6794768459014227] 
		\draw   (254,82) -- (320.5,82) -- (320.5,110) -- (254,110) -- cycle ;
		%Shape: Rectangle [id:dp614220808397832] 
		\draw   (225,151) -- (282.5,151) -- (282.5,179) -- (225,179) -- cycle ;
		%Shape: Rectangle [id:dp12023553575316281] 
		\draw   (296,151) -- (353.5,151) -- (353.5,179) -- (296,179) -- cycle ;
		%Straight Lines [id:da5322765829834197] 
		\draw    (268,110) -- (254.5,151) ;
		%Straight Lines [id:da9069110213651619] 
		\draw    (306.5,110) -- (324.5,151) ;
		%Shape: Circle [id:dp8528664249174864] 
		\draw  [line width=1.5]  (247.25,165) .. controls (247.25,161.41) and (250.16,158.5) .. (253.75,158.5) .. controls (257.34,158.5) and (260.25,161.41) .. (260.25,165) .. controls (260.25,168.59) and (257.34,171.5) .. (253.75,171.5) .. controls (250.16,171.5) and (247.25,168.59) .. (247.25,165) -- cycle ;
		%Shape: Square [id:dp2775647859608126] 
		\draw  [line width=1.5]  (317.75,158) -- (331.75,158) -- (331.75,172) -- (317.75,172) -- cycle ;
		%Straight Lines [id:da7063181638112976] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (80,210) -- (80,66) ;
		%Shape: Axis 2D [id:dp24404242960503608] 
		\draw  (37.55,467) -- (223.5,467)(56.15,296) -- (56.15,486) (216.5,462) -- (223.5,467) -- (216.5,472) (51.15,303) -- (56.15,296) -- (61.15,303) (80.15,462) -- (80.15,472)(104.15,462) -- (104.15,472)(128.14,462) -- (128.14,472)(152.14,462) -- (152.14,472)(176.15,462) -- (176.15,472)(200.15,462) -- (200.15,472)(51.15,443) -- (61.15,443)(51.15,419) -- (61.15,419)(51.15,395) -- (61.15,395)(51.15,371) -- (61.15,371)(51.15,347) -- (61.15,347)(51.15,323) -- (61.15,323) ;
		\draw   (87.15,479) node[anchor=east, scale=0.75]{1} (111.15,479) node[anchor=east, scale=0.75]{2} (135.14,479) node[anchor=east, scale=0.75]{3} (159.14,479) node[anchor=east, scale=0.75]{4} (183.15,479) node[anchor=east, scale=0.75]{5} (207.15,479) node[anchor=east, scale=0.75]{6} (53.15,443) node[anchor=east, scale=0.75]{1} (53.15,419) node[anchor=east, scale=0.75]{2} (53.15,395) node[anchor=east, scale=0.75]{3} (53.15,371) node[anchor=east, scale=0.75]{4} (53.15,347) node[anchor=east, scale=0.75]{5} (53.15,323) node[anchor=east, scale=0.75]{6} ;
		%Shape: Rectangle [id:dp1823037973539181] 
		\draw   (56.15,323) -- (200.5,323) -- (200.5,467) -- (56.15,467) -- cycle ;
		%Shape: Circle [id:dp786081653987448] 
		\draw  [line width=1.5]  (65,401.5) .. controls (65,397.91) and (67.91,395) .. (71.5,395) .. controls (75.09,395) and (78,397.91) .. (78,401.5) .. controls (78,405.09) and (75.09,408) .. (71.5,408) .. controls (67.91,408) and (65,405.09) .. (65,401.5) -- cycle ;
		%Shape: Circle [id:dp11081385416386214] 
		\draw  [line width=1.5]  (59,453) .. controls (59,449.13) and (62.13,446) .. (66,446) .. controls (69.87,446) and (73,449.13) .. (73,453) .. controls (73,456.87) and (69.87,460) .. (66,460) .. controls (62.13,460) and (59,456.87) .. (59,453) -- cycle ;
		%Shape: Circle [id:dp1282588195592005] 
		\draw  [line width=1.5]  (96,348.5) .. controls (96,344.91) and (98.91,342) .. (102.5,342) .. controls (106.09,342) and (109,344.91) .. (109,348.5) .. controls (109,352.09) and (106.09,355) .. (102.5,355) .. controls (98.91,355) and (96,352.09) .. (96,348.5) -- cycle ;
		%Shape: Circle [id:dp41560534710669805] 
		\draw  [line width=1.5]  (118,336.5) .. controls (118,332.91) and (120.91,330) .. (124.5,330) .. controls (128.09,330) and (131,332.91) .. (131,336.5) .. controls (131,340.09) and (128.09,343) .. (124.5,343) .. controls (120.91,343) and (118,340.09) .. (118,336.5) -- cycle ;
		%Shape: Circle [id:dp9971928251774174] 
		\draw  [line width=1.5]  (141,343.5) .. controls (141,339.91) and (143.91,337) .. (147.5,337) .. controls (151.09,337) and (154,339.91) .. (154,343.5) .. controls (154,347.09) and (151.09,350) .. (147.5,350) .. controls (143.91,350) and (141,347.09) .. (141,343.5) -- cycle ;
		%Shape: Square [id:dp46603761724586446] 
		\draw  [line width=1.5]  (178.5,339) -- (192.5,339) -- (192.5,353) -- (178.5,353) -- cycle ;
		%Shape: Square [id:dp8349928100440709] 
		\draw  [line width=1.5]  (179.5,440) -- (193.5,440) -- (193.5,454) -- (179.5,454) -- cycle ;
		%Shape: Square [id:dp12362667786059545] 
		\draw  [line width=1.5]  (105.5,374) -- (119.5,374) -- (119.5,388) -- (105.5,388) -- cycle ;
		%Shape: Square [id:dp27982539579750143] 
		\draw  [line width=1.5]  (95.5,400) -- (109.5,400) -- (109.5,414) -- (95.5,414) -- cycle ;
		%Shape: Square [id:dp42707171539201805] 
		\draw  [line width=1.5]  (131.32,387) -- (145.32,387) -- (145.32,401) -- (131.32,401) -- cycle ;
		%Shape: Square [id:dp8206748929253243] 
		\draw  [line width=1.5]  (120.32,433) -- (134.32,433) -- (134.32,447) -- (120.32,447) -- cycle ;
		%Shape: Rectangle [id:dp44024655965141024] 
		\draw   (256,339) -- (322.5,339) -- (322.5,367) -- (256,367) -- cycle ;
		%Shape: Rectangle [id:dp949603118812103] 
		\draw   (227,408) -- (284.5,408) -- (284.5,436) -- (227,436) -- cycle ;
		%Shape: Rectangle [id:dp28609380619308844] 
		\draw   (298,408) -- (355.5,408) -- (355.5,436) -- (298,436) -- cycle ;
		%Straight Lines [id:da10655410652405406] 
		\draw    (270,367) -- (256.5,408) ;
		%Straight Lines [id:da16675787828510447] 
		\draw    (308.5,367) -- (326.5,408) ;
		%Shape: Circle [id:dp6777148975691352] 
		\draw  [line width=1.5]  (249.25,422) .. controls (249.25,418.41) and (252.16,415.5) .. (255.75,415.5) .. controls (259.34,415.5) and (262.25,418.41) .. (262.25,422) .. controls (262.25,425.59) and (259.34,428.5) .. (255.75,428.5) .. controls (252.16,428.5) and (249.25,425.59) .. (249.25,422) -- cycle ;
		%Shape: Square [id:dp23276051708750933] 
		\draw  [line width=1.5]  (319.75,415) -- (333.75,415) -- (333.75,429) -- (319.75,429) -- cycle ;
		%Straight Lines [id:da9826965841835138] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (80,467) -- (80,323) ;
		%Straight Lines [id:da8559027759895605] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (200.5,371) -- (56.5,371) ;
		%Shape: Rectangle [id:dp8487980666384738] 
		\draw   (404,339) -- (470.5,339) -- (470.5,367) -- (404,367) -- cycle ;
		%Shape: Rectangle [id:dp3255118026632726] 
		\draw   (375,408) -- (432.5,408) -- (432.5,436) -- (375,436) -- cycle ;
		%Shape: Rectangle [id:dp1514999738784697] 
		\draw   (446,408) -- (503.5,408) -- (503.5,436) -- (446,436) -- cycle ;
		%Straight Lines [id:da8149496627775572] 
		\draw    (418,367) -- (404.5,408) ;
		%Straight Lines [id:da2725271823220876] 
		\draw    (456.5,367) -- (474.5,408) ;
		%Shape: Circle [id:dp5631083274552771] 
		\draw  [line width=1.5]  (397.25,422) .. controls (397.25,418.41) and (400.16,415.5) .. (403.75,415.5) .. controls (407.34,415.5) and (410.25,418.41) .. (410.25,422) .. controls (410.25,425.59) and (407.34,428.5) .. (403.75,428.5) .. controls (400.16,428.5) and (397.25,425.59) .. (397.25,422) -- cycle ;
		%Shape: Square [id:dp8662668504408206] 
		\draw  [line width=1.5]  (467.75,415) -- (481.75,415) -- (481.75,429) -- (467.75,429) -- cycle ;
		%Straight Lines [id:da15904741202629302] 
		\draw    (365.44,341.5) -- (365.44,360.25) ;
		%Straight Lines [id:da9738767054464001] 
		\draw    (356.38,350.88) -- (374.5,350.88) ;
		%Shape: Axis 2D [id:dp49319763183793786] 
		\draw  (37.55,728) -- (223.5,728)(56.15,557) -- (56.15,747) (216.5,723) -- (223.5,728) -- (216.5,733) (51.15,564) -- (56.15,557) -- (61.15,564) (80.15,723) -- (80.15,733)(104.15,723) -- (104.15,733)(128.14,723) -- (128.14,733)(152.14,723) -- (152.14,733)(176.15,723) -- (176.15,733)(200.15,723) -- (200.15,733)(51.15,704) -- (61.15,704)(51.15,680) -- (61.15,680)(51.15,656) -- (61.15,656)(51.15,632) -- (61.15,632)(51.15,608) -- (61.15,608)(51.15,584) -- (61.15,584) ;
		\draw   (87.15,740) node[anchor=east, scale=0.75]{1} (111.15,740) node[anchor=east, scale=0.75]{2} (135.14,740) node[anchor=east, scale=0.75]{3} (159.14,740) node[anchor=east, scale=0.75]{4} (183.15,740) node[anchor=east, scale=0.75]{5} (207.15,740) node[anchor=east, scale=0.75]{6} (53.15,704) node[anchor=east, scale=0.75]{1} (53.15,680) node[anchor=east, scale=0.75]{2} (53.15,656) node[anchor=east, scale=0.75]{3} (53.15,632) node[anchor=east, scale=0.75]{4} (53.15,608) node[anchor=east, scale=0.75]{5} (53.15,584) node[anchor=east, scale=0.75]{6} ;
		%Shape: Rectangle [id:dp4220309431144511] 
		\draw   (56.15,584) -- (200.5,584) -- (200.5,728) -- (56.15,728) -- cycle ;
		%Shape: Circle [id:dp9043527794049997] 
		\draw  [line width=1.5]  (65,662.5) .. controls (65,658.91) and (67.91,656) .. (71.5,656) .. controls (75.09,656) and (78,658.91) .. (78,662.5) .. controls (78,666.09) and (75.09,669) .. (71.5,669) .. controls (67.91,669) and (65,666.09) .. (65,662.5) -- cycle ;
		%Shape: Circle [id:dp48417019121421645] 
		\draw  [line width=1.5]  (59,714) .. controls (59,710.13) and (62.13,707) .. (66,707) .. controls (69.87,707) and (73,710.13) .. (73,714) .. controls (73,717.87) and (69.87,721) .. (66,721) .. controls (62.13,721) and (59,717.87) .. (59,714) -- cycle ;
		%Shape: Circle [id:dp09725195382808582] 
		\draw  [line width=1.5]  (96,609.5) .. controls (96,605.91) and (98.91,603) .. (102.5,603) .. controls (106.09,603) and (109,605.91) .. (109,609.5) .. controls (109,613.09) and (106.09,616) .. (102.5,616) .. controls (98.91,616) and (96,613.09) .. (96,609.5) -- cycle ;
		%Shape: Circle [id:dp012507960266899154] 
		\draw  [line width=1.5]  (118,597.5) .. controls (118,593.91) and (120.91,591) .. (124.5,591) .. controls (128.09,591) and (131,593.91) .. (131,597.5) .. controls (131,601.09) and (128.09,604) .. (124.5,604) .. controls (120.91,604) and (118,601.09) .. (118,597.5) -- cycle ;
		%Shape: Circle [id:dp5638106518960142] 
		\draw  [line width=1.5]  (141,604.5) .. controls (141,600.91) and (143.91,598) .. (147.5,598) .. controls (151.09,598) and (154,600.91) .. (154,604.5) .. controls (154,608.09) and (151.09,611) .. (147.5,611) .. controls (143.91,611) and (141,608.09) .. (141,604.5) -- cycle ;
		%Shape: Square [id:dp6279403211786283] 
		\draw  [line width=1.5]  (178.5,600) -- (192.5,600) -- (192.5,614) -- (178.5,614) -- cycle ;
		%Shape: Square [id:dp13457169699192417] 
		\draw  [line width=1.5]  (179.5,701) -- (193.5,701) -- (193.5,715) -- (179.5,715) -- cycle ;
		%Shape: Square [id:dp763160108017249] 
		\draw  [line width=1.5]  (105.5,635) -- (119.5,635) -- (119.5,649) -- (105.5,649) -- cycle ;
		%Shape: Square [id:dp5267711169857987] 
		\draw  [line width=1.5]  (95.5,661) -- (109.5,661) -- (109.5,675) -- (95.5,675) -- cycle ;
		%Shape: Square [id:dp3635026330788227] 
		\draw  [line width=1.5]  (131.32,648) -- (145.32,648) -- (145.32,662) -- (131.32,662) -- cycle ;
		%Shape: Square [id:dp0467966572292271] 
		\draw  [line width=1.5]  (120.32,694) -- (134.32,694) -- (134.32,708) -- (120.32,708) -- cycle ;
		%Shape: Rectangle [id:dp4146987483214173] 
		\draw   (256,600) -- (322.5,600) -- (322.5,628) -- (256,628) -- cycle ;
		%Shape: Rectangle [id:dp5731382311395292] 
		\draw   (227,669) -- (284.5,669) -- (284.5,697) -- (227,697) -- cycle ;
		%Shape: Rectangle [id:dp6525696961292573] 
		\draw   (298,669) -- (355.5,669) -- (355.5,697) -- (298,697) -- cycle ;
		%Straight Lines [id:da6922850033996371] 
		\draw    (270,628) -- (256.5,669) ;
		%Straight Lines [id:da9123957155490066] 
		\draw    (308.5,628) -- (326.5,669) ;
		%Shape: Circle [id:dp9545285314971164] 
		\draw  [line width=1.5]  (249.25,683) .. controls (249.25,679.41) and (252.16,676.5) .. (255.75,676.5) .. controls (259.34,676.5) and (262.25,679.41) .. (262.25,683) .. controls (262.25,686.59) and (259.34,689.5) .. (255.75,689.5) .. controls (252.16,689.5) and (249.25,686.59) .. (249.25,683) -- cycle ;
		%Shape: Square [id:dp17298915307073126] 
		\draw  [line width=1.5]  (319.75,676) -- (333.75,676) -- (333.75,690) -- (319.75,690) -- cycle ;
		%Straight Lines [id:da9806640311696773] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (80,728) -- (80,584) ;
		%Straight Lines [id:da3560672460054848] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (200.5,632) -- (56.5,632) ;
		%Shape: Rectangle [id:dp7997637300704064] 
		\draw   (404,600) -- (470.5,600) -- (470.5,628) -- (404,628) -- cycle ;
		%Shape: Rectangle [id:dp9282708737174146] 
		\draw   (375,669) -- (432.5,669) -- (432.5,697) -- (375,697) -- cycle ;
		%Shape: Rectangle [id:dp39456299138780127] 
		\draw   (446,669) -- (503.5,669) -- (503.5,697) -- (446,697) -- cycle ;
		%Straight Lines [id:da005737368878677707] 
		\draw    (418,628) -- (404.5,669) ;
		%Straight Lines [id:da8956038361583056] 
		\draw    (456.5,628) -- (474.5,669) ;
		%Shape: Circle [id:dp49395616087316085] 
		\draw  [line width=1.5]  (397.25,683) .. controls (397.25,679.41) and (400.16,676.5) .. (403.75,676.5) .. controls (407.34,676.5) and (410.25,679.41) .. (410.25,683) .. controls (410.25,686.59) and (407.34,689.5) .. (403.75,689.5) .. controls (400.16,689.5) and (397.25,686.59) .. (397.25,683) -- cycle ;
		%Shape: Square [id:dp45072578282119746] 
		\draw  [line width=1.5]  (467.75,676) -- (481.75,676) -- (481.75,690) -- (467.75,690) -- cycle ;
		%Straight Lines [id:da7161420259003037] 
		\draw    (365.44,602.5) -- (365.44,621.25) ;
		%Straight Lines [id:da17440651756319347] 
		\draw    (356.38,611.88) -- (374.5,611.88) ;
		%Straight Lines [id:da3432718674577664] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (176,728) -- (176,584) ;
		%Shape: Rectangle [id:dp27446663405709826] 
		\draw   (547,600) -- (613.5,600) -- (613.5,628) -- (547,628) -- cycle ;
		%Shape: Rectangle [id:dp8933150478912417] 
		\draw   (518,669) -- (575.5,669) -- (575.5,697) -- (518,697) -- cycle ;
		%Shape: Rectangle [id:dp1943417280909463] 
		\draw   (589,669) -- (646.5,669) -- (646.5,697) -- (589,697) -- cycle ;
		%Straight Lines [id:da47293623368404725] 
		\draw    (561,628) -- (547.5,669) ;
		%Straight Lines [id:da6961253857723049] 
		\draw    (599.5,628) -- (617.5,669) ;
		%Shape: Circle [id:dp00288138334085275] 
		\draw  [line width=1.5]  (540.25,683) .. controls (540.25,679.41) and (543.16,676.5) .. (546.75,676.5) .. controls (550.34,676.5) and (553.25,679.41) .. (553.25,683) .. controls (553.25,686.59) and (550.34,689.5) .. (546.75,689.5) .. controls (543.16,689.5) and (540.25,686.59) .. (540.25,683) -- cycle ;
		%Shape: Square [id:dp00877662781367583] 
		\draw  [line width=1.5]  (610.75,676) -- (624.75,676) -- (624.75,690) -- (610.75,690) -- cycle ;
		%Straight Lines [id:da6688266590859007] 
		\draw    (508.44,602.5) -- (508.44,621.25) ;
		%Straight Lines [id:da30730484783411294] 
		\draw    (499.38,611.88) -- (517.5,611.88) ;
		
		% Text Node
		\draw (22,113.4) node [anchor=north west][inner sep=0.75pt]    {$Y$};
		% Text Node
		\draw (128,231.4) node [anchor=north west][inner sep=0.75pt]    {$X$};
		% Text Node
		\draw (199,9) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{Iteration 1}};
		% Text Node
		\draw (255,35) node [anchor=north west][inner sep=0.75pt]   [align=left] {Model $\displaystyle F_{1}$:};
		% Text Node
		\draw (278,61.4) node [anchor=north west][inner sep=0.75pt]    {$T_{1}$};
		% Text Node
		\draw (265,88.4) node [anchor=north west][inner sep=0.75pt]    {$X< 1$};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (244,116) -- (275,116) -- (275,141) -- (244,141) -- cycle  ;
		\draw (247,120) node [anchor=north west][inner sep=0.75pt]   [align=left] {yes};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (305,116) -- (330,116) -- (330,141) -- (305,141) -- cycle  ;
		\draw (308,120) node [anchor=north west][inner sep=0.75pt]   [align=left] {no};
		% Text Node
		\draw (22,370.4) node [anchor=north west][inner sep=0.75pt]    {$Y$};
		% Text Node
		\draw (128,488.4) node [anchor=north west][inner sep=0.75pt]    {$X$};
		% Text Node
		\draw (199,266) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{Iteration 2}};
		% Text Node
		\draw (336,292) node [anchor=north west][inner sep=0.75pt]   [align=left] {Model $\displaystyle F_{2}$:};
		% Text Node
		\draw (280,318.4) node [anchor=north west][inner sep=0.75pt]    {$T_{1}$};
		% Text Node
		\draw (267,345.4) node [anchor=north west][inner sep=0.75pt]    {$X< 1$};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (246,373) -- (277,373) -- (277,398) -- (246,398) -- cycle  ;
		\draw (249,377) node [anchor=north west][inner sep=0.75pt]   [align=left] {yes};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (307,373) -- (332,373) -- (332,398) -- (307,398) -- cycle  ;
		\draw (310,377) node [anchor=north west][inner sep=0.75pt]   [align=left] {no};
		% Text Node
		\draw (428,318.4) node [anchor=north west][inner sep=0.75pt]    {$T_{2}$};
		% Text Node
		\draw (415,345.4) node [anchor=north west][inner sep=0.75pt]    {$Y >4$};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (394,373) -- (425,373) -- (425,398) -- (394,398) -- cycle  ;
		\draw (397,377) node [anchor=north west][inner sep=0.75pt]   [align=left] {yes};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (455,373) -- (480,373) -- (480,398) -- (455,398) -- cycle  ;
		\draw (458,377) node [anchor=north west][inner sep=0.75pt]   [align=left] {no};
		% Text Node
		\draw (22,631.4) node [anchor=north west][inner sep=0.75pt]    {$Y$};
		% Text Node
		\draw (128,749.4) node [anchor=north west][inner sep=0.75pt]    {$X$};
		% Text Node
		\draw (199,527) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{Iteration 3}};
		% Text Node
		\draw (403,553) node [anchor=north west][inner sep=0.75pt]   [align=left] {Model $\displaystyle F_{3}$:};
		% Text Node
		\draw (280,579.4) node [anchor=north west][inner sep=0.75pt]    {$T_{1}$};
		% Text Node
		\draw (267,606.4) node [anchor=north west][inner sep=0.75pt]    {$X< 1$};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (246,634) -- (277,634) -- (277,659) -- (246,659) -- cycle  ;
		\draw (249,638) node [anchor=north west][inner sep=0.75pt]   [align=left] {yes};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (307,634) -- (332,634) -- (332,659) -- (307,659) -- cycle  ;
		\draw (310,638) node [anchor=north west][inner sep=0.75pt]   [align=left] {no};
		% Text Node
		\draw (428,579.4) node [anchor=north west][inner sep=0.75pt]    {$T_{2}$};
		% Text Node
		\draw (415,606.4) node [anchor=north west][inner sep=0.75pt]    {$Y >4$};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (394,634) -- (425,634) -- (425,659) -- (394,659) -- cycle  ;
		\draw (397,638) node [anchor=north west][inner sep=0.75pt]   [align=left] {yes};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (455,634) -- (480,634) -- (480,659) -- (455,659) -- cycle  ;
		\draw (458,638) node [anchor=north west][inner sep=0.75pt]   [align=left] {no};
		% Text Node
		\draw (571,579.4) node [anchor=north west][inner sep=0.75pt]    {$T_{3}$};
		% Text Node
		\draw (558,606.4) node [anchor=north west][inner sep=0.75pt]    {$X< 5$};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (537,634) -- (568,634) -- (568,659) -- (537,659) -- cycle  ;
		\draw (540,638) node [anchor=north west][inner sep=0.75pt]   [align=left] {yes};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (598,634) -- (623,634) -- (623,659) -- (598,659) -- cycle  ;
		\draw (601,638) node [anchor=north west][inner sep=0.75pt]   [align=left] {no};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Illustrated Gradient Boosting with Trees}
	\end{figure}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	It should be quite obvious at this level of the book that take a linear regression model as weak learner is a non-sense. Indeed, the minimization of the linear regression Loss function is a closed form problem whose solution is perfectly known and that gives us the best linear unbiased estimator $\vec{\beta}$ out of all possibilities. However, dropping unbiasedness may allow to do a bit better particularly under high multicollinearity.
	\end{tcolorbox}
	
	\pagebreak
	\subsubsection{Neural networks}\label{neural network}
	Neural networks made from artificial cell structures are an approach for addressing from a new angle of perspective the problems of perception, memory, learning and non-linear reasoning (in other words ... artificial intelligence, abbreviated "IA") as well as genetic algorithms (see further below). They have also proved to be very promising alternatives to bypass some of the limitations of conventional numerical methods (see autodriven cars, autodriven airplanes, automatic trading systems). Thanks to their parallel processing of information and mechanisms inspired of nerve cells (neurons), they infer emergent properties to solve problems once referred to be as highly complex. Sometimes they result of artificial neurons is so bluffy that even best engineers have difficulties to explain how the neural network could arrive to a given observed performance (an typical non-business well known example is the Google Deep-learning dreaming machine).
	\begin{figure}[H]
		\centering
		\begin{subfigure}{.5\textwidth}
		  \centering
		  \includegraphics[width=0.9\linewidth]{img/computing/dream_machine_google_original.jpg}
		\end{subfigure}%
		\begin{subfigure}{.5\textwidth}
		  \centering
		  \includegraphics[width=0.9\linewidth]{img/computing/dream_machine_google_result.jpg}
		\end{subfigure}
		\caption[Google Dream Machine test]{Google Dream Machine test obtained the 12016-09-27 T20:42GMT\\ (source: \url{http://psychic-vr-lab.com/deepdream})}
	\end{figure}
	However, the major problem of artificial neurons (as far as we know...) is that they are not able to self-organize, nor to self-structured intelligently by themselves, they can only change weights and change some layer parameters in a range given previously by a human. So we need at this date to proceed heuristically to find the best neural network structure adapted to a problem and this is their large current actual weakness (either using brute force via a database containing millions of models or genetic algorithms that we will see a bit further below).
	
	We will discuss here the main architectures of neural networks. The purpose is not to study them all because they are too many variations (see figure on the next page), but rather to understand the basic internal mechanisms and how and when to use them with a minimal companion example with a common spreadsheet software. We will also discuss some concepts on fuzzy sets and logic (\SeeChapter{see section Logical Systems page \pageref{fuzzy logic}}) in the idea that these later are incorporated into some neural network architectures that we will study.
	
	The human brain is said ton contains about $100$ billion neurons. These neurons enable us among others to read a text while maintaining regular breathing to oxygenate our blood, activating our heart which ensures efficient circulation of the blood to nourish our cells, etc. After a very long learning path and trial and errors they also enable us to read books, innovate, create, copy concepts and gentle with other people of the same species (but the learning path doesn't work for all human being for all humans machine as some reject respect of life and of differences...).
	\begin{figure}[H]
		\centering
	  	\includegraphics[scale=0.20]{img/computing/neural_networks.jpg}
		\caption[Neural Networks complete chart]{Neural Networks complete chart (source: \url{http://www.asimovinstitute.org}, author: Fjodor van Veen)}
	\end{figure}
	Each of these neurons is also quit complex. Essentially, it is living tissue and chemistry and application of physics law (as the rest of our body). Neuro-physicists are just beginning to understand some of their internal mechanisms and are also able to influence them since a few decades. We usually think that their different neuronal functions, including memory, is stored at the connections (synapses) between neurons. It is this kind of theory that has inspired most of the artificial neural network architectures (say to be "formal neural networks"). Learning is then the process that consist either to establish new connections, or modify existing one by trial and error or by an external reference (we will focus especially on the latter option).
	
	This brings us to a fundamental question, based on our current knowledge: can we build approximate mathematical models of neurons and make them to possibly perform useful tasks? Well, the short answer is: yes, even if the networks that we develop have only a tiny fraction of the power of the human brain actually (year 12003 according to holocene calendar), and that's the goal here to show how we can do it formally (because technically it is obvious that more powerful are the computer, more impressive will be the results)!
	
	Neural networks are now used in all kinds of application in various fields. For example, there are neural networks developed for: aircraft autopilots, car autopilot, automotive control systems automatic reading of bank checks, automatic reading of postal addresses, signal processing, ballistic missile autopilots, voice synthesis, personal assistant, computer vision systems, market predictions, financial risks assessments, various manufacturing processes, medical diagnosis, oil and gas exploration, robotics, telecommunications, management decisioneering systems, classification and many others. In short, neural networks today have a significant impact and there is a safe bet that their importance will continue to grow in the future.
	
	\paragraph{Neuron model}\mbox{}\\\\
	The mathematical model of an artificial neuron, or "\NewTerm{perceptron}\index{perceptron}" is shown in the figure below (also in the previous figure with the overall summary of classical neural networks). A neuron consists essentially of an "integrator" which performs the weighted sum of its inputs (such as statistical mean ponderated by the inverse number of items!). The result $n$ of that is then transformed by a transfer function $f$ that produces the output $a$ of the formal neuron.

	The $R$ neuron inputs  correspond to vector correspond traditionally denoted:
	
	while:
	
	represents the neuron's vector weight (we distinguish them to prepare ourselves to the study of multiple layers formal neurons... also named "deep learning" systems):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1107); %set diagram left start at 0, and has height of 1107
		
		%Shape: Rectangle [id:dp18295936691979353] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (114.5,66) -- (194.5,66) -- (194.5,347) -- (114.5,347) -- cycle ;
		%Shape: Rectangle [id:dp4310992089634078] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (203.5,66) -- (528.5,66) -- (528.5,347) -- (203.5,347) -- cycle ;
		%Shape: Diamond [id:dp33055753486510997] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (148,93) -- (161,106) -- (148,119) -- (135,106) -- cycle ;
		%Shape: Diamond [id:dp21706838234806236] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (148,152) -- (161,165) -- (148,178) -- (135,165) -- cycle ;
		%Shape: Diamond [id:dp029345781146275174] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (148,215) -- (161,228) -- (148,241) -- (135,228) -- cycle ;
		%Shape: Diamond [id:dp2561474489243811] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (148,314) -- (161,327) -- (148,340) -- (135,327) -- cycle ;
		%Straight Lines [id:da19769552541575353] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (148,250) -- (148,302) ;
		%Shape: Circle [id:dp9751168592583066] 
		\draw  [line width=1.5]  (246,200.5) .. controls (246,177.58) and (264.58,159) .. (287.5,159) .. controls (310.42,159) and (329,177.58) .. (329,200.5) .. controls (329,223.42) and (310.42,242) .. (287.5,242) .. controls (264.58,242) and (246,223.42) .. (246,200.5) -- cycle ;
		%Straight Lines [id:da36190521360641914] 
		\draw    (148,106) -- (256.5,173) ;
		%Straight Lines [id:da9251016832796668] 
		\draw    (148,165) -- (246.5,187) ;
		%Straight Lines [id:da7501725765868923] 
		\draw    (148,228) -- (243.5,207) ;
		%Straight Lines [id:da6726332127744443] 
		\draw    (148,228) -- (243.5,207) ;
		%Straight Lines [id:da19464294515879343] 
		\draw    (148,327) -- (257.5,229) ;
		%Straight Lines [id:da072194412750199] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (229,218) -- (229,247) ;
		%Straight Lines [id:da8097609204463692] 
		\draw    (287.5,290) -- (287.5,242) ;
		%Shape: Diamond [id:dp9074203527213871] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (287.5,277) -- (300.5,290) -- (287.5,303) -- (274.5,290) -- cycle ;
		%Straight Lines [id:da44460197301163773] 
		\draw    (329,200.5) -- (402.5,200.5) ;
		\draw [shift={(405.5,200.5)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Rounded Rect [id:dp013268751612053764] 
		\draw  [line width=1.5]  (406.5,183.2) .. controls (406.5,176.46) and (411.96,171) .. (418.7,171) -- (464.3,171) .. controls (471.04,171) and (476.5,176.46) .. (476.5,183.2) -- (476.5,219.8) .. controls (476.5,226.54) and (471.04,232) .. (464.3,232) -- (418.7,232) .. controls (411.96,232) and (406.5,226.54) .. (406.5,219.8) -- cycle ;
		%Straight Lines [id:da20553843622993062] 
		\draw    (477,200.5) -- (550.5,200.5) ;
		\draw [shift={(553.5,200.5)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		
		% Text Node
		\draw (124,40) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle R$ inputs};
		% Text Node
		\draw (299,40.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {Neuron Model};
		% Text Node
		\draw (90,98) node [anchor=north west][inner sep=0.75pt]    {$p_{1}$};
		% Text Node
		\draw (90,159) node [anchor=north west][inner sep=0.75pt]    {$p_{2}$};
		% Text Node
		\draw (90,218) node [anchor=north west][inner sep=0.75pt]    {$p_{3}$};
		% Text Node
		\draw (90,319) node [anchor=north west][inner sep=0.75pt]    {$p_{R}$};
		% Text Node
		\draw (216,129) node [anchor=north west][inner sep=0.75pt]    {$w_{1,1}$};
		% Text Node
		\draw (216,163) node [anchor=north west][inner sep=0.75pt]    {$w_{1,2}$};
		% Text Node
		\draw (216,189) node [anchor=north west][inner sep=0.75pt]    {$w_{1,3}$};
		% Text Node
		\draw (215,266) node [anchor=north west][inner sep=0.75pt]    {$w_{1,R}$};
		% Text Node
		\draw (276,186) node [anchor=north west][inner sep=0.75pt]  [font=\Huge]  {$\Sigma $};
		% Text Node
		\draw (282,310.4) node [anchor=north west][inner sep=0.75pt]    {$b$};
		% Text Node
		\draw (290,250.4) node [anchor=north west][inner sep=0.75pt]    {$-1$};
		% Text Node
		\draw (361,180.4) node [anchor=north west][inner sep=0.75pt]    {$n$};
		% Text Node
		\draw (432,185) node [anchor=north west][inner sep=0.75pt]  [font=\Huge]  {$f$};
		% Text Node
		\draw (498,181.4) node [anchor=north west][inner sep=0.75pt]    {$a$};
		% Text Node
		\draw (299,353.4) node [anchor=north west][inner sep=0.75pt]    {$a=f\left(\vec{w}^{T} \circ \vec{p} -b\right)$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Example of one-layer formal neuron with the input vector and scalar output}
	\end{figure}
	The output of the integrator is defined (as it is an engineering technique) by the following relation (weighted sum minus the bias):
	
	that we can also write in vector form (it could also be written as tensor but ...):
	
	Such that finally:
	
	This output is obviously a weighted sum of the weights and inputs less what we name the "\NewTerm{bias $b$ of the neuron}\index{bias of a neuron}" (correction factor determined by trial and error and often null in practice). The weighted sum is named "\NewTerm{activation level of the neuron}\index{activation level of a neuron}". The bias $b$ is also named "\NewTerm{activation threshold of the neuron}\index{activation threshold of a neuron}". When the activation level reaches or exceeds the threshold $b$, then $n$, the argument of $f$, named "\NewTerm{activation function}\index{activation function}", becomes zero or positive of course. Otherwise it is negative.
	
	That latter relation is also sometimes denoted:
	
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	There's absolutely no reason for the weights in the linear layer (a.k.a. dense or fully-connected layer) to sum up to anything specific, such as $1.0$. They are usually initialized with small random numbers (so initial sum is unlikely to be $1.0$).
	\end{tcolorbox}

	We can draw a parallel between this mathematical model and some information that we know (or think we know) about the biological neuron. That latter has three main components: the dendrites, the cell body and axon:	
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/neuron.jpg}
		\caption{Simplified representation of the vocabulary of human neurons}
	\end{figure}
	The dendrites form a network of nerve receptors that are used to route to the body of the neuron the electrical signals from other neurons. It acts as like an integrator, accumulating electric charges. When the neuron becomes sufficiently excited (when the accumulated charge exceeds a certain threshold), by an electrochemical process, it generates an electric potential that spreads through its axon to eventually excite other neurons. The point of contact between the axon of one neuron and the dendrite of another neuron is named the "synapse". It seems that this is the spatial arrangement of neurons and their axons and also their quantity ($86,000,000,000$ neurons and $1.5\cdot 10^{14}$ synapses for the average human brain), and the quality of individual synaptic connections that determine the precise function of a biological neural network. This is based on this knowledge that the mathematical model described above has been defined (this is "Biomimicry Engineering" or "Biomimestim Engineering"). The detailed circuitry of neural pathways within it is known as a "connectome".
	
	A weight of an artificial neuron represents somehow the effectiveness of a synaptic connection. A negative weight inhibits in way the entrance, while a positive weight increase its effect. It is important to remember that this is a rough approximation of a real synapse resulting in fact of a quite complex chemical process and dependent on many external factors still unclear. We must understand that our artificial neuron is a pragmatic model which, as we shall see later, will help us to accomplish interesting tasks and more neurons we have more the tasks can be complex. The biological plausibility of this model is not important to us. What counts is the result that this model will allow us to achieve.
	
	Another limiting factor in the model we set ourselves is regarding its discreet nature. Indeed, in order to simulate a neural network, we will make the time discrete in our equations. In other words, we assume that all neurons are synchronous, that is to say that at each time $t$, they will simultaneously calculate the weighted sum and produce an output $a(t)=f(n(t))$. In biological networks, all neurons are actually asynchronous...

	So let us come back to our model as formulated by the previous relation and by doing a change of notation $\vec{w}^T$:
	
	This equation leads us to introduce a new more formal scheme of our neural network (FNW) or perceptron:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1107); %set diagram left start at 0, and has height of 1107
		
		%Shape: Rectangle [id:dp18295936691979353] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (114.5,66) -- (194.5,66) -- (194.5,347) -- (114.5,347) -- cycle ;
		%Shape: Rectangle [id:dp4310992089634078] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (203.5,66) -- (528.5,66) -- (528.5,347) -- (203.5,347) -- cycle ;
		%Shape: Circle [id:dp9751168592583066] 
		\draw  [line width=1.5]  (294,195.5) .. controls (294,177.55) and (308.55,163) .. (326.5,163) .. controls (344.45,163) and (359,177.55) .. (359,195.5) .. controls (359,213.45) and (344.45,228) .. (326.5,228) .. controls (308.55,228) and (294,213.45) .. (294,195.5) -- cycle ;
		%Straight Lines [id:da20553843622993062] 
		\draw    (471.5,197.5) -- (540.5,197.5) ;
		\draw [shift={(543.5,197.5)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Rounded Rect [id:dp6454821919836329] 
		\draw  [line width=1.5]  (411.5,181.63) .. controls (411.5,175.76) and (416.26,171) .. (422.13,171) -- (461.87,171) .. controls (467.74,171) and (472.5,175.76) .. (472.5,181.63) -- (472.5,213.53) .. controls (472.5,219.4) and (467.74,224.16) .. (461.87,224.16) -- (422.13,224.16) .. controls (416.26,224.16) and (411.5,219.4) .. (411.5,213.53) -- cycle ;
		%Shape: Rectangle [id:dp3103598552442033] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (141.25,77.5) -- (167.75,77.5) -- (167.75,320) -- (141.25,320) -- cycle ;
		%Straight Lines [id:da11143726437296575] 
		\draw    (157.5,101.5) -- (249.5,101.5) ;
		\draw [shift={(252.5,101.5)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da9441271635743058] 
		\draw    (359,195.5) -- (407.5,195.5) ;
		\draw [shift={(410.5,195.5)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Rounded Rect [id:dp8790634976957183] 
		\draw  [line width=1.5]  (252.5,85.63) .. controls (252.5,79.76) and (257.26,75) .. (263.13,75) -- (302.87,75) .. controls (308.74,75) and (313.5,79.76) .. (313.5,85.63) -- (313.5,117.53) .. controls (313.5,123.4) and (308.74,128.16) .. (302.87,128.16) -- (263.13,128.16) .. controls (257.26,128.16) and (252.5,123.4) .. (252.5,117.53) -- cycle ;
		%Straight Lines [id:da9499622509127759] 
		\draw    (312.5,100) -- (325.85,160.07) ;
		\draw [shift={(326.5,163)}, rotate = 257.47] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da5401620844403945] 
		\draw    (191.5,295.5) -- (263.5,295.5) ;
		\draw [shift={(266.5,295.5)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Rounded Rect [id:dp47094902099471114] 
		\draw  [line width=1.5]  (265.5,279.63) .. controls (265.5,273.76) and (270.26,269) .. (276.13,269) -- (315.87,269) .. controls (321.74,269) and (326.5,273.76) .. (326.5,279.63) -- (326.5,311.53) .. controls (326.5,317.4) and (321.74,322.16) .. (315.87,322.16) -- (276.13,322.16) .. controls (270.26,322.16) and (265.5,317.4) .. (265.5,311.53) -- cycle ;
		%Straight Lines [id:da025634596847625257] 
		\draw    (296.5,269) -- (324.73,230.42) ;
		\draw [shift={(326.5,228)}, rotate = 126.19] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		
		% Text Node
		\draw (130,40) node [anchor=north west][inner sep=0.75pt]   [align=left] {Inputs};
		% Text Node
		\draw (299,40.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {Neuron Model};
		% Text Node
		\draw (314,184) node [anchor=north west][inner sep=0.75pt]  [font=\Huge]  {$+$};
		% Text Node
		\draw (498,180) node [anchor=north west][inner sep=0.75pt]    {$a$};
		% Text Node
		\draw (299,353.4) node [anchor=north west][inner sep=0.75pt]    {$a=f\left(\vec{w}^{T}\vec{p} -b\right)$};
		% Text Node
		\draw (432,182) node [anchor=north west][inner sep=0.75pt]  [font=\Huge]  {$f$};
		% Text Node
		\draw (485,201.4) node [anchor=north west][inner sep=0.75pt]    {$1\times 1$};
		% Text Node
		\draw (148,325.4) node [anchor=north west][inner sep=0.75pt]    {$R$};
		% Text Node
		\draw (215,78.4) node [anchor=north west][inner sep=0.75pt]    {$\vec{p}$};
		% Text Node
		\draw (204,106.4) node [anchor=north west][inner sep=0.75pt]    {$R\times 1$};
		% Text Node
		\draw (269,86) node [anchor=north west][inner sep=0.75pt]  [font=\Huge]  {$\vec{w}$};
		% Text Node
		\draw (376,180) node [anchor=north west][inner sep=0.75pt]    {$n$};
		% Text Node
		\draw (170,285.4) node [anchor=north west][inner sep=0.75pt]    {$-1$};
		% Text Node
		\draw (287,280) node [anchor=north west][inner sep=0.75pt]  [font=\Huge]  {$b$};
		% Text Node
		\draw (278.13,325.56) node [anchor=north west][inner sep=0.75pt]    {$1\times 1$};
		% Text Node
		\draw (363,198.9) node [anchor=north west][inner sep=0.75pt]    {$1\times 1$};
		% Text Node
		\draw (265.13,131.56) node [anchor=north west][inner sep=0.75pt]    {$1\times R$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Vector written example of one-layer formal neuron with the input vector and scalar output}
	\end{figure}
	We represent the $R$ inputs as a black rectangle (the number of entries is indicated below the rectangle). From this rectangle result a vector $\vec{p}$ whose dimensions are $R\times 1$. This vector is multiplied by a vector $\vec{w}$ that contains the weights (synaptic) neuron. In the case of a single neuron, this vector has dimension $1\times R$. The result of the multiplication is the "activation level" (scalar) which is then compared to the threshold $b$ (a scalar) by subtraction. Finally, the output of the neuron is calculated by the function $f$. The output of a single neuron is then always a scalar in this special case.

	To find the components of the matrix $\vec{w}$ (neuron input weight), and the bias $b$ we use operational research techniques (simplex method, conjugate gradient method, evolutionary algorithms, etc.) on a sample of data sample of the company of size $n$ in order to "train the model of the neuron". The purpose will then be to find weights $\vec{w}$ that minimize the quadratic cost error function given by:
	
	where $a_k$ is the model output for the vector $\vec{r}_k$ and $y_k$ is the real value (the expected answer) corresponding to the $\vec{r}_k$ .
	
	After we check the results on a sample test before putting the neural network in production for not yet existing data (we will do a detailed example with Microsoft Excel just after the presentations of the transfer functions).
		
	\pagebreak
	\paragraph{Transfer functions}\mbox{}\\\\
	So far, we have not specified the nature of the activation function $a=f(n)$ of our model. It turns out that several possibilities exist and these are empirical and must adapt to different situations (and the adaptation is sometimes also dynamic). The most common and most cited in the literature are listed in the figure below:
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|l||c|c|}
					\hline
					{\cellcolor[gray]{0.75}\textbf{Activation function name $f$}} & {\cellcolor[gray]{0.75}\textbf{Input/Output relation}} & {\cellcolor[gray]{0.75}\textbf{Profile plot}}  \\ \hline
					Threshold & $\begin{matrix}a=0 & \text{if} & n<0\\a=1 & \text{if} & n\geq 0 \end{matrix}$ & \parbox{1.5cm}{\includegraphics[scale=0.5]{img/computing/neural_network_treshold.jpg}}\\ \hline
					Symmetric threshold & $\begin{matrix}a=-1 & \text{if} & n<0\\a=1 & \text{if} & n\geq 0 \end{matrix}$ & \parbox{1.5cm}{\includegraphics[scale=0.5]{img/computing/neural_network_sym_treshold.jpg}}\\ \hline
					Linear & $a=n$ & \parbox{1.5cm}{\includegraphics[scale=0.5]{img/computing/neural_network_linear.jpg}}\\ \hline
					Saturated linear & $\begin{matrix}a=0 & \text{if} & n<0\\a=n & \text{if} & 0\geq n \geq 1\\ a=1 & \text{if} & n>1 \end{matrix}$ & \parbox{1.5cm}{\includegraphics[scale=0.5]{img/computing/neural_network_saturated_linear.jpg}}\\  \hline
					Symmetric saturated linear & $\begin{matrix}a=-1 & \text{if} & n<-1\\a=n & \text{if} & -1\geq n \geq 1\\ a=1 & \text{if} & n>1 \end{matrix}$ & \parbox{1.5cm}{\includegraphics[scale=0.5]{img/computing/neural_network_symetric_saturated_linear.jpg}}\\ \hline
					Rectified Linear Unit Function (RELU) & $\begin{matrix}a=0 & \text{if} & n<0\\a=n & \text{if} & n\geq 0 \end{matrix}$ & \parbox{1.5cm}{\includegraphics[scale=0.5]{img/computing/neural_network_linear_positive.jpg}}\\ \hline
					Sigmoid & $a=\dfrac{1}{1+e^{-n}}$ & \parbox{1.5cm}{\includegraphics[scale=0.5]{img/computing/neural_network_sigmoid.jpg}}\\ \hline
					Hyperbolic tangent (TANH) & $a=\dfrac{e^n-e^{-n}}{e^n+e^{-n}}$ & \parbox{1.5cm}{\includegraphics[scale=0.5]{img/computing/neural_network_hyperbolic_tangent.jpg}}\\ \hline
					Competitive & $\begin{matrix}a=1 & \text{if } n \text{ is maximum} \\ a=0 & \text{otherwise} \end{matrix}$ & \parbox{1.5cm}{\includegraphics[scale=0.5]{img/computing/neural_network_competitive.jpg}}\\ \hline
					Gompertz & $a=\alpha e^{-\beta e^{\gamma n}}$ & \parbox{1.5cm}{\includegraphics[scale=0.5]{img/computing/neural_network_sigmoid.jpg}}\\ \hline
					... & ... & ...\\ \hline
				\end{tabular}
		\end{center}
		\caption[]{Types of transfer functions for FNN}
	\end{table}
	The three most used in the field of engineering are the functions "threshold" (I) , "linear" (II) and "sigmoid" (III) as shown in details below:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1215); %set diagram left start at 0, and has height of 1215
		
		%Shape: Axis 2D [id:dp5238427797429279] 
		\draw  (13.5,130) -- (184.5,130)(101.04,46) -- (101.04,210) (177.5,125) -- (184.5,130) -- (177.5,135) (96.04,53) -- (101.04,46) -- (106.04,53)  ;
		%Shape: Axis 2D [id:dp526610240263863] 
		\draw  (225.5,130) -- (396.5,130)(313.04,46) -- (313.04,210) (389.5,125) -- (396.5,130) -- (389.5,135) (308.04,53) -- (313.04,46) -- (318.04,53)  ;
		%Shape: Axis 2D [id:dp26571698438336666] 
		\draw  (440.5,130) -- (611.5,130)(528.04,46) -- (528.04,210) (604.5,125) -- (611.5,130) -- (604.5,135) (523.04,53) -- (528.04,46) -- (533.04,53)  ;
		%Straight Lines [id:da05552109330975852] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (39,130) -- (141.5,130) ;
		%Straight Lines [id:da06596967736596837] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (141.5,73) -- (141.5,130) ;
		%Straight Lines [id:da700425364542097] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (141.5,73) -- (176.5,73) ;
		%Straight Lines [id:da47991990536501095] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=0.75]  [dash pattern={on 4.5pt off 4.5pt}]  (524.5,73) -- (608.5,73) ;
		%Straight Lines [id:da7258080989510789] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (276,206) -- (375.5,79) ;
		%Straight Lines [id:da10985838587033325] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=0.75]  [dash pattern={on 4.5pt off 4.5pt}]  (318.5,160) -- (336.5,160) ;
		%Straight Lines [id:da9704253576898481] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=0.75]  [dash pattern={on 4.5pt off 4.5pt}]  (336.5,130) -- (336.5,160) ;
		%Curve Lines [id:da07979914088906126] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (451.5,130) .. controls (578.5,132) and (514.5,74) .. (608.5,73) ;
		%Straight Lines [id:da31254624372429096] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=0.75]  [dash pattern={on 4.5pt off 4.5pt}]  (546.5,74) -- (546.5,137) ;
		%Straight Lines [id:da09688887675011593] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=0.75]  [dash pattern={on 4.5pt off 4.5pt}]  (141.5,73) -- (94.5,73) ;
		
		% Text Node
		\draw (82,36.4) node [anchor=north west][inner sep=0.75pt]    {$a$};
		% Text Node
		\draw (85,133.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (176,131.4) node [anchor=north west][inner sep=0.75pt]    {$\vec{w}^{T}\vec{p}$};
		% Text Node
		\draw (93,225) node [anchor=north west][inner sep=0.75pt]   [align=left] {(I)};
		% Text Node
		\draw (75,63.4) node [anchor=north west][inner sep=0.75pt]    {$+1$};
		% Text Node
		\draw (75,172.4) node [anchor=north west][inner sep=0.75pt]    {$-1$};
		% Text Node
		\draw (294,36.4) node [anchor=north west][inner sep=0.75pt]    {$a$};
		% Text Node
		\draw (297,133.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (388,131.4) node [anchor=north west][inner sep=0.75pt]    {$\vec{w}^{T}\vec{p}$};
		% Text Node
		\draw (305,225) node [anchor=north west][inner sep=0.75pt]   [align=left] {(II)};
		% Text Node
		\draw (509,36.4) node [anchor=north west][inner sep=0.75pt]    {$a$};
		% Text Node
		\draw (512,133.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (603,131.4) node [anchor=north west][inner sep=0.75pt]    {$\vec{w}^{T}\vec{p}$};
		% Text Node
		\draw (516,225) node [anchor=north west][inner sep=0.75pt]   [align=left] {(III)};
		% Text Node
		\draw (502,63.4) node [anchor=north west][inner sep=0.75pt]    {$+1$};
		% Text Node
		\draw (502,172.4) node [anchor=north west][inner sep=0.75pt]    {$-1$};
		% Text Node
		\draw (136,134.4) node [anchor=north west][inner sep=0.75pt]    {$b$};
		% Text Node
		\draw (317.5,163.4) node [anchor=north west][inner sep=0.75pt]    {$-b$};
		% Text Node
		\draw (346.5,111.4) node [anchor=north west][inner sep=0.75pt]    {$+b$};
		% Text Node
		\draw (535.5,135.4) node [anchor=north west][inner sep=0.75pt]    {$+b$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Most used transfer functions for neural networks in the 120th century}
	\end{figure}
	As its name suggests it, the threshold function applies a threshold on its input. Specifically, a negative input does not pass the threshold, the function returns then a $0$ (false), while a positive or zero input exceeds the threshold, and the function returns then a $1$ (true). It is obvious that this kind of feature is to make binary decisions (this function can also be assimilated to the Heaviside function for those who know it...).
	
	The linear function is itself very simple, it directly associate its input to an output according to the relation $a=f(n)=n$. It is then evident that the output of the neuron corresponds to its activation level for which the zero value (the ordinate at the origin) occurs when $\vec{w}^T\vec{p}=b$.
	
	The sigmoid transfer function is itself defined by the mathematical relation:
	
	It looks like two threshold function, either the linear function, as we are far or near to $b$ respectively. The threshold function is very non-linear because there is a discontinuity when $\vec{w}^T\vec{p}=b$. For its part, the linear function is entirely linear. It has no change in slope. The sigmoid is an interesting compromise between the two. Finally notice that the hyperbolic tangent (TANH) function is a symmetric version of the sigmoid.
	
	It may be quite obvious to the reader that if the activation function is linear, then the neural network is a regression model! If it is a logistic function, then the neural network is a binary classification model!
	
	We have introduced during our study of logistics regressions the softmax function (see above page \pageref{softmax function}). The softmax function is a typical activation function for a multi-class neural network classifier. That is a neural network with multiple input but also multiple outputs. If we denote the weights $\vec{v}$ and we index each different output by the letter $j$, the such a neural network will be represented by:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,737); %set diagram left start at 0, and has height of 737
		
		%Shape: Circle [id:dp03718577468221085] 
		\draw  [line width=1.5]  (135,194) .. controls (135,180.19) and (146.19,169) .. (160,169) .. controls (173.81,169) and (185,180.19) .. (185,194) .. controls (185,207.81) and (173.81,219) .. (160,219) .. controls (146.19,219) and (135,207.81) .. (135,194) -- cycle ;
		%Shape: Circle [id:dp8948959658026507] 
		\draw  [line width=1.5]  (263.5,100) .. controls (263.5,86.19) and (274.69,75) .. (288.5,75) .. controls (302.31,75) and (313.5,86.19) .. (313.5,100) .. controls (313.5,113.81) and (302.31,125) .. (288.5,125) .. controls (274.69,125) and (263.5,113.81) .. (263.5,100) -- cycle ;
		%Shape: Circle [id:dp8792158167957036] 
		\draw  [line width=1.5]  (263.5,186) .. controls (263.5,172.19) and (274.69,161) .. (288.5,161) .. controls (302.31,161) and (313.5,172.19) .. (313.5,186) .. controls (313.5,199.81) and (302.31,211) .. (288.5,211) .. controls (274.69,211) and (263.5,199.81) .. (263.5,186) -- cycle ;
		%Shape: Circle [id:dp28324977387671546] 
		\draw  [line width=1.5]  (263.5,338) .. controls (263.5,324.19) and (274.69,313) .. (288.5,313) .. controls (302.31,313) and (313.5,324.19) .. (313.5,338) .. controls (313.5,351.81) and (302.31,363) .. (288.5,363) .. controls (274.69,363) and (263.5,351.81) .. (263.5,338) -- cycle ;
		%Straight Lines [id:da7880526951309001] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (288.5,224) -- (288.5,301) ;
		%Straight Lines [id:da3867247875550379] 
		\draw    (56.5,194) -- (132,194) ;
		\draw [shift={(135,194)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da39912602171831035] 
		\draw    (175.5,172) -- (261.18,101.9) ;
		\draw [shift={(263.5,100)}, rotate = 140.71] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da7089718008624617] 
		\draw    (185,194) -- (260.52,186.3) ;
		\draw [shift={(263.5,186)}, rotate = 174.18] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da36108341971131197] 
		\draw    (174.5,214) -- (261.75,335.56) ;
		\draw [shift={(263.5,338)}, rotate = 234.33] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Shape: Circle [id:dp34299586916394387] 
		\draw  [line width=1.5]  (135,320) .. controls (135,306.19) and (146.19,295) .. (160,295) .. controls (173.81,295) and (185,306.19) .. (185,320) .. controls (185,333.81) and (173.81,345) .. (160,345) .. controls (146.19,345) and (135,333.81) .. (135,320) -- cycle ;
		%Straight Lines [id:da9671860952498097] 
		\draw    (175.5,300) -- (268.13,119.67) ;
		\draw [shift={(269.5,117)}, rotate = 117.19] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da6927193104468099] 
		\draw    (185,320) -- (270.66,209.37) ;
		\draw [shift={(272.5,207)}, rotate = 127.75] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da26136139511070344] 
		\draw    (176.5,339) -- (262.52,348.67) ;
		\draw [shift={(265.5,349)}, rotate = 186.41] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da3091132124354603] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (197.5,308) -- (197.5,337) ;
		%Straight Lines [id:da17585031839766208] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (192.5,200) -- (192.5,228) ;
		%Straight Lines [id:da9302220750760704] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (313.5,338) -- (417.31,241.05) ;
		\draw [shift={(419.5,239)}, rotate = 136.96] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da5333892267920812] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (313.5,186) -- (416.57,209.34) ;
		\draw [shift={(419.5,210)}, rotate = 192.76] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da7579984751170119] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (313.5,100) -- (417.14,181.15) ;
		\draw [shift={(419.5,183)}, rotate = 218.06] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Shape: Rectangle [id:dp8688826988055596] 
		\draw   (419.5,171) -- (563.5,171) -- (563.5,248) -- (419.5,248) -- cycle ;
		
		% Text Node
		\draw (63,158) node [anchor=north west][inner sep=0.75pt]   [align=left] {input\\features};
		% Text Node
		\draw (155,182.4) node [anchor=north west][inner sep=0.75pt]    {$\vec{p}$};
		% Text Node
		\draw (278,90.4) node [anchor=north west][inner sep=0.75pt]    {$Z_{1}$};
		% Text Node
		\draw (278,176.4) node [anchor=north west][inner sep=0.75pt]    {$Z_{2}$};
		% Text Node
		\draw (278,328.4) node [anchor=north west][inner sep=0.75pt]    {$Z_{n}$};
		% Text Node
		\draw (145,273) node [anchor=north west][inner sep=0.75pt]   [align=left] {bias};
		% Text Node
		\draw (148,312.4) node [anchor=north west][inner sep=0.75pt]    {$+1$};
		% Text Node
		\draw (172,256.4) node [anchor=north west][inner sep=0.75pt]    {$b_{1}$};
		% Text Node
		\draw (185,282) node [anchor=north west][inner sep=0.75pt]    {$b_{2}$};
		% Text Node
		\draw (188,346.4) node [anchor=north west][inner sep=0.75pt]    {$b_{n}$};
		% Text Node
		\draw (175,133) node [anchor=north west][inner sep=0.75pt]    {$w_{1}$};
		% Text Node
		\draw (192,175) node [anchor=north west][inner sep=0.75pt]    {$w_{2}$};
		% Text Node
		\draw (168,229) node [anchor=north west][inner sep=0.75pt]    {$w_{3}$};
		% Text Node
		\draw (233,18) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{76.16pt}\setlength\topsep{0pt}
		\begin{center}
		each node:\\$\displaystyle Z_{k} =p_{k} w_{k} +b_{k}$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (425,180.4) node [anchor=north west][inner sep=0.75pt]    {$P_{Y|\vec{X}}( k,\vec{p}) =\dfrac{e^{Z_{k}}}{\sum _{i} e^{Z_{i}}}$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Multi-class softmax neural network}
	\end{figure}
	
	\pagebreak
	\paragraph{Various cost functions}\mbox{}\\\\
	Relatively to Neural Networks, as we already know, a cost function is a measure of how good a neural network did with respect to it's given training sample and the expected output. It also may depend on variables such as weights and biases.

	Remember also that a cost function is a single value (scalar), not a vector, because it rates how good the neural network did as a whole.
	
	Specifically, a cost function is of the form:
	
	where $W$ is our neural network's weights, $\vec{b}$ is our neural network's biases, $\vec{x}$ is the input of a single training sample, $\vec{y}$ is the desired output (i.e. expected output) of that training sample and $\phi_i$ is the activation function. 
	
	Here are some common empirical cost functions:
	\begin{itemize}
		\item The "\NewTerm{Quadratic cost}" also known as mean squared error, maximum likelihood, and sum squared error, is defined as:
		
		The gradient of this cost function with respect to the output of a neural network is:
		
	
		\item  The "\NewTerm{Cross-entropy cost}" also known as "\NewTerm{Bernoulli negative log-likelihood}" and "\NewTerm{Binary Cross-Entropy}", is defined as:
		
		The gradient of this cost function with respect to the output of a neural network is:
		
	
		\item The "\NewTerm{Exponential cost}" requires choosing some parameter $\tau$ that you think will give you the behaviour you want, typically you'll just need to play with this until things work good, and is it is defined by:
		
		The gradient of this cost function with respect to the output of a neural network  is:
		
	
		\item The "\NewTerm{Hellinger distance}" is defined as:
		
		This needs to have positive values, and ideally values between $0$ and $1$. The same is true for the following divergences.
	
		The gradient of this cost function with respect to the output of a neural network is:
		
	
		\item The "\NewTerm{Kullback–Leibler cost}" is defined as (\SeeChapter{see section Statistical Mechanics page \pageref{kullback-leibler divergence}}):
		
	
		The gradient of this cost function with respect to the output of a neural network is:
		
	
		\item The "\NewTerm{Generalized Kullback–Leibler cost}", a special case of the "\NewTerm{Bregman divergence}\footnote{It seems that in machine learning, Bregman divergences are used to calculate the bi-tempered logistic loss, performing better than the softmax function with noisy datasets}", is defined as:
		
	
		The gradient of this cost function with respect to the output of a neural network is:
		
	
		\item The "\NewTerm{Itakura–Saito distance}", also a special case of the Bregman divergence, is defined as:
		
	
		The gradient of this cost function with respect to the output of a neural network is:
		
		
		\item And very likely many others...
	\end{itemize}
	To summarize:
	\begin{table}[H]
		\resizebox{\textwidth}{!}{\centering
		\begin{tabular}{|l|l|l|}
		\hline
		\rowcolor[gray]{0.75} 
		\textbf{Cost function name} & \textbf{Mathematical expression} & \textbf{Gradient} \\ \hline
		 Quadratic cost & $J_\text{MST}\left(W,\vec{b},\vec{y},\vec{\phi}\right)=\dfrac{1}{2}\displaystyle\sum_{i=1}^n\left(\phi_i(z)-y_i\right)^2$ & $\nabla_\phi J_\text{MST}\left(W,\vec{b},\vec{y},\vec{\phi}\right) = \displaystyle\sum_{i=1}^n (\phi_i(z)-y_i)$ \\ \hline
		 Cross-entropy cost & $J_\text{CE}\left(W,\vec{b},\vec{y},\vec{\phi}\right) =-\displaystyle\sum_{i=1}^n\left[y_i\ln(\phi(z))+(1-y_i)\ln(1-y_i)\right]$ & $\nabla_\phi  J_\text{CE}\left(W,\vec{b},\vec{y},\vec{\phi}\right) =\displaystyle\sum_{i=1}^n\dfrac{\phi_i(z)-y_i}{(1-\phi_i(z))\phi_i(z)}$ \\ \hline
		 Exponential cost & $J_\text{exp}\left(W,\vec{b},\vec{y},\vec{\phi},\tau\right) =\tau \exp\left(\dfrac{1}{\tau} \displaystyle\sum_{i=1}^n \left(\phi_i(z)-y_i\right)^2 \right)$ & $\nabla_\phi  J_\text{exp}\left(W,\vec{b},\vec{y},\vec{\phi},\tau\right) =\dfrac{2}{\tau}J_\text{exp}\left(W,\vec{b},\vec{y},\vec{\phi},\tau\right)\displaystyle\sum_{i=1}^n \phi_i(z)-y_i$ \\ \hline
		 Hellinger distance & $J_\text{HD}\left(W,\vec{b},\vec{y},\vec{\phi}\right) =\dfrac{1}{\sqrt{2}}\displaystyle\sum_{i=1}^n \left(\sqrt{\phi_i(z)}-\sqrt{y_i} \right)^2$ & $\nabla_\phi  J_\text{exp}\left(W,\vec{b},\vec{y},\vec{\phi}\right) =\dfrac{1}{\sqrt{2}}\displaystyle\sum_{i=1}^n \dfrac{\sqrt{\phi_i(z)}-\sqrt{y_i}}{\sqrt{\phi_i(z)}}$ \\ \hline
		 Kullback-Leibler (KL) cost & $J_\text{KL}\left(W,\vec{b},\vec{y},\vec{\phi}\right) =\displaystyle\sum_{i=1}^n y_i\log\left(\dfrac{y_i}{\phi_i(z)}\right)$ & $\nabla_\phi  J_\text{GKL}\left(W,\vec{b},\vec{y},\vec{\phi}\right) =\displaystyle\sum_{i=1}^n \dfrac{\phi_i(z)-y_i}{\phi_i(z)}$ \\ \hline
		 Generalized KL cost & $J_\text{GKL}\left(W,\vec{b},\vec{y},\vec{\phi}\right) =\displaystyle\sum_{i=1}^n y_i\log\left(\dfrac{y_i}{\phi_i(z)}\right)-\displaystyle\sum_{i=1}^n y_i+\displaystyle\sum_{i=1}^n \phi_i(z)$ & $\nabla_\phi  J_\text{GKL}\left(W,\vec{b},\vec{y},\vec{\phi}\right) =\displaystyle\sum_{i=1}^n \dfrac{\phi_i(z)-y_i}{\phi_i(z)}$  \\ \hline
		 Itakura-Saito distance cost & $J_\text{IS}\left(W,\vec{b},\vec{y},\vec{\phi}\right) =\displaystyle\sum_{i=1}^n \left(\dfrac{y_i}{\phi_i(z)}-\log\left(\dfrac{y_i}{\phi_i(z)}\right)-1\right)$ & $\nabla_\phi  J_\text{IS}\left(W,\vec{b},\vec{y},\vec{\phi}\right) =\displaystyle\sum_{i=1}^n \dfrac{\phi_i(z)-y_i}{\phi_i^2(z)}$ \\ \hline
		 ... & ... & ... \\ \hline
		\end{tabular}}
	\end{table}
	
	\pagebreak
	\paragraph{Network Architecture}\mbox{}\\\\
	By definition, a "\NewTerm{neural network}\index{neural network}" (we will see the formal mathematical definition further below at page \pageref{neural network definition}) is a network of several neurons, usually organized in layers. To build one layer of neurons $S$, we simply need to assemble them as in the figure below ($R$ inputs with $S$ hidden neurons and $S$ outputs):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1107); %set diagram left start at 0, and has height of 1107
		
		%Shape: Rectangle [id:dp18295936691979353] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (114.5,66) -- (194.5,66) -- (194.5,347) -- (114.5,347) -- cycle ;
		%Shape: Rectangle [id:dp4310992089634078] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (203.5,66) -- (528.5,66) -- (528.5,347) -- (203.5,347) -- cycle ;
		%Shape: Diamond [id:dp33055753486510997] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (148,93) -- (161,106) -- (148,119) -- (135,106) -- cycle ;
		%Shape: Diamond [id:dp21706838234806236] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (148,152) -- (161,165) -- (148,178) -- (135,165) -- cycle ;
		%Shape: Diamond [id:dp029345781146275174] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (148,215) -- (161,228) -- (148,241) -- (135,228) -- cycle ;
		%Shape: Diamond [id:dp2561474489243811] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (148,314) -- (161,327) -- (148,340) -- (135,327) -- cycle ;
		%Straight Lines [id:da19769552541575353] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (148,250) -- (148,302) ;
		%Shape: Circle [id:dp9751168592583066] 
		\draw  [line width=1.5]  (265,195.5) .. controls (265,177.55) and (279.55,163) .. (297.5,163) .. controls (315.45,163) and (330,177.55) .. (330,195.5) .. controls (330,213.45) and (315.45,228) .. (297.5,228) .. controls (279.55,228) and (265,213.45) .. (265,195.5) -- cycle ;
		%Straight Lines [id:da36190521360641914] 
		\draw    (148,106) -- (270.5,177) ;
		%Straight Lines [id:da9251016832796668] 
		\draw    (148,165) -- (265,195.5) ;
		%Straight Lines [id:da7501725765868923] 
		\draw    (148,228) -- (267.5,209) ;
		%Straight Lines [id:da6726332127744443] 
		\draw    (148,228) -- (268.5,116) ;
		%Straight Lines [id:da19464294515879343] 
		\draw    (148,327) -- (273.5,124) ;
		%Straight Lines [id:da072194412750199] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (229,218) -- (229,247) ;
		%Straight Lines [id:da8097609204463692] 
		\draw    (297.5,151) -- (297.5,134) ;
		%Shape: Diamond [id:dp9074203527213871] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (297.5,145) -- (303.5,151) -- (297.5,157) -- (291.5,151) -- cycle ;
		%Straight Lines [id:da44460197301163773] 
		\draw    (329,195.5) -- (402.5,195.5) ;
		\draw [shift={(405.5,195.5)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Rounded Rect [id:dp013268751612053764] 
		\draw  [line width=1.5]  (404.5,180.63) .. controls (404.5,174.76) and (409.26,170) .. (415.13,170) -- (454.87,170) .. controls (460.74,170) and (465.5,174.76) .. (465.5,180.63) -- (465.5,212.53) .. controls (465.5,218.4) and (460.74,223.16) .. (454.87,223.16) -- (415.13,223.16) .. controls (409.26,223.16) and (404.5,218.4) .. (404.5,212.53) -- cycle ;
		%Straight Lines [id:da20553843622993062] 
		\draw    (464.5,195.5) -- (550.5,195.5) ;
		\draw [shift={(553.5,195.5)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Shape: Circle [id:dp3188524675895934] 
		\draw  [line width=1.5]  (265,101.5) .. controls (265,83.55) and (279.55,69) .. (297.5,69) .. controls (315.45,69) and (330,83.55) .. (330,101.5) .. controls (330,119.45) and (315.45,134) .. (297.5,134) .. controls (279.55,134) and (265,119.45) .. (265,101.5) -- cycle ;
		%Straight Lines [id:da004847226979944663] 
		\draw    (329,101.5) -- (402.5,101.5) ;
		\draw [shift={(405.5,101.5)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Rounded Rect [id:dp848735515893005] 
		\draw  [line width=1.5]  (404.5,86.63) .. controls (404.5,80.76) and (409.26,76) .. (415.13,76) -- (454.87,76) .. controls (460.74,76) and (465.5,80.76) .. (465.5,86.63) -- (465.5,118.53) .. controls (465.5,124.4) and (460.74,129.16) .. (454.87,129.16) -- (415.13,129.16) .. controls (409.26,129.16) and (404.5,124.4) .. (404.5,118.53) -- cycle ;
		%Straight Lines [id:da9812096521708584] 
		\draw    (464.5,101.5) -- (550.5,101.5) ;
		\draw [shift={(553.5,101.5)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Shape: Circle [id:dp28511228043698766] 
		\draw  [line width=1.5]  (265,288.5) .. controls (265,270.55) and (279.55,256) .. (297.5,256) .. controls (315.45,256) and (330,270.55) .. (330,288.5) .. controls (330,306.45) and (315.45,321) .. (297.5,321) .. controls (279.55,321) and (265,306.45) .. (265,288.5) -- cycle ;
		%Straight Lines [id:da9750857840109659] 
		\draw    (331,288.5) -- (404.5,288.5) ;
		\draw [shift={(407.5,288.5)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Rounded Rect [id:dp6454821919836329] 
		\draw  [line width=1.5]  (406.5,273.63) .. controls (406.5,267.76) and (411.26,263) .. (417.13,263) -- (456.87,263) .. controls (462.74,263) and (467.5,267.76) .. (467.5,273.63) -- (467.5,305.53) .. controls (467.5,311.4) and (462.74,316.16) .. (456.87,316.16) -- (417.13,316.16) .. controls (411.26,316.16) and (406.5,311.4) .. (406.5,305.53) -- cycle ;
		%Straight Lines [id:da7473690379171618] 
		\draw    (466.5,288.5) -- (552.5,288.5) ;
		\draw [shift={(555.5,288.5)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da6808674782356081] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (366,206.5) -- (366,261) ;
		%Straight Lines [id:da8435344019220103] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (504.5,205) -- (504.5,262) ;
		%Straight Lines [id:da754883226004667] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (435.5,229) -- (435.5,256) ;
		%Straight Lines [id:da766987373675377] 
		\draw    (298.5,244) -- (298.5,227) ;
		%Shape: Diamond [id:dp6375031216656446] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (298.5,238) -- (304.5,244) -- (298.5,250) -- (292.5,244) -- cycle ;
		%Straight Lines [id:da579601262988285] 
		\draw    (297.5,338) -- (297.5,321) ;
		%Shape: Diamond [id:dp9614903062332885] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (297.5,332) -- (303.5,338) -- (297.5,344) -- (291.5,338) -- cycle ;
		%Straight Lines [id:da742382643534379] 
		\draw    (148,106) -- (266.5,92) ;
		%Straight Lines [id:da05608229147276722] 
		\draw    (148,106) -- (271.5,270) ;
		%Straight Lines [id:da45843491753507437] 
		\draw    (148,165) -- (265,101.5) ;
		%Straight Lines [id:da3013195297452482] 
		\draw    (148,165) -- (265,288.5) ;
		%Straight Lines [id:da206324123013091] 
		\draw    (148,228) -- (267.5,302) ;
		%Straight Lines [id:da9431462184802677] 
		\draw    (148,327) -- (275.5,221) ;
		%Straight Lines [id:da15476856872226197] 
		\draw    (148,327) -- (272.5,311) ;
		
		% Text Node
		\draw (124,40) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle R$ inputs};
		% Text Node
		\draw (299,40.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {Neuron Model};
		% Text Node
		\draw (90,98) node [anchor=north west][inner sep=0.75pt]    {$p_{1}$};
		% Text Node
		\draw (90,159) node [anchor=north west][inner sep=0.75pt]    {$p_{2}$};
		% Text Node
		\draw (90,218) node [anchor=north west][inner sep=0.75pt]    {$p_{3}$};
		% Text Node
		\draw (88,319) node [anchor=north west][inner sep=0.75pt]    {$p_{R}$};
		% Text Node
		\draw (184,80) node [anchor=north west][inner sep=0.75pt]    {$w_{1,1}$};
		% Text Node
		\draw (182,325) node [anchor=north west][inner sep=0.75pt]    {$w_{S,R}$};
		% Text Node %deuxième sigma
		\draw (285,184) node [anchor=north west][inner sep=0.75pt]  [font=\Huge]  {$\Sigma $};
		% Text Node
		\draw (272,140.4) node [anchor=north west][inner sep=0.75pt]    {$b_{1}$};
		% Text Node
		\draw (302,132.4) node [anchor=north west][inner sep=0.75pt]    {$-1$};
		% Text Node
		\draw (357,177) node [anchor=north west][inner sep=0.75pt]    {$n_{2}$};
		% Text Node %deuxième f
		\draw (425,184) node [anchor=north west][inner sep=0.75pt]  [font=\Huge]  {$f$};
		% Text Node
		\draw (498,178) node [anchor=north west][inner sep=0.75pt]    {$a_{2}$};
		% Text Node
		\draw (299,353.4) node [anchor=north west][inner sep=0.75pt]    {$a=f( W\vec{p} -\vec{b})$};
		% Text Node %premier sigma
		\draw (285,88) node [anchor=north west][inner sep=0.75pt]  [font=\Huge]  {$\Sigma $};
		% Text Node
		\draw (357,85) node [anchor=north west][inner sep=0.75pt]    {$n_{1}$};
		% Text Node %premier f
		\draw (423,88) node [anchor=north west][inner sep=0.75pt]  [font=\Huge]  {$f$};
		% Text Node
		\draw (498,84) node [anchor=north west][inner sep=0.75pt]    {$a_{1}$};
		% Text Node %troisième sigma
		\draw (285,275) node [anchor=north west][inner sep=0.75pt]  [font=\Huge]  {$\Sigma $};
		% Text Node
		\draw (359,271) node [anchor=north west][inner sep=0.75pt]    {$n_{S}$};
		% Text Node %troisième f
		\draw (427,275) node [anchor=north west][inner sep=0.75pt]  [font=\Huge]  {$f$};
		% Text Node
		\draw (500,273) node [anchor=north west][inner sep=0.75pt]    {$a_{S}$};
		% Text Node
		\draw (303,225.4) node [anchor=north west][inner sep=0.75pt]    {$-1$};
		% Text Node
		\draw (302,319.4) node [anchor=north west][inner sep=0.75pt]    {$-1$};
		% Text Node
		\draw (272,234.4) node [anchor=north west][inner sep=0.75pt]    {$b_{2}$};
		% Text Node
		\draw (272,325.4) node [anchor=north west][inner sep=0.75pt]    {$b_{S}$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Example of one-layer formal neural network with the input vector and output vector}
	\end{figure}
	The $S$ neurons of a same layer are all connected to the $R$ inputs in the figure above. We then say that the layer is "fully connected". But this is a special case and not a generality. Often the inputs of a neuron are different from those of another neuron, etc.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	If a neural network that has for purpose to learn that can learn a probability distribution over its set of inputs has only one hidden layer of $S$ neurons not interconnected between them (as is the case in the figure above), that is to say independent, we speak then of "\NewTerm{restricted Boltzmann machine RBM} \index{restricted Boltzmann machine}".
	\end{tcolorbox}
	
	A weight $w_{i,j}$ is associated with each connection. We will always denote the first index by $i$ and the second by $j$. The first index (row) always means the neuron number on the layer, while the second index (column) specifies the number of the input. Thus, $w_{i,j}$ denotes the weight of the connection that connects the neuron $i$ to its input $j$. All the weights of a neuron lays thus forms a matrix $W$ of dimension $S\times R$:
	
	We must of course take into account that dimensionally we have not necessarily $S=R$ in the general case (the numbers of neurons and inputs are independent). If we consider that the $S$ neurons form a vector, then we can create the vectors:
	
	This brings us to the simplified representation illustrated below:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1107); %set diagram left start at 0, and has height of 1107
		
		%Shape: Rectangle [id:dp18295936691979353] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (114.5,66) -- (194.5,66) -- (194.5,347) -- (114.5,347) -- cycle ;
		%Shape: Rectangle [id:dp4310992089634078] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (203.5,66) -- (528.5,66) -- (528.5,347) -- (203.5,347) -- cycle ;
		%Shape: Circle [id:dp9751168592583066] 
		\draw  [line width=1.5]  (294,195.5) .. controls (294,177.55) and (308.55,163) .. (326.5,163) .. controls (344.45,163) and (359,177.55) .. (359,195.5) .. controls (359,213.45) and (344.45,228) .. (326.5,228) .. controls (308.55,228) and (294,213.45) .. (294,195.5) -- cycle ;
		%Straight Lines [id:da20553843622993062] 
		\draw    (471.5,197.5) -- (540.5,197.5) ;
		\draw [shift={(543.5,197.5)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Rounded Rect [id:dp6454821919836329] 
		\draw  [line width=1.5]  (411.5,181.63) .. controls (411.5,175.76) and (416.26,171) .. (422.13,171) -- (461.87,171) .. controls (467.74,171) and (472.5,175.76) .. (472.5,181.63) -- (472.5,213.53) .. controls (472.5,219.4) and (467.74,224.16) .. (461.87,224.16) -- (422.13,224.16) .. controls (416.26,224.16) and (411.5,219.4) .. (411.5,213.53) -- cycle ;
		%Shape: Rectangle [id:dp3103598552442033] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (141.25,77.5) -- (167.75,77.5) -- (167.75,320) -- (141.25,320) -- cycle ;
		%Straight Lines [id:da11143726437296575] 
		\draw    (157.5,101.5) -- (249.5,101.5) ;
		\draw [shift={(252.5,101.5)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da9441271635743058] 
		\draw    (359,195.5) -- (407.5,195.5) ;
		\draw [shift={(410.5,195.5)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Rounded Rect [id:dp8790634976957183] 
		\draw  [line width=1.5]  (252.5,85.63) .. controls (252.5,79.76) and (257.26,75) .. (263.13,75) -- (302.87,75) .. controls (308.74,75) and (313.5,79.76) .. (313.5,85.63) -- (313.5,117.53) .. controls (313.5,123.4) and (308.74,128.16) .. (302.87,128.16) -- (263.13,128.16) .. controls (257.26,128.16) and (252.5,123.4) .. (252.5,117.53) -- cycle ;
		%Straight Lines [id:da9499622509127759] 
		\draw    (312.5,100) -- (325.85,160.07) ;
		\draw [shift={(326.5,163)}, rotate = 257.47] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da5401620844403945] 
		\draw    (191.5,295.5) -- (263.5,295.5) ;
		\draw [shift={(266.5,295.5)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Rounded Rect [id:dp47094902099471114] 
		\draw  [line width=1.5]  (265.5,279.63) .. controls (265.5,273.76) and (270.26,269) .. (276.13,269) -- (315.87,269) .. controls (321.74,269) and (326.5,273.76) .. (326.5,279.63) -- (326.5,311.53) .. controls (326.5,317.4) and (321.74,322.16) .. (315.87,322.16) -- (276.13,322.16) .. controls (270.26,322.16) and (265.5,317.4) .. (265.5,311.53) -- cycle ;
		%Straight Lines [id:da025634596847625257] 
		\draw    (296.5,269) -- (324.73,230.42) ;
		\draw [shift={(326.5,228)}, rotate = 126.19] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		
		% Text Node
		\draw (130,40) node [anchor=north west][inner sep=0.75pt]   [align=left] {Inputs};
		% Text Node
		\draw (299,40.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {Neuron Model};
		% Text Node
		\draw (314,184) node [anchor=north west][inner sep=0.75pt]  [font=\Huge]  {$+$};
		% Text Node
		\draw (498,180) node [anchor=north west][inner sep=0.75pt]    {$\vec a$};
		% Text Node
		\draw (299,353.4) node [anchor=north west][inner sep=0.75pt]    {$\vec a=f\left(W\vec{p} -\vec b\right)$};
		% Text Node
		\draw (432,182) node [anchor=north west][inner sep=0.75pt]  [font=\Huge]  {$f$};
		% Text Node
		\draw (485,201.4) node [anchor=north west][inner sep=0.75pt]    {$S\times 1$};
		% Text Node
		\draw (148,325.4) node [anchor=north west][inner sep=0.75pt]    {$R$};
		% Text Node
		\draw (215,78.4) node [anchor=north west][inner sep=0.75pt]    {$\vec{p}$};
		% Text Node
		\draw (204,106.4) node [anchor=north west][inner sep=0.75pt]    {$R\times 1$};
		% Text Node
		\draw (269,86) node [anchor=north west][inner sep=0.75pt]  [font=\Huge]  {$W$};
		% Text Node
		\draw (376,180) node [anchor=north west][inner sep=0.75pt]    {$\vec n$};
		% Text Node
		\draw (170,285.4) node [anchor=north west][inner sep=0.75pt]    {$-1$};
		% Text Node
		\draw (287,280) node [anchor=north west][inner sep=0.75pt]  [font=\Huge]  {$\vec b$};
		% Text Node
		\draw (278.13,325.56) node [anchor=north west][inner sep=0.75pt]    {$S\times 1$};
		% Text Node
		\draw (363,198.9) node [anchor=north west][inner sep=0.75pt]    {$S\times 1$};
		% Text Node
		\draw (265.13,131.56) node [anchor=north west][inner sep=0.75pt]    {$S\times R$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Vector written example of one-layer formal neural network with the input vector and output vector}
	\end{figure}
	Finally, to build a neural network (or MLP for "\NewTerm{Multi-Layer Perceptron}\index{multi-layer perceptron}"), it is just sufficient to combine layers as below:
	\begin{figure}[H]
		\centering
		\resizebox{\textwidth}{!}{%
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1107); %set diagram left start at 0, and has height of 1107
		
		%Shape: Rectangle [id:dp26704859533670544] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (5.57,122.76) -- (45.5,122.76) -- (45.5,281) -- (5.57,281) -- cycle ;
		%Shape: Rectangle [id:dp13354705652486265] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (49.5,124.76) -- (243.5,124.76) -- (243.5,283) -- (49.5,283) -- cycle ;
		%Shape: Rectangle [id:dp7166843205882028] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (250.5,124.76) -- (440.5,124.76) -- (440.5,283) -- (250.5,283) -- cycle ;
		%Shape: Rectangle [id:dp8832691968635509] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (447.5,124.76) -- (640.5,124.76) -- (640.5,283) -- (447.5,283) -- cycle ;
		%Shape: Rectangle [id:dp6393252043589952] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (18.94,134.8) -- (33.5,134.8) -- (33.5,263) -- (18.94,263) -- cycle ;
		%Straight Lines [id:da816174630500963] 
		\draw    (32.02,146.69) -- (67.5,146.69) ;
		\draw [shift={(70.5,146.69)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da9979988648170137] 
		\draw    (103.02,146.69) -- (125.69,176.61) ;
		\draw [shift={(127.5,179)}, rotate = 232.85] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da7383181327902333] 
		\draw    (95.02,237.69) -- (123.28,212.02) ;
		\draw [shift={(125.5,210)}, rotate = 137.75] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da6135988583936882] 
		\draw    (161.02,196.69) -- (194.5,196.69) ;
		\draw [shift={(197.5,196.69)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Rounded Rect [id:dp4586945714358608] 
		\draw  [line width=1.5]  (198.52,139.89) .. controls (198.52,136.47) and (201.3,133.7) .. (204.72,133.7) -- (223.3,133.7) .. controls (226.73,133.7) and (229.5,136.47) .. (229.5,139.89) -- (229.5,255.8) .. controls (229.5,259.23) and (226.73,262) .. (223.3,262) -- (204.72,262) .. controls (201.3,262) and (198.52,259.23) .. (198.52,255.8) -- cycle ;
		%Rounded Rect [id:dp6788907367113088] 
		\draw  [line width=1.5]  (72.25,140.33) .. controls (72.25,137.39) and (74.63,135) .. (77.58,135) -- (97.6,135) .. controls (100.54,135) and (102.93,137.39) .. (102.93,140.33) -- (102.93,156.32) .. controls (102.93,159.26) and (100.54,161.65) .. (97.6,161.65) -- (77.58,161.65) .. controls (74.63,161.65) and (72.25,159.26) .. (72.25,156.32) -- cycle ;
		%Rounded Rect [id:dp3004209804051534] 
		\draw  [line width=1.5]  (69.67,243.02) .. controls (69.67,240.08) and (72.06,237.69) .. (75,237.69) -- (95.02,237.69) .. controls (97.96,237.69) and (100.35,240.08) .. (100.35,243.02) -- (100.35,259.01) .. controls (100.35,261.95) and (97.96,264.34) .. (95.02,264.34) -- (75,264.34) .. controls (72.06,264.34) and (69.67,261.95) .. (69.67,259.01) -- cycle ;
		%Straight Lines [id:da540084547503523] 
		\draw    (52.5,253.17) -- (66.67,253.17) ;
		\draw [shift={(69.67,253.17)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Shape: Ellipse [id:dp8364605779359722] 
		\draw  [line width=1.5]  (119.97,196.69) .. controls (119.97,185.02) and (129.16,175.56) .. (140.5,175.56) .. controls (151.83,175.56) and (161.02,185.02) .. (161.02,196.69) .. controls (161.02,208.36) and (151.83,217.82) .. (140.5,217.82) .. controls (129.16,217.82) and (119.97,208.36) .. (119.97,196.69) -- cycle ;
		%Straight Lines [id:da07081181307773687] 
		\draw    (230.5,147.69) -- (267.5,147.69) ;
		\draw [shift={(270.5,147.69)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da21264130054312913] 
		\draw    (302.02,146.69) -- (324.69,176.61) ;
		\draw [shift={(326.5,179)}, rotate = 232.85] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da4932051879560906] 
		\draw    (294.02,237.69) -- (322.28,212.02) ;
		\draw [shift={(324.5,210)}, rotate = 137.75] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da11481527485321652] 
		\draw    (360.02,196.69) -- (393.5,196.69) ;
		\draw [shift={(396.5,196.69)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Rounded Rect [id:dp6418958639064931] 
		\draw  [line width=1.5]  (397.52,139.89) .. controls (397.52,136.47) and (400.3,133.7) .. (403.72,133.7) -- (422.3,133.7) .. controls (425.73,133.7) and (428.5,136.47) .. (428.5,139.89) -- (428.5,255.8) .. controls (428.5,259.23) and (425.73,262) .. (422.3,262) -- (403.72,262) .. controls (400.3,262) and (397.52,259.23) .. (397.52,255.8) -- cycle ;
		%Rounded Rect [id:dp7066099708099041] 
		\draw  [line width=1.5]  (271.25,140.33) .. controls (271.25,137.39) and (273.63,135) .. (276.58,135) -- (296.6,135) .. controls (299.54,135) and (301.93,137.39) .. (301.93,140.33) -- (301.93,156.32) .. controls (301.93,159.26) and (299.54,161.65) .. (296.6,161.65) -- (276.58,161.65) .. controls (273.63,161.65) and (271.25,159.26) .. (271.25,156.32) -- cycle ;
		%Rounded Rect [id:dp7442003107658814] 
		\draw  [line width=1.5]  (268.67,243.02) .. controls (268.67,240.08) and (271.06,237.69) .. (274,237.69) -- (294.02,237.69) .. controls (296.96,237.69) and (299.35,240.08) .. (299.35,243.02) -- (299.35,259.01) .. controls (299.35,261.95) and (296.96,264.34) .. (294.02,264.34) -- (274,264.34) .. controls (271.06,264.34) and (268.67,261.95) .. (268.67,259.01) -- cycle ;
		%Shape: Ellipse [id:dp3687490616027769] 
		\draw  [line width=1.5]  (318.97,196.69) .. controls (318.97,185.02) and (328.16,175.56) .. (339.5,175.56) .. controls (350.83,175.56) and (360.02,185.02) .. (360.02,196.69) .. controls (360.02,208.36) and (350.83,217.82) .. (339.5,217.82) .. controls (328.16,217.82) and (318.97,208.36) .. (318.97,196.69) -- cycle ;
		%Straight Lines [id:da27520676263927757] 
		\draw    (429.5,147.69) -- (466.5,147.69) ;
		\draw [shift={(469.5,147.69)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da4873221109747483] 
		\draw    (499.02,146.69) -- (521.69,176.61) ;
		\draw [shift={(523.5,179)}, rotate = 232.85] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da7307627775028651] 
		\draw    (491.02,237.69) -- (519.28,212.02) ;
		\draw [shift={(521.5,210)}, rotate = 137.75] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da16793238240574282] 
		\draw    (557.02,196.69) -- (590.5,196.69) ;
		\draw [shift={(593.5,196.69)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Rounded Rect [id:dp5538051259513701] 
		\draw  [line width=1.5]  (594.52,139.89) .. controls (594.52,136.47) and (597.3,133.7) .. (600.72,133.7) -- (619.3,133.7) .. controls (622.73,133.7) and (625.5,136.47) .. (625.5,139.89) -- (625.5,255.8) .. controls (625.5,259.23) and (622.73,262) .. (619.3,262) -- (600.72,262) .. controls (597.3,262) and (594.52,259.23) .. (594.52,255.8) -- cycle ;
		%Rounded Rect [id:dp44806788375616646] 
		\draw  [line width=1.5]  (468.25,140.33) .. controls (468.25,137.39) and (470.63,135) .. (473.58,135) -- (493.6,135) .. controls (496.54,135) and (498.93,137.39) .. (498.93,140.33) -- (498.93,156.32) .. controls (498.93,159.26) and (496.54,161.65) .. (493.6,161.65) -- (473.58,161.65) .. controls (470.63,161.65) and (468.25,159.26) .. (468.25,156.32) -- cycle ;
		%Rounded Rect [id:dp0063984388572448925] 
		\draw  [line width=1.5]  (465.67,243.02) .. controls (465.67,240.08) and (468.06,237.69) .. (471,237.69) -- (491.02,237.69) .. controls (493.96,237.69) and (496.35,240.08) .. (496.35,243.02) -- (496.35,259.01) .. controls (496.35,261.95) and (493.96,264.34) .. (491.02,264.34) -- (471,264.34) .. controls (468.06,264.34) and (465.67,261.95) .. (465.67,259.01) -- cycle ;
		%Shape: Ellipse [id:dp09860270198623078] 
		\draw  [line width=1.5]  (515.97,196.69) .. controls (515.97,185.02) and (525.16,175.56) .. (536.5,175.56) .. controls (547.83,175.56) and (557.02,185.02) .. (557.02,196.69) .. controls (557.02,208.36) and (547.83,217.82) .. (536.5,217.82) .. controls (525.16,217.82) and (515.97,208.36) .. (515.97,196.69) -- cycle ;
		%Straight Lines [id:da5234592699493159] 
		\draw    (626.5,147.69) -- (651.5,147.69) ;
		\draw [shift={(654.5,147.69)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da2394785013545353] 
		\draw    (253.5,253.17) -- (267.67,253.17) ;
		\draw [shift={(270.67,253.17)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da9810955121362503] 
		\draw    (449.5,253.17) -- (463.67,253.17) ;
		\draw [shift={(466.67,253.17)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		
		% Text Node
		\draw (109.35,100.72) node [anchor=north west][inner sep=0.75pt]   [align=left] {Layer $1$};
		% Text Node
		\draw (73.94,288.11) node [anchor=north west][inner sep=0.75pt]    {$\vec{a}^{\,1} =f^{1}\left( W^{1}\vec{p}^{\,1} -\vec{b}^{\,1}\right)$};
		% Text Node
		\draw (20.94,268.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$R$};
		% Text Node
		\draw (47.5,126.16) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$\vec{p}^{\,1}$};
		% Text Node
		\draw (34.02,150.09) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$R\times 1$};
		% Text Node
		\draw (34.06,247.57) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$-1$};
		% Text Node
		\draw (65,166.02) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$S^{1} \times R$};
		% Text Node
		\draw (128,185) node [anchor=north west][inner sep=0.75pt]  [font=\Huge]  {$+$};
		% Text Node
		\draw (203.02,180.67) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$f^{1}$};
		% Text Node
		\draw (315.35,100.72) node [anchor=north west][inner sep=0.75pt]   [align=left] {Layer $2$};
		% Text Node
		\draw (506.35,101.72) node [anchor=north west][inner sep=0.75pt]   [align=left] {Layer $3$};
		% Text Node
		\draw (62,267.41) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$S^{1} \times 1$};
		% Text Node
		\draw (170.5,176.16) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$\vec{n}^{\,1}$};
		% Text Node
		\draw (5,104.44) node [anchor=north west][inner sep=0.75pt]   [align=left] {Inputs};
		% Text Node
		\draw (159,201.09) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$S^{1} \times 1$};
		% Text Node
		\draw (74.25,139.73) node [anchor=north west][inner sep=0.75pt]    {$W^{1}$};
		% Text Node
		\draw (79,240) node [anchor=north west][inner sep=0.75pt]    {$\vec{b}^{\,1}$};
		% Text Node
		\draw (194,267.41) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$S^{1} \times 1$};
		% Text Node
		\draw (250.5,126.16) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$\vec{a}^{\,1}$};
		% Text Node
		\draw (268.94,288.11) node [anchor=north west][inner sep=0.75pt]    {$\vec{a}^{\,2} =f^{2}\left( W^{2}\vec{p}^{\,2} -\vec{b}^{\,2}\right)$};
		% Text Node
		\draw (468.94,288.11) node [anchor=north west][inner sep=0.75pt]    {$\vec{a}^{\,3} =f^{3}\left( W^{3}\vec{p}^{\,3} -\vec{b}^{\,3}\right)$};
		% Text Node
		\draw (267.66,166.02) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$S^{2} \times S^{1}$};
		% Text Node
		\draw (325.45,185) node [anchor=north west][inner sep=0.75pt]  [font=\Huge]  {$+$};
		% Text Node
		\draw (402.02,180.67) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$f^{2}$};
		% Text Node
		\draw (264,267.41) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$S^{2} \times 1$};
		% Text Node
		\draw (369.5,176.16) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$\vec{n}^{\,2}$};
		% Text Node
		\draw (358,200.09) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$S^{2} \times 1$};
		% Text Node
		\draw (273.25,139.73) node [anchor=north west][inner sep=0.75pt]    {$W^{2}$};
		% Text Node
		\draw (278,240) node [anchor=north west][inner sep=0.75pt]    {$\vec{b}^{\,2}$};
		% Text Node
		\draw (392,267.41) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$S^{2} \times 1$};
		% Text Node
		\draw (463,166.02) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$S^{3} \times S^{2}$};
		% Text Node
		\draw (522.45,185) node [anchor=north west][inner sep=0.75pt]  [font=\Huge]  {$+$};
		% Text Node
		\draw (599.02,180.67) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$f^{3}$};
		% Text Node
		\draw (461,267.41) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$S^{3} \times 1$};
		% Text Node
		\draw (566.5,176.16) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$\vec{n}^{\,3}$};
		% Text Node
		\draw (555,200.09) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$S^{3} \times 1$};
		% Text Node
		\draw (470.25,139.73) node [anchor=north west][inner sep=0.75pt]    {$W^{3}$};
		% Text Node
		\draw (475,240) node [anchor=north west][inner sep=0.75pt]    {$\vec{b}^{\,3}$};
		% Text Node
		\draw (592,267.41) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$S^{3} \times 1$};
		% Text Node
		\draw (448.5,126.16) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$\vec{a}^{\,2}$};
		% Text Node
		\draw (231,151.09) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$S^{1} \times 1$};
		% Text Node
		\draw (429.5,151.09) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$S^{2} \times 1$};
		% Text Node
		\draw (629.5,126.16) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$\vec{a}^{\,3}$};
		% Text Node
		\draw (627.5,151.09) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$S^{3} \times 1$};
		% Text Node
		\draw (233.5,247.57) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$-1$};
		% Text Node
		\draw (431,247.57) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$-1$};
		
		\end{tikzpicture}
		}
		\caption{Principle of construction of a Multi-Layer Perceptron}
	\end{figure}
	This example includes $R$ inputs and three layers of neurons having respectively $S^1,S^2,S^3$ neurons. In the general case, again these numbers are not necessarily equal. Each layer also has its own weight matrix $W^k$, where $k$ is the layer index. In the context of vectors and matrices relatively to one a layer, we always will use an exponent to describe this index. Thus, the vectors $\vec{b}^k$, $\vec{n}^k$, $\vec{a}^k$ are also associated with the layer $A$.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Multilayer perceptrons are also named "\NewTerm{vanilla feed-forward neural networks}\footnote{"vanilla"= the basic version.}\index{vanilla feed-forward neural networks}"\index{feed-forward neural networks}. A perceptron is always feedforward, that is, all the arrows are going in the direction of the output. Neural networks in general might have loops, and if so, are often named "\NewTerm{recurrent neural networks}\index{recurrent neural networks}" (RNN). A superposition of at least three MLP is what seems to be usually named a "\NewTerm{deep beliefs net}\index{deep beliefs net}" (DLN).
	\end{tcolorbox}
	It should be noticed in this example that the layers that follows the first has as input the output of the previous layer. So we can put on as many layers as we want, at least in theory. We can set any number of neurons of each layer. In practice, we will see later however it is not desirable to use too many neurons. Note also that nothing prevents us from changing transfer function from one layer to another. Thus, in the general case we have not necessarily $f^1=f^2=f^3$.
	
	The first last layer is obviously named "\NewTerm{input layer}\index{input layer}", and the last layer the "\NewTerm{output layer}\index{output layer}". The layers between the output and input layers are commonly named "\NewTerm{hidden layers}\index{hidden layers}".
	
	Not all textbooks and softwares count the number of layers in the same way. Here is a typical example where the first layer doesn't have any activation function, but is however counted as one layer (and the notation also differs slightly):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,972); %set diagram left start at 0, and has height of 972
		
		%Shape: Rectangle [id:dp11274424436296626] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] (231.75,101) -- (263.25,101) -- (263.25,126) -- (231.75,126) -- cycle ;
		%Curve Lines [id:da5510759555089864] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (231.75,126) .. controls (248.25,126) and (244.25,101) .. (263.25,101) ;
		%Shape: Rectangle [id:dp5677966815778512] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] (231.75,186) -- (263.25,186) -- (263.25,211) -- (231.75,211) -- cycle ;
		%Curve Lines [id:da813285008117431] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (231.75,211) .. controls (248.25,211) and (244.25,186) .. (263.25,186) ;
		%Shape: Rectangle [id:dp6726478438784587] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] (231.75,268) -- (263.25,268) -- (263.25,293) -- (231.75,293) -- cycle ;
		%Curve Lines [id:da5393510518548694] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (231.75,293) .. controls (248.25,293) and (244.25,268) .. (263.25,268) ;
		%Shape: Rectangle [id:dp2276019965271998] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] (379.25,54) -- (410.75,54) -- (410.75,79) -- (379.25,79) -- cycle ;
		%Curve Lines [id:da46982574734336024] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (379.25,79) .. controls (395.75,79) and (391.75,54) .. (410.75,54) ;
		%Shape: Rectangle [id:dp8162629459375181] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] (379.25,140) -- (410.75,140) -- (410.75,165) -- (379.25,165) -- cycle ;
		%Curve Lines [id:da42289501097807713] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (379.25,165) .. controls (395.75,165) and (391.75,140) .. (410.75,140) ;
		%Shape: Rectangle [id:dp26770807161322296] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] (379.25,225) -- (410.75,225) -- (410.75,250) -- (379.25,250) -- cycle ;
		%Curve Lines [id:da60663270316931] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (379.25,250) .. controls (395.75,250) and (391.75,225) .. (410.75,225) ;
		%Shape: Rectangle [id:dp3852011702454321] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] (379.25,312) -- (410.75,312) -- (410.75,337) -- (379.25,337) -- cycle ;
		%Curve Lines [id:da20679817752292928] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (379.25,337) .. controls (395.75,337) and (391.75,312) .. (410.75,312) ;
		%Shape: Rectangle [id:dp7552335621819146] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] (528.25,130) -- (559.75,130) -- (559.75,155) -- (528.25,155) -- cycle ;
		%Curve Lines [id:da8164260917877999] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (528.25,155) .. controls (544.75,155) and (540.75,130) .. (559.75,130) ;
		%Shape: Rectangle [id:dp4683204682121269] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] (528.25,235) -- (559.75,235) -- (559.75,260) -- (528.25,260) -- cycle ;
		%Curve Lines [id:da10001083682545264] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (528.25,260) .. controls (544.75,260) and (540.75,235) .. (559.75,235) ;
		%Shape: Circle [id:dp543377755834384] 
		\draw  [line width=1.5]  (228,146.5) .. controls (228,135.73) and (236.73,127) .. (247.5,127) .. controls (258.27,127) and (267,135.73) .. (267,146.5) .. controls (267,157.27) and (258.27,166) .. (247.5,166) .. controls (236.73,166) and (228,157.27) .. (228,146.5) -- cycle ;
		%Shape: Circle [id:dp4712818422894891] 
		\draw  [line width=1.5]  (228,231.5) .. controls (228,220.73) and (236.73,212) .. (247.5,212) .. controls (258.27,212) and (267,220.73) .. (267,231.5) .. controls (267,242.27) and (258.27,251) .. (247.5,251) .. controls (236.73,251) and (228,242.27) .. (228,231.5) -- cycle ;
		%Shape: Circle [id:dp755241098798239] 
		\draw  [line width=1.5]  (228,313.5) .. controls (228,302.73) and (236.73,294) .. (247.5,294) .. controls (258.27,294) and (267,302.73) .. (267,313.5) .. controls (267,324.27) and (258.27,333) .. (247.5,333) .. controls (236.73,333) and (228,324.27) .. (228,313.5) -- cycle ;
		%Shape: Circle [id:dp6140020201746195] 
		\draw  [line width=1.5]  (92.5,146.5) .. controls (92.5,135.73) and (101.23,127) .. (112,127) .. controls (122.77,127) and (131.5,135.73) .. (131.5,146.5) .. controls (131.5,157.27) and (122.77,166) .. (112,166) .. controls (101.23,166) and (92.5,157.27) .. (92.5,146.5) -- cycle ;
		%Shape: Circle [id:dp9823854263683574] 
		\draw  [line width=1.5]  (92.5,231.5) .. controls (92.5,220.73) and (101.23,212) .. (112,212) .. controls (122.77,212) and (131.5,220.73) .. (131.5,231.5) .. controls (131.5,242.27) and (122.77,251) .. (112,251) .. controls (101.23,251) and (92.5,242.27) .. (92.5,231.5) -- cycle ;
		%Shape: Circle [id:dp999168996136262] 
		\draw  [line width=1.5]  (92.5,313.5) .. controls (92.5,302.73) and (101.23,294) .. (112,294) .. controls (122.77,294) and (131.5,302.73) .. (131.5,313.5) .. controls (131.5,324.27) and (122.77,333) .. (112,333) .. controls (101.23,333) and (92.5,324.27) .. (92.5,313.5) -- cycle ;
		%Shape: Circle [id:dp5815403140493358] 
		\draw  [line width=1.5]  (375.5,98.5) .. controls (375.5,87.73) and (384.23,79) .. (395,79) .. controls (405.77,79) and (414.5,87.73) .. (414.5,98.5) .. controls (414.5,109.27) and (405.77,118) .. (395,118) .. controls (384.23,118) and (375.5,109.27) .. (375.5,98.5) -- cycle ;
		%Shape: Circle [id:dp40024959366389834] 
		\draw  [line width=1.5]  (375.5,185.5) .. controls (375.5,174.73) and (384.23,166) .. (395,166) .. controls (405.77,166) and (414.5,174.73) .. (414.5,185.5) .. controls (414.5,196.27) and (405.77,205) .. (395,205) .. controls (384.23,205) and (375.5,196.27) .. (375.5,185.5) -- cycle ;
		%Shape: Circle [id:dp5841976931907888] 
		\draw  [line width=1.5]  (375.5,270.5) .. controls (375.5,259.73) and (384.23,251) .. (395,251) .. controls (405.77,251) and (414.5,259.73) .. (414.5,270.5) .. controls (414.5,281.27) and (405.77,290) .. (395,290) .. controls (384.23,290) and (375.5,281.27) .. (375.5,270.5) -- cycle ;
		%Shape: Circle [id:dp19155248587676788] 
		\draw  [line width=1.5]  (375.5,357.5) .. controls (375.5,346.73) and (384.23,338) .. (395,338) .. controls (405.77,338) and (414.5,346.73) .. (414.5,357.5) .. controls (414.5,368.27) and (405.77,377) .. (395,377) .. controls (384.23,377) and (375.5,368.27) .. (375.5,357.5) -- cycle ;
		%Shape: Circle [id:dp043242034208033964] 
		\draw  [line width=1.5]  (524.5,175.5) .. controls (524.5,164.73) and (533.23,156) .. (544,156) .. controls (554.77,156) and (563.5,164.73) .. (563.5,175.5) .. controls (563.5,186.27) and (554.77,195) .. (544,195) .. controls (533.23,195) and (524.5,186.27) .. (524.5,175.5) -- cycle ;
		%Shape: Circle [id:dp9988692075115606] 
		\draw  [line width=1.5]  (524.5,279.5) .. controls (524.5,268.73) and (533.23,260) .. (544,260) .. controls (554.77,260) and (563.5,268.73) .. (563.5,279.5) .. controls (563.5,290.27) and (554.77,299) .. (544,299) .. controls (533.23,299) and (524.5,290.27) .. (524.5,279.5) -- cycle ;
		%Straight Lines [id:da702931464913396] 
		\draw    (56.5,146.5) -- (89.5,146.5) ;
		\draw [shift={(92.5,146.5)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da017962854462656663] 
		\draw    (56.5,231.5) -- (89.5,231.5) ;
		\draw [shift={(92.5,231.5)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da9035780551520467] 
		\draw    (56.5,313.5) -- (89.5,313.5) ;
		\draw [shift={(92.5,313.5)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da7824974400849263] 
		\draw    (563.5,175.5) -- (600.5,175.5) ;
		\draw [shift={(603.5,175.5)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da6724765523159195] 
		\draw    (131.5,146.5) -- (225,146.5) ;
		\draw [shift={(228,146.5)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da6285829430778851] 
		\draw    (131.5,146.5) -- (227.14,221.15) ;
		\draw [shift={(229.5,223)}, rotate = 217.98] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da7704496919605615] 
		\draw    (131.5,146.5) -- (228.89,299.47) ;
		\draw [shift={(230.5,302)}, rotate = 237.52] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da15372411476264158] 
		\draw    (131.5,231.5) -- (222.66,157.73) -- (226.17,154.89) ;
		\draw [shift={(228.5,153)}, rotate = 141.02] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da6048380955354768] 
		\draw    (131.5,231.5) -- (226.5,231.5) ;
		\draw [shift={(229.5,231.5)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da6397333209412903] 
		\draw    (131.5,231.5) -- (225.14,305.15) ;
		\draw [shift={(227.5,307)}, rotate = 218.18] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da5775758976208369] 
		\draw    (131.5,313.5) -- (224.5,313.5) ;
		\draw [shift={(227.5,313.5)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da9309374817498759] 
		\draw    (131.5,313.5) -- (228.08,242.77) ;
		\draw [shift={(230.5,241)}, rotate = 143.78] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da5649634693800494] 
		\draw    (131.5,313.5) -- (231.83,163.49) ;
		\draw [shift={(233.5,161)}, rotate = 123.78] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da005011164442066951] 
		\draw    (267,146.5) -- (374.83,91.37) ;
		\draw [shift={(377.5,90)}, rotate = 152.92] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da7286539349588372] 
		\draw    (267,231.5) -- (372.74,186.67) ;
		\draw [shift={(375.5,185.5)}, rotate = 157.02] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da5732465672908573] 
		\draw    (267,313.5) -- (373.74,268.17) ;
		\draw [shift={(376.5,267)}, rotate = 156.99] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da33755032924872297] 
		\draw    (267,313.5) -- (372.72,356.37) ;
		\draw [shift={(375.5,357.5)}, rotate = 202.07] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da6920743388298818] 
		\draw    (267,231.5) -- (374.61,261.2) ;
		\draw [shift={(377.5,262)}, rotate = 195.43] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da5587623224422524] 
		\draw    (267,146.5) -- (374.61,176.2) ;
		\draw [shift={(377.5,177)}, rotate = 195.43] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da48480953680015326] 
		\draw    (267,231.5) -- (373.59,102.31) ;
		\draw [shift={(375.5,100)}, rotate = 129.53] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da1222504398119697] 
		\draw    (267,146.5) -- (377.37,255.89) ;
		\draw [shift={(379.5,258)}, rotate = 224.74] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da43331127932897706] 
		\draw    (267,231.5) -- (373.48,348.78) ;
		\draw [shift={(375.5,351)}, rotate = 227.76] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da8711541940400607] 
		\draw    (267,146.5) -- (378.03,343.39) ;
		\draw [shift={(379.5,346)}, rotate = 240.58] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da9960793245251176] 
		\draw    (267,313.5) -- (377.06,112.63) ;
		\draw [shift={(378.5,110)}, rotate = 118.72] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da7005720890213867] 
		\draw    (267,313.5) -- (375.44,198.19) ;
		\draw [shift={(377.5,196)}, rotate = 133.24] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da3154815781195943] 
		\draw    (414.5,98.5) -- (524.9,162.5) ;
		\draw [shift={(527.5,164)}, rotate = 210.1] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da0778522210168413] 
		\draw    (414.5,185.5) -- (521.51,175.77) ;
		\draw [shift={(524.5,175.5)}, rotate = 174.81] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da881039027330319] 
		\draw    (414.5,270.5) -- (524.14,184.85) ;
		\draw [shift={(526.5,183)}, rotate = 142] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da36146590428316405] 
		\draw    (414.5,357.5) -- (530.76,194.44) ;
		\draw [shift={(532.5,192)}, rotate = 125.49] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da5649038207025097] 
		\draw    (414.5,98.5) -- (524.85,266.49) ;
		\draw [shift={(526.5,269)}, rotate = 236.7] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da5566183849753767] 
		\draw    (414.5,185.5) -- (522.22,277.55) ;
		\draw [shift={(524.5,279.5)}, rotate = 220.52] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da2625193514495321] 
		\draw    (414.5,270.5) -- (521.53,284.61) ;
		\draw [shift={(524.5,285)}, rotate = 187.51] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da06311284976944509] 
		\draw    (414.5,357.5) -- (525.9,293.49) ;
		\draw [shift={(528.5,292)}, rotate = 150.12] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da4049387156898454] 
		\draw    (563.5,279.5) -- (600.5,279.5) ;
		\draw [shift={(603.5,279.5)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		
		% Text Node
		\draw (223,74) node [anchor=north west][inner sep=0.75pt]   [align=left] {Layer $\displaystyle 2$};
		% Text Node
		\draw (369,28) node [anchor=north west][inner sep=0.75pt]   [align=left] {Layer $\displaystyle 3$};
		% Text Node
		\draw (518,104) node [anchor=north west][inner sep=0.75pt]   [align=left] {Layer $\displaystyle 4$};
		% Text Node
		\draw (90,104) node [anchor=north west][inner sep=0.75pt]   [align=left] {Layer $\displaystyle 1$};
		% Text Node
		\draw (37,138.4) node [anchor=north west][inner sep=0.75pt]    {$I_{1}$};
		% Text Node
		\draw (37,224.4) node [anchor=north west][inner sep=0.75pt]    {$I_{2}$};
		% Text Node
		\draw (37,305.4) node [anchor=north west][inner sep=0.75pt]    {$I_{3}$};
		% Text Node
		\draw (139,135.4) node [anchor=north west][inner sep=0.75pt]    {$a_{1}^{1}$};
		% Text Node
		\draw (139,220.4) node [anchor=north west][inner sep=0.75pt]    {$a_{2}^{1}$};
		% Text Node
		\draw (139,303.4) node [anchor=north west][inner sep=0.75pt]    {$a_{3}^{1}$};
		% Text Node
		\draw (231.75,134.4) node [anchor=north west][inner sep=0.75pt]    {$+b_{1}^{2}$};
		% Text Node
		\draw (231.75,221) node [anchor=north west][inner sep=0.75pt]    {$+b_{2}^{2}$};
		% Text Node
		\draw (231.75,304.4) node [anchor=north west][inner sep=0.75pt]    {$+b_{3}^{2}$};
		% Text Node
		\draw (277,135.4) node [anchor=north west][inner sep=0.75pt]    {$a_{1}^{2}$};
		% Text Node
		\draw (277,220.4) node [anchor=north west][inner sep=0.75pt]    {$a_{2}^{2}$};
		% Text Node
		\draw (277,303.4) node [anchor=north west][inner sep=0.75pt]    {$a_{3}^{2}$};
		% Text Node
		\draw (186,133.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$w_{1,1}^{2}$};
		% Text Node
		\draw (191,156.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$w_{1,2}^{2}$};
		% Text Node
		\draw (204,177.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$w_{1,3}^{2}$};
		% Text Node
		\draw (195,194.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$w_{2,1}^{2}$};
		% Text Node
		\draw (192,220.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$w_{2,2}^{2}$};
		% Text Node
		\draw (201,241.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$w_{2,3}^{2}$};
		% Text Node
		\draw (203,266.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$w_{3,1}^{2}$};
		% Text Node
		\draw (191,282.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$w_{3,2}^{2}$};
		% Text Node
		\draw (190,304.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$w_{3,3}^{2}$};
		% Text Node
		\draw (379.88,86.4) node [anchor=north west][inner sep=0.75pt]    {$+b_{1}^{3}$};
		% Text Node
		\draw (379.88,174.4) node [anchor=north west][inner sep=0.75pt]    {$+b_{2}^{3}$};
		% Text Node
		\draw (379.88,259.4) node [anchor=north west][inner sep=0.75pt]    {$+b_{3}^{3}$};
		% Text Node
		\draw (379.88,347.4) node [anchor=north west][inner sep=0.75pt]    {$+b_{4}^{3}$};
		% Text Node
		\draw (529,163.4) node [anchor=north west][inner sep=0.75pt]    {$+b_{1}^{4}$};
		% Text Node
		\draw (528.5,270.4) node [anchor=north west][inner sep=0.75pt]    {$+b_{2}^{4}$};
		% Text Node
		\draw (605,165.4) node [anchor=north west][inner sep=0.75pt]    {$O_{1}$};
		% Text Node
		\draw (605,271.4) node [anchor=north west][inner sep=0.75pt]    {$O_{2}$};
		% Text Node
		\draw (333,93.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$w_{1,1}^{3}$};
		% Text Node
		\draw (339,116.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$w_{1,2}^{3}$};
		% Text Node
		\draw (350,137.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$w_{1,3}^{3}$};
		% Text Node
		\draw (343,160.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$w_{2,1}^{3}$};
		% Text Node
		\draw (342,181.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$w_{2,2}^{3}$};
		% Text Node
		\draw (350,204.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$w_{2,3}^{3}$};
		% Text Node
		\draw (348,227.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$w_{2,1}^{3}$};
		% Text Node
		\draw (342,246.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$w_{3,2}^{3}$};
		% Text Node
		\draw (347,268.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$w_{3,3}^{3}$};
		% Text Node
		\draw (350,302.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$w_{4,1}^{3}$};
		% Text Node
		\draw (343,321.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$w_{4,2}^{3}$};
		% Text Node
		\draw (336,339.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$w_{4,3}^{3}$};
		% Text Node
		\draw (426,81.4) node [anchor=north west][inner sep=0.75pt]    {$a_{1}^{3}$};
		% Text Node
		\draw (426,173.4) node [anchor=north west][inner sep=0.75pt]    {$a_{2}^{3}$};
		% Text Node
		\draw (426,251.4) node [anchor=north west][inner sep=0.75pt]    {$a_{3}^{3}$};
		% Text Node
		\draw (426,347.4) node [anchor=north west][inner sep=0.75pt]    {$a_{4}^{3}$};
		% Text Node
		\draw (492,134.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$w_{1,1}^{4}$};
		% Text Node
		\draw (487,162.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$w_{1,2}^{4}$};
		% Text Node
		\draw (491,188.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$w_{1,3}^{4}$};
		% Text Node
		\draw (509,205.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$w_{1,4}^{4}$};
		% Text Node
		\draw (501,233.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$w_{2,1}^{4}$};
		% Text Node
		\draw (491,252.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$w_{2,2}^{4}$};
		% Text Node
		\draw (485,273.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$w_{2,3}^{4}$};
		% Text Node
		\draw (494,295.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$w_{2,4}^{4}$};
		% Text Node
		\draw (568.5,163.4) node [anchor=north west][inner sep=0.75pt]    {$a_{1}^{4}$};
		% Text Node
		\draw (568.5,267.4) node [anchor=north west][inner sep=0.75pt]    {$a_{2}^{4}$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Explicit construction of a Multi-layer Perceptron}
	\end{figure}
	Then we can relate the next layer's input to it's previous via the following relation:
	
	where:
	\begin{itemize}
		\item $f$ is the activation function
		\item $w_{jk}^i$ is the weight from the $k$-th neuron in the $(i-1)$-th layer to the $j$-th neuron in the $i$-th layer
		\item $b_j^i$ is the bias of the $j$-th neuron in the $i$-th layer
		\item $a_j^i$ represents the activation value of the $j$-th neuron in the $i$-th layer
	\end{itemize}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Multilayer neural networks are more powerful than simple single layer neural networks of course. Using two layers, provided you use a sigmoid activation function on the hidden layer we can "train" a network to produce an approximation of most functions with arbitrary precision. Except in rare cases, artificial neural networks use two or three layers.
	\end{tcolorbox}
	"\NewTerm{Train}\index{train a neural network}" a neural network means changing the value of its weight matrices and its so that it realizes the desired input/output function (I / O). We will study in detail various algorithms and methods of heuristics approach to achieve it in different contexts.
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/computing/neural_information_processing.jpg}
		\caption[Neural Information Processing]{Neural Information Processing (source: Purdue University image/e-Lab)}
	\end{figure}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In the early 121st century (holocene calendar) there seems to no exist any standard nor accepted method for selecting the number of layers, and the number of nodes in each layer, in a feed-forward neural network. It's more trial and error.
	\end{tcolorbox}
	
	Let us now see an easy companion example (originally developed by Joe Breedlove) as always in this book first done with a spreadsheet software like Microsoft Excel. Afterwards we will show the same output result with \texttt{R} and MATLAB™ for which you can find the detailed procedure in the corresponding companion books.
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	A company has measured during $14$ weeks its actual sales (Column: \textit{Value to predict}) in function of forecast sales of five of its branches (\textit{Variable1}, \textit{Variable2}, etc.) and has reproduced them in Microsoft Excel 14.0.6123:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/computing/neural_network_list_training_set_microsoft_excel.jpg}
		\caption[]{Training data list for our neural network in Microsoft Excel 14.0.6123}
	\end{figure}
	Notice that there is absolutely no formula in the list above! The return on experience (especially with Microsoft Excel...) tell us it would be better to do a network architecture with two neurons, first with branches $\{1,2,3\}$ based on a sigmoid and a second with branches $\{4.5\}$ also based on a sigmoid. In addition, all should have a single bias and the both neurons should have a specific weight in comparison with the one and the other.\\

	We then prepare the following table:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/computing/neural_network_initial_weight_bias_and_ponderations_microsoft_excel.jpg}
		\caption[]{Weight bias and weights to determined for our neural network}
	\end{figure}
	Once the table of weights, bias and ponderations built, we write our two neurons network  with the sigmoid function, for example, right next to the training sample data (which will facilitate the comparison):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/computing/neural_network_list_training_set_with_neural_network_microsoft_excel.jpg}
		\caption[]{List of sample training data with neural network cells}
	\end{figure}
	Or with the explicit formulas for the last three columns of interest:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/computing/neural_network_list_training_set_with_neural_network_formulas_microsoft_excel.jpg}
		\caption[]{List of sample training data with neural network cells}
	\end{figure}
	To apply operational research techniques, we need to minimize or maximize something. Therefore, we will seek to minimize the sum of squared errors by creating the following column:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/computing/neural_network_list_training_set_error_minimization_microsoft_excel.jpg}
		\caption[]{List of sample training data with neural network cells and quadratic error minimization}
	\end{figure}
	Or with the explicit formulas (we see well that this corresponds indeed to the square of the difference between the measurements and model):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/computing/neural_network_list_training_set_error_minimization_formulas_microsoft_excel.jpg}
		\caption[]{List of sample training data with neural network cells and quadratic error minimization}
	\end{figure}
	Now with the solver of Microsoft Excel 14.0.6123 we minimize the content of the cell \texttt{L36}:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/computing/neural_network_solver_excel.jpg}
		\caption[]{Neural network solver quadratic error minimization in Microsoft Excel 14.0.6123}
	\end{figure}
	we should not be too focused about the accuracy of the constraints for this case and therefore we have to play a little with this setting to get a satisfactory result:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.65]{img/computing/neural_network_solver_settings_excel.jpg}
		\caption[]{Neural network solver settings in Microsoft Excel 14.0.6123}
	\end{figure}
	To get a satisfactory result, it will be necessary in this case to request an accuracy of $0.001$. Which will give after the execution of the search by the solver a total square error of $0.8479$ (cell \texttt{L36}) and for the parameters of the neural network:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/computing/neural_network_solver_excel_solution.jpg}
		\caption[]{Neural network solver solution in Microsoft Excel 14.0.6123}
	\end{figure}
	Specialized software will do better with a total square error of $0.8405$ (still for cell \texttt{L36}) and for the parameters of the neural network:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/computing/neural_network_optimal_values_external_software.jpg}
		\caption[]{Neural network optimal values with specialized software}
	\end{figure}
	We can graphically compare the measurements used to train the neural network and the result of the neural network model itself. Then we have:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.9]{img/computing/neural_network_measurements_vs_model_plot_excel.jpg}
		\caption[]{Neural network model vs Measurements}
	\end{figure}
	Which seems not bad for a non-linear model! But once the model trained, we must always see if it applies to other data (test sample). Therefore let us consider:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/computing/neural_network_test_sample_excel.jpg}
		\caption[]{Neural network test data sample}
	\end{figure}
	Always with the same formulas:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/computing/neural_network_test_sample_formula_excel.jpg}
		\caption[]{Neural network test data sample neural network formula in Microsoft Excel 14.0.6123}
	\end{figure}
	And if we also graphically compare the real data and the modelled data, we get:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.9]{img/computing/neural_network_test_sample_plot_excel.jpg}
		\caption[]{Neural network test data sample neural network formula in Microsoft Excel 14.0.6123}
	\end{figure}
	and here we see that the model is significantly worse. But it is so! The predictive science is not an exact science but a heuristic...\\
	
	In the MATLAB™ companion book, working with the same data and building a similar neural network:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.9]{img/computing/neural_network_matlab.jpg}
		\caption{Neural network in MATLAB™ 2013a}
	\end{figure}
	we get the following model fitting plot:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.85]{img/computing/neural_network_measurements_vs_model_plot_matlab.jpg}
		\caption[]{Neural network model vs Measurements in MATLAB™ 2013a}
	\end{figure}
	and test sample plot (MATLAB™ performs better than Microsoft Excel solver in this special case):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.85]{img/computing/neural_network_test_sample_plot_matlab.jpg}
		\caption[]{Neural network model test sample plot in MATLAB™ 2013a}
	\end{figure}
	\end{tcolorbox}
	Now that we have some basic knowledge of perceptrons and vanilla neural networks we can introduce the mathematical definition of a "\NewTerm{neural network}\index{neural network}" as it should be crystal clear enough for the reader!
	
	\textbf{Definition (\#\thesection.\mydef):}\label{neural network definition} Let $d, L \in \mathbb{N}, N=\left(N_0, N_{\ell}, \cdots, N_L\right) \in \mathbb{N}^{L+1}$ and $\sigma: \mathbb{R} \rightarrow \mathbb{R}$. We say that $\sigma$ is an "activation function", $L$ is the number of layers, and $N_0, N_L, N_{\ell}, \ell \in[L-1]$ as number of neurons in the input, output, and $\ell$-th hidden layer, respectively. Let $\theta=\left(\theta^{(\ell)}\right)_{\ell}$ be the parameters such that:
	
	We denote the number of parameters by $P_N=\sum_{\ell=1}^L N_{\ell} N_{\ell-1}+N_{\ell}$. Define the corresponding realization function $\vec\Phi_\alpha: \mathbb{R}^{N_0} \times \mathbb{R}^{P_N} \rightarrow \mathbb{R}^{N_L}$ which satisfies, for any input $\vec x \in \mathbb{R}^{N_0}$ and parameters $\theta$, we set $\vec\Phi_\alpha=\vec\Phi^{(L)}$ where $\alpha=(N, \sigma)$, then:
	
	and $\sigma$ applied component-wise. As we already know, we refer $W$ as the "weight matrix" and $\vec b$ as the "bias vector".

	Note that $W^{(\ell)} \in \mathbb{R}^{N_{\ell}} \times \mathbb{R}^{N_{\ell-1}}$ are matrices, so they represent a linear transformation from $\mathbb{R}^{N_{\ell}}$ to $\mathbb{R}^{N_{\ell-1}}$. Therefore we may see the Neural Networks as successive composition of affine linear transformations, that is:
	
	Now we can also provide the formal definition of a set of set neural networks.
	
	\textbf{Definition (\#\thesection.\mydef):}
	 Let $\alpha=(N, \sigma)$ be a neural network with input $N_0=d$ and $N_L=1$, and activation function $\sigma$. The "set of Neural Networks: is defined by:
	 
	 There is a more simple way to write the two previous definitions, in terms of activation function and affine transformations. Since the realization of a neural network is given by recursively applying $\sigma$ we may write as:
	 
	where $\vec T_L$ is the corresponding affine transformation, i.e. $\vec T_{\ell}=W^{(\ell)} \circ \vec x+ \vec b^{(\ell)}$. Then previous definition of a set of neural network basically becomes the set of all functions $F(x)$ of the form which is described above, that is $F \in \mathcal{F}_{(N, \sigma)}$.
	 
	 \paragraph{Backpropagation}\label{backpropagation}\mbox{}\\\\
	 Gradient descent (see earlier above page \pageref{gradient descent}) is a good technique to minimize a cost function. But if we cannot analytically compute the function local derivatives, we will instead have to compute the slope  manually by evaluating the function around a neighbourhood. Translated to the example of  descending a mountain, it is like either walking with eyes wide open (analytical way), or  being blindfolded and having to walk a few steps in each direction, each time we want to  assess the slope around us (manual way). This manual option is very time consuming,  especially if you have many parameters—which is the case with neural networks. 

	Hopefully, a technique was found to analytically compute the local derivatives in a neural  network: back-propagation. The idea is that a neural network can be used in two directions: 
	\begin{enumerate}
		\item Forward propagation where the neurons propagate information through the network by applying their activation functions to their weighted inputs and biases. 
		
		\item Backpropagation where the neurons propagate their local derivatives (that is the impact of changing weights, input values, and biases). In other words, each neuron will propagate how much a change in one of its inputs (weights, input values, bias) will impact its own output.
	\end{enumerate}
	During the training phase of a neural network, forward propagation and back-propagation will be performed in turns. Forward propagation will be used to generate predictions (as data flow through the network). Backpropagation will be used to tune the weights based on the latest errors. 

	 Therefore in the field of neural networks "\NewTerm{back-propagation}\index{back-propagation}" is a method to calculate the gradient of the loss function with respect to the weights in an artificial neural network. It is commonly used as a part of algorithms that optimize the performance of the network by adjusting the weights, for example in the gradient descent algorithm (see page \pageref{gradient descent}). It is also named "\NewTerm{backward propagation of errors}\index{backward propagation of errors}". It seems that there is no shortage of papers online that attempt to explain how back-propagation works, but few that include an example with actual numbers. The text and example below is an attempt of Matt Mazur\footnote{Reproduced from \url{https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/} with its authorization} to explain how it works with a concrete example that everybody can compare their own calculations to in order to ensure they understand backpropagation correctly.

	To explain backpropagation, we're going to use a neural network with two inputs, two hidden neurons, two output neurons. Additionally, the hidden and output neurons will include a bias.

	Here's the basic structure:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,737); %set diagram left start at 0, and has height of 737
		
		%Shape: Circle [id:dp5536989191575898] 
		\draw  [fill={rgb, 255:red, 184; green, 233; blue, 134 }  ,fill opacity=1 ][line width=1.5]  (111,73) .. controls (111,59.19) and (122.19,48) .. (136,48) .. controls (149.81,48) and (161,59.19) .. (161,73) .. controls (161,86.81) and (149.81,98) .. (136,98) .. controls (122.19,98) and (111,86.81) .. (111,73) -- cycle ;
		%Shape: Circle [id:dp33966243986992595] 
		\draw  [fill={rgb, 255:red, 145; green, 195; blue, 255 }  ,fill opacity=1 ][line width=1.5]  (283,73) .. controls (283,59.19) and (294.19,48) .. (308,48) .. controls (321.81,48) and (333,59.19) .. (333,73) .. controls (333,86.81) and (321.81,98) .. (308,98) .. controls (294.19,98) and (283,86.81) .. (283,73) -- cycle ;
		%Shape: Circle [id:dp5621409189413658] 
		\draw  [fill={rgb, 255:red, 255; green, 223; blue, 173 }  ,fill opacity=1 ][line width=1.5]  (455,73) .. controls (455,59.19) and (466.19,48) .. (480,48) .. controls (493.81,48) and (505,59.19) .. (505,73) .. controls (505,86.81) and (493.81,98) .. (480,98) .. controls (466.19,98) and (455,86.81) .. (455,73) -- cycle ;
		%Shape: Circle [id:dp8881254700324481] 
		\draw  [fill={rgb, 255:red, 184; green, 233; blue, 134 }  ,fill opacity=1 ][line width=1.5]  (111,203) .. controls (111,189.19) and (122.19,178) .. (136,178) .. controls (149.81,178) and (161,189.19) .. (161,203) .. controls (161,216.81) and (149.81,228) .. (136,228) .. controls (122.19,228) and (111,216.81) .. (111,203) -- cycle ;
		%Shape: Circle [id:dp6030296594200577] 
		\draw  [fill={rgb, 255:red, 145; green, 195; blue, 255 }  ,fill opacity=1 ][line width=1.5]  (283,203) .. controls (283,189.19) and (294.19,178) .. (308,178) .. controls (321.81,178) and (333,189.19) .. (333,203) .. controls (333,216.81) and (321.81,228) .. (308,228) .. controls (294.19,228) and (283,216.81) .. (283,203) -- cycle ;
		%Shape: Circle [id:dp41525710534620397] 
		\draw  [fill={rgb, 255:red, 255; green, 223; blue, 173 }  ,fill opacity=1 ][line width=1.5]  (455,203) .. controls (455,189.19) and (466.19,178) .. (480,178) .. controls (493.81,178) and (505,189.19) .. (505,203) .. controls (505,216.81) and (493.81,228) .. (480,228) .. controls (466.19,228) and (455,216.81) .. (455,203) -- cycle ;
		%Shape: Circle [id:dp9145992720023319] 
		\draw  [fill={rgb, 255:red, 233; green, 233; blue, 233 }  ,fill opacity=1 ][line width=1.5]  (173,304) .. controls (173,290.19) and (184.19,279) .. (198,279) .. controls (211.81,279) and (223,290.19) .. (223,304) .. controls (223,317.81) and (211.81,329) .. (198,329) .. controls (184.19,329) and (173,317.81) .. (173,304) -- cycle ;
		%Shape: Circle [id:dp931370841851727] 
		\draw  [fill={rgb, 255:red, 233; green, 233; blue, 233 }  ,fill opacity=1 ][line width=1.5]  (354,304) .. controls (354,290.19) and (365.19,279) .. (379,279) .. controls (392.81,279) and (404,290.19) .. (404,304) .. controls (404,317.81) and (392.81,329) .. (379,329) .. controls (365.19,329) and (354,317.81) .. (354,304) -- cycle ;
		%Straight Lines [id:da8036703376910099] 
		\draw    (70.5,73) -- (108,73) ;
		\draw [shift={(111,73)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da12341803119447192] 
		\draw    (70.5,203) -- (108,203) ;
		\draw [shift={(111,203)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da17331129487172037] 
		\draw    (161,73) -- (280,73) ;
		\draw [shift={(283,73)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da11562884496500714] 
		\draw    (333,73) -- (452,73) ;
		\draw [shift={(455,73)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da3721693694656818] 
		\draw    (161,203) -- (280,203) ;
		\draw [shift={(283,203)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da3857692800052075] 
		\draw    (333,203) -- (452,203) ;
		\draw [shift={(455,203)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da3190322715481002] 
		\draw    (159,84) -- (289.11,183.18) ;
		\draw [shift={(291.5,185)}, rotate = 217.32] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da5283949818015485] 
		\draw    (155.5,187) -- (285.8,87.32) ;
		\draw [shift={(288.19,85.5)}, rotate = 142.59] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da5271179063584648] 
		\draw    (330,84) -- (460.11,183.18) ;
		\draw [shift={(462.5,185)}, rotate = 217.32] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da6529144988245621] 
		\draw    (327.5,187) -- (456.39,85.85) ;
		\draw [shift={(458.75,84)}, rotate = 141.88] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da2882751268486945] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (198,279) -- (306.44,100.56) ;
		\draw [shift={(308,98)}, rotate = 121.29] [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da8474353748665209] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (223,304) -- (305.76,230) ;
		\draw [shift={(308,228)}, rotate = 138.2] [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da7930249013471451] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (404,304) -- (477.88,230.12) ;
		\draw [shift={(480,228)}, rotate = 135] [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da5317897411635704] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (379,279) -- (478.54,100.62) ;
		\draw [shift={(480,98)}, rotate = 119.16] [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da9551646385505397] 
		\draw    (505,203) -- (542.5,203) ;
		\draw [shift={(545.5,203)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da752122766370239] 
		\draw    (505,73) -- (542.5,73) ;
		\draw [shift={(545.5,73)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		
		% Text Node
		\draw (196.5,55) node [anchor=north west][inner sep=0.75pt]    {$w_{1}$};
		% Text Node
		\draw (163.5,101) node [anchor=north west][inner sep=0.75pt]    {$w_{3}$};
		% Text Node
		\draw (163.5,149) node [anchor=north west][inner sep=0.75pt]    {$w_{2}$};
		% Text Node
		\draw (196.5,210) node [anchor=north west][inner sep=0.75pt]    {$w_{4}$};
		% Text Node
		\draw (190,240.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,opacity=1 ]  {$b_{1}$};
		% Text Node
		\draw (253,279.9) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,opacity=1 ]  {$b_{1}$};
		% Text Node
		\draw (371,240.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,opacity=1 ]  {$b_{2}$};
		% Text Node
		\draw (429,279.9) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,opacity=1 ]  {$b_{2}$};
		% Text Node
		\draw (333.5,101) node [anchor=north west][inner sep=0.75pt]    {$w_{7}$};
		% Text Node
		\draw (333.5,149) node [anchor=north west][inner sep=0.75pt]    {$w_{6}$};
		% Text Node
		\draw (361.5,55) node [anchor=north west][inner sep=0.75pt]    {$w_{5}$};
		% Text Node
		\draw (372.5,209) node [anchor=north west][inner sep=0.75pt]    {$w_{8}$};
		% Text Node
		\draw (129,65) node [anchor=north west][inner sep=0.75pt]    {$i_{1}$};
		% Text Node
		\draw (129,195) node [anchor=north west][inner sep=0.75pt]    {$i_{2}$};
		% Text Node
		\draw (299,61.4) node [anchor=north west][inner sep=0.75pt]    {$h_{1}$};
		% Text Node
		\draw (299,192.4) node [anchor=north west][inner sep=0.75pt]    {$h_{2}$};
		% Text Node
		\draw (472,65) node [anchor=north west][inner sep=0.75pt]    {$o_{1}$};
		% Text Node
		\draw (472,197) node [anchor=north west][inner sep=0.75pt]    {$o_{2}$};
		% Text Node
		\draw (189,294.4) node [anchor=north west][inner sep=0.75pt]    {$b_{1}$};
		% Text Node
		\draw (372,294.4) node [anchor=north west][inner sep=0.75pt]    {$b_{2}$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[]{Basis neural network for backpropagation study}
	\end{figure}
	In order to have some numbers to work with, here are the initial weights (in red), the biases (in orange), and training inputs/outputs (in blue):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,737); %set diagram left start at 0, and has height of 737
		
		%Shape: Circle [id:dp5536989191575898] 
		\draw  [fill={rgb, 255:red, 184; green, 233; blue, 134 }  ,fill opacity=1 ][line width=1.5]  (111,73) .. controls (111,59.19) and (122.19,48) .. (136,48) .. controls (149.81,48) and (161,59.19) .. (161,73) .. controls (161,86.81) and (149.81,98) .. (136,98) .. controls (122.19,98) and (111,86.81) .. (111,73) -- cycle ;
		%Shape: Circle [id:dp33966243986992595] 
		\draw  [fill={rgb, 255:red, 145; green, 195; blue, 255 }  ,fill opacity=1 ][line width=1.5]  (283,73) .. controls (283,59.19) and (294.19,48) .. (308,48) .. controls (321.81,48) and (333,59.19) .. (333,73) .. controls (333,86.81) and (321.81,98) .. (308,98) .. controls (294.19,98) and (283,86.81) .. (283,73) -- cycle ;
		%Shape: Circle [id:dp5621409189413658] 
		\draw  [fill={rgb, 255:red, 255; green, 223; blue, 173 }  ,fill opacity=1 ][line width=1.5]  (455,73) .. controls (455,59.19) and (466.19,48) .. (480,48) .. controls (493.81,48) and (505,59.19) .. (505,73) .. controls (505,86.81) and (493.81,98) .. (480,98) .. controls (466.19,98) and (455,86.81) .. (455,73) -- cycle ;
		%Shape: Circle [id:dp8881254700324481] 
		\draw  [fill={rgb, 255:red, 184; green, 233; blue, 134 }  ,fill opacity=1 ][line width=1.5]  (111,203) .. controls (111,189.19) and (122.19,178) .. (136,178) .. controls (149.81,178) and (161,189.19) .. (161,203) .. controls (161,216.81) and (149.81,228) .. (136,228) .. controls (122.19,228) and (111,216.81) .. (111,203) -- cycle ;
		%Shape: Circle [id:dp6030296594200577] 
		\draw  [fill={rgb, 255:red, 145; green, 195; blue, 255 }  ,fill opacity=1 ][line width=1.5]  (283,203) .. controls (283,189.19) and (294.19,178) .. (308,178) .. controls (321.81,178) and (333,189.19) .. (333,203) .. controls (333,216.81) and (321.81,228) .. (308,228) .. controls (294.19,228) and (283,216.81) .. (283,203) -- cycle ;
		%Shape: Circle [id:dp41525710534620397] 
		\draw  [fill={rgb, 255:red, 255; green, 223; blue, 173 }  ,fill opacity=1 ][line width=1.5]  (455,203) .. controls (455,189.19) and (466.19,178) .. (480,178) .. controls (493.81,178) and (505,189.19) .. (505,203) .. controls (505,216.81) and (493.81,228) .. (480,228) .. controls (466.19,228) and (455,216.81) .. (455,203) -- cycle ;
		%Shape: Circle [id:dp9145992720023319] 
		\draw  [fill={rgb, 255:red, 233; green, 233; blue, 233 }  ,fill opacity=1 ][line width=1.5]  (173,304) .. controls (173,290.19) and (184.19,279) .. (198,279) .. controls (211.81,279) and (223,290.19) .. (223,304) .. controls (223,317.81) and (211.81,329) .. (198,329) .. controls (184.19,329) and (173,317.81) .. (173,304) -- cycle ;
		%Shape: Circle [id:dp931370841851727] 
		\draw  [fill={rgb, 255:red, 233; green, 233; blue, 233 }  ,fill opacity=1 ][line width=1.5]  (354,304) .. controls (354,290.19) and (365.19,279) .. (379,279) .. controls (392.81,279) and (404,290.19) .. (404,304) .. controls (404,317.81) and (392.81,329) .. (379,329) .. controls (365.19,329) and (354,317.81) .. (354,304) -- cycle ;
		%Straight Lines [id:da8036703376910099] 
		\draw    (70.5,73) -- (108,73) ;
		\draw [shift={(111,73)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da12341803119447192] 
		\draw    (70.5,203) -- (108,203) ;
		\draw [shift={(111,203)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da17331129487172037] 
		\draw    (161,73) -- (280,73) ;
		\draw [shift={(283,73)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da11562884496500714] 
		\draw    (333,73) -- (452,73) ;
		\draw [shift={(455,73)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da3721693694656818] 
		\draw    (161,203) -- (280,203) ;
		\draw [shift={(283,203)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da3857692800052075] 
		\draw    (333,203) -- (452,203) ;
		\draw [shift={(455,203)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da3190322715481002] 
		\draw    (159,84) -- (289.11,183.18) ;
		\draw [shift={(291.5,185)}, rotate = 217.32] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da5283949818015485] 
		\draw    (155.5,187) -- (285.8,87.32) ;
		\draw [shift={(288.19,85.5)}, rotate = 142.59] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da5271179063584648] 
		\draw    (330,84) -- (460.11,183.18) ;
		\draw [shift={(462.5,185)}, rotate = 217.32] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da6529144988245621] 
		\draw    (327.5,187) -- (456.39,85.85) ;
		\draw [shift={(458.75,84)}, rotate = 141.88] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da2882751268486945] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (198,279) -- (306.44,100.56) ;
		\draw [shift={(308,98)}, rotate = 121.29] [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da8474353748665209] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (223,304) -- (305.76,230) ;
		\draw [shift={(308,228)}, rotate = 138.2] [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da7930249013471451] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (404,304) -- (477.88,230.12) ;
		\draw [shift={(480,228)}, rotate = 135] [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da5317897411635704] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (379,279) -- (478.54,100.62) ;
		\draw [shift={(480,98)}, rotate = 119.16] [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da9551646385505397] 
		\draw    (505,203) -- (542.5,203) ;
		\draw [shift={(545.5,203)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da752122766370239] 
		\draw    (505,73) -- (542.5,73) ;
		\draw [shift={(545.5,73)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		
		% Text Node
		\draw (196.5,55) node [anchor=north west][inner sep=0.75pt]    {$w_{1}$};
		% Text Node
		\draw (163.5,101) node [anchor=north west][inner sep=0.75pt]    {$w_{3}$};
		% Text Node
		\draw (163.5,149) node [anchor=north west][inner sep=0.75pt]    {$w_{2}$};
		% Text Node
		\draw (196.5,210) node [anchor=north west][inner sep=0.75pt]    {$w_{4}$};
		% Text Node
		\draw (190,240.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,opacity=1 ]  {$b_{1}$};
		% Text Node
		\draw (253,279.9) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,opacity=1 ]  {$b_{1}$};
		% Text Node
		\draw (371,240.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,opacity=1 ]  {$b_{2}$};
		% Text Node
		\draw (429,279.9) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,opacity=1 ]  {$b_{2}$};
		% Text Node
		\draw (333.5,101) node [anchor=north west][inner sep=0.75pt]    {$w_{7}$};
		% Text Node
		\draw (333.5,149) node [anchor=north west][inner sep=0.75pt]    {$w_{6}$};
		% Text Node
		\draw (361.5,55) node [anchor=north west][inner sep=0.75pt]    {$w_{5}$};
		% Text Node
		\draw (372.5,209) node [anchor=north west][inner sep=0.75pt]    {$w_{8}$};
		% Text Node
		\draw (129,65) node [anchor=north west][inner sep=0.75pt]    {$i_{1}$};
		% Text Node
		\draw (129,195) node [anchor=north west][inner sep=0.75pt]    {$i_{2}$};
		% Text Node
		\draw (299,61.4) node [anchor=north west][inner sep=0.75pt]    {$h_{1}$};
		% Text Node
		\draw (299,192.4) node [anchor=north west][inner sep=0.75pt]    {$h_{2}$};
		% Text Node
		\draw (472,65) node [anchor=north west][inner sep=0.75pt]    {$o_{1}$};
		% Text Node
		\draw (472,197) node [anchor=north west][inner sep=0.75pt]    {$o_{2}$};
		% Text Node
		\draw (189,294.4) node [anchor=north west][inner sep=0.75pt]    {$b_{1}$};
		% Text Node
		\draw (372,294.4) node [anchor=north west][inner sep=0.75pt]    {$b_{2}$};
		
		% Text Node
		\draw (120,104.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,opacity=1 ]  {$0.05$};
		% Text Node
		\draw (120,233.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,opacity=1 ]  {$0.10$};
		% Text Node
		\draw (165,259.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,opacity=1 ]  {$0.35$};
		% Text Node
		\draw (240,301.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,opacity=1 ]  {$0.35$};
		% Text Node
		\draw (348,259.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,opacity=1 ]  {$0.60$};
		% Text Node
		\draw (417,301.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,opacity=1 ]  {$0.60$};
		% Text Node
		\draw (242.25,53.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ]  {$0.15$};
		% Text Node
		\draw (182.5,85.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ]  {$0.25$};
		% Text Node
		\draw (182.5,168.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ]  {$0.20$};
		% Text Node
		\draw (242.25,207.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ]  {$0.30$};
		% Text Node
		\draw (355.5,85.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ]  {$0.50$};
		% Text Node
		\draw (355.5,168.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ]  {$0.45$};
		% Text Node
		\draw (412.25,53.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ]  {$0.40$};
		% Text Node
		\draw (482,101.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,opacity=1 ]  {$0.01$};
		% Text Node
		\draw (482,231.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,opacity=1 ]  {$0.99$};
		% Text Node
		\draw (423.25,209) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ]  {$0.55$};
		
		\end{tikzpicture}
	\end{figure}
	The goal of backpropagation is to optimize the weights so that the neural network can learn how to correctly map arbitrary inputs to outputs.

	For the rest of this tutorial we're going to work with a single training set: given inputs $0.05$ and $0.10$ and $b_1=1$ and $b_2=2$, we want the neural network to output $0.01$ and $0.99$.
	
	Let us decompose the method:
	\begin{enumerate}
		\item The Forward Pass\index{forward pass}:
		
		To begin, lets see what the neural network currently predicts given the weights and biases above and inputs of $0.05$ and $0.10$. To do this we will feed those inputs forward though the network.

		We figure out the total net input\footnote{The "Total net input" is also referred to as just "net input" by some sources.} to each hidden layer neuron, squash the total net input using an activation function (here we use the logistic function), then repeat the process with the output layer neurons.
		
		Here is how we calculate the total net input for $h_1$:
		
		We then squash it using (arbitrarily) using the logistic function to get the output of $h_1$:
		
		Carrying out the same process for $h_2$ we get:
		
		We repeat this process for the output layer neurons, using the output from the hidden layer neurons as inputs.
		
		Here's the output for $o_1$:
		
		Hence:
		
		And carrying out the same process for $o_2$ we get:
		
		We can now calculate the error for each output neuron using the squared error function (a loss function) and sum them to get the total error (using the traditional notations in the field...), ie a cost function:
		
		Or if written in another common way:
		
		Since backpropagation uses the gradient descent method\index{gradient descent method} (see page \pageref{gradient descent}), the factor of $\frac{1}{2}$ is included to cancel the exponent when differentiating. Later, the expression will be multiplied with an arbitrary learning rate, so that it doesn't matter if a constant coefficient is introduced now.
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		Some textbooks refer to the "target" as the "ideal" and the "output" as the "actual".
		\end{tcolorbox}
		For example, the target output for $o_1$ is $0.01$ but the neural network output $0.75136507$, therefore its error is:
		
		Repeating this process for $o_2$ (remembering that the target is $0.99$) we get:
		
		The total error for the neural network is the sum of these errors:
		
		
		\item The Backwards Pass:
		Our goal with backpropagation is to update each of the weights in the network so that they cause the actual output to be closer the target output, thereby minimizing the error for each output neuron and the network as a whole.
		
		Consider $w_5$. We want to know how much a change in $w_5$ affects the total error, aka $\frac{\partial E_{\text{tot}}}{\partial w_{5}}$.
		
		By applying the chain rule twice (\SeeChapter{see section Differential and Integral Calculus page \pageref{multivariate chain rule}}) we know that:
		
		We need to figure out each piece in this equation.
		\begin{itemize}
			\item First, how much does the total error change with respect to the output?
			
			As:
			
			Therefore:
			
			
			\item Next, how much does the output of $o_1$ change with respect to its total net input?
			As:
			
			Therefore:
			
			
			\item Finally, how much does the total net input of $o_1$ change with respect to $w_5$?
			
			As:
			
			Therefore:
			
		\end{itemize}
		Putting it all together:
		
		To decrease the error, we then subtract this value from the current weight, by applying the relation derived earlier above (see page \pageref{gradient descent}):
				
		 optionally multiplied by some learning rate, $\eta$, which we will set to $0.5$:
		
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		Some textbooks use the notation $\alpha$ to represent the learning rate, others use $\eta$, and others even use $\varepsilon$...
		\end{tcolorbox}
		We can repeat this process to get the new weights $w_6$, $w_7$, and $w_8$:
		
		We perform the actual updates in the neural network after we have the new weights leading into the hidden layer neurons (ie, we use the original weights, not the updated weights, when we continue the backpropagation algorithm below)!
		
		Next, we will continue the backwards pass by calculating new values for $w_1$, $w_2$, $w_3$, and $w_4$ of the hidden layer.

		Here is what we need to figure out:
		
		We are going to use a similar process as we did for the output layer, but slightly different to account for the fact that the output of each hidden layer neuron contributes to the output (and therefore error) of multiple output neurons.
		
		We know that $\text{out}_{h_1}$ affects both $\text{out}_{o_1}$ and $\text{out}_{o_2}$ therefore the $\frac{\partial E_{\text{tot}}}{\partial \text{out}_{h_1}}$ needs to take into consideration its effect on the both output neurons:
		
		Starting with $\frac{\partial E_{o_1}}{\partial \text{out}_{h_1}}$:
		
		We can calculate $\frac{\partial E_{o_1}}{\partial \text{net}_{o_1}}$ using values we calculated earlier:
		
		And $\frac{\partial \text{net}_{o_1}}{\partial \text{out}_{h_1}}$ is equal to $w_5$. Indeed:
		
		Therefore:
		
		Plugging them in:
		
		Following the same process for $\frac{\partial E_{o_2}}{\partial \text{out}_{h_1}}$, we get:
		
		Therefore:
		
		Now that we have $\frac{\partial E_{\text{tot}}}{\partial \text{out}_{h1}}$, we need to figure out $\frac{\partial \text{out}_{h_1}}{\partial \text{net}_{h1}}$ and then $\frac{\partial \text{net}_{h_1}}{\partial w_1}$.

		As:
		
		Therefore identically as earlier above:
		
		And as:
		
		Therefore:
		
		Putting it all together (and writing all the steps explicitly):
		
		We can now update $w_1$:
		
		Repeating this for $w_2$, $w_3$, and $w_4$:
		
	\end{enumerate}
	Finally, we have updated all of our weights! When we fed forward the $0.05$ and $0.1$ inputs originally, the error on the network was $0.298371109$. After this first round of backpropagation, the total error is now down to $0.291027924$. It might not seem like much, but after repeating this process $10,000$ times, for example, the error plummets to $0.000035085$. At this point, when we feed forward $0.05$ and $0.1$, the two outputs neurons generate $0.015912196$ (vs $0.01$ target) and $0.984065734$ (vs $0.99$ target).
	
	Therefore we have seen that this optimization algorithm repeats a two phase cycle, propagation and weight update. When an input vector is presented to the network, it is propagated forward through the network, layer by layer, until it reaches the output layer. The output of the network is then compared to the desired output, using a loss function, and an error value is calculated for each of the neurons in the output layer. The error values are then propagated backwards, starting from the output, until each neuron has an associated error value which roughly represents its contribution to the original output. Backpropagation uses these error values to calculate the gradient of the loss function. In the second phase, this gradient is fed to the optimization method, which in turn uses it to update the weights, in an attempt to minimize the loss function.
	
	We have also seen that the backpropagation requires that the activation function used by the artificial neurons (or "nodes") to be differentiable.
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} Backpropagation requires a known, desired output for each input value in order to calculate the loss function gradient – it is therefore usually considered to be a supervised learning method.\\
	
	\textbf{R2.} The choice of learning rate $\eta$ is important for the method, since a high value can cause too strong a change, causing the minimum to be missed, while a too low learning rate slows the training unnecessarily. In order to avoid oscillation inside the network, such as alternating connection weights, and to improve the rate of convergence, there are refinements of this algorithm that use an adaptive learning rate.\\
	
	\textbf{R3.} Backpropagation learning does not require normalization of input vectors; however, normalization could improve performance.\\
	
	\textbf{R4.} Over-fitting is a major-concern in deep learning (and to humans also...) since large networks can have hundreds of millions of weights. In image recognition, the number of training images can be significantly increased by random jittering of the images. Another technique named \NewTerm{dropout} randomly deletes a fraction of the weights at each training iteration. Regularization (see page \pageref{regularization}) is used to assign a cost to the size of weights and many other ideas are being explored.
	\end{tcolorbox}
	For information... when the net is passed to the activation (transfer) function and the function's output is used for adjusting the weights we then speak of "\NewTerm{McCulloch–Pitts perceptron}\index{McCulloch–Pitts perceptron}" or just simply of a "\NewTerm{(feed-forward) perceptron}\index{feed-forward perceptron}". If the weights are adjusted only according to the weighted sum of the inputs (that is without passing through the activation function!), we then speak of "\NewTerm{adaptive linear neuron}\index{Adaptive Linear Neuron}\index{ADALINE}" (ADALINE) or of "\NewTerm{feed-forward adaptive linear neuron}\footnote{A multilayer network of ADALINE units is known as a MADALINE.}".
	
	Finally, don't forget (as we have seen earlier) that gradient descent with backpropagation is not guaranteed to find the global minimum of the error function, but only a local minimum!
	
	As the number of layers grew, we encounter mainly two calculatory difficulties (aside computer memory issues): "\NewTerm{exploding gradient}" and "\NewTerm{vanishing gradient}".

While the problem of exploding gradient can be treated quite well by applying simple techniques like gradient clipping and $L_1$ or $L_2$ regularization (see earlier above page \pageref{LASSO regularization}), the problem of vanishing gradient remained intractable for decades.

	What is vanishing gradient and why does it arise? To update the values of the parameters in neural networks, we have seen that the back-propagation algorithm was typically used and is based on the chain rule to calculated partial derivatives of some more or less complex functions. During gradient descent, the neural network's parameters receive an update proportional to the derivative of the cost function with respect to the current parameter in each iteration of training. The problem is that in some cases, the gradient will be vanishingly small, effectively preventing some parameters from chaining their value. In the worst case, this may completely stop the neural network from further training.

	Traditional activation functions, such as the hyperbolic tangent function i mentioned above, have gradients in the range $[0,1]$, and back-propagation computes gradients by the chain rule. That has the effect of multiplying $n$ of these small numbers to compute gradients of the earlier (leftmost) layers in an $n$-layer network, meaning that the gradient decreases exponentially with $n$. That results in the effect that the earlier layers train very slowly, if at all.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	At this point the reader may have noticed that technically, logistic regression is equivalent to a single layer neural network with a sigmoid transfer function (and vice-versa).
	\end{tcolorbox}	
	
	\pagebreak
	\paragraph{Recursive Neural Networks (RNN)}\label{Recursive neural networks}\index{recursive neural networks}\mbox{}\\\\
	Sometimes we want to remember an input for later use. There are many examples of such a situation, such as the stock market. To make a good investment judgement, we have to at least look at the stock data from a time window.

	The naive way to let neural network accept a time series data is connecting several neural networks together. Each of the neural networks handles one time step. Instead of feeding the data at each individual time step, you provide data at all time steps within a window, or a context, to the neural network.
	
	A "\NewTerm{recurrent neural network}" (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behaviour. Derived from feed-forward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs.
	
	There are three common type of recursive neural network\footnote{Huge thanks to Shi Yan, senior software engineer at NVIDIA, for having authorized us to use and slightly modify his LSTM illustrations!} (recurrent neural network, long short term memory unit and gated recurrent unit):
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/rnn_lstm_gru.jpg}
		\caption{Three common type of recursive neural networks}
	\end{figure}
	As we will provide further below the equations of LSTM (Long-Short Term Memory) and GRU (Gated Recurrent Unit), the reader will be able to notice that GRU (introduced in year 12014 according to holocene calendar) are a special case of LSTM (introduced in year 11997 according to holocene calendar)!
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The GRU is an alternative to the LSTM which is similarly difficult to justify. It seems that comparison of the GRU to the LSTM and its variants provides evidence that the GRU outperforms the LSTM on nearly all tasks except language modelling.
	\end{tcolorbox}

	A lot of times, you need to process data that has \underline{periodic patterns} (LSTM at the day we write these lines seems to be efficient only for such patterns!). As a silly example, suppose you want to predict Christmas tree sales. This is a very seasonal thing and likely to peak only once a year. So a good strategy to predict Christmas tree sale is looking at the data from exactly a year back. For this kind of problems, you either need to have a big context to include ancient data points, or you have a good memory. You know what data is valuable to remember for later use and what needs to be forgotten when it is useless.

	Theoretically the naively connected neural network, so named "recurrent neural network":
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/vanilla_rnn.jpg}
		\caption{Recurrent neural network}
	\end{figure}
	
	can work. But in practice, it suffers from two problems: vanishing gradient and exploding gradient, which make it unusable.

	Then later,"\NewTerm{Long-Short Term memory Neural Networks}\index{long-short term memory neural network}\label{long-short term memory neural network}" (LSTM) were introduced by Hochreiter \& Schmidhuber (11997 according to holocene calendar) \cite{doi:10.1162/neco.1997.9.8.1735} to solve this issue by explicitly introducing a memory unit, named the "cell" into the network. This is the diagram of a LSTM building block:
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/lstm_building_block.jpg}
		\caption{LSTM (long short term memory) building block}
	\end{figure} 
	Obviously in the general case most variables are vectors. But in most typical univariate forecasting techniques, most vectors (excepted $\vec{X}_t$) are one-dimensional vectors: ie scalars!
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	LSTM networks are a type of RNN that uses special units in addition to standard units. Again (!), standard RNNs (Recurrent Neural Networks) suffer from vanishing and exploding gradient problems. LSTMs (Long Short Term Memory) deal with these problems by introducing new gates, such as input and forget gates, which allow for a better control over the gradient flow and enable better preservation of long-range dependencies.
	\end{tcolorbox}
	At a first sight, this looks intimidating. Let's ignore the internals, but only look at the inputs and outputs of the unit. The network takes three inputs. $X_t$ is the input of the current time step. $h_{t-1}$ is the output from the previous LSTM unit and $C_{t-1}$ is the "memory" of the previous unit, which may be seen as the most important input. As for outputs, $h_t$ is the output of the current network. $C_t$ is the memory of the current unit.
	
	Therefore, this single unit makes decision by considering the current input, previous output and previous memory. And it generates a new output and alters its memory.
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/lstm_overall.jpg}
		\caption{Typical LSTM overall structure}
	\end{figure}
	The way its internal memory $C_t$ changes is pretty similar to piping water through a pipe. Assuming the memory is water, it flows into a pipe. You want to change this memory flow along the way and this change is controlled by two valves:
	\begin{enumerate}
		\item The first valve is named the "\NewTerm{forget valve}". If you shut it, no old memory will be kept. If you fully open this valve, all old memory will pass through.
		\begin{figure}[H]
			\centering
			\includegraphics[scale=0.5]{img/computing/forget_valve.jpg}
		\end{figure}
	
		\item The second valve is the new "\NewTerm{memory valve}". New memory will come in through a T shaped joint like below and merge with the old memory. Exactly how much new memory should come in is controlled by the second valve.
		\begin{figure}[H]
			\centering
			\includegraphics[scale=0.5]{img/computing/memory_valve.jpg}
		\end{figure}
	\end{enumerate}
	On the LSTM diagram below the red area is the memory pipe (do not confuse it with the  "memory valve"!). The input is the old memory (a vector). The first cross $\times$ it passes through is the forget valve. It is actually an element-wise multiplication operation (i.e. "Hadamard product"). So if you multiply the old memory $C_{t-1}$ with a vector that is close to $0$, that means you want to forget most of the old memory. You let the old memory goes through, if you forget valve equals $1$.
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/lstm_memory_pipe.jpg}
		\caption[]{Memory pipe with memory valve and forget valve of a LSTM}
	\end{figure}
	Then the second operation the memory flow will go through is this $+$ operator. This operator means piece-wise summation. It resembles the T shape joint pipe. New memory and the old memory will merge by this operation.
	
	After these two operations, you have the old memory $C_{t-1}$ changed to the new memory $C_t$.
	
	Now let us look at the input of both valves more in details.

	We begin with the "forget valve"! It is controlled by a simple feed-forward one layer neural network. The inputs of the neural network is $h_{t-1}$, the output of the previous LSTM block, $X_t$, the input for the current LSTM block, $C_{t-1}$, the memory of the previous block and finally a bias vector $b_0$. This neural network has a sigmoid function as activation, and it's output vector is the forget valve, which will applied to the old memory $C_{t-1}$ by element-wise multiplication.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/lstm_memory_pipe_forget_valve.jpg}
		\caption[]{Details of the forget valve on the memory pipe}
	\end{figure}
	Now the second valve named "memory valve"!	Again, it is a one layer simple neural network that takes the same inputs as the forget valve. This valve controls how much the new memory should influence the old memory:
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/lstm_memory_pipe_below_memory_valve.jpg}
		\caption[]{Details of the memory valve below the memory pipe}
	\end{figure}
	The output of this network will element-wise multiple the new memory valve, and add to the old memory to form the new memory:
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/lstm_add_new_a_old_memory.jpg}
	\end{figure}
	And finally, we need to generate the output for this LSTM unit. This step has an output valve that is controlled by the new memory, the previous output $h_{t-1}$, the input $X_t$ and a bias vector. This valve controls how much new memory should output to the next LSTM unit:
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/lstm_output.jpg}
	\end{figure}
	Let us see now the mathematical point of view. For this consider again the LSTM block but with more explicit notations (for recall, $\diamond$ is the vector element-wise multiplication):
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/lstm_explicit.jpg}
	\end{figure}
	And here are the corresponding relations:
	\begin{itemize}
		\item Gating variables (respectively $\vec{f}_t$ forget gate, $\vec{i}_t$ update gate, $\vec{o}_t$ output gate):
		
		
		\item Candidate (memory) cell state:
		
		
		\item Cell and hidden state:
		
	\end{itemize}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	For a "\NewTerm{Gated Recurrent Unit Neural Network}\index{gated recurrent unit neural network}\label{gated recurrent unit neural network}" (GRU), only two relations change:
	
	hence the fact that GRU is a special case of LSTM!
	\end{tcolorbox}
	
	The core reason that recurrent nets are more exciting is that they allow us to operate over sequences of vectors: Sequences in the input, the output, or in the most general case both. A few examples may make this more concrete:
	 \begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/rnn_outputs_inputs.jpg}
		\caption[]{Each rectangle is a vector and arrows represent functions (e.g. matrix multiply). Input vectors are in red, output vectors are in blue and green vectors hold the RNN's state (more on this soon). From left to right: (1) Vanilla mode of processing without RNN, from fixed-sized input to fixed-sized output (e.g. image classification). (2) Sequence output (e.g. image captioning takes an image and outputs a sentence of words). (3) Sequence input (e.g. sentiment analysis where a given sentence is classified as expressing positive or negative sentiment). (4) Sequence input and sequence output (e.g. Machine Translation: a RNN reads a sentence in English and then outputs a sentence in French). (5) Synced sequence input and output (e.g. video classification where we wish to label each frame of the video). Notice that in every case are no pre-specified constraints on the lengths sequences because the recurrent transformation (green) is fixed and can be applied as many times as we like.}
	\end{figure}
	RNNs are often used in text processing because sentences and texts are naturally sequences
of either words/punctuation marks or sequences of characters. For the same reason, recurrent
neural networks are also used in speech processing.

	In the case of a recurrent neural network, the loss function $L$ of all time:
	
	And as always we do first a forward pass (first estimation based on initial chosen values) and afterwards we correct the weights with a backward pass done at each point in time (BPTT: Back-propagation Through Time). At time-step $T$ the derivative of the loss $L_\text{RNN}$ with respect to the different weight matrices $W$ and biases is expressed as follows:
	
	Remember that during our example introducing back-propagation earlier above, we used the chain rule twice. The ideal here would be to write something similar:
	
	However this is incomplete as $h_t$ depends on previous $h_{t-\ldots}$. Therefore we should write instead:
	
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	A recursive network is just a generalization of a recurrent network. In a recurrent network the weights are shared (and dimensionality remains constant) along the length of the sequence because how would you deal with position-dependent weights when you encounter a sequence at test-time of different length to any you saw at train-time. In a recursive network the weights are shared (and dimensionality remains constant) at every node for the same reason.
	\end{tcolorbox}
	
	There are lots of others variant of the LSTM presented above, like "\NewTerm{Depth Gated RNNs}" by Yao, et al. (12015 according to holocene calendar). There's also some completely different approach to tackling long-term dependencies, like "\NewTerm{Clockwork RNNs}" by Koutnik, et al. (12014).
	
	Which of these variants is best? Do the differences matter? Greff, et al. (12015) do a nice comparison of popular variants, finding that they're all about the same. Jozefowicz, et al. (12015) tested more than ten thousand RNN architectures, finding some that worked better than LSTMs on certain tasks \cite{jozefowicz2015empirical}.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	For people interested in practical application, see our \texttt{R} companion book where there is an example with a vanilla RNN, GRU and LSTM neural network on the same meteorological dateset.
	\end{tcolorbox}
	
	\pagebreak
	\paragraph{Convolutional Neural Networks (CNN)}\label{convolutional neural network}\mbox{}\\\\
	A "\NewTerm{Convolutional Neural Network}\index{convolutional neural network}" (ConvNet/CNN) is a class of Deep Learning  feed-forward artificial neural networks algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other\footnote{A common misconception is to think that CNN are dedicated only to pure Computer Vision applications. However they are also used in Time Series analysis in Finance or Supply chain to detect anomalies thanks to Spectral Residual CNN (SR-CNN). Softwares like Microsoft Power BI have such tools natively available.}.
	
	The whole idea of Convolutional Neural Networks is inspired by the biology of the eye. While we as humans perceive a visual image as a detailed, coloured image of the world around us, there is actually quiet a lot of processing done in our brain to get to this point. The higher we go into the brain, the more concrete features are detected by the cells. With combination of cells that can perceive more and more complex contrast patterns, the brain is able to form cells that react to very specific visual stimulation, like cells that respond when we see cats or dogs.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	CNNs are inspired by the layered structure of mammalian visual cortex based on studies of Hubel and Wiesel (11968 according to holocen calendar) in which the understanding of a scenery occurs through a hierarchical construction of images at different levels of abstraction from simple primitive feature extraction in earlier layers to more complex and expressive high-level contextual features in the later layers (Aggarwal, 12018).
	\end{tcolorbox}
	
	What researchers did with Convolutional Neural Networks is exactly the same: CNN try to use this concept of combining low-level features in the image to higher and higher-level features, until we have cells that react to very specific things: Fur, eyes, cat ears etc. Then, we use a classic Neural Network to combine these features to a meaningful context: Two ears, fur and two eyes will with a high probability be a cat. You get the idea.
	
	The objective of the Convolution Operation is to extract the high-level features such as edges, from the input image. ConvNets need not be limited to only one Convolutional Layer. Conventionally, the first ConvLayer is responsible for capturing the Low-Level features such as edges, color, gradient orientation, etc. With added layers, the architecture adapts to the High-Level features as well, giving us a network which has the wholesome understanding of images in the dataset, similar to how we would.
	
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Caution! The fields of CNN is in its nascent age in this early 121st century (holocene calendar). At the time we write these lines there are, as far as we know, no textbooks for "pure mathematical" lovers detailing in a purely mathematical way each step of a CNN. 
	\end{tcolorbox}
	
	The reader may have noticed that in images, pixels that are close to one another usually represent the same type of information: sky water, leaver, fur bricks, and so on. The exception from the rule are the edges: the parts of an image where two different objects "touch" one another.
	
	If we can train the neural network to recognize regions of the same information as well as the edges, this knowledge would allow the neural network to predict the object represented in the image. For example, if the neural network detected multiple skin regions and edges that look like parts of an oval with skin-like tone on the inside and bluish tone on the outside, then it is likely that it's a face on the sky background. If our goal is to detect people on pictures, the neural network will most likely succeed in predicting a person in this picture. 
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/cnn.jpg}
		\caption[]{The big picture of a convolution Neural Network}
	\end{figure}
	Or explicitly under the hood (even if the reader may not understand well the vocabulary in the image, seeing it now may help to grasp what will follow):
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/cnn_car.jpg}
	\end{figure}
	
	In the figure below, we have a RGB image which has been separated by its three color planes: Red, Green, and Blue. There are a number of such color spaces in which images exist — Grayscale, RGB, HSV, CMYK, etc.
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1467); %set diagram left start at 0, and has height of 1467
		
		%Shape: Square [id:dp42644655506102946] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][fill={rgb, 255:red, 141; green, 193; blue, 255 }  ,fill opacity=1 ] (99.25,63) -- (299.25,63) -- (299.25,263) -- (99.25,263) -- cycle ;
		%Straight Lines [id:da936317014367358] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (99,117) -- (299.5,117) ;
		%Straight Lines [id:da23166319441207306] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (99,167.5) -- (299.5,167.5) ;
		%Straight Lines [id:da10585770751123613] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (99,218) -- (299.5,218) ;
		%Straight Lines [id:da7051096830727512] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (147.5,63) -- (147.5,263) ;
		%Straight Lines [id:da43494327836491187] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (198.25,63) -- (198.25,263) ;
		%Straight Lines [id:da21593593648434273] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (253,63) -- (253,263) ;
		
		%Shape: Square [id:dp22362327334096377] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][fill={rgb, 255:red, 136; green, 218; blue, 60 }  ,fill opacity=1 ] (132.25,102) -- (332.25,102) -- (332.25,302) -- (132.25,302) -- cycle ;
		%Straight Lines [id:da922506919822202] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (132,156) -- (332.5,156) ;
		%Straight Lines [id:da6546955642051875] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (132,206.5) -- (332.5,206.5) ;
		%Straight Lines [id:da738938208015278] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (132,257) -- (332.5,257) ;
		%Straight Lines [id:da6628152966138934] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (180.5,102) -- (180.5,302) ;
		%Straight Lines [id:da21239794923703403] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (232.25,102) -- (232.25,302) ;
		%Straight Lines [id:da5903573077231581] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (286,102) -- (286,302) ;
		%Shape: Square [id:dp2418950997537339] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][fill={rgb, 255:red, 255; green, 191; blue, 191 }  ,fill opacity=1 ] (167.25,139) -- (367.25,139) -- (367.25,339) -- (167.25,339) -- cycle ;
		%Straight Lines [id:da2029837368774643] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (167,193) -- (367.5,193) ;
		%Straight Lines [id:da08249534434439876] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (167,243.5) -- (367.5,243.5) ;
		%Straight Lines [id:da059951324802091976] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (167,294) -- (367.5,294) ;
		%Straight Lines [id:da7234332130546193] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (215.5,139) -- (215.5,339) ;
		%Straight Lines [id:da6761797794321625] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (269.25,139) -- (269.25,339) ;
		%Straight Lines [id:da6698921617770721] 
		\draw [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ]   (321,139) -- (321,339) ;
		%Left Right Arrow [id:dp8063282563369312] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (326.68,39.69) -- (345.32,38.21) -- (338.94,44.38) -- (406.01,113.81) -- (412.39,107.65) -- (410.27,126.22) -- (391.63,127.71) -- (398.01,121.54) -- (330.93,52.1) -- (324.55,58.27) -- cycle ;
		%Left Right Arrow [id:dp30432997856734034] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (410.47,138.8) -- (424.91,158.58) -- (416.04,158.58) -- (416.04,319.23) -- (424.91,319.23) -- (410.47,339) -- (396.04,319.23) -- (404.91,319.23) -- (404.91,158.58) -- (396.04,158.58) -- cycle ;
		%Left Right Arrow [id:dp000273862072606601] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (365.57,362.9) -- (345.8,377.34) -- (345.8,368.46) -- (185.15,368.46) -- (185.15,377.34) -- (165.38,362.9) -- (185.15,348.47) -- (185.15,357.34) -- (345.8,357.34) -- (345.8,348.47) -- cycle ;
		
		% Text Node
		\draw (149.5,120.65) node [anchor=north west][inner sep=0.75pt]    {$9$};
		% Text Node
		\draw (198,120.65) node [anchor=north west][inner sep=0.75pt]    {$7$};
		% Text Node
		\draw (254,120.65) node [anchor=north west][inner sep=0.75pt]    {$3$};
		% Text Node
		\draw (305,120.65) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (146,174.9) node [anchor=north west][inner sep=0.75pt]    {$26$};
		% Text Node
		\draw (144.5,225.9) node [anchor=north west][inner sep=0.75pt]    {$15$};
		% Text Node
		\draw (149.5,270.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (114.75,81.65) node [anchor=north west][inner sep=0.75pt]    {$4$};
		% Text Node
		\draw (165,81.65) node [anchor=north west][inner sep=0.75pt]    {$6$};
		% Text Node
		\draw (221,81.65) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (272,81.65) node [anchor=north west][inner sep=0.75pt]    {$3$};
		% Text Node
		\draw (114.75,135.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (114.75,186.9) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (114.75,231.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (380,55) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 3$ colour channel};
		% Text Node
		\draw (424,223) node [anchor=north west][inner sep=0.75pt]   [align=left] {Height: $\displaystyle 4$ units (pixels)};
		% Text Node
		\draw (193,386) node [anchor=north west][inner sep=0.75pt]   [align=left] {Width: $\displaystyle 4$ units (pixels)};
		% Text Node
		\draw (181,158.9) node [anchor=north west][inner sep=0.75pt]    {$35$};
		% Text Node
		\draw (233,158.4) node [anchor=north west][inner sep=0.75pt]    {$19$};
		% Text Node
		\draw (285,158.4) node [anchor=north west][inner sep=0.75pt]    {$25$};
		% Text Node
		\draw (340,158.9) node [anchor=north west][inner sep=0.75pt]    {$6$};
		% Text Node
		\draw (181,211.9) node [anchor=north west][inner sep=0.75pt]    {$13$};
		% Text Node
		\draw (233,211.4) node [anchor=north west][inner sep=0.75pt]    {$22$};
		% Text Node
		\draw (285,211.4) node [anchor=north west][inner sep=0.75pt]    {$16$};
		% Text Node
		\draw (337,211.9) node [anchor=north west][inner sep=0.75pt]    {$53$};
		% Text Node
		\draw (186,262.65) node [anchor=north west][inner sep=0.75pt]    {$4$};
		% Text Node
		\draw (237,262.65) node [anchor=north west][inner sep=0.75pt]    {$3$};
		% Text Node
		\draw (289,262.65) node [anchor=north west][inner sep=0.75pt]    {$7$};
		% Text Node
		\draw (337,262.65) node [anchor=north west][inner sep=0.75pt]    {$10$};
		% Text Node
		\draw (186,307.65) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (237,307.65) node [anchor=north west][inner sep=0.75pt]    {$8$};
		% Text Node
		\draw (289,307.65) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (342,307.65) node [anchor=north west][inner sep=0.75pt]    {$3$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[]{$4\times 4\times 3$ RGB Image}
	\end{figure}
	The reader can imagine how computationally intensive things would get once the images reach dimensions, say $8$K ($7680\times 4320$). The role of the ConvNet is to reduce the images into a form which is easier to process, without losing features which are critical for getting a good prediction. This is important when we are to design an architecture which is not only good at learning features but also is scalable to massive datasets.
	
	Having in mind that the most important information in the image is local, we can split the image into square patches using a moving window approach\footnote{Consider this as if we looked at a dollar bill in a microscope. To see the whole bill we have to gradually move our bill from left to right and from top to bottom. At each moment in time, we see only a part of the bill of fixed dimensions. This approach is named "moving window"}. We can then train multiple smaller regression models at once, each small regression model receiving a square patch as input. The goal of each small regression model is to learn to detect a specific kind of pattern in the input patch. For example, one small regression model will learn to detect the sky; another one will detect the grass, the third one will detect edges of a building, and so on. 
	
	In the below demonstration, the image is a $5\times 5\times 1$ input image, The element involved in carrying out the convolution operation in the first part of a convolutional layer is named the "\NewTerm{Kernel}" or "\NewTerm{filter}" and is denoted $K$, represented in the figure below by the black border square. Below we have selected $K$ as a $2\times 2 \times 1$ matrix (\SeeChapter{see section Functional Analysis page \pageref{matrix convolution}}):
	\begin{figure}[H]
		\centering
		\resizebox{\textwidth}{!}{\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1467); %set diagram left start at 0, and has height of 1467
		
		%Shape: Grid [id:dp687778600003083] 
		\draw  [draw opacity=0] (77,54) -- (157,54) -- (157,134) -- (77,134) -- cycle ; \draw   (97,54) -- (97,134)(117,54) -- (117,134)(137,54) -- (137,134) ; \draw   (77,74) -- (157,74)(77,94) -- (157,94)(77,114) -- (157,114) ; \draw   (77,54) -- (157,54) -- (157,134) -- (77,134) -- cycle ;
		%Shape: Square [id:dp23374225508549573] 
		\draw  [line width=2.25]  (77,54) -- (117,54) -- (117,94) -- (77,94) -- cycle ;
		%Shape: Grid [id:dp6030255814637004] 
		\draw  [draw opacity=0] (174,33) -- (214,33) -- (214,73) -- (174,73) -- cycle ; \draw   (194,33) -- (194,73) ; \draw   (174,53) -- (214,53) ; \draw   (174,33) -- (214,33) -- (214,73) -- (174,73) -- cycle ;
		%Straight Lines [id:da24898753574086596] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (77,54) -- (174,33) ;
		%Straight Lines [id:da6554488696508538] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (117,94) -- (214,73) ;
		%Shape: Grid [id:dp27879921637944993] 
		\draw  [draw opacity=0] (174,114) -- (194,114) -- (194,134) -- (174,134) -- cycle ; \draw    ; \draw    ; \draw   (174,114) -- (194,114) -- (194,134) -- (174,134) -- cycle ;
		%Shape: Grid [id:dp5836743180651311] 
		\draw  [draw opacity=0] (244,69) -- (304,69) -- (304,129) -- (244,129) -- cycle ; \draw   (264,69) -- (264,129)(284,69) -- (284,129) ; \draw   (244,89) -- (304,89)(244,109) -- (304,109) ; \draw   (244,69) -- (304,69) -- (304,129) -- (244,129) -- cycle ;
		%Straight Lines [id:da5157278298896046] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (264,69) -- (214,33) ;
		%Straight Lines [id:da6233362422182664] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (244,89) -- (174,73) ;
		%Shape: Square [id:dp8756343719540223] 
		\draw  [line width=2.25]  (244,69) -- (264,69) -- (264,89) -- (244,89) -- cycle ;
		%Straight Lines [id:da7095002396679766] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (194,134) -- (264,89) ;
		%Straight Lines [id:da2435943258056621] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (174,114) -- (244,69) ;
		%Shape: Grid [id:dp8894680898759031] 
		\draw  [draw opacity=0] (409,54) -- (489,54) -- (489,134) -- (409,134) -- cycle ; \draw   (429,54) -- (429,134)(449,54) -- (449,134)(469,54) -- (469,134) ; \draw   (409,74) -- (489,74)(409,94) -- (489,94)(409,114) -- (489,114) ; \draw   (409,54) -- (489,54) -- (489,134) -- (409,134) -- cycle ;
		%Shape: Square [id:dp3392668856483416] 
		\draw  [line width=2.25]  (409,74) -- (449,74) -- (449,114) -- (409,114) -- cycle ;
		%Shape: Grid [id:dp6541780317610564] 
		\draw  [draw opacity=0] (506,33) -- (546,33) -- (546,73) -- (506,73) -- cycle ; \draw   (526,33) -- (526,73) ; \draw   (506,53) -- (546,53) ; \draw   (506,33) -- (546,33) -- (546,73) -- (506,73) -- cycle ;
		%Straight Lines [id:da7325144159339032] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (409,74) -- (506,33) ;
		%Straight Lines [id:da07117164822994204] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (449,114) -- (546,73) ;
		%Shape: Grid [id:dp6087168689897751] 
		\draw  [draw opacity=0] (506,114) -- (526,114) -- (526,134) -- (506,134) -- cycle ; \draw    ; \draw    ; \draw   (506,114) -- (526,114) -- (526,134) -- (506,134) -- cycle ;
		%Shape: Grid [id:dp40139149836116017] 
		\draw  [draw opacity=0] (576,69) -- (636,69) -- (636,129) -- (576,129) -- cycle ; \draw   (596,69) -- (596,129)(616,69) -- (616,129) ; \draw   (576,89) -- (636,89)(576,109) -- (636,109) ; \draw   (576,69) -- (636,69) -- (636,129) -- (576,129) -- cycle ;
		%Straight Lines [id:da5008864985705928] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (596,89) -- (546,33) ;
		%Straight Lines [id:da22307287150887412] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (576,109) -- (506,73) ;
		%Shape: Square [id:dp9304814744191492] 
		\draw  [line width=2.25]  (576,89) -- (596,89) -- (596,109) -- (576,109) -- cycle ;
		%Straight Lines [id:da8933668376932846] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (526,134) -- (596,109) ;
		%Straight Lines [id:da49600940406848326] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (506,114) -- (576,89) ;
		%Shape: Grid [id:dp39363780195977083] 
		\draw  [draw opacity=0] (77,211) -- (157,211) -- (157,291) -- (77,291) -- cycle ; \draw   (97,211) -- (97,291)(117,211) -- (117,291)(137,211) -- (137,291) ; \draw   (77,231) -- (157,231)(77,251) -- (157,251)(77,271) -- (157,271) ; \draw   (77,211) -- (157,211) -- (157,291) -- (77,291) -- cycle ;
		%Shape: Square [id:dp45761655470940177] 
		\draw  [line width=2.25]  (97,211) -- (137,211) -- (137,251) -- (97,251) -- cycle ;
		%Shape: Grid [id:dp793886286612185] 
		\draw  [draw opacity=0] (174,190) -- (214,190) -- (214,230) -- (174,230) -- cycle ; \draw   (194,190) -- (194,230) ; \draw   (174,210) -- (214,210) ; \draw   (174,190) -- (214,190) -- (214,230) -- (174,230) -- cycle ;
		%Straight Lines [id:da7777110901701554] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (97,211) -- (174,190) ;
		%Straight Lines [id:da7328658959471286] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (137,251) -- (214,230) ;
		%Shape: Grid [id:dp9314833371618421] 
		\draw  [draw opacity=0] (174,271) -- (194,271) -- (194,291) -- (174,291) -- cycle ; \draw    ; \draw    ; \draw   (174,271) -- (194,271) -- (194,291) -- (174,291) -- cycle ;
		%Shape: Grid [id:dp7617490994334706] 
		\draw  [draw opacity=0] (244,226) -- (304,226) -- (304,286) -- (244,286) -- cycle ; \draw   (264,226) -- (264,286)(284,226) -- (284,286) ; \draw   (244,246) -- (304,246)(244,266) -- (304,266) ; \draw   (244,226) -- (304,226) -- (304,286) -- (244,286) -- cycle ;
		%Straight Lines [id:da3560294463839313] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (284,226) -- (214,190) ;
		%Straight Lines [id:da8154288868445101] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (264,246) -- (174,230) ;
		%Straight Lines [id:da37688729657707754] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (194,291) -- (284,246) ;
		%Straight Lines [id:da09657311558732884] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (174,271) -- (264,226) ;
		%Shape: Grid [id:dp3375532844428595] 
		\draw  [draw opacity=0] (409,211) -- (489,211) -- (489,291) -- (409,291) -- cycle ; \draw   (429,211) -- (429,291)(449,211) -- (449,291)(469,211) -- (469,291) ; \draw   (409,231) -- (489,231)(409,251) -- (489,251)(409,271) -- (489,271) ; \draw   (409,211) -- (489,211) -- (489,291) -- (409,291) -- cycle ;
		%Shape: Square [id:dp2607102451969412] 
		\draw  [line width=2.25]  (429,231) -- (469,231) -- (469,271) -- (429,271) -- cycle ;
		%Shape: Grid [id:dp1463723413993303] 
		\draw  [draw opacity=0] (506,190) -- (546,190) -- (546,230) -- (506,230) -- cycle ; \draw   (526,190) -- (526,230) ; \draw   (506,210) -- (546,210) ; \draw   (506,190) -- (546,190) -- (546,230) -- (506,230) -- cycle ;
		%Straight Lines [id:da58531975097674] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (429,231) -- (506,190) ;
		%Straight Lines [id:da790018395720834] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (469,271) -- (546,230) ;
		%Shape: Grid [id:dp06539646276155575] 
		\draw  [draw opacity=0] (506,271) -- (526,271) -- (526,291) -- (506,291) -- cycle ; \draw    ; \draw    ; \draw   (506,271) -- (526,271) -- (526,291) -- (506,291) -- cycle ;
		%Shape: Grid [id:dp899633078277936] 
		\draw  [draw opacity=0] (576,226) -- (636,226) -- (636,286) -- (576,286) -- cycle ; \draw   (596,226) -- (596,286)(616,226) -- (616,286) ; \draw   (576,246) -- (636,246)(576,266) -- (636,266) ; \draw   (576,226) -- (636,226) -- (636,286) -- (576,286) -- cycle ;
		%Straight Lines [id:da3468277073593593] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (616,246) -- (546,190) ;
		%Straight Lines [id:da7386834638271753] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (596,266) -- (506,230) ;
		%Shape: Square [id:dp06476759411275479] 
		\draw  [line width=2.25]  (596,246) -- (616,246) -- (616,266) -- (596,266) -- cycle ;
		%Straight Lines [id:da37301098692697665] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (526,291) -- (616,266) ;
		%Straight Lines [id:da24582510489469933] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (506,271) -- (596,246) ;
		%Shape: Square [id:dp775231116129623] 
		\draw  [line width=2.25]  (264,226) -- (284,226) -- (284,246) -- (264,246) -- cycle ;
		%Shape: Grid [id:dp3462926447768848] 
		\draw  [draw opacity=0] (77,370) -- (157,370) -- (157,450) -- (77,450) -- cycle ; \draw   (97,370) -- (97,450)(117,370) -- (117,450)(137,370) -- (137,450) ; \draw   (77,390) -- (157,390)(77,410) -- (157,410)(77,430) -- (157,430) ; \draw   (77,370) -- (157,370) -- (157,450) -- (77,450) -- cycle ;
		%Shape: Square [id:dp7960893356266028] 
		\draw  [line width=2.25]  (117,370) -- (157,370) -- (157,410) -- (117,410) -- cycle ;
		%Shape: Grid [id:dp6145875520304971] 
		\draw  [draw opacity=0] (174,349) -- (214,349) -- (214,389) -- (174,389) -- cycle ; \draw   (194,349) -- (194,389) ; \draw   (174,369) -- (214,369) ; \draw   (174,349) -- (214,349) -- (214,389) -- (174,389) -- cycle ;
		%Straight Lines [id:da09431771130040056] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (117,370) -- (174,349) ;
		%Straight Lines [id:da26857706983285623] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (157,410) -- (214,389) ;
		%Shape: Grid [id:dp29719925715685735] 
		\draw  [draw opacity=0] (174,430) -- (194,430) -- (194,450) -- (174,450) -- cycle ; \draw    ; \draw    ; \draw   (174,430) -- (194,430) -- (194,450) -- (174,450) -- cycle ;
		%Shape: Grid [id:dp8937510159320112] 
		\draw  [draw opacity=0] (244,385) -- (304,385) -- (304,445) -- (244,445) -- cycle ; \draw   (264,385) -- (264,445)(284,385) -- (284,445) ; \draw   (244,405) -- (304,405)(244,425) -- (304,425) ; \draw   (244,385) -- (304,385) -- (304,445) -- (244,445) -- cycle ;
		%Straight Lines [id:da5487972668564955] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (304,385) -- (214,349) ;
		%Straight Lines [id:da5928351876593188] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (284,405) -- (174,389) ;
		%Straight Lines [id:da6798556697085099] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (194,450) -- (284,405) ;
		%Straight Lines [id:da9212687993846769] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (174,430) -- (284,385) ;
		%Shape: Grid [id:dp9850885542066508] 
		\draw  [draw opacity=0] (409,370) -- (489,370) -- (489,450) -- (409,450) -- cycle ; \draw   (429,370) -- (429,450)(449,370) -- (449,450)(469,370) -- (469,450) ; \draw   (409,390) -- (489,390)(409,410) -- (489,410)(409,430) -- (489,430) ; \draw   (409,370) -- (489,370) -- (489,450) -- (409,450) -- cycle ;
		%Shape: Square [id:dp5749687574710414] 
		\draw  [line width=2.25]  (449,390) -- (489,390) -- (489,430) -- (449,430) -- cycle ;
		%Shape: Grid [id:dp3655970143916043] 
		\draw  [draw opacity=0] (506,349) -- (546,349) -- (546,389) -- (506,389) -- cycle ; \draw   (526,349) -- (526,389) ; \draw   (506,369) -- (546,369) ; \draw   (506,349) -- (546,349) -- (546,389) -- (506,389) -- cycle ;
		%Straight Lines [id:da8002305950764466] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (449,390) -- (506,349) ;
		%Straight Lines [id:da02142615066311837] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (489,430) -- (546,389) ;
		%Shape: Grid [id:dp38816674564673637] 
		\draw  [draw opacity=0] (506,430) -- (526,430) -- (526,450) -- (506,450) -- cycle ; \draw    ; \draw    ; \draw   (506,430) -- (526,430) -- (526,450) -- (506,450) -- cycle ;
		%Shape: Grid [id:dp5521234178250767] 
		\draw  [draw opacity=0] (576,385) -- (636,385) -- (636,445) -- (576,445) -- cycle ; \draw   (596,385) -- (596,445)(616,385) -- (616,445) ; \draw   (576,405) -- (636,405)(576,425) -- (636,425) ; \draw   (576,385) -- (636,385) -- (636,445) -- (576,445) -- cycle ;
		%Straight Lines [id:da24082157932409132] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (636,405) -- (546,349) ;
		%Straight Lines [id:da624771616888881] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (616,425) -- (506,389) ;
		%Shape: Square [id:dp8386571951652833] 
		\draw  [line width=2.25]  (616,405) -- (636,405) -- (636,425) -- (616,425) -- cycle ;
		%Straight Lines [id:da787782609897584] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (526,450) -- (636,425) ;
		%Straight Lines [id:da9685428035086794] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (506,430) -- (616,405) ;
		%Shape: Square [id:dp4085864232755896] 
		\draw  [line width=2.25]  (284,385) -- (304,385) -- (304,405) -- (284,405) -- cycle ;
		
		% Text Node
		\draw (21,84) node [anchor=north west][inner sep=0.75pt]   [align=left] {Conv $\displaystyle 1$};
		% Text Node
		\draw (83.15,56.9) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (102.43,57.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (121.71,57.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (141,57.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (83.15,76.9) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (102.43,77.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (122,77.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (141,77.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (83.15,98) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (102,98) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (121.71,98) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (141,98) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (83,116.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (102,116.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (121.71,116.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (141,116.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (95,23) node [anchor=north west][inner sep=0.75pt]   [align=left] {Image};
		% Text Node
		\draw (175,13) node [anchor=north west][inner sep=0.75pt]   [align=left] {Filter};
		% Text Node
		\draw (174,36.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$-1$};
		% Text Node
		\draw (200,36.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$2$};
		% Text Node
		\draw (193.5,56.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$-2$};
		% Text Node
		\draw (180,56.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$4$};
		% Text Node
		\draw (179,117.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (169,95) node [anchor=north west][inner sep=0.75pt]   [align=left] {Bias};
		% Text Node
		\draw (250,72.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$4$};
		% Text Node
		\draw (227,32) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{75pt}\setlength\topsep{0pt}
		\begin{center}
		Output before\\nonlinearity
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (353,84) node [anchor=north west][inner sep=0.75pt]   [align=left] {Conv $\displaystyle 4$};
		% Text Node
		\draw (415.15,56.9) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (434.43,57.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (453.71,57.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (473,57.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (415.15,76.9) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (434.43,77.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (454,77.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (473,77.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (415.15,95.9) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (434,96.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (453.71,96.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (473,96.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (415,116.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (434,116.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (453.71,116.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (473,116.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (427,23) node [anchor=north west][inner sep=0.75pt]   [align=left] {Image};
		% Text Node
		\draw (507,13) node [anchor=north west][inner sep=0.75pt]   [align=left] {Filter};
		% Text Node
		\draw (507,36.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$-1$};
		% Text Node
		\draw (532,36.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$2$};
		% Text Node
		\draw (525,56.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$-2$};
		% Text Node
		\draw (512,56.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$4$};
		% Text Node
		\draw (511,117.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (501,95) node [anchor=north west][inner sep=0.75pt]   [align=left] {Bias};
		% Text Node
		\draw (582,72.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$4$};
		% Text Node
		\draw (621,72.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$7$};
		% Text Node
		\draw (582,92.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$2$};
		% Text Node
		\draw (559,32) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{75pt}\setlength\topsep{0pt}
		\begin{center}
		Output before\\nonlinearity
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (21,241) node [anchor=north west][inner sep=0.75pt]   [align=left] {Conv $\displaystyle 2$};
		% Text Node
		\draw (83.15,213.9) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (102.43,214.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (121.71,214.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (141,214.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (83.15,233.9) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (102.43,234.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (122,234.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (141,234.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (83.15,254) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (102,254) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (121.71,254) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (141,254) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (83,273.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (102,273.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (121.71,273.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (141,273.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (95,185) node [anchor=north west][inner sep=0.75pt]   [align=left] {Image};
		% Text Node
		\draw (175,170) node [anchor=north west][inner sep=0.75pt]   [align=left] {Filter};
		% Text Node
		\draw (174,193.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$-1$};
		% Text Node
		\draw (200,193.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$2$};
		% Text Node
		\draw (193.5,213.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$-2$};
		% Text Node
		\draw (180,213.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$4$};
		% Text Node
		\draw (179,274.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (169,252) node [anchor=north west][inner sep=0.75pt]   [align=left] {Bias};
		% Text Node
		\draw (250,229.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$4$};
		% Text Node
		\draw (227,189) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{75pt}\setlength\topsep{0pt}
		\begin{center}
		Output before\\nonlinearity
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (353,241) node [anchor=north west][inner sep=0.75pt]   [align=left] {Conv $\displaystyle 5$};
		% Text Node
		\draw (415.15,213.9) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (434.43,214.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (453.71,214.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (473,214.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (415.15,233.9) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (434.43,234.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (454,234.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (473,234.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (415.15,252.9) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (434,253.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (453.71,253.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (473,253.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (415,273.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (434,273.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (451,274.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (473,273.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (427,185) node [anchor=north west][inner sep=0.75pt]   [align=left] {Image};
		% Text Node
		\draw (507,170) node [anchor=north west][inner sep=0.75pt]   [align=left] {Filter};
		% Text Node
		\draw (506.5,193.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$-1$};
		% Text Node
		\draw (532,193.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$2$};
		% Text Node
		\draw (525.5,213.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$-2$};
		% Text Node
		\draw (512,213.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$4$};
		% Text Node
		\draw (511,274.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (501,252) node [anchor=north west][inner sep=0.75pt]   [align=left] {Bias};
		% Text Node
		\draw (581,229.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$4$};
		% Text Node
		\draw (596,229.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$-1$};
		% Text Node
		\draw (581,249.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$2$};
		% Text Node
		\draw (559,189) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{75pt}\setlength\topsep{0pt}
		\begin{center}
		Output before\\nonlinearity
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (264,229.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$-1$};
		% Text Node
		\draw (602,249.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$7$};
		% Text Node
		\draw (21,400) node [anchor=north west][inner sep=0.75pt]   [align=left] {Conv $\displaystyle 3$};
		% Text Node
		\draw (83.15,372.9) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (102.43,373.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (121.71,373.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (141,373.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (83.15,392.9) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (102.43,393.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (122,393.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (141,393.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (83.15,411.9) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (102,412.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (121.71,412.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (141,412.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (83,432.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (102,432.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (121.71,432.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (141,432.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (95,344) node [anchor=north west][inner sep=0.75pt]   [align=left] {Image};
		% Text Node
		\draw (175,329) node [anchor=north west][inner sep=0.75pt]   [align=left] {Filter};
		% Text Node
		\draw (174.5,352.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$-1$};
		% Text Node
		\draw (200,352.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$2$};
		% Text Node
		\draw (193.5,372.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$-2$};
		% Text Node
		\draw (180,372.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$4$};
		% Text Node
		\draw (179,433.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (169,411) node [anchor=north west][inner sep=0.75pt]   [align=left] {Bias};
		% Text Node
		\draw (246,388.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$4$};
		% Text Node
		\draw (227,348) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{75pt}\setlength\topsep{0pt}
		\begin{center}
		Output before\\nonlinearity
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (353,400) node [anchor=north west][inner sep=0.75pt]   [align=left] {Conv $\displaystyle 6$};
		% Text Node
		\draw (415.15,372.9) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (434.43,373.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (453.71,373.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (473,373.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (415.15,392.9) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (434.43,393.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (454,393.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (473,393.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (415.15,411.9) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (434,412.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (453.71,412.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (473,412.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (415,432.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (434,432.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (453.71,432.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (473,432.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (427,344) node [anchor=north west][inner sep=0.75pt]   [align=left] {Image};
		% Text Node
		\draw (507,329) node [anchor=north west][inner sep=0.75pt]   [align=left] {Filter};
		% Text Node
		\draw (507,352.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$-1$};
		% Text Node
		\draw (532,352.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$2$};
		% Text Node
		\draw (525.5,372.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$-2$};
		% Text Node
		\draw (512,372.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$4$};
		% Text Node
		\draw (511,433.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (501,411) node [anchor=north west][inner sep=0.75pt]   [align=left] {Bias};
		% Text Node
		\draw (581,388.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$4$};
		% Text Node
		\draw (596.5,388.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$-1$};
		% Text Node
		\draw (578,408.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$2$};
		% Text Node
		\draw (559,348) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{75pt}\setlength\topsep{0pt}
		\begin{center}
		Output before\\nonlinearity
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (290,388.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$7$};
		% Text Node
		\draw (622,408.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (264,388.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$-1$};
		% Text Node
		\draw (597,72.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$-1$};
		% Text Node
		\draw (622,229.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$7$};
		% Text Node
		\draw (622,388.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$7$};
		% Text Node
		\draw (601,408.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$7$};
		\end{tikzpicture}}
		\vspace*{3mm}
		\caption{A filter convolving across an image}
	\end{figure}
	As we can see above see above, the kernel, filter shifts $9$ times because of stride length $= 1$ (non-strided), every time performing a matrix multiplication operation between $K$ and the portion $P$ of the image over which the kernel is hovering.
	
	Also notice that if we denote by $H\times W$ the dimension on the input image (Height $\times$ Width), then the output dimension after the convolution by a kernel of dimension $k_1\times k_2$ will be:
	
	And if we denote by $S$ the stride value we then have:
	
	
	The filter moves to the right with a certain stride value till it parses the complete width. Moving on, it hops down to the beginning (left) of the image with the same Stride Value and repeats the process until the entire image is traversed.
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1467); %set diagram left start at 0, and has height of 1467
		
		%Shape: Grid [id:dp7167879620011206] 
		\draw  [draw opacity=0] (60,102) -- (180,102) -- (180,222) -- (60,222) -- cycle ; \draw   (90,102) -- (90,222)(120,102) -- (120,222)(150,102) -- (150,222) ; \draw   (60,132) -- (180,132)(60,162) -- (180,162)(60,192) -- (180,192) ; \draw   (60,102) -- (180,102) -- (180,222) -- (60,222) -- cycle ;
		%Shape: Grid [id:dp8956865329453967] 
		\draw  [draw opacity=0] (205,102) -- (325,102) -- (325,222) -- (205,222) -- cycle ; \draw   (235,102) -- (235,222)(265,102) -- (265,222)(295,102) -- (295,222) ; \draw   (205,132) -- (325,132)(205,162) -- (325,162)(205,192) -- (325,192) ; \draw   (205,102) -- (325,102) -- (325,222) -- (205,222) -- cycle ;
		%Straight Lines [id:da04299701436228398] 
		\draw    (44,89) -- (44,114) ;
		%Straight Lines [id:da5736507099874473] 
		\draw    (44,89) -- (487.5,89) ;
		%Straight Lines [id:da6704992289474991] 
		\draw    (264,63) -- (264,88) ;
		%Shape: Grid [id:dp9288928930902214] 
		\draw  [draw opacity=0] (353,102) -- (473,102) -- (473,222) -- (353,222) -- cycle ; \draw   (383,102) -- (383,222)(413,102) -- (413,222)(443,102) -- (443,222) ; \draw   (353,132) -- (473,132)(353,162) -- (473,162)(353,192) -- (473,192) ; \draw   (353,102) -- (473,102) -- (473,222) -- (353,222) -- cycle ;
		%Straight Lines [id:da6890573491502161] 
		\draw    (487.5,89) -- (487.5,114) ;
		%Shape: Grid [id:dp8010102415523406] 
		\draw  [draw opacity=0] (525,123) -- (615,123) -- (615,213) -- (525,213) -- cycle ; \draw   (555,123) -- (555,213)(585,123) -- (585,213) ; \draw   (525,153) -- (615,153)(525,183) -- (615,183) ; \draw   (525,123) -- (615,123) -- (615,213) -- (525,213) -- cycle ;
		%Shape: Square [id:dp8492065186489808] 
		\draw  [line width=2.25]  (60,102) -- (120,102) -- (120,162) -- (60,162) -- cycle ;
		%Shape: Square [id:dp05040182063763221] 
		\draw  [line width=2.25]  (205,102) -- (265,102) -- (265,162) -- (205,162) -- cycle ;
		%Shape: Square [id:dp439106854661653] 
		\draw  [line width=2.25]  (353,102) -- (413,102) -- (413,162) -- (353,162) -- cycle ;
		%Shape: Square [id:dp011057637399622733] 
		\draw  [line width=2.25]  (525,123) -- (555,123) -- (555,153) -- (525,153) -- cycle ;
		%Shape: Grid [id:dp06776885027006241] 
		\draw  [draw opacity=0] (184,315) -- (244,315) -- (244,375) -- (184,375) -- cycle ; \draw   (214,315) -- (214,375) ; \draw   (184,345) -- (244,345) ; \draw   (184,315) -- (244,315) -- (244,375) -- (184,375) -- cycle ;
		%Shape: Grid [id:dp9308123816212965] 
		\draw  [draw opacity=0] (324,315) -- (354,315) -- (354,345) -- (324,345) -- cycle ; \draw    ; \draw    ; \draw   (324,315) -- (354,315) -- (354,345) -- (324,345) -- cycle ;
		%Straight Lines [id:da7155489640667783] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (60,162) -- (184,375) ;
		%Straight Lines [id:da4883295221376911] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (120,102) -- (244,315) ;
		%Straight Lines [id:da6291476009364705] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (205,102) -- (184,315) ;
		%Straight Lines [id:da05338671366928471] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (265,162) -- (244,375) ;
		%Straight Lines [id:da7632357280882198] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (353,102) -- (183.5,321) ;
		%Straight Lines [id:da1754284105966133] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (413,162) -- (244,375) ;
		%Straight Lines [id:da06811548825858171] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (525,123) -- (324,315) ;
		%Straight Lines [id:da42266572457818263] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (555,153) -- (354,345) ;
		%Straight Lines [id:da8828455485590456] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (525,123) -- (184,315) ;
		%Straight Lines [id:da3361298286417089] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (555,153) -- (244,375) ;
		
		% Text Node
		\draw (69,109.9) node [anchor=north west][inner sep=0.75pt]    {$3$};
		% Text Node
		\draw (100,109.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (123,109.9) node [anchor=north west][inner sep=0.75pt]    {$-2$};
		% Text Node
		\draw (159,109.9) node [anchor=north west][inner sep=0.75pt]    {$4$};
		% Text Node
		\draw (69,139.15) node [anchor=north west][inner sep=0.75pt]    {$4$};
		% Text Node
		\draw (99,139.15) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (129,139.15) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (159,139.15) node [anchor=north west][inner sep=0.75pt]    {$5$};
		% Text Node
		\draw (69,169.15) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (99,169.15) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (129,169.15) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (159,169.15) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (69,198.15) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (95,198.15) node [anchor=north west][inner sep=0.75pt]    {$-2$};
		% Text Node
		\draw (126,198.15) node [anchor=north west][inner sep=0.75pt]    {$-1$};
		% Text Node
		\draw (159,198.15) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (214,110.15) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (240,110.15) node [anchor=north west][inner sep=0.75pt]    {$-1$};
		% Text Node
		\draw (273.5,110.15) node [anchor=north west][inner sep=0.75pt]    {$3$};
		% Text Node
		\draw (299.5,110.15) node [anchor=north west][inner sep=0.75pt]    {$-2$};
		% Text Node
		\draw (210,139.15) node [anchor=north west][inner sep=0.75pt]    {$-3$};
		% Text Node
		\draw (244.5,139.15) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (273.5,139.15) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (304,139.15) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (210,169.15) node [anchor=north west][inner sep=0.75pt]    {$-1$};
		% Text Node
		\draw (244.5,169.15) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (269,169.15) node [anchor=north west][inner sep=0.75pt]    {$-3$};
		% Text Node
		\draw (304,169.15) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (214,198.78) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (244.5,198.78) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (273.5,198.78) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (299.5,198.78) node [anchor=north west][inner sep=0.75pt]    {$-5$};
		% Text Node
		\draw (239,42) node [anchor=north west][inner sep=0.75pt]   [align=left] {Volume};
		% Text Node
		\draw (362,110.15) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (389.17,110.15) node [anchor=north west][inner sep=0.75pt]    {$-1$};
		% Text Node
		\draw (425.34,110.15) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (452.5,110.15) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (363,139.28) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (386.67,139.28) node [anchor=north west][inner sep=0.75pt]    {$-1$};
		% Text Node
		\draw (419.34,139.28) node [anchor=north west][inner sep=0.75pt]    {$-1$};
		% Text Node
		\draw (452,139.28) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (362,168.15) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (392.67,168.15) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (423.34,168.15) node [anchor=north west][inner sep=0.75pt]    {$4$};
		% Text Node
		\draw (454,168.15) node [anchor=north west][inner sep=0.75pt]    {$3$};
		% Text Node
		\draw (362,198.59) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (387.33,198.59) node [anchor=north west][inner sep=0.75pt]    {$-3$};
		% Text Node
		\draw (421.66,198.59) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (447,198.59) node [anchor=north west][inner sep=0.75pt]    {$-2$};
		% Text Node
		\draw (529.5,130.15) node [anchor=north west][inner sep=0.75pt]    {$-3$};
		% Text Node
		\draw (525,85) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{67.94pt}\setlength\topsep{0pt}
		\begin{center}
		Output before \\nonlinearity
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (197,292.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {Filter};
		% Text Node
		\draw (189,321.4) node [anchor=north west][inner sep=0.75pt]    {$-2$};
		% Text Node
		\draw (225,321.9) node [anchor=north west][inner sep=0.75pt]    {$3$};
		% Text Node
		\draw (193,352.15) node [anchor=north west][inner sep=0.75pt]    {$5$};
		% Text Node
		\draw (216,348.4) node [anchor=north west][inner sep=0.75pt]    {$-1$};
		% Text Node
		\draw (322,292.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {Bias};
		% Text Node
		\draw (329,321.4) node [anchor=north west][inner sep=0.75pt]    {$-2$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Convolution of a volume consisting of three matrices}
	\end{figure}
	An example of a convolution of a patch of a volume consisting of depth $L=3$ is shown above. The value of the convolution, $-3$, was obtained as:
	
	Or in the most general case this will be written (the expression is slightly different from what we saw during our study of matrix convolution):
	
	What some engineer also write sometimes (...) assuming the Kernel is zero centered:
	
	or :
	
	where $C$ denotes the number of layers, $I$ the input, $K$ the kernel.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us consider the following notation:
	\begin{gather*}
		C[x,y] = (I\ast K)(x,y) = \sum_{\alpha=-\infty}^{+\infty} \sum_{\beta=-\infty}^{+\infty} I(x-\alpha, y-\beta) K(\alpha,\beta)
	\end{gather*}
	Let's not zero-center $I$ and compute $C[2,2]$ with a $3\times 3$ kernel:
	\begin{gather*}
		\begin{aligned}
		C_{22} &= \sum_{\alpha=-1}^{+1}\sum_{\beta=-1}^{+1} I(x-\alpha, y-\beta) K(\alpha,\beta) \\
		&=    I_{2+1,2+1} K_{-1,-1} + I_{2+1,2-0} K_{-1,0} + I_{2+1,2-1} K_{-1,1} \\
		    &\hspace{0.5in} + I_{2-0,2+1} K_{0,-1}  + I_{2-0,2-0} K_{0,0}  + I_{2-0,2-1} K_{0,1} \\
		    &\hspace{0.5in} + I_{2-1,2+1} K_{1,-1}  + I_{2-1,2-0} K_{1,0}  + I_{2-1,2-1} K_{1,1} \\
		&=    I_{3,3} K_{-1,-1} + I_{3,2} K_{-1,0} + I_{3,1} K_{-1,1} \\
		    &\hspace{0.5in} + I_{2,3} K_{0,-1}  + I_{2,2} K_{0,0}  + I_{2,1} K_{0,1} \\
		    &\hspace{0.5in} + I_{1,3} K_{1,-1}  + I_{1,2} K_{1,0}  + I_{1,1} K_{1,1} \\
		\end{aligned}
	\end{gather*}
	We note that a $3\times 3$ kernel that is zero-centered (i.e., its origin is in the middle of the filter) has indices running from $-1$ to $+1$. To reiterate, $-1$ is a valid index for $K$ (not padded) because we are zero-centered, but anything lower than that will just return zero.
	\end{tcolorbox}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Given an input image $I$ and a filter (kernel) $K$ of dimensions $k_1\times k_2$, the cross-correlation operation is given by:
	
	Given an input image $I$ and a filter (kernel) $K$ of dimensions $k_1\times k_2$, the convolution operation is given by:
	
	It is easy to see that convolution is the same as cross-correlation with a flipped kernel i.e: for a kernel $K$ where $K(-m,-n)=K(m,n)$.
	\end{tcolorbox}
	
	In computer vision, CNNs often get volumes as input, since an image is usually represented by three channels: R, G, and B, each channel being a monochrome picture. 
	
	Two important properties of convolution are "\NewTerm{stride}" and "\NewTerm{padding}". Stride is the step size of the moving window. In the first example above, the stride is $1$, that is the filter slides to the right and to the bottom by one cell at a time. In the figure below we can see a partial example of convolution with stride $2$. We can see that the output matrix is smaller when the stride is bigger:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1467); %set diagram left start at 0, and has height of 1467
		
		%Shape: Grid [id:dp687778600003083] 
		\draw  [draw opacity=0] (77,54) -- (157,54) -- (157,134) -- (77,134) -- cycle ; \draw   (97,54) -- (97,134)(117,54) -- (117,134)(137,54) -- (137,134) ; \draw   (77,74) -- (157,74)(77,94) -- (157,94)(77,114) -- (157,114) ; \draw   (77,54) -- (157,54) -- (157,134) -- (77,134) -- cycle ;
		%Shape: Square [id:dp23374225508549573] 
		\draw  [line width=2.25]  (77,54) -- (117,54) -- (117,94) -- (77,94) -- cycle ;
		%Shape: Grid [id:dp6030255814637004] 
		\draw  [draw opacity=0] (174,33) -- (214,33) -- (214,73) -- (174,73) -- cycle ; \draw   (194,33) -- (194,73) ; \draw   (174,53) -- (214,53) ; \draw   (174,33) -- (214,33) -- (214,73) -- (174,73) -- cycle ;
		%Straight Lines [id:da24898753574086596] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (77,54) -- (174,33) ;
		%Straight Lines [id:da6554488696508538] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (117,94) -- (214,73) ;
		%Shape: Grid [id:dp27879921637944993] 
		\draw  [draw opacity=0] (174,114) -- (194,114) -- (194,134) -- (174,134) -- cycle ; \draw    ; \draw    ; \draw   (174,114) -- (194,114) -- (194,134) -- (174,134) -- cycle ;
		%Shape: Grid [id:dp5836743180651311] 
		\draw  [draw opacity=0] (244,69) -- (284,69) -- (284,109) -- (244,109) -- cycle ; \draw   (264,69) -- (264,109) ; \draw   (244,89) -- (284,89) ; \draw   (244,69) -- (284,69) -- (284,109) -- (244,109) -- cycle ;
		%Straight Lines [id:da5157278298896046] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (264,69) -- (214,33) ;
		%Straight Lines [id:da6233362422182664] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (244,89) -- (174,73) ;
		%Shape: Square [id:dp8756343719540223] 
		\draw  [line width=2.25]  (244,69) -- (264,69) -- (264,89) -- (244,89) -- cycle ;
		%Straight Lines [id:da7095002396679766] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (194,134) -- (264,89) ;
		%Straight Lines [id:da2435943258056621] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (174,114) -- (244,69) ;
		%Shape: Grid [id:dp8894680898759031] 
		\draw  [draw opacity=0] (409,54) -- (489,54) -- (489,134) -- (409,134) -- cycle ; \draw   (429,54) -- (429,134)(449,54) -- (449,134)(469,54) -- (469,134) ; \draw   (409,74) -- (489,74)(409,94) -- (489,94)(409,114) -- (489,114) ; \draw   (409,54) -- (489,54) -- (489,134) -- (409,134) -- cycle ;
		%Shape: Square [id:dp3392668856483416] 
		\draw  [line width=2.25]  (409,94) -- (449,94) -- (449,134) -- (409,134) -- cycle ;
		%Shape: Grid [id:dp6541780317610564] 
		\draw  [draw opacity=0] (506,33) -- (546,33) -- (546,73) -- (506,73) -- cycle ; \draw   (526,33) -- (526,73) ; \draw   (506,53) -- (546,53) ; \draw   (506,33) -- (546,33) -- (546,73) -- (506,73) -- cycle ;
		%Straight Lines [id:da7325144159339032] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (409,94) -- (506,33) ;
		%Straight Lines [id:da07117164822994204] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (449,134) -- (546,73) ;
		%Shape: Grid [id:dp6087168689897751] 
		\draw  [draw opacity=0] (506,114) -- (526,114) -- (526,134) -- (506,134) -- cycle ; \draw    ; \draw    ; \draw   (506,114) -- (526,114) -- (526,134) -- (506,134) -- cycle ;
		%Shape: Grid [id:dp40139149836116017] 
		\draw  [draw opacity=0] (576,69) -- (616,69) -- (616,109) -- (576,109) -- cycle ; \draw   (596,69) -- (596,109) ; \draw   (576,89) -- (616,89) ; \draw   (576,69) -- (616,69) -- (616,109) -- (576,109) -- cycle ;
		%Straight Lines [id:da5008864985705928] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (596,89) -- (546,33) ;
		%Straight Lines [id:da22307287150887412] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (576,109) -- (506,73) ;
		%Shape: Square [id:dp9304814744191492] 
		\draw  [line width=2.25]  (576,89) -- (596,89) -- (596,109) -- (576,109) -- cycle ;
		%Straight Lines [id:da8933668376932846] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (526,134) -- (596,109) ;
		%Straight Lines [id:da49600940406848326] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (506,114) -- (576,89) ;
		%Shape: Grid [id:dp39363780195977083] 
		\draw  [draw opacity=0] (77,211) -- (157,211) -- (157,291) -- (77,291) -- cycle ; \draw   (97,211) -- (97,291)(117,211) -- (117,291)(137,211) -- (137,291) ; \draw   (77,231) -- (157,231)(77,251) -- (157,251)(77,271) -- (157,271) ; \draw   (77,211) -- (157,211) -- (157,291) -- (77,291) -- cycle ;
		%Shape: Square [id:dp45761655470940177] 
		\draw  [line width=2.25]  (117,211) -- (157,211) -- (157,251) -- (117,251) -- cycle ;
		%Shape: Grid [id:dp793886286612185] 
		\draw  [draw opacity=0] (174,190) -- (214,190) -- (214,230) -- (174,230) -- cycle ; \draw   (194,190) -- (194,230) ; \draw   (174,210) -- (214,210) ; \draw   (174,190) -- (214,190) -- (214,230) -- (174,230) -- cycle ;
		%Straight Lines [id:da7777110901701554] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (117,211) -- (174,190) ;
		%Straight Lines [id:da7328658959471286] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (157,251) -- (214,230) ;
		%Shape: Grid [id:dp9314833371618421] 
		\draw  [draw opacity=0] (174,271) -- (194,271) -- (194,291) -- (174,291) -- cycle ; \draw    ; \draw    ; \draw   (174,271) -- (194,271) -- (194,291) -- (174,291) -- cycle ;
		%Shape: Grid [id:dp7617490994334706] 
		\draw  [draw opacity=0] (244,226) -- (284,226) -- (284,266) -- (244,266) -- cycle ; \draw   (264,226) -- (264,266) ; \draw   (244,246) -- (284,246) ; \draw   (244,226) -- (284,226) -- (284,266) -- (244,266) -- cycle ;
		%Straight Lines [id:da3560294463839313] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (284,226) -- (214,190) ;
		%Straight Lines [id:da8154288868445101] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (264,246) -- (174,230) ;
		%Straight Lines [id:da37688729657707754] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (194,291) -- (284,246) ;
		%Straight Lines [id:da09657311558732884] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (174,271) -- (264,226) ;
		%Shape: Grid [id:dp3375532844428595] 
		\draw  [draw opacity=0] (409,211) -- (489,211) -- (489,291) -- (409,291) -- cycle ; \draw   (429,211) -- (429,291)(449,211) -- (449,291)(469,211) -- (469,291) ; \draw   (409,231) -- (489,231)(409,251) -- (489,251)(409,271) -- (489,271) ; \draw   (409,211) -- (489,211) -- (489,291) -- (409,291) -- cycle ;
		%Shape: Square [id:dp2607102451969412] 
		\draw  [line width=2.25]  (409,251) -- (449,251) -- (449,291) -- (409,291) -- cycle ;
		%Shape: Grid [id:dp1463723413993303] 
		\draw  [draw opacity=0] (506,190) -- (546,190) -- (546,230) -- (506,230) -- cycle ; \draw   (526,190) -- (526,230) ; \draw   (506,210) -- (546,210) ; \draw   (506,190) -- (546,190) -- (546,230) -- (506,230) -- cycle ;
		%Straight Lines [id:da58531975097674] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (409,251) -- (506,190) ;
		%Straight Lines [id:da790018395720834] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (449,291) -- (546,230) ;
		%Shape: Grid [id:dp06539646276155575] 
		\draw  [draw opacity=0] (506,271) -- (526,271) -- (526,291) -- (506,291) -- cycle ; \draw    ; \draw    ; \draw   (506,271) -- (526,271) -- (526,291) -- (506,291) -- cycle ;
		%Shape: Grid [id:dp899633078277936] 
		\draw  [draw opacity=0] (576,226) -- (616,226) -- (616,266) -- (576,266) -- cycle ; \draw   (596,226) -- (596,266) ; \draw   (576,246) -- (616,246) ; \draw   (576,226) -- (616,226) -- (616,266) -- (576,266) -- cycle ;
		%Straight Lines [id:da3468277073593593] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (616,246) -- (546,190) ;
		%Straight Lines [id:da7386834638271753] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (596,266) -- (506,230) ;
		%Shape: Square [id:dp06476759411275479] 
		\draw  [line width=2.25]  (596,246) -- (616,246) -- (616,266) -- (596,266) -- cycle ;
		%Straight Lines [id:da37301098692697665] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (526,291) -- (616,266) ;
		%Straight Lines [id:da24582510489469933] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (506,271) -- (596,246) ;
		%Shape: Square [id:dp775231116129623] 
		\draw  [line width=2.25]  (264,226) -- (284,226) -- (284,246) -- (264,246) -- cycle ;
		
		% Text Node
		\draw (21,84) node [anchor=north west][inner sep=0.75pt]   [align=left] {Conv $\displaystyle 1$};
		% Text Node
		\draw (83.15,56.9) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (102.43,57.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (121.71,57.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (141,57.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (83.15,76.9) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (102.43,77.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (122,77.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (141,77.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (83.15,95.9) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (102,96.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (121.71,96.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (141,96.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (83,116.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (102,116.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (121.71,116.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (141,116.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (95,23) node [anchor=north west][inner sep=0.75pt]   [align=left] {Image};
		% Text Node
		\draw (178,13) node [anchor=north west][inner sep=0.75pt]   [align=left] {Filter};
		% Text Node
		\draw (174,36.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$-1$};
		% Text Node
		\draw (200,36.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$2$};
		% Text Node
		\draw (193,56.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$-2$};
		% Text Node
		\draw (180,56.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$4$};
		% Text Node
		\draw (179,117.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (169,95) node [anchor=north west][inner sep=0.75pt]   [align=left] {Bias};
		% Text Node
		\draw (250,72.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$4$};
		% Text Node
		\draw (221,34) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{75pt}\setlength\topsep{0pt}
		\begin{center}
		Output before\\nonlinearity
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (353,84) node [anchor=north west][inner sep=0.75pt]   [align=left] {Conv $\displaystyle 3$};
		% Text Node
		\draw (415.15,56.9) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (434.43,57.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (453.71,57.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (473,57.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (415.15,76.9) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (434.43,77.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (454,77.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (473,77.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (415.15,95.9) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (434,96.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (453.71,96.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (473,96.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (415,116.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (434,116.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (453.71,116.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (473,116.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (427,23) node [anchor=north west][inner sep=0.75pt]   [align=left] {Image};
		% Text Node
		\draw (510,13) node [anchor=north west][inner sep=0.75pt]   [align=left] {Filter};
		% Text Node
		\draw (506,36.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$-1$};
		% Text Node
		\draw (532,36.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$2$};
		% Text Node
		\draw (525,56.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$-2$};
		% Text Node
		\draw (512,56.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$4$};
		% Text Node
		\draw (511,117.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (501,95) node [anchor=north west][inner sep=0.75pt]   [align=left] {Bias};
		% Text Node
		\draw (582,72.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$4$};
		% Text Node
		\draw (602,72.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$7$};
		% Text Node
		\draw (581,92.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (553,34) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{75pt}\setlength\topsep{0pt}
		\begin{center}
		Output before\\nonlinearity
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (21,241) node [anchor=north west][inner sep=0.75pt]   [align=left] {Conv $\displaystyle 2$};
		% Text Node
		\draw (83.15,213.9) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (102.43,214.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (121.71,214.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (141,214.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (83.15,233.9) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (102.43,234.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (122,234.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (141,234.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (83.15,252.9) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (102,253.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (121.71,253.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (141,253.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (83,273.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (102,273.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (121.71,273.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (141,273.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (95,185) node [anchor=north west][inner sep=0.75pt]   [align=left] {Image};
		% Text Node
		\draw (178,170) node [anchor=north west][inner sep=0.75pt]   [align=left] {Filter};
		% Text Node
		\draw (174,193.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$-1$};
		% Text Node
		\draw (200,193.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$2$};
		% Text Node
		\draw (193,213.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$-2$};
		% Text Node
		\draw (180,213.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$4$};
		% Text Node
		\draw (179,274.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (169,252) node [anchor=north west][inner sep=0.75pt]   [align=left] {Bias};
		% Text Node
		\draw (250,229.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$4$};
		% Text Node
		\draw (221,191) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{75pt}\setlength\topsep{0pt}
		\begin{center}
		Output before\\nonlinearity
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (353,241) node [anchor=north west][inner sep=0.75pt]   [align=left] {Conv $\displaystyle 4$};
		% Text Node
		\draw (415.15,213.9) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (434.43,214.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (453.71,214.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (473,214.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (415.15,233.9) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (434.43,234.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (454,234.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (473,234.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (415.15,252.9) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (434,253.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (453.71,253.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (473,253.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (415,273.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (434,273.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (453.71,273.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (473,273.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (427,185) node [anchor=north west][inner sep=0.75pt]   [align=left] {Image};
		% Text Node
		\draw (510,170) node [anchor=north west][inner sep=0.75pt]   [align=left] {Filter};
		% Text Node
		\draw (506,193.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$-1$};
		% Text Node
		\draw (532,193.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$2$};
		% Text Node
		\draw (525,213.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$-2$};
		% Text Node
		\draw (512,213.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$4$};
		% Text Node
		\draw (511,274.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$1$};
		% Text Node
		\draw (501,252) node [anchor=north west][inner sep=0.75pt]   [align=left] {Bias};
		% Text Node
		\draw (581,229.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$4$};
		% Text Node
		\draw (603,229.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$7$};
		% Text Node
		\draw (581,249.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$0$};
		% Text Node
		\draw (553,191) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{75pt}\setlength\topsep{0pt}
		\begin{center}
		Output before\\nonlinearity
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (270,229.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$7$};
		% Text Node
		\draw (596,249.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$-1$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Convolution with stride $2$}
	\end{figure}
	
	
	The convolutional formula above with a spride $s$ becomes:
	
	Padding allows getting a larger output matrix (often, we want the output of a convolution to have the same size as the input\footnote{It's quite common to see convolution layers with stride of $1$, filters of size $k$, and zero padding of size $(k-1)/2$ to preserve size!}); it's the width of the square of additional cells with which we surround the image (or volume) before we convolve it with the filter. The cells added by padding usually contain zeroes. In the first example, the padding is $0$, so no additional cells are added to the image. In the figure below, on the other hand, the stride is $2$ and padding is $1$, so a square of width $1$ of additional cells are added to the image. We can see that the output matrix is bigger when padding is bigger (to save space only the first tow of the nine convolutions are shown).
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1467); %set diagram left start at 0, and has height of 1467
		
		%Shape: Grid [id:dp8938401336280799] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 217; green, 236; blue, 255 }  ,fill opacity=1 ] (71,62) -- (281,62) -- (281,272) -- (71,272) -- cycle ; \draw   (106,62) -- (106,272)(141,62) -- (141,272)(176,62) -- (176,272)(211,62) -- (211,272)(246,62) -- (246,272) ; \draw   (71,97) -- (281,97)(71,132) -- (281,132)(71,167) -- (281,167)(71,202) -- (281,202)(71,237) -- (281,237) ; \draw   (71,62) -- (281,62) -- (281,272) -- (71,272) -- cycle ;
		%Shape: Grid [id:dp004646777317475648] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (106,97) -- (246,97) -- (246,237) -- (106,237) -- cycle ; \draw   (141,97) -- (141,237)(176,97) -- (176,237)(211,97) -- (211,237) ; \draw   (106,132) -- (246,132)(106,167) -- (246,167)(106,202) -- (246,202) ; \draw   (106,97) -- (246,97) -- (246,237) -- (106,237) -- cycle ;
		%Shape: Grid [id:dp4861583084561023] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 217; green, 236; blue, 255 }  ,fill opacity=1 ] (71,323) -- (281,323) -- (281,533) -- (71,533) -- cycle ; \draw   (106,323) -- (106,533)(141,323) -- (141,533)(176,323) -- (176,533)(211,323) -- (211,533)(246,323) -- (246,533) ; \draw   (71,358) -- (281,358)(71,393) -- (281,393)(71,428) -- (281,428)(71,463) -- (281,463)(71,498) -- (281,498) ; \draw   (71,323) -- (281,323) -- (281,533) -- (71,533) -- cycle ;
		%Shape: Grid [id:dp9819482840061313] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (106,358) -- (246,358) -- (246,498) -- (106,498) -- cycle ; \draw   (141,358) -- (141,498)(176,358) -- (176,498)(211,358) -- (211,498) ; \draw   (106,393) -- (246,393)(106,428) -- (246,428)(106,463) -- (246,463) ; \draw   (106,358) -- (246,358) -- (246,498) -- (106,498) -- cycle ;
		%Shape: Square [id:dp38358868931369816] 
		\draw  [line width=2.25]  (71,62) -- (141,62) -- (141,132) -- (71,132) -- cycle ;
		%Shape: Square [id:dp3695036567363841] 
		\draw  [line width=2.25]  (141,323) -- (211,323) -- (211,393) -- (141,393) -- cycle ;
		%Shape: Grid [id:dp8627277937328537] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (340,27) -- (410,27) -- (410,97) -- (340,97) -- cycle ; \draw   (375,27) -- (375,97) ; \draw   (340,62) -- (410,62) ; \draw   (340,27) -- (410,27) -- (410,97) -- (340,97) -- cycle ;
		%Straight Lines [id:da1644804570302234] 
		\draw  [dash pattern={on 0.84pt off 2.51pt}]  (71,62) -- (340,27) ;
		%Straight Lines [id:da11017235227367639] 
		\draw  [dash pattern={on 0.84pt off 2.51pt}]  (141,132) -- (410,97) ;
		%Shape: Grid [id:dp04234716368527636] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (340,203) -- (375,203) -- (375,238) -- (340,238) -- cycle ; \draw    ; \draw    ; \draw   (340,203) -- (375,203) -- (375,238) -- (340,238) -- cycle ;
		%Shape: Grid [id:dp9317227314055798] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (465,96) -- (570,96) -- (570,201) -- (465,201) -- cycle ; \draw   (500,96) -- (500,201)(535,96) -- (535,201) ; \draw   (465,131) -- (570,131)(465,166) -- (570,166) ; \draw   (465,96) -- (570,96) -- (570,201) -- (465,201) -- cycle ;
		%Shape: Square [id:dp6015991795302433] 
		\draw  [line width=2.25]  (465,96) -- (500,96) -- (500,131) -- (465,131) -- cycle ;
		%Straight Lines [id:da9651157905211987] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 0.84pt off 2.51pt}]  (340,97) -- (465,131) ;
		%Straight Lines [id:da09281741016555811] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 0.84pt off 2.51pt}]  (410,27) -- (500,96) ;
		%Straight Lines [id:da9321476942002798] 
		\draw  [dash pattern={on 0.84pt off 2.51pt}]  (375,238) -- (500,131) ;
		%Straight Lines [id:da07032486806454652] 
		\draw  [dash pattern={on 0.84pt off 2.51pt}]  (340,203) -- (465,96) ;
		%Shape: Grid [id:dp4011505959701165] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (340,290) -- (410,290) -- (410,360) -- (340,360) -- cycle ; \draw   (375,290) -- (375,360) ; \draw   (340,325) -- (410,325) ; \draw   (340,290) -- (410,290) -- (410,360) -- (340,360) -- cycle ;
		%Straight Lines [id:da7712545419273373] 
		\draw  [dash pattern={on 0.84pt off 2.51pt}]  (141,323) -- (340,290) ;
		%Straight Lines [id:da7545611126907172] 
		\draw  [dash pattern={on 0.84pt off 2.51pt}]  (211,393) -- (410,360) ;
		%Shape: Grid [id:dp21507837778603323] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (340,466) -- (375,466) -- (375,501) -- (340,501) -- cycle ; \draw    ; \draw    ; \draw   (340,466) -- (375,466) -- (375,501) -- (340,501) -- cycle ;
		%Shape: Grid [id:dp41618819380299854] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (465,359) -- (570,359) -- (570,464) -- (465,464) -- cycle ; \draw   (500,359) -- (500,464)(535,359) -- (535,464) ; \draw   (465,394) -- (570,394)(465,429) -- (570,429) ; \draw   (465,359) -- (570,359) -- (570,464) -- (465,464) -- cycle ;
		%Shape: Square [id:dp6236085603610348] 
		\draw  [line width=2.25]  (500,359) -- (535,359) -- (535,394) -- (500,394) -- cycle ;
		%Straight Lines [id:da9138612881600507] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 0.84pt off 2.51pt}]  (340,360) -- (465,394) ;
		%Straight Lines [id:da6677428319364158] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 0.84pt off 2.51pt}]  (410,290) -- (500,359) ;
		%Straight Lines [id:da6599590684942596] 
		\draw  [dash pattern={on 0.84pt off 2.51pt}]  (375,501) -- (535,394) ;
		%Straight Lines [id:da34828702880524487] 
		\draw  [dash pattern={on 0.84pt off 2.51pt}]  (340,466) -- (500,359) ;
		
		% Text Node
		\draw (154.72,105.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (259.3,105.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (85,70.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (119.86,70.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (154.72,70.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (189.58,70.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (224.44,70.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (259.3,70.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (85,105.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (119.86,105.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (189.58,105.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (224.44,105.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (154.72,139.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (259.3,139.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (85,139.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (119.86,139.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (189.58,139.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (224.44,139.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (154.72,175.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (259.3,175.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (85,175.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (119.86,175.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (189.58,175.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (224.44,175.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (154.72,209.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (259.3,209.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (85,209.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (119.86,209.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (189.58,209.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (224.44,209.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (154.72,245.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (259.3,245.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (85,245.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (119.86,245.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (189.58,245.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (224.44,245.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (104,11) node [anchor=north west][inner sep=0.75pt]   [align=left] {Image with padding $\displaystyle 1$};
		% Text Node
		\draw (16,147) node [anchor=north west][inner sep=0.75pt]   [align=left] {Conv $\displaystyle 1$};
		% Text Node
		\draw (154.72,366.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (259.3,366.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (85,331.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (119.86,331.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (154.72,331.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (189.58,331.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (224.44,331.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (259.3,331.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (85,366.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (119.86,366.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (189.58,366.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (224.44,366.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (154.72,400.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (259.3,400.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (85,400.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (119.86,400.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (189.58,400.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (224.44,400.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (154.72,436.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (259.3,436.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (85,436.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (119.86,436.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (189.58,436.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (224.44,436.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (154.72,470.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (259.3,470.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (85,470.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (119.86,470.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (189.58,470.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (224.44,470.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (154.72,506.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (259.3,506.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (85,506.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (119.86,506.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (189.58,506.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (224.44,506.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (16,408) node [anchor=north west][inner sep=0.75pt]   [align=left] {Conv $\displaystyle 2$};
		% Text Node
		\draw (346,35.4) node [anchor=north west][inner sep=0.75pt]    {$-1$};
		% Text Node
		\draw (389,35.4) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (353,71.4) node [anchor=north west][inner sep=0.75pt]    {$4$};
		% Text Node
		\draw (381,71.4) node [anchor=north west][inner sep=0.75pt]    {$-2$};
		% Text Node
		\draw (357,6) node [anchor=north west][inner sep=0.75pt]   [align=left] {Filter};
		% Text Node
		\draw (339,180) node [anchor=north west][inner sep=0.75pt]   [align=left] {Bias};
		% Text Node
		\draw (353,212.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (472,105.4) node [anchor=north west][inner sep=0.75pt]    {$-1$};
		% Text Node
		\draw (469,57) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{75pt}\setlength\topsep{0pt}
		\begin{center}
		Output before\\nonlinearity
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (346,298.4) node [anchor=north west][inner sep=0.75pt]    {$-1$};
		% Text Node
		\draw (389,298.4) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (353,334.4) node [anchor=north west][inner sep=0.75pt]    {$4$};
		% Text Node
		\draw (381,334.4) node [anchor=north west][inner sep=0.75pt]    {$-2$};
		% Text Node
		\draw (357,269) node [anchor=north west][inner sep=0.75pt]   [align=left] {Filter};
		% Text Node
		\draw (339,443) node [anchor=north west][inner sep=0.75pt]   [align=left] {Bias};
		% Text Node
		\draw (353,475.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (472,368.4) node [anchor=north west][inner sep=0.75pt]    {$-1$};
		% Text Node
		\draw (469,320) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{75pt}\setlength\topsep{0pt}
		\begin{center}
		Output before\\nonlinearity
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (511,368.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[]{Convolution with stride $2$}
	\end{figure}
	An example of an image with padding $2$ is shown below. Padding is helpful with larger filters because it allows them to better scan the boundaries of the image:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1467); %set diagram left start at 0, and has height of 1467
		
		%Shape: Grid [id:dp8938401336280799] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 217; green, 236; blue, 255 }  ,fill opacity=1 ] (175,15) -- (455,15) -- (455,295) -- (175,295) -- cycle ; \draw   (210,15) -- (210,295)(245,15) -- (245,295)(280,15) -- (280,295)(315,15) -- (315,295)(350,15) -- (350,295)(385,15) -- (385,295)(420,15) -- (420,295) ; \draw   (175,50) -- (455,50)(175,85) -- (455,85)(175,120) -- (455,120)(175,155) -- (455,155)(175,190) -- (455,190)(175,225) -- (455,225)(175,260) -- (455,260) ; \draw   (175,15) -- (455,15) -- (455,295) -- (175,295) -- cycle ;
		%Shape: Grid [id:dp004646777317475648] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (245,85) -- (385,85) -- (385,225) -- (245,225) -- cycle ; \draw   (280,85) -- (280,225)(315,85) -- (315,225)(350,85) -- (350,225) ; \draw   (245,120) -- (385,120)(245,155) -- (385,155)(245,190) -- (385,190) ; \draw   (245,85) -- (385,85) -- (385,225) -- (245,225) -- cycle ;
		
		% Text Node
		\draw (257.72,94.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (362.3,94.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (327,128.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (257,128.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (256.72,163.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (291.58,163.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (291.58,199.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (361.3,199.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (187,25.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (221.86,25.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (256.72,25.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (291.58,25.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (326.44,25.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (361.3,25.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (396.16,25.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (431,25.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (187,58.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (221.86,58.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (256.72,58.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (291.58,58.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (326.44,58.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (361.3,58.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (396.16,58.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (431,58.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (188,94.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (222.86,94.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (292.58,94.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (327.44,94.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (397.16,94.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (432,94.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (187,128.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (221.86,128.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (291.58,128.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (361.3,128.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (396.16,128.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (431,128.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (187,163.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (221.86,163.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (326.44,163.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (361.3,163.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (396.16,163.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (431,163.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (187,199.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (221.86,199.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (256.72,199.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (326.44,199.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (396.16,199.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (431,199.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (187,234.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (221.86,234.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (256.72,234.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (291.58,234.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (326.44,234.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (361.3,234.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (396.16,234.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (431,234.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (187,269.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (221.86,269.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (256.72,269.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (291.58,269.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (326.44,269.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (361.3,269.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (396.16,269.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (431,269.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[]{Image with padding $2$}
	\end{figure}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	There are two types of results to the operation — one in which the convolved feature is reduced in dimensionality as compared to the input, and the other in which the dimensionality is either increased or remains the same. This is done by applying "\NewTerm{Valid Padding}" in case of the former, or "\NewTerm{Same Padding}" in the case of the latter.
	\end{tcolorbox}
	
	This section would not be complete without presenting "\NewTerm{pooling}", a technique very often used in CNNs. Pooling works in a way very similar to convolution, as a filter applied using a moving window approach. However, instead of applying a trainable filter to an input matrix or a volume, pooling layer applies a fixed operator, usually either max ("\NewTerm{Max pooling}") or average ("\NewTerm{Average pooling}"). Similarly to convolution, pooling has hyperparameters: the size of the filter, the stride and the type of pooling.
	
	Mainly the purpose of pooling is to decrease the computational power required to process the data through dimensionality reduction. Furthermore, it is useful for extracting dominant features which are rotational and positional invariant, thus maintaining the process of effectively training of the model.
	
	 An example of max pooling with filter of size $2$ and stride $2$ is shown below:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1215); %set diagram left start at 0, and has height of 1215
		
		%Shape: Grid [id:dp010658240380345463] 
		\draw  [draw opacity=0] (55,46) -- (199,46) -- (199,190) -- (55,190) -- cycle ; \draw   (91,46) -- (91,190)(127,46) -- (127,190)(163,46) -- (163,190) ; \draw   (55,82) -- (199,82)(55,118) -- (199,118)(55,154) -- (199,154) ; \draw   (55,46) -- (199,46) -- (199,190) -- (55,190) -- cycle ;
		%Shape: Grid [id:dp9493095751525764] 
		\draw  [draw opacity=0] (232,82) -- (304,82) -- (304,154) -- (232,154) -- cycle ; \draw   (268,82) -- (268,154) ; \draw   (232,118) -- (304,118) ; \draw   (232,82) -- (304,82) -- (304,154) -- (232,154) -- cycle ;
		%Shape: Grid [id:dp6873760260955286] 
		\draw  [draw opacity=0] (362,45) -- (506,45) -- (506,189) -- (362,189) -- cycle ; \draw   (398,45) -- (398,189)(434,45) -- (434,189)(470,45) -- (470,189) ; \draw   (362,81) -- (506,81)(362,117) -- (506,117)(362,153) -- (506,153) ; \draw   (362,45) -- (506,45) -- (506,189) -- (362,189) -- cycle ;
		%Shape: Grid [id:dp3863126071138139] 
		\draw  [draw opacity=0] (539,81) -- (611,81) -- (611,153) -- (539,153) -- cycle ; \draw   (575,81) -- (575,153) ; \draw   (539,117) -- (611,117) ; \draw   (539,81) -- (611,81) -- (611,153) -- (539,153) -- cycle ;
		%Shape: Grid [id:dp7251956040379877] 
		\draw  [draw opacity=0] (55,241) -- (199,241) -- (199,385) -- (55,385) -- cycle ; \draw   (91,241) -- (91,385)(127,241) -- (127,385)(163,241) -- (163,385) ; \draw   (55,277) -- (199,277)(55,313) -- (199,313)(55,349) -- (199,349) ; \draw   (55,241) -- (199,241) -- (199,385) -- (55,385) -- cycle ;
		%Shape: Grid [id:dp7540541769568281] 
		\draw  [draw opacity=0] (232,277) -- (304,277) -- (304,349) -- (232,349) -- cycle ; \draw   (268,277) -- (268,349) ; \draw   (232,313) -- (304,313) ; \draw   (232,277) -- (304,277) -- (304,349) -- (232,349) -- cycle ;
		%Shape: Grid [id:dp756835617092726] 
		\draw  [draw opacity=0] (362,241) -- (506,241) -- (506,385) -- (362,385) -- cycle ; \draw   (398,241) -- (398,385)(434,241) -- (434,385)(470,241) -- (470,385) ; \draw   (362,277) -- (506,277)(362,313) -- (506,313)(362,349) -- (506,349) ; \draw   (362,241) -- (506,241) -- (506,385) -- (362,385) -- cycle ;
		%Shape: Grid [id:dp20845084453634777] 
		\draw  [draw opacity=0] (539,277) -- (611,277) -- (611,349) -- (539,349) -- cycle ; \draw   (575,277) -- (575,349) ; \draw   (539,313) -- (611,313) ; \draw   (539,277) -- (611,277) -- (611,349) -- (539,349) -- cycle ;
		%Shape: Square [id:dp8860797547190538] 
		\draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ][line width=2.25]  (55,47) -- (126,47) -- (126,118) -- (55,118) -- cycle ;
		%Shape: Square [id:dp6090754019687543] 
		\draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ][line width=2.25]  (363,117) -- (434,117) -- (434,188) -- (363,188) -- cycle ;
		%Shape: Square [id:dp9583633801161597] 
		\draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ][line width=2.25]  (128,241) -- (199,241) -- (199,312) -- (128,312) -- cycle ;
		%Shape: Square [id:dp5731160266506985] 
		\draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ][line width=2.25]  (434,313) -- (505,313) -- (505,384) -- (434,384) -- cycle ;
		%Shape: Square [id:dp1606179173703357] 
		\draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ][line width=2.25]  (231.5,81.5) -- (268,81.5) -- (268,118) -- (231.5,118) -- cycle ;
		%Shape: Square [id:dp6134824459416495] 
		\draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ][line width=2.25]  (539,116.5) -- (575.5,116.5) -- (575.5,153) -- (539,153) -- cycle ;
		%Shape: Square [id:dp3672588628185087] 
		\draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ][line width=2.25]  (268,277) -- (304.5,277) -- (304.5,313.5) -- (268,313.5) -- cycle ;
		%Shape: Square [id:dp06908566630889523] 
		\draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ][line width=2.25]  (574.5,313) -- (611,313) -- (611,349.5) -- (574.5,349.5) -- cycle ;
		%Straight Lines [id:da9489852449301255] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (126,47) -- (268,81.5) ;
		%Straight Lines [id:da1377404226889163] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (126,118) -- (232,118) ;
		%Straight Lines [id:da5607012471328168] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (434,117) -- (540,117) ;
		%Straight Lines [id:da5196639050910867] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (434,188) -- (575,153) ;
		%Straight Lines [id:da572399607413705] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (199,241) -- (304,277) ;
		%Straight Lines [id:da9069946991161228] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (199,313) -- (268,313) ;
		%Straight Lines [id:da3376958818173563] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (505,313) -- (574.5,313) ;
		%Straight Lines [id:da5079772537174256] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (436.5,385) -- (611,349) ;
		
		% Text Node
		\draw (67.5,54.9) node [anchor=north west][inner sep=0.75pt]    {$3$};
		% Text Node
		\draw (101.5,54.9) node [anchor=north west][inner sep=0.75pt]    {$8$};
		% Text Node
		\draw (138.5,54.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (176.5,54.9) node [anchor=north west][inner sep=0.75pt]    {$4$};
		% Text Node
		\draw (67.5,91.4) node [anchor=north west][inner sep=0.75pt]    {$5$};
		% Text Node
		\draw (101.5,91.4) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (138.5,91.4) node [anchor=north west][inner sep=0.75pt]    {$6$};
		% Text Node
		\draw (172,91.4) node [anchor=north west][inner sep=0.75pt]    {$-1$};
		% Text Node
		\draw (63,128.4) node [anchor=north west][inner sep=0.75pt]    {$-3$};
		% Text Node
		\draw (101.5,128.4) node [anchor=north west][inner sep=0.75pt]    {$5$};
		% Text Node
		\draw (138.5,128.4) node [anchor=north west][inner sep=0.75pt]    {$9$};
		% Text Node
		\draw (176.5,128.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (67.5,164.4) node [anchor=north west][inner sep=0.75pt]    {$4$};
		% Text Node
		\draw (101.5,164.4) node [anchor=north west][inner sep=0.75pt]    {$5$};
		% Text Node
		\draw (138.5,164.4) node [anchor=north west][inner sep=0.75pt]    {$7$};
		% Text Node
		\draw (176.5,164.4) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (243.5,90.9) node [anchor=north west][inner sep=0.75pt]    {$8$};
		% Text Node
		\draw (374.5,53.9) node [anchor=north west][inner sep=0.75pt]    {$3$};
		% Text Node
		\draw (408.5,53.9) node [anchor=north west][inner sep=0.75pt]    {$8$};
		% Text Node
		\draw (445.5,53.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (483.5,53.9) node [anchor=north west][inner sep=0.75pt]    {$4$};
		% Text Node
		\draw (374.5,90.4) node [anchor=north west][inner sep=0.75pt]    {$5$};
		% Text Node
		\draw (408.5,90.4) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (445.5,90.4) node [anchor=north west][inner sep=0.75pt]    {$6$};
		% Text Node
		\draw (479,90.4) node [anchor=north west][inner sep=0.75pt]    {$-1$};
		% Text Node
		\draw (370,127.4) node [anchor=north west][inner sep=0.75pt]    {$-3$};
		% Text Node
		\draw (408.5,127.4) node [anchor=north west][inner sep=0.75pt]    {$5$};
		% Text Node
		\draw (445.5,127.4) node [anchor=north west][inner sep=0.75pt]    {$9$};
		% Text Node
		\draw (483.5,127.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (374.5,163.4) node [anchor=north west][inner sep=0.75pt]    {$4$};
		% Text Node
		\draw (408.5,163.4) node [anchor=north west][inner sep=0.75pt]    {$5$};
		% Text Node
		\draw (445.5,163.4) node [anchor=north west][inner sep=0.75pt]    {$7$};
		% Text Node
		\draw (483.5,163.4) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (552.5,125.9) node [anchor=north west][inner sep=0.75pt]    {$5$};
		% Text Node
		\draw (67.5,249.9) node [anchor=north west][inner sep=0.75pt]    {$3$};
		% Text Node
		\draw (101.5,249.9) node [anchor=north west][inner sep=0.75pt]    {$8$};
		% Text Node
		\draw (138.5,249.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (176.5,249.9) node [anchor=north west][inner sep=0.75pt]    {$4$};
		% Text Node
		\draw (67.5,286.4) node [anchor=north west][inner sep=0.75pt]    {$5$};
		% Text Node
		\draw (101.5,286.4) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (138.5,286.4) node [anchor=north west][inner sep=0.75pt]    {$6$};
		% Text Node
		\draw (172,286.4) node [anchor=north west][inner sep=0.75pt]    {$-1$};
		% Text Node
		\draw (63,323.4) node [anchor=north west][inner sep=0.75pt]    {$-3$};
		% Text Node
		\draw (101.5,323.4) node [anchor=north west][inner sep=0.75pt]    {$5$};
		% Text Node
		\draw (138.5,323.4) node [anchor=north west][inner sep=0.75pt]    {$9$};
		% Text Node
		\draw (176.5,323.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (67.5,359.4) node [anchor=north west][inner sep=0.75pt]    {$4$};
		% Text Node
		\draw (101.5,359.4) node [anchor=north west][inner sep=0.75pt]    {$5$};
		% Text Node
		\draw (138.5,359.4) node [anchor=north west][inner sep=0.75pt]    {$7$};
		% Text Node
		\draw (176.5,359.4) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (245.5,285.9) node [anchor=north west][inner sep=0.75pt]    {$8$};
		% Text Node
		\draw (374.5,249.9) node [anchor=north west][inner sep=0.75pt]    {$3$};
		% Text Node
		\draw (408.5,249.9) node [anchor=north west][inner sep=0.75pt]    {$8$};
		% Text Node
		\draw (445.5,249.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (483.5,249.9) node [anchor=north west][inner sep=0.75pt]    {$4$};
		% Text Node
		\draw (374.5,286.4) node [anchor=north west][inner sep=0.75pt]    {$5$};
		% Text Node
		\draw (408.5,286.4) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (445.5,286.4) node [anchor=north west][inner sep=0.75pt]    {$6$};
		% Text Node
		\draw (479,286.4) node [anchor=north west][inner sep=0.75pt]    {$-1$};
		% Text Node
		\draw (370,323.4) node [anchor=north west][inner sep=0.75pt]    {$-3$};
		% Text Node
		\draw (408.5,323.4) node [anchor=north west][inner sep=0.75pt]    {$5$};
		% Text Node
		\draw (445.5,323.4) node [anchor=north west][inner sep=0.75pt]    {$9$};
		% Text Node
		\draw (483.5,323.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (374.5,359.4) node [anchor=north west][inner sep=0.75pt]    {$4$};
		% Text Node
		\draw (408.5,359.4) node [anchor=north west][inner sep=0.75pt]    {$5$};
		% Text Node
		\draw (445.5,359.4) node [anchor=north west][inner sep=0.75pt]    {$7$};
		% Text Node
		\draw (483.5,359.4) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (281.5,285.9) node [anchor=north west][inner sep=0.75pt]    {$6$};
		% Text Node
		\draw (551.5,89.9) node [anchor=north west][inner sep=0.75pt]    {$8$};
		% Text Node
		\draw (587.5,89.9) node [anchor=north west][inner sep=0.75pt]    {$6$};
		% Text Node
		\draw (550.5,322.9) node [anchor=north west][inner sep=0.75pt]    {$5$};
		% Text Node
		\draw (549.5,286.9) node [anchor=north west][inner sep=0.75pt]    {$8$};
		% Text Node
		\draw (585.5,286.9) node [anchor=north west][inner sep=0.75pt]    {$6$};
		% Text Node
		\draw (586.5,323.9) node [anchor=north west][inner sep=0.75pt]    {$9$};
		% Text Node
		\draw (7,109) node [anchor=north west][inner sep=0.75pt]   [align=left] {Pool $\displaystyle 1$};
		% Text Node
		\draw (314,108) node [anchor=north west][inner sep=0.75pt]   [align=left] {Pool $\displaystyle 3$};
		% Text Node
		\draw (7,304) node [anchor=north west][inner sep=0.75pt]   [align=left] {Pool $\displaystyle 2$};
		% Text Node
		\draw (315,306) node [anchor=north west][inner sep=0.75pt]   [align=left] {Pool $\displaystyle 4$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[]{Pooling with filter of size $2$ and stride $2$}
	\end{figure}
	Or an overview of a max pooling versus an average pooling:
	\begin{figure}[H]
			\centering
			\includegraphics[scale=1]{img/computing/max_average_pooling.pdf}
			\vspace*{3mm}
			\caption[$2\times 2$ Average vs Max pooling  of stride $2$]{$2\times 2$ Average vs Max pooling  of stride $2$ (author: Simon Dispa)}
		\end{figure}	
	
	The Convolutional Layer and the Pooling Layer, together form the $i$-th layer of a Convolutional Neural Network. 
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,970); %set diagram left start at 0, and has height of 970
		
		%Shape: Rectangle [id:dp9130358935189962] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]  (28,33) -- (78.5,33) -- (78.5,379) -- (28,379) -- cycle ;
		%Shape: Rectangle [id:dp12389935850321865] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ][line width=1.5]  (96,143) -- (172.5,143) -- (172.5,238) -- (96,238) -- cycle ;
		%Shape: Rectangle [id:dp31365042300800594] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ][line width=1.5]  (108,155) -- (184.5,155) -- (184.5,250) -- (108,250) -- cycle ;
		%Shape: Rectangle [id:dp7099690994056924] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ][line width=1.5]  (120,167) -- (196.5,167) -- (196.5,262) -- (120,262) -- cycle ;
		%Shape: Rectangle [id:dp07253372570354144] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ][line width=1.5]  (234,99) -- (278.5,99) -- (278.5,312.79) -- (234,312.79) -- cycle ;
		%Shape: Rectangle [id:dp2692133066835345] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ][line width=1.5]  (247,108.5) -- (291.5,108.5) -- (291.5,322.29) -- (247,322.29) -- cycle ;
		%Shape: Rectangle [id:dp48832415540649854] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ][line width=1.5]  (260,117.21) -- (304.5,117.21) -- (304.5,331) -- (260,331) -- cycle ;
		%Shape: Circle [id:dp3813171916099598] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]  (347,67.5) .. controls (347,56.73) and (355.73,48) .. (366.5,48) .. controls (377.27,48) and (386,56.73) .. (386,67.5) .. controls (386,78.27) and (377.27,87) .. (366.5,87) .. controls (355.73,87) and (347,78.27) .. (347,67.5) -- cycle ;
		%Shape: Circle [id:dp38045040517681583] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]  (347,120.1) .. controls (347,109.33) and (355.73,100.6) .. (366.5,100.6) .. controls (377.27,100.6) and (386,109.33) .. (386,120.1) .. controls (386,130.87) and (377.27,139.6) .. (366.5,139.6) .. controls (355.73,139.6) and (347,130.87) .. (347,120.1) -- cycle ;
		%Shape: Circle [id:dp18968428566261508] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]  (347,172.7) .. controls (347,161.93) and (355.73,153.2) .. (366.5,153.2) .. controls (377.27,153.2) and (386,161.93) .. (386,172.7) .. controls (386,183.47) and (377.27,192.2) .. (366.5,192.2) .. controls (355.73,192.2) and (347,183.47) .. (347,172.7) -- cycle ;
		%Shape: Circle [id:dp9768563507321626] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]  (347,225.3) .. controls (347,214.53) and (355.73,205.8) .. (366.5,205.8) .. controls (377.27,205.8) and (386,214.53) .. (386,225.3) .. controls (386,236.07) and (377.27,244.8) .. (366.5,244.8) .. controls (355.73,244.8) and (347,236.07) .. (347,225.3) -- cycle ;
		%Shape: Circle [id:dp11833328657241671] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]  (347,360.5) .. controls (347,349.73) and (355.73,341) .. (366.5,341) .. controls (377.27,341) and (386,349.73) .. (386,360.5) .. controls (386,371.27) and (377.27,380) .. (366.5,380) .. controls (355.73,380) and (347,371.27) .. (347,360.5) -- cycle ;
		%Straight Lines [id:da18075219857485703] 
		\draw [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ][line width=1.5]  [dash pattern={on 1.69pt off 2.76pt}]  (366.5,258.4) -- (366.5,327.4) ;
		%Shape: Circle [id:dp12629973276035455] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]  (431,86.5) .. controls (431,75.73) and (439.73,67) .. (450.5,67) .. controls (461.27,67) and (470,75.73) .. (470,86.5) .. controls (470,97.27) and (461.27,106) .. (450.5,106) .. controls (439.73,106) and (431,97.27) .. (431,86.5) -- cycle ;
		%Shape: Circle [id:dp6401023113145055] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]  (431,139.5) .. controls (431,128.73) and (439.73,120) .. (450.5,120) .. controls (461.27,120) and (470,128.73) .. (470,139.5) .. controls (470,150.27) and (461.27,159) .. (450.5,159) .. controls (439.73,159) and (431,150.27) .. (431,139.5) -- cycle ;
		%Shape: Circle [id:dp8555739697981344] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]  (431,192.5) .. controls (431,181.73) and (439.73,173) .. (450.5,173) .. controls (461.27,173) and (470,181.73) .. (470,192.5) .. controls (470,203.27) and (461.27,212) .. (450.5,212) .. controls (439.73,212) and (431,203.27) .. (431,192.5) -- cycle ;
		%Shape: Circle [id:dp6219103855533403] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]  (431,328.5) .. controls (431,317.73) and (439.73,309) .. (450.5,309) .. controls (461.27,309) and (470,317.73) .. (470,328.5) .. controls (470,339.27) and (461.27,348) .. (450.5,348) .. controls (439.73,348) and (431,339.27) .. (431,328.5) -- cycle ;
		%Straight Lines [id:da1070426364122028] 
		\draw [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ][line width=1.5]  [dash pattern={on 1.69pt off 2.76pt}]  (450.5,226) -- (450.5,295) ;
		%Shape: Circle [id:dp6398393751048268] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]  (531.5,116) .. controls (531.5,106.61) and (539.11,99) .. (548.5,99) .. controls (557.89,99) and (565.5,106.61) .. (565.5,116) .. controls (565.5,125.39) and (557.89,133) .. (548.5,133) .. controls (539.11,133) and (531.5,125.39) .. (531.5,116) -- cycle ;
		%Shape: Circle [id:dp05098410656577346] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]  (531.5,163) .. controls (531.5,153.61) and (539.11,146) .. (548.5,146) .. controls (557.89,146) and (565.5,153.61) .. (565.5,163) .. controls (565.5,172.39) and (557.89,180) .. (548.5,180) .. controls (539.11,180) and (531.5,172.39) .. (531.5,163) -- cycle ;
		%Shape: Circle [id:dp16210738209856235] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]  (531.5,210) .. controls (531.5,200.61) and (539.11,193) .. (548.5,193) .. controls (557.89,193) and (565.5,200.61) .. (565.5,210) .. controls (565.5,219.39) and (557.89,227) .. (548.5,227) .. controls (539.11,227) and (531.5,219.39) .. (531.5,210) -- cycle ;
		%Shape: Circle [id:dp005653539852236822] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]  (531.5,257) .. controls (531.5,247.61) and (539.11,240) .. (548.5,240) .. controls (557.89,240) and (565.5,247.61) .. (565.5,257) .. controls (565.5,266.39) and (557.89,274) .. (548.5,274) .. controls (539.11,274) and (531.5,266.39) .. (531.5,257) -- cycle ;
		%Shape: Circle [id:dp4062733798158058] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]  (531.5,304) .. controls (531.5,294.61) and (539.11,287) .. (548.5,287) .. controls (557.89,287) and (565.5,294.61) .. (565.5,304) .. controls (565.5,313.39) and (557.89,321) .. (548.5,321) .. controls (539.11,321) and (531.5,313.39) .. (531.5,304) -- cycle ;
		%Straight Lines [id:da9938458380171247] 
		\draw    (470,86.5) -- (529.7,115.14) ;
		\draw [shift={(531.5,116)}, rotate = 205.63] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da9878279271198143] 
		\draw    (470,139.5) -- (529.63,116.71) ;
		\draw [shift={(531.5,116)}, rotate = 159.09] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da8736874272335233] 
		\draw    (470,139.5) -- (529.63,162.29) ;
		\draw [shift={(531.5,163)}, rotate = 200.91] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da8914960783409749] 
		\draw    (470,192.5) -- (530.25,117.56) ;
		\draw [shift={(531.5,116)}, rotate = 128.8] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da42001329510780727] 
		\draw    (470,192.5) -- (529.7,163.86) ;
		\draw [shift={(531.5,163)}, rotate = 154.37] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da9447480397069001] 
		\draw    (470,328.5) -- (530.94,117.92) ;
		\draw [shift={(531.5,116)}, rotate = 106.14] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da1767384190523993] 
		\draw    (470,328.5) -- (530.8,164.87) ;
		\draw [shift={(531.5,163)}, rotate = 110.39] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da7997840000963699] 
		\draw    (565.5,116) -- (621.5,116) ;
		\draw [shift={(623.5,116)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da9511675969970301] 
		\draw    (565.5,163) -- (621.5,163) ;
		\draw [shift={(623.5,163)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da08809465963767593] 
		\draw    (565.5,210) -- (621.5,210) ;
		\draw [shift={(623.5,210)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da47275123336213243] 
		\draw    (565.5,257) -- (621.5,257) ;
		\draw [shift={(623.5,257)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da7597854798035693] 
		\draw    (565.5,304) -- (621.5,304) ;
		\draw [shift={(623.5,304)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		
		% Text Node
		\draw (13,386) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {\begin{minipage}[lt]{65pt}\setlength\topsep{0pt}
		\begin{center}
		input volume\\$\displaystyle 32\times 32\times 1$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (85,106) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {\begin{minipage}[lt]{95pt}\setlength\topsep{0pt}
		\begin{center}
		ReLU activation fn.\\volume: $\displaystyle 28\times 28\times 3$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (109,272) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {\begin{minipage}[lt]{65pt}\setlength\topsep{0pt}
		\begin{center}
		convolution\\layer stride 1
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (219,59) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {\begin{minipage}[lt]{70pt}\setlength\topsep{0pt}
		\begin{center}
		output volume\\$\displaystyle 14\times 14\times 3$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (232,341) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {\begin{minipage}[lt]{65pt}\setlength\topsep{0pt}
		\begin{center}
		max pool\\layer stride 2
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (325,14) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {\begin{minipage}[lt]{70pt}\setlength\topsep{0pt}
		\begin{center}
		output volume\\$\displaystyle 588\times 1$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (409,31) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {\begin{minipage}[lt]{70pt}\setlength\topsep{0pt}
		\begin{center}
		output volume\\$\displaystyle 20\times 1$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (509,61) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {\begin{minipage}[lt]{65pt}\setlength\topsep{0pt}
		\begin{center}
		output nodes\\$\displaystyle 5\times 1$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (328,391) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {\begin{minipage}[lt]{60pt}\setlength\topsep{0pt}
		\begin{center}
		flatten layer
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (388,355) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {\begin{minipage}[lt]{100pt}\setlength\topsep{0pt}
		\begin{center}
		fully connected\\layer ReLU activation\\fn.
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (492,328) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {\begin{minipage}[lt]{90pt}\setlength\topsep{0pt}
		\begin{center}
		Soft-max activation\\fn. layer
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (577,98) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {\begin{minipage}[lt]{33.43pt}\setlength\topsep{0pt}
		\begin{center}
		Class $\displaystyle 1$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (577,144) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {\begin{minipage}[lt]{33.43pt}\setlength\topsep{0pt}
		\begin{center}
		Class $\displaystyle 2$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (577,190) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {\begin{minipage}[lt]{33.43pt}\setlength\topsep{0pt}
		\begin{center}
		Class $\displaystyle 3$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (577,237) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {\begin{minipage}[lt]{33.43pt}\setlength\topsep{0pt}
		\begin{center}
		Class $\displaystyle 4$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (577,284) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {\begin{minipage}[lt]{33.43pt}\setlength\topsep{0pt}
		\begin{center}
		Class $\displaystyle 5$
		\end{center}
		
		\end{minipage}};
		\end{tikzpicture}
	\end{figure}
	Usually, a pooling layer follows a convolution layer, and it gets the output of convolution as input. When pooling is applied to a volume, each matrix in the volume is processed independently of others. Therefore, the output of the pooling layer applied to a volume of the same depth as the input. 
	
	Depending on the complexities in the images, the number of such layers may be increased for capturing low-levels details even further, but at the cost of more computational power.
	
	We may have noticed that we create quite a lot of images by running our input image through many different filters. Even though the images size will slightly decrease with each filter in each layer, we generally generate exponentially more and more data with each convolutional layer we add. To combat this issue, we use a process named MaxPooling: After filtering an image, we will reduce its size drastically by unifying a pixel neighbourhood to one single value. Most prominently, we use MaxPooling, meaning we take the maximum pixel value of a pixel neighbourhood, but we could also use other methods like MinPolling, AvgPolling or MedianPolling. 
	
	After going through the above process, we have successfully enabled the model to understand the features. Moving on, we are going to flatten the final output and feed it to a regular Neural Network for classification purposes.
	
	In the example below depicting a complete forward pass:
	\begin{enumerate}
		\item We start with an input image of size $5\times 5$.
		
		\item We then apply convolution using $2\times 2$ kernel and stride $=1$, that produces feature map of size $4\times 4$.
		
		\item We then apply a $2\times 2$ max-pooling with stride $2$, that reduces feature map to size $2\times 2$.
		
		\item We then apply a logistic sigmoid.
		
		\item Then one fully connected layer with two neurons.
		
		\item And an output layer
	\end{enumerate}
	\begin{figure}[H]
		\centering
		\resizebox{\textwidth}{!}{\contourlength{2pt}
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1500); %set diagram left start at 0, and has height of 1500
		
		%Shape: Square [id:dp14054757702228704] 
		\draw   (15,103.5) -- (91,103.5) -- (91,179.5) -- (15,179.5) -- cycle ;
		%Right Arrow [id:dp6578278476296369] 
		\draw   (95.5,134.25) -- (125.5,134.25) -- (125.5,127) -- (145.5,141.5) -- (125.5,156) -- (125.5,148.75) -- (95.5,148.75) -- cycle ;
		%Shape: Square [id:dp524689751402089] 
		\draw   (148,103.5) -- (224,103.5) -- (224,179.5) -- (148,179.5) -- cycle ;
		%Right Arrow [id:dp5530173771990676] 
		\draw   (226.5,134.25) -- (252.9,134.25) -- (252.9,127) -- (270.5,141.5) -- (252.9,156) -- (252.9,148.75) -- (226.5,148.75) -- cycle ;
		%Shape: Rectangle [id:dp10813571201478167] 
		\draw   (374.5,45.33) -- (420,45.33) -- (420,244.33) -- (374.5,244.33) -- cycle ;
		%Right Arrow [id:dp610729800517944] 
		\draw   (328.5,134.25) -- (354.9,134.25) -- (354.9,127) -- (372.5,141.5) -- (354.9,156) -- (354.9,148.75) -- (328.5,148.75) -- cycle ;
		%Shape: Circle [id:dp6776267874220732] 
		\draw   (375,72.25) .. controls (375,59.96) and (384.96,50) .. (397.25,50) .. controls (409.54,50) and (419.5,59.96) .. (419.5,72.25) .. controls (419.5,84.54) and (409.54,94.5) .. (397.25,94.5) .. controls (384.96,94.5) and (375,84.54) .. (375,72.25) -- cycle ;
		%Curve Lines [id:da2645556472992445] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (382.12,83.82) .. controls (406.6,83.82) and (390.58,58.9) .. (411.05,59.79) ;
		
		%Shape: Circle [id:dp8741792632151841] 
		\draw   (375,120.58) .. controls (375,108.29) and (384.96,98.33) .. (397.25,98.33) .. controls (409.54,98.33) and (419.5,108.29) .. (419.5,120.58) .. controls (419.5,132.87) and (409.54,142.83) .. (397.25,142.83) .. controls (384.96,142.83) and (375,132.87) .. (375,120.58) -- cycle ;
		%Curve Lines [id:da025956189585911726] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (382.12,132.15) .. controls (406.6,132.15) and (390.58,107.23) .. (411.05,108.12) ;
		
		%Shape: Circle [id:dp4869187194313609] 
		\draw   (375,168.91) .. controls (375,156.62) and (384.96,146.66) .. (397.25,146.66) .. controls (409.54,146.66) and (419.5,156.62) .. (419.5,168.91) .. controls (419.5,181.2) and (409.54,191.16) .. (397.25,191.16) .. controls (384.96,191.16) and (375,181.2) .. (375,168.91) -- cycle ;
		%Curve Lines [id:da3104807517973438] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (382.12,180.48) .. controls (406.6,180.48) and (390.58,155.56) .. (411.05,156.45) ;
		
		%Shape: Circle [id:dp6546408493339482] 
		\draw   (375,217.25) .. controls (375,204.96) and (384.96,195) .. (397.25,195) .. controls (409.54,195) and (419.5,204.96) .. (419.5,217.25) .. controls (419.5,229.54) and (409.54,239.5) .. (397.25,239.5) .. controls (384.96,239.5) and (375,229.54) .. (375,217.25) -- cycle ;
		%Curve Lines [id:da46603982025712454] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (382.12,228.82) .. controls (406.6,228.82) and (390.58,203.9) .. (411.05,204.79) ;
		
		%Shape: Circle [id:dp46255227241615793] 
		\draw   (473.5,35.5) .. controls (473.5,18.1) and (487.6,4) .. (505,4) .. controls (522.4,4) and (536.5,18.1) .. (536.5,35.5) .. controls (536.5,52.9) and (522.4,67) .. (505,67) .. controls (487.6,67) and (473.5,52.9) .. (473.5,35.5) -- cycle ;
		%Shape: Circle [id:dp7670002956649575] 
		\draw   (473.5,253.5) .. controls (473.5,236.1) and (487.6,222) .. (505,222) .. controls (522.4,222) and (536.5,236.1) .. (536.5,253.5) .. controls (536.5,270.9) and (522.4,285) .. (505,285) .. controls (487.6,285) and (473.5,270.9) .. (473.5,253.5) -- cycle ;
		%Straight Lines [id:da10247951928617849] 
		\draw    (419.5,72.25) -- (471.85,36.63) ;
		\draw [shift={(473.5,35.5)}, rotate = 145.76] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da6144682783784836] 
		\draw    (419.5,120.58) -- (472.43,37.19) ;
		\draw [shift={(473.5,35.5)}, rotate = 122.4] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da07964750767521811] 
		\draw    (419.5,168.91) -- (472.75,37.35) ;
		\draw [shift={(473.5,35.5)}, rotate = 112.04] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da8047772112452638] 
		\draw    (419.5,217.25) -- (472.93,37.42) ;
		\draw [shift={(473.5,35.5)}, rotate = 106.55] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da6831163376704086] 
		\draw    (419.5,217.25) -- (471.84,252.39) ;
		\draw [shift={(473.5,253.5)}, rotate = 213.87] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da9830848270717312] 
		\draw    (419.5,168.91) -- (472.42,251.81) ;
		\draw [shift={(473.5,253.5)}, rotate = 237.45] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da46737622255134736] 
		\draw    (419.5,120.58) -- (472.75,251.65) ;
		\draw [shift={(473.5,253.5)}, rotate = 247.89] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da7994561480386091] 
		\draw    (419.5,72.25) -- (472.93,251.58) ;
		\draw [shift={(473.5,253.5)}, rotate = 253.41] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Circle [id:dp564699443242902] 
		\draw   (564.5,34.5) .. controls (564.5,17.1) and (578.6,3) .. (596,3) .. controls (613.4,3) and (627.5,17.1) .. (627.5,34.5) .. controls (627.5,51.9) and (613.4,66) .. (596,66) .. controls (578.6,66) and (564.5,51.9) .. (564.5,34.5) -- cycle ;
		%Shape: Circle [id:dp692818648150423] 
		\draw   (564.5,252.5) .. controls (564.5,235.1) and (578.6,221) .. (596,221) .. controls (613.4,221) and (627.5,235.1) .. (627.5,252.5) .. controls (627.5,269.9) and (613.4,284) .. (596,284) .. controls (578.6,284) and (564.5,269.9) .. (564.5,252.5) -- cycle ;
		%Straight Lines [id:da08371226426680001] 
		\draw    (536.5,35.5) -- (562.5,35.5) ;
		\draw [shift={(564.5,35.5)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da2644226206968381] 
		\draw    (536.5,252.5) -- (562.5,252.5) ;
		\draw [shift={(564.5,252.5)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da5009536336262386] 
		\draw    (505,222) -- (594.99,67.73) ;
		\draw [shift={(596,66)}, rotate = 120.26] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da7379624296989971] 
		\draw    (505,67) -- (594.98,219.28) ;
		\draw [shift={(596,221)}, rotate = 239.42] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da8373577998416035] 
		\draw    (627.5,34.5) -- (644.5,34.5) ;
		\draw [shift={(646.5,34.5)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da01949229720198553] 
		\draw    (627.5,252.5) -- (644.5,252.5) ;
		\draw [shift={(646.5,252.5)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Grid [id:dp14924956956735103] 
		\draw  [draw opacity=0] (15,354) -- (190,354) -- (190,529) -- (15,529) -- cycle ; \draw   (50,354) -- (50,529)(85,354) -- (85,529)(120,354) -- (120,529)(155,354) -- (155,529) ; \draw   (15,389) -- (190,389)(15,424) -- (190,424)(15,459) -- (190,459)(15,494) -- (190,494) ; \draw   (15,354) -- (190,354) -- (190,529) -- (15,529) -- cycle ;
		%Shape: Grid [id:dp4853747875692118] 
		\draw  [draw opacity=0] (197,369.5) -- (337,369.5) -- (337,509.5) -- (197,509.5) -- cycle ; \draw   (232,369.5) -- (232,509.5)(267,369.5) -- (267,509.5)(302,369.5) -- (302,509.5) ; \draw   (197,404.5) -- (337,404.5)(197,439.5) -- (337,439.5)(197,474.5) -- (337,474.5) ; \draw   (197,369.5) -- (337,369.5) -- (337,509.5) -- (197,509.5) -- cycle ;
		%Shape: Grid [id:dp28715190595080164] 
		\draw  [draw opacity=0] (343,404.5) -- (413,404.5) -- (413,474.5) -- (343,474.5) -- cycle ; \draw   (378,404.5) -- (378,474.5) ; \draw   (343,439.5) -- (413,439.5) ; \draw   (343,404.5) -- (413,404.5) -- (413,474.5) -- (343,474.5) -- cycle ;
		%Shape: Grid [id:dp8063199363917093] 
		\draw  [draw opacity=0] (420,370) -- (455,370) -- (455,510) -- (420,510) -- cycle ; \draw    ; \draw   (420,405) -- (455,405)(420,440) -- (455,440)(420,475) -- (455,475) ; \draw   (420,370) -- (455,370) -- (455,510) -- (420,510) -- cycle ;
		%Right Arrow [id:dp14520345147775537] 
		\draw   (289.5,321.5) -- (353.1,321.5) -- (353.1,313) -- (395.5,330) -- (353.1,347) -- (353.1,338.5) -- (289.5,338.5) -- cycle ;
		%Right Arrow [id:dp7210437439214428] 
		\draw   (142.5,321.5) -- (206.1,321.5) -- (206.1,313) -- (248.5,330) -- (206.1,347) -- (206.1,338.5) -- (142.5,338.5) -- cycle ;
		%Shape: Circle [id:dp39717344916956376] 
		\draw   (522,367.5) .. controls (522,350.1) and (536.1,336) .. (553.5,336) .. controls (570.9,336) and (585,350.1) .. (585,367.5) .. controls (585,384.9) and (570.9,399) .. (553.5,399) .. controls (536.1,399) and (522,384.9) .. (522,367.5) -- cycle ;
		%Shape: Circle [id:dp009224709956572541] 
		\draw   (522,510.5) .. controls (522,493.1) and (536.1,479) .. (553.5,479) .. controls (570.9,479) and (585,493.1) .. (585,510.5) .. controls (585,527.9) and (570.9,542) .. (553.5,542) .. controls (536.1,542) and (522,527.9) .. (522,510.5) -- cycle ;
		%Straight Lines [id:da7267100443333161] 
		\draw    (455,389) -- (520.1,368.11) ;
		\draw [shift={(522,367.5)}, rotate = 162.21] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da6854613686232436] 
		\draw    (455,389) -- (521.03,508.75) ;
		\draw [shift={(522,510.5)}, rotate = 241.13] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da34132035511014736] 
		\draw    (455,424) -- (520.47,368.79) ;
		\draw [shift={(522,367.5)}, rotate = 139.86] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da12108034609199825] 
		\draw    (455,424) -- (520.78,508.92) ;
		\draw [shift={(522,510.5)}, rotate = 232.24] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da6141192074682378] 
		\draw    (456,460) -- (520.84,369.13) ;
		\draw [shift={(522,367.5)}, rotate = 125.51] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da6494999878025782] 
		\draw    (456,460) -- (520.41,509.28) ;
		\draw [shift={(522,510.5)}, rotate = 217.42] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da6909822650505033] 
		\draw    (455,493) -- (521.06,369.26) ;
		\draw [shift={(522,367.5)}, rotate = 118.1] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da2174616738281412] 
		\draw    (455,493) -- (520.06,509.99) ;
		\draw [shift={(522,510.5)}, rotate = 194.64] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		
		% Text Node
		\draw (34.5,126) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{26.59pt}\setlength\topsep{0pt}
		\begin{center}
		$\displaystyle 5\times 5$\\input
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw    (98.5,86.5) -- (139.5,86.5) -- (139.5,121.5) -- (98.5,121.5) -- cycle  ;
		\draw (101.5,88) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{24.95pt}\setlength\topsep{0pt}
		\begin{center}
		{\footnotesize $\displaystyle 2\times 2$}\\{\footnotesize kernel}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (163.5,118.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{34.48pt}\setlength\topsep{0pt}
		\begin{center}
		$\displaystyle 4\times 4$\\feature\\map
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (230.5,76.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{21.85pt}\setlength\topsep{0pt}
		\begin{center}
		{\footnotesize $\displaystyle 2\times 2$}\\{\footnotesize max}\\{\footnotesize pool}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw    (271.5,114.5) -- (326.5,114.5) -- (326.5,168.5) -- (271.5,168.5) -- cycle  ;
		\draw (274.5,115) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{34.48pt}\setlength\topsep{0pt}
		\begin{center}
		$\displaystyle 2\times 2$\\feature\\map
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (373,14) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {\begin{minipage}[lt]{34.36pt}\setlength\topsep{0pt}
		\begin{center}
		non-\\linearity
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (493.5,26.9) node [anchor=north west][inner sep=0.75pt]    {$H_{1}$};
		% Text Node
		\draw (493.5,244.9) node [anchor=north west][inner sep=0.75pt]    {$H_{2}$};
		% Text Node
		\draw (431,54.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {\contour{white}{$w_{1}$}};
		% Text Node
		\draw (435.5,75.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {\contour{white}{$w_{2}$}};
		% Text Node
		\draw (440,90.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {\contour{white}{$w_{3}$}};
		% Text Node
		\draw (444.5,109.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {\contour{white}{$w_{4}$}};
		% Text Node
		\draw (442.5,159.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {\contour{white}{$w_{5}$}};
		% Text Node
		\draw (437,177.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {\contour{white}{$w_{6}$}};
		% Text Node
		\draw (435.5,200.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {\contour{white}{$w_{7}$}};
		% Text Node
		\draw (431,224.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {\contour{white}{$w_{8}$}};
		% Text Node
		\draw (584.5,25.9) node [anchor=north west][inner sep=0.75pt]    {$O_{1}$};
		% Text Node
		\draw (584.5,243.9) node [anchor=north west][inner sep=0.75pt]    {$O_{2}$};
		% Text Node
		\draw (539.5,21.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$w_{9}$};
		% Text Node
		\draw (539.5,235.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$w_{12}$};
		% Text Node
		\draw (503,178) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$w_{10}$};
		% Text Node
		\draw (503,98) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$w_{11}$};
		% Text Node
		\draw (650.5,27) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (650.5,245.5) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (20.79,364.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.51$};
		% Text Node
		\draw (54.04,364.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.90$};
		% Text Node
		\draw (88.26,364.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.88$};
		% Text Node
		\draw (123.48,364.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.84$};
		% Text Node
		\draw (157.71,364.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.05$};
		% Text Node
		\draw (21,399.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.40$};
		% Text Node
		\draw (54.75,399.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.62$};
		% Text Node
		\draw (89.5,399.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.22$};
		% Text Node
		\draw (125.25,399.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.59$};
		% Text Node
		\draw (160,399.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.10$};
		% Text Node
		\draw (21.25,434.9) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.11$};
		% Text Node
		\draw (55.5,434.9) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.20$};
		% Text Node
		\draw (90.75,434.9) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.74$};
		% Text Node
		\draw (125,434.9) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.33$};
		% Text Node
		\draw (159.25,434.9) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.14$};
		% Text Node
		\draw (20,469.9) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.47$};
		% Text Node
		\draw (55.25,469.9) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.01$};
		% Text Node
		\draw (90.5,469.9) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.85$};
		% Text Node
		\draw (126.76,469.9) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.70$};
		% Text Node
		\draw (161.01,469.9) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.09$};
		% Text Node
		\draw (20,504.9) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.76$};
		% Text Node
		\draw (55.25,504.9) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.19$};
		% Text Node
		\draw (90.5,504.9) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.72$};
		% Text Node
		\draw (126.76,504.9) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.17$};
		% Text Node
		\draw (161.01,504.9) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.57$};
		% Text Node
		\draw (200,380.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.25$};
		% Text Node
		\draw (231,380.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$-0.16$};
		% Text Node
		\draw (272.16,380.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.27$};
		% Text Node
		\draw (301.75,380.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$-0.34$};
		% Text Node
		\draw (201,416.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.11$};
		% Text Node
		\draw (237.08,416.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.31$};
		% Text Node
		\draw (267.16,416.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$-0.11$};
		% Text Node
		\draw (301.75,416.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$-0.14$};
		% Text Node
		\draw (197,452.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$-0.22$};
		% Text Node
		\draw (238.08,452.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.61$};
		% Text Node
		\draw (266,452.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$-0.05$};
		% Text Node
		\draw (302.25,452.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$-0.32$};
		% Text Node
		\draw (291.5,321.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {max-pooling};
		% Text Node
		\draw (144.5,321.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {convolution};
		% Text Node
		\draw (197,485.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$-0.33$};
		% Text Node
		\draw (238.08,485.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.48$};
		% Text Node
		\draw (266,485.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$-0.27$};
		% Text Node
		\draw (306.25,485.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.19$};
		% Text Node
		\draw (348,416.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.31$};
		% Text Node
		\draw (382.08,416.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.27$};
		% Text Node
		\draw (348,450.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.61$};
		% Text Node
		\draw (382.08,450.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.19$};
		% Text Node
		\draw (423.25,380.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.58$};
		% Text Node
		\draw (423.25,416.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.57$};
		% Text Node
		\draw (423.25,452.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.65$};
		% Text Node
		\draw (423.25,485.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.55$};
		% Text Node
		\draw (530,360) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$\Delta =0.25$};
		% Text Node
		\draw (527,504) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$\Delta =-0.15$};
		% Text Node
		\draw  [draw opacity=0]  (458.5,373) -- (487.5,373) -- (487.5,391) -- (458.5,391) -- cycle  ;
		\draw (461.5,377.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {\contour{white}{$0.61$}};
		% Text Node
		\draw (468,394.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {\contour{white}{$0.82$}};
		% Text Node
		\draw (476.5,409.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {\contour{white}{$0.96$}};
		% Text Node
		\draw (482,424.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {\contour{white}{$-1.00$}};
		% Text Node
		\draw (486,447.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {\contour{white}{$0.02$}};
		% Text Node
		\draw (474,464.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {\contour{white}{$-0.50$}};
		% Text Node
		\draw (475.5,481.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {\contour{white}{$0.23$}};
		% Text Node
		\draw (468.5,496) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {\contour{white}{$0.17$}};
		
		
		%Shape: Grid [id:dp1470314745364174] 
		\draw  [draw opacity=0] (67,559.5) -- (137,559.5) -- (137,629.5) -- (67,629.5) -- cycle ; \draw   (102,559.5) -- (102,629.5) ; \draw   (67,594.5) -- (137,594.5) ; \draw   (67,559.5) -- (137,559.5) -- (137,629.5) -- (67,629.5) -- cycle ;
		\draw (67,571.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$-0.13$};
		% Text Node
		\draw (106.08,571.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.15$};
		% Text Node
		\draw (67,605.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$-0.51$};
		% Text Node
		\draw (106.08,605.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0.62$};
		% Text Node
		\draw (81,536) node [anchor=north west][inner sep=0.75pt]   [align=left] {kernel};
		
		%Right Arrow [id:dp7888186446421128] 
\draw   (400.5,322.5) -- (451.98,322.5) -- (451.98,314) -- (467.5,331) -- (451.98,348) -- (451.98,339.5) -- (400.5,339.5) -- cycle ;
		% Text Node
		\draw (404,322.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {sigmoid};
		\end{tikzpicture}}
	\end{figure}
	In the figure below we can see how CNN are influenced by the presence of a perfect gray square on a photo:
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/cnn_gray_square_text.jpg}
	\end{figure}
	
	The last level of a CNN, the most popular known one in massmedia, is sometimes named a "\NewTerm{segmentation mask}" because the input image is reduced to its simple geometrical elements:
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/cnn_segmentation_mask.jpg}
	\end{figure}
	
	Our Convolutional neural network really consists of two parts: Convolutional layers and fully connected decision layers. Both are connected using a flattening layer that converts an array of 2D images to a single 1D list of numeric values.
	
	In most cases CNNs use a cross-entropy loss on the one-hot encoded output. For a single image the cross entropy loss looks like this:
	
	where $M$ is the number of classes (i.e. $1000$ in ImageNet) and $\hat{y}_c$ is the model's prediction for that class (i.e. the output of the softmax for class $c$). Due to the fact that the labels are one-hot encoded and $y$ is a $[1000\times 1]$ vector of ones and zeroes, $y_c$ is either $1$ or $0$. Thus, out of the whole sum only one term will actually be added: the one with $y_c=1$.

	There are various architectures of CNNs available which have been key in building algorithms which power and shall power AI as a whole in the foreseeable future. Like LeNet, AlexNet, VGGNet, GoogLeNet, ResNet, ZFNet, SR-CNN:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/computing/cnn_examples.pdf}
		\caption[Various type of CNN internal structures]{Various type of CNN internal structures (author: Aqeel Anwar)}
	\end{figure}
	
	
	\pagebreak
	\paragraph{Generative Adversarial Network (GAN)}\mbox{}\\\\
	A "\NewTerm{generative adversarial network}\index{generative adversarial network}\label{generative adversarial network}" (GAN) is a class of machine learning systems invented by Ian Goodfellow and his colleagues in 12014 (holocene calendar). Two neural networks contest with each other in a game (in the sense of game theory, often but not always in the form of a zero-sum game). Given a training set, this technique learns to generate new data with the same statistics as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics. Though originally proposed as a form of generative model for unsupervised learning, GANs seems also proven useful for semi-supervised learning, fully supervised learning and reinforcement learning.
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/gna_principle.jpg}
		\caption{General idea of a GAN}
	\end{figure} 
	The generative network generates candidates while the discriminative network evaluates them. The contest operates in terms of data distributions. Typically, the generative network learns to map from a latent space to a data distribution of interest, while the discriminative $D$ network distinguishes candidates produced by the generator $G$ from the true data distribution. The generative network's training objective is to increase the error rate of the discriminative network (i.e., "fool" the discriminator network by producing novel candidates that the discriminator thinks are not synthesized (are part of the true data distribution)).

	A known dataset serves as the initial training data for the discriminator $D$. Training it involves presenting it with samples from the training dataset, until it achieves acceptable accuracy. The generator trains based on whether it succeeds in fooling the discriminator. Typically the generator is seeded with randomized input that is sampled from a predefined latent space (e.g. a multivariate Normal distribution). Thereafter, candidates synthesized by the generator are evaluated by the discriminator. Back-propagation is applied in both networks so that the generator produces better images, while the discriminator becomes more skilled at flagging synthetic images. The generator is typically a deconvolutional neural network, and the discriminator is a convolutional neural network.
	
	Training a vanilla GAN is like a 2-players game, where:
	\begin{itemize}
		\item The discriminator is trying to maximize the cost function $L(D,G)$
		
		\item The generator is trying to minimize the cost function $L(D,G)$
	\end{itemize}
	The discriminator is the generator's opponent, and performs a mapping $D(x)\in [0,1]$. Its goal is to look at sample images (that could be real or synthetic from the generator), and determine if they are real samples ($D(x)$ closer to $1$) or synthetic samples from the generator ($D(x)$ closer to $0$). $D(x)$ can be interpreted as the probability that the image is a real training example.

	The generator, $G(z),$ has parameters $\theta^{(G)},$ and the discriminator, $D(x),$ has parameters $\theta^{(D)} .$ The generator can only control $\theta^{(G)},$ while the discriminator can only control $\theta^{(D)}$.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Keep in mind that $D(x)$ is simply a function that maps to $[0,1]$. So the possible values of $D(G(z)) + D(x)$ range from $0$ to $2$.
	\end{tcolorbox}
	
	In addition to this, the discriminator and generator have different cost functions they wish to optimize:
	\begin{itemize}
		\item This ought to be intuitive as the generator and discriminator have different goals.
	
		\item We denote the discriminator's cost function as $L^{(D)}\left(\theta^{(D)}, \theta^{(G)}\right) $. For convenience, we will sometimes denote this as $L(D)$.
	
		\item We denote the generator's cost function as $L^{(G)}\left(\theta^{(D)}, \theta^{(G)}\right) $. For convenience, we will sometimes denote this as $L(G)$
	\end{itemize}
	
	What is the solution?
	\begin{itemize}
		\item The discriminator wishes to minimize $L^{(D)}$, but can only do so by changing $\theta^{(D)}$
		
		\item The generator wishes to minimize $L^{(G)}$, but can only do so by changing $\theta^{(G)}$
	\end{itemize}
	This is slightly different from the optimization problems we've described thus far, where we have one set of parameters to minimize one cost function $L$.

	Instead of treating this as an optimization problem, we treat this as a game between two players. The solution to a game is named a "Nash equilibrium". For GANs, a Nash equilibrium is a tuple, $(\theta^{(D)},\theta^{(G)})$ that is:
	\begin{itemize}
		\item A local minimum of $L(D)$ with respect to $\theta^{(D)}$
		\item A local minimum of $L(G)$ with respect to $\theta^{(G)}$
	\end{itemize}

	The simplest type of game to analyse is a zero-sum game, in which the sum of the generator's loss and the discriminator's loss is always zero. In a zero-sum game, the generator's loss is:
	
	The solution for a zero-sum game is named a minimax solution, where the goal is to minimize the maximum loss. Since the game is zero-sum, we can summarize (see proof just below) the entire game by stating that the loss function is explicitly (based on binary cross-entropy as introduced at page \pageref{cross-entropy}):
	
	Often condensed in the following form:
	
	with obviously:
	
	\begin{itemize}
		\item The discriminator wants to maximize the objective (i.e., its payoff) such that $D(x)$ is close to $1$ and $D(G(z))$ is close to zero.
	
		\item The generator wants to minimize the objective (i.e., its loss) so that $D(G(z))$ is close to $1$.
	\end{itemize}
	The Nash equilibrium  (\SeeChapter{see section Games and Decision Theory page \pageref{Nash equilibrium}}) of this particular game is achieved at:
	
	
	\begin{dem}
	Let us recall the definition of cross-entropy (see page \pageref{cross-entropy}):
	
	Let us rewrite it $H(y,D(x))$ where $H$ remains the cross-entropy $x$ is sampled either from $p_\text{data}$ or from $p_\text{model}$ with a probability of $50\%$. More formally:
	
	We consider $y$ to be $1$ if $x$ is sampled from the real distribution and $0$ if it is sampled from the fake one. Finally, $D(x)$ represents the probability with which $D$ thinks that $x$ belongs to $p_\text{data}$. By writing the cross-entropy formula we get:
	
	where $N$ is the size of the dataset. Since each class has $N/2$ samples we can split this sum into two parts:
	
	The first of the two terms represents the samples from the $p_\text{data}$ distribution, while the second one the samples from the $ p_\text{model}$ distribution. Since all $y_i$ are equally likely to occur, we can convert the sums into expectations:
	
	At this point, we will ignore $1/2$ from the equations since it's constant and thus irrelevant when optimizing this equation. Now, remember that samples that were drawn from $p_\text{model}$ were actually outputs from the generator (obviously this affects only the second term). If we substitute $D(x)$,$x \sim p_\text{model}$ with $D(G(z))$,$z\sim p_z$ we will get:
	
	This is the final form of the discriminator loss.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	This whole endeavour was to provide a mathematical formulation to training GANs. In practice there are maaaany tricks that are invoked to effectively train a GAN, that are not depicted in the above equations.
	\end{tcolorbox}
	
	For GANs, in practice this game is implemented in an iterative numerical approach. It involves two steps:
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/gna_principle_training_discriminator.jpg}
		\caption{General idea of a GAN Discriminator training}
	\end{figure} 
	Gradient ascent for the discriminator. We modify $\theta^{(D)}$ to maximize the minimax objective:
	
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/gna_principle_training_generator.jpg}
		\caption{General idea of a GAN Generator training}
	\end{figure} 
	Gradient descent on the discriminator. We modify $\theta^{(G)}$ to minimize the minimax objective:
	
	For the sake of intuition, consider that instead of optimizing with respect to $\theta^{(D)},$ we get to optimize $D(x)$ for every value of $x$. Further, assume that $p_{\text {data }}$ and $p_{\text {model }}$ are non-zero everywhere. What would be the optimal strategy for $D(x)$?
	
	To answer this question, we differentiate with respect to $D(x)$ and set the derivative equal to zero. In particular, we start with:
	
	Differentiating with respect to $D(x)$, we get:
	
	We can now set the derivative equal to zero to find the optimal $D(x)$. This leads to:
	
	A solution occurs when the integrands are equal for all $x$, i.e.:
	
	Rearranging terms, this gives:
	
	If the generator has high enough capacity, it will then move to set $p_{\text {model }}(x)=p_{\text {data }}(x)$ for all $x$. This results in the output:
	
	This is the Nash equilibrium!
	
	As stated, the current training paradigm has an important limitation. Note that the gradient of $\log (1-D(G(z)))$ is $-\frac{1}{1-D(G(z))}$, and thus has the following gradient as function of $D(G(z))$:
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{img/computing/gan_gradient_issue.jpg}
	\end{figure}
	Therefore, if $D(G(z)) \cong 0$, as may happen early on in training when the discriminator can tell the difference between real and synthetic examples, the gradient is close to zero. This results in little learning for $\theta^{(G)}$, and thus in practice the generator cost function:
	
	is rarely ever used.
	
	Instead, we opt for a cost function that has a large gradient when $D(G(z)) \cong 0$, so that the generator is encouraged to learn much more early in training. This is the cost function:
	
	This still obtains the same overall goal of being minimized when $D(G(z)) = 1$, but now admits far more learning when the generator performs poorly.
	
	The gradient of this new cost for the generator encourages more learning when the generator performs poorly.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{img/computing/gan_gradien_correction.jpg}
	\end{figure}
	Note that by changing the cost function for the generator network, the game is no longer zero-sum. This is a heuristic change made to the game to solve the practical problem of saturating gradients when the generator isn't doing well!
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/gan_pix2pix.jpg}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/gan_emotion_generator.jpg}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/computing/gan_painter_learning.jpg}
	\end{figure}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	There are largely empirical good practices to train a GAN network like the fact to normalize the images between $-1$ and $+1$, and use $\tanh$ as the output of the generator model, let the prior on $z$ be Gaussian rather than uniform, avoid sparse gradients, avoid using ReLU or maxpool and use LeakyReLU instead and to downsample, increase the stride, etc.
	\end{tcolorbox}
	
	\pagebreak
	\paragraph{Objective, cost and loss function}\mbox{}\\\\
	In machine learning (i.e. Statistics...), people talk about objective function, cost function, loss function\footnote{Not to be confused with the "activation functions" used in neural networks!}. These are not very strict terms and they are highly related. However:
	\begin{itemize}
		\item "\NewTerm{Loss function}\index{loss function}", often denoted $L$, is usually a function defined on a \underline{single data point}, prediction and label, and measures the penalty. For example:
		\begin{itemize}
			\item Square loss (also named "quadratic loss", "$L_2$ loss" or MSE):
			
			used in linear regression.
			
			\item Mean Absolute Error (also named "$L_2$ loss" or MAE):
			
			used in linear regression.
			
			\item Cross entropy loss:
			
			used in binary classification.
	
			\item Hinge loss: 
			
			used in SVM.
			
			\item Huber loss:
			
			Typically used for regression. It's less sensitive to outliers than the MSE as it treats error as square only inside an interval. There is also a "pseudo-Huber loss" function and a "modified Huber loss".
	
			\item $0/1$ loss: 
			
			used in theoretical analysis.
			
			\item ...
		\end{itemize}
	
		\item "\NewTerm{Cost function}\index{cost function}", often denoted $J$, is usually more general. It might be a \underline{sum of loss functions} over your training set plus some model complexity penalty (regularization). For example:
		\begin{itemize}
			\item Mean Squared Error: 
			
	
			\item SVM cost function: 
			
			
			\item ...
		\end{itemize}
		Or as we have seen earlier typically for neural networks the following cost functions:
		\begin{table}[H]
			\resizebox{\textwidth}{!}{\centering
			\begin{tabular}{|l|l|l|}
			\hline
			\rowcolor[gray]{0.75} 
			\textbf{Cost function name} & \textbf{Mathematical expression} & \textbf{Gradient} \\ \hline
			 Quadratic cost & $J_\text{MST}\left(W,\vec{b},\vec{y},\vec{\phi}\right)=\dfrac{1}{2}\displaystyle\sum_{i=1}^n\left(\phi_i(z)-y_i\right)^2$ & $\nabla_\phi J_\text{MST}\left(W,\vec{b},\vec{y},\vec{\phi}\right) = \displaystyle\sum_{i=1}^n (\phi_i(z)-y_i)$ \\ \hline
			 Cross-entropy cost & $J_\text{CE}\left(W,\vec{b},\vec{y},\vec{\phi}\right) =-\displaystyle\sum_{i=1}^n\left[y_i\ln(\phi(z))+(1-y_i)\ln(1-y_i)\right]$ & $\nabla_\phi  J_\text{CE}\left(W,\vec{b},\vec{y},\vec{\phi}\right) =\displaystyle\sum_{i=1}^n\dfrac{\phi_i(z)-y_i}{(1-\phi_i(z))\phi_i(z)}$ \\ \hline
			 Exponential cost & $J_\text{exp}\left(W,\vec{b},\vec{y},\vec{\phi},\tau\right) =\tau \exp\left(\dfrac{1}{\tau} \displaystyle\sum_{i=1}^n \left(\phi_i(z)-y_i\right)^2 \right)$ & $\nabla_\phi  J_\text{exp}\left(W,\vec{b},\vec{y},\vec{\phi},\tau\right) =\dfrac{2}{\tau}J_\text{exp}\left(W,\vec{b},\vec{y},\vec{\phi},\tau\right)\displaystyle\sum_{i=1}^n \phi_i(z)-y_i$ \\ \hline
			 Hellinger distance & $J_\text{HD}\left(W,\vec{b},\vec{y},\vec{\phi}\right) =\dfrac{1}{\sqrt{2}}\displaystyle\sum_{i=1}^n \left(\sqrt{\phi_i(z)}-\sqrt{y_i} \right)^2$ & $\nabla_\phi  J_\text{exp}\left(W,\vec{b},\vec{y},\vec{\phi}\right) =\dfrac{1}{\sqrt{2}}\displaystyle\sum_{i=1}^n \dfrac{\sqrt{\phi_i(z)}-\sqrt{y_i}}{\sqrt{\phi_i(z)}}$ \\ \hline
			 Kullback-Leibler (KL) cost & $J_\text{KL}\left(W,\vec{b},\vec{y},\vec{\phi}\right) =\displaystyle\sum_{i=1}^n y_i\log\left(\dfrac{y_i}{\phi_i(z)}\right)$ & $\nabla_\phi  J_\text{GKL}\left(W,\vec{b},\vec{y},\vec{\phi}\right) =\displaystyle\sum_{i=1}^n \dfrac{\phi_i(z)-y_i}{\phi_i(z)}$ \\ \hline
			 Generalized KL cost & $J_\text{GKL}\left(W,\vec{b},\vec{y},\vec{\phi}\right) =\displaystyle\sum_{i=1}^n y_i\log\left(\dfrac{y_i}{\phi_i(z)}\right)-\displaystyle\sum_{i=1}^n y_i+\displaystyle\sum_{i=1}^n \phi_i(z)$ & $\nabla_\phi  J_\text{GKL}\left(W,\vec{b},\vec{y},\vec{\phi}\right) =\displaystyle\sum_{i=1}^n \dfrac{\phi_i(z)-y_i}{\phi_i(z)}$  \\ \hline
			 Itakura-Saito distance cost & $J_\text{IS}\left(W,\vec{b},\vec{y},\vec{\phi}\right) =\displaystyle\sum_{i=1}^n \left(\dfrac{y_i}{\phi_i(z)}-\log\left(\dfrac{y_i}{\phi_i(z)}\right)-1\right)$ & $\nabla_\phi  J_\text{IS}\left(W,\vec{b},\vec{y},\vec{\phi}\right) =\displaystyle\sum_{i=1}^n \dfrac{\phi_i(z)-y_i}{\phi_i^2(z)}$ \\ \hline
			 ... & ... & ... \\ \hline
			\end{tabular}}
		\end{table}
	
		\item "\NewTerm{Objective function}\index{objective function}" is the most general term for any function that you optimize during training. For example, a probability of generating training set in maximum likelihood approach is a well defined objective function, but it is not a loss function nor cost function (however you could define an equivalent cost function). For example:
		\begin{itemize}
			\item MLE (Maximum Likelihood Estimator) is a type of objective function (which you maximize)
	
			\item Divergence between classes can be an objective function but it is barely a cost function, unless you define something artificial
		\end{itemize}
	\end{itemize}
	Long story short, we may say that: A loss function is a part of a cost function which is a type of an objective function.
	
	\pagebreak
	\subsubsection{Genetic Algorithms}\label{genetic algorithms}
	Genetic algorithms (GAs) are iterated  stochastic optimization algorithms based on the mechanisms of natural selection and genetics belonging to the family of "\NewTerm{evolutionary algorithms}\index{evolutionary algorithms}". This is an optimization technique that has spread widely since the beginning of the 121st century (holocene calendar) through the version 14.0.6123 of Microsoft Excel wherein the solver incorporates an evolutionary algorithm by default as shown by the screenshot below:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.85]{img/computing/evolutionary_solver.jpg}
		\caption[]{Screenshot of the evolutionary Microsoft Excel 14.0.6123 solver}
	\end{figure}
	with the corresponding options:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.85]{img/computing/evolutionary_solver_options.jpg}
		\caption[]{Screenshot of the evolutionary Microsoft Excel 14.0.6123 solver options}
	\end{figure}
	The process of the genetic algorithm is quite simple:
	\begin{enumerate}
		\item We start with an initial population of potential solutions (chromosomes) arbitrarily selected 

		\item We evaluate their relative performance (fitness) 

		\item Based on this performance, we create a new population of potential solutions using simple evolutionary operators: selection, crossover and mutation

		\item We start this cycle until we find a satisfactory solution
	\end{enumerate}
	Genetic Algorithms were originally developed by John Holland (11975 according to holocene calendar). This is the book of David E. Goldberg (11989 according to holocene calendar) that we own their popularization. Their fields of application are widespread. Besides the economy (portfolio risk minimization), they are used for optimization functions in  finance, in optimal control theory (operational research), in the theory of repetitive and differentials games (namely: in evolutionary games and the prisoner's dilemma) and information retrieval (Google) and search for shortest path in graph theory (Internet routing or GPS). The reason for the large number of applications is clear: simplicity and efficiency. Of course, other stochastic exploration techniques exist, the Monte Carlo can be regarded as a similar concept.
	
	To summarize, Lerman and Ngouenet (11995 according to holocene calendar) identified four main properties that make the fundamental difference between these algorithms and other methods:
	\begin{enumerate}
		\item The genetic algorithms use a coding of the input parameters, not the parameters themselves

		\item  Genetic algorithms work on a population of points, instead of a single point

		\item Genetic algorithms use only the values of the function considered, not its derivative, or other auxiliary knowledge

		\item  The algorithms use probabilistic transition rules, not deterministic one
	\end{enumerate}
	The simplicity of their mechanisms, their ease of implementation and efficiency even for complex problems led to a growing number of publication this recent years in the scientific community.
	
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] A "\NewTerm{genetic algorithm}\index{genetic algorithm}" is defined by an individual / chromosome / sequence and a potential solution to the given problem.

		\item[D2.]  A "\NewTerm{population}\index{population (algorithm)}" is a set of chromosomes or points of the search space

		\item[D3.]  The "\NewTerm{environment}\index{environment}" is assimilated with the search space

		\item[D4.]  The function that we seek to maximize is named "\NewTerm{fitness function}\index{fitness function}"
	\end{enumerate}

	Before going further, we need to define more formally the above concepts but under the particular case of Binary coding!
	
	The organisms in competition are the "individuals". Given an alphabet $A=\{a_1,a_2,\ldots,a_n\}$, we assume that each individual can be represented by a word of fixed-length $l$ caught in the in $A^{*}$. 

	The word associated with an individual of the population will be named a "\NewTerm{chromosome}\index{chromosome}" or "\NewTerm{sequence}\index{sequence}" (the term is not quite equivalent to its biological namesake, however, it is common practice to use the term here too ) and thus given by $A$ of the length $l(A)$ with $\forall i\in[1,l]: a_i\in A=\{0,1\}$ (reason: assumption of binary coding).

	If there is no risk of confusion, we will identify the terms of "individual" and "chromosome".

	The individuals form a population $P$ of size $P$, denoted by:
	
	with $i=1\ldots N$.
	We will make another important statement, that is to say, there is a function $f$ from one sequence with positive values which we denote $f(A)$, named "\NewTerm{fitness function}\index{fitness function}" that to any $A_i$ associates real number such that for $i\neq j$:
	
	if and only if $A_i$ is better suited to the environment than $A_j$.

	Notice that the term "appropriate" is not defined. For this, we would characterize the environment in which the individuals evolve, what we will not do. In fact, since we assume the existence of such a function and we put it in equivalence to the degree of adaptation, it is automatically set by the definition of $f$.

	We will name "\NewTerm{generation}\index{generation (algorithm)}" a population at time $t$, what must be put in relation with the notion of lifetime or age. However, we place ourselves here in the particular case where each individual has a life equal to $1$, so the generation $(t + 1)$ consists of different individuals from the generation$ $t, we name them obviously the "\NewTerm{descendants}\index{descendants}". Conversely, individuals of generation $t$ are the "\NewTerm{ancestors}\index{ancestors}" of the individuals of the generation $(t + 1)$. We denote the generation at time $t$ by $P(t)$, thus the population at time $t$.

	Thus, a chromosome is seen as a bit sequence in binary code known as "\NewTerm{bit string}\index{bit string}". In the case of a non-binary coding, such as the real number encoding for example, then the sequence A contains only one point, we have then $A=\{a\}$ with $a\in\mathbb{R}$. 
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The fitness (effectiveness) is given by a function with real positive values. In the case of binary encoding, we will often use a function of decoding $d$ that will gives the possibility to transform  a binary string to a real number:
	
	afterwards the fitness function is chosen such that it transforms this value into a positive value:
	
	\end{tcolorbox}	
	The purpose of a genetic algorithm is then simply to find the string that maximizes this function $f$. Of course, each individual problem will require its own functions $d$ and $f$.

	GAs are then based roughly on the following phases:
	\begin{enumerate}
		\item Initialization: an initial population of $N$ chromosomes is randomly chosen

		\item Evaluation: each chromosome is decoded and evaluated

		\item Selection: creation of a new population of $N$ chromosomes based on previous step by using an appropriate method of selection.

		\item Reproduction: possibility of crossover and mutation in the new population

		\item Return to the Evaluation phase until the stop of the algorithm
	\end{enumerate}
	Or for people that are more visual:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
		
		%Straight Lines [id:da19117419375419398] 
		\draw    (305.25,80) -- (305.25,319) ;
		%Shape: Rectangle [id:dp33295863877075815] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (179,39) -- (431.5,39) -- (431.5,78) -- (179,78) -- cycle ;
		%Shape: Rectangle [id:dp8351384194020663] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (179,109) -- (431.5,109) -- (431.5,148) -- (179,148) -- cycle ;
		%Shape: Rectangle [id:dp9025669808642218] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (179,179) -- (431.5,179) -- (431.5,218) -- (179,218) -- cycle ;
		%Shape: Rectangle [id:dp4681872761120769] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (179,249) -- (431.5,249) -- (431.5,288) -- (179,288) -- cycle ;
		%Shape: Rectangle [id:dp07981929041201918] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (178,319) -- (430.5,319) -- (430.5,358) -- (178,358) -- cycle ;
		%Shape: Rectangle [id:dp6866225117416811] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (59,109) -- (146.5,109) -- (146.5,148) -- (59,148) -- cycle ;
		%Straight Lines [id:da7232298333542462] 
		\draw    (178.5,129) -- (146.5,129) ;
		%Straight Lines [id:da5917944275611333] 
		\draw    (305.25,87.93) -- (305.25,102.79) ;
		\draw [shift={(305.25,104.79)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da7806107321570541] 
		\draw    (305.25,157.35) -- (305.25,172.21) ;
		\draw [shift={(305.25,174.21)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da4027147172191876] 
		\draw    (305.25,226.77) -- (305.25,241.63) ;
		\draw [shift={(305.25,243.63)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da5433233206799368] 
		\draw    (305.25,295.2) -- (305.25,310.06) ;
		\draw [shift={(305.25,312.06)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da08991810752203189] 
		\draw    (431.5,127) -- (451.5,127) -- (451.5,339) -- (430.5,339) ;
		%Straight Lines [id:da08251820298781665] 
		\draw    (451.25,243) -- (451.25,213.79) ;
		\draw [shift={(451.25,211.79)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da41944238533868705] 
		\draw    (172.5,129) -- (158.5,129) ;
		\draw [shift={(156.5,129)}, rotate = 360] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		
		% Text Node
		\draw (239.75,50) node [anchor=north west][inner sep=0.75pt]   [align=left] {Initialize Population};
		% Text Node
		\draw (269.25,120) node [anchor=north west][inner sep=0.75pt]   [align=left] {Evaluation};
		% Text Node
		\draw (273.25,190) node [anchor=north west][inner sep=0.75pt]   [align=left] {Selection};
		% Text Node
		\draw (270.25,258) node [anchor=north west][inner sep=0.75pt]   [align=left] {Crossover};
		% Text Node
		\draw (275.25,330) node [anchor=north west][inner sep=0.75pt]   [align=left] {Mutation};
		% Text Node
		\draw (83.75,120) node [anchor=north west][inner sep=0.75pt]   [align=left] {Done};
		
		\end{tikzpicture}
	\end{figure}
	
	\paragraph{Encoding and Initial population}\mbox{}\\\\
	There exist three main types of coding:
	\begin{enumerate}
		\item Binary
		\item Gray
		\item Real
	\end{enumerate}
	We can easily move from one encoding to another. Some authors do not hesitate, moreover, to draw parallels with biology, by speaking of "genotype\index{genotype}" (\SeeChapter{see section Population Dynamics page \pageref{genotype}}) regarding the binary representation of an individual, and "phenotype\index{phenotype}" (\SeeChapter{see section Population Dynamics page \pageref{phenotype}}) with respect to its corresponding real value in the search space.

	Let us recall that the simplest transformation (decoding function $d$) of a binary string $A$ into an integer $x$ is done by the following rule (\SeeChapter{see section Numbers page \pageref{number power decomposition} or this section page \pageref{computer representation of numbers}}):
	
	where $l$ is the number of digits of the string minus $1$.	
	
	Therefore the chromosome $A=\{1,0,1,1\}$ has trivially for value:
	
	Obviously, the function needs to be adapted (by trial and error!) depending on the problem. Thus, if we seek to maximize a function $f:[0,1]\rightarrow [0,1]$ a possible method would be as follows (the size of the chromosome of course dependent on the desired accuracy):
	
	
	This can be assimilated to a harmonic series (\SeeChapter{see section Sequences and Series page \pageref{harmonic series}}). For accuracy to the fifth decimal place, we will put $l=17-1$ since:
	
	Again another way to do would be to choose $d$ such as:
	
	Let us give an explanation of this choice:
	
	Let us put $l=n-1$:
	
	So, with $l=16$ we have $2^{17}-1=131071$ and:
	
	This last rule can be generalized. Thus, suppose that we seek to maximize ("normalize" would be a more suited term perhaps...) $f$ according to a real variable $x$. Given $D=[x_{\min},x_{\max}]$, with $D\subset \mathbb{R}$, the allowed  search space with $x_{\min}$ and $x_{\max}$ the lower and upper bounds of this space. Given $\mathrm{prec}$ the precision (decimal) with which we seek $x$. Given:
	
	the length (range) of the interval $D$. We then have to divide this interval at worst in:
	
	equal sub-intervals to meet accuracy expectations. For example, given $D=[-1,2]$ so we have $R=3$, if we wanted a precision $\mathrm{prec}=6$, then we must divide this interval in $n=3,000,000$ sub-intervals.
	
	Let $k$ denote the natural integer such that $2^k>n$, which in our example involves $k=22$ as:
	
	the transformation of a binary string $A=\{a_1,\ldots,a_l\}$ in a real number $x$ can then be run in three stages:
	\begin{enumerate}
		\item Conversion (base $2$ into base $10$):
		

		\item Normalization:
		

		\item Maximization:
		
	\end{enumerate}
	Or what remains the same directly in one step by using:
	
	Therefore for $f:[0,1]\rightarrow [0,1]$ and $\forall i,a_i=1$ we fall back well on:
	
	About the initialization phase, the procedure is quite simple. It consists of a random selection of $N$ individuals in the space of allowed individuals. In binary coding, according to the size $l$ of the string, we do for a chromosome $l$ sampling in $\{0,1\}$ with equal probability.
	
	\paragraph{Operators}\mbox{}\\\\
	Operators play a key role in the possible success of a GA. We number three main one: 
		\begin{enumerate}
			\item the selection operator
			\item the crossover operator
			\item the mutation operator
		\end{enumerate}
	If the principle of each of these operators is easy to understand, it is difficult to explain the isolated importance of each of these operators in the success of the AG. This is partly due to the fact that each of these operators acts according to various criteria that depends on its own characteristics (fitness of individuals, likelihood of activation of the operator, etc.).
		
	\pagebreak
	\subparagraph{Operator of selection}\mbox{}\\\\
	This operator may be the most important since it allows individuals in a population to survive, reproduce or die. Generally, the probability of survival of an individual will be directly connected to its relative effectiveness in the population.  The basic part of the selection process is to stochastically select from one generation to create the basis of the next generation. 

	There are several methods for reproduction.  The most known and used method is undoubtedly the biased David E. Goldberg's (11989 according to holocene calendar) lottery wheel (roulette wheel). According to this method, each chromosome is duplicated in a new population in proportion to its adaptive value. We perform in some way, as many sampling that there are individuals in the population. Thus, in the case of a binary coding, the fitness of a particular chromosome being $f (d (A))$, the probability with which it will be reintroduced into the new population of size $N$ is given by the relative fitness:
	
	Individuals with high fitness value thus have more chance of being selected by the wheel. We speak then of "\NewTerm{proportional selection}\index{proportional selection}":
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/goldberg_wheel.jpg}
		\caption{Example of David E. Goldberg's wheel with five individuals with their respective relative fitness}
	\end{figure}
	Obviously the number of times the roulette wheel is spun is equal to the size of the new population.

	Each time the wheel stops this gives the fitter individuals the greatest chance of being selected for the next generation and subsequent: "\NewTerm{mating pool}\index{mating pool}".

	The major drawback of this method lies in the fact that an individual that is not the best may still dominate the selection (imagine the search for maxima of a function in $\mathbb{R}^2$, there may be several of them - maxima - and therefore we could get a wrong selection ...), we will speak rightly then of "\NewTerm{premature convergence}\index{premature convergence}" and this is one of the most common problems when using genetic algorithms. It can therefore also result in a loss of diversity by the domination of a super-individual. Another drawback is its poor performance towards the end when all individuals are alike.

	One solution to this problem lies not in the use of another method of selection but the use of a modified fitness function. So we can use a scaling to decrease or increase artificially the relative difference between the fitness of individuals.

	Briefly, there are other methods, the best known being that of the tournament (tournament selection) we draw two random individuals in the population and reproduce the best of both in the new population. We repeat this procedure until the new population $P$ is complete. This method gives good results. However, as important as the selection phase, it does not create new individuals in the population. This is the role of crossover and mutation operators.
	
	\subparagraph{Crossover operator}\mbox{}\\\\
	The crossover operator allows the creation of new individuals in a very simple process. It allows the exchange of information between chromosomes (individuals). First, two individuals, which then form a couple, are sample in the new population issued from the selection (or reproduction). Then one (or potentially many) crossing site is randomly draw (number between $1$ and $l-1$). Finally, according to a probability $P_c$ that the crossing is done, the end segments (in the case of a single crossing site) of both parents are then exchanged around this site:
	\begin{figure}[H]
		\centering		
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,970); %set diagram left start at 0, and has height of 970
		
		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_og5k1ithl}{
		\makeatletter
		\pgfdeclarepatternformonly[\mcRadius,\mcThickness,\mcSize]{_og5k1ithl}
		{\pgfpoint{-0.5*\mcSize}{-0.5*\mcSize}}
		{\pgfpoint{0.5*\mcSize}{0.5*\mcSize}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathcircle\pgfpointorigin{\mcRadius}
		\pgfusepath{stroke}
		}}
		
		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_qg7ddp5tv}{
		\makeatletter
		\pgfdeclarepatternformonly[\mcRadius,\mcThickness,\mcSize]{_qg7ddp5tv}
		{\pgfpoint{-0.5*\mcSize}{-0.5*\mcSize}}
		{\pgfpoint{0.5*\mcSize}{0.5*\mcSize}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathcircle\pgfpointorigin{\mcRadius}
		\pgfusepath{stroke}
		}}
		
		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_dh8tk2ftk}{
		\makeatletter
		\pgfdeclarepatternformonly[\mcRadius,\mcThickness,\mcSize]{_dh8tk2ftk}
		{\pgfpoint{-0.5*\mcSize}{-0.5*\mcSize}}
		{\pgfpoint{0.5*\mcSize}{0.5*\mcSize}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathcircle\pgfpointorigin{\mcRadius}
		\pgfusepath{stroke}
		}}
		
		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_74u9smnpw}{
		\makeatletter
		\pgfdeclarepatternformonly[\mcRadius,\mcThickness,\mcSize]{_74u9smnpw}
		{\pgfpoint{-0.5*\mcSize}{-0.5*\mcSize}}
		{\pgfpoint{0.5*\mcSize}{0.5*\mcSize}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathcircle\pgfpointorigin{\mcRadius}
		\pgfusepath{stroke}
		}}
		
		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_jtbhushdo}{
		\makeatletter
		\pgfdeclarepatternformonly[\mcRadius,\mcThickness,\mcSize]{_jtbhushdo}
		{\pgfpoint{-0.5*\mcSize}{-0.5*\mcSize}}
		{\pgfpoint{0.5*\mcSize}{0.5*\mcSize}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathcircle\pgfpointorigin{\mcRadius}
		\pgfusepath{stroke}
		}}
		
		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_prptvm1wb}{
		\makeatletter
		\pgfdeclarepatternformonly[\mcRadius,\mcThickness,\mcSize]{_prptvm1wb}
		{\pgfpoint{-0.5*\mcSize}{-0.5*\mcSize}}
		{\pgfpoint{0.5*\mcSize}{0.5*\mcSize}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathcircle\pgfpointorigin{\mcRadius}
		\pgfusepath{stroke}
		}}
		
		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_yeuw8huf8}{
		\makeatletter
		\pgfdeclarepatternformonly[\mcRadius,\mcThickness,\mcSize]{_yeuw8huf8}
		{\pgfpoint{-0.5*\mcSize}{-0.5*\mcSize}}
		{\pgfpoint{0.5*\mcSize}{0.5*\mcSize}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathcircle\pgfpointorigin{\mcRadius}
		\pgfusepath{stroke}
		}}
		
		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_2cthuishj}{
		\makeatletter
		\pgfdeclarepatternformonly[\mcRadius,\mcThickness,\mcSize]{_2cthuishj}
		{\pgfpoint{-0.5*\mcSize}{-0.5*\mcSize}}
		{\pgfpoint{0.5*\mcSize}{0.5*\mcSize}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathcircle\pgfpointorigin{\mcRadius}
		\pgfusepath{stroke}
		}}
		
		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_4b5q72k01}{
		\makeatletter
		\pgfdeclarepatternformonly[\mcRadius,\mcThickness,\mcSize]{_4b5q72k01}
		{\pgfpoint{-0.5*\mcSize}{-0.5*\mcSize}}
		{\pgfpoint{0.5*\mcSize}{0.5*\mcSize}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathcircle\pgfpointorigin{\mcRadius}
		\pgfusepath{stroke}
		}}
		
		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_tkb3p9rtz}{
		\makeatletter
		\pgfdeclarepatternformonly[\mcRadius,\mcThickness,\mcSize]{_tkb3p9rtz}
		{\pgfpoint{-0.5*\mcSize}{-0.5*\mcSize}}
		{\pgfpoint{0.5*\mcSize}{0.5*\mcSize}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathcircle\pgfpointorigin{\mcRadius}
		\pgfusepath{stroke}
		}}
		
		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_x516wb54g}{
		\makeatletter
		\pgfdeclarepatternformonly[\mcRadius,\mcThickness,\mcSize]{_x516wb54g}
		{\pgfpoint{-0.5*\mcSize}{-0.5*\mcSize}}
		{\pgfpoint{0.5*\mcSize}{0.5*\mcSize}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathcircle\pgfpointorigin{\mcRadius}
		\pgfusepath{stroke}
		}}
		
		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_np15cjylb}{
		\makeatletter
		\pgfdeclarepatternformonly[\mcRadius,\mcThickness,\mcSize]{_np15cjylb}
		{\pgfpoint{-0.5*\mcSize}{-0.5*\mcSize}}
		{\pgfpoint{0.5*\mcSize}{0.5*\mcSize}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathcircle\pgfpointorigin{\mcRadius}
		\pgfusepath{stroke}
		}}
		
		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_6fah65moi}{
		\makeatletter
		\pgfdeclarepatternformonly[\mcRadius,\mcThickness,\mcSize]{_6fah65moi}
		{\pgfpoint{-0.5*\mcSize}{-0.5*\mcSize}}
		{\pgfpoint{0.5*\mcSize}{0.5*\mcSize}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathcircle\pgfpointorigin{\mcRadius}
		\pgfusepath{stroke}
		}}
		
		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_76ccdas62}{
		\makeatletter
		\pgfdeclarepatternformonly[\mcRadius,\mcThickness,\mcSize]{_76ccdas62}
		{\pgfpoint{-0.5*\mcSize}{-0.5*\mcSize}}
		{\pgfpoint{0.5*\mcSize}{0.5*\mcSize}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathcircle\pgfpointorigin{\mcRadius}
		\pgfusepath{stroke}
		}}
		
		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_ratcdpuab}{
		\makeatletter
		\pgfdeclarepatternformonly[\mcRadius,\mcThickness,\mcSize]{_ratcdpuab}
		{\pgfpoint{-0.5*\mcSize}{-0.5*\mcSize}}
		{\pgfpoint{0.5*\mcSize}{0.5*\mcSize}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathcircle\pgfpointorigin{\mcRadius}
		\pgfusepath{stroke}
		}}
		
		%Shape: Rectangle [id:dp13217311784149932] 
		\draw  [pattern=_og5k1ithl,pattern size=3.75pt,pattern thickness=0.75pt,pattern radius=0.75pt, pattern color={rgb, 255:red, 155; green, 155; blue, 155}] (82,124) -- (103.5,124) -- (103.5,147) -- (82,147) -- cycle ;
		%Shape: Rectangle [id:dp8993136107833315] 
		\draw  [pattern=_qg7ddp5tv,pattern size=3.75pt,pattern thickness=0.75pt,pattern radius=0.75pt, pattern color={rgb, 255:red, 155; green, 155; blue, 155}] (103.5,124) -- (125,124) -- (125,147) -- (103.5,147) -- cycle ;
		%Shape: Rectangle [id:dp6190616723248556] 
		\draw  [pattern=_dh8tk2ftk,pattern size=3.75pt,pattern thickness=0.75pt,pattern radius=0.75pt, pattern color={rgb, 255:red, 155; green, 155; blue, 155}] (125,124) -- (146.5,124) -- (146.5,147) -- (125,147) -- cycle ;
		%Shape: Rectangle [id:dp5817743394889971] 
		\draw   (146.5,124) -- (168,124) -- (168,147) -- (146.5,147) -- cycle ;
		%Shape: Rectangle [id:dp4873675320735713] 
		\draw   (168,124) -- (189.5,124) -- (189.5,147) -- (168,147) -- cycle ;
		%Shape: Rectangle [id:dp07446848133032247] 
		\draw   (189.5,124) -- (211,124) -- (211,147) -- (189.5,147) -- cycle ;
		%Shape: Rectangle [id:dp41998252084824417] 
		\draw  [pattern=_74u9smnpw,pattern size=3.75pt,pattern thickness=0.75pt,pattern radius=0.75pt, pattern color={rgb, 255:red, 155; green, 155; blue, 155}] (82,167) -- (103.5,167) -- (103.5,190) -- (82,190) -- cycle ;
		%Shape: Rectangle [id:dp8021253555600187] 
		\draw   (103.5,167) -- (125,167) -- (125,190) -- (103.5,190) -- cycle ;
		%Shape: Rectangle [id:dp31143481635661785] 
		\draw   (125,167) -- (146.5,167) -- (146.5,190) -- (125,190) -- cycle ;
		%Shape: Rectangle [id:dp04439336959731022] 
		\draw   (146.5,167) -- (168,167) -- (168,190) -- (146.5,190) -- cycle ;
		%Shape: Rectangle [id:dp29423285395795573] 
		\draw   (168,167) -- (189.5,167) -- (189.5,190) -- (168,190) -- cycle ;
		%Shape: Rectangle [id:dp7062320734265382] 
		\draw  [pattern=_jtbhushdo,pattern size=3.75pt,pattern thickness=0.75pt,pattern radius=0.75pt, pattern color={rgb, 255:red, 155; green, 155; blue, 155}] (189.5,167) -- (211,167) -- (211,190) -- (189.5,190) -- cycle ;
		%Shape: Rectangle [id:dp5543681850516302] 
		\draw  [pattern=_prptvm1wb,pattern size=3.75pt,pattern thickness=0.75pt,pattern radius=0.75pt, pattern color={rgb, 255:red, 155; green, 155; blue, 155}] (273.5,168) -- (295,168) -- (295,191) -- (273.5,191) -- cycle ;
		%Shape: Rectangle [id:dp7285725842865307] 
		\draw  [pattern=_yeuw8huf8,pattern size=3.75pt,pattern thickness=0.75pt,pattern radius=0.75pt, pattern color={rgb, 255:red, 155; green, 155; blue, 155}] (270.29,126.5) -- (291.79,126.5) -- (291.79,148.5) -- (270.29,148.5) -- cycle ;
		%Shape: Rectangle [id:dp06052070687082978] 
		\draw  [pattern=_2cthuishj,pattern size=3.75pt,pattern thickness=0.75pt,pattern radius=0.75pt, pattern color={rgb, 255:red, 155; green, 155; blue, 155}] (370.76,127.5) -- (392.26,127.5) -- (392.26,150.32) -- (370.76,150.32) -- cycle ;
		%Shape: Rectangle [id:dp20812728427270355] 
		\draw  [pattern=_4b5q72k01,pattern size=3.75pt,pattern thickness=0.75pt,pattern radius=0.75pt, pattern color={rgb, 255:red, 155; green, 155; blue, 155}] (459,124) -- (480.5,124) -- (480.5,147) -- (459,147) -- cycle ;
		%Shape: Rectangle [id:dp1787599648506546] 
		\draw  [pattern=_tkb3p9rtz,pattern size=3.75pt,pattern thickness=0.75pt,pattern radius=0.75pt, pattern color={rgb, 255:red, 155; green, 155; blue, 155}] (480.5,124) -- (502,124) -- (502,147) -- (480.5,147) -- cycle ;
		%Shape: Rectangle [id:dp5394218650533524] 
		\draw  [pattern=_x516wb54g,pattern size=3.75pt,pattern thickness=0.75pt,pattern radius=0.75pt, pattern color={rgb, 255:red, 155; green, 155; blue, 155}] (502,124) -- (523.5,124) -- (523.5,147) -- (502,147) -- cycle ;
		%Shape: Rectangle [id:dp5360706071060422] 
		\draw   (523.5,124) -- (545,124) -- (545,147) -- (523.5,147) -- cycle ;
		%Shape: Rectangle [id:dp928730599082265] 
		\draw   (545,124) -- (566.5,124) -- (566.5,147) -- (545,147) -- cycle ;
		%Shape: Rectangle [id:dp9329428792742289] 
		\draw  [pattern=_np15cjylb,pattern size=3.75pt,pattern thickness=0.75pt,pattern radius=0.75pt, pattern color={rgb, 255:red, 155; green, 155; blue, 155}] (566.5,124) -- (588,124) -- (588,147) -- (566.5,147) -- cycle ;
		%Shape: Rectangle [id:dp6657681404061033] 
		\draw  [pattern=_6fah65moi,pattern size=3.75pt,pattern thickness=0.75pt,pattern radius=0.75pt, pattern color={rgb, 255:red, 155; green, 155; blue, 155}] (459,168) -- (480.5,168) -- (480.5,191) -- (459,191) -- cycle ;
		%Shape: Rectangle [id:dp296771919070961] 
		\draw   (480.5,168) -- (502,168) -- (502,191) -- (480.5,191) -- cycle ;
		%Shape: Rectangle [id:dp471993994553604] 
		\draw   (502,168) -- (523.5,168) -- (523.5,191) -- (502,191) -- cycle ;
		%Shape: Rectangle [id:dp6992189288971329] 
		\draw   (523.5,168) -- (545,168) -- (545,191) -- (523.5,191) -- cycle ;
		%Shape: Rectangle [id:dp3420629036946876] 
		\draw   (545,168) -- (566.5,168) -- (566.5,191) -- (545,191) -- cycle ;
		%Shape: Rectangle [id:dp2664105038921363] 
		\draw   (566.5,168) -- (588,168) -- (588,191) -- (566.5,191) -- cycle ;
		%Straight Lines [id:da8107688618373146] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]  [dash pattern={on 5.63pt off 4.5pt}]  (146,108) -- (146,209) ;
		%Shape: Rectangle [id:dp35779818027414945] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (373.37,169.15) -- (394.87,169.15) -- (394.87,190.57) -- (373.37,190.57) -- cycle ;
		%Shape: Ellipse [id:dp28380812239406916] 
		\draw  [line width=1.5]  (149.92,252.44) .. controls (151.67,252.12) and (153.58,254.59) .. (154.17,257.94) .. controls (154.77,261.29) and (153.83,264.25) .. (152.08,264.56) .. controls (150.33,264.88) and (148.42,262.41) .. (147.83,259.06) .. controls (147.23,255.71) and (148.17,252.75) .. (149.92,252.44) -- cycle ;
		%Shape: Ellipse [id:dp43879870252120257] 
		\draw  [line width=1.5]  (137.92,264.06) .. controls (139.67,264.38) and (141.58,261.91) .. (142.17,258.56) .. controls (142.77,255.21) and (141.83,252.25) .. (140.08,251.94) .. controls (138.33,251.62) and (136.42,254.09) .. (135.83,257.44) .. controls (135.23,260.79) and (136.17,263.75) .. (137.92,264.06) -- cycle ;
		%Shape: Triangle [id:dp6228907230125191] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (152.69,219.67) -- (141.95,253) -- (139.08,251.94) -- cycle ;
		%Shape: Triangle [id:dp7072702783999656] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (139.17,218.53) -- (150.85,251.54) -- (147.92,252.44) -- cycle ;
		%Shape: Rectangle [id:dp005310240084971829] 
		\draw  [pattern=_76ccdas62,pattern size=3.75pt,pattern thickness=0.75pt,pattern radius=0.75pt, pattern color={rgb, 255:red, 155; green, 155; blue, 155}] (292,148) -- (291.79,126.5) -- (311.92,137.08) -- (312.13,158.58) -- cycle ;
		%Shape: Rectangle [id:dp730774565123028] 
		\draw  [pattern=_ratcdpuab,pattern size=3.75pt,pattern thickness=0.75pt,pattern radius=0.75pt, pattern color={rgb, 255:red, 155; green, 155; blue, 155}] (312.97,158.84) -- (312.77,137.34) -- (332.9,147.92) -- (333.1,169.42) -- cycle ;
		%Shape: Rectangle [id:dp9126415444955993] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (295.02,168.26) -- (313.96,158.09) -- (313.94,180.83) -- (295,191) -- cycle ;
		%Shape: Rectangle [id:dp6185865053764277] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (313.96,158.09) -- (332.9,147.92) -- (332.88,170.66) -- (313.94,180.83) -- cycle ;
		%Shape: Rectangle [id:dp3219920458773482] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (332.9,147.92) -- (351.84,137.75) -- (351.82,160.49) -- (332.88,170.66) -- cycle ;
		%Shape: Rectangle [id:dp02737503156787735] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (351.84,137.75) -- (370.78,127.58) -- (370.76,150.32) -- (351.82,160.49) -- cycle ;
		%Shape: Rectangle [id:dp7641681779525133] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (333.1,169.42) -- (332.9,147.92) -- (353.03,158.5) -- (353.23,180) -- cycle ;
		%Shape: Rectangle [id:dp8882972246100118] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (353.23,180) -- (353.03,158.5) -- (373.16,169.08) -- (373.37,190.57) -- cycle ;
		
		% Text Node
		\draw (107,87.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {Ancestors};
		% Text Node
		\draw (296,87.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {Crossover};
		% Text Node
		\draw (482,87.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {Descendants};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Illustrative example of crossover}
	\end{figure}
	This operator allows the creation of two new individuals. However, an individual selected in the reproduction (selection) is not necessarily subjected to a crossover. The latter is carried out with a certain probability $P_c$. The more this probability is high and the more the population will undergo a crossover modification.

	Anyway, it is possible that the joint action of reproduction and the crossing is insufficient to ensure the success of the GA. Thus, in the case of binary encoding we have chosen so far, some information (i.e. the characters of the alphabet) may disappear from the population. Thus if no individual of the initial population contains a $1$ in the last position of the string and that we know a priori that this $1$ in the last position is part of the optimal string to find, all possible crosses will never show this $1$ initially unknown. In real number coding, such a situation can happen when using a simple crossover operator, it was such that the initial population was between $0$ and $40$ and that the optimal value was $50$. All possible combinations of convex digits belonging to the range $[0,40]$ will never allow to reach a the number of $50$. This is to address, among others, this problem that the mutation operator is used.
	
	As usually the crossover operation uses $2$ individuals. So, if you have $20$ individuals, we choose $10$ pairs to cross, and with a probability of $P_c=80\%$, on average we're going to cross only $8$ pairs.
	

	\subparagraph{Mutation operator}\mbox{}\\\\
	The purpose of this operator is to change randomly, with some probability, the value of a component of the individual. In the case of binary encoding, each bit $a_i\in\{0,1\}$ is replaced following a probability $P_m$ by its inverse ${a'}_i=1-a_i$. This is what is shown in the figure below. Like many crossover positions may be possible, we can very well admit that a same string can undergo several mutations.
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,970); %set diagram left start at 0, and has height of 970
		
		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_ygxtbjpju}{
		\makeatletter
		\pgfdeclarepatternformonly[\mcRadius,\mcThickness,\mcSize]{_ygxtbjpju}
		{\pgfpoint{-0.5*\mcSize}{-0.5*\mcSize}}
		{\pgfpoint{0.5*\mcSize}{0.5*\mcSize}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathcircle\pgfpointorigin{\mcRadius}
		\pgfusepath{stroke}
		}}
		
		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_b8429liec}{
		\makeatletter
		\pgfdeclarepatternformonly[\mcRadius,\mcThickness,\mcSize]{_b8429liec}
		{\pgfpoint{-0.5*\mcSize}{-0.5*\mcSize}}
		{\pgfpoint{0.5*\mcSize}{0.5*\mcSize}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathcircle\pgfpointorigin{\mcRadius}
		\pgfusepath{stroke}
		}}
		
		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_2t5hl85lx}{
		\makeatletter
		\pgfdeclarepatternformonly[\mcRadius,\mcThickness,\mcSize]{_2t5hl85lx}
		{\pgfpoint{-0.5*\mcSize}{-0.5*\mcSize}}
		{\pgfpoint{0.5*\mcSize}{0.5*\mcSize}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathcircle\pgfpointorigin{\mcRadius}
		\pgfusepath{stroke}
		}}
		
		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_6v1xlz7g3}{
		\makeatletter
		\pgfdeclarepatternformonly[\mcRadius,\mcThickness,\mcSize]{_6v1xlz7g3}
		{\pgfpoint{-0.5*\mcSize}{-0.5*\mcSize}}
		{\pgfpoint{0.5*\mcSize}{0.5*\mcSize}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathcircle\pgfpointorigin{\mcRadius}
		\pgfusepath{stroke}
		}}
		
		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_pnslnin5c}{
		\makeatletter
		\pgfdeclarepatternformonly[\mcRadius,\mcThickness,\mcSize]{_pnslnin5c}
		{\pgfpoint{-0.5*\mcSize}{-0.5*\mcSize}}
		{\pgfpoint{0.5*\mcSize}{0.5*\mcSize}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathcircle\pgfpointorigin{\mcRadius}
		\pgfusepath{stroke}
		}}
		
		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_egszundcu}{
		\makeatletter
		\pgfdeclarepatternformonly[\mcRadius,\mcThickness,\mcSize]{_egszundcu}
		{\pgfpoint{-0.5*\mcSize}{-0.5*\mcSize}}
		{\pgfpoint{0.5*\mcSize}{0.5*\mcSize}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathcircle\pgfpointorigin{\mcRadius}
		\pgfusepath{stroke}
		}}
		
		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_yxr24fqim}{
		\makeatletter
		\pgfdeclarepatternformonly[\mcRadius,\mcThickness,\mcSize]{_yxr24fqim}
		{\pgfpoint{-0.5*\mcSize}{-0.5*\mcSize}}
		{\pgfpoint{0.5*\mcSize}{0.5*\mcSize}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathcircle\pgfpointorigin{\mcRadius}
		\pgfusepath{stroke}
		}}
		
		%Shape: Rectangle [id:dp13217311784149932] 
		\draw  [pattern=_ygxtbjpju,pattern size=3.75pt,pattern thickness=0.75pt,pattern radius=0.75pt, pattern color={rgb, 255:red, 155; green, 155; blue, 155}] (137,162) -- (171.5,162) -- (171.5,190) -- (137,190) -- cycle ;
		%Shape: Rectangle [id:dp5018098225776586] 
		\draw  [pattern=_b8429liec,pattern size=3.75pt,pattern thickness=0.75pt,pattern radius=0.75pt, pattern color={rgb, 255:red, 155; green, 155; blue, 155}] (171.5,162) -- (206,162) -- (206,190) -- (171.5,190) -- cycle ;
		%Shape: Rectangle [id:dp735407971445361] 
		\draw   (206,162) -- (240.5,162) -- (240.5,190) -- (206,190) -- cycle ;
		%Shape: Rectangle [id:dp6821315347380095] 
		\draw  [pattern=_2t5hl85lx,pattern size=3.75pt,pattern thickness=0.75pt,pattern radius=0.75pt, pattern color={rgb, 255:red, 155; green, 155; blue, 155}] (240.5,162) -- (275,162) -- (275,190) -- (240.5,190) -- cycle ;
		%Shape: Rectangle [id:dp8484290416654925] 
		\draw   (275,162) -- (309.5,162) -- (309.5,190) -- (275,190) -- cycle ;
		%Shape: Rectangle [id:dp84741525822509] 
		\draw   (309.5,162) -- (344,162) -- (344,190) -- (309.5,190) -- cycle ;
		%Shape: Rectangle [id:dp11842005872572137] 
		\draw  [pattern=_6v1xlz7g3,pattern size=3.75pt,pattern thickness=0.75pt,pattern radius=0.75pt, pattern color={rgb, 255:red, 155; green, 155; blue, 155}] (137,232) -- (171.5,232) -- (171.5,260) -- (137,260) -- cycle ;
		%Shape: Rectangle [id:dp6820056767388305] 
		\draw  [pattern=_pnslnin5c,pattern size=3.75pt,pattern thickness=0.75pt,pattern radius=0.75pt, pattern color={rgb, 255:red, 155; green, 155; blue, 155}] (171.5,232) -- (206,232) -- (206,260) -- (171.5,260) -- cycle ;
		%Shape: Rectangle [id:dp4515019372680884] 
		\draw   (206,232) -- (240.5,232) -- (240.5,260) -- (206,260) -- cycle ;
		%Shape: Rectangle [id:dp48696775812238124] 
		\draw  [pattern=_egszundcu,pattern size=3.75pt,pattern thickness=0.75pt,pattern radius=0.75pt, pattern color={rgb, 255:red, 155; green, 155; blue, 155}] (240.5,232) -- (275,232) -- (275,260) -- (240.5,260) -- cycle ;
		%Shape: Rectangle [id:dp24318280140408266] 
		\draw   (275,232) -- (309.5,232) -- (309.5,260) -- (275,260) -- cycle ;
		%Shape: Rectangle [id:dp47224417051630607] 
		\draw  [pattern=_yxr24fqim,pattern size=3.75pt,pattern thickness=0.75pt,pattern radius=0.75pt, pattern color={rgb, 255:red, 155; green, 155; blue, 155}] (309.5,232) -- (344,232) -- (344,260) -- (309.5,260) -- cycle ;
		%Curve Lines [id:da788738177745401] 
		\draw    (273.5,138) .. controls (305.5,123) and (313.5,147) .. (321.5,157) ;
		%Curve Lines [id:da0799441626183599] 
		\draw    (321.5,157) .. controls (344.5,135) and (410.5,111) .. (440.5,177) ;
		%Shape: Cube [id:dp8671430740479884] 
		\draw   (425.5,202) -- (444.5,183) -- (487.5,183) -- (487.5,218) -- (468.5,237) -- (425.5,237) -- cycle ; \draw   (487.5,183) -- (468.5,202) -- (425.5,202) ; \draw   (468.5,202) -- (468.5,237) ;
		%Shape: Circle [id:dp23940536568113324] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (432,227) .. controls (432,224.79) and (433.79,223) .. (436,223) .. controls (438.21,223) and (440,224.79) .. (440,227) .. controls (440,229.21) and (438.21,231) .. (436,231) .. controls (433.79,231) and (432,229.21) .. (432,227) -- cycle ;
		%Shape: Circle [id:dp3668676113091487] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (443,219) .. controls (443,216.79) and (444.79,215) .. (447,215) .. controls (449.21,215) and (451,216.79) .. (451,219) .. controls (451,221.21) and (449.21,223) .. (447,223) .. controls (444.79,223) and (443,221.21) .. (443,219) -- cycle ;
		%Shape: Circle [id:dp3680240052509234] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (455,211) .. controls (455,208.79) and (456.79,207) .. (459,207) .. controls (461.21,207) and (463,208.79) .. (463,211) .. controls (463,213.21) and (461.21,215) .. (459,215) .. controls (456.79,215) and (455,213.21) .. (455,211) -- cycle ;
		%Shape: Ellipse [id:dp7359437437302359] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (475.72,213.15) .. controls (474.01,211.84) and (473.86,209.15) .. (475.4,207.13) .. controls (476.93,205.12) and (479.57,204.55) .. (481.28,205.85) .. controls (482.99,207.16) and (483.14,209.85) .. (481.6,211.87) .. controls (480.07,213.88) and (477.43,214.45) .. (475.72,213.15) -- cycle ;
		%Shape: Ellipse [id:dp6340362881921557] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (442.02,197.48) .. controls (440.61,195.84) and (441.04,193.18) .. (442.96,191.53) .. controls (444.89,189.89) and (447.58,189.88) .. (448.98,191.52) .. controls (450.39,193.16) and (449.96,195.82) .. (448.04,197.47) .. controls (446.11,199.11) and (443.42,199.12) .. (442.02,197.48) -- cycle ;
		%Shape: Ellipse [id:dp8342798141557779] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (458.82,193.23) .. controls (457.53,191.5) and (458.14,188.87) .. (460.18,187.37) .. controls (462.21,185.86) and (464.9,186.04) .. (466.18,187.77) .. controls (467.47,189.5) and (466.86,192.13) .. (464.82,193.63) .. controls (462.79,195.14) and (460.1,194.96) .. (458.82,193.23) -- cycle ;
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Illustrative example of mutation}
	\end{figure}
	The mutation is traditionally considered a marginal operator even if somehow it gives to genetic algorithms the ergodic property (i.e. all points of the search space can be achieved). However operator is of great importance. It performs a dual role: perform a local search and / or out a global search (remote search).
	
	The operators of the genetic algorithm are guided by a number of parameters set in advance. The value of these parameters affect the success of failure of a genetic algorithm. These parameters are (the reader can compare this list with the parameters available in the Microsoft Excel solver screenshot given earlier above):
	\begin{itemize}
		\item The size of the initial population $N$, and the coding length $l$ of each individual (in the case of binary encoding). If $N$ is too large, the computation time of the algorithm can be very important, and if $N$ is too small, it may converge too quickly to the wrong chromosome.

		\item The crossover probability $P_c$, that depends on the form of the fitness function. Its choice is general heuristic (just like $P_m$). The higher it is, the more the initial population obviously undergoes significant changes. The generally accepted values are between $0.5$ and $0.9$.

		\item The probability of mutation $P_m$ is generally small since a high rate may lead to a suboptimal solution.
		
		Rather than reducing $P_m$, another way to avoid the best individuals to be altered is to use explicit report of the elite individuals in a certain proportion. So often, the top $5\%$, for example, of the population is directly reused directly, the operator of reproduction (selection) or mutation operating then only on the remaining $95\%$. This is named an "\NewTerm{elitist strategy}\index{elitist strategy}".
	\end{itemize}
	Let us now see an example of GA (example of David E. Goldberg - 11989 according to holocene calendar).
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	We want to find the maximum of the function $(f)=x$ on the interval $[0,31]$ where $x$ is an integer. The first step consist in coding the function. For example, we use a binary coding of $x$, the sequence (chromosome) containing a maximum of $5$ bits. Thus, we have $x=2\rightarrow \{0,0,0,1,0\}$, and also $x=31\rightarrow \{1,1,1,1,1\}$. We are therefore seeking maximum of a fitness function (we will choose $f(x)$ itself in this simple example) in a space of $2^5=32$ possible values of $x$.
	\begin{enumerate}
		\item Sampling and evaluation of the initial population

		We set the size of the population to $N=4$. We draw randomly $4$ chromosomes knowing that a chromosome consists of $5$ bits, and each bit has a $50\%$ probability of having a value of $0$ or $1$. The maximum, (randomly) $16$ is reached by the second sequence. Let us see how the algorithm will try to improve this result.

		First, we get the following table:
		\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|c|c|c|c|c|}
				\hline
				\multicolumn{1}{c}{\cellcolor[gray]{0.75}N$^\circ$} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Chromosome}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Value}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Fitness}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}$\pmb{P_i\%}$} \\ \hline
				\cellcolor[gray]{0.75}$1$ & $00101$ & $5$ & $5$ & $14.3$ \\ \hline	
				\cellcolor[gray]{0.75}$2$ & $10000$ & $16$ & $16$ & $45.7$ \\ \hline	
				\cellcolor[gray]{0.75}$3$ & $00010$ & $2$ & $2$ & $5.7$ \\ \hline	
				\cellcolor[gray]{0.75}$4$ & $01100$ & $12$ & $12$ & $24.2$ \\ \hhline{|=|=|=|=|=|}	
				\cellcolor[gray]{0.75}\textbf{Total} & & & $\mathbf{35}$ & $\mathbf{100}$ \\ \hline	
				\end{tabular}
		\end{center}
		\caption[]{Evolution (mutation) of chromosomes}
		\end{table}
		We turn again the David E. Goldberg's wheel $4$ times to obtain the following sequence:
		\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|c|c|}
				\hline
				\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Selection (sampling)}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Chromosome}}  \\ \hline
				\cellcolor[gray]{0.75}$1$ & $10000$  \\ \hline
				\cellcolor[gray]{0.75}$2$ & $01100$  \\ \hline
				\cellcolor[gray]{0.75}$3$ & $00101$  \\ \hline
				\cellcolor[gray]{0.75}$4$ & $00101$  \\ \hline
				\end{tabular}
		\end{center}
		\caption[]{Sampling sequence of chromosomes}
		\end{table}
		We see here well the risk that we would have to lose the Sequence N$^\circ 2$ from the start... that's the problem with this method. It can converge more slowly than others. However, the reader will notice that we have lost the sequence N$^\circ 3$.

		We now turn to the crossover part: the ancestors are randomly selected. We randomly draw a crossover location ("site" or "loci") in the sequence. The crossing then operates at this location with a probability $P_c$. The table below shows the consequences of this operator assuming chromosomes $1$ and $3$, afterwards $2$ and $4$ are paired, and each time the crossing takes place (e.g. with $P_c=1$):
	\end{enumerate}
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|l|}
		\hline
	    \cellcolor[gray]{0.75}& \cellcolor[gray]{0.75}$\pmb{l=2}$ & \cellcolor[gray]{0.75}$\pmb{l=3}$  \\ \hline
		\textbf{Original Sequences}\multirow{2}{*}{\cellcolor[gray]{0.75}} & $100|00$  & $01|100$  \\ 
		\cellcolor[gray]{0.75} & $001|01$ & $10|000$  \\ \hline
		\textbf{Crossed Sequences}\multirow{2}{*}{\cellcolor[gray]{0.75}} & $10001$  & $01000$  \\ 
		 \cellcolor[gray]{0.75} & $00100$ & $10100$ \\ \hline
		\end{tabular}
		\caption[]{Chromosome crossing}
	\end{table}
	We now turn to the mutation part: in this binary coding example, the mutation is the occasional random modification (low probability) of the value of a bit (bit reversal). We thus draw for each bit a random number between $0$ and $1$ and if this digit is less than $P_m$ then the mutation takes place. The table below with $P_m=0.05$ highlights this process:
	\begin{table}[H]
		\begin{center}
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|c|c|c|c|}
		\hline
		\cellcolor[gray]{0.75}\textbf{Old chromosome} & 
		\cellcolor[gray]{0.75}\textbf{Random drawing} & \cellcolor[gray]{0.75}\textbf{New bit} & \cellcolor[gray]{0.75}\textbf{New chromosome} \\ \hline
		$10001$ & $15\;25\;36\;$ \textit{04} $\;12$ & $1$ & $10011$  \\ \hline
		$00100$ & $26\;89\;13\;48\;59$ & $-$ & $00100$  \\ \hline
		$01000$ & $32\;45\;87\;22\;65$ & $-$ & $01000$  \\ \hline
		$10100$ & $47\;$\textit{01}$\;85\;62\;35$ & $1$ & $11100$  \\ \hline
		\end{tabular}
		\end{center}
		\caption[]{Mutation of chromosomes}
	\end{table}
	Now that the new population is fully created, we can evaluate it again:
	\begin{table}[H]
		\begin{center}
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|c|c|c|c|c|}
		\hline
		\cellcolor[gray]{0.75}N$^\circ$& 
		\cellcolor[gray]{0.75}\textbf{Chromosome} & \cellcolor[gray]{0.75}\textbf{Value} & \cellcolor[gray]{0.75}\textbf{Fitness}$f(x)$ & $\pmb{P_i\%}$ \\ \hline
		\cellcolor[gray]{0.75}$1$ & $10011$ & $19$ & $19$ & $32.2$\\ \hline
		\cellcolor[gray]{0.75}$2$ & $00100$ & $4$ & $4$ & $6.8$\\ \hline
		\cellcolor[gray]{0.75}$3$ & $01000$ & $8$ & $8$ & $13.5$\\ \hline
		\cellcolor[gray]{0.75}$4$ & $11100$ & $28$ & $28$ & $47.5$\\ \hhline{|=|=|=|=|=|}
		\cellcolor[gray]{0.75}\textbf{Total} &  & & \pmb{59} & \pmb{100} \\ \hline
		\end{tabular}
		\end{center}
		\caption[]{Evaluation of the mutation of chromosomes}
	\end{table}
	The maximum is now $28$ (N$^\circ 4$). So we went from $16$ to $28$ after a single generation. Of course, we must repeat the procedure from the selection stage until the overall maximum, $31$, is obtained, or until that a stop criterion has been satisfied.
	\end{tcolorbox}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	It is possible to prove mathematically, what's remarkable (!!!), that the portions of chromosomes that are found in the best individuals will tend to reproduce ...
	\end{tcolorbox}
	The reader interested can also take a look to the MATLAB™ companion book where we use GAs to find the optimum of the Rastriging function:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/rastriging_function.jpg}
		\caption{Rastriging function in MATLAB™ 2013a}
	\end{figure}
	where it works quite well as shown it the result below:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/rastriging_function_matlab_optimum.jpg}
	\end{figure}
	and also for neural networks optimization.
	
	\pagebreak
	\subsubsection{Total Unduplicated Reach and Frequency Analysis (TURF)}
	"\NewTerm{TURF Analysis}\index{TURF Analysis}", an acronym for "\NewTerm{Total Unduplicated Reach and Frequency}\index{total unduplicated reach and frequency}", is a type of statistical analysis used for providing estimates of media or market potential and devising optimal communication and placement strategies given limited resources. TURF analysis identifies the number of users reached by a communication, and how often they are reached.

	Although originally used by media schedulers to maximize reach and frequency of media spending across different items (print, broadcast, etc.), TURF is also now used to provide estimates of market potential. For example, if a company plans to market a new yoghurt, they may consider launching $10$ possible flavours, but in reality, only three might be purchased in large quantities. The TURF algorithm identifies the optimal product line to maximize the total number of consumers who will purchase at least one SKU\footnote{A SKU is a distinct string of letters and numbers that helps retailers identify every product in their inventory and each product's specific traits, like its manufacturer, brand, price, style, color, and size.} (Stock Keeping Unit). Typically, when TURF is undertaken for optimizing a product range, the analysis only looks at the reach of the product range (ignoring the Frequency component of TURF).

	In order to obtain data on the items being evaluated, ratings/choices may be obtained via quantitative marketing research (such as a survey).
	
	Let us give a companion example! Consider we want to sell three new ice flavours. We did a non-representative survey on $10$ people that lead us to the following table:
	\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|c|c|c|}
	\hline
	\rowcolor[gray]{0.75} 
	\textbf{Individual} & \textbf{Weight} & \textbf{Chocolate ($A$)} & \textbf{Vanilla ($B$)} & \textbf{Pistachio ($C$)} \\ \hline
	1 & 1 & 1 & 0 & 0 \\ \hline
	2 & 1 & 1 & 0 & 0 \\ \hline
	3 & 1 & 1 & 0 & 0 \\ \hline
	4 & 1 & 0 & 1 & 0 \\ \hline
	5 & 1 & 0 & 1 & 0 \\ \hline
	6 & 1 & 0 & 1 & 0 \\ \hline
	7 & 1 & 1 & 1 & 0 \\ \hline
	8 & 1 & 1 & 1 & 0 \\ \hline
	9 & 1 & 0 & 1 & 1 \\ \hline
	10 & 1 & 1 & 1 & 1 \\ \hline
	\end{tabular}
	\end{table}
	
	Three possible ice cream flavours available. After a survey, we get:
	\begin{itemize}
		\item Flavour $A$ was chosen by $60\%$ (6/10) of the people
		\begin{itemize}
			\item $50\%$ (3/6) chose $A$ exclusively
			\item $33.\bar{3}\%$ (2/6) chose also $B$
			\item $0\%$ (0/6) chose also $C$
			\item $16.\bar{6}\%$ (1/6) chose also $B$ plus $C$
		\end{itemize}
		\item Flavour $B$ was chosen by $70\%$ (7/10) of the people
		\begin{itemize}
			\item $42.85\%$ (3/7) chose $B$ exclusively
			\item $28.57\%$ (2/7) chose also $A$
			\item $14.29\%$ (1/7) chose also $C$
			\item $14.29\%$ (1/7) chose also $A$ plus $C$
		\end{itemize}
		\item Flavour $C$ was chosen by $20\%$ (2/10) of the people
		\begin{itemize}
			\item $0\%$  (0/2) chose $C$ exclusively
			\item $0\%$ (0/2) chose also $A$
			\item $50\%$ (1/2) chose also $B$
			\item $50\%$ (1/2) chose also $A$ plus $B$
		\end{itemize}
	\end{itemize}
	An obvious "macro-result" if we would have to choose only $2$ flavour, would be to choose flavours $A$ and $B$.
	
	However we must also focus on the combinations focus on all combinations of $2$ among $3$ flavours to check if it match our previous naive macro analysis (and that's may not be always the case!!!).
	
	We then have:
	
	to analyse! It's easy to do by hand and lead us to the following reaching rates:
	\begin{itemize}
		\item $100\%$ Would chose $A$ alone, $B$ alone or $A$ and $B$ together
		\item $70\%$ Would chose $A$ alone, $C$ alone or $A$ and $C$ together
		\item $70\%$ Would chose $B$ alone, $C$ alone or $B$ and $C$ together
	\end{itemize}
	So in this special case. The TURF analysis confirms the naive macro analysis!
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The reader can find this example in our \texttt{R} companion book.
	\end{tcolorbox}
	
	TURF analysis allows us to see the effect of the combinations of answers that people make that we may not see in simple percentage choices. 
	
	Making decisions based on simple percentage choice can be wrong. TURF analysis then allows us to take into account the complex combinations of choices that people make when selecting products or services.
	
	Obviously, TURF analysis, has many flaws, among other the fact that  is purely deterministic. But this can be quite simply resolved using bootstrapping.
	
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{60} & \pbox{20cm}{\score{3}{5} \\ {\tiny 23 votes,  58.26\%}} 
	\end{tabular} 
	\end{flushright}


	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Fractals}\label{fractals}
	\lettrine[lines=4]{\color{BrickRed}F}{ractals} are figures invariant by scale change (we also talk about "self-similar structures") and are the graphic representation of contractant recurrent sequences (for IFS fractals that we will see later) or not divergent (for escape-time fractals as we will see further below).
	
	The basic idea - simple and great ... at the same time - often involves taking a starting point, to build its image through a particular mathematical function, to take the image of the image and so on. The goal is to study how the successive points are allocate in the global target set of the defined function, if they are approaching a limit or if they roam between different values that can we explain, if more points in part of the set than another?
	
	The advantage of this type of questions concerns both the study of the evolution of biological populations than the future of the solar system, 3D computing (the origin being the generation of mountains for 3D landscapes) changes in stock prices or random number generation in particular fields, or even medical diagnostic (especially for brain or heart).
	
	See below a simple example image by image:
	\begin{figure}[H]
		\centering
		\includegraphics{img/geometry/fractal_mountain_1.jpg}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics{img/geometry/fractal_mountain_2.jpg}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics{img/geometry/fractal_mountain_3.jpg}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics{img/geometry/fractal_mountain_4.jpg}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics{img/geometry/fractal_mountain_5.jpg}
		\caption{Pseudo-mountains generation from a random fractal (probabilistic fractal)}
	\end{figure}
	For the average person, fractals are used to look pretty. But they have far more serious applications: for example we already saw in this book that some of these "attractive" images reproduced physical phenomena (population dynamics for the Feigenbaum's Fractal, turbulence in a fluid with the Lorentz attractor, distribution of galaxies, L-Fractals, clusters and supercluster of galaxies, ...). Fractals have also found applications in music (with software generating fractal music) and in film (3D to generate mountains, fire, grass). In the field of computer graphics, fractals are used to compress images very effectively, with consistent quality regardless of the zoom, they help to create realistic textures, and can afford to dither an image with good results. Fractals are also used to reduce the size of the receive antennas and to extend their effectiveness frequency spectrum (some of our cell phones of the early 121st century (holocene calendar) have fractal receptor of the type "Sierpinski carpet" - see below - because of all types of frequencies they need to manage!). In civil engineering fractals are used for building some sound absorbers walls. And many other things... Finally, in cosmology, fractals are also used to attempt to give a maximum structure size to check if the cosmological principle really holds in our observable universe (see \cite{yadav2010fractal}).
	
	This fractal geometry differs from the Euclidean geometry first by its definition: the figures of Euclidean geometry are generally determined by algebraic relations, while the fractal curves are defined recursively as we have already mentioned. Fractals also have fractional dimensions (we have already discussed this topic in the section of Euclidean geometry when defining the concept of dimension). On the other hand, we must not neglect their autosimilar appearance: each part of a fractal can be observed at any scale: each part is (essentially) a copy of the whole.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The developments that follow could easily have been put in the section of Sequences and Series or even of Functional Analysis or seen as a special case of the section Topology reduces to the Euclidean space (this is why you will find here also many references to the topology section). Our choice is pedagogical as well as for the section of Cryptography, in the sense that it is much more interesting for a high-school student to see an application of abstract concepts of topology in a practical framework (and furthermore aesthetics) where they are absolutely necessary for a proper understanding of the subject rather than in a framework where we can escape them very well without too suffer. The reader will find here some developments and theorems proposed elsewhere in this book and this only in the order to avoid having too "turn pages" too much.
	\end{tcolorbox}	
	
	Natural fractals are named "\NewTerm{non-deterministic objects}\index{non-deterministic objects}" because the dynamic process that allows their creation itself varies with time randomly (see the section of Population Dynamics for an excellent example). Nevertheless, we can try to model dynamic systems that lead to fractal objects under a rigorous mathematical form (that is still a good example of the way in which the mathematicians manage to make a simple and intuitive concept into a concrete abstract mathematical model and somewhat confusing as for Knot Theory).
	
	In this section, we will consider the study of two families of fractals that will be in order:
	\begin{enumerate}
		\item Deterministic fractals based on iterated function that are strictly self-similar. They are generated, as we shall see, by the recursive application of contracting functions on subsets of a metric space. The fixed point theorem will guarantee (as we shall also see!) the existence and uniqueness of a "\NewTerm{fixed subset}\index{fixed subset}" of the metric space, towards which every subset converge.
		
		\item Escape-time fractals (also known as "fractals by induction") that are not strictly self-similar: They are generated as we shall see later by recurring non-divergent sequences. The fixed point theorem serving as guarantee for the non-divergence of the function with respect to the chosen starting points.
	\end{enumerate}
	
	\subsection{IFS Fractals}
	Let us start by looking at the first family of fractals discovered by Michael Barnsley in 11987 (holocene calendar): the "\NewTerm{deterministic iterated function systems IFS}\index{deterministic iterated function systems}".
	
	Of all fractals, figures only those built by iterated function systems usually shows the self-similarity property, meaning that their complexity is invariant under change of scale.
	
	Let us start by "bounding" the thing...
	
	We take an initial geometry $E_0$ of space $E$, a function $f$ from $E$ to $E$ such that:
	
	(which requires that the initial object can not leave its own definition domain through the iteration of the function $f$) and we create the discrete dynamical system defined by:
	
	Under certain conditions we will now see, the sequence of geometric objects $(E_0)$ "tends" to a limit, which is often a fractal object (we will see further below some famous examples).
	
	Naturally, there is a rigorous mathematical framework in which the mentioned conditions and the verb "tends" have a precise definition. In particular, the objects $E_n$ are all compacts of $E$, that is to say bounded subsets (that we can include in a segment if $E$ is a straight line, in a disk if $E$ is a plane or a ball if $E$ is the three-dimensional space) and closed (every convergent sequence of $E_n$ has its limit in $E$). We place ourselves then in the compact metric space, equipped with the Hausdorff distance (see below for definition) for which we will show that it is complete when we work with compact sets the plane and  of space, and we will check that $f$ is an "\NewTerm{Hutchinson operator}\index{Hutchinson operator}", i.e. a contraction application from the space of the compact in itself for that distance. It then will then just remain to apply the fixed point theorem.
	
	Dynamic systems of this type are said to be "deterministic", and therefore named "IFS" (iterated function systems deterministic). Let us precise that the limit of the IFS is named the "\NewTerm{attractor of the IFS}\index{attractor of the IFS}". We can show that under the conditions mentioned above, this attractor does not depend on the shape of the original geometric object (we will see practical examples further below).
	
	Initially, we will limit our study to $\mathbb{R}$ (the general case is given in section Topology) knowing anyway that a generalization to the two-dimensional Euclidean space does not require too big and intellectual work and that the whole complex is isomorphic to it.
	
	\textbf{Definition (\#\thesection.\mydef):}
	To enable us to define the boundaries of our fractal functions let us consider $X \subseteq \mathbb{R}$. We say that $\xi$ is the "\NewTerm{supremum}\index{supremum}" of $X$ and denote it by:
	
	if $\xi$ is the smallest "\NewTerm{upper bound}\index{upper bound}" of $X$ (an upper bound of $X$ is a number $a$ that satisfies $\forall x \in X,x\leq a$).
	
	Similarly, we say that $\xi$ is an "\NewTerm{infimum}\index{infimum}" $X$ and denote it by:
	
	if $\xi$ is the biggest "\NewTerm{lower bound}\index{lower bound}" of $X$ (a lower bound of $X$ is a number $a$ that satisfies $\forall x \in X,a\leq x$).
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	We often use the following characterization of the supremum:
	
	if and only if:
	
	which is almost obvious because we can approach as close as we want of $\xi$ with elements of $X$ (think with small $\varepsilon$). For information, we then also have in the same idea:
	
	if and only if:
	
	\end{tcolorbox}	
	We consider as intuitive that if $X \subseteq \mathbb{R}$ has an upper bound, that is to say if there exists $a \in \mathbb{R}$ as $\forall x \in X,x \leq a$ (respectively lower bounded), then $X$ has a supremum (respectively infimum).
	
	We will see later that it is this property that will give us the possibility to prove later $\mathbb{R}$ a "\NewTerm{complete metric space}\index{complete metric space}"!
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	By the way, let us underline the importance of taking $\mathbb{R}$ as a definition for metric space for this property to be satisfied. We can in fact notice that it is not verified in the set $\mathbb{Q}$ of rational numbers with the following simple example:
	
	which is majorated but has no supremum in $\mathbb{Q}$ because this supremum is in $\mathbb{R}$ as:
	
	Therefore:
	
	This is what makes $\mathbb{Q}$ is not "complete".
	\end{tcolorbox}
	
	\textbf{Definition (\#\thesection.\mydef)}: We say that $X\subseteq\mathbb{R} $ is "\NewTerm{bounded}\index{bounded}" if $X$ is minorated and majorated.
	
	From the definition it follows immediately that $X$ is bounded if and only if there exists $a,b$ with such that $X\subseteq [a,b]$.
	
	Now that the concept of bound is relatively well defined, let see how a sequence can behave near from it.
	
	\textbf{Definition (\#\thesection.\mydef):} We say that $(a_n)_{\mathbb{N}}$ of $\mathbb{R}$ is an "\NewTerm{increasing sequence}\index{increasing sequence}" ("decreasing" respectively) if:
	
	respectively:
	
	We say the sequence $(a_n)_{\mathbb{N}}$ is "\NewTerm{monotone}\index{monotone}" if it is increasing or decreasing as we have already seen in the section Sequences and Series.
	
	\textbf{Definition (\#\thesection.\mydef):} Given $T=\left\lbrace n_0,n_1,n_2,\ldots \right\rbrace$ infinite subset of $\mathbb{N}$ with $n_0<n_1<n_2<\ldots $. We say the sequence $(a_{n_i})_{i\in \mathbb{N}}$  is a "\NewTerm{subsequence}\index{subsequence}" of the sequence $(a_n)_{\mathbb{N}}$.
	
	\begin{theorem}
	Let us now prove that every sequence in $\mathbb{R}$ admits a monotone subsequence (it's a bit the idea of a fractal!)
	\end{theorem}
	\begin{dem}
	We say that $a_m$ is a "\NewTerm{peak}\index{peak}" of the sequence if:
	
	Consider the set $P$ of peaks of the sequence $(a_n)_{\mathbb{N}}$.
	\begin{itemize}
		\item If $P$ is infinite then the subsequence $(a_n)_{n\in P}$ is monotone since decreasing.
		
		\item If $P$ is finite or empty:
		
		(if $P=\varnothing$ we choose any $m_1$). $a_{m_1}$ is therefore not by construction not a peak, so there exists $m_2\geq m_1$ such as $a_{m_1}\leq a_{m_2}$. In turn $a_{m_2}$ is not a peak, so there exist $m_3\geq m_2$ such as $a_{m_2}\leq a_{m_3}$ etc. We see that we define thus as an increasing subsequence.
		
		\textbf{Definition (\#\thesection.\mydef):} We say that the sequence $(a_n)_\mathbb{N}$ "\NewTerm{converge}\index{convergent sequence}" into $a\in \mathbb{R}$ and we note this:
		
		if:
		
		In this case we say that $a$ is the "\NewTerm{limit of the sequence}\index{limit of the sequence}" $(a_n)_\mathbb{N}$.
	\end{itemize}
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	In the example of the figure below where the sequence seems to converge towards the value $1.13$, we observe that for a particular non-zero positive $\varepsilon$, there exists a particular $n$ which we will denote $N$ (which value is $17$ in the below example) from which the sequence converges.
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,728); %set diagram left start at 0, and has height of 728
		
		%Straight Lines [id:da739727983387134] 
		\draw    (154,69) -- (154,351) ;
		%Straight Lines [id:da7971225667363602] 
		\draw    (154,70.48) -- (166.5,70.48) ;
		%Straight Lines [id:da18455643524522003] 
		\draw    (154,75.14) -- (156.52,75.14) ;
		%Straight Lines [id:da4338673118759575] 
		\draw    (154,79.8) -- (156.52,79.8) ;
		%Straight Lines [id:da9842016535464555] 
		\draw    (154,84.46) -- (156.52,84.46) ;
		%Straight Lines [id:da9493773111417427] 
		\draw    (154,89.12) -- (156.52,89.12) ;
		%Straight Lines [id:da3329609136438856] 
		\draw    (154,93.78) -- (156.52,93.78) ;
		%Straight Lines [id:da007243773999864089] 
		\draw    (154,98.44) -- (156.52,98.44) ;
		%Straight Lines [id:da38626577473906565] 
		\draw    (154,103.1) -- (156.52,103.1) ;
		%Straight Lines [id:da38707294602934716] 
		\draw    (154,107.76) -- (156.52,107.76) ;
		%Straight Lines [id:da4421966044011423] 
		\draw    (154,112.42) -- (156.52,112.42) ;
		%Straight Lines [id:da7401852666704418] 
		\draw    (154,117.08) -- (166.5,117.08) ;
		%Straight Lines [id:da19064451073147426] 
		\draw    (154,121.74) -- (156.52,121.74) ;
		%Straight Lines [id:da2326129331452158] 
		\draw    (154,126.4) -- (156.52,126.4) ;
		%Straight Lines [id:da7748845578685282] 
		\draw    (154,131.06) -- (156.52,131.06) ;
		%Straight Lines [id:da09102832275596007] 
		\draw    (154,135.72) -- (156.52,135.72) ;
		%Straight Lines [id:da5266737154580199] 
		\draw    (154,140.38) -- (156.52,140.38) ;
		%Straight Lines [id:da01738912887664923] 
		\draw    (154,145.04) -- (156.52,145.04) ;
		%Straight Lines [id:da5287687806315862] 
		\draw    (154,149.7) -- (156.52,149.7) ;
		%Straight Lines [id:da4264143621690357] 
		\draw    (154,154.36) -- (156.52,154.36) ;
		%Straight Lines [id:da3430188585270566] 
		\draw    (154,159.02) -- (156.52,159.02) ;
		%Straight Lines [id:da9371098507248226] 
		\draw    (154,163.68) -- (166.5,163.68) ;
		%Straight Lines [id:da8111901143336537] 
		\draw    (154,168.34) -- (156.52,168.34) ;
		%Straight Lines [id:da3983418074019711] 
		\draw    (154,173) -- (156.52,173) ;
		%Straight Lines [id:da2695298114198268] 
		\draw    (154,177.66) -- (156.52,177.66) ;
		%Straight Lines [id:da46623686997373714] 
		\draw    (154,182.32) -- (156.52,182.32) ;
		%Straight Lines [id:da050965934367303234] 
		\draw    (154,186.98) -- (156.52,186.98) ;
		%Straight Lines [id:da8415488811312217] 
		\draw    (154,191.64) -- (156.52,191.64) ;
		%Straight Lines [id:da42779227712892465] 
		\draw    (154,196.3) -- (156.52,196.3) ;
		%Straight Lines [id:da3089190674826958] 
		\draw    (154,200.96) -- (156.52,200.96) ;
		%Straight Lines [id:da7335994957750736] 
		\draw    (154,205.62) -- (156.52,205.62) ;
		%Straight Lines [id:da1954323979441388] 
		\draw    (154,210.28) -- (166.5,210.28) ;
		%Straight Lines [id:da2362618100325251] 
		\draw    (154,214.94) -- (156.52,214.94) ;
		%Straight Lines [id:da47356336902709883] 
		\draw    (154,219.6) -- (156.52,219.6) ;
		%Straight Lines [id:da5587584722274859] 
		\draw    (154,224.26) -- (156.52,224.26) ;
		%Straight Lines [id:da5147361112549096] 
		\draw    (154,228.92) -- (156.52,228.92) ;
		%Straight Lines [id:da477226908735203] 
		\draw    (154,233.58) -- (156.52,233.58) ;
		%Straight Lines [id:da4318928231929231] 
		\draw    (154,238.24) -- (156.52,238.24) ;
		%Straight Lines [id:da05046082675113217] 
		\draw    (154,242.9) -- (156.52,242.9) ;
		%Straight Lines [id:da36241665423745206] 
		\draw    (154,247.56) -- (156.52,247.56) ;
		%Straight Lines [id:da4972667914710063] 
		\draw    (154,252.2) -- (156.52,252.2) ;
		%Straight Lines [id:da5794248950923369] 
		\draw    (154.4,256.88) -- (166.9,256.88) ;
		%Straight Lines [id:da5246571795151036] 
		\draw    (154.4,261.54) -- (156.92,261.54) ;
		%Straight Lines [id:da11344171271835157] 
		\draw    (154.4,266.2) -- (156.92,266.2) ;
		%Straight Lines [id:da7839124593508286] 
		\draw    (154.4,270.86) -- (156.92,270.86) ;
		%Straight Lines [id:da3497964796421318] 
		\draw    (154.4,275.52) -- (156.92,275.52) ;
		%Straight Lines [id:da37540029791180163] 
		\draw    (154.4,280.18) -- (156.92,280.18) ;
		%Straight Lines [id:da48527077104666194] 
		\draw    (154.4,284.84) -- (156.92,284.84) ;
		%Straight Lines [id:da09063447037501815] 
		\draw    (154.4,289.5) -- (156.92,289.5) ;
		%Straight Lines [id:da680411192192413] 
		\draw    (154.4,294.16) -- (156.92,294.16) ;
		%Straight Lines [id:da23596459881228915] 
		\draw    (154.4,298.82) -- (156.92,298.82) ;
		%Straight Lines [id:da25396869483505236] 
		\draw    (154.4,303.48) -- (166.9,303.48) ;
		%Straight Lines [id:da3569958387089418] 
		\draw    (154.4,308.14) -- (156.92,308.14) ;
		%Straight Lines [id:da5929289523814973] 
		\draw    (154.4,312.8) -- (156.92,312.8) ;
		%Straight Lines [id:da20960831317393636] 
		\draw    (154.4,317.46) -- (156.92,317.46) ;
		%Straight Lines [id:da35116135865830334] 
		\draw    (154.4,322.12) -- (156.92,322.12) ;
		%Straight Lines [id:da9155357508824211] 
		\draw    (154.4,326.78) -- (156.92,326.78) ;
		%Straight Lines [id:da0026003956719078403] 
		\draw    (154.4,331.44) -- (156.92,331.44) ;
		%Straight Lines [id:da6000859082372039] 
		\draw    (154.4,336.1) -- (156.92,336.1) ;
		%Straight Lines [id:da9243788074546431] 
		\draw    (154.4,340.76) -- (156.92,340.76) ;
		%Straight Lines [id:da20324495492384753] 
		\draw    (154.4,345.4) -- (156.92,345.4) ;
		%Straight Lines [id:da469497312872023] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (156.52,242.9) -- (580.5,242.9) ;
		%Straight Lines [id:da8112633718002489] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 0.84pt off 2.51pt}]  (156.52,214.94) -- (592.5,214.94) ;
		%Straight Lines [id:da8059944112352218] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 0.84pt off 2.51pt}]  (154.4,270.86) -- (595.5,270.86) ;
		%Shape: Brace [id:dp8307810419087867] 
		\draw   (568,242.6) .. controls (571.84,242.6) and (573.76,240.68) .. (573.76,236.84) -- (573.76,236.84) .. controls (573.76,231.35) and (575.68,228.6) .. (579.53,228.6) .. controls (575.68,228.6) and (573.76,225.85) .. (573.76,220.36)(573.76,222.84) -- (573.76,220.36) .. controls (573.76,216.52) and (571.84,214.6) .. (568,214.6) ;
		%Shape: Brace [id:dp5445555844617578] 
		\draw   (568,270.6) .. controls (571.84,270.6) and (573.76,268.68) .. (573.76,264.84) -- (573.76,264.84) .. controls (573.76,259.35) and (575.68,256.6) .. (579.53,256.6) .. controls (575.68,256.6) and (573.76,253.85) .. (573.76,248.36)(573.76,250.84) -- (573.76,248.36) .. controls (573.76,244.52) and (571.84,242.6) .. (568,242.6) ;
		%Straight Lines [id:da49538460117792393] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (362,244) -- (362,323.6) ;
		%Shape: Circle [id:dp5366576278683739] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (162.6,304.78) .. controls (162.6,302.41) and (164.53,300.48) .. (166.9,300.48) .. controls (169.27,300.48) and (171.2,302.41) .. (171.2,304.78) .. controls (171.2,307.15) and (169.27,309.08) .. (166.9,309.08) .. controls (164.53,309.08) and (162.6,307.15) .. (162.6,304.78) -- cycle ;
		%Shape: Circle [id:dp658813906644065] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (172.4,70.3) .. controls (172.4,67.93) and (174.33,66) .. (176.7,66) .. controls (179.07,66) and (181,67.93) .. (181,70.3) .. controls (181,72.67) and (179.07,74.6) .. (176.7,74.6) .. controls (174.33,74.6) and (172.4,72.67) .. (172.4,70.3) -- cycle ;
		%Shape: Circle [id:dp9847432053807776] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (186.4,226.3) .. controls (186.4,223.93) and (188.33,222) .. (190.7,222) .. controls (193.07,222) and (195,223.93) .. (195,226.3) .. controls (195,228.67) and (193.07,230.6) .. (190.7,230.6) .. controls (188.33,230.6) and (186.4,228.67) .. (186.4,226.3) -- cycle ;
		%Shape: Circle [id:dp09416634324329887] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (197.4,345.3) .. controls (197.4,342.93) and (199.33,341) .. (201.7,341) .. controls (204.07,341) and (206,342.93) .. (206,345.3) .. controls (206,347.67) and (204.07,349.6) .. (201.7,349.6) .. controls (199.33,349.6) and (197.4,347.67) .. (197.4,345.3) -- cycle ;
		%Shape: Circle [id:dp9443133941138235] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (210.4,249.3) .. controls (210.4,246.93) and (212.33,245) .. (214.7,245) .. controls (217.07,245) and (219,246.93) .. (219,249.3) .. controls (219,251.67) and (217.07,253.6) .. (214.7,253.6) .. controls (212.33,253.6) and (210.4,251.67) .. (210.4,249.3) -- cycle ;
		%Shape: Circle [id:dp3385759221349227] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (222.4,172.3) .. controls (222.4,169.93) and (224.33,168) .. (226.7,168) .. controls (229.07,168) and (231,169.93) .. (231,172.3) .. controls (231,174.67) and (229.07,176.6) .. (226.7,176.6) .. controls (224.33,176.6) and (222.4,174.67) .. (222.4,172.3) -- cycle ;
		%Shape: Circle [id:dp9250043049430812] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (234.4,239.3) .. controls (234.4,236.93) and (236.33,235) .. (238.7,235) .. controls (241.07,235) and (243,236.93) .. (243,239.3) .. controls (243,241.67) and (241.07,243.6) .. (238.7,243.6) .. controls (236.33,243.6) and (234.4,241.67) .. (234.4,239.3) -- cycle ;
		%Shape: Circle [id:dp2892986975494274] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (246.4,298.3) .. controls (246.4,295.93) and (248.33,294) .. (250.7,294) .. controls (253.07,294) and (255,295.93) .. (255,298.3) .. controls (255,300.67) and (253.07,302.6) .. (250.7,302.6) .. controls (248.33,302.6) and (246.4,300.67) .. (246.4,298.3) -- cycle ;
		%Shape: Circle [id:dp2966579330935062] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (258.4,244.3) .. controls (258.4,241.93) and (260.33,240) .. (262.7,240) .. controls (265.07,240) and (267,241.93) .. (267,244.3) .. controls (267,246.67) and (265.07,248.6) .. (262.7,248.6) .. controls (260.33,248.6) and (258.4,246.67) .. (258.4,244.3) -- cycle ;
		%Shape: Circle [id:dp3954726798692523] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (270.4,198.3) .. controls (270.4,195.93) and (272.33,194) .. (274.7,194) .. controls (277.07,194) and (279,195.93) .. (279,198.3) .. controls (279,200.67) and (277.07,202.6) .. (274.7,202.6) .. controls (272.33,202.6) and (270.4,200.67) .. (270.4,198.3) -- cycle ;
		%Shape: Circle [id:dp00013164174141322427] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (282.4,241.3) .. controls (282.4,238.93) and (284.33,237) .. (286.7,237) .. controls (289.07,237) and (291,238.93) .. (291,241.3) .. controls (291,243.67) and (289.07,245.6) .. (286.7,245.6) .. controls (284.33,245.6) and (282.4,243.67) .. (282.4,241.3) -- cycle ;
		%Shape: Circle [id:dp09250067456248967] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (295.4,281.3) .. controls (295.4,278.93) and (297.33,277) .. (299.7,277) .. controls (302.07,277) and (304,278.93) .. (304,281.3) .. controls (304,283.67) and (302.07,285.6) .. (299.7,285.6) .. controls (297.33,285.6) and (295.4,283.67) .. (295.4,281.3) -- cycle ;
		%Shape: Circle [id:dp8490442415957002] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (308.4,243.3) .. controls (308.4,240.93) and (310.33,239) .. (312.7,239) .. controls (315.07,239) and (317,240.93) .. (317,243.3) .. controls (317,245.67) and (315.07,247.6) .. (312.7,247.6) .. controls (310.33,247.6) and (308.4,245.67) .. (308.4,243.3) -- cycle ;
		%Shape: Circle [id:dp2995469087269129] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (319.4,210.3) .. controls (319.4,207.93) and (321.33,206) .. (323.7,206) .. controls (326.07,206) and (328,207.93) .. (328,210.3) .. controls (328,212.67) and (326.07,214.6) .. (323.7,214.6) .. controls (321.33,214.6) and (319.4,212.67) .. (319.4,210.3) -- cycle ;
		%Shape: Circle [id:dp030602887718145233] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (332.4,242.3) .. controls (332.4,239.93) and (334.33,238) .. (336.7,238) .. controls (339.07,238) and (341,239.93) .. (341,242.3) .. controls (341,244.67) and (339.07,246.6) .. (336.7,246.6) .. controls (334.33,246.6) and (332.4,244.67) .. (332.4,242.3) -- cycle ;
		%Shape: Circle [id:dp09764444331952404] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (343.4,271.3) .. controls (343.4,268.93) and (345.33,267) .. (347.7,267) .. controls (350.07,267) and (352,268.93) .. (352,271.3) .. controls (352,273.67) and (350.07,275.6) .. (347.7,275.6) .. controls (345.33,275.6) and (343.4,273.67) .. (343.4,271.3) -- cycle ;
		%Shape: Circle [id:dp17411395469792068] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (356.4,243.3) .. controls (356.4,240.93) and (358.33,239) .. (360.7,239) .. controls (363.07,239) and (365,240.93) .. (365,243.3) .. controls (365,245.67) and (363.07,247.6) .. (360.7,247.6) .. controls (358.33,247.6) and (356.4,245.67) .. (356.4,243.3) -- cycle ;
		%Shape: Circle [id:dp2625133342343453] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (369.4,217.3) .. controls (369.4,214.93) and (371.33,213) .. (373.7,213) .. controls (376.07,213) and (378,214.93) .. (378,217.3) .. controls (378,219.67) and (376.07,221.6) .. (373.7,221.6) .. controls (371.33,221.6) and (369.4,219.67) .. (369.4,217.3) -- cycle ;
		%Shape: Circle [id:dp9929930524238977] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (381.4,242.3) .. controls (381.4,239.93) and (383.33,238) .. (385.7,238) .. controls (388.07,238) and (390,239.93) .. (390,242.3) .. controls (390,244.67) and (388.07,246.6) .. (385.7,246.6) .. controls (383.33,246.6) and (381.4,244.67) .. (381.4,242.3) -- cycle ;
		%Shape: Circle [id:dp7271720485400492] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (394.4,268.3) .. controls (394.4,265.93) and (396.33,264) .. (398.7,264) .. controls (401.07,264) and (403,265.93) .. (403,268.3) .. controls (403,270.67) and (401.07,272.6) .. (398.7,272.6) .. controls (396.33,272.6) and (394.4,270.67) .. (394.4,268.3) -- cycle ;
		%Shape: Circle [id:dp12904906439386843] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (406.4,243.3) .. controls (406.4,240.93) and (408.33,239) .. (410.7,239) .. controls (413.07,239) and (415,240.93) .. (415,243.3) .. controls (415,245.67) and (413.07,247.6) .. (410.7,247.6) .. controls (408.33,247.6) and (406.4,245.67) .. (406.4,243.3) -- cycle ;
		%Shape: Circle [id:dp3044997975117185] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (418.4,223.3) .. controls (418.4,220.93) and (420.33,219) .. (422.7,219) .. controls (425.07,219) and (427,220.93) .. (427,223.3) .. controls (427,225.67) and (425.07,227.6) .. (422.7,227.6) .. controls (420.33,227.6) and (418.4,225.67) .. (418.4,223.3) -- cycle ;
		%Shape: Circle [id:dp6158609160729389] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (430.4,241.3) .. controls (430.4,238.93) and (432.33,237) .. (434.7,237) .. controls (437.07,237) and (439,238.93) .. (439,241.3) .. controls (439,243.67) and (437.07,245.6) .. (434.7,245.6) .. controls (432.33,245.6) and (430.4,243.67) .. (430.4,241.3) -- cycle ;
		%Shape: Circle [id:dp9758624791409862] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (442.4,261.3) .. controls (442.4,258.93) and (444.33,257) .. (446.7,257) .. controls (449.07,257) and (451,258.93) .. (451,261.3) .. controls (451,263.67) and (449.07,265.6) .. (446.7,265.6) .. controls (444.33,265.6) and (442.4,263.67) .. (442.4,261.3) -- cycle ;
		%Shape: Circle [id:dp27393362190771664] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (455.4,242.3) .. controls (455.4,239.93) and (457.33,238) .. (459.7,238) .. controls (462.07,238) and (464,239.93) .. (464,242.3) .. controls (464,244.67) and (462.07,246.6) .. (459.7,246.6) .. controls (457.33,246.6) and (455.4,244.67) .. (455.4,242.3) -- cycle ;
		%Shape: Circle [id:dp3288963315630187] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (466.4,225.3) .. controls (466.4,222.93) and (468.33,221) .. (470.7,221) .. controls (473.07,221) and (475,222.93) .. (475,225.3) .. controls (475,227.67) and (473.07,229.6) .. (470.7,229.6) .. controls (468.33,229.6) and (466.4,227.67) .. (466.4,225.3) -- cycle ;
		%Shape: Circle [id:dp46153743741847775] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (479.4,242.3) .. controls (479.4,239.93) and (481.33,238) .. (483.7,238) .. controls (486.07,238) and (488,239.93) .. (488,242.3) .. controls (488,244.67) and (486.07,246.6) .. (483.7,246.6) .. controls (481.33,246.6) and (479.4,244.67) .. (479.4,242.3) -- cycle ;
		%Shape: Circle [id:dp7996970048556706] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (491.4,259.3) .. controls (491.4,256.93) and (493.33,255) .. (495.7,255) .. controls (498.07,255) and (500,256.93) .. (500,259.3) .. controls (500,261.67) and (498.07,263.6) .. (495.7,263.6) .. controls (493.33,263.6) and (491.4,261.67) .. (491.4,259.3) -- cycle ;
		%Shape: Circle [id:dp6230974920785486] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (504.4,243.3) .. controls (504.4,240.93) and (506.33,239) .. (508.7,239) .. controls (511.07,239) and (513,240.93) .. (513,243.3) .. controls (513,245.67) and (511.07,247.6) .. (508.7,247.6) .. controls (506.33,247.6) and (504.4,245.67) .. (504.4,243.3) -- cycle ;
		%Shape: Circle [id:dp03327836348502644] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (515.4,228.3) .. controls (515.4,225.93) and (517.33,224) .. (519.7,224) .. controls (522.07,224) and (524,225.93) .. (524,228.3) .. controls (524,230.67) and (522.07,232.6) .. (519.7,232.6) .. controls (517.33,232.6) and (515.4,230.67) .. (515.4,228.3) -- cycle ;
		%Shape: Circle [id:dp9309237858949309] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (528.4,242.3) .. controls (528.4,239.93) and (530.33,238) .. (532.7,238) .. controls (535.07,238) and (537,239.93) .. (537,242.3) .. controls (537,244.67) and (535.07,246.6) .. (532.7,246.6) .. controls (530.33,246.6) and (528.4,244.67) .. (528.4,242.3) -- cycle ;
		%Shape: Circle [id:dp8965457340374714] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (541.4,256.3) .. controls (541.4,253.93) and (543.33,252) .. (545.7,252) .. controls (548.07,252) and (550,253.93) .. (550,256.3) .. controls (550,258.67) and (548.07,260.6) .. (545.7,260.6) .. controls (543.33,260.6) and (541.4,258.67) .. (541.4,256.3) -- cycle ;
		%Shape: Circle [id:dp9792359835688376] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (552.4,242.3) .. controls (552.4,239.93) and (554.33,238) .. (556.7,238) .. controls (559.07,238) and (561,239.93) .. (561,242.3) .. controls (561,244.67) and (559.07,246.6) .. (556.7,246.6) .. controls (554.33,246.6) and (552.4,244.67) .. (552.4,242.3) -- cycle ;
		%Straight Lines [id:da13924014629844472] 
		\draw    (166.9,304.78) -- (176.7,68.3) ;
		%Straight Lines [id:da808042872619074] 
		\draw    (201.7,345.3) -- (176.7,72.6) ;
		%Straight Lines [id:da40143976566490247] 
		\draw    (201.7,345.3) -- (226.7,172.3) ;
		%Straight Lines [id:da6654622672278678] 
		\draw    (250.7,298.3) -- (226.7,172.3) ;
		%Straight Lines [id:da30965904003975786] 
		\draw    (274.7,198.3) -- (250.7,298.3) ;
		%Straight Lines [id:da5939328708649343] 
		\draw    (274.7,198.3) -- (299.7,281.3) ;
		%Straight Lines [id:da32579811148345006] 
		\draw    (299.7,281.3) -- (323.7,210.3) ;
		%Straight Lines [id:da41173251649426956] 
		\draw    (347.7,271.3) -- (323.7,210.3) ;
		%Straight Lines [id:da6187419230002116] 
		\draw    (373.7,217.3) -- (347.7,271.3) ;
		%Straight Lines [id:da6996513014560126] 
		\draw    (373.7,217.3) -- (398.7,268.3) ;
		%Straight Lines [id:da04786139249486698] 
		\draw    (398.7,268.3) -- (422.7,223.3) ;
		%Straight Lines [id:da11823598781338607] 
		\draw    (446.7,261.3) -- (422.7,223.3) ;
		%Straight Lines [id:da7672329736596701] 
		\draw    (470.7,225.3) -- (446.7,261.3) ;
		%Straight Lines [id:da5148649483739762] 
		\draw    (470.7,225.3) -- (495.7,259.3) ;
		%Straight Lines [id:da6608386822954244] 
		\draw    (495.7,259.3) -- (519.7,228.3) ;
		%Straight Lines [id:da11601442142456309] 
		\draw    (545.7,256.3) -- (519.7,228.3) ;
		%Straight Lines [id:da660097788728921] 
		\draw    (556.7,242.3) -- (545.7,256.3) ;
		
		% Text Node
		\draw (125,62.4) node [anchor=north west][inner sep=0.75pt]    {$1.5$};
		% Text Node
		\draw (125,108.4) node [anchor=north west][inner sep=0.75pt]    {$1.4$};
		% Text Node
		\draw (125,152.4) node [anchor=north west][inner sep=0.75pt]    {$1.3$};
		% Text Node
		\draw (125,201.4) node [anchor=north west][inner sep=0.75pt]    {$1.2$};
		% Text Node
		\draw (125,249.4) node [anchor=north west][inner sep=0.75pt]    {$1.1$};
		% Text Node
		\draw (125,345.4) node [anchor=north west][inner sep=0.75pt]    {$0.9$};
		% Text Node
		\draw (125,295.4) node [anchor=north west][inner sep=0.75pt]    {$1.0$};
		% Text Node
		\draw (582,218.4) node [anchor=north west][inner sep=0.75pt]    {$\varepsilon $};
		% Text Node
		\draw (582.5,246.3) node [anchor=north west][inner sep=0.75pt]    {$\varepsilon $};
		% Text Node
		\draw (354,328.4) node [anchor=north west][inner sep=0.75pt]    {$N$};
		% Text Node
		\draw (168.9,312.48) node [anchor=north west][inner sep=0.75pt]    {$a_{1}$};
		% Text Node
		\draw (180.9,51.48) node [anchor=north west][inner sep=0.75pt]    {$a_{2}$};
		% Text Node
		\draw (195.9,209.4) node [anchor=north west][inner sep=0.75pt]    {$a_{3}$};
		% Text Node
		\draw (203.7,344.4) node [anchor=north west][inner sep=0.75pt]    {$a_{4}$};
		% Text Node
		\draw (216.7,252.7) node [anchor=north west][inner sep=0.75pt]    {$a_{5}$};
		% Text Node
		\draw (227.7,150.7) node [anchor=north west][inner sep=0.75pt]    {$a_{6}$};
		% Text Node
		\draw (239.7,217.7) node [anchor=north west][inner sep=0.75pt]    {$a_{7}$};
		% Text Node
		\draw (252.7,301.7) node [anchor=north west][inner sep=0.75pt]    {$a_{8}$};
		% Text Node
		\draw (264.7,247.7) node [anchor=north west][inner sep=0.75pt]    {$a_{9}$};
		% Text Node
		\draw (274.7,177.7) node [anchor=north west][inner sep=0.75pt]    {$a_{10}$};
		% Text Node
		\draw (288.7,222.7) node [anchor=north west][inner sep=0.75pt]    {$a_{11}$};
		% Text Node
		\draw (301.7,289) node [anchor=north west][inner sep=0.75pt]    {$a_{12}$};
		% Text Node
		\draw (314.7,246.7) node [anchor=north west][inner sep=0.75pt]    {$a_{13}$};
		% Text Node
		\draw (323.7,189.7) node [anchor=north west][inner sep=0.75pt]    {$a_{14}$};
		% Text Node
		\draw (336.7,220.7) node [anchor=north west][inner sep=0.75pt]    {$a_{15}$};
		% Text Node
		\draw (349.7,274.7) node [anchor=north west][inner sep=0.75pt]    {$a_{16}$};
		% Text Node
		\draw (362.7,246.7) node [anchor=north west][inner sep=0.75pt]    {$a_{17}$};
		% Text Node
		\draw (373.7,195.7) node [anchor=north west][inner sep=0.75pt]    {$a_{18}$};
		% Text Node
		\draw (385.7,221.7) node [anchor=north west][inner sep=0.75pt]    {$a_{19}$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[]{Illustration of the principle of convergence of a sequence}
	\end{figure}
	\end{tcolorbox}
		If there is no $a$ (respectively $N$) for which the previous relation is true, then we say that the sequence "\NewTerm{diverge}\index{divergent sequence}".
	\begin{theorem}
	Let us prove now that every increasing sequence $(a_n)_\mathbb{N}$ (respectively decreasing) and majorated (resp. minorated) converges.
	
	In other words, we seek to prove that any monotonous and bounded sequence $(a_n)_\mathbb{N}$ converges (obviously ... by construction).
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	If it does not converge, we could not easily find out what is its lower bound and upper bound ... hence the fact that the need of this theorem becomes trivial.
	\end{tcolorbox}
	\end{theorem}
	\begin{dem}
	This theorem is actually quite intuitive. Consider for this an increasing sequence. We suspect that:
	
	is the limit of this sequence. Note first of all that $a=\sup\left\lbrace a_0,a_1,a_2,\ldots \right\rbrace$ exists because $(a_n)_\mathbb{N}$ is majorated (see theorem proved previously).
	
	Given $\varepsilon>0$. It exists an $a_N$ such tat $a-\varepsilon\leq a_N \leq a$. But in this case as the sequence is increasing, we have $\forall\geq N,a-\varepsilon\leq a_n\leq a$. That is to say $|a_n-a|\leq \varepsilon$. In the case where the sequence is decreasing by proceeding in the same way we prove that $a=\inf{a_0,a_1,a_2,\ldots }$ is the limit of this sequence.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{theorem}
	And now the important theorem to remember after all this: Every bounded sequence of real numbers has a convergent subsequence  (that is intuitive ... but again ... when formalized it becomes sometimes less intuitive...).
	
	This is what mathematicians name the "\NewTerm{Bolzano-Weierstrass theorem}\index{Bolzano-Weierstrass theorem}" and it is extremely important in many areas of mathematics:
	\end{theorem}
	\begin{dem}
	Given $(a_n)_\mathbb{N}$ such a sequence. By a previous proposal we know there is a monotonic subsequence which we denote $(b_n)_\mathbb{N}$. $(b_n)_\mathbb{N}$ is therefore a monotone and bounded sequence and by the previous theorem, $(b_n)_\mathbb{N}$ converges.
	
	So if we are unable to determine whether the subsequence converges nor his exact limit (which in practice is often very difficult), we only need to know that the subsequence is monotone and bounded to ensure that it converges (which is most of time much easier).
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{theorem}
	Remember that we saw in the section of Sequences and Series that a Cauchy sequence is a sequence $(a_n)_\mathbb{N}$ that verifies (we restrict ourselves to the special case of Euclidean distance):
	
	The difference between the two terms of a Cauchy sequence can be made arbitrarily small provided that the indices of these terms are big enough.
	
	We have also proved (again in the section of Sequences and Series) that in the case of a distance in the general topological sense any convergent sequence is a Cauchy sequence (by cons the reciprocal is not always true at the condition that do not complete the set... otherwise the reciprocal is always true). For example, a sequence of rational numbers that converges to a real number is not a Cauchy sequence, except if we complete the set of rationals to get the set of real numbers.
	\end{theorem}
	Let us redo the proof restricted to Euclidean distance (the method is exactly the same as the reader will notice):
	\begin{dem}
	Given $\varepsilon$, we must show that there is:
	
	But $(a_n)_\mathbb{N}$ tends to $a$ therefore it exists a $N\in \mathbb{N}$ such that $n\geq N \Rightarrow |a_n-a|\leq \varepsilon/2$. For $n,m\geq N$ we therefore have:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{theorem}
	Let us now prove that every Cauchy sequence is bounded (we never talked about this until now anywhere in this book therefore we need to do the proof). Since currently we have just proved that every convergent sequence is a Cauchy sequence...
	\end{theorem}
	\begin{dem}
	If $(a_n)_\mathbb{N}$ is a Cauchy sequence then particularly for $\varepsilon=1$ (randomly) we know that there exist $N\in \mathbb{N}$ such that $n,m\geq N\Rightarrow |a_n-a_m|\leq 1$. So if we fix $m$, we get:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Let us see now the fundamental theorem (it is at this level that there is a huge impact on the understanding of what is actually a fractal!) that can be deduced from the previous  lines.
	\begin{theorem}
	We will show that every Cauchy sequence of real numbers is convergent (by construction ...). We say then that the metric space $\mathbb{R}$ provided with the Euclidean distance (absolute value) is a "\NewTerm{complete space}\index{complete space}\label{complete space cauchy sequence}".
	\end{theorem}
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} The completeness property is related to the metric (hence this theorem could equally have its place in the section of Topology!): The same space can be complete for a given distance and incomplete for another one. It is therefore important to always specify the distance that we take when we speak of complete space.\\
	
	\textbf{R2.} Intuitively, a space is complete if it has no holes. The set of rational numbers $\mathbb{Q}$ is by example complete if the real numbers are added to it.
	\end{tcolorbox}
	\begin{theorem}
	Consider first $(a_n)_\mathbb{N}$ a Cauchy sequence. We have seen just before that  $(a_n)_\mathbb{N}$ is bounded and then that by the Bolzano-Weierstrass theorem, there exists a convergent subsequence  $(a_{n_i})_{i\in\mathbb{N}}$. Let us denote by $a$ the limit of the subsequence $(a_{n_i})_{i\in\mathbb{N}}$. We will now prove that the sequence $(a_n)_\mathbb{N}$ is convergent of limit $a$.
	\end{theorem}
	
	\begin{dem}
	Given $\varepsilon>0$, there exist a $N\in \mathbb{N}$ such as (application of the definition of convergence for a subsequence):
	
	For this same $\varepsilon$ it exists $M\geq 0$ (application of the definition of convergence for a Cauchy sequence):
	
	Given $C=N+M$. We choose $i\geq C$. We then have $|a_{n_i}-a|\leq \varepsilon/2$ and for $n,i>C$:
	
	So by the triangle inequality (\SeeChapter{see section Vector Calculus page \pageref{triangle inequality}}), for any $n\geq C$:
	
	This means precisely that $(a_n)_\mathbb{N}$ converges to $a$.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Basically it is an intuitive result but at the time when real numbers were not known or not rigorously defined it was a different story! In fact, it is simply enough complete any set by the real numbers to get a complete space. Moreover, some mathematicians define the set of real number saying that it is the set for which every Cauchy sequence converges...
	
	\textbf{Definition (naive \#\mydef):} An "\NewTerm{accumulation point}\index{accumulation point}" or "\NewTerm{cluster point}\index{cluster point}" is a point which we can approach as much as we want thanks to elements of a given set $X$ (we will approach it for example with a sequence). However, this accumulation point can be both inside and outside of $X$ (all items within $X$ are obviously limit points). A good image is to see a series that approach this accumulation point and define circles around it that are becoming smaller and smaller containing elements of the sequence.
	
	We can imagine as an example a sequence defined by the set $X$ of rational numbers $\mathbb{Q}$ which tends to an irrational or to a transcendental number (these two points being elements not belonging to the set of rational $\mathbb{Q}$). So in this case, the accumulation point is outside $X$ (the set of rational $\mathbb{Q}$). By cons, any accumulation point that would be a rational number for a sequence of rational number will necessarily ... in $X$ (that is to say $\mathbb{Q}$).
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	With respect to the usual Euclidean topology, the sequence of rational numbers:
	
	has no limit (i.e. does not converge) when $n\rightarrow \pm \infty$, but has two accumulation points (which are considered limit points here), that are $-1$ and $+1$:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.43]{img/computing/accumulation_point.jpg}
	\end{figure}
	\end{tcolorbox}
	Therefore, comes the following definition (for more details see the section Topology page \pageref{topology}):
	
	\textbf{Definition (formal \#\mydef):} Given $X \subseteq \mathbb{R}^n$. We say that $x\in \mathbb{R}^n$ is an "\NewTerm{accumulation point}\index{accumulation point}" to $X$ if for all ball $\mathcal{B}(x,r)$ of radius $r$ center on $x$ we have (for more details see the section Topology):
	
	The set of all accumulation points (of a sequence) to $X$ is the "\NewTerm{limit set}\index{limit set}" of $X$ and denoted by $\bar{X}$. We have obviously (it suffice to conceptualize it in an abstract way for all possible ball) $X \subseteq \bar{X}$.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us consider the interval $]0,1]$ with the ball $B(0,1)$. The intersection between the ball and the interval is not zero, we can say that $0$ is an accumulation point! But now let us take a sequence $1 / n$ for example in the interval $]0,1]$. This sequence tends to zero but $0$ is not in interval. It is a good example of $X \subseteq \bar{X}$.
	\end{tcolorbox}
	We can therefore make the proposition:
	\begin{theorem}
	Let us prove now that $x\in \mathbb{R}^n$ is adherent to $X$ if and only if there is a sequence $\left(u_n\right)_{\mathbb{N}}$ in $X$ converging to $x$ (note that the previous example shows that $x$ is not necessarily in $X$).
	
	In fact, we will instead prove (if we can say it is a proof..) that if we choose an accumulation point $x$ then we can always find a sequence in$ $X converging to $x$.
	\end{theorem}
	\begin{dem}
	If $x\in \mathbb{R}^n$ is adherent to $X$ then let us consider the following concentric balls $B\left(x,\dfrac{1}{n}\right)$ with $n\geq 1$ such as:
	
	and then there are always elements $u_n$ that satisfy:
	
	with whom we can create a sequence by the infinity of existing sequences.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}	
	\textbf{Definition (\#\thesection.\mydef):} We say that $X\subseteq \mathbb{R}^n$ is a "closed space" if $X=\bar{X}$.
	
	From the previous proposals it follows that in any closed space $F$, a sequence $(x_n)_{\mathbb{N}}$ that converges has its limit in $F$.
	
	We consider as if trivial that if $(F_i)_I$ is a family of closed indexed spaces on any set $I$, then $\bigcap_I F_i$ is closed.
	
	\textbf{Definition (\#\thesection.\mydef):} $X\subseteq \mathbb{R}^n$ is a "\NewTerm{compact space}\index{compact space}" if $X$ is closed and bounded.
	
	The following theorem gives a characterization of the compact from the sequences:
	
	\begin{theorem}
	$X\subseteq \mathbb{R}^n$ is compact if and only if any sequence $(x_n)_\mathbb{N}$ of $X$ possess a sub-sequence that converges in $X$.
	\end{theorem}
	\begin{dem}
	First let us prove that $X$ is closed:
	
	If $X$ is compact and $(x_n)_\mathbb{N}$ is a sequence of $X$ then by the Bolzano-Weierstrass theorem, $(x_n)_\mathbb{N}$  has a convergent subsequence of limit $x\in \mathbb{R}^n$. But since $X$ is closed, we have $x\in X$. Conversely, let us assume that any sequence $(x_n)_\mathbb{N}$  of $X$ has a subsequence which converges in $X$. Then $X$ is closed because if $x\in \bar{X}$ there is a sequence $(x_n)_\mathbb{N}$ of $X$ which tends to $x$. By assumption, $(x_n)_\mathbb{N}$ has a subsequence that converges $y\in X$. The sequence $(x_n)_\mathbb{N}$ being convergent all the sub-sequences converge to the same value, therefore $x=y\in X$ (is this not great?!!). Thus $X=\bar{X}$ that is to say $X$ is closed!
	
	Let us now prove that $X$ is bounded:
	
	Let us suppose the opposite. So there exists a sequence $(x_n)_\mathbb{N}$ of $X$ such that $||x_n||\geq n$. But in this case, no subsequence of $(x_n)_\mathbb{N}$ is convergent, which is a contradiction! So $X$ is bounded. Finally, $X$ is compact!!!
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	A property of compacts is that if we consider $(A_n)_\mathbb{N}$ a decreasing sequence of non-empty compacts, that is to say $A_{n+1}\subseteq A_n$, then $\bigcap_{\in \mathbb{N}} A_n$ is a non-empty compact. We will pass trough the proof that is relatively trivial by the definition of the concept of Adherence Set that oblige that compacts are by construction non-empty...!
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	We build the Cantor set\label{Cantor set} $C$ as follows:\\
	
	We begin by considering the closed bounded interval $C_0=[0,1]$ of $\mathbb{R}$ which is therefore a compact space (bounded and closed set ). We split $C_0$ into three equal parts and we remove the middle interval. This gives us all the set:
	
	which can be also considered also as the application of a contracting scaling factor of $1/3$ on the closed bounded interval of departure of which we translate the center of homothety.\\
	
	We start again with the two intervals $[0,1/3],[2/3,1]$ for:
	
	disjoint union of $4$ intervals. And so on... So we get a decreasing sequence $C_n$ of compact. We define:
	
	Thanks to the previous proposal, we know that $C$ is not empty and is compact which shows that the compacts are not all "trivial" as intervals. The Cantor set (because he had played by doing the drawing below starting from the bottom) is an example of  (compact) fractal:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/cantor_set_maple.jpg}
		\caption{Cantor set with Maple 4.00b}
	\end{figure}
	that is possible to get with the following Maple 4.00b code:\\
	
	\texttt{
	>with(plots):\\
	line := proc(a:: list, b:: list)\\
	local plotoptionen, n;\\
	if nargs > 2 then\\
	plotoptionen := seq(args[n], n=3 .. nargs)\\
	else \\
	plotoptionen := NULL\\
	fi;\\
	plot([a, b], style=line, plotoptionen);\\
	end:\\\\
	cree\_segment := (a,b,h) -> line([a,h],[b,h],color=black): \\
	f1:=x->x/3: f2:=x->(x+2)/3:\\\\
	f := s -> s union map(f1, s) union map(f2, s):\\\\
	sequence\_de\_segments := proc(l,h) \\
	local accu, i;\\
	accu := NULL;\\
	for i to nops(l) by 2 do\\
	accu := accu,cree\_segment(l[i], l[i+1], h) od;\\
	accu\\
	end:
	}
	
	\texttt{
	>Cantor:= proc(n) local s, i;\\
	>option remember;\\
	>s:=sequence\_de\_segments([0,1], 1);\\
	>for i from 1 to n do\\
	>s:=sequence\_de\_segments(sort([op((f@@i)({0,1}))]), (1-i/n)), 
	>s;\\
	>od;\\
	>display({s}union{seq(textplot([[0,(i+1/2)/n, '0'], [1, (i+1/2)/n, '1']] \\
	), i=0 .. n)}, color=blue,axes=NONE,thickness=7)\\
	>end:\\\\
	>Cantor(7);
	}\\
	
	It is very interesting to notice that we converges to the Cantor fractal (in terms of geometry but also in term of values!) whatever the chosen starting compact we start from (the closed bounded interval) and also ... whatever the chosen contractor factor!\\
	
	Benoît Mandelbrot also observed this type of self similar structure in the analysis of electrical signals transmitted at his (junior) time when working for IBM on copper cables (IBM had transmission information loss problems).
	\end{tcolorbox}
	Now let us look for finish how behave compactis vis-à-vis continuous applications (we need this to prove how to determine the distance from a point to a set which we will need absolutely after to determine the properties of the Hausdorff distance).
	
	Let us recall (\SeeChapter{see section Topology page \pageref{continuity and uniform continuity}}) that an application $f:X \mapsto \mathbb{R}^m$ whatever $X \subseteq \mathbb{R}^n$ is delta-epsilon continuous on a point $x\in X$ if:
	
	This reflects the fact that for $y$ close enough to $x$, $f (y)$ is arbitrarily close to $f (x)$. We also say that $f$ is continuous on $X$ if it is continuous at each point of $X$.
	\begin{theorem}
	Given $f:X\mapsto \mathbb{R}^m$ a delta-epsilon continuous application on $x\in X$ and $(x_n)_\mathbb{N}$ a sequence of $X$ with:
	
	Then the sequence $f(x_n)$ converges and (this proposal is very important!):
	
	In other words, if we use as a set of starting values of a convergent sequence, then the function that take as input the values of this sequence  will converge too!
	\end{theorem}
	\begin{dem}
	Given $\varepsilon >0$. $f$ is delta-epsilon continuous on $x$, so there exists $\delta>0$ such that:
	
	The sequence $(x_n)_\mathbb{N}$ tends to $x$ therefore it exists $N\in \mathbb{N}$ such that:
	
	If follows that $n\geq N$, we have:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	If we now consider a compact $X\subseteq \mathbb{R}^n$ and $f:X\mapsto \mathbb{R}^n$ a continuous application. The application $f(X)$ is compact. In particular $\sup (f)$ and $\inf (f)$ will be reached by definition and  by construction of a compact (closed and bounded set) which is equal to its adherence.
	\begin{theorem}
		In other words, a continuous real-valued function on a compact always reaches its supremum or infimum.
	\end{theorem}
	\begin{dem}
	We will do the proof in two steps:
	\begin{itemize}
		\item Let us prove that $f(X)$ is closed.

		Indeed, given $f(x_n)$ a sequence that tends to $y\in \overline{f(X)}$ (we take theadherence to hope to prove that it is equal to the set itself) then $X$ being compact, then $(x_n)_\mathbb{N}$ has a convergent subsequence $(x_{n_i})_{i\in \mathbb{N}}$.
		Let us put:
		
		The function $f$ is delta-epsilon continuous, therefore:
		
		But as:
		
		we have:
		
		This proves that:
		
		and therefore that $f(X)$ is closed.
	
	\item Let us now prove that $f (X)$ is bounded.

		For this let us suppose the contrary. There is therefore a sequence $f(x_n)$ such that:
		
		for every natural integer $n$ (precisely because it is assumed unbounded). Given $(x_{n_i})_{i\in \mathbb{N}}$ a convergent subsequence of $(x_n)_\mathbb{N}$ with:
		
		Then:
		
		and it follows:
		
		but this is in contradiction with:
		
		Therefore $f(X)$ is bounded. Then $f(X)$ being closed is bounded and therefore is compact.
	\end{itemize}
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
		Now let apply this result (because this is what interests us in fractal spaces) to calculate the distance from a point to a set:
	\begin{theorem}
	Given $x\in \mathbb{R}^n$, the application $f:\mathbb{R}^n \mapsto \mathbb{R}$ defined by $f (y) = d (x, y)$ is continuous.
	\end{theorem}
	\begin{dem}
	For $(y,z)\in\mathbb{R}^2$, the triangle inequality gives us:
	
	By changing the roles of $y, z$ we get:
	
	and therefore:
	
	Therefore for a given $\varepsilon >0$, $d(z,y)\leq \varepsilon$ implies:
	
	That is to say:
	
	and $f$ is therefore continuous on $y$.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\textbf{Definition (\#\thesection.\mydef):} For $x\in \mathbb{R}^n$ and $A\subseteq \mathbb{R}^n$ we define the distance $x$ to $A$ as being the value:
	
	\begin{theorem}
	If $x\in A$ then $d(x,A)=0$ (should me almost trivial). The reciprocal is not true. Indeed in the case $x=0$ and $A=]0,1]$ we have indeed $d(x,A)=0$ but $x \in A$. So we have the following important proposal:
	
	\end{theorem}
	\begin{dem}
	The fact that $d(x,A)=0$ implies the existence of a sequence $(a_n)_{\mathbb{N}}$ of elements of $A$ such that:
	
	which means
	
	therefore $x\in \bar{A}$ (see previous developments).

	Conversely, if $x\in \bar{A}$ then for every $\varepsilon>0$ there exists $a\in A$ such as $d(x,a)\leq \varepsilon$. But $d(x,A)\leq d(x,a)\leq \varepsilon$. Thus for any $\varepsilon>0$, $d(x,A)\leq \varepsilon$. That is to say:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	In general the distance from $x$ to $A$ is not reached. That is to say that there is no $a\in A$ such that $d(x,A)=d(x,a)$. To check this, it is sufficient to consider the example $x=-1$ and $A=]0,1]$. We have in this example $d(x,A)=1$ but for any $a\in A$, $d(x,a)>1$. If $A$ is compact, the situation is obviously different according to the following proposal (the most important for the Hausdorff distance in our point of view):	
	\begin{theorem}
		If $A\subseteq \mathbb{R}^n$ is compact, there exist $a\in A$ such that $d(x,A)=d(x,a)$. Therefore:
		
	\end{theorem}
	\begin{dem}
	The application $f:A\mapsto \mathbb{R}$ defined by $f(a)=d(x,a)$ is continuous as previously proved. Therefore $f(A)$ is compact (see a previous proposal). Thus, $f$ reaches its bounds, that is to say, there is $a\int A$ such that $f (a) =\inf(f (A))$. Therefore:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	This previous proposition does not say that $a$ is unique, in general in fact it is not!
	\end{tcolorbox}
	
	\subsubsection{Fractals Metric Space}\label{fractal metric space}
	Fractals are often perceived by people as pretty drawings on paper, but when we look in detail the geometry of fractal, we need a particular space to study them, much like the biologist who puts small worms on a wafer to observe the worms in detail to the microscope. We will do the same for our fractals by placing them in a place they like...
	
	This place is likely to be a subspace of $\mathbb{R}^2$ or  $\mathbb{R}^3$, since in the end it will produce drawings... And to illustrate this we often will place ourselves in  $\mathbb{R}^2$ (with the Euclidean metric), and unless otherwise stated, we always consider the case where $(X,d)$ is a complete metric space.
	
	Let us collect different items in order to construct this space:
	
	\textbf{Definition (\#\thesection.\mydef):} We define $\mathcal{H}(X)$ as the space whose points are the compacts subsets of $X$ other than $X$ itself. From now, we will name "\NewTerm{fractal}\index{fractal}" any element of $\mathcal{H}(X)$.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	It is immediate that if $x,y\in\mathcal{H}$, then $x\cup y\in \mathcal{H}$ but $x\cap y$ is not necessarily in $\mathcal{H}$. It is sufficient to see the figure below with the two compact sets of $\mathbb{R}^2$ (closed and bounded therefore) below. There are therefore two points of $\mathcal{H}$.. Their union is still a compact, and therefore:
	

	By cons, if the sets are disjoint (as here), $x\cap y=\varnothing$ and therefore $x$, $y$ are not a point of $\mathcal{H}(\mathbb{R}^2)$ (see previous theory).
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,428); %set diagram left start at 0, and has height of 428
		
		%Shape: Rectangle [id:dp7771638342140235] 
		\draw   (100,105) -- (355.3,105) -- (355.3,223) -- (100,223) -- cycle ;
		%Shape: Circle [id:dp41001013186273316] 
		\draw   (377,159) .. controls (377,103.22) and (422.22,58) .. (478,58) .. controls (533.78,58) and (579,103.22) .. (579,159) .. controls (579,214.78) and (533.78,260) .. (478,260) .. controls (422.22,260) and (377,214.78) .. (377,159) -- cycle ;
		
		% Text Node
		\draw (112,114.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (403,117.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[]{Source: IFS and L-System V. Rezzonico, C. Hebeisen}
	\end{figure}
	\end{tcolorbox}
	Another example involves taking the Cantor fractal...
	
	\textbf{Definition (\#\thesection.\mydef):} Given $x\in X$ and $B\in\mathcal{H}(X)$, we define the distance $d(x,B)$ of a point $x$ to a set $B$  by:
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} This definition is quite general and applies to any non-empty subset of $X$, by replacing $\min$ by $\inf$. But in our specific case, we are interested in taking precisely $\mathcal{H}(X)$ as a subspace.\\
	
	\textbf{R2.} This distance is well defined (is exists) as $B$ is non-empty and compact.\\
	
	\textbf{R3.} It is trivial to see that if this distance is zero, then $x\in\bar{B}$.
	\end{tcolorbox}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Illustration in the case where $X=\mathbb{R}^2$:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,428); %set diagram left start at 0, and has height of 428
		
		%Shape: Polygon Curved [id:ds9856946573290128] 
		\draw   (217.3,49) .. controls (332.6,10) and (536.3,178) .. (487.3,239) .. controls (438.3,300) and (263.3,104) .. (201.3,196) .. controls (139.3,288) and (102,88) .. (217.3,49) -- cycle ;
		%Straight Lines [id:da5769775866981257] 
		\draw    (339.3,264) -- (383.3,223) ;
		\draw [shift={(383.3,223)}, rotate = 317.02] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		
		% Text Node
		\draw (178,100.4) node [anchor=north west][inner sep=0.75pt]    {$B$};
		% Text Node
		\draw (323,259.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (314,216.4) node [anchor=north west][inner sep=0.75pt]    {$d( x,B)$};
		
		\end{tikzpicture}
		\caption[]{Source: IFS and L-System V. Rezzonico, C. Hebeisen}
	\end{figure}
	\end{tcolorbox}
	\textbf{Definition (\#\thesection.\mydef):} Given $A,B\in\mathcal{H}(X)$. We define and denote the distance from $A$ to $B$ by:
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} This definition is quite general and applies to any non-empty subset of $X$, by replacing $\min$ by $\inf$. But in our specific case, we are interested in taking precisely $\mathcal{H}(X)$ as a subspace.\\
	
	\textbf{R2.} We notice that this distance does not provide a $\mathcal{H}(X)$ metric: indeed, $d(A,B)\neq d(B,A)$ in general (take for example the Cantor fractal where some compact we have $A\subset B$ with $A\neq B$, then we have $d(A, B) = 0$ but $d(B,A)>0$).
	\end{tcolorbox}
	\textbf{Definition (\#\thesection.\mydef):} Given $A,B\in\mathcal{H}(X)$. We define and denote "\NewTerm{Hausdorff distance}\index{Hausdorff distance}" between two sets $A,B\in \mathcal{H}(X)$ by:
	
	This time, by this definition, we have well a metric on $\mathcal{H}(X)$.

	Indeed, let us check that the five properties of a distance are satisifed (\SeeChapter{see section Topology page \pageref{distance}}):
	
	Given $A,B,C\in \mathcal{H}(X)$. Clearly we have without proof\footnote{But let us know as always if you want we put the proofs} (symmetry, nullity and separation on the diagonal):
	
	Moreover, since $A$ and $B$ are compact, $h(A,B)=d(A,B)$ (see on of the previous proposals) for a given $a\in A$ and a given $b\in B$. But, since $d(a,b)>0$ by definition, we have (property of positivity) finally $h(A,B)>0$ such that $a\in A$,$A\notin B$:
	
	as $B$ is closed.
	
	Finally, since $h(A,B)=d(a,b)$ (see the extension of one of previous proposals), the triangle inequality is necessarily respected and then:
	
	So $h$ is indeed a metric of $\mathcal{X}$, which makes $(\mathcal{H}(X),h)$ a metric space. This is a first step in the desired direction, we now have the tools to compare two sets belonging to $\mathcal{X}(X)$ by the Hausdorff distance between them. If the two are not "too different", so intuitively that distance should be fairly small.
	
	If we choose a strictly contracting function $f:X\mapsto X$ of constant $\lambda$ (\SeeChapter{see section Topology page \pageref{strictly contracting}}). Then, the application:
	
	defined by:
	
	is by onstruction also strictly contracting of constant $\lambda$.
	
	Given $f_i:\mathbb{R}^2\mapsto \mathbb{R}^n$, $i\leq i\leq k$ strictly contracting applications of contraction constant  $0\leq \lambda_i<1$. Then, there exists a unique compact $A\in\mathcal{H}(\mathbb{R}^n)$ such that:
	
	($A$ is the unique fixe point of $T_{\lambda_1,\ldots,\lambda_k}$) and for any compact $B$, we have:
	
	where $T_{\lambda_1,\ldots,\lambda_k}^m(B)$ is the $m$-th iteration of $B$ by $T_{\lambda_1,\ldots,\lambda_k}$.
	
	This result derives from the fixed-point theorem (\SeeChapter{see section Sequences and Series page \pageref{banach fixed point theorem}}) applied to the space $\mathcal{H}(\mathbb{R}^n)$ that is complete.

	With the same notation, we say that $f_1,\ldots,f_k$ is an IFS (Iterated Function Systems) coding of the compact $A$. Thus the functions $f_1,\ldots,f_k$ define the compact $A$. What is surprising, as we will see in the following examples, is that the $f_1,\ldots,f_k$ are usually quite simple (as homotheties of the plan) while the compact $A$ is in many cases relatively or very "complicated" visually speaking.

	If case $k=1$ is without interest, we would have $A=(x)$ where $x$ is the fixed point of $f_1$. With $k=2$ we already obtain nontrivial results.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	When the iterative contracting functions are all homotheties we speak then of "\NewTerm{Sierpinski's fractal}\index{Sierpinski's fractal}". Thus, the Cantor fractal  belongs to the family of Sierpinski's fractals.
	\end{tcolorbox}
	A frequently used method for to generate IFS fractals (as it will be the case below with Maple 4.00b\footnote{But if some readers have reproduced all the example below in C++ they are welcome to share their code}) is to consider a point in the plane $(x_n,y_n)$ on which we can without conditions or constraints apply an affine transformation to get a new point such that:
	
	where $a$, $b$, $c$, $d$, $e$ and $f$ are any constants, and $(x_0,y_0)$ is given (chosen).
	
	We can therefore consider an application $T$ that describes our transformation, and in matrix form we can write the previous system as follows:
	
	or even:
	
	So in general, the vector $\vec{b}$ above simply describes a translation, and the matrix $A$ is the composition of a rotations and a scaling (\SeeChapter{see section Euclidean Geometry page \pageref{geometric transformations}}). Computer programs (as it will be the case in the examples below), thus often require that the knowledge of the six parameters $a$, $b$, $c$, $d$, $e$ and $f$ that can for majority be equal to zero.
	
	\pagebreak
	\subsection{Fractals Visualization}
	
	\subsubsection{Cantor's Fractal (Cantor Set)}
	Let us come back now on Cantor's Fractal but see now from the point of view of the application of two iterative contracting functions $f_1,f_2$ (thus corresponding to $k = 2$).

	So we start from the following closed bounded set (then a compact as it is bounded):
	
	Therefore: 
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/cantor_set_01.jpg}
		\caption[]{Start set of Cantor's Fractal}
	\end{figure}
	So now by definition we split in three equal parts and we remove the middle interval. This gives us the set:
	
	Therefore:
	\begin{figure}[H]
		\centering
		\includegraphics[]{img/computing/cantor_set_03.jpg}
		\caption{First iteration of Cantor's Fractal}
	\end{figure}
	We can see that $[0,1/3]$ can be obtained by the following homothety (scaling) application of factor $1/3$ centered at $(0,0)$:
	
	and that $[2/3,1]$ can be obtained by the following homothety (scaling) of factor $1/3$ centered at $(1,0)$:
	
	and so on, and we get as we already know (see Maple 4.00b code already given above):
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/cantor_set_02.jpg}
		\caption{Cantor attractor after $6$ iterations}
	\end{figure}
	Which corresponds using the formalism seen earlier to:
	
	This process is continued ad infinitum, where the $n$-th set is
	
	with obviously $C_{0}=[0,1]$.
	
	But let us see that it works with any compact of $\mathbb{R}$ as a square for example with the following Maple 4.00b code  (we always show all the details of Maple 4.00b, because nothing says that readers have the software or that the software will still exist in 50 years for reproductibility purposes...).
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	It seems that there may be sometimes issues by copying and pasting the below code into Maple 4.00b. If it's the case one your computer, just rewrite it from scratch directly in the Maple console.
	\end{tcolorbox}
	
	\texttt{>transforme\textunderscore point := proc(t, p)}\\
	\texttt{   [t[1]*p[1]+t[2]*p[2]+t[5], t[3]*p[1]+t[4]*p[2]+t[6]]}\\
  	\texttt{end:}\\

  	\texttt{>IFSS := proc(n, liste\textunderscore de\textunderscore transformations,col)}\\
  	\texttt{local i, j, k, s, seq\textunderscore square:}\\
     \texttt{seq\textunderscore square :=[[0,0],[1,0],[1,1],[0,1]];} \\
     \texttt{for j to n do}\\
     \texttt{   s := NULL;}\\   
     \texttt{   for i to nops(liste\textunderscore de\textunderscore transformations) do}\\
        \texttt{       {} {} {} s := s,}\\
        \texttt{      {} {} {} seq(transform\textunderscore square(liste\textunderscore de\textunderscore transformations[i],}\\
        \texttt{      {} {} {} op(k, [seq\textunderscore square])),}\\
        \texttt{      {} {} {} k=1 .. nops([seq\textunderscore square]))}\\
      \texttt{   od;}\\
      \texttt{   seq\textunderscore square := s}\\
    \texttt{od;}\\
    \texttt{plots[polygonplot]([seq\textunderscore square], axes=none, color=col, scaling=constrained)}\\
    \texttt{end:}\\
    
	\texttt{>cantor:=[[evalf(1/3),0,0,evalf(1/3),0,0],[evalf(1/3),0,0,evalf(1/3), evalf(2/3),0]]:}\\
	
	\texttt{>IFSS(1, cantor,blue);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/cantor_set_square_01.jpg}
		\caption[]{First iteration on the Cantor set with squares}
	\end{figure}
	\texttt{>IFSS(2, cantor,blue);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/cantor_set_square_02.jpg}
		\caption[]{Second iteration on the Cantor set with squares}
	\end{figure}
	\texttt{>IFSS(3, cantor,blue);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/cantor_set_square_03.jpg}
		\caption[]{Third iteration on the Cantor set with squares}
	\end{figure}
	etc.
	
	So, whatever the starting set, the sequence of compact obtained by successive application of these two plane homotheties always converge (in the sense of the Hausdorff distance) to the same compact/attractor (assimilated to the fixed of the Fixed point theorem...) named Cantor's fractal or Cantor set (thus belonging to the family of Serpienski fractals). The latest figure above is a good approximation of this set.
	
	The Cantor set being self-similar, consisting
of $N=2$ congruent subsets, each when magnified by a factor of $M = 3$ yields the original set. Hence the fractal dimension (\SeeChapter{see section Euclidean Geometry page \pageref{dimensions}}) of the Cantor set is:
	
	
	\subsubsection{Triangle Sierpinski Fractal}\label{sierpinski fractal}
	To build the Sierpinski fractal (which can be found as curiosity sometimes on the seashell Cymbiola innexa REEVE), based on three iterative contracting functions $f_1,f_2,f_3$ (thus corresponding to $k = 3$), we assume for example three following points of $\mathbb{R}^2$:
	
	Which gives with Maple 4.00b:
	
	The Sierkpinski Triangle consists of $3^n$ subsets with magnification factor $2^n$. So the fractal dimension is:
	
	
	\texttt{>plots[polygonplot]( [[0, 0], [1, 0], [0.5, 1]],axes=none,color=black, scaling=constrained);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/sierpinski_set_01.jpg}
		\caption[]{Start set of triangle Sierpinski fractal}
	\end{figure}
	This is a triangle, but we could start from any shape and we always arrive at the same result as we will see later.
	
	We apply on each set a contracting function of factor $0.5$, this gives the triangle:
	
	and we denote that this homothety (scaling) of factor $0.5$ and center $(0,0)$ on the original triangle by:
	
	We do now on this triangle a translation of $0.5$ in the direction of the $x$-axis, which gives the triangle:
	
	which corresponds to a scaling factor of $0.5$ and center $(1.0)$ on the original triangle:
	
	We now translate $[[0,0], [0.5,0], [0.25,0.5]]$ of $0.25$ along the $x$-axis and of $0.5$ according to the $y$-axis to have:
	
	which corresponds to a scaling factor of $0.5$ of center $(0.5,0.75)$ on the original triangle:
	
	With Maple 4.00b it now gives for the three triangles:
	
	\texttt{>plots[polygonplot]([[[0,0],[0.5,0],[0.25,0.5]],[[0.5,0],[1,0],[.75,0.5]], [[0.25,0.5],[0.75,0.5],[0.5,1]]], axes=none,color=black, scaling=constrained);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/sierpinski_set_02.jpg}
		\caption[]{First iteration on the Sierpinksi triangle}
	\end{figure}
	and so on...:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/sierpinski_set_03.jpg}
		\caption[]{Second iteration on the Sierpinksi triangle}
	\end{figure}
	and so on...:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/sierpinski_set_04.jpg}
		\caption[]{Third iteration on the Sierpinksi triangle}
	\end{figure}
	and so on...:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/sierpinski_set_05.jpg}
		\caption[]{Fourth iteration on the Sierpinksi triangle}
	\end{figure}
	and so on (the triangles begins to be quite small to see a difference without zoom)...:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/sierpinski_set_06.jpg}
		\caption[]{Sixth iteration on the Sierpinksi triangle}
	\end{figure}
	Which corresponds by taking the formalism seen previously:
	
	We can make the same remark as when we have study the Cantor's Fractal: whatever the starting set, the sequence of compact obtained by successive application of these three plane homotheties always converge (in the sense of the Hausdorff distance) to the same compact/attractor (assimilated to the fixed of the Fixed point theorem...) named Sierpinski fractal. The latest figure above is a good approximation of this set.
	
	Let's see this with a Maple 4.00b code (once again if the copy/paste from the book in Maple 4.00b does not work, simply rewrite the code in the Maple console):
	
	\texttt{>transforme\textunderscore triangle := proc(t, triangle)}\\	
    \texttt{   local i;}\\
    \texttt{   [seq(transforme\textunderscore point(t, triangle[i]), i=1 .. 3)]}    
	\texttt{end:}\\

	\texttt{>IFS := proc(n, liste\textunderscore de\textunderscore transformations,col)}\\
     \texttt{local i, j, k, s, sequence\textunderscore de\textunderscore triangles:}\\
     \texttt{options `Copyright by Alain Schauber, 1996`;}\\
     \texttt{sequence\textunderscore de\textunderscore triangles := [[0, 0], [1, 0], [0.5, 1]];}\\
     \texttt{for j to n do}\\
     \texttt{   s := NULL;}\\   
     \texttt{   for i to nops(liste\textunderscore de\textunderscore transformations) do}\\
        \texttt{   {} {} {} s:= s,}\\
        \texttt{   {} {} {} seq(transforme\textunderscore triangle(liste\textunderscore de\textunderscore transformations[i],}\\
        \texttt{   {} {} {} op(k, [sequence\textunderscore de\textunderscore triangles])),}\\
        \texttt{   {} {} {} k=1 .. nops([sequence\textunderscore de\textunderscore triangles]))}\\
       \texttt{   {} {} {} od;}\\       
      \texttt{   {} {} {} sequence\textunderscore de\textunderscore triangles := s}\\
    \texttt{od;}\\
    \texttt{plots[polygonplot]([sequence\textunderscore de\textunderscore triangles], axes=none, color=col, scaling=constrained)}\\
  	\texttt{end:}
  	
	\texttt{>triangle\textunderscore de\textunderscore  Sierpinski:=[[0.5,0,0,0.5,0,0],[0.5,0,0,0.5,0.5,0], [0.5,0,0,0.5,0.25,0.5]]:}

	\texttt{>IFS(6, triangle\textunderscore de\textunderscore Sierpinski,blue);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/sierpinski_maple.jpg}
		\caption{Sierpinski triangle attractor}
	\end{figure}
	And this time we don't start from a triangle but from a square (IFS Square) with the following Maple 4.00b code:
	
	\texttt{>transforme\textunderscore square := proc(t, square)}\\	
    \texttt{   local i;}\\
    \texttt{   [seq(transforme\textunderscore point(t, square[i]), i=1 .. 4)]}    
	\texttt{end:}\\

	\texttt{>IFS := proc(n, liste\textunderscore de\textunderscore transformations,col)}\\
     \texttt{local i, j, k, s, seq\textunderscore  square:}\\
     \texttt{seq\textunderscore square := [[0, 0], [1, 0], [1, 1],[0, 1]];}\\
     \texttt{for j to n do}\\
     \texttt{   s := NULL;}\\   
     \texttt{   for i to nops(liste\textunderscore de\textunderscore transformations) do}\\
        \texttt{   {} {} {} s:= s,}\\
        \texttt{   {} {} {} seq(transform\textunderscore square(liste\textunderscore de\textunderscore transformations[i],}\\
        \texttt{   {} {} {} op(k, [seq\textunderscore square])),}\\
        \texttt{   {} {} {} k=1 .. nops([seq\textunderscore square]))}\\
       \texttt{   {} {} {} od;}\\       
      \texttt{   {} {} {} seq\textunderscore square := s}\\
    \texttt{od;}\\
    \texttt{plots[polygonplot]([seq\textunderscore square], axes=none, color=col, scaling=constrained)}\\
  	\texttt{end:}
  	  	
	\texttt{>square\textunderscore  Sierpinski:=[[0.5,0,0,0.5,0,0],[0.5,0,0,0.5,0.5,0], [0.5,0,0,0.5,0.25,0.5]]:}

	\texttt{>IFS(1, square\textunderscore  Sierpinski,green);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/sierpinski_square_set_01.jpg}
		\caption[]{First iteration on the Sierpinksi square}
	\end{figure}
	\texttt{>IFS(2, square\textunderscore  Sierpinski,green);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/sierpinski_square_set_02.jpg}
		\caption[]{Second iteration on the Sierpinksi square}
	\end{figure}
	\texttt{>IFS(3, square\textunderscore  Sierpinski,green);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/sierpinski_square_set_03.jpg}
		\caption[]{Third iteration on the Sierpinksi square}
	\end{figure}
	and so on until...:
	
	\texttt{>IFS(6, square\textunderscore  Sierpinski,green);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/sierpinski_square_set_04.jpg}
		\caption[]{Sixth iteration on the Sierpinksi square}
	\end{figure}
	Basically, the Sierpinski fractal can obviously be also seen as a triangle to which the middle of the triangle is removed and where for each of the remaining triangles, we restart the process!
	
	\subsubsection{Sierpinski carpet fractal}
	The Sierpinski carpet is the attractor of eight contracting iterative functions of homothety of ratio $1/3$ centered at the vertices and sides of a square in which can be any put any plane geometric shape.

	This time in $\mathbb{R}^2$ we consider the eight homotheties ($h$):
	
	and we start for example from the four following points:
	
	that corresponds to a filled square (but we can choose anything else!):
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/sierpinski_carpet_set_01.jpg}
		\caption[]{Start set of Sierpinski carpet fractal}
	\end{figure}
	After application of the eight homotheties functions (we leave to the reader the manual calculations in the same we way we have already do it for the triangle), we get the following form of eight squares:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/sierpinski_carpet_set_02.jpg}
		\caption[]{Second iteration of Sierpinski carpet fractal}
	\end{figure}
	and applying again the eight homotheties (fortunately there are computers...):
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/sierpinski_carpet_set_03.jpg}
		\caption[]{Third iteration of Sierpinski carpet fractal}
	\end{figure}
	and again:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/sierpinski_carpet_set_04.jpg}
		\caption[]{Fourth iteration of Sierpinski carpet fractal}
	\end{figure}
	etc.
	
	The resulting fixed point (attractor) is obviously named the "\NewTerm{Sierpinski's carpet}\index{Sierpinski's carpet}" and this is the shape of the receiving antenna of the majority of our cell phones in the early 121st century (holocene calendar).

	The above figures can be obtained successively with the following Maple 4.00b code (if the copy/paste form the book in Maple 4.00b does not work, simply rewrite the code in the Maple console):
	
	\texttt{>transforme\textunderscore point := proc(t, p)}\\
      	\texttt{[t[1]*p[1]+t[2]*p[2]+t[5], t[3]*p[1]+t[4]*p[2]+t[6]]}\\
		\texttt{end:}

	\texttt{>transform\textunderscore square := proc(t, square) }\\
      	\texttt{local i;}\\
     	\texttt{[seq(transforme\textunderscore point(t, square[i]), i=1 .. 4)]}\\
		\texttt{end:}

		\texttt{>IFSS := proc(n, liste\textunderscore de \textunderscore transformations,col)}\\
      	\texttt{local i, j, k, s, seq\textunderscore square:}\\
      	\texttt{seq\textunderscore square :=[[0,0],[1,0],[1,1],[0,1]];}\\
      	\texttt{for j to n do}\\
      	\texttt{s := NULL;}\\
      	\texttt{for i to nops(liste\textunderscore de\textunderscore transformations) do}\\
         	\texttt{s := s,}\\
         	\texttt{seq(transform\textunderscore square(liste\textunderscore de \textunderscore transformations[i],}\\
         	\texttt{op(k, [seq\textunderscore square])),}\\
         	\texttt{k=1 .. nops([seq\textunderscore square]))}\\
       	\texttt{od;}\\
       	\texttt{seq\textunderscore square := s }\\
     	\texttt{od;}\\
     	\texttt{plots[polygonplot]([seq\textunderscore square], axes=none, color=col, scaling=constrained)}\\
   	\texttt{end:}

		\texttt{> dywan:= [[evalf(1/3),0,0,evalf(1/3),0,0],[evalf(1/3),0,0,evalf(1/3), evalf(1/3),0],[evalf(1/3),0,0,evalf(1/3),evalf(2/3),0],  [evalf(1/3),0,0,evalf(1/3),0,evalf(2/3)], [evalf(1/3),0,0,evalf(1/3),evalf(1/3), evalf(2/3)],[evalf(1/3),0,0,evalf(1/3), evalf(2/3),evalf(2/3)],	[evalf(1/3),0,0,evalf(1/3),0,evalf(1/3)], [evalf(1/3),0,0,evalf(1/3),evalf(2/3),evalf(1/3)]]:}\\

	\texttt{>IFSS(0, dywan, blue);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/sierpinski_carpet_set_maple_01.jpg}
		\caption[]{Start set of Sierpinski carpet fractal}
	\end{figure}
	\texttt{>IFSS(1, dywan, blue);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/sierpinski_carpet_set_maple_02.jpg}
		\caption[]{First iteration of Sierpinski carpet fractal}
	\end{figure}
	\texttt{>IFSS(2, dywan, blue);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/sierpinski_carpet_set_maple_03.jpg}
		\caption[]{Second iteration of Sierpinski carpet fractal}
	\end{figure}
	\texttt{>IFSS(3, dywan, blue);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/sierpinski_carpet_set_maple_04.jpg}
		\caption[]{Third iteration of Sierpinski carpet fractal}
	\end{figure}
	\texttt{>IFSS(4, dywan, blue);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/sierpinski_carpet_set_maple_05.jpg}
		\caption[]{Fourth iteration of Sierpinski carpet fractal}
	\end{figure}
	
	\pagebreak
	\subsubsection{Fractal spirals}
	We just saw two fractals of the Sierpinski's fractal family therefore based solely on contracting homotheties. Let us now see a fractal that combines rotation and scaling contraction.
	
	In $\mathbb{R}^2$ we consider the two applications of homotheties ($h$) and rotations ($R$) as follows:
	
	With a triangle and always with Maple 4.00b, this gives us (if the copy/paste form the book in Maple 4.00b does not work, simply rewrite the code in the Maple console):
	
	\texttt{>transforme\textunderscore triangle := proc(t, triangle)}\\
     \texttt{local i;}\\
     \texttt{[seq(transforme\textunderscore point(t, triangle[i]), i=1 .. 3)]}\\
	\texttt{end:}\\

	\texttt{>IFS := proc(n, liste\textunderscore de\textunderscore transformations,col)}\\
     \texttt{local i, j, k, s, sequence\textunderscore de\textunderscore triangles:}\\
     \texttt{options `Copyright by Alain Schauber, 1996`;}\\
     \texttt{sequence\textunderscore de\textunderscore triangles := [[0, 0], [1, 0], [0.5, 1]];}\\
     \texttt{for j to n do}\\
     \texttt{ s := NULL;}\\
     \texttt{for i to nops(liste\textunderscore de\textunderscore transformations) do}\\
        \texttt{s := s,}\\
        \texttt{seq(transforme\textunderscore triangle(liste\textunderscore de\textunderscore transformations[i],}\\
        \texttt{op(k, [sequence\textunderscore de\textunderscore triangles])),}\\
        \texttt{k=1 .. nops([sequence\textunderscore de\textunderscore triangles]))}\\
       \texttt{od;}\\
      \texttt{sequence\textunderscore de\textunderscore triangles := s}\\
     \texttt{od;}\\
    \texttt{plots[polygonplot]([sequence\textunderscore de\textunderscore triangles], axes=none, color=col, scaling=constrained)}\\
  \texttt{end:}

  \texttt{>a:=evalf(5*Pi/6);b:=evalf(Pi/6);}\\
  \texttt{>c1x:=0.25;c1y:=0.5;c2x:=0.5;c2y:=0.5;}\\
  \texttt{>h1:=0.2;h2:=0.95;}\\
  \texttt{>spirale:=[[h1*cos(a),-h1*sin(a),h1*sin(a),h1*cos(a),(1-h1*cos(a))*c1x}\\
  \texttt{+h1*sin(a)*c1y,-h1*sin(a)*c1x+(1-h1*cos(a))*c1y],[h2*cos(b),-h2*sin(b),h2*sin(b),}\\
  \texttt{h2*cos(b),(1-h2*cos(b))*c2x+h2*sin(b)*c2y,-h2*sin(b)*c2x+(1-h2*cos(b))*c2y]]:}\\

 	\texttt{>IFS(1,spirale,blue);}\\
 	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/spiral_fractal_set_01.jpg}
		\caption[]{First iteration of spiral fractal}
	\end{figure}
	and as the convergence is very slow, we will give the results by step of  $5$ iterations.	
	\texttt{>IFS(6,spirale,blue);}\\
 	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/spiral_fractal_set_02.jpg}
		\caption[]{Sixth iteration of spiral fractal}
	\end{figure}
	\texttt{>IFS(11,spirale,blue);}\\
 	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/spiral_fractal_set_03.jpg}
		\caption[]{Eleventh iteration of spiral fractal}
	\end{figure}
	\texttt{>IFS(16,spirale,blue);}\\
 	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/spiral_fractal_set_04.jpg}
		\caption[]{Sixteenth iteration of spiral fractal}
	\end{figure}
	
	
	\subsubsection{Von Koch fractal (Koch snowflake)} 
	Still, in fractal obtained by contracting homotethies ($h$) and rotation ($R$) but to which we add now a translation ($T$), the Von Koch curve is a well known fractal, it can be obtained by the following applications (we already met this fractal in the section of Euclidean Geometry when we have introduced the concept of fractal dimension):	
	
	Here is the corresponding Maple 4.00b code (if the copy/paste form the book in Maple 4.00b does not work, simply rewrite the code in the Maple console):
	
	\texttt{>koch := proc(p:: numeric)}\\
	\texttt{local m, n, k, l, s, h, x, y, pts, t, i;}\\
     \texttt{h := 3\string^(-p);}\\
     \texttt{pts := table([]): \# [0, 0];}\\
     \texttt{pts[0]:=[0,0];}\\
     \texttt{x := 0; y := 0;}\\
     \texttt{for n from 0 to (4\string^p) do}\\
        \texttt{m := n;}\\
        \texttt{s := 0;}\\
       \texttt{ for l from 0 to p-1 do}\\
           \texttt{t := irem(m, 4);}\\
           \texttt{m := iquo(m, 4);}\\
           \texttt{s := s+irem((t+1), 3) - 1}\\
        \texttt{od;  \# end of for l}\\
        \texttt{x := evalhf(x+cos(Pi*s/3)*h);}\\
        \texttt{y := evalhf(y+sin(Pi*s/3)*h);}\\
        \texttt{pts[n+1] := [x, y];}\\
     \texttt{od;}\\
    \texttt{[seq(pts[i], i=0 .. n-1)];}\\
  \texttt{end:}\\

	\texttt{>plot(koch(0), scaling=constrained, style=LINE, axes=NONE, color=blue,thickness=2);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/von_koch_set_01.jpg}
		\caption[]{Start set of Von Koch fractal}
	\end{figure}
	\texttt{>plot(koch(1), scaling=constrained, style=LINE, axes=NONE, color=blue,thickness=2);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/von_koch_set_02.jpg}
		\caption[]{First iteration of Von Koch fractal}
	\end{figure}
	\texttt{>plot(koch(2), scaling=constrained, style=LINE, axes=NONE, color=blue,thickness=2);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/von_koch_set_03.jpg}
		\caption[]{Second iteration of Von Koch fractal}
	\end{figure}
	\texttt{>plot(koch(3), scaling=constrained, style=LINE, axes=NONE, color=blue,thickness=2);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/von_koch_set_04.jpg}
		\caption[]{Third iteration of Von Koch fractal}
	\end{figure}
	etc. etc. Until the following attractor:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/von_koch_set_05.jpg}
		\caption[]{Fourth iteration of Von Koch fractal}
	\end{figure}
	
	The Koch snowflake (see figure below) is a variant of the Von Koch line and thab can be constructed by starting with an equilateral triangle, then recursively altering each line segment as follows:
	\begin{enumerate}
		\item Divide the line segment into three segments of equal length.
		\item Draw an equilateral triangle that has the middle segment from step 1 as its base and points outward.
		\item Remove the line segment that is the base of the triangle from step 2.
	\end{enumerate}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/von_kock_snowflake.jpg}
		\caption[Von Koch snowflake]{Von Koch snowflake (source: Wikipedia)}
	\end{figure}
	After each iteration, the number of sides of the Koch snowflake increases by a factor of $4$, so the number of sides after n iterations is given by:
	
	If the original equilateral triangle has sides of length $s$, the length of each side of the snowflake after $n$ iterations is:
	
	the perimeter of the snowflake after $n$ iterations is therefore of:
	
	The Koch curve has an infinite length because the total length of the curve increases by one third with each iteration. Thas is to say:
	
	The funny thing is that the area is finite... when the perimeter is infinite...
	
	Indeed, in each iteration a new triangle is added on each side of the previous iteration, so the number of new triangles added in iteration $n$ is:
	
	The area of each new equilateral triangle added in an iteration is one ninth of the area of each equilateral  triangle added in the previous iteration, so the area of each equilateral  triangle added in iteration $n$ is:
	
	where $a_0$ is the area of the original equilateral  triangle. The total new area added in iteration $n$ is therefore:
	
	The total area of the snowflake after $n$ iterations is then obviously:
	
	Collapsing the geometric sum of the type $\sum x^n$ using (\SeeChapter{see section Sequences and Series page \pageref{geometric sequence}}):
	
	we get:
	
	The limit of the area is then immediate:
	
	So the area of the Koch snowflake is $8/5$ of the area of the original triangle. 
	
	What is so disturbing with Von Koch fractal is that we start from a line of finite length, to arrive at the end to a line of infinite length if we reiterate infinitly structure but it has a finished surface ... it's a "pathological" curve as the mathematicians say sometimes.
	
	The Koch curve is legendary because it was used by Mandelbrot to write an article about the problem of measuring the length of the coasts of sea coasts (because the most the basic unit of measurement taken was small, more the perimeter ot the coast was great). He proposed to consider the coasts as fractals for which it is impossible to measure the perimeter but "fractal tree" or in other words: the fractal dimension.
	
	The Koch Curve consists of $4^n$ subsets with magnification factor $3^n$. So the fractal dimension is:
	
	
	\paragraph{Coastline paradox}\mbox{}\\\\
	It is now the right moment in our point of view to speak about the "\NewTerm{coastline paradox}\index{coastline paradox}" that is the counterintuitive observation that the coastline of a landmass does not have a well-defined length. This results from the fractal-like properties of coastlines. The first recorded observation of this phenomenon was by Lewis Fry Richardson and it was expanded by Benoit Mandelbrot.

	More concretely, the length of the coastline depends on the method used to measure it. Since a landmass has features at all scales, from hundreds of kilometers in size to tiny fractions of a millimeter and below, there is no obvious size of the smallest feature that should be measured around, and hence no single well-defined perimeter to the landmass. Various approximations exist when specific assumptions are made about minimum feature size.
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/coastline_paradox.jpg}
		\caption[Coastline paradox]{Coastline paradox (source: Wikipedia)}
	\end{figure}
	The length of "true fractal" therefore always diverges to infinity, as if one were to measure a coastline with infinite, or near-infinite resolution, the length of the infinitely smaller bends of the coastline would add up to infinity. However, this figure relies on the assumption that space can be subdivided indefinitely. The truth value of this assumption - which underlies Euclidean geometry and serves as a useful model in everyday measurement - is a matter of philosophical speculation, and may or may not reflect the changing realities of 'space' and 'distance' on the atomic level (approximately the scale of a nanometer). The Planck length, many orders of magnitude smaller than an atom, is proposed as the smallest measurable unit possible in the universe.

	In reality, permanent features of the coastline of order of size $1$ cm or less do not exist, because of erosion and other action of the sea. In most places the minimum size is much larger than this. Thus the concept of an infinite fractal is not applicable to the coastline.
For practical considerations, an appropriate choice of minimum feature size is on the order of the units being used to measure. If a coastline is measured in kilometers, then small variations much smaller than one kilometer are easily ignored.

	
	
	\subsubsection{Natural fractals}
	Besides the purely mathematical aspect of fractals, we can find, via heuristics, contracting applications for fractal shapes similar to that we can find in nature. Let us see some examples with Maple 4.00b always taking first for common basis of all fractals that follow, the following procedures (if the copy/paste form the book in Maple 4.00b does not work, simply rewrite the code in the Maple console):
	
	\texttt{>transforme\textunderscore point := proc(t, p)}\\
    \texttt{[t[1]*p[1]+t[2]*p[2]+t[5], t[3]*p[1]+t[4]*p[2]+t[6]]}\\
	\texttt{end:}

	\texttt{>transforme\textunderscore triangle := proc(t, triangle)}\\
    \texttt{local i;}\\
    \texttt{[seq(transforme\textunderscore point(t, triangle[i]), i=1 .. 3)]}\\
  	\texttt{end:}

  	\texttt{>transform\textunderscore square := proc(t, square)}\\
    \texttt{local i;}\\
    \texttt{[seq(transforme\textunderscore point(t, square[i]), i=1 .. 4)]}\\
	\texttt{end:}

	\texttt{>IFS := proc(n, liste\textunderscore de\textunderscore transformations,col)}\\
     \texttt{local i, j, k, s, sequence\textunderscore de\textunderscore triangles:}\\
     \texttt{options `Copyright by Alain Schauber, 1996`;}\\
     \texttt{sequence\textunderscore de\textunderscore triangles := [[0, 0], [1, 0], [0.5, 1]];}\\
     \texttt{for j to n do}\\
     \texttt{s := NULL;}\\
     \texttt{for i to nops(liste\textunderscore de\textunderscore transformations) do}\\
        \texttt{s := s,}\\
        \texttt{seq(transforme\textunderscore triangle(liste\textunderscore de\textunderscore transformations[i],}\\
        \texttt{op(k, [sequence\textunderscore de\textunderscore triangles])),}\\
        \texttt{k=1 .. nops([sequence\textunderscore de\textunderscore triangles]))}\\
       \texttt{od;}\\
      \texttt{sequence\textunderscore de\textunderscore triangles := s}\\
     \texttt{od;}\\
    \texttt{plots[polygonplot]([sequence\textunderscore de\textunderscore triangles], axes=none, color=col, scaling=constrained)}\\
  \texttt{end:}

	\texttt{>IFSS := proc(n, liste\textunderscore de\textunderscore transformations,col)}\\
     \texttt{local i, j, k, s, seq\textunderscore square:}\\
     \texttt{seq\textunderscore square :=[[0,0],[1,0],[1,1],[0,1]];}\\
     \texttt{for j to n do}\\
     \texttt{s := NULL;}\\
     \texttt{for i to nops(liste\textunderscore de\textunderscore transformations) do}\\
        \texttt{s := s,}\\
        \texttt{seq(transform\textunderscore square(liste\textunderscore de\textunderscore transformations[i],}\\
        \texttt{op(k, [seq\textunderscore square])),}\\
        \texttt{k=1 .. nops([seq\textunderscore square]))}\\
      \texttt{od;}\\
      \texttt{seq\textunderscore square := s }\\
    \texttt{od;}\\
   \texttt{plots[polygonplot]([seq\textunderscore square], axes=none, color=col, scaling=constrained)}\\
  \texttt{end:}\\
	
	\paragraph{Branch}\mbox{}\\\\
	We start from:
	
	\texttt{>rameau:=[[.387,.430,.430,-.387,.2560,.5220], }\\
	\texttt{[.441,-.091,-.009,-.322,.4219,.5059], [-.468,.020,-.113,.015,.4,.4]]:}

	And we get:

	\texttt{> IFSS(0,rameau,green);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/branch_set_01.jpg}
		\caption[]{Start set for generic branch fractal}
	\end{figure}
	\texttt{> IFSS(1,rameau,green);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/branch_set_02.jpg}
		\caption[]{First iteration set for generic branch fractal}
	\end{figure}
	\texttt{> IFSS(2,rameau,green);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/branch_set_03.jpg}
		\caption[]{Second iteration set for generic branch fractal}
	\end{figure}
	\texttt{> IFSS(3,rameau,green);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/branch_set_04.jpg}
		\caption[]{Third iteration set for generic branch fractal}
	\end{figure}
	\texttt{> IFSS(4,rameau,green);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/branch_set_05.jpg}
		\caption[]{Fourth iteration set for generic branch fractal}
	\end{figure}
	\texttt{> IFSS(5,rameau,green);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/branch_set_06.jpg}
		\caption[]{Fifth iteration set for generic branch fractal}
	\end{figure}
	\texttt{> IFSS(6,rameau,green);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/branch_set_07.jpg}
		\caption[]{Sixth iteration set for generic branch fractal}
	\end{figure}
	
	\paragraph{Snowflake}\mbox{}\\\\
	We start from:
	
	\texttt{>cristal:=[[.255,0,0,.255,.3726,.6714],[.255,0,0,.255,.1146,.2232], }
	\texttt{[.255,0,0,.255,.6306,.2232],[.37,-.642,.642,.37,.6356,-.0061]]:}

	And we get:
	
	\texttt{>IFSS(0, cristal,green);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/snowflake_set_01.jpg}
		\caption[]{Start set for snowflake fractal}
	\end{figure}
	\texttt{> IFSS(1, cristal,green);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/snowflake_set_02.jpg}
		\caption[]{First iteration set for snowflake fractal}
	\end{figure}
	\texttt{> IFSS(2, cristal,green);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/snowflake_set_03.jpg}
		\caption[]{Second iteration set for snowflake fractal}
	\end{figure}
	\texttt{> IFSS(3, cristal,green);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/snowflake_set_04.jpg}
		\caption[]{Third iteration set for snowflake fractal}
	\end{figure}
	\texttt{> IFSS(4, cristal,green);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/snowflake_set_05.jpg}
		\caption[]{Fourth iteration set for snowflake fractal}
	\end{figure}
	\texttt{> IFSS(5, cristal,green);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/snowflake_set_06.jpg}
		\caption[]{Fifth iteration set for snowflake fractal}
	\end{figure}
	\texttt{> IFSS(6, cristal,green);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/snowflake_set_07.jpg}
		\caption[]{Sixth iteration set for snowflake fractal}
	\end{figure}
	\texttt{> IFSS(7, cristal,green);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/snowflake_set_08.jpg}
		\caption[]{Seventh iteration set for snowflake fractal}
	\end{figure}
	
	\pagebreak
	\paragraph{Tree}\mbox{}\\\\
	We start from:
	
	\texttt{>tree := [[-0.04, 0, -0.23, -0.65, -0.08, 0.26], [0.61, 0, 0, 0.31, 0.07, 2.5],}
	\texttt{[0.65, 0.29, -0.3, 0.48, 0.54, 0.39], [0.64, -0.3, 0.16, 0.56, -0.56, 0.4]]:}

	And we get:
	
	\texttt{>IFS(0, tree, green);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/tree_set_01.jpg}
		\caption[]{Start set for tree fractal}
	\end{figure}
	\texttt{> IFS(1, tree, green);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/tree_set_02.jpg}
		\caption[]{First iteration set for tree fractal}
	\end{figure}
	\texttt{> IFS(2, tree, green);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/tree_set_03.jpg}
		\caption[]{Second iteration set for tree fractal}
	\end{figure}
	\texttt{> IFS(3, tree, green);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/tree_set_04.jpg}
		\caption[]{Third iteration set for tree fractal}
	\end{figure}
	\texttt{> IFS(4, tree, green);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/tree_set_05.jpg}
		\caption[]{Fourth iteration set for tree fractal}
	\end{figure}
	\texttt{> IFS(5, tree, green);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/tree_set_06.jpg}
		\caption[]{Fifth iteration set for tree fractal}
	\end{figure}
	
	\pagebreak
	\paragraph{Fern}\mbox{}\\\\
	We start from:
	
	\texttt{>fougere:=[[0,0,0,0.16,0,0],[0.2,-0.26,0.23,0.22,0,1.6],}
	\texttt{[-0.15,0.28,0.26,0.24,0,0.44],[0.85,0.04,-0.04,0.85,0,1.6]]:}

	And we get:
	
	\texttt{>IFS(0, fougere, blue);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/fern_set_01.jpg}
		\caption[]{Start set for fern fractal}
	\end{figure}
	\texttt{>IFS(1, fougere, blue);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/fern_set_02.jpg}
		\caption[]{First iteration set for fern fractal}
	\end{figure}
	\texttt{>IFS(2, fougere, blue);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/fern_set_03.jpg}
		\caption[]{Second iteration set for fern fractal}
	\end{figure}
	\texttt{>IFS(3, fougere, blue);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/fern_set_04.jpg}
		\caption[]{Third iteration set for fern fractal}
	\end{figure}
	\texttt{>IFS(4, fougere, blue);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/fern_set_05.jpg}
		\caption[]{Fourth iteration set for fern fractal}
	\end{figure}
	\texttt{>IFS(5, fougere, blue);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/fern_set_06.jpg}
		\caption[]{Fifth iteration set for fern fractal}
	\end{figure}
	\texttt{>IFS(6, fougere, blue);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/fern_set_07.jpg}
		\caption[]{Sixth iteration set for fern fractal}
	\end{figure}
	\texttt{>IFS(7, fougere, blue);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/fern_set_08.jpg}
		\caption[]{Eighth iteration set for fern fractal}
	\end{figure}
	etc. Unitl we get:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/fern_set_09.jpg}
		\caption[]{Attractor set for fern fractal}
	\end{figure}
	and we will stop here because the examples of IFS and natural fractals are uncountable...
	
	\pagebreak
	\subsection{Escape Time Algorithm Fractals}
	Several methods have been proposed to construct fractal images as we mentioned at the beginning of this section. The are generally three way of generating fractals that are well known:
	\begin{itemize}
		\item Iterated function systems (seen previously)
		\item Escape time fractals (we will see now)
		\item Random fractals
	\end{itemize}
	So we will now focus on the methods named "\NewTerm{escape time methods}\index{escape time methods}".
	
	For this, we place ourselves in the complex plane $\mathbb{C}$ consisting of the points $M (x, y)$ of affix:
	
	Afterwards we consider a complex sequence defined by:
	
	and:
	
	the function $f$ being a complex continuous function. We will assumed that $f$ is build in such a way that is has a fixed point $x_0$, that is to say that there exists $x_0$ such as:
	
	It is therefore simply the application of fixed-point theorem already mentioned several times before. Under some conditions on f and equation, we find that the following points does not diverge (which means it is not interested in points that converge but to those who do not diverge!). This method is the basis for the construction of Mandelbrot and Julia Sets.
	
	Building a fractal image from such a set of defined sequences, is equivalent to study for each pair $(x, y)$ of the plane the behaviour of the sequence itself. We then associates a color to each result (that is to say, each pair $(x, y)$) representing the "speed" of divergence of the sequence.
	
	To study the convergence of a series, we look at his first $n$ elements, if we detect that the divergence conditions are satisfied then we can say that this sequence diverges, otherwise this result is potentially convergent. We notice that more $n$ is big, the more accurate are the results (but the computing time will be great).
	
	The simplest algorithm for generating a representation of an escape time fractal (set) consist in repeating a calculation performed for each $x$, $y$ point in the plot area and based on the behaviour of that calculation, a color is chosen for that pixel.

	The $x$ and $y$ locations of each point are used as starting values in a repeating, or iterating calculation (described in detail below). The result of each iteration is used as the starting values for the next. The values are checked during each iteration to see whether they have reached a critical "escape" condition. If that condition is reached, the calculation is stopped, the pixel is drawn, and the next $x$, $y$ point is examined. For some starting values, escape occurs quickly, after only a small number of iterations. For starting values very close to but not in the set, it may take hundreds or thousands of iterations to escape. For values within the Mandelbrot Set, escape will never occur. The programmer or user must choose how much iteration, or "depth", they wish to examine. The higher the maximal number of iterations, the more detail and subtlety emerge in the final image, but the longer time it will take to calculate the fractal image.

	Escape conditions can be simple or complex. Because no complex number with a real or imaginary part greater than $2$ can be part of the set, a common bailout is to escape when either coefficient exceeds$ $2. A more computationally complex method that detects escapes sooner, is to compute distance from the origin using the Pythagorean theorem, i.e., to determine the absolute value, or modulus, of the complex number. If this value exceeds $2$, the point has reached escape. More computationally intensive rendering variations include the Buddhabrot method, which finds escaping points and plots their iterated coordinates.

	The color of each point represents how quickly the values reached the escape point. Often black is used to show values that fail to escape before the iteration limit, and gradually brighter colors are used for points that escape. This gives a visual representation of how many cycles were required before reaching the escape condition.

	To render such an image, the region of the complex plane we are considering is subdivided into a certain number of pixels. To color any such pixel, let $c$  be the midpoint of that pixel. We now iterate the critical point under the chosen function, checking at each step whether the orbit point has modulus larger than $R$ the convergence radius. When this is the case, we know that $c$  does not belong to the escape time fractal, and we color our pixel according to the number of iterations used to find out. Otherwise, we keep iterating up to a fixed number of steps, after which we decide that our parameter is "probably" in the escape time fractal, or at least very close to it, and color the pixel black.

	In pseudocode, this algorithm would look as follows. The algorithm does not use complex numbers and manually simulates complex-number operations using two real numbers, for those who do not have a complex data type. The program may be simplified if the programming language includes complex-data-type operations.
	
	\begin{algorithm}[H]
	\KwData{I,R}
	\For{\text{each pixel} $(P_x,P_y)$ \text{on the screen}}{
        $x_0 =$ scaled $x$ coordinate of pixel \;
        $y_0 =$ scaled $y$ coordinate of pixel \;
		$x=0.0$\;
		$y=0.0$\;
		iteration$=0$\;
		max\textunderscore iteration$=I$\;
		\While{$x^2 + y^2 < R^2$  AND  $i <$ max\textunderscore iteration}{
			$x=\Re(z(x,y))+x_0$\;
			$y=\Im(z(x,y))+y_0$\;
			$i:=i+1$\;
		}
		color := palette[iteration]\;
	}
	 plot $(P_x,P_y,\text{colors})$\;
	 \caption{Escape Time Fractal pseudocode algorithm}
	\end{algorithm}
	
	\subsubsection{Mandelbrot Set}
	We construct the Mandelbrot Set through iterations in the complex plane (this is named also "\NewTerm{holomorphic dynamics}\index{holomorphic dynamics}"). The function is of the form:
	
	where $c$ a constant parameter such that $c\in\mathbb{C}$ (so we double the argument of the initial complex number we squared its norm!).
	
	The first term of the sequence is zero. So we have the following defined by:
	
	Why do we start with $z_0=0$?: Because zero is the critical point of $z^2+c$, that is to say the point satisfying the extremum:
	
	For each point of affix $x + \mathrm{i}y$ of the plane, we study the above sequence for $c=x+\mathrm{i}y$. If the sequence diverges, we say that the tested point does not belong to the Mandelbrot Set, if the sequence converges, we say that the point belongs to the Mandelbrot Set defined then by (the definition and name is due to Adrien Douady, in tribute to Benoit Mandelbrot):
	
	
	To reproduce a basic representation of the Mandelbrot fractal, we associate to $c$ complex values of the plane. It is generally considered the portion of the complex plane having as real part, values between $-2.5$ and $1.5$, and as imaginary part, values between $-1.5$ and $1.5$. This portion of the complex plane is divided so to form a grid whose elements will be associated with values of $c$. For each value of $c$, we get a result that modules can converge (bounded sequence) or diverge (non-bounded sequence).
	
	In practice, it is considered that sequence of modules converges if the first $30$ modules are less than $2$ ($R=2$ in the previous pseudocode), that is to say $|z_{30}|\leq 30$. When the sequence of the modules converges (bounded sequence), we color in black the grid point. After considering all points of the grid, we get a set of blackened points named: "Mandelbrot Set" or "Mandelbrot fractal" as we already know and denoted by $\mathcal{M}$. What constitutes a remarkably curious result!
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/elementary_mandelbrot_set_reprsentation.jpg}
		\caption{Bichromic Mandelbrot Fractal}
	\end{figure}
	We can also color the points outside the Mandelbrot Set using colors that depend on the number of terms calculated before obtaining a module greater than or equal to $2$. The Module points of the same color can be interpreted as points away at the same speed of the Mandelbrot Set.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The list of $z_i$ generated by the iteration is named the "\NewTerm{orbit}\index{orbit (fractal)}" of $z_0$.
	\end{tcolorbox}
	We can also discover the Mandelbrot fractal in-deep using the following Maple 4.00b code derived from the earlier pseudocode (available usually in high-school). The reader just have to copy the program below on a Maple worksheet and indicate instead of -$2 .. 1 .. 1.5 -1.5$ of the last line, the range of the real and imaginary parts of $c$ he wants to discover:\\
	
	 \texttt{>restart: with(plots):\\
	>couleur:=proc(a,b)\\
	local x,y,xi,yi,n;\\
	x:=a;\\
	y:=b;\\
	for n from 0 to 30 while evalf(x\string ^2+y\string ^2) < 4 do;\\
	   xi:=evalf(x\string ^2-y\string ^2+a);\\
	   yi:=evalf(2*x*y+b);\\
	   x:=xi;\\
	   y:=yi;\\
	od;\\
	n\\
	end:}
	
	You will therefore get the following result (the equivalent code for MATLAB™ and \texttt{R} are given in the companion books about MATLAB™ and \texttt{R}):
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/mandelbrot_fractal_maple.jpg}
		\caption{Mandelbrot Fractal with Maple 4.00b}
	\end{figure}
	The Mandelbrot Set is self-similar in the vicinity of points named "\NewTerm{Misiurewicz points}\index{Misiurewicz points}":
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/computing/mandelbrot_fractal_auto_similarity.jpg}
		\caption{Self-Similarity of Mandelbrot Fractal}
	\end{figure}
	It seems also that it is possible to prove (we are still searching for the proof...) that the Mandelbrot fractal real axes can be put in correspondence with the logistic bifurcation diagram that we have studied in the section of Population Dynamics such that:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/mandelbrot_fractal_logistic_bifurcation.jpg}
		\caption[Mandelbrot-Logistic bifurcation correspondence]{Mandelbrot-Logistic bifurcation correspondence (source: Wikipedia)}
	\end{figure}
	The functions are obviously both quadratic. In fact, the Mandelbrot Set can be recoded into the form of the logistic map (and vice versa).
	
	Indeed, as the Mandelbrot Fractal is obtained by iteration:
	
	and that we know that the logistic bifurcation is obtained by the iteration of:
	
	So let us put in Mandelbrot function the (anticipated) change of variable such that $z_n$ be dependent of $x_n$ and $r$ but $c$ must as constant must also be only dependent of the constant $r$:
	
	Then:
	
	
	\subsubsection{Julia Set}
	The Julia Set is built almost in the same way that the Mandelbrot Set (since the Julia Set is actually a subset of it after investigation!). In the Mandelbrot Set, $c$ scans the plane. For the Julia Set, $c$ is fixed during the computation of the image. To each $c$ corresponds a particular set that will be denoted $J(c)$. What varies this time is $z_0$, which takes the value of the point to test. It is therefore $z_0$ that scans the plan.
	
	A point of initial coordinates $(x_0, y_0)$ and affix $x + \mathrm{i}y$ belongs to $J(c)$ if and only if the sequence defined by:
	
	converge (is bounded).
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/computing/julia_set_making_of.jpg}
		\caption{Makingofa Julia Set}
	\end{figure}
	In fact, the Mandelbrot Set is the set of points $c$ such that the Julia Set of parameter $c$ is connex to, that is to say we can always found a $c$ such that when starting with $z_0$ in the Mandelbrot Set is equivalent after a few iterations as starting with a fixed $z_0\neq 0$ and a given $c$ (thus the Mandelbrot Set generalizes all the Julia Sets !!!). So the figure of the Mandelbrot Set contains figures all the Julia Sets, which is remarkable (but logic ...!):
	
	If again we adapt the pseudocode given earlier, we get to a given scale factor  the fractal shown below (obtained through the small Maple 4.00b code below and already used earlier for the Mandelbrot fractal):
	
	\texttt{>restart; with(plots):\\
	>julia:= proc(c,x, y)local z, m;\\
	z:= evalf(x+y*I);\\
	for m from 0 to 30 while abs(z) < 3 do\\
	   z:= z\string^2 + c\\
	   od;\\
	   m\\
	end:\\\\
	>J:= proc(d)\\
	global phonyvar;\\
	phonyvar:= d;\\
	(x, y) -> julia(phonyvar, x, y)\\
	end:\\\\
	>plot3d(0, -2..2, -1.3..1.3, style=patchnogrid,orientation=[-90,0], grid=[270, 270],scaling=constrained, color=J(-1.25));\\}
	
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/julia_fractal_maple.jpg}
		\caption{Julia Fractal with Maple 4.00b}
	\end{figure}
	and to show that the Mandelbrot Set contains all the Julia Sets we have qualitatively:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/computing/mandelbrot_family.jpg}
		\caption{Illustration of "fatherhood" ... of the Mandelbrot Set}
	\end{figure}
	So we must be able to write a single algorithm (see below) that achieves all of Julia fractals by simply selecting a good starting point as shown in the following figures (we can see on top right the "Douady's Rabbit Fractal" also named "dragon fractal":
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/julia_sets.jpg}
	\end{figure}
	That we get with the following Maple 4.00b  code:
	
	\texttt{>couleur:=proc(a,b)\\
	local x,y,xi,yi,n;\\
	global reel,imaginaire;\\
	x:=a;\\
	y:=b;\\
	for n from 0 to 100 while evalf(x\string^2+y\string^2)<4 do;\\
	xi:=evalf(x\string^2-y\string^2+reel);\\
	yi:=evalf(2*x*y+imaginaire);\\
	x:=xi;\\
	y:=yi;\\
	od;\\
	n;\\
	end:\\\\
	>reel:=-0.181;\\
	>imaginaire:=-0.667;\\\\
	>plot3d(0,(-13/10)..(13/10),(-13/10)..(13/10),orientation=[-90,0], style=patchnogrid,scaling=constrained,axes=framed,numpoints=20000
	,color=couleur);\\}
	
	\subsubsection{Newton Set}
	Newton Sets are also so named because they arise from the resolution of the problem looking for zeros of a function by the Newton's method (\SeeChapter{see section Numerical Methods page \pageref{newton method}}).

	Given $f$ a function with values in $\mathbb{C}$ and differentiable in $\mathbb{C}$ , we take $z_0$ in $\mathbb{C}$ such that:
	
	There are then two ways to proceed:
	\begin{enumerate}
		\item Either we focus on $|z_0-z_i|$ and then we do same as before.

		\item Either we wonder to which zero $r_k$ the sequence converges and we focus on $|z_i-z_k|$.
	\end{enumerate}
	If again we translate our pseudocode algorithm, into Maple language we get to a given scale factor the fractal shown below with Maple 4.00b:
	
	\texttt{>restart:\\
	>newton:= proc(x, y)\\
	local z, m;\\
	z:= evalf(x+y*I);\\
	for m from 0 to 50 while abs(z\string^3-1) >= 0.001 \\	do\\
	z:= z - (z\string^3-1)/(3*z\string^2)\\
	od;\\
	m\\
	end:\\\\
	>plot3d(0,-2..2,-1.5..1.5,orientation=[-90,0],grid=[250, 250], 	\\ style=patchnogrid,scaling=constrained,color=newton);\\}

	
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/newton_fractal.jpg}
		\caption{Newton Set fractal with Maple 4.00b}
	\end{figure}
	
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{70} & \pbox{20cm}{\score{3}{5} \\ {\tiny 20 votes,  66.00\%}} 
	\end{tabular} 
	\end{flushright}

	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Logical Systems}\label{logical systems}
	\lettrine[lines=4]{\color{BrickRed}T}{he} reader familiar with the purpose of this book should not expect to see here any schemes of buttons, switches, timing diagrams or MIL standard wiring diagrams. We will remain in a purely formal framework of logical systems and their tools.\\
	
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] We speak of "\NewTerm{asynchronous logic model}\index{asynchronous logic model}" (commonly named "\NewTerm{sequential logic model}\index{sequential logic model}") when the outputs of a system depends on the chronological order in which the entries will succeed.
		
		\item[D2.] We speak of "\NewTerm{combinatorial logic model}\index{combinatorial logic model}" when the outputs of a system depend only on the combination of the input variables.
	\end{enumerate}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	We differentiate the "\NewTerm{strict logic}\index{strict logic}" of "\NewTerm{fuzzy logic}\index{fuzzy logic}" that will both  be defined in the details later.
	\end{tcolorbox}
	
	\subsection{Strict Logic}
	Consider first a set which we will denote by $\mathcal{B}$ with two elements (more formally denoted by $\bot,\top$).
	
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] A "\NewTerm{strict logic variable}\index{strict logic variable}" or "\NewTerm{boolean variable}\index{Boolean variable}" is an element of $\mathcal{B}$ that has only two states $0$ and $1$ (as opposed to a fuzzy variable whose value can be \underline{between} $0$ and $1$). It is represented by Latin uppercase letters or lowercase Latin letters (depending on your choice).
		
		\item[D2.] A multivariate "\NewTerm{logic function $F$}\index{logic function}" of $n$ variables applies $\mathcal{B}^n$ in $\mathcal{B}$ such that:
		
		It combines to a $n$-tuple of logical variables $(b_0,b_1,\ldots ,b_{n-1})$ a value $F(b_0,b_1,\ldots ,b_{n-1})$.
		
		\item[D3.] There are different ways of expressing a logic function ("\NewTerm{Boolean function}\index{Boolean function}"). A function of $n$ variables is fully described by stating the values of this function for the set (or the subset of definition) of the combinations of the $n$-tuple variables:
		
	\end{enumerate}
	This statement usually takes the form of a table with $n + 1$ columns and no more than $2^n$ lines, each line exposing a combination of variables and the corresponding value of the function. The following table gives the general form of a "\NewTerm{truth table}\index{truth table}" function of three variables completely defined through a function $F$ (we already saw some simple examples in the section of Proof Theory at page \pageref{proof theory}):
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
					\hline
					\multicolumn{1}{c}{\cellcolor[gray]{0.75}$\pmb{A}$} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}$\pmb{B}$}  & \multicolumn{1}{c}{\cellcolor[gray]{0.75}$\pmb{C}$} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}$\pmb{F(A,B,C)}$} \\ \hline
					\centering\arraybackslash\ $0$ & \centering\arraybackslash\ $0$ & \centering\arraybackslash\ $0$ & \centering\arraybackslash\ $F(0,0,0)$ \\ \hline
					\centering\arraybackslash\ $0$ & \centering\arraybackslash\ $0$ & \centering\arraybackslash\ $1$ & \centering\arraybackslash\ $F(0,0,1)$ \\ \hline
					\centering\arraybackslash\ $0$ & \centering\arraybackslash\ $1$ & \centering\arraybackslash\ $0$ & \centering\arraybackslash\ $F(0,1,0)$ \\ \hline
					\centering\arraybackslash\ $0$ & \centering\arraybackslash\ $1$ & \centering\arraybackslash\ $1$ & \centering\arraybackslash\ $F(0,1,1)$ \\ \hline
					\centering\arraybackslash\ $1$ & \centering\arraybackslash\ $0$ & \centering\arraybackslash\ $0$ & \centering\arraybackslash\ $F(1,0,0)$ \\ \hline
					\centering\arraybackslash\ $1$ & \centering\arraybackslash\ $0$ & \centering\arraybackslash\ $1$ & \centering\arraybackslash\ $F(1,0,1)$ \\ \hline
					\centering\arraybackslash\ $1$ & \centering\arraybackslash\ $1$ & \centering\arraybackslash\ $0$ & \centering\arraybackslash\ $F(1,1,0)$ \\ \hline
					\centering\arraybackslash\ $1$ & \centering\arraybackslash\ $1$ & \centering\arraybackslash\ $1$ & \centering\arraybackslash\ $F(1,1,1)$  \\ \hline
			\end{tabular}
		\end{center}
		\caption{Generic truth table}
		\end{table}
		The elements of input of the systems will be considered as Boolean variables on which we can build a ring structure set, that by adding a particular axiom, we can bring to an algebra (in the computational sense and the set one!) commonly named "\NewTerm{Boolean Algebra}\index{Boolean algebra}" as we will see now.
		
		So Boolean Algebra is an algebra on itself (with a ring structure as we will define it rigorously later) proposing to translate signals with a value of the type $0/1$ (assimilated to: True/False) in mathematical expressions. For this, we define each elementary signal by "logical variables" and their treatment by "logical functions". Methods ("truth tables") exists to define the operations that we want to achieve, and to transcribe the result into an algebraic expression. Thanks the rules we will see later, these expressions can be simplified. This will allow to represent with simple symbols a logic circuit capable of performing basic arithmetic operations, that is to say a circuit that design the core components (at logic level) regardless of the realization through transistors (physical level).
	
	\subsubsection{Boolean Algebra}\label{boolean algebra}
	Boolean Algebra (or "Boolean ring" to a given axiom...) is therefore a structure which is most often used in electronic (or microelectronics/optoelectronics) this is why some people name it sometimes "\NewTerm{Switching Algebra}\index{switching algebra}" or "\NewTerm{logic gates}\index{logic gates}". Therefore, a processor is composed of transistors for performing functions on digital signals. These transistors assembled together form components for performing simple functions. From these components it is possible to create circuits performing fairly complex operations. Boolean Algebra (named after the English mathematician George Boole 11815-11864 according to holocene calendar) is a mean to create more or less easily such circuits.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	It would be better before you start reading this chapter, to read at least in diagonal the subsection about Logic in the section of Proof Theory (see page \pageref{proof theory}) and on algebraic structures in the section of Set Theory (see page \pageref{set theory}).
	\end{tcolorbox}
	It is necessary for a rigorous definition of a Boolean Algebra to give it in terms of abstract algebra.
	
	Reminder: A "\NewTerm{Boolean Algebra}\index{Boolean algebra}" $(\mathcal{B},\vee,\wedge)$ is a set containing two particular elements $\bot,\top$, (abstract forms the $0$ and $1$) and has two internal composition laws, $\vee,\wedge$ (AND and logical OR) and verifies the following axioms to form a ring structure such that $\forall a,b,c\in \mathcal{B}$:
	
	\begin{itemize}
		\item[A1.] Associativity: $(a \vee b)\vee c=a \vee (b \vee c)$ and $(a \wedge b)\wedge c=a \wedge (b\wedge c)$
		
		\item[A2.] Commutativity: $a \vee b=b \vee a$ and $a \wedge b=b\wedge a$
		
		\item[A3.] Absorption: $a \wedge (a \vee b)=a$ and $a \vee (a \wedge b)=a$
		
		\item[A4.] Distributivity: $(a \vee b)\wedge c=(a \wedge c)\vee b \wedge c$ and $(a\wedge b)\vee c=(a \vee c)\wedge(b\vee c)$
		
		\item[A5.] Idempotence: $a \vee a=a$ and $a\wedge a=a$
		
		\item[A6.] Completation (or inversion): $a$ has a complement (negation) denoted by $\neg a$ or $\bar{a}$ (NOT) such as: $a \wedge \neg a=\top$ and $a \vee \neg a=\bot$
	\end{itemize}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The first four axioms establish a ring structure. The fifth axiom (idempotence) added to the first four defines the concept of "Boolean Algebra".
	\end{tcolorbox}
	Strictly speaking to form a Boolean Algebra we required a symmetrical element (\SeeChapter{see section Set Theory page \pageref{symmetrical element}}) with one of the two fundamental operators and we cannot do this directly with the two previous operators previously defined. That is why the real operators of Boolean Algebra are normally the $\wedge$ (AND) and the $\Delta$ (symmetric difference), denoted in Boolean Algebra by the symbol $\oplus$, the latter one being given by the logic operation:
	
	but to simplify, in the early school grades, it is common that we do implicitly reference to it without going into details.
	
	It follows that the binary set $(\mathcal{B},0,1)$ is therefore relatively to the laws $\vee,\wedge$ an "Abelian group". (\SeeChapter{see section Set Theory page \pageref{abelian group}}) Therefore, $(\mathcal{B},\vee)$ being an Abelian group, the law  $\wedge$  being associative and distributive with respect to $\vee$, $(\mathcal{B},\vee,\wedge)$ is therefore a "commutative ring with unit" (\SeeChapter{see section Set Theory page \pageref{communtative ring}}) since $\mathcal{B}$ has a neutral element relatively to the law $\vee$.
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} Thus, the operations $\vee,\wedge$ admits each a neutral element such that the value $1$ is the neutral element of $\wedge$ and $0$ the neutral element of $\vee$.\\
	
	\textbf{R2.} The two operations that we usually use to form a Boolean Algebra are the "inclusive OR" rigorously denoted $\vee$ but more frequently denoted by the addition sign "$+$" and the "inclusive AND" rigorously denoted by $\wedge$ but more frequently denotey by the multiplication sign "$\cdot$".
	\end{tcolorbox}
	
	The preceding axioms may, however been proved from the "\NewTerm{axioms of the definition}\index{axioms of the definition}":
	\begin{enumerate}
		\item[A1.] Negation: $\top\neg\bot$ and $\bot\neg \top$
		
		\item[A2.] Double complentation: $\neg\neg a=\bar{\bar{a}}=a$
		
		\item[A3.] Neutral element 1: $\top$ is the neutral element of $\wedge$
		\item[A4.] Neutral element 2: $\perp$ is the neutral element of $\vee$
		
		\item[A5.] De Morgan theorem: $\neg (a \vee b)=\neg a \wedge \neg b$ and $\neg (a\wedge b)=\neg a \vee \neg b$
	\end{enumerate}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	De Morgan's theorem can be proved using a simple truth table or algebraically as we will see just a little further below.
	\end{tcolorbox}
	It thus follows the following dual expressions:
	\begin{gather*}
	\begin{rcases*}
	0\cdot 0=0\\
	1+1=1
	\end{rcases*} \text{dual expression}
	\end{gather*}
	
	\begin{gather*}
	\begin{rcases*}
	1\cdot 0=0\\
	0+1=1
	\end{rcases*} \text{dual expression}
	\end{gather*}
	\begin{gather*}
	\begin{rcases*}
	\bar{0}=1\\
	\bar{1}=0
	\end{rcases*} \text{dual expression}
	\end{gather*}
	We name these expressions "\NewTerm{dual expressions}\index{dual expressions}" because by replacing in one equation logic equation, the $0$ by the $1$, the $\cdot$ by $+$ and inversely, by this same equation remains verified.
	
	\begin{theorem}
	Let us see now what we name the "\NewTerm{constants theorem}\index{constants theorem}" that consists to prove that:
	
	\end{theorem}
	\begin{dem}
	The proof is trivial (if necessary the reader can quickly do a truth table) as it comes from the same property of the concept of "Boolean ring" and the identity element $1$ relative to the $\wedge$ and its neutral element $0$ relative to $\vee$.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{theorem}
	We have:
	
	\end{theorem}
	\begin{dem}
	The distributivity brings us to write:
	
	and applying the complementation:
	
	applying commutativity:
	
	and finally by applying the theorem of constants:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	This proof will allow us to prove  the famous "\NewTerm{consensus theorem}\index{consensus theorem}":
	\begin{theorem}
	In Boolean Algebra, the consensus theorem or rule of consensus is the identity:
	
	\end{theorem}
	\begin{dem}
	To verify the consensus theorem relative to the logical product:
	
	we can make use of a Venn diagram where we can see trivially that the therm term $ab$ is contained in the other two:
	\begin{figure}[H]
		\centering
		\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        
		
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_c7ath6uy9}{
		\pgfdeclarepatternformonly[\mcThickness,\mcSize]{_c7ath6uy9}
		{\pgfqpoint{-\mcThickness}{-\mcThickness}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathmoveto{\pgfpointorigin}
		\pgfpathlineto{\pgfpoint{\mcSize}{0}}
		\pgfpathmoveto{\pgfpointorigin}
		\pgfpathlineto{\pgfpoint{0}{\mcSize}}
		\pgfusepath{stroke}}}
		\makeatother
		
		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_68b9c2dac}{
		\pgfdeclarepatternformonly[\mcThickness,\mcSize]{_68b9c2dac}
		{\pgfqpoint{-\mcThickness}{-\mcThickness}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathmoveto{\pgfpointorigin}
		\pgfpathlineto{\pgfpoint{\mcSize}{0}}
		\pgfpathmoveto{\pgfpointorigin}
		\pgfpathlineto{\pgfpoint{0}{\mcSize}}
		\pgfusepath{stroke}}}
		%uncomment if require: \path (0,428); %set diagram left start at 0, and has height of 428
		
		%Shape: Rectangle [id:dp4097232830848947] 
		\draw   (153,38) -- (480.3,38) -- (480.3,299) -- (153,299) -- cycle ;
		%Shape: Circle [id:dp9187948750891701] 
		\draw   (432.44,206.96) .. controls (432.92,244.24) and (403.1,274.84) .. (365.82,275.33) .. controls (328.54,275.81) and (297.93,245.98) .. (297.45,208.7) .. controls (296.97,171.43) and (326.8,140.82) .. (364.08,140.34) .. controls (401.35,139.86) and (431.96,169.68) .. (432.44,206.96) -- cycle ;
		%Shape: Circle [id:dp4738228698112583] 
		\draw   (329.45,208.29) .. controls (329.93,245.57) and (300.1,276.18) .. (262.83,276.66) .. controls (225.55,277.14) and (194.94,247.31) .. (194.46,210.03) .. controls (193.98,172.76) and (223.81,142.15) .. (261.08,141.67) .. controls (298.36,141.19) and (328.97,171.01) .. (329.45,208.29) -- cycle ;
		%Shape: Circle [id:dp7126399066386437] 
		\draw   (380.54,137.62) .. controls (381.02,174.9) and (351.2,205.51) .. (313.92,205.99) .. controls (276.64,206.47) and (246.04,176.64) .. (245.55,139.37) .. controls (245.07,102.09) and (274.9,71.48) .. (312.18,71) .. controls (349.45,70.52) and (380.06,100.35) .. (380.54,137.62) -- cycle ;
		%Shape: Path Data [id:dp21508128097135315] 
		\draw  [pattern=_c7ath6uy9,pattern size=6pt,pattern thickness=0.75pt,pattern radius=0pt, pattern color={rgb, 255:red, 0; green, 0; blue, 0}] (312.18,71) .. controls (349.45,70.52) and (380.06,100.35) .. (380.54,137.62) .. controls (380.56,139.13) and (380.53,140.63) .. (380.45,142.12) .. controls (375.2,140.89) and (369.71,140.26) .. (364.08,140.34) .. controls (328.31,140.8) and (299.4,169) .. (297.54,204.21) .. controls (268.07,197.28) and (245.96,171.01) .. (245.55,139.37) .. controls (245.07,102.09) and (274.9,71.48) .. (312.18,71) -- cycle ;
		%Shape: Path Data [id:dp5437672790798094] 
		\draw  [pattern=_68b9c2dac,pattern size=6pt,pattern thickness=0.75pt,pattern radius=0pt, pattern color={rgb, 255:red, 0; green, 0; blue, 0}] (297.45,208.7) .. controls (297.24,192.07) and (303.06,176.76) .. (312.89,164.86) .. controls (323.02,176.5) and (329.24,191.65) .. (329.45,208.29) .. controls (329.67,224.93) and (323.84,240.24) .. (314.02,252.13) .. controls (303.88,240.49) and (297.67,225.34) .. (297.45,208.7) -- cycle ;
		
		% Text Node
		\draw (308,50.4) node [anchor=north west][inner sep=0.75pt]    {$b$};
		% Text Node
		\draw (173,236.4) node [anchor=north west][inner sep=0.75pt]    {$a$};
		% Text Node
		\draw (442,236.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Venn diagram of the consensus theorem}
	\end{figure}
	Or more formally with other notations:
	
	Proceeding the request of a reader we can also build a truth table:
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
					\hline
					\multicolumn{1}{c}{\cellcolor[gray]{0.75}$\pmb{x}$} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}$\pmb{y}$}  & \multicolumn{1}{c}{\cellcolor[gray]{0.75}$\pmb{z}$} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}$\pmb{xy \vee \bar{x}z \vee yz}$} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}$\pmb{xy \vee \bar{x}z}$} \\ \hline
					\centering\arraybackslash\ $0$ & \centering\arraybackslash\ $0$ & \centering\arraybackslash\ $0$ & \centering\arraybackslash\ $0$ & \centering\arraybackslash\ $0$ \\ \hline
					\centering\arraybackslash\ $0$ & \centering\arraybackslash\ $0$ & \centering\arraybackslash\ $1$ & \centering\arraybackslash\ $1$ & \centering\arraybackslash\ $0$ \\ \hline
					\centering\arraybackslash\ $0$ & \centering\arraybackslash\ $1$ & \centering\arraybackslash\ $0$ & \centering\arraybackslash\ $0$ & \centering\arraybackslash\ $0$ \\ \hline
					\centering\arraybackslash\ $0$ & \centering\arraybackslash\ $1$ & \centering\arraybackslash\ $1$ & \centering\arraybackslash\ $1$ & \centering\arraybackslash\ $0$ \\ \hline
					\centering\arraybackslash\ $1$ & \centering\arraybackslash\ $0$ & \centering\arraybackslash\ $0$ & \centering\arraybackslash\ $0$ & \centering\arraybackslash\ $0$ \\ \hline
					\centering\arraybackslash\ $1$ & \centering\arraybackslash\ $0$ & \centering\arraybackslash\ $1$ & \centering\arraybackslash\ $0$ & \centering\arraybackslash\ $0$ \\ \hline
					\centering\arraybackslash\ $1$ & \centering\arraybackslash\ $1$ & \centering\arraybackslash\ $0$ & \centering\arraybackslash\ $1$ & \centering\arraybackslash\ $0$ \\ \hline
					\centering\arraybackslash\ $1$ & \centering\arraybackslash\ $1$ & \centering\arraybackslash\ $1$ & \centering\arraybackslash\ $1$ & \centering\arraybackslash\ $0$  \\ \hline
			\end{tabular}
		\end{center}
		\caption{Consensus theorem truth table}
		\end{table}
		
	Proceeding also with a Venn diagram, the reader will see without problem that we also have:
	
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{theorem}
	And finally the very famous "\NewTerm{Shannon's theorem}\index{Shannon's theorem}" (not to be confused with the Shannon theorem in signal theory!):
	
	\end{theorem}
	\begin{dem}
	We begin with the first relation:
	
	and for the second relation:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\label{de morgan theorem}
	\begin{theorem}
	Now let us come back on the De Morgan theorems previously presented as axioms:
	
	These two relations therefore express that the inverse (or opposite) of  product (or respectively the sum) of two variables is equal to the sum (respectively the product) of the inverse of these same variables.
	\end{theorem}
	\begin{dem}
	Suppose $\overline{(a+b)}=\bar{a}\bar{b}$ is true. So under the relations $\bar{a}+a=1$ and $\bar{a}a=1$ (axiom of complementation) we must have:
	
	So we need to prove that these relations are true:
	
	and:
	
	The second De Morgan theorem can be proven in the same way (we can put the details on request).
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	These two theorems can be extended to as many variables as we want.
	\end{tcolorbox}
	\begin{corollary}
	As immediate corollary we have:
	\begin{itemize}
		\item $ab=\overline{\bar{a}+\bar{b}}$
		\item $a+b=\overline{\bar{a}\bar{b}}$
		\item $ab=\overline{\overline{ab}}$
		\item $a+b=\overline{\overline{a+b}}$
		\item $\overline{a+b+c+\ldots }=\bar{a}\bar{b}\bar{c}\ldots $
		\item $\overline{abc\ldots }=\bar{a}+\bar{b}+\bar{c}+\ldots $
	\end{itemize}
	\end{corollary}
	The logical expressions, as we have seen it until now thanks to the previous axioms, properties and theorems, can always be writtent into two different forms (by paling also with the negations $\neg$):
	\begin{enumerate}
		\item Under the form of a sum of logical products, also named "\NewTerm{normal disjcontive form NDF}\index{normal disjcontive form}", such as for example:
		
		The constitutive terms of this polynomial are in this example the monomials $\bar{a}\bar{c}d,\bar{a}c\bar{d}$. The variables or complementary variables of the monomials are the "letters": $\bar{a},\bar{c},d,c\bar{d}$.
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		If each (all) of the product contains all the input variables in a direct or complementary form, then the form is named "\NewTerm{first canonical form}\index{first canonical form}" or "\NewTerm{disjonctive canonical form}\index{disjonctive canonical form}". Each of the products is therefore named the "\NewTerm{minterm}\index{minterm}" (therefore in the previous relation, there are $2$ minterms).\\
		
		Obviously if we consider that each term and its negation is equivalent to $0$ and $1$ if we have one variable, the first canonical form has two terms  ($2^1$), if we have two variables, the first canonical form has four terms ($2^n$) and if $n$ variables we have $2^n$ terms.
		\end{tcolorbox}
		
		\item In the form of a product of logical sum, also named "\NewTerm{normal conjunctive form NCF}\index{normal conjunctive form}":
		
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		If each (all) of the sums contains all the input variables in a direct or complementary form, the form is named "\NewTerm{second canonical form}\index{second canonical form}" or "\NewTerm{conjunctive canonical form}\index{conjunctive canonical form}". Each of the sum is therefore named the "\NewTerm{maxterm}\index{maxterm}".\\
		
		Obviously if we consider that each term and it negation is equivalent to $0$ and $1$ if we have one variable, the second canonical form has two terms  ($2^1$), if we have two variables, the second canonical form has four terms ($2^n$) and if $n$ variables we have $2^n$ terms.
		\end{tcolorbox}
		Therefore, in other words, a normal disjonctive form is either a litteral and its complementary (one letter) or a disjonction of formulas written as conjonction of litterals. A conjonctive form is either a litteral (one letter) and its complementary, or a conjonction of formulas written as disjonction of litterals.
	\end{enumerate}
	The simplification methods we will see later aim to minimize the number of letters of the expressions so as to reduce the number of inputs of our logic system logic and therefore also the number of its components.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The algebraic simplification of an expression is to transform it so as to minimize the number of letters by applying theorems proven previously.
	\end{tcolorbox}
	To simplify expressions (or identify them) a known technique is therefore to use the "\NewTerm{Karnaugh tables}\index{Karnaugh tables}" that we will see further below in details.
	
	\subsubsection{Logical Functions (Boolean operators)}\label{boolean operators}
	So when we talk about Boolean Algebra unless other indication, we refer to the three basic Boolean operations (AND, OR, NOT) and some other logic functions arising from them for which we have the following symbols as defined in circuit theory (MIL norm representation if no error...):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1215); %set diagram left start at 0, and has height of 1215
		
		%Shape: And Gate [id:dp11317325758382268] 
		\draw   (441.2,59) -- (465.2,59) .. controls (478.45,59) and (489.2,72.44) .. (489.2,89) .. controls (489.2,105.56) and (478.45,119) .. (465.2,119) -- (441.2,119) -- (441.2,59) -- cycle (425.2,69) -- (441.2,69) (425.2,109) -- (441.2,109) (489.2,89) -- (505.2,89) ;
		%Shape: Or Gate [id:dp6510184071720768] 
		\draw   (164.8,59) -- (184.8,59) .. controls (198.75,59.54) and (211.22,71.23) .. (216.8,89) .. controls (211.22,106.77) and (198.75,118.46) .. (184.8,119) -- (164.8,119) .. controls (173.37,100.44) and (173.37,77.56) .. (164.8,59) -- cycle (152.8,69) -- (168.8,69) (152.8,109) -- (168.8,109) (216.8,89) -- (232.8,89) ;
		%Shape: Nand Gate [id:dp4258517608491956] 
		\draw   (258.41,59) -- (280.64,59) .. controls (292.9,59) and (302.86,72.44) .. (302.86,89) .. controls (302.86,105.56) and (292.9,119) .. (280.64,119) -- (258.41,119) -- (258.41,59) -- cycle (243.6,69) -- (258.41,69) (243.6,109) -- (258.41,109) (311.75,89) -- (323.6,89) (302.86,89) .. controls (302.86,85.69) and (304.85,83) .. (307.3,83) .. controls (309.76,83) and (311.75,85.69) .. (311.75,89) .. controls (311.75,92.31) and (309.76,95) .. (307.3,95) .. controls (304.85,95) and (302.86,92.31) .. (302.86,89) -- cycle ;
		%Shape: Nor Gate [id:dp7906604291113766] 
		\draw   (345.51,59) -- (364.03,59) .. controls (376.95,59.54) and (388.49,71.23) .. (393.66,89) .. controls (388.49,106.77) and (376.95,118.46) .. (364.03,119) -- (345.51,119) .. controls (353.45,100.44) and (353.45,77.56) .. (345.51,59) -- cycle (334.4,69) -- (349.21,69) (334.4,109) -- (349.21,109) (402.55,89) -- (414.4,89) (393.66,89) .. controls (393.66,85.69) and (395.65,83) .. (398.1,83) .. controls (400.56,83) and (402.55,85.69) .. (402.55,89) .. controls (402.55,92.31) and (400.56,95) .. (398.1,95) .. controls (395.65,95) and (393.66,92.31) .. (393.66,89) -- cycle ;
		%Shape: Xor Gate [id:dp5587524817634881] 
		\draw   (74,59) -- (94,59) .. controls (107.95,59.54) and (120.42,71.23) .. (126,89) .. controls (120.42,106.77) and (107.95,118.46) .. (94,119) -- (74,119) .. controls (82.57,100.44) and (82.57,77.56) .. (74,59) -- cycle (62,69) -- (78,69) (62,109) -- (78,109) (126,89) -- (142,89) (70,59) .. controls (78.57,77.56) and (78.57,100.44) .. (70,119) ;
		%Shape: Not/Inverter Gate [id:dp8793540819513128] 
		\draw   (530.81,59) -- (575.26,89) -- (530.81,119) -- (530.81,59) -- cycle (516,89) -- (530.81,89) (584.15,89) -- (596,89) (575.26,89) .. controls (575.26,85.69) and (577.25,83) .. (579.7,83) .. controls (582.16,83) and (584.15,85.69) .. (584.15,89) .. controls (584.15,92.31) and (582.16,95) .. (579.7,95) .. controls (577.25,95) and (575.26,92.31) .. (575.26,89) -- cycle ;
		
		% Text Node
		\draw (78,139.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {AND};
		% Text Node
		\draw (168.8,139.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {OR};
		% Text Node
		\draw (250.6,139.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {NAND};
		% Text Node
		\draw (352.4,139.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {NOR};
		% Text Node
		\draw (444.2,139.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {XOR};
		% Text Node
		\draw (536,139.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {NOR};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Logic MIL Gates}
	\end{figure}
	and their respective "\NewTerm{truth tables}\index{truth tables}":
	\begin{table}[H]
	\newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}}
	\begin{center}
		\begin{tabular}{|c|C{2cm}|C{2.5cm}|}
			\hline
			\multicolumn{3}{|c|}{\cellcolor[gray]{0.75}\textbf{AND ($\wedge $) Truth Table}} \\
			\hline
			\cellcolor[gray]{0.75}\textbf{AND} & \cellcolor[gray]{0.75}\textbf{0} & \cellcolor[gray]{0.75}\textbf{1}\\
			\hline
			\cellcolor[gray]{0.75}\textbf{0} & $0$ & $0$  \\
			\hline
			\cellcolor[gray]{0.75}\textbf{1} & $0$ & $1$ \\
			\hline
		\end{tabular}
		\caption{AND ($\wedge$) Truth table}
	\end{center}
	\end{table}
	\begin{table}[H]
	\newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}}
	\begin{center}
		\begin{tabular}{|c|C{2cm}|C{2cm}|}
			\hline
			\multicolumn{3}{|c|}{\cellcolor[gray]{0.75}\textbf{OR ($\vee $) Truth Table}} \\
			\hline
			\cellcolor[gray]{0.75}\textbf{OR} & \cellcolor[gray]{0.75}\textbf{0} & \cellcolor[gray]{0.75}\textbf{1}\\
			\hline
			\cellcolor[gray]{0.75}\textbf{0} & $0$ & $1$  \\
			\hline
			\cellcolor[gray]{0.75}\textbf{1} & $1$ & $1$ \\
			\hline
		\end{tabular}
		\caption{OR ($\vee$) truth table}
	\end{center}
	\end{table}
	\begin{table}[H]
	\newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}}
	\begin{center}
		\begin{tabular}{|c|C{2cm}|}
			\hline
			\multicolumn{2}{|c|}{\cellcolor[gray]{0.75}\textbf{NOT ($\neg $) Truth Table}} \\
			\hline
			\cellcolor[gray]{0.75}\textbf{NOT} & \cellcolor[gray]{0.75}\textbf{-} \\
			\hline
			\cellcolor[gray]{0.75}\textbf{0} & $0$   \\
			\hline
			\cellcolor[gray]{0.75}\textbf{1} & $1$ \\
			\hline
		\end{tabular}
		\caption{NOT ($\neg$) truth table}
	\end{center}
	\end{table}
	All others known (common) "logic functions" can be composed of these two basic operators. Such that by definition (given with their standard definition in the first line and with their different algebraic forms under their respective truth table):
	\begin{center}
	$\neg (a \wedge b)=\overline{a\cdot b}=\bar{a}+\bar{b}$
	\end{center}
	\begin{table}[H]
	\newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}}
	\begin{center}
		\begin{tabular}{|c|C{2.3cm}|C{2.3cm}|}
			\hline
			\multicolumn{3}{|c|}{\cellcolor[gray]{0.75}\textbf{NOT-AND ($\mathrm{NAND}$): $\mathrm{NOT}$ ($a$ $\mathrm{AND}$ $b$)}} \\
			\hline
			\cellcolor[gray]{0.75}$\pmb{\mathrm{NAND}}$ & \cellcolor[gray]{0.75}\textbf{0} & \cellcolor[gray]{0.75}\textbf{1}\\
			\hline
			\cellcolor[gray]{0.75}\textbf{0} & $1$ & $0$  \\
			\hline
			\cellcolor[gray]{0.75}\textbf{1} & $1$ & $0$ \\
			\hline
		\end{tabular}
		\caption{NOT-AND truth table}
	\end{center}
	\end{table}
	\begin{center}
		$\neg (a \vee b)=\overline{a + b}=\bar{a}\cdot\bar{b}$
	\end{center}
	\begin{table}[H]
	\newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}}
	\begin{center}
		\begin{tabular}{|c|C{2cm}|C{2cm}|}
			\hline
			\multicolumn{3}{|c|}{\cellcolor[gray]{0.75}\textbf{NOT-OR ($\mathrm{NOR}$): $\mathrm{NOT}$ ($a$ $\mathrm{OR}$ $b$)}} \\
			\hline
			\cellcolor[gray]{0.75}$\pmb{\mathrm{NAND}}$ & \cellcolor[gray]{0.75}\textbf{0} & \cellcolor[gray]{0.75}\textbf{1}\\
			\hline
			\cellcolor[gray]{0.75}\textbf{0} & $1$ & $0$  \\
			\hline
			\cellcolor[gray]{0.75}\textbf{1} & $0$ & $0$ \\
			\hline
		\end{tabular}
		\caption{NOT-OR truth table}
	\end{center}
	\end{table}
	\begin{center}
		$a\oplus b=(a \vee b)\wedge \neg(a \wedge b)=(a+b)\cdot \overline{(a\cdot b)}$\\
		$a\oplus b=a\wedge \neg b+b\wedge \neg q=a\cdot\bar{b}+b\cdot \bar{a}$\\
		$a\oplus b=\neg(a\wedge b+\neg a\wedge\neg b)=\overline{a\cdot b+\bar{a}\cdot\bar{b}}$
	\end{center}
	\begin{table}[H]
	\newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}}
	\begin{center}
		\begin{tabular}{|c|C{4.1cm}|C{4.1cm}|}
			\hline
			\multicolumn{3}{|c|}{\cellcolor[gray]{0.75}\textbf{EXCLUSIVE OR ($\mathrm{XOR}$): $[a\;\mathrm{OR}\;b]\;\mathrm{AND}\;[\mathrm{NOT}\;(a\;\mathrm{AND}\;\mathrm{b}]$}} \\
			\hline
			\cellcolor[gray]{0.75}$\pmb{\mathrm{XOR}}$ & \cellcolor[gray]{0.75}\textbf{0} & \cellcolor[gray]{0.75}\textbf{1}\\
			\hline
			\cellcolor[gray]{0.75}\textbf{0} & $0$ & $1$  \\
			\hline
			\cellcolor[gray]{0.75}\textbf{1} & $1$ & $0$ \\
			\hline
		\end{tabular}
		\caption{EXCLUSIVE OR $\oplus$ Truth table}
	\end{center}
	\end{table}
	where $a$ and $b$ are, as you will have understood, variables (or "bit" of Binary Digit) that can arbitrarily take the binary values $0$ or $1$.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The $\mathrm{XOR}$ logic function is often denoted in the literature by the operator $\oplus$ and we will consider as obvious that the $\mathrm{XOR}$ is also a group law and thus allows to construct an abelian commutative group. This property of the $\mathrm{XOR}$ is particularly used in cryptography.
	\end{tcolorbox}
	All the tables above can be sum up in the following figure with the corresponding MIL circuit theory symbols:
		\begin{figure}[H]
		\centering
		\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        
		
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,810); %set diagram left start at 0, and has height of 810
		
		%Straight Lines [id:da9694093092172729] 
		\draw    (562,156) -- (535.5,156) ;
		%Shape: Xor Gate [id:dp36464511188530224] 
		\draw   (558,146) -- (578,146) .. controls (591.95,146.54) and (604.42,158.23) .. (610,176) .. controls (604.42,193.77) and (591.95,205.46) .. (578,206) -- (558,206) .. controls (566.57,187.44) and (566.57,164.56) .. (558,146) -- cycle (546,156) -- (562,156) (546,196) -- (562,196) (610,176) -- (626,176) (554,146) .. controls (562.57,164.56) and (562.57,187.44) .. (554,206) ;
		%Shape: Or Gate [id:dp6749370528636052] 
		\draw   (86,260) -- (106,260) .. controls (119.95,260.54) and (132.42,272.23) .. (138,290) .. controls (132.42,307.77) and (119.95,319.46) .. (106,320) -- (86,320) .. controls (94.57,301.44) and (94.57,278.56) .. (86,260) -- cycle (74,270) -- (90,270) (74,310) -- (90,310) (138,290) -- (154,290) ;
		%Straight Lines [id:da06705019891121244] 
		\draw    (88,29) -- (88,98) ;
		%Straight Lines [id:da405353094973514] 
		\draw    (67.5,49) -- (145.5,49) ;
		%Straight Lines [id:da4363628977088525] 
		\draw    (116.5,29) -- (116.5,99) ;
		%Straight Lines [id:da10168052332241517] 
		\draw    (67.5,74) -- (145.5,74) ;
		%Straight Lines [id:da20666676767899195] 
		\draw    (145.5,29) -- (145.5,99) ;
		%Shape: And Gate [id:dp568942840741306] 
		\draw   (90,146) -- (114,146) .. controls (127.25,146) and (138,159.44) .. (138,176) .. controls (138,192.56) and (127.25,206) .. (114,206) -- (90,206) -- (90,146) -- cycle (74,156) -- (90,156) (74,196) -- (90,196) (138,176) -- (154,176) ;
		%Shape: Circle [id:dp1359396502974699] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (80,270) .. controls (80,267.24) and (82.24,265) .. (85,265) .. controls (87.76,265) and (90,267.24) .. (90,270) .. controls (90,272.76) and (87.76,275) .. (85,275) .. controls (82.24,275) and (80,272.76) .. (80,270) -- cycle ;
		%Shape: Circle [id:dp2075239784687557] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (80,310) .. controls (80,307.24) and (82.24,305) .. (85,305) .. controls (87.76,305) and (90,307.24) .. (90,310) .. controls (90,312.76) and (87.76,315) .. (85,315) .. controls (82.24,315) and (80,312.76) .. (80,310) -- cycle ;
		%Shape: Circle [id:dp8456403366851848] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (138,290) .. controls (138,287.24) and (140.24,285) .. (143,285) .. controls (145.76,285) and (148,287.24) .. (148,290) .. controls (148,292.76) and (145.76,295) .. (143,295) .. controls (140.24,295) and (138,292.76) .. (138,290) -- cycle ;
		%Shape: Rectangle [id:dp897947908433238] 
		\draw   (58,378) -- (167.5,378) -- (167.5,450) -- (58,450) -- cycle ;
		%Shape: Circle [id:dp2253185833380793] 
		\draw   (70,412) .. controls (70,398.19) and (81.19,387) .. (95,387) .. controls (108.81,387) and (120,398.19) .. (120,412) .. controls (120,425.81) and (108.81,437) .. (95,437) .. controls (81.19,437) and (70,425.81) .. (70,412) -- cycle ;
		%Shape: Circle [id:dp853081545922117] 
		\draw   (107,412) .. controls (107,398.19) and (118.19,387) .. (132,387) .. controls (145.81,387) and (157,398.19) .. (157,412) .. controls (157,425.81) and (145.81,437) .. (132,437) .. controls (118.19,437) and (107,425.81) .. (107,412) -- cycle ;
		%Shape: Path Data [id:dp3227108508952272] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (120,412) .. controls (120,418.48) and (117.54,424.38) .. (113.5,428.82) .. controls (109.46,424.38) and (107,418.48) .. (107,412) .. controls (107,405.53) and (109.46,399.62) .. (113.5,395.18) .. controls (117.54,399.62) and (120,405.53) .. (120,412) -- cycle ;
		%Straight Lines [id:da3813551045876318] 
		\draw    (248,29) -- (248,98) ;
		%Straight Lines [id:da09788116754946552] 
		\draw    (227.5,49) -- (305.5,49) ;
		%Straight Lines [id:da16674427036421569] 
		\draw    (276.5,29) -- (276.5,99) ;
		%Straight Lines [id:da6241659728842943] 
		\draw    (227.5,74) -- (305.5,74) ;
		%Straight Lines [id:da43267367577481397] 
		\draw    (305.5,29) -- (305.5,99) ;
		%Shape: Or Gate [id:dp8161165684266278] 
		\draw   (247,146) -- (267,146) .. controls (280.95,146.54) and (293.42,158.23) .. (299,176) .. controls (293.42,193.77) and (280.95,205.46) .. (267,206) -- (247,206) .. controls (255.57,187.44) and (255.57,164.56) .. (247,146) -- cycle (235,156) -- (251,156) (235,196) -- (251,196) (299,176) -- (315,176) ;
		%Shape: And Gate [id:dp4558035766496302] 
		\draw   (246,260) -- (270,260) .. controls (283.25,260) and (294,273.44) .. (294,290) .. controls (294,306.56) and (283.25,320) .. (270,320) -- (246,320) -- (246,260) -- cycle (230,270) -- (246,270) (230,310) -- (246,310) (294,290) -- (310,290) ;
		%Shape: Circle [id:dp21882193033153285] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (236,270) .. controls (236,267.24) and (238.24,265) .. (241,265) .. controls (243.76,265) and (246,267.24) .. (246,270) .. controls (246,272.76) and (243.76,275) .. (241,275) .. controls (238.24,275) and (236,272.76) .. (236,270) -- cycle ;
		%Shape: Circle [id:dp41057266559859196] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (236,310) .. controls (236,307.24) and (238.24,305) .. (241,305) .. controls (243.76,305) and (246,307.24) .. (246,310) .. controls (246,312.76) and (243.76,315) .. (241,315) .. controls (238.24,315) and (236,312.76) .. (236,310) -- cycle ;
		%Shape: Circle [id:dp7553117947023309] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (294,290) .. controls (294,287.24) and (296.24,285) .. (299,285) .. controls (301.76,285) and (304,287.24) .. (304,290) .. controls (304,292.76) and (301.76,295) .. (299,295) .. controls (296.24,295) and (294,292.76) .. (294,290) -- cycle ;
		%Shape: Rectangle [id:dp01361226336010879] 
		\draw   (218,378) -- (327.5,378) -- (327.5,450) -- (218,450) -- cycle ;
		%Shape: Circle [id:dp014883114999103553] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (230,412) .. controls (230,398.19) and (241.19,387) .. (255,387) .. controls (268.81,387) and (280,398.19) .. (280,412) .. controls (280,425.81) and (268.81,437) .. (255,437) .. controls (241.19,437) and (230,425.81) .. (230,412) -- cycle ;
		%Shape: Circle [id:dp07323230153301541] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (267,412) .. controls (267,398.19) and (278.19,387) .. (292,387) .. controls (305.81,387) and (317,398.19) .. (317,412) .. controls (317,425.81) and (305.81,437) .. (292,437) .. controls (278.19,437) and (267,425.81) .. (267,412) -- cycle ;
		%Shape: Circle [id:dp6611920103110038] 
		\draw   (230,412) .. controls (230,398.19) and (241.19,387) .. (255,387) .. controls (268.81,387) and (280,398.19) .. (280,412) .. controls (280,425.81) and (268.81,437) .. (255,437) .. controls (241.19,437) and (230,425.81) .. (230,412) -- cycle ;
		%Straight Lines [id:da6503657366631221] 
		\draw    (397,29) -- (397,98) ;
		%Straight Lines [id:da6795248741592765] 
		\draw    (376.5,49) -- (454.5,49) ;
		%Straight Lines [id:da1467883801540233] 
		\draw    (425.5,29) -- (425.5,99) ;
		%Straight Lines [id:da7782180678908859] 
		\draw    (376.5,74) -- (454.5,74) ;
		%Straight Lines [id:da6280862402064935] 
		\draw    (454.5,29) -- (454.5,99) ;
		%Shape: Or Gate [id:dp7562151639171932] 
		\draw   (397,146) -- (417,146) .. controls (430.95,146.54) and (443.42,158.23) .. (449,176) .. controls (443.42,193.77) and (430.95,205.46) .. (417,206) -- (397,206) .. controls (405.57,187.44) and (405.57,164.56) .. (397,146) -- cycle (385,156) -- (401,156) (385,196) -- (401,196) (449,176) -- (465,176) ;
		%Shape: Circle [id:dp822976102396441] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (391,156) .. controls (391,153.24) and (393.24,151) .. (396,151) .. controls (398.76,151) and (401,153.24) .. (401,156) .. controls (401,158.76) and (398.76,161) .. (396,161) .. controls (393.24,161) and (391,158.76) .. (391,156) -- cycle ;
		%Shape: And Gate [id:dp1456103313530981] 
		\draw   (398,260) -- (422,260) .. controls (435.25,260) and (446,273.44) .. (446,290) .. controls (446,306.56) and (435.25,320) .. (422,320) -- (398,320) -- (398,260) -- cycle (382,270) -- (398,270) (382,310) -- (398,310) (446,290) -- (462,290) ;
		%Shape: Circle [id:dp7449857815049405] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (388,310) .. controls (388,307.24) and (390.24,305) .. (393,305) .. controls (395.76,305) and (398,307.24) .. (398,310) .. controls (398,312.76) and (395.76,315) .. (393,315) .. controls (390.24,315) and (388,312.76) .. (388,310) -- cycle ;
		%Shape: Circle [id:dp20804306853558674] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (446,290) .. controls (446,287.24) and (448.24,285) .. (451,285) .. controls (453.76,285) and (456,287.24) .. (456,290) .. controls (456,292.76) and (453.76,295) .. (451,295) .. controls (448.24,295) and (446,292.76) .. (446,290) -- cycle ;
		%Shape: Rectangle [id:dp47748637726045606] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (372,377) -- (481.5,377) -- (481.5,449) -- (372,449) -- cycle ;
		%Shape: Circle [id:dp22907748306169706] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (384,411) .. controls (384,397.19) and (395.19,386) .. (409,386) .. controls (422.81,386) and (434,397.19) .. (434,411) .. controls (434,424.81) and (422.81,436) .. (409,436) .. controls (395.19,436) and (384,424.81) .. (384,411) -- cycle ;
		%Shape: Circle [id:dp8090738600148739] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (421,411) .. controls (421,397.19) and (432.19,386) .. (446,386) .. controls (459.81,386) and (471,397.19) .. (471,411) .. controls (471,424.81) and (459.81,436) .. (446,436) .. controls (432.19,436) and (421,424.81) .. (421,411) -- cycle ;
		%Shape: Circle [id:dp11127802879364834] 
		\draw   (384,411) .. controls (384,397.19) and (395.19,386) .. (409,386) .. controls (422.81,386) and (434,397.19) .. (434,411) .. controls (434,424.81) and (422.81,436) .. (409,436) .. controls (395.19,436) and (384,424.81) .. (384,411) -- cycle ;
		%Straight Lines [id:da11324979527998424] 
		\draw    (552,29) -- (552,98) ;
		%Straight Lines [id:da03136651260972578] 
		\draw    (531.5,49) -- (609.5,49) ;
		%Straight Lines [id:da20441867442889872] 
		\draw    (580.5,29) -- (580.5,99) ;
		%Straight Lines [id:da4575616463293257] 
		\draw    (531.5,74) -- (609.5,74) ;
		%Straight Lines [id:da6635900626845599] 
		\draw    (609.5,29) -- (609.5,99) ;
		%Shape: Circle [id:dp28563532674536196] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (546,156) .. controls (546,153.24) and (548.24,151) .. (551,151) .. controls (553.76,151) and (556,153.24) .. (556,156) .. controls (556,158.76) and (553.76,161) .. (551,161) .. controls (548.24,161) and (546,158.76) .. (546,156) -- cycle ;
		%Straight Lines [id:da3400115877929355] 
		\draw    (562,196) -- (535.5,196) ;
		%Straight Lines [id:da9014573669034576] 
		\draw    (562,269) -- (535.5,269) ;
		%Shape: Xor Gate [id:dp7312066873523095] 
		\draw   (558,259) -- (578,259) .. controls (591.95,259.54) and (604.42,271.23) .. (610,289) .. controls (604.42,306.77) and (591.95,318.46) .. (578,319) -- (558,319) .. controls (566.57,300.44) and (566.57,277.56) .. (558,259) -- cycle (546,269) -- (562,269) (546,309) -- (562,309) (610,289) -- (626,289) (554,259) .. controls (562.57,277.56) and (562.57,300.44) .. (554,319) ;
		%Shape: Circle [id:dp47240857275231285] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (546,269) .. controls (546,266.24) and (548.24,264) .. (551,264) .. controls (553.76,264) and (556,266.24) .. (556,269) .. controls (556,271.76) and (553.76,274) .. (551,274) .. controls (548.24,274) and (546,271.76) .. (546,269) -- cycle ;
		%Straight Lines [id:da22311973327605106] 
		\draw    (562,309) -- (535.5,309) ;
		%Shape: Circle [id:dp6487122585716323] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (546.75,309) .. controls (546.75,306.24) and (548.99,304) .. (551.75,304) .. controls (554.51,304) and (556.75,306.24) .. (556.75,309) .. controls (556.75,311.76) and (554.51,314) .. (551.75,314) .. controls (548.99,314) and (546.75,311.76) .. (546.75,309) -- cycle ;
		%Shape: Rectangle [id:dp1497829792176637] 
		\draw   (525,378) -- (634.5,378) -- (634.5,450) -- (525,450) -- cycle ;
		%Shape: Circle [id:dp7053266095234718] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (537,412) .. controls (537,398.19) and (548.19,387) .. (562,387) .. controls (575.81,387) and (587,398.19) .. (587,412) .. controls (587,425.81) and (575.81,437) .. (562,437) .. controls (548.19,437) and (537,425.81) .. (537,412) -- cycle ;
		%Shape: Circle [id:dp5495036981963817] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (574,412) .. controls (574,398.19) and (585.19,387) .. (599,387) .. controls (612.81,387) and (624,398.19) .. (624,412) .. controls (624,425.81) and (612.81,437) .. (599,437) .. controls (585.19,437) and (574,425.81) .. (574,412) -- cycle ;
		%Shape: Path Data [id:dp2475543056564995] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (587,412) .. controls (587,418.48) and (584.54,424.38) .. (580.5,428.82) .. controls (576.46,424.38) and (574,418.48) .. (574,412) .. controls (574,405.53) and (576.46,399.62) .. (580.5,395.18) .. controls (584.54,399.62) and (587,405.53) .. (587,412) -- cycle ;
		
		% Text Node
		\draw (97.5,25.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (125,25.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (72,52.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (71.5,78.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (97.5,52.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (125,52.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (97.5,78.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (125,78.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (65,26.4) node [anchor=north west][inner sep=0.75pt]    {$\land $};
		% Text Node
		\draw (47,68.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (112,9.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (58,150.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (58,190.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (141,158.4) node [anchor=north west][inner sep=0.75pt]    {$x\land y$};
		% Text Node
		\draw (58,264.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (58,304.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (141,272.4) node [anchor=north west][inner sep=0.75pt]    {$x\land y$};
		% Text Node
		\draw (289,111) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{Truth Tables}};
		% Text Node
		\draw (294.5,226) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{Logic gates}};
		% Text Node
		\draw (250,345) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{De Morgan Equivalents}};
		% Text Node
		\draw (60,433.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (155,433.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (257.5,25.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (285,25.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (232,52.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (231.5,78.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (257.5,52.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (285,52.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (257.5,78.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (285,78.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (225,26.4) node [anchor=north west][inner sep=0.75pt]    {$\lor $};
		% Text Node
		\draw (207,68.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (272,9.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (215,150.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (215,190.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (304,158.4) node [anchor=north west][inner sep=0.75pt]    {$x\lor y$};
		% Text Node
		\draw (214,264.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (214,304.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (297,272.4) node [anchor=north west][inner sep=0.75pt]    {$x\lor y$};
		% Text Node
		\draw (220,433.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (315,433.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (406.5,25.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (434,25.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (381,52.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (380.5,78.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (406.5,52.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (434,52.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (406.5,78.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (434,78.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (371,31.4) node [anchor=north west][inner sep=0.75pt]    {$\rightarrow $};
		% Text Node
		\draw (356,68.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (421,9.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (365,150.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (365,190.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (454,158.4) node [anchor=north west][inner sep=0.75pt]    {$x\rightarrow y$};
		% Text Node
		\draw (366,264.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (366,304.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (449,272.4) node [anchor=north west][inner sep=0.75pt]    {$x\rightarrow y$};
		% Text Node
		\draw (374,432.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (469,432.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (561.5,25.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (589,25.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (536,52.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (535.5,78.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (561.5,52.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (589,52.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (561.5,78.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (589,78.9) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (530,26.4) node [anchor=north west][inner sep=0.75pt]    {$\oplus $};
		% Text Node
		\draw (511,68.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (576,9.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (520,150.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (520,190.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (609,158.4) node [anchor=north west][inner sep=0.75pt]    {$x\oplus y$};
		% Text Node
		\draw (520,263.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (520,303.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (609,271.4) node [anchor=north west][inner sep=0.75pt]    {$x\oplus y$};
		% Text Node
		\draw (527,433.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (622,433.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Summary of basic Boole algebra}
	\end{figure}
	The reader can easily check by himself that with a NAND gate (or an AND gate and an INVERTER) you've got all the parts you need to build a modern computer (classical or quantum one!)\label{all gates from NAND}:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,970); %set diagram left start at 0, and has height of 970
		
		%Shape: Nand Gate [id:dp11694064958874728] 
		\draw   (251.43,9) -- (279.06,9) .. controls (294.32,9) and (306.7,22.44) .. (306.7,39) .. controls (306.7,55.56) and (294.32,69) .. (279.06,69) -- (251.43,69) -- (251.43,9) -- cycle (233,19) -- (251.43,19) (233,59) -- (251.43,59) (317.76,39) -- (332.5,39) (306.7,39) .. controls (306.7,35.69) and (309.18,33) .. (312.23,33) .. controls (315.28,33) and (317.76,35.69) .. (317.76,39) .. controls (317.76,42.31) and (315.28,45) .. (312.23,45) .. controls (309.18,45) and (306.7,42.31) .. (306.7,39) -- cycle ;
		%Straight Lines [id:da24548138741539582] 
		\draw    (233,19) -- (233,59) ;
		%Straight Lines [id:da8475110773330126] 
		\draw    (233,39) -- (206.5,39) ;
		\draw [shift={(233,39)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		%Shape: Nand Gate [id:dp42147886498426446] 
		\draw   (335.43,102) -- (363.06,102) .. controls (378.32,102) and (390.7,115.44) .. (390.7,132) .. controls (390.7,148.56) and (378.32,162) .. (363.06,162) -- (335.43,162) -- (335.43,102) -- cycle (317,112) -- (335.43,112) (317,152) -- (335.43,152) (401.76,132) -- (416.5,132) (390.7,132) .. controls (390.7,128.69) and (393.18,126) .. (396.23,126) .. controls (399.28,126) and (401.76,128.69) .. (401.76,132) .. controls (401.76,135.31) and (399.28,138) .. (396.23,138) .. controls (393.18,138) and (390.7,135.31) .. (390.7,132) -- cycle ;
		%Straight Lines [id:da9975516042139123] 
		\draw    (317,112) -- (317,152) ;
		%Straight Lines [id:da17989107719880582] 
		\draw    (317,132) -- (306.5,132) ;
		\draw [shift={(317,132)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		%Shape: Nand Gate [id:dp7278841173411272] 
		\draw   (225.43,102) -- (253.06,102) .. controls (268.32,102) and (280.7,115.44) .. (280.7,132) .. controls (280.7,148.56) and (268.32,162) .. (253.06,162) -- (225.43,162) -- (225.43,102) -- cycle (207,112) -- (225.43,112) (207,152) -- (225.43,152) (291.76,132) -- (306.5,132) (280.7,132) .. controls (280.7,128.69) and (283.18,126) .. (286.23,126) .. controls (289.28,126) and (291.76,128.69) .. (291.76,132) .. controls (291.76,135.31) and (289.28,138) .. (286.23,138) .. controls (283.18,138) and (280.7,135.31) .. (280.7,132) -- cycle ;
		%Shape: Nand Gate [id:dp5378786345834301] 
		\draw   (253.43,202) -- (281.06,202) .. controls (296.32,202) and (308.7,215.44) .. (308.7,232) .. controls (308.7,248.56) and (296.32,262) .. (281.06,262) -- (253.43,262) -- (253.43,202) -- cycle (235,212) -- (253.43,212) (235,252) -- (253.43,252) (319.76,232) -- (334.5,232) (308.7,232) .. controls (308.7,228.69) and (311.18,226) .. (314.23,226) .. controls (317.28,226) and (319.76,228.69) .. (319.76,232) .. controls (319.76,235.31) and (317.28,238) .. (314.23,238) .. controls (311.18,238) and (308.7,235.31) .. (308.7,232) -- cycle ;
		%Straight Lines [id:da30203207870053483] 
		\draw    (235,212) -- (235,252) ;
		%Straight Lines [id:da560114061227662] 
		\draw    (235,232) -- (208.5,232) ;
		\draw [shift={(235,232)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		%Shape: Nand Gate [id:dp28598236553410694] 
		\draw   (253.43,287) -- (281.06,287) .. controls (296.32,287) and (308.7,300.44) .. (308.7,317) .. controls (308.7,333.56) and (296.32,347) .. (281.06,347) -- (253.43,347) -- (253.43,287) -- cycle (235,297) -- (253.43,297) (235,337) -- (253.43,337) (319.76,317) -- (334.5,317) (308.7,317) .. controls (308.7,313.69) and (311.18,311) .. (314.23,311) .. controls (317.28,311) and (319.76,313.69) .. (319.76,317) .. controls (319.76,320.31) and (317.28,323) .. (314.23,323) .. controls (311.18,323) and (308.7,320.31) .. (308.7,317) -- cycle ;
		%Straight Lines [id:da09919382255175302] 
		\draw    (235,297) -- (235,337) ;
		%Straight Lines [id:da6918243026769499] 
		\draw    (235,317) -- (208.5,317) ;
		\draw [shift={(235,317)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		%Shape: Nand Gate [id:dp8045571227252166] 
		\draw   (352.93,245) -- (380.56,245) .. controls (395.82,245) and (408.2,258.44) .. (408.2,275) .. controls (408.2,291.56) and (395.82,305) .. (380.56,305) -- (352.93,305) -- (352.93,245) -- cycle (334.5,255) -- (352.93,255) (334.5,295) -- (352.93,295) (419.26,275) -- (434,275) (408.2,275) .. controls (408.2,271.69) and (410.68,269) .. (413.73,269) .. controls (416.78,269) and (419.26,271.69) .. (419.26,275) .. controls (419.26,278.31) and (416.78,281) .. (413.73,281) .. controls (410.68,281) and (408.2,278.31) .. (408.2,275) -- cycle ;
		%Straight Lines [id:da8154528342158318] 
		\draw    (334.5,255) -- (334.5,232) ;
		%Straight Lines [id:da8436712808938343] 
		\draw    (334.5,317) -- (334.5,295) ;
		%Shape: Nand Gate [id:dp9320874041129539] 
		\draw   (252.43,380) -- (280.06,380) .. controls (295.32,380) and (307.7,393.44) .. (307.7,410) .. controls (307.7,426.56) and (295.32,440) .. (280.06,440) -- (252.43,440) -- (252.43,380) -- cycle (234,390) -- (252.43,390) (234,430) -- (252.43,430) (318.76,410) -- (333.5,410) (307.7,410) .. controls (307.7,406.69) and (310.18,404) .. (313.23,404) .. controls (316.28,404) and (318.76,406.69) .. (318.76,410) .. controls (318.76,413.31) and (316.28,416) .. (313.23,416) .. controls (310.18,416) and (307.7,413.31) .. (307.7,410) -- cycle ;
		%Straight Lines [id:da3658226889489151] 
		\draw    (234,390) -- (234,430) ;
		%Straight Lines [id:da0777940306904783] 
		\draw    (234,410) -- (207.5,410) ;
		\draw [shift={(234,410)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		%Shape: Nand Gate [id:dp6595301509843374] 
		\draw   (252.43,465) -- (280.06,465) .. controls (295.32,465) and (307.7,478.44) .. (307.7,495) .. controls (307.7,511.56) and (295.32,525) .. (280.06,525) -- (252.43,525) -- (252.43,465) -- cycle (234,475) -- (252.43,475) (234,515) -- (252.43,515) (318.76,495) -- (333.5,495) (307.7,495) .. controls (307.7,491.69) and (310.18,489) .. (313.23,489) .. controls (316.28,489) and (318.76,491.69) .. (318.76,495) .. controls (318.76,498.31) and (316.28,501) .. (313.23,501) .. controls (310.18,501) and (307.7,498.31) .. (307.7,495) -- cycle ;
		%Straight Lines [id:da16321916270747816] 
		\draw    (234,475) -- (234,515) ;
		%Straight Lines [id:da29496476548368356] 
		\draw    (234,495) -- (207.5,495) ;
		\draw [shift={(234,495)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		%Shape: Nand Gate [id:dp28194620199773945] 
		\draw   (351.93,423) -- (379.56,423) .. controls (394.82,423) and (407.2,436.44) .. (407.2,453) .. controls (407.2,469.56) and (394.82,483) .. (379.56,483) -- (351.93,483) -- (351.93,423) -- cycle (333.5,433) -- (351.93,433) (333.5,473) -- (351.93,473) (418.26,453) -- (433,453) (407.2,453) .. controls (407.2,449.69) and (409.68,447) .. (412.73,447) .. controls (415.78,447) and (418.26,449.69) .. (418.26,453) .. controls (418.26,456.31) and (415.78,459) .. (412.73,459) .. controls (409.68,459) and (407.2,456.31) .. (407.2,453) -- cycle ;
		%Straight Lines [id:da5943105945113074] 
		\draw    (333.5,433) -- (333.5,410) ;
		%Straight Lines [id:da9958651284549738] 
		\draw    (333.5,495) -- (333.5,473) ;
		%Shape: Nand Gate [id:dp604688281196226] 
		\draw   (451.93,423) -- (479.56,423) .. controls (494.82,423) and (507.2,436.44) .. (507.2,453) .. controls (507.2,469.56) and (494.82,483) .. (479.56,483) -- (451.93,483) -- (451.93,423) -- cycle (433.5,433) -- (451.93,433) (433.5,473) -- (451.93,473) (518.26,453) -- (533,453) (507.2,453) .. controls (507.2,449.69) and (509.68,447) .. (512.73,447) .. controls (515.78,447) and (518.26,449.69) .. (518.26,453) .. controls (518.26,456.31) and (515.78,459) .. (512.73,459) .. controls (509.68,459) and (507.2,456.31) .. (507.2,453) -- cycle ;
		%Straight Lines [id:da626196336797249] 
		\draw    (433,453) -- (433,433) ;
		%Straight Lines [id:da266711715896534] 
		\draw    (433,473) -- (433,453) ;
		%Shape: Nand Gate [id:dp41152450126366613] 
		\draw   (372.43,558) -- (400.06,558) .. controls (415.32,558) and (427.7,571.44) .. (427.7,588) .. controls (427.7,604.56) and (415.32,618) .. (400.06,618) -- (372.43,618) -- (372.43,558) -- cycle (354,568) -- (372.43,568) (354,608) -- (372.43,608) (438.76,588) -- (453.5,588) (427.7,588) .. controls (427.7,584.69) and (430.18,582) .. (433.23,582) .. controls (436.28,582) and (438.76,584.69) .. (438.76,588) .. controls (438.76,591.31) and (436.28,594) .. (433.23,594) .. controls (430.18,594) and (427.7,591.31) .. (427.7,588) -- cycle ;
		%Straight Lines [id:da02945454234867806] 
		\draw    (354,608) -- (327.5,608) ;
		%Shape: Nand Gate [id:dp5711771892427948] 
		\draw   (372.43,643) -- (400.06,643) .. controls (415.32,643) and (427.7,656.44) .. (427.7,673) .. controls (427.7,689.56) and (415.32,703) .. (400.06,703) -- (372.43,703) -- (372.43,643) -- cycle (354,653) -- (372.43,653) (354,693) -- (372.43,693) (438.76,673) -- (453.5,673) (427.7,673) .. controls (427.7,669.69) and (430.18,667) .. (433.23,667) .. controls (436.28,667) and (438.76,669.69) .. (438.76,673) .. controls (438.76,676.31) and (436.28,679) .. (433.23,679) .. controls (430.18,679) and (427.7,676.31) .. (427.7,673) -- cycle ;
		%Straight Lines [id:da9496094577806098] 
		\draw    (354,653) -- (327.5,653) ;
		%Shape: Nand Gate [id:dp025296409895373184] 
		\draw   (471.93,601) -- (499.56,601) .. controls (514.82,601) and (527.2,614.44) .. (527.2,631) .. controls (527.2,647.56) and (514.82,661) .. (499.56,661) -- (471.93,661) -- (471.93,601) -- cycle (453.5,611) -- (471.93,611) (453.5,651) -- (471.93,651) (538.26,631) -- (553,631) (527.2,631) .. controls (527.2,627.69) and (529.68,625) .. (532.73,625) .. controls (535.78,625) and (538.26,627.69) .. (538.26,631) .. controls (538.26,634.31) and (535.78,637) .. (532.73,637) .. controls (529.68,637) and (527.2,634.31) .. (527.2,631) -- cycle ;
		%Straight Lines [id:da7773670696226334] 
		\draw    (453.5,611) -- (453.5,588) ;
		%Straight Lines [id:da805326111298079] 
		\draw    (453.5,673) -- (453.5,651) ;
		%Shape: Nand Gate [id:dp44954199243385906] 
		\draw   (246.43,600) -- (274.06,600) .. controls (289.32,600) and (301.7,613.44) .. (301.7,630) .. controls (301.7,646.56) and (289.32,660) .. (274.06,660) -- (246.43,660) -- (246.43,600) -- cycle (228,610) -- (246.43,610) (228,650) -- (246.43,650) (312.76,630) -- (327.5,630) (301.7,630) .. controls (301.7,626.69) and (304.18,624) .. (307.23,624) .. controls (310.28,624) and (312.76,626.69) .. (312.76,630) .. controls (312.76,633.31) and (310.28,636) .. (307.23,636) .. controls (304.18,636) and (301.7,633.31) .. (301.7,630) -- cycle ;
		%Straight Lines [id:da5936909822297569] 
		\draw    (327.5,630) -- (327.5,608) ;
		\draw [shift={(327.5,630)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		%Straight Lines [id:da6810423166090658] 
		\draw    (327.5,653) -- (327.5,631) ;
		%Straight Lines [id:da6220415887783992] 
		\draw    (208.5,568) -- (354,568) ;
		%Straight Lines [id:da8390645396143117] 
		\draw    (228,569) -- (228,610) ;
		\draw [shift={(228,569)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		%Straight Lines [id:da7208515876632795] 
		\draw    (208.5,693) -- (354,693) ;
		%Straight Lines [id:da12055171979881085] 
		\draw    (228,693) -- (228,650) ;
		\draw [shift={(228,693)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
		
		% Text Node
		\draw (118,28) node [anchor=north west][inner sep=0.75pt]  [font=\Large] [align=left] {\textbf{NOT}};
		% Text Node
		\draw (187.5,31.9) node [anchor=north west][inner sep=0.75pt]    {$A$};
		% Text Node
		\draw (337,31.9) node [anchor=north west][inner sep=0.75pt]    {$Q$};
		% Text Node
		\draw (117,119) node [anchor=north west][inner sep=0.75pt]  [font=\Large] [align=left] {\textbf{AND}};
		% Text Node
		\draw (421,124.9) node [anchor=north west][inner sep=0.75pt]    {$Q$};
		% Text Node
		\draw (187.5,102.9) node [anchor=north west][inner sep=0.75pt]    {$A$};
		% Text Node
		\draw (189,145.9) node [anchor=north west][inner sep=0.75pt]    {$B$};
		% Text Node
		\draw (132,300) node [anchor=north west][inner sep=0.75pt]  [font=\Large] [align=left] {\textbf{OR}};
		% Text Node
		\draw (187.5,224.9) node [anchor=north west][inner sep=0.75pt]    {$A$};
		% Text Node
		\draw (189,308.9) node [anchor=north west][inner sep=0.75pt]    {$B$};
		% Text Node
		\draw (442,266.9) node [anchor=north west][inner sep=0.75pt]    {$Q$};
		% Text Node
		\draw (116,440) node [anchor=north west][inner sep=0.75pt]  [font=\Large] [align=left] {\textbf{NOR}};
		% Text Node
		\draw (187.5,402.9) node [anchor=north west][inner sep=0.75pt]    {$A$};
		% Text Node
		\draw (189,486.9) node [anchor=north west][inner sep=0.75pt]    {$B$};
		% Text Node
		\draw (540,444.9) node [anchor=north west][inner sep=0.75pt]    {$Q$};
		% Text Node
		\draw (117,619) node [anchor=north west][inner sep=0.75pt]  [font=\Large] [align=left] {\textbf{XOR}};
		% Text Node
		\draw (187.5,560.9) node [anchor=north west][inner sep=0.75pt]    {$A$};
		% Text Node
		\draw (189,685.9) node [anchor=north west][inner sep=0.75pt]    {$B$};
		% Text Node
		\draw (561,622.9) node [anchor=north west][inner sep=0.75pt]    {$Q$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Construction of all gates from NAND gates}
	\end{figure}
	
	\pagebreak
	\subsubsection{Karnaugh maps}
	The "\NewTerm{Karnaugh map}\index{Karnaugh map}", also known as the "\NewTerm{K-map}\index{K-map}", is a method to simplify Boolean Algebra expressions. Maurice Karnaugh introduced it in 11953 (holocene calendar). The Karnaugh map reduces the need for extensive calculations by taking advantage of humans' pattern-recognition capability\footnote{The reader can find easily on the Internet many K-map generators that simplifies expressions automatically!}. 
	
	The required boolean results are transferred from a truth table onto a two-dimensional grid where the cells are ordered in Gray code, and each cell position represents one combination of input conditions, while each cell value represents the corresponding output value. Optimal groups of $1$s or $0$s are identified, which represent the terms of a canonical form of the logic in the original truth table. These terms can be used to write a minimal boolean expression representing the required logic.
	
	Karnaugh maps are used to simplify real-world logic requirements so that they can be implemented using a minimum number of physical logic gates. A sum-of-products expression can always be implemented using AND gates feeding into an OR gate, and a product-of-sums expression leads to OR gates feeding an AND gate. Karnaugh maps can also be used to simplify logic expressions in software design. Boolean conditions, as used for example in conditional statements, can get very complicated, which makes the code difficult to read and to maintain. Once minimized, canonical sum-of-products and product-of-sums expressions can be implemented directly using AND and OR logic operators.
	
	Let us consider for example the function:
	
	(in disjunctive normal form) and its respective truth table:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}$\pmb{b}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{a}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{z(a,b)}$} \\ \hline
		$0$ & $0$ & $1$ \\ \hline
		$0$ & $1$ & $0$ \\ \hline
		$1$ & $0$ & $1$ \\ \hline
		$1$ & $1$ & $1$ \\ \hline
		\end{tabular}
	\end{table}
	The Karnaugh map is defined by a representation like the one below following the \texttt{karnaugh-map} package of \LaTeX{} (as it does sadly not exist at this day any international norm on how to represent Karnaugh map):
	
	\begin{center}
	\begin{tikzpicture}[thick]
		\karnaughmapcolorfield{2}{0}{teal!50}%
		\karnaughmapcolorfield{2}{1}{violet!50}%
		\karnaughmapcolorfield{2}{3}{red!50}%
		\karnaughmap[omitnegated=false,binaryidx,omitzeros=false]{1011}
	\end{tikzpicture}
	\end{center}
	
	The colored items are those that we need to keep. That is: $\bar{a}\bar{b}$, $\bar{a}b$, $ab$.
	
	The Karnaugh table of a logic function thus has as many cells as possible combinations of variables which compose it, ie four cells for a function with two variables, and $2^n$ cells for a function with $n$ variables. Each cell, which is at the intersection of a row and column of the Karnaugh table, has the state $0$ or $1$ that the function $z(a,b)$ takes for the corresponding logical product of the variables (minterms).

	In the preceding example, however, we can see something interesting, the function $z(a,b)$, as we see very well, can be simplified in two ways:
	
	or also:
	
	This possible simplification is always done with two adjacent minterms in the Karnaugh table such as for the first solution $z=b+\bar{a}\bar{b}$:
	\begin{center}
	\begin{tikzpicture}[thick]
		\karnaughmapcolorfield{2}{0}{red!50}%
		\karnaughmapcolorfield{2}{2}{red!50}%
		\karnaughmap[omitnegated=false,binaryidx,omitzeros=false]{1011}
	\end{tikzpicture}
	\end{center}
	
	and for $z=ab+\bar{a}$:
	\begin{center}
	\begin{tikzpicture}[thick]
		\karnaughmapcolorfield{2}{2}{red!50}%
		\karnaughmapcolorfield{2}{3}{red!50}%
		\karnaughmap[omitnegated=false,binaryidx,omitzeros=false]{1011}
	\end{tikzpicture}
	\end{center}
	
	We see that indeed the first grouping / simplification (horizontal) is done on the row $\bar{a}$ and the second grouping / simplification (vertical) is done on column $b$ both results of the algebraic simplification of the function as following:
	
	and:
	
	So we could make the hypothesize that the Karnaugh table has for properties:
	\begin{enumerate}
		\item[P1.] To give us the normal disjunctive form of a function.

		\item[P2.] That all adjoining cells with a value of $1$ can be simplified in the respective symbol (letter) of their union
	\end{enumerate}
	It is therefore an extremely powerful tool (algorithm) for simplifying and determining logical functions.

	Let's look at an example with three variables!
	
	First we give the truth table that can help for a better understanding:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}$\pmb{a}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{b}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{c}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{z(a,b,c)}$} \\ \hline
		$0$ & $0$ & $0$ & $1$ \\ \hline
		$0$ & $0$ & $1$ & $1$ \\ \hline
		$0$ & $1$ & $0$ & $1$ \\ \hline
		$0$ & $1$ & $1$ & $1$ \\ \hline
		$1$ & $1$ & $0$ & $0$ \\ \hline
		$1$ & $1$ & $1$ & $0$ \\ \hline
		$1$ & $0$ & $0$ & $1$ \\ \hline
		$1$ & $0$ & $1$ & $1$ \\ \hline
		\end{tabular}
	\end{table}
	and the corresponding Karnaugh map:
	\begin{center}
	\begin{tikzpicture}[thick]
		\karnaughmap[omitnegated=false,binaryidx,omitzeros=false]{1111 0011}
	\end{tikzpicture}
	\end{center}
	The normal disjonctive form is the given by all the cells that are equal to $1$:
	
	This corresponds to the following colored Karnaugh map:
	\begin{center}
	\begin{tikzpicture}[thick]
		\karnaughmapcolorfield{2}{0}{red!50}%
		\karnaughmapcolorfield{2}{1}{red!50}%
		\karnaughmapcolorfield{2}{2}{red!50}%
		\karnaughmapcolorfield{2}{3}{red!50}%
		\karnaughmapcolorfield{3}{6}{red!50}%
		\karnaughmapcolorfield{3}{7}{red!50}%
		\karnaughmap[omitnegated=false,binaryidx,omitzeros=false]{1111 0011}
	\end{tikzpicture}
	\end{center}
	We see quickly that the previous normal disjonctive form can be simplified as:
	
	That gives the following Karnaugh map:
	\begin{center}
	\begin{tikzpicture}[thick]
		\karnaughmapcolorfield{2}{0}{teal!50}%
		\karnaughmapcolorfield{2}{1}{teal!50}%
		\karnaughmapcolorfield{2}{2}{teal!50}%
		\karnaughmapcolorfield{2}{3}{teal!50}%
		\karnaughmapcolorfield{3}{6}{violet!50}%
		\karnaughmapcolorfield{3}{7}{violet!50}%
		\karnaughmap[omitnegated=false,binaryidx,omitzeros=false]{1111 0011}
	\end{tikzpicture}
	\end{center}
	
	But it is less obvious with a Karnaugh map to see that all previous relations can be simplified into (using binary addition rules):
	
	This latter result can be seen with the available Karnaugh map minimizer freeware that we can find nowadays on the Internet. For example the freeware Karnaugh Map Minimizer that gives us for the previous example (we also see at the same time why it is boring - as always also in other field of engineering a science - that there are no ISO norm to standardize Karnaugh map representation at the days we write these lines):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/computing/karnaugh_map_minimizer.jpg}
		\caption{Karnaugh Map Minimizer 0.4}
	\end{figure}
	A difficulty remains however sometimes with this technique: how to choose the best construction of the table (disposition of letters)?

	In fact, there is a specific way of associating the Boolean Algebra complement rule with what we name the "Gray code".

	\textbf{Definition (\#\thesection.\mydef):} The "\NewTerm{reflected binary code (RBC)}\index{reflected binary code}", also known as "\NewTerm{Gray code}\index{Gray code}" after Frank Gray, is a binary numeral system where two successive values differ in only one bit (binary digit). The reflected binary code was originally designed to prevent spurious output from electromechanical switches\footnote{The problem with natural binary codes is that physical switches are not ideal: it is very unlikely that physical switches will change states exactly in synchrony. In the transition between the two states shown above, all three switches change state.} and is useful to optimally build Karnaugh maps.

	Here is an example of the Gray code for decimal $0$ to $15$ (the reader can notice that only one bit/switch at a time change at each row):
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}\textbf{Decimal}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Binary}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Gray}} \\ \hline
		$0$ & $0000$ & $0000$ \\ \hline
		$1$ & $0001$ & $0001$ \\ \hline
		$2$ & $0010$ & $0011$ \\ \hline
		$3$ & $0011$ & $0010$ \\ \hline
		$4$ & $0100$ & $0110$ \\ \hline
		$5$ & $0101$ & $0111$ \\ \hline
		$6$ & $0110$ & $0101$ \\ \hline
		$7$ & $0111$ & $0100$ \\ \hline
		$8$ & $1000$ & $1100$ \\ \hline
		$9$ & $1001$ & $1101$ \\ \hline
		$10$ & $1010$ & $1111$ \\ \hline
		$11$ & $1011$ & $1110$ \\ \hline
		$12$ & $1100$ & $1010$ \\ \hline
		$13$ & $1101$ & $1011$ \\ \hline
		$14$ & $1110$ & $1001$ \\ \hline
		$15$ & $1111$ & $1000$ \\ \hline
		\end{tabular}
		\caption{Gray code of $0$ to $15$}
	\end{table}
	Using Gray code we can create optimal Karnaugh tables. The reason is simple, the Gray code changes only one bit at a time at each increment as we have just seen. In practice this means that for two consecutive values, $1$ and $2$ for example, one of the two variables will be the opposite of the other one.
	
	We can see this very well with the following structures:
	\begin{center}
	\begin{tikzpicture}[thick]
		\karnaughmap{4}
	\end{tikzpicture}
	\end{center}
	but more especially with greater tables (the order of the cell is not obvious):
	\begin{center}
	\begin{tikzpicture}[thick]
		\karnaughmap{8}
	\end{tikzpicture}
	\end{center}
	or:
	\begin{center}
	\begin{tikzpicture}[thick]
		\karnaughmap{16}
	\end{tikzpicture}
	\end{center}
	or more explicitly for that latter (you can therefore compare with the above Gray code table with $4$ digits to understand from where the internal numbering comes from):
	\begin{center}
	\begin{tikzpicture}[thick]
		\karnaughmap[defaultmap=16,binaryidx,omitnegated=false]{}
	\end{tikzpicture}
	\end{center}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Given $01$ corresponding to $\bar{b}a$ and $11$ corresponding to $ba$, the sum (disjunctive form) would give us:
	
	 which is reduced by using the complementation rule directly to:
	
	hence the advantage to represent them next to each other in a Karnaugh table.
	\end{tcolorbox}
	All this to say that when two formulas are found side by side in a Karnaugh map, we retain the similar elements only.

	The rules are such that we can reduce therefore when (see previous concrete example):
	\begin{itemize}
		\item[R1.] Two $1$ are juxtaposed in the table (here the $\bar{c}+c$ will be simplified in the disjunctive form):
		\begin{center}
		\begin{tikzpicture}[thick]
			\karnaughmapcolorfield{2}{0}{red!50}%
			\karnaughmapcolorfield{2}{1}{red!50}%
			\karnaughmap[omitnegated=false,binaryidx,omitzeros=true]{1100 0001}
		\end{tikzpicture}
		\end{center}
		
		\item[R2.] When two $1$ are at the extremities of the table (here the $\bar{b}+b$ will be simplified in the disjunctive form:
		\begin{center}
		\begin{tikzpicture}[thick]
			\karnaughmapcolorfield{2}{0}{red!50}%
			\karnaughmapcolorfield{3}{4}{red!50}%
			\karnaughmap[omitnegated=false,binaryidx,omitzeros=true]{1000 1001}
		\end{tikzpicture}
		\end{center}
		
		\item[R3.] A whole row is full of $1$ (in this case the both variables $ab$ disappear from the disjonctive form):
		\begin{center}
		\begin{tikzpicture}[thick]
			\karnaughmapcolorfield{2}{0}{red!50}%
			\karnaughmapcolorfield{2}{2}{red!50}%
			\karnaughmapcolorfield{3}{4}{red!50}%
			\karnaughmapcolorfield{3}{6}{red!50}%
			\karnaughmap[omitnegated=false,binaryidx,omitzeros=true]{1010 1010}
		\end{tikzpicture}
		\end{center}
		
		\item[R4.] A whole column is full of $1$ (in this case the both variables $cd$ disappear from the disjonctive form):
		\begin{center}
		\begin{tikzpicture}[thick]
			\karnaughmapcolorfield{2}{2}{red!50}%
			\karnaughmapcolorfield{3}{3}{red!50}%
			\karnaughmapcolorfield{4}{4}{red!50}%
			\karnaughmapcolorfield{4}{5}{red!50}%
			\karnaughmap[omitnegated=false,binaryidx,omitzeros=true]{0000 1111 1000 0000}
		\end{tikzpicture}
		\end{center}
		
		\item[R5.] Four adjacents cells are full of $1$ (in this case $b$ and $d$ disappear):
		\begin{center}
		\begin{tikzpicture}[thick]
			\karnaughmapcolorfield{4}{4}{red!50}%
			\karnaughmapcolorfield{4}{5}{red!50}%
			\karnaughmapcolorfield{4}{0}{red!50}%
			\karnaughmapcolorfield{4}{1}{red!50}%
			\karnaughmap[omitnegated=false,binaryidx,omitzeros=true]{1100 1100 0000 0000}
		\end{tikzpicture}
		\end{center}
		
		\item[R6.] The same cells can be used for two reductions:
		\begin{center}
		\begin{tikzpicture}[thick]
			\karnaughmapcolorfield{4}{4}{red!50}%
			\karnaughmapcolorfield{4}{5}{red!50}%
			\karnaughmapcolorfield{4}{0}{red!50}%
			\karnaughmapcolorfield{4}{1}{red!50}%
			\karnaughmapcolorfield[outline,ultra thick]{4}{4}{violet}%
			\karnaughmapcolorfield[outline,ultra thick]{4}{5}{violet}%
			\karnaughmapcolorfield[outline,ultra thick]{4}{c}{violet}%
			\karnaughmapcolorfield[outline,ultra thick]{4}{d}{violet}%
			
			\karnaughmap[omitnegated=false,binaryidx,omitzeros=true]{1100 1100 0000 1100}
		\end{tikzpicture}
		\end{center}
		
		\item[R7.] The same box can be used for two reductions:
		\begin{center}
		\begin{tikzpicture}[thick]
			\karnaughmapcolorfield{4}{0}{red!50}%
			\karnaughmapcolorfield{4}{4}{red!50}%
			\karnaughmapcolorfield[outline,ultra thick]{4}{4}{violet}%
			\karnaughmapcolorfield[outline,ultra thick]{4}{c}{violet}%
			
			\karnaughmap[omitnegated=false,omitzeros=true]{1000 1000 0000 1000}
		\end{tikzpicture}
		\end{center}
	\end{itemize}
	and without errors ... that's all but it's already not bad!
	
	\pagebreak
	\subsubsection{Arithmetic Boolean (binary) operations}
	Using all the elements demonstrated and given previously, we are now able to rigorously determine the logic function allowing Boolean addition and subtraction. Let us also recall that this being done, we can construct multiplication and division using respectively addition and subtraction.

	However, we can not with formal digital systems build elements for integration and differentiation. For this we refer the reader to the section of Electrokinetics where it is shown how to use inductors and capacitors to perform such operations with signals.

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	We will work on integers but the reader must remember that rational numbers can always be increased in power to be represented in an integral way (it remains to perform the inverse operation if necessary).
	\end{tcolorbox}
	The sum of two bytes will be denoted $S$, the retention $C_s$ (outgoing retention, also often denoted $C_\text{out}$) and the deferred retention $C_e$ (inward retention, also often denoted $C_\text{in}$).

	The truth table will be build with the "tip" that the system inputs $(a,b,C_e)$ take all possible values on $3$ bits (three letters) thus $2^3=8$ rows that we have represented in the following table:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}$\pmb{C_e}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{a}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{b}$} \\ \hline
		$0$ & $0$ & $0$ \\ \hline
		$0$ & $0$ & $1$ \\ \hline
		$0$ & $1$ & $0$ \\ \hline
		$0$ & $1$ & $1$ \\ \hline
		$1$ & $0$ & $0$ \\ \hline
		$1$ & $0$ & $1$ \\ \hline
		$1$ & $1$ & $0$ \\ \hline
		$1$ & $1$ & $1$ \\ \hline
		\end{tabular}
		\caption[Identification of the reported retentions for the binary sum]{Identification of the reported\\retentions for the binary sum}
	\end{table}
	And now the idea consists in adding the column constituted by the sum:
	
	row by row (without thinking to the outgoing retention $C_s$ that we will see a little bit further below):
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}$\pmb{C_e}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{a}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{b}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{S}$} \\ \hline
		$0$ & $0$ & $0$ & $0$ \\ \hline
		$0$ & $0$ & $1$ & $1$ \\ \hline
		$0$ & $1$ & $0$ & $1$ \\ \hline
		$0$ & $1$ & $1$ & $0$ \\ \hline
		$1$ & $0$ & $0$ & $1$ \\ \hline
		$1$ & $0$ & $1$ & $0$ \\ \hline
		$1$ & $1$ & $0$ & $0$ \\ \hline
		$1$ & $1$ & $1$ & $1$ \\ \hline
		\end{tabular}
		\caption[]{Sum for the binary sum}
	\end{table}
	Now, row by row, we add the outgoing retention $C_s$ (which is none other than the value that is sent to the incoming retention of the next row) of the sum $S$:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}$\pmb{C_e}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{a}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{b}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{S}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{C_s}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Minterms}} \\ \hline
		$0$ & $0$ & $0$ & $0$ & $0$ & $\bar{C}_e\bar{a}\bar{b}$  \\ \hline
		$0$ & $0$ & $1$ & $1$ & $0$ & $\bar{C}_e\bar{a}b$  \\ \hline
		$0$ & $1$ & $0$ & $1$ & $0$ & $\bar{C}_ea\bar{b}$ \\ \hline
		$0$ & $1$ & $1$ & $0$ & $1$ & $\bar{C}_eab$  \\ \hline
		$1$ & $0$ & $0$ & $1$ & $0$ & $C_e\bar{a}\bar{b}$  \\ \hline
		$1$ & $0$ & $1$ & $0$ & $1$ & $C_e\bar{a}b$  \\ \hline
		$1$ & $1$ & $0$ & $0$ & $1$ & $C_e a\bar{b}$   \\ \hline
		$1$ & $1$ & $1$ & $1$ & $1$ & $C_eab$  \\ \hline
		\end{tabular}
		\caption[]{Retentions reported for the binary sum}
	\end{table}
	Therefore we have $4$ minterms (that is, the terms for which $S$ is non-zero in the rows $2$, $3$, $5$ and $8$) such that the normal disjonctive form is written:
	
	A possible simplification is:
	
	It also comes for the outgoing retention the following minterms:
	
	So finally we have:
	
	To build physically with fundamental logic gates this addition, it is useful first to introduce an intermediary logic circuit (but it's not obliged, it's just for pedagogical reasons and by educational tradition!). 
	
	Let us for this purpose consider the truth table of the addition without incoming retention, named a "\NewTerm{half adder}\index{half adder}", and represented by the following "technical" drawing:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1241); %set diagram left start at 0, and has height of 1241
		
		%Shape: Rectangle [id:dp28391896942955874] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (237,141.82) -- (366.82,141.82) -- (366.82,220.1) -- (237,220.1) -- cycle ;
		%Straight Lines [id:da27151765883645607] 
		\draw    (237,164.82) -- (161.82,164.82) ;
		%Straight Lines [id:da538273061551303] 
		\draw    (237,197.82) -- (161.82,197.82) ;
		%Straight Lines [id:da14464589117517423] 
		\draw    (442,164.82) -- (366.82,164.82) ;
		%Straight Lines [id:da019078496761083974] 
		\draw    (442,197.82) -- (366.82,197.82) ;
		%Shape: Xor Gate [id:dp355138807017108] 
		\draw   (277,262.82) -- (297,262.82) .. controls (310.95,263.36) and (323.42,275.05) .. (329,292.82) .. controls (323.42,310.59) and (310.95,322.29) .. (297,322.82) -- (277,322.82) .. controls (285.57,304.26) and (285.57,281.39) .. (277,262.82) -- cycle (265,272.82) -- (281,272.82) (265,312.82) -- (281,312.82) (329,292.82) -- (345,292.82) (273,262.82) .. controls (281.57,281.39) and (281.57,304.26) .. (273,322.82) ;
		%Straight Lines [id:da14939174532870902] 
		\draw    (265,272.82) -- (161.82,272.82) ;
		%Straight Lines [id:da34887443120219874] 
		\draw    (265,312.82) -- (161.82,312.82) ;
		%Straight Lines [id:da4726437326987931] 
		\draw    (439.82,292.82) -- (345,292.82) ;
		%Shape: And Gate [id:dp1781470594408605] 
		\draw   (277,361.82) -- (301,361.82) .. controls (314.25,361.82) and (325,375.27) .. (325,391.82) .. controls (325,408.38) and (314.25,421.82) .. (301,421.82) -- (277,421.82) -- (277,361.82) -- cycle (261,371.82) -- (277,371.82) (261,411.82) -- (277,411.82) (325,391.82) -- (341,391.82) ;
		%Straight Lines [id:da5920096393038845] 
		\draw    (261,371.82) -- (227.82,371.82) ;
		%Straight Lines [id:da7999664243752023] 
		\draw    (261,411.82) -- (211.82,411.82) ;
		%Straight Lines [id:da628939191040568] 
		\draw    (227.82,273.1) -- (227.82,371.82) ;
		%Straight Lines [id:da38143380428126084] 
		\draw    (211.82,312.1) -- (211.82,411.82) ;
		%Shape: Circle [id:dp854212877411628] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (224.18,273.1) .. controls (224.18,271.09) and (225.81,269.46) .. (227.82,269.46) .. controls (229.83,269.46) and (231.45,271.09) .. (231.45,273.1) .. controls (231.45,275.1) and (229.83,276.73) .. (227.82,276.73) .. controls (225.81,276.73) and (224.18,275.1) .. (224.18,273.1) -- cycle ;
		%Shape: Circle [id:dp9480383045675802] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (208.18,312.1) .. controls (208.18,310.09) and (209.81,308.46) .. (211.82,308.46) .. controls (213.83,308.46) and (215.45,310.09) .. (215.45,312.1) .. controls (215.45,314.1) and (213.83,315.73) .. (211.82,315.73) .. controls (209.81,315.73) and (208.18,314.1) .. (208.18,312.1) -- cycle ;
		%Straight Lines [id:da24053327805592684] 
		\draw    (439.82,391.82) -- (341,391.82) ;
		
		% Text Node
		\draw (280,163.82) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{29.92pt}\setlength\topsep{0pt}
		\begin{center}
		Half\\Adder
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (160,147.22) node [anchor=north west][inner sep=0.75pt]    {$a$};
		% Text Node
		\draw (161,180.22) node [anchor=north west][inner sep=0.75pt]    {$b$};
		% Text Node
		\draw (429,148.22) node [anchor=north west][inner sep=0.75pt]    {$S$};
		% Text Node
		\draw (429,180.22) node [anchor=north west][inner sep=0.75pt]    {$C$};
		% Text Node
		\draw (160,254.22) node [anchor=north west][inner sep=0.75pt]    {$a$};
		% Text Node
		\draw (161,294.22) node [anchor=north west][inner sep=0.75pt]    {$b$};
		% Text Node
		\draw (337,271.22) node [anchor=north west][inner sep=0.75pt]    {$S=a\oplus b=\overline{a} b+a\overline{b}$};
		% Text Node
		\draw (409,372.22) node [anchor=north west][inner sep=0.75pt]    {$C_{s} =ab$};
		% Text Node
		\draw (413,393.82) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\footnotesize carry out}};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Half adder logic diagram}
	\end{figure}
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{a}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{b}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{S}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{C_s}$} \\ \hline
		$0$ & $0$ & $0$ & $0$  \\ \hline
		$0$ & $1$ & $1$ & $0$  \\ \hline
		$1$ & $0$ & $1$ & $0$  \\ \hline
		$1$ & $1$ & $0$ & $1$  \\ \hline
		\end{tabular}
	\end{table}
	And now we can introduce the "\NewTerm{full adder}\index{full adder}" that adds binary numbers and accounts for values carried in as well as out following the relation proved earlier above and is made of two half-adder. So here is for recall the minterms and we introduce the corresponding logical circuit:
	\begin{gather*}
		\begin{aligned}
		S&=C_e\oplus a\oplus b\\
		C_s&=ab+C_ea\oplus b
		\end{aligned}
	\end{gather*}
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1241); %set diagram left start at 0, and has height of 1241
		
		%Straight Lines [id:da27151765883645607] 
		\draw    (271.82,87.82) -- (161.82,87.82) ;
		%Straight Lines [id:da538273061551303] 
		\draw    (271.82,127.82) -- (249.45,127.82) ;
		%Straight Lines [id:da14464589117517423] 
		\draw    (427,107.82) -- (351.82,107.82) ;
		%Straight Lines [id:da019078496761083974] 
		\draw    (301.82,227.1) -- (232.82,227.1) ;
		%Shape: Circle [id:dp854212877411628] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (229.73,87.82) .. controls (229.73,85.82) and (231.36,84.19) .. (233.36,84.19) .. controls (235.37,84.19) and (237,85.82) .. (237,87.82) .. controls (237,89.83) and (235.37,91.46) .. (233.36,91.46) .. controls (231.36,91.46) and (229.73,89.83) .. (229.73,87.82) -- cycle ;
		%Shape: Circle [id:dp9480383045675802] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (245.82,267.1) .. controls (245.82,265.09) and (247.45,263.46) .. (249.45,263.46) .. controls (251.46,263.46) and (253.09,265.09) .. (253.09,267.1) .. controls (253.09,269.1) and (251.46,270.73) .. (249.45,270.73) .. controls (247.45,270.73) and (245.82,269.1) .. (245.82,267.1) -- cycle ;
		%Shape: Xor Gate [id:dp9469807277379385] 
		\draw   (283.82,77.82) -- (303.82,77.82) .. controls (317.77,78.36) and (330.24,90.05) .. (335.82,107.82) .. controls (330.24,125.59) and (317.77,137.29) .. (303.82,137.82) -- (283.82,137.82) .. controls (292.39,119.26) and (292.39,96.39) .. (283.82,77.82) -- cycle (271.82,87.82) -- (287.82,87.82) (271.82,127.82) -- (287.82,127.82) (335.82,107.82) -- (351.82,107.82) (279.82,77.82) .. controls (288.39,96.39) and (288.39,119.26) .. (279.82,137.82) ;
		%Straight Lines [id:da8032346288598833] 
		\draw    (233.36,227.1) -- (233.36,91.46) ;
		%Shape: And Gate [id:dp8298014638141165] 
		\draw   (317.82,217.1) -- (341.82,217.1) .. controls (355.06,217.1) and (365.82,230.54) .. (365.82,247.1) .. controls (365.82,263.65) and (355.06,277.1) .. (341.82,277.1) -- (317.82,277.1) -- (317.82,217.1) -- cycle (301.82,227.1) -- (317.82,227.1) (301.82,267.1) -- (317.82,267.1) (365.82,247.1) -- (381.82,247.1) ;
		%Shape: Or Gate [id:dp8234488021952531] 
		\draw   (421.82,268.91) -- (441.82,268.91) .. controls (455.77,269.45) and (468.24,281.14) .. (473.82,298.91) .. controls (468.24,316.68) and (455.77,328.38) .. (441.82,328.91) -- (421.82,328.91) .. controls (430.39,310.35) and (430.39,287.48) .. (421.82,268.91) -- cycle (409.82,278.91) -- (425.82,278.91) (409.82,318.91) -- (425.82,318.91) (473.82,298.91) -- (489.82,298.91) ;
		%Straight Lines [id:da7611697227284033] 
		\draw    (409.82,247.1) -- (381.82,247.1) ;
		%Straight Lines [id:da021452907277445643] 
		\draw    (409.82,278.91) -- (409.82,247.1) ;
		%Straight Lines [id:da008863638118008721] 
		\draw    (530.82,298.91) -- (489.82,298.91) ;
		%Shape: Xor Gate [id:dp5824801446566352] 
		\draw   (166.82,237.1) -- (186.82,237.1) .. controls (200.77,237.64) and (213.24,249.33) .. (218.82,267.1) .. controls (213.24,284.87) and (200.77,296.56) .. (186.82,297.1) -- (166.82,297.1) .. controls (175.39,278.53) and (175.39,255.66) .. (166.82,237.1) -- cycle (154.82,247.1) -- (170.82,247.1) (154.82,287.1) -- (170.82,287.1) (218.82,267.1) -- (234.82,267.1) (162.82,237.1) .. controls (171.39,255.66) and (171.39,278.53) .. (162.82,297.1) ;
		%Straight Lines [id:da2627730394726162] 
		\draw    (301.82,267.1) -- (234.82,267.1) ;
		%Straight Lines [id:da10784923768194021] 
		\draw    (249.45,267.1) -- (249.45,127.91) ;
		%Straight Lines [id:da13755413276154416] 
		\draw    (110.82,247.1) -- (154.82,247.1) ;
		%Straight Lines [id:da16604428291575668] 
		\draw    (110.82,287.1) -- (154.82,287.1) ;
		%Shape: And Gate [id:dp2967635738043757] 
		\draw   (213.82,319.1) -- (237.82,319.1) .. controls (251.06,319.1) and (261.82,332.54) .. (261.82,349.1) .. controls (261.82,365.65) and (251.06,379.1) .. (237.82,379.1) -- (213.82,379.1) -- (213.82,319.1) -- cycle (197.82,329.1) -- (213.82,329.1) (197.82,369.1) -- (213.82,369.1) (261.82,349.1) -- (277.82,349.1) ;
		%Straight Lines [id:da6109567581691902] 
		\draw    (154.82,329.1) -- (154.82,247.1) ;
		%Straight Lines [id:da9878228544144645] 
		\draw    (197.82,329.1) -- (154.82,329.1) ;
		%Shape: Circle [id:dp024381940032086602] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (151.18,247.1) .. controls (151.18,245.09) and (152.81,243.46) .. (154.82,243.46) .. controls (156.83,243.46) and (158.45,245.09) .. (158.45,247.1) .. controls (158.45,249.1) and (156.83,250.73) .. (154.82,250.73) .. controls (152.81,250.73) and (151.18,249.1) .. (151.18,247.1) -- cycle ;
		%Straight Lines [id:da8742997026849633] 
		\draw    (132.82,369.1) -- (132.82,287.1) ;
		%Straight Lines [id:da22720996939704152] 
		\draw    (197.82,369.1) -- (132.82,369.1) ;
		%Shape: Circle [id:dp173524754874391] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (129.18,287.1) .. controls (129.18,285.09) and (130.81,283.46) .. (132.82,283.46) .. controls (134.83,283.46) and (136.45,285.09) .. (136.45,287.1) .. controls (136.45,289.1) and (134.83,290.73) .. (132.82,290.73) .. controls (130.81,290.73) and (129.18,289.1) .. (129.18,287.1) -- cycle ;
		%Straight Lines [id:da16752890597975045] 
		\draw    (409.82,349.1) -- (277.82,349.1) ;
		%Straight Lines [id:da7973361357190278] 
		\draw    (409.82,318.91) -- (409.82,349.1) ;
		%Shape: Rectangle [id:dp6782008272549691] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (222.82,35.91) -- (375.82,35.91) -- (375.82,293.91) -- (222.82,293.91) -- cycle ;
		%Shape: Rectangle [id:dp7295482708175431] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (122.82,218.91) -- (275.82,218.91) -- (275.82,403.91) -- (122.82,403.91) -- cycle ;
		
		% Text Node
		\draw (108,269.22) node [anchor=north west][inner sep=0.75pt]    {$b$};
		% Text Node
		\draw (413,91.22) node [anchor=north west][inner sep=0.75pt]    {$S$};
		% Text Node
		\draw (163.82,90.82) node [anchor=north west][inner sep=0.75pt]   [align=left] {carry in};
		% Text Node
		\draw (379.82,109.82) node [anchor=north west][inner sep=0.75pt]   [align=left] {sum out};
		% Text Node
		\draw (515,281.22) node [anchor=north west][inner sep=0.75pt]    {$C_{s}$};
		% Text Node
		\draw (491.82,301.91) node [anchor=north west][inner sep=0.75pt]   [align=left] {carry out};
		% Text Node
		\draw (108,230.22) node [anchor=north west][inner sep=0.75pt]    {$a$};
		% Text Node
		\draw (236.82,272) node [anchor=north west][inner sep=0.75pt]   [align=left] {sum};
		% Text Node
		\draw (296.82,334) node [anchor=north west][inner sep=0.75pt]   [align=left] {carry};
		% Text Node
		\draw (264,17.64) node [anchor=north west][inner sep=0.75pt]   [align=left] {Half-Adder};
		% Text Node
		\draw (169,409.64) node [anchor=north west][inner sep=0.75pt]   [align=left] {Half-Adder};
		% Text Node
		\draw (384.82,227.82) node [anchor=north west][inner sep=0.75pt]   [align=left] {carry};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Full adder logic diagram}
	\end{figure}
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}$\pmb{C_e}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{a}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{b}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{S}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{C_s}$}\\ \hline
		$0$ & $0$ & $0$ & $0$ & $0$ \\ \hline
		$0$ & $0$ & $1$ & $1$ & $0$ \\ \hline
		$0$ & $1$ & $0$ & $1$ & $0$ \\ \hline
		$0$ & $1$ & $1$ & $0$ & $1$ \\ \hline
		$1$ & $0$ & $0$ & $1$ & $0$ \\ \hline
		$1$ & $0$ & $1$ & $0$ & $1$ \\ \hline
		$1$ & $1$ & $0$ & $0$ & $1$  \\ \hline
		$1$ & $1$ & $1$ & $1$ & $1$ \\ \hline
		\end{tabular}
	\end{table}
	
	\pagebreak
	The subtraction (difference) of two bytes will be denoted $D$, the borrowing $B_s$ (outgoing borrowing, also often denoted $B_\text{out}$) and the reported borrowing $B_e$ (inbound borrowing, also often denoted $B_\text{in}$). The truth table will first be build as for the addition. That is, the system inputs $(a,b,e_e)$ take all possible values on $3$-bit (three-letter) the $2^3=8$ rows. Therefore:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}$\pmb{B_e}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{a}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{b}$} \\ \hline
		$0$ & $0$ & $0$ \\ \hline
		$0$ & $0$ & $1$ \\ \hline
		$0$ & $1$ & $0$ \\ \hline
		$0$ & $1$ & $1$ \\ \hline
		$1$ & $0$ & $0$ \\ \hline
		$1$ & $0$ & $1$ \\ \hline
		$1$ & $1$ & $0$ \\ \hline
		$1$ & $1$ & $1$ \\ \hline
		\end{tabular}
		\caption[]{Identification of the reported borrowing for the binary subtraction}
	\end{table}
	But we will a little subtlety. Rather than bore us to calculate $D=a-b-B_e$, we will calculate $D=a+(-b)+(-B_e)$ in the purpose to be able to work with the following truth table:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}$\pmb{-B_e}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{a}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{-b}$} \\ \hline
		$0$ & $0$ & $0$ \\ \hline
		$0$ & $0$ & $1$ \\ \hline
		$0$ & $1$ & $0$ \\ \hline
		$0$ & $1$ & $1$ \\ \hline
		$1$ & $0$ & $0$ \\ \hline
		$1$ & $0$ & $1$ \\ \hline
		$1$ & $1$ & $0$ \\ \hline
		$1$ & $1$ & $1$ \\ \hline
		\end{tabular}
		\caption[]{Inversion of the reported borrowing for the binary subtraction}
	\end{table}
	and now the idea is to add the difference column $D=a+(-b)+(-B_e)$ row by row (without thinking about the borrowing $B_s$) which will be strictly identical to the truth table of the sum:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}$\pmb{-B_e}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{a}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{-b}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{D}$} \\ \hline
		$0$ & $0$ & $0$ & $0$\\ \hline
		$0$ & $0$ & $1$ & $1$\\ \hline
		$0$ & $1$ & $0$ & $1$\\ \hline
		$0$ & $1$ & $1$ & $0$\\ \hline
		$1$ & $0$ & $0$ & $1$\\ \hline
		$1$ & $0$ & $1$ & $0$\\ \hline
		$1$ & $1$ & $0$ & $0$\\ \hline
		$1$ & $1$ & $1$ & $1$\\ \hline
		\end{tabular}
		\caption[]{Reported borrowing for the binary subtraction}
	\end{table}
	Now, row by row, we add the outgoing borrowing $B_s$ of the difference $D=a+(-b)+(-B_e)$ which written so, then becomes a sum $S$:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}$\pmb{-B_e}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{a}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{-b}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{S(D)}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{B_s}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{minterms}} \\ \hline
		$0$ & $0$ & $0$ & $0$ & $0$ & $\bar{B}_e\bar{a}\bar{b}$\\ \hline
		$0$ & $0$ & $1$ & $1$ & $1$ & $\bar{B}_e\bar{a}b$\\ \hline
		$0$ & $1$ & $0$ & $1$ & $0$ & $\bar{B}_ea\bar{b}$\\ \hline
		$0$ & $1$ & $1$ & $0$ & $0$ & $\bar{B}_eab$\\ \hline
		$1$ & $0$ & $0$ & $1$ & $1$ & $B_e\bar{a}\bar{b}$\\ \hline
		$1$ & $0$ & $1$ & $0$ & $1$ & $B_e\bar{a}b$\\ \hline
		$1$ & $1$ & $0$ & $0$ & $0$ & $B_ea\bar{b}$\\ \hline
		$1$ & $1$ & $1$ & $1$ & $1$ & $B_eab$\\ \hline
		\end{tabular}
		\caption[]{Identification of subtraction minterms}
	\end{table}
	Therefore it comes $4$ minterms (that is, the terms for which $S(D)$ is non-zero at row $2$, $3$, $5$, $8$) such that the normal disjonctive form of the subtraction can be written:
	
	A trivial possible simplification is:
	
	It also comes for the outgoing borrowing the following minterms:
	
	So finally:
	
	To build physically with fundamental logic gates this subtraction, it is useful first to introduce an intermediary logic circuit (but it's not obliged, it's just for pedagogical reasons and by educational tradition!). 
	
	Let us for this purpose consider the truth table of the subtraction without incoming borrowing, named a "\NewTerm{half subtracter}\index{half subtracter}", and represented by the following "technical" drawing (don't forget that $b$ means in fact $-b$!):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1241); %set diagram left start at 0, and has height of 1241
		
		%Shape: Rectangle [id:dp28391896942955874] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (237,141.82) -- (366.82,141.82) -- (366.82,220.1) -- (237,220.1) -- cycle ;
		%Straight Lines [id:da27151765883645607] 
		\draw    (237,164.82) -- (161.82,164.82) ;
		%Straight Lines [id:da538273061551303] 
		\draw    (237,197.82) -- (161.82,197.82) ;
		%Straight Lines [id:da14464589117517423] 
		\draw    (442,164.82) -- (366.82,164.82) ;
		%Straight Lines [id:da019078496761083974] 
		\draw    (442,197.82) -- (366.82,197.82) ;
		%Shape: Xor Gate [id:dp355138807017108] 
		\draw   (277,262.82) -- (297,262.82) .. controls (310.95,263.36) and (323.42,275.05) .. (329,292.82) .. controls (323.42,310.59) and (310.95,322.29) .. (297,322.82) -- (277,322.82) .. controls (285.57,304.26) and (285.57,281.39) .. (277,262.82) -- cycle (265,272.82) -- (281,272.82) (265,312.82) -- (281,312.82) (329,292.82) -- (345,292.82) (273,262.82) .. controls (281.57,281.39) and (281.57,304.26) .. (273,322.82) ;
		%Straight Lines [id:da14939174532870902] 
		\draw    (265,272.82) -- (161.82,272.82) ;
		%Straight Lines [id:da34887443120219874] 
		\draw    (265,312.82) -- (161.82,312.82) ;
		%Straight Lines [id:da4726437326987931] 
		\draw    (439.82,292.82) -- (345,292.82) ;
		%Shape: And Gate [id:dp1781470594408605] 
		\draw   (342.82,361.82) -- (366.82,361.82) .. controls (380.06,361.82) and (390.82,375.27) .. (390.82,391.82) .. controls (390.82,408.38) and (380.06,421.82) .. (366.82,421.82) -- (342.82,421.82) -- (342.82,361.82) -- cycle (326.82,371.82) -- (342.82,371.82) (326.82,411.82) -- (342.82,411.82) (390.82,391.82) -- (406.82,391.82) ;
		%Straight Lines [id:da5920096393038845] 
		\draw    (326.82,371.82) -- (227.82,371.82) ;
		%Straight Lines [id:da7999664243752023] 
		\draw    (261,411.82) -- (211.82,411.82) ;
		%Straight Lines [id:da628939191040568] 
		\draw    (227.82,273.1) -- (227.82,371.82) ;
		%Straight Lines [id:da38143380428126084] 
		\draw    (211.82,312.1) -- (211.82,411.82) ;
		%Shape: Circle [id:dp854212877411628] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (224.18,273.1) .. controls (224.18,271.09) and (225.81,269.46) .. (227.82,269.46) .. controls (229.83,269.46) and (231.45,271.09) .. (231.45,273.1) .. controls (231.45,275.1) and (229.83,276.73) .. (227.82,276.73) .. controls (225.81,276.73) and (224.18,275.1) .. (224.18,273.1) -- cycle ;
		%Shape: Circle [id:dp9480383045675802] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (208.18,312.1) .. controls (208.18,310.09) and (209.81,308.46) .. (211.82,308.46) .. controls (213.83,308.46) and (215.45,310.09) .. (215.45,312.1) .. controls (215.45,314.1) and (213.83,315.73) .. (211.82,315.73) .. controls (209.81,315.73) and (208.18,314.1) .. (208.18,312.1) -- cycle ;
		%Straight Lines [id:da24053327805592684] 
		\draw    (439.82,391.82) -- (406.82,391.82) ;
		%Shape: Not/Inverter Gate [id:dp919387754390844] 
		\draw   (273.07,381.82) -- (309.29,411.82) -- (273.07,441.82) -- (273.07,381.82) -- cycle (261,411.82) -- (273.07,411.82) (316.53,411.82) -- (326.19,411.82) (309.29,411.82) .. controls (309.29,408.51) and (310.91,405.82) .. (312.91,405.82) .. controls (314.91,405.82) and (316.53,408.51) .. (316.53,411.82) .. controls (316.53,415.14) and (314.91,417.82) .. (312.91,417.82) .. controls (310.91,417.82) and (309.29,415.14) .. (309.29,411.82) -- cycle ;
		
		% Text Node
		\draw (267,163.82) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{49.76pt}\setlength\topsep{0pt}
		\begin{center}
		Half\\Subtracter
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (160,147.22) node [anchor=north west][inner sep=0.75pt]    {$a$};
		% Text Node
		\draw (161,180.22) node [anchor=north west][inner sep=0.75pt]    {$b$};
		% Text Node
		\draw (429,148.22) node [anchor=north west][inner sep=0.75pt]    {$D$};
		% Text Node
		\draw (429,180.22) node [anchor=north west][inner sep=0.75pt]    {$B_{s}$};
		% Text Node
		\draw (160,254.22) node [anchor=north west][inner sep=0.75pt]    {$a$};
		% Text Node
		\draw (161,294.22) node [anchor=north west][inner sep=0.75pt]    {$b$};
		% Text Node
		\draw (337,271.22) node [anchor=north west][inner sep=0.75pt]    {$D=a\oplus b=\overline{a} b+a\overline{b}$};
		% Text Node
		\draw (409,372.22) node [anchor=north west][inner sep=0.75pt]    {$B_{s} =\overline{a} b$};
		% Text Node
		\draw (413,393.82) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\footnotesize borrow out}};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Half subtracter logic diagram}
	\end{figure}
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{a}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{-b}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{S(D)}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{B_s}$} \\ \hline
		$0$ & $0$ & $0$ & $0$ \\ \hline
		$0$ & $1$ & $1$ & $1$ \\ \hline
		$1$ & $0$ & $1$ & $0$ \\ \hline
		$1$ & $1$ & $0$ & $0$ \\ \hline
		\end{tabular}
	\end{table}
	And now we can introduce the "\NewTerm{full subtracter}\index{full subtracter}" that substracts binary numbers and accounts for values carried in as well as out following the relation proved earlier above and is made of two half-adder. So here is for recall the minterms and we introduce the corresponding logical circuit:
	\begin{gather*}
		\begin{aligned}
		D&=B_e\oplus a\oplus b\\
		B_s&=\bar{a}b+B_e(\overline{a\oplus b})
		\end{aligned}
	\end{gather*}
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1241); %set diagram left start at 0, and has height of 1241
		
		%Straight Lines [id:da27151765883645607] 
		\draw    (335.82,81.82) -- (188.82,81.82) ;
		%Straight Lines [id:da538273061551303] 
		\draw    (335.82,121.82) -- (252.82,121.82) ;
		%Straight Lines [id:da14464589117517423] 
		\draw    (530.82,101.82) -- (415.82,101.82) ;
		%Straight Lines [id:da019078496761083974] 
		\draw    (365.82,221.1) -- (296.82,221.1) ;
		%Shape: Circle [id:dp854212877411628] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (293.73,81.82) .. controls (293.73,79.82) and (295.36,78.19) .. (297.36,78.19) .. controls (299.37,78.19) and (301,79.82) .. (301,81.82) .. controls (301,83.83) and (299.37,85.46) .. (297.36,85.46) .. controls (295.36,85.46) and (293.73,83.83) .. (293.73,81.82) -- cycle ;
		%Shape: Circle [id:dp9480383045675802] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (249.18,261.1) .. controls (249.18,259.09) and (250.81,257.46) .. (252.82,257.46) .. controls (254.83,257.46) and (256.45,259.09) .. (256.45,261.1) .. controls (256.45,263.1) and (254.83,264.73) .. (252.82,264.73) .. controls (250.81,264.73) and (249.18,263.1) .. (249.18,261.1) -- cycle ;
		%Shape: Xor Gate [id:dp9469807277379385] 
		\draw   (347.82,71.82) -- (367.82,71.82) .. controls (381.77,72.36) and (394.24,84.05) .. (399.82,101.82) .. controls (394.24,119.59) and (381.77,131.29) .. (367.82,131.82) -- (347.82,131.82) .. controls (356.39,113.26) and (356.39,90.39) .. (347.82,71.82) -- cycle (335.82,81.82) -- (351.82,81.82) (335.82,121.82) -- (351.82,121.82) (399.82,101.82) -- (415.82,101.82) (343.82,71.82) .. controls (352.39,90.39) and (352.39,113.26) .. (343.82,131.82) ;
		%Straight Lines [id:da8032346288598833] 
		\draw    (297.36,221.1) -- (297.36,85.46) ;
		%Shape: And Gate [id:dp8298014638141165] 
		\draw   (381.82,211.1) -- (405.82,211.1) .. controls (419.06,211.1) and (429.82,224.54) .. (429.82,241.1) .. controls (429.82,257.65) and (419.06,271.1) .. (405.82,271.1) -- (381.82,271.1) -- (381.82,211.1) -- cycle (365.82,221.1) -- (381.82,221.1) (365.82,261.1) -- (381.82,261.1) (429.82,241.1) -- (445.82,241.1) ;
		%Shape: Or Gate [id:dp8234488021952531] 
		\draw   (485.82,262.91) -- (505.82,262.91) .. controls (519.77,263.45) and (532.24,275.14) .. (537.82,292.91) .. controls (532.24,310.68) and (519.77,322.38) .. (505.82,322.91) -- (485.82,322.91) .. controls (494.39,304.35) and (494.39,281.48) .. (485.82,262.91) -- cycle (473.82,272.91) -- (489.82,272.91) (473.82,312.91) -- (489.82,312.91) (537.82,292.91) -- (553.82,292.91) ;
		%Straight Lines [id:da7611697227284033] 
		\draw    (473.82,241.1) -- (445.82,241.1) ;
		%Straight Lines [id:da021452907277445643] 
		\draw    (473.82,272.91) -- (473.82,241.1) ;
		%Straight Lines [id:da008863638118008721] 
		\draw    (594.82,292.91) -- (553.82,292.91) ;
		%Shape: Xor Gate [id:dp5824801446566352] 
		\draw   (140.82,231.1) -- (160.82,231.1) .. controls (174.77,231.64) and (187.24,243.33) .. (192.82,261.1) .. controls (187.24,278.87) and (174.77,290.56) .. (160.82,291.1) -- (140.82,291.1) .. controls (149.39,272.53) and (149.39,249.66) .. (140.82,231.1) -- cycle (128.82,241.1) -- (144.82,241.1) (128.82,281.1) -- (144.82,281.1) (192.82,261.1) -- (208.82,261.1) (136.82,231.1) .. controls (145.39,249.66) and (145.39,272.53) .. (136.82,291.1) ;
		%Straight Lines [id:da2627730394726162] 
		\draw    (285.82,261.1) -- (208.82,261.1) ;
		%Straight Lines [id:da10784923768194021] 
		\draw    (252.82,261.1) -- (252.82,121.82) ;
		%Straight Lines [id:da13755413276154416] 
		\draw    (68.82,241.1) -- (128.82,241.1) ;
		%Straight Lines [id:da16604428291575668] 
		\draw    (67.82,281.1) -- (128.82,281.1) ;
		%Shape: And Gate [id:dp2967635738043757] 
		\draw   (187.82,313.1) -- (211.82,313.1) .. controls (225.06,313.1) and (235.82,326.54) .. (235.82,343.1) .. controls (235.82,359.65) and (225.06,373.1) .. (211.82,373.1) -- (187.82,373.1) -- (187.82,313.1) -- cycle (171.82,323.1) -- (187.82,323.1) (171.82,363.1) -- (187.82,363.1) (235.82,343.1) -- (251.82,343.1) ;
		%Straight Lines [id:da6109567581691902] 
		\draw    (128.82,323.1) -- (128.82,241.1) ;
		%Straight Lines [id:da9878228544144645] 
		\draw    (171.82,323.1) -- (128.82,323.1) ;
		%Shape: Circle [id:dp024381940032086602] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (125.18,241.1) .. controls (125.18,239.09) and (126.81,237.46) .. (128.82,237.46) .. controls (130.83,237.46) and (132.45,239.09) .. (132.45,241.1) .. controls (132.45,243.1) and (130.83,244.73) .. (128.82,244.73) .. controls (126.81,244.73) and (125.18,243.1) .. (125.18,241.1) -- cycle ;
		%Straight Lines [id:da8742997026849633] 
		\draw    (92,363.1) -- (92,281.1) ;
		%Shape: Circle [id:dp173524754874391] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (88.37,281.1) .. controls (88.37,279.09) and (90,277.46) .. (92,277.46) .. controls (94.01,277.46) and (95.64,279.09) .. (95.64,281.1) .. controls (95.64,283.1) and (94.01,284.73) .. (92,284.73) .. controls (90,284.73) and (88.37,283.1) .. (88.37,281.1) -- cycle ;
		%Straight Lines [id:da16752890597975045] 
		\draw    (473.82,343.1) -- (249.45,343.1) ;
		%Straight Lines [id:da7973361357190278] 
		\draw    (473.82,312.91) -- (473.82,343.1) ;
		%Shape: Rectangle [id:dp6782008272549691] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (286.82,29.91) -- (439.82,29.91) -- (439.82,300.96) -- (286.82,300.96) -- cycle ;
		%Shape: Rectangle [id:dp7295482708175431] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (81.82,215.41) -- (243.95,215.41) -- (243.95,400.41) -- (81.82,400.41) -- cycle ;
		%Shape: Not/Inverter Gate [id:dp8163716749638783] 
		\draw   (300.63,231.1) -- (345.08,261.1) -- (300.63,291.1) -- (300.63,231.1) -- cycle (285.82,261.1) -- (300.63,261.1) (353.97,261.1) -- (365.82,261.1) (345.08,261.1) .. controls (345.08,257.78) and (347.07,255.1) .. (349.52,255.1) .. controls (351.98,255.1) and (353.97,257.78) .. (353.97,261.1) .. controls (353.97,264.41) and (351.98,267.1) .. (349.52,267.1) .. controls (347.07,267.1) and (345.08,264.41) .. (345.08,261.1) -- cycle ;
		%Shape: Not/Inverter Gate [id:dp6363201520353845] 
		\draw   (106.82,333.1) -- (151.26,363.1) -- (106.82,393.1) -- (106.82,333.1) -- cycle (92,363.1) -- (106.82,363.1) (160.15,363.1) -- (172,363.1) (151.26,363.1) .. controls (151.26,359.78) and (153.25,357.1) .. (155.71,357.1) .. controls (158.16,357.1) and (160.15,359.78) .. (160.15,363.1) .. controls (160.15,366.41) and (158.16,369.1) .. (155.71,369.1) .. controls (153.25,369.1) and (151.26,366.41) .. (151.26,363.1) -- cycle ;
		
		% Text Node
		\draw (68,263.22) node [anchor=north west][inner sep=0.75pt]    {$b$};
		% Text Node
		\draw (519,83.22) node [anchor=north west][inner sep=0.75pt]    {$D$};
		% Text Node
		\draw (185.82,84.82) node [anchor=north west][inner sep=0.75pt]   [align=left] {borrowing in};
		% Text Node
		\draw (443.82,103.82) node [anchor=north west][inner sep=0.75pt]   [align=left] {difference out};
		% Text Node
		\draw (579,275.22) node [anchor=north west][inner sep=0.75pt]    {$B_{s}$};
		% Text Node
		\draw (547.82,295.91) node [anchor=north west][inner sep=0.75pt]   [align=left] {borrow out};
		% Text Node
		\draw (67,221.22) node [anchor=north west][inner sep=0.75pt]    {$a$};
		% Text Node
		\draw (199.82,267.6) node [anchor=north west][inner sep=0.75pt]   [align=left] {subtraction};
		% Text Node
		\draw (326.82,326.1) node [anchor=north west][inner sep=0.75pt]   [align=left] {borrow};
		% Text Node
		\draw (315,10.64) node [anchor=north west][inner sep=0.75pt]   [align=left] {Half-Subtracter};
		% Text Node
		\draw (114.5,404.64) node [anchor=north west][inner sep=0.75pt]   [align=left] {Half-Subtracter};
		% Text Node
		\draw (442.82,221.82) node [anchor=north west][inner sep=0.75pt]   [align=left] {borrow};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Full subtracter logic diagram}
	\end{figure}
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}$\pmb{-B_e}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{a}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{-b}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{S(D)}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{B_s}$}\\ \hline
		$0$ & $0$ & $0$ & $0$ & $0$\\ \hline
		$0$ & $0$ & $1$ & $1$ & $1$\\ \hline
		$0$ & $1$ & $0$ & $1$ & $0$ \\ \hline
		$0$ & $1$ & $1$ & $0$ & $0$\\ \hline
		$1$ & $0$ & $0$ & $1$ & $1$\\ \hline
		$1$ & $0$ & $1$ & $0$ & $1$\\ \hline
		$1$ & $1$ & $0$ & $0$ & $0$\\ \hline
		$1$ & $1$ & $1$ & $1$ & $1$\\ \hline
		\end{tabular}
	\end{table}
	
	\pagebreak
	\paragraph{Prefix codes}\label{binary comparators}\mbox{}\\\\
	"\NewTerm{Binary comparators}\index{binary comparators}" are logic gates circuit used to compare two binary inputs. There are two types of binary comparators:
	\begin{itemize}
		\item "\NewTerm{Equality comparators}\index{equality comparators}" are used to check if the two binary inputs ( $A$ and $B$ ) are equal or not.
		
		\item "\NewTerm{Magnitude comparators}\index{magnitude comparators}" are used to fully compare two binary inputs $A$ and $B$ and produce three possible outpus if $A>B, A==B$ or $A<B$
	\end{itemize}
	We will focus in what follows on Magnitude Operators symbolized by:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,637); %set diagram left start at 0, and has height of 637
		
		%Shape: Rectangle [id:dp47458563125522457] 
		\draw  [fill={rgb, 255:red, 196; green, 192; blue, 192 }  ,fill opacity=1 ] (260,69) -- (366.37,69) -- (366.37,169.28) -- (260,169.28) -- cycle ;
		%Straight Lines [id:da15828897152641996] 
		\draw    (198.37,87) -- (260,87) ;
		%Straight Lines [id:da9454400997686445] 
		\draw    (198.37,111) -- (260,111) ;
		%Straight Lines [id:da08970059916951545] 
		\draw    (366,87) -- (427.63,87) ;
		%Straight Lines [id:da739004826887486] 
		\draw    (367,109) -- (428.63,109) ;
		%Straight Lines [id:da2052494459405294] 
		\draw    (367,131) -- (428.63,131) ;
		
		% Text Node
		\draw (263,116) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 1$-bit\\magnitude\\comparator};
		% Text Node
		\draw (179,79.4) node [anchor=north west][inner sep=0.75pt]    {$A$};
		% Text Node
		\draw (180.5,102.4) node [anchor=north west][inner sep=0.75pt]    {$B$};
		% Text Node
		\draw (431,78.4) node [anchor=north west][inner sep=0.75pt]    {$A< B$};
		% Text Node
		\draw (433,99.4) node [anchor=north west][inner sep=0.75pt]    {$A=B$};
		% Text Node
		\draw (434,122.4) node [anchor=north west][inner sep=0.75pt]    {$A >B$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{$1$-bit magnitude comparator}
	\end{figure}
	With the following truth table:
	\begin{table}[H]
		\centering
		\begin{tabular}{|cc|ccc|}
		\hline
		\rowcolor[HTML]{9B9B9B} 
		\multicolumn{2}{|c|}{\cellcolor[HTML]{9B9B9B}\textbf{INPUT}} & \multicolumn{3}{c|}{\cellcolor[HTML]{9B9B9B}\textbf{OUTPUT}} \\ \hline
		\rowcolor[HTML]{EFEFEF} 
		\multicolumn{1}{|c|}{\cellcolor[HTML]{EFEFEF}$A$} & $B$ & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}$A<B$} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}$A=B$} & $A>B$ \\ \hline
		\multicolumn{1}{|c|}{$0$} & $0$ & \multicolumn{1}{c|}{$0$} & \multicolumn{1}{c|}{$\mathbf{1}$} & $0$ \\ \hline
		\multicolumn{1}{|c|}{$1$} & $0$ & \multicolumn{1}{c|}{$0$} & \multicolumn{1}{c|}{$0$} & $\mathbf{1}$ \\ \hline
		\multicolumn{1}{|c|}{$0$} & $1$ & \multicolumn{1}{c|}{$\mathbf{1}$} & \multicolumn{1}{c|}{$0$} & $0$ \\ \hline
		\multicolumn{1}{|c|}{$1$} & $1$ & \multicolumn{1}{c|}{$0$} & \multicolumn{1}{c|}{$\mathbf{1}$} & $0$ \\ \hline
		\end{tabular}
		\caption{$1$-bit magnitude comparator truth table}
	\end{table}
	And here is the logic gates diagram for this circuit:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,637); %set diagram left start at 0, and has height of 637
		
		%Shape: Not/Inverter Gate [id:dp9020481405520102] 
		\draw   (116.81,44) -- (161.26,74) -- (116.81,104) -- (116.81,44) -- cycle (102,74) -- (116.81,74) (170.15,74) -- (182,74) (161.26,74) .. controls (161.26,70.69) and (163.25,68) .. (165.7,68) .. controls (168.16,68) and (170.15,70.69) .. (170.15,74) .. controls (170.15,77.31) and (168.16,80) .. (165.7,80) .. controls (163.25,80) and (161.26,77.31) .. (161.26,74) -- cycle ;
		%Shape: Not/Inverter Gate [id:dp16980697085615248] 
		\draw   (116.81,166) -- (161.26,196) -- (116.81,226) -- (116.81,166) -- cycle (102,196) -- (116.81,196) (170.15,196) -- (182,196) (161.26,196) .. controls (161.26,192.69) and (163.25,190) .. (165.7,190) .. controls (168.16,190) and (170.15,192.69) .. (170.15,196) .. controls (170.15,199.31) and (168.16,202) .. (165.7,202) .. controls (163.25,202) and (161.26,199.31) .. (161.26,196) -- cycle ;
		%Straight Lines [id:da16894012019276428] 
		\draw    (102,74) -- (60.37,74) ;
		%Straight Lines [id:da8374094292376784] 
		\draw    (102,196) -- (60.37,196) ;
		%Shape: And Gate [id:dp5881534288514898] 
		\draw   (198,64) -- (222,64) .. controls (235.25,64) and (246,77.44) .. (246,94) .. controls (246,110.56) and (235.25,124) .. (222,124) -- (198,124) -- (198,64) -- cycle (182,74) -- (198,74) (182,114) -- (198,114) (246,94) -- (262,94) ;
		%Shape: And Gate [id:dp576195276798015] 
		\draw   (198,146) -- (222,146) .. controls (235.25,146) and (246,159.44) .. (246,176) .. controls (246,192.56) and (235.25,206) .. (222,206) -- (198,206) -- (198,146) -- cycle (182,156) -- (198,156) (182,196) -- (198,196) (246,176) -- (262,176) ;
		%Straight Lines [id:da8482263880241443] 
		\draw    (81.18,196) -- (81.18,171.82) ;
		%Straight Lines [id:da10679661502558324] 
		\draw    (182,114) -- (81.18,171.82) ;
		%Straight Lines [id:da3326930754543709] 
		\draw    (182,156) -- (81.18,99) ;
		%Straight Lines [id:da23905847280327874] 
		\draw    (81.18,99) -- (81.18,74.82) ;
		%Straight Lines [id:da2005808566835927] 
		\draw    (262,94) -- (365.37,94) ;
		%Straight Lines [id:da0718676231448967] 
		\draw    (262,176) -- (365.37,176) ;
		%Shape: Circle [id:dp2245321681843775] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (77.77,196) .. controls (77.77,194.12) and (79.3,192.59) .. (81.18,192.59) .. controls (83.07,192.59) and (84.6,194.12) .. (84.6,196) .. controls (84.6,197.88) and (83.07,199.41) .. (81.18,199.41) .. controls (79.3,199.41) and (77.77,197.88) .. (77.77,196) -- cycle ;
		%Shape: Circle [id:dp250678955296594] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (77.77,74.82) .. controls (77.77,72.94) and (79.3,71.41) .. (81.18,71.41) .. controls (83.07,71.41) and (84.6,72.94) .. (84.6,74.82) .. controls (84.6,76.71) and (83.07,78.24) .. (81.18,78.24) .. controls (79.3,78.24) and (77.77,76.71) .. (77.77,74.82) -- cycle ;
		%Shape: Nor Gate [id:dp3640757115720945] 
		\draw   (295.11,104) -- (313.63,104) .. controls (326.55,104.54) and (338.09,116.23) .. (343.26,134) .. controls (338.09,151.77) and (326.55,163.46) .. (313.63,164) -- (295.11,164) .. controls (303.05,145.44) and (303.05,122.56) .. (295.11,104) -- cycle (284,114) -- (298.81,114) (284,154) -- (298.81,154) (352.15,134) -- (364,134) (343.26,134) .. controls (343.26,130.69) and (345.25,128) .. (347.7,128) .. controls (350.16,128) and (352.15,130.69) .. (352.15,134) .. controls (352.15,137.31) and (350.16,140) .. (347.7,140) .. controls (345.25,140) and (343.26,137.31) .. (343.26,134) -- cycle ;
		%Straight Lines [id:da6458285976433373] 
		\draw    (284,114) -- (284,93.82) ;
		%Straight Lines [id:da0775355514642948] 
		\draw    (284,175.82) -- (284,154) ;
		%Shape: Circle [id:dp8479859431516372] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (280.59,93.82) .. controls (280.59,91.94) and (282.12,90.41) .. (284,90.41) .. controls (285.88,90.41) and (287.41,91.94) .. (287.41,93.82) .. controls (287.41,95.71) and (285.88,97.24) .. (284,97.24) .. controls (282.12,97.24) and (280.59,95.71) .. (280.59,93.82) -- cycle ;
		%Shape: Circle [id:dp7617872459154513] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (280.59,175.82) .. controls (280.59,173.94) and (282.12,172.41) .. (284,172.41) .. controls (285.88,172.41) and (287.41,173.94) .. (287.41,175.82) .. controls (287.41,177.71) and (285.88,179.24) .. (284,179.24) .. controls (282.12,179.24) and (280.59,177.71) .. (280.59,175.82) -- cycle ;
		
		% Text Node
		\draw (53,55.4) node [anchor=north west][inner sep=0.75pt]    {$A$};
		% Text Node
		\draw (54.5,176.4) node [anchor=north west][inner sep=0.75pt]    {$B$};
		% Text Node
		\draw (367,85.4) node [anchor=north west][inner sep=0.75pt]    {$A< B$};
		% Text Node
		\draw (366,125.4) node [anchor=north west][inner sep=0.75pt]    {$A=B$};
		% Text Node
		\draw (369,168.4) node [anchor=north west][inner sep=0.75pt]    {$A >B$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{$1$-bit magnitude comparator}
	\end{figure}
	We can now combine several of the above diagrams to create a $4$-bit magnitude comparator as follows (thanks to Karnaugh table simplification process):
	\begin{figure}[H]
		\centering
		\resizebox{\textwidth}{!}{
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1182); %set diagram left start at 0, and has height of 1182
		
		%Shape: Not/Inverter Gate [id:dp9020481405520102] 
		\draw   (77.81,55) -- (122.26,85) -- (77.81,115) -- (77.81,55) -- cycle (63,85) -- (77.81,85) (131.15,85) -- (143,85) (122.26,85) .. controls (122.26,81.69) and (124.25,79) .. (126.7,79) .. controls (129.16,79) and (131.15,81.69) .. (131.15,85) .. controls (131.15,88.31) and (129.16,91) .. (126.7,91) .. controls (124.25,91) and (122.26,88.31) .. (122.26,85) -- cycle ;
		%Shape: Not/Inverter Gate [id:dp16980697085615248] 
		\draw   (77.81,177) -- (122.26,207) -- (77.81,237) -- (77.81,177) -- cycle (63,207) -- (77.81,207) (131.15,207) -- (143,207) (122.26,207) .. controls (122.26,203.69) and (124.25,201) .. (126.7,201) .. controls (129.16,201) and (131.15,203.69) .. (131.15,207) .. controls (131.15,210.31) and (129.16,213) .. (126.7,213) .. controls (124.25,213) and (122.26,210.31) .. (122.26,207) -- cycle ;
		%Straight Lines [id:da16894012019276428] 
		\draw    (63,85) -- (21.37,85) ;
		%Straight Lines [id:da8374094292376784] 
		\draw    (63,207) -- (21.37,207) ;
		%Shape: And Gate [id:dp5881534288514898] 
		\draw   (159,75) -- (183,75) .. controls (196.25,75) and (207,88.44) .. (207,105) .. controls (207,121.56) and (196.25,135) .. (183,135) -- (159,135) -- (159,75) -- cycle (143,85) -- (159,85) (143,125) -- (159,125) (207,105) -- (223,105) ;
		%Shape: And Gate [id:dp576195276798015] 
		\draw   (159,157) -- (183,157) .. controls (196.25,157) and (207,170.44) .. (207,187) .. controls (207,203.56) and (196.25,217) .. (183,217) -- (159,217) -- (159,157) -- cycle (143,167) -- (159,167) (143,207) -- (159,207) (207,187) -- (223,187) ;
		%Straight Lines [id:da8482263880241443] 
		\draw    (42.18,207) -- (42.18,182.82) ;
		%Straight Lines [id:da10679661502558324] 
		\draw    (143,125) -- (42.18,182.82) ;
		%Straight Lines [id:da3326930754543709] 
		\draw    (143,167) -- (42.18,110) ;
		%Straight Lines [id:da23905847280327874] 
		\draw    (42.18,110) -- (42.18,85.82) ;
		%Straight Lines [id:da2005808566835927] 
		\draw    (223,105) -- (557.37,105) ;
		%Straight Lines [id:da0718676231448967] 
		\draw    (227,186.82) -- (227,221.1) ;
		%Shape: Circle [id:dp2245321681843775] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (38.77,207) .. controls (38.77,205.12) and (40.3,203.59) .. (42.18,203.59) .. controls (44.07,203.59) and (45.6,205.12) .. (45.6,207) .. controls (45.6,208.88) and (44.07,210.41) .. (42.18,210.41) .. controls (40.3,210.41) and (38.77,208.88) .. (38.77,207) -- cycle ;
		%Shape: Circle [id:dp250678955296594] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (38.77,85.82) .. controls (38.77,83.94) and (40.3,82.41) .. (42.18,82.41) .. controls (44.07,82.41) and (45.6,83.94) .. (45.6,85.82) .. controls (45.6,87.71) and (44.07,89.24) .. (42.18,89.24) .. controls (40.3,89.24) and (38.77,87.71) .. (38.77,85.82) -- cycle ;
		%Shape: Nor Gate [id:dp3640757115720945] 
		\draw   (238.11,115) -- (256.63,115) .. controls (269.55,115.54) and (281.09,127.23) .. (286.26,145) .. controls (281.09,162.77) and (269.55,174.46) .. (256.63,175) -- (238.11,175) .. controls (246.05,156.44) and (246.05,133.56) .. (238.11,115) -- cycle (227,125) -- (241.81,125) (227,165) -- (241.81,165) (295.15,145) -- (307,145) (286.26,145) .. controls (286.26,141.69) and (288.25,139) .. (290.7,139) .. controls (293.16,139) and (295.15,141.69) .. (295.15,145) .. controls (295.15,148.31) and (293.16,151) .. (290.7,151) .. controls (288.25,151) and (286.26,148.31) .. (286.26,145) -- cycle ;
		%Straight Lines [id:da6458285976433373] 
		\draw    (227,125) -- (227,104.82) ;
		%Straight Lines [id:da0775355514642948] 
		\draw    (227,186.82) -- (227,165) ;
		%Shape: Circle [id:dp8479859431516372] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (223.59,104.82) .. controls (223.59,102.94) and (225.12,101.41) .. (227,101.41) .. controls (228.88,101.41) and (230.41,102.94) .. (230.41,104.82) .. controls (230.41,106.71) and (228.88,108.24) .. (227,108.24) .. controls (225.12,108.24) and (223.59,106.71) .. (223.59,104.82) -- cycle ;
		%Shape: Circle [id:dp7617872459154513] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (223.59,186.82) .. controls (223.59,184.94) and (225.12,183.41) .. (227,183.41) .. controls (228.88,183.41) and (230.41,184.94) .. (230.41,186.82) .. controls (230.41,188.71) and (228.88,190.24) .. (227,190.24) .. controls (225.12,190.24) and (223.59,188.71) .. (223.59,186.82) -- cycle ;
		%Shape: Not/Inverter Gate [id:dp6045677238405309] 
		\draw   (77.81,253) -- (122.26,283) -- (77.81,313) -- (77.81,253) -- cycle (63,283) -- (77.81,283) (131.15,283) -- (143,283) (122.26,283) .. controls (122.26,279.69) and (124.25,277) .. (126.7,277) .. controls (129.16,277) and (131.15,279.69) .. (131.15,283) .. controls (131.15,286.31) and (129.16,289) .. (126.7,289) .. controls (124.25,289) and (122.26,286.31) .. (122.26,283) -- cycle ;
		%Shape: Not/Inverter Gate [id:dp19179934381086894] 
		\draw   (77.81,375) -- (122.26,405) -- (77.81,435) -- (77.81,375) -- cycle (63,405) -- (77.81,405) (131.15,405) -- (143,405) (122.26,405) .. controls (122.26,401.69) and (124.25,399) .. (126.7,399) .. controls (129.16,399) and (131.15,401.69) .. (131.15,405) .. controls (131.15,408.31) and (129.16,411) .. (126.7,411) .. controls (124.25,411) and (122.26,408.31) .. (122.26,405) -- cycle ;
		%Straight Lines [id:da8660396888911013] 
		\draw    (63,283) -- (21.37,283) ;
		%Straight Lines [id:da22652480998832036] 
		\draw    (63,405) -- (21.37,405) ;
		%Shape: And Gate [id:dp04651736450930377] 
		\draw   (159,273) -- (183,273) .. controls (196.25,273) and (207,286.44) .. (207,303) .. controls (207,319.56) and (196.25,333) .. (183,333) -- (159,333) -- (159,273) -- cycle (143,283) -- (159,283) (143,323) -- (159,323) (207,303) -- (223,303) ;
		%Shape: And Gate [id:dp5452856689276333] 
		\draw   (159,355) -- (183,355) .. controls (196.25,355) and (207,368.44) .. (207,385) .. controls (207,401.56) and (196.25,415) .. (183,415) -- (159,415) -- (159,355) -- cycle (143,365) -- (159,365) (143,405) -- (159,405) (207,385) -- (223,385) ;
		%Straight Lines [id:da830363551587056] 
		\draw    (42.18,405) -- (42.18,380.82) ;
		%Straight Lines [id:da3421991637369415] 
		\draw    (143,323) -- (42.18,380.82) ;
		%Straight Lines [id:da33680646724118257] 
		\draw    (143,365) -- (42.18,308) ;
		%Straight Lines [id:da8352004086721008] 
		\draw    (42.18,308) -- (42.18,283.82) ;
		%Straight Lines [id:da8021458753763406] 
		\draw    (223,303) -- (259.63,303) ;
		%Straight Lines [id:da5604953281186651] 
		\draw    (223,385) -- (332.37,385) ;
		%Shape: Circle [id:dp6508753172712665] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (38.77,405) .. controls (38.77,403.12) and (40.3,401.59) .. (42.18,401.59) .. controls (44.07,401.59) and (45.6,403.12) .. (45.6,405) .. controls (45.6,406.88) and (44.07,408.41) .. (42.18,408.41) .. controls (40.3,408.41) and (38.77,406.88) .. (38.77,405) -- cycle ;
		%Shape: Circle [id:dp28950314409969] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (38.77,283.82) .. controls (38.77,281.94) and (40.3,280.41) .. (42.18,280.41) .. controls (44.07,280.41) and (45.6,281.94) .. (45.6,283.82) .. controls (45.6,285.71) and (44.07,287.24) .. (42.18,287.24) .. controls (40.3,287.24) and (38.77,285.71) .. (38.77,283.82) -- cycle ;
		%Shape: Nor Gate [id:dp13757929938436142] 
		\draw   (238.11,313) -- (256.63,313) .. controls (269.55,313.54) and (281.09,325.23) .. (286.26,343) .. controls (281.09,360.77) and (269.55,372.46) .. (256.63,373) -- (238.11,373) .. controls (246.05,354.44) and (246.05,331.56) .. (238.11,313) -- cycle (227,323) -- (241.81,323) (227,363) -- (241.81,363) (295.15,343) -- (307,343) (286.26,343) .. controls (286.26,339.69) and (288.25,337) .. (290.7,337) .. controls (293.16,337) and (295.15,339.69) .. (295.15,343) .. controls (295.15,346.31) and (293.16,349) .. (290.7,349) .. controls (288.25,349) and (286.26,346.31) .. (286.26,343) -- cycle ;
		%Straight Lines [id:da5620626093730439] 
		\draw    (227,323) -- (227,302.82) ;
		%Straight Lines [id:da05744357520593257] 
		\draw    (227,384.82) -- (227,363) ;
		%Shape: Circle [id:dp912003327245706] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (223.59,302.82) .. controls (223.59,300.94) and (225.12,299.41) .. (227,299.41) .. controls (228.88,299.41) and (230.41,300.94) .. (230.41,302.82) .. controls (230.41,304.71) and (228.88,306.24) .. (227,306.24) .. controls (225.12,306.24) and (223.59,304.71) .. (223.59,302.82) -- cycle ;
		%Shape: Circle [id:dp8741634152916917] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (223.59,384.82) .. controls (223.59,382.94) and (225.12,381.41) .. (227,381.41) .. controls (228.88,381.41) and (230.41,382.94) .. (230.41,384.82) .. controls (230.41,386.71) and (228.88,388.24) .. (227,388.24) .. controls (225.12,388.24) and (223.59,386.71) .. (223.59,384.82) -- cycle ;
		%Shape: Not/Inverter Gate [id:dp9604629146544508] 
		\draw   (77.81,451) -- (122.26,481) -- (77.81,511) -- (77.81,451) -- cycle (63,481) -- (77.81,481) (131.15,481) -- (143,481) (122.26,481) .. controls (122.26,477.69) and (124.25,475) .. (126.7,475) .. controls (129.16,475) and (131.15,477.69) .. (131.15,481) .. controls (131.15,484.31) and (129.16,487) .. (126.7,487) .. controls (124.25,487) and (122.26,484.31) .. (122.26,481) -- cycle ;
		%Shape: Not/Inverter Gate [id:dp19137500658762918] 
		\draw   (77.81,573) -- (122.26,603) -- (77.81,633) -- (77.81,573) -- cycle (63,603) -- (77.81,603) (131.15,603) -- (143,603) (122.26,603) .. controls (122.26,599.69) and (124.25,597) .. (126.7,597) .. controls (129.16,597) and (131.15,599.69) .. (131.15,603) .. controls (131.15,606.31) and (129.16,609) .. (126.7,609) .. controls (124.25,609) and (122.26,606.31) .. (122.26,603) -- cycle ;
		%Straight Lines [id:da42323025002705883] 
		\draw    (63,481) -- (21.37,481) ;
		%Straight Lines [id:da3439925930777772] 
		\draw    (63,603) -- (21.37,603) ;
		%Shape: And Gate [id:dp6719707959881185] 
		\draw   (159,471) -- (183,471) .. controls (196.25,471) and (207,484.44) .. (207,501) .. controls (207,517.56) and (196.25,531) .. (183,531) -- (159,531) -- (159,471) -- cycle (143,481) -- (159,481) (143,521) -- (159,521) (207,501) -- (223,501) ;
		%Shape: And Gate [id:dp5174345301917209] 
		\draw   (159,553) -- (183,553) .. controls (196.25,553) and (207,566.44) .. (207,583) .. controls (207,599.56) and (196.25,613) .. (183,613) -- (159,613) -- (159,553) -- cycle (143,563) -- (159,563) (143,603) -- (159,603) (207,583) -- (223,583) ;
		%Straight Lines [id:da3380281324385179] 
		\draw    (42.18,603) -- (42.18,578.82) ;
		%Straight Lines [id:da8034559634119722] 
		\draw    (143,521) -- (42.18,578.82) ;
		%Straight Lines [id:da8152087086773601] 
		\draw    (143,563) -- (42.18,506) ;
		%Straight Lines [id:da3583909602043789] 
		\draw    (42.18,506) -- (42.18,481.82) ;
		%Straight Lines [id:da655118128625999] 
		\draw    (223,501) -- (403.37,501) ;
		%Straight Lines [id:da10408746017372428] 
		\draw    (223,583) -- (326.37,583) ;
		%Shape: Circle [id:dp5012375401601004] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (38.77,603) .. controls (38.77,601.12) and (40.3,599.59) .. (42.18,599.59) .. controls (44.07,599.59) and (45.6,601.12) .. (45.6,603) .. controls (45.6,604.88) and (44.07,606.41) .. (42.18,606.41) .. controls (40.3,606.41) and (38.77,604.88) .. (38.77,603) -- cycle ;
		%Shape: Circle [id:dp8537491860249731] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (38.77,481.82) .. controls (38.77,479.94) and (40.3,478.41) .. (42.18,478.41) .. controls (44.07,478.41) and (45.6,479.94) .. (45.6,481.82) .. controls (45.6,483.71) and (44.07,485.24) .. (42.18,485.24) .. controls (40.3,485.24) and (38.77,483.71) .. (38.77,481.82) -- cycle ;
		%Shape: Nor Gate [id:dp596911878356523] 
		\draw   (238.11,511) -- (256.63,511) .. controls (269.55,511.54) and (281.09,523.23) .. (286.26,541) .. controls (281.09,558.77) and (269.55,570.46) .. (256.63,571) -- (238.11,571) .. controls (246.05,552.44) and (246.05,529.56) .. (238.11,511) -- cycle (227,521) -- (241.81,521) (227,561) -- (241.81,561) (295.15,541) -- (307,541) (286.26,541) .. controls (286.26,537.69) and (288.25,535) .. (290.7,535) .. controls (293.16,535) and (295.15,537.69) .. (295.15,541) .. controls (295.15,544.31) and (293.16,547) .. (290.7,547) .. controls (288.25,547) and (286.26,544.31) .. (286.26,541) -- cycle ;
		%Straight Lines [id:da2788934731611905] 
		\draw    (227,521) -- (227,500.82) ;
		%Straight Lines [id:da7159680616521669] 
		\draw    (227,582.82) -- (227,561) ;
		%Shape: Circle [id:dp1691840716591977] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (223.59,500.82) .. controls (223.59,498.94) and (225.12,497.41) .. (227,497.41) .. controls (228.88,497.41) and (230.41,498.94) .. (230.41,500.82) .. controls (230.41,502.71) and (228.88,504.24) .. (227,504.24) .. controls (225.12,504.24) and (223.59,502.71) .. (223.59,500.82) -- cycle ;
		%Shape: Circle [id:dp28436563387079694] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (223.59,582.82) .. controls (223.59,580.94) and (225.12,579.41) .. (227,579.41) .. controls (228.88,579.41) and (230.41,580.94) .. (230.41,582.82) .. controls (230.41,584.71) and (228.88,586.24) .. (227,586.24) .. controls (225.12,586.24) and (223.59,584.71) .. (223.59,582.82) -- cycle ;
		%Shape: Not/Inverter Gate [id:dp5982740960790853] 
		\draw   (77.81,650) -- (122.26,680) -- (77.81,710) -- (77.81,650) -- cycle (63,680) -- (77.81,680) (131.15,680) -- (143,680) (122.26,680) .. controls (122.26,676.69) and (124.25,674) .. (126.7,674) .. controls (129.16,674) and (131.15,676.69) .. (131.15,680) .. controls (131.15,683.31) and (129.16,686) .. (126.7,686) .. controls (124.25,686) and (122.26,683.31) .. (122.26,680) -- cycle ;
		%Shape: Not/Inverter Gate [id:dp9903293749318856] 
		\draw   (77.81,772) -- (122.26,802) -- (77.81,832) -- (77.81,772) -- cycle (63,802) -- (77.81,802) (131.15,802) -- (143,802) (122.26,802) .. controls (122.26,798.69) and (124.25,796) .. (126.7,796) .. controls (129.16,796) and (131.15,798.69) .. (131.15,802) .. controls (131.15,805.31) and (129.16,808) .. (126.7,808) .. controls (124.25,808) and (122.26,805.31) .. (122.26,802) -- cycle ;
		%Straight Lines [id:da25032033246412966] 
		\draw    (63,680) -- (21.37,680) ;
		%Straight Lines [id:da6685791121464704] 
		\draw    (63,802) -- (21.37,802) ;
		%Shape: And Gate [id:dp5009221336820602] 
		\draw   (159,670) -- (183,670) .. controls (196.25,670) and (207,683.44) .. (207,700) .. controls (207,716.56) and (196.25,730) .. (183,730) -- (159,730) -- (159,670) -- cycle (143,680) -- (159,680) (143,720) -- (159,720) (207,700) -- (223,700) ;
		%Shape: And Gate [id:dp4421557607809481] 
		\draw   (159,752) -- (183,752) .. controls (196.25,752) and (207,765.44) .. (207,782) .. controls (207,798.56) and (196.25,812) .. (183,812) -- (159,812) -- (159,752) -- cycle (143,762) -- (159,762) (143,802) -- (159,802) (207,782) -- (223,782) ;
		%Straight Lines [id:da03000008623914896] 
		\draw    (42.18,802) -- (42.18,777.82) ;
		%Straight Lines [id:da7050752355153205] 
		\draw    (143,720) -- (42.18,777.82) ;
		%Straight Lines [id:da45948335828359266] 
		\draw    (143,762) -- (42.18,705) ;
		%Straight Lines [id:da14759420049022642] 
		\draw    (42.18,705) -- (42.18,680.82) ;
		%Straight Lines [id:da4255751653588362] 
		\draw    (227,699.82) -- (328.37,699.82) ;
		%Straight Lines [id:da2744766545929107] 
		\draw    (223,782) -- (464.37,782) ;
		%Shape: Circle [id:dp1461541726533624] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (38.77,802) .. controls (38.77,800.12) and (40.3,798.59) .. (42.18,798.59) .. controls (44.07,798.59) and (45.6,800.12) .. (45.6,802) .. controls (45.6,803.88) and (44.07,805.41) .. (42.18,805.41) .. controls (40.3,805.41) and (38.77,803.88) .. (38.77,802) -- cycle ;
		%Shape: Circle [id:dp5170944814130558] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (38.77,680.82) .. controls (38.77,678.94) and (40.3,677.41) .. (42.18,677.41) .. controls (44.07,677.41) and (45.6,678.94) .. (45.6,680.82) .. controls (45.6,682.71) and (44.07,684.24) .. (42.18,684.24) .. controls (40.3,684.24) and (38.77,682.71) .. (38.77,680.82) -- cycle ;
		%Shape: Nor Gate [id:dp8332063501134399] 
		\draw   (238.11,710) -- (256.63,710) .. controls (269.55,710.54) and (281.09,722.23) .. (286.26,740) .. controls (281.09,757.77) and (269.55,769.46) .. (256.63,770) -- (238.11,770) .. controls (246.05,751.44) and (246.05,728.56) .. (238.11,710) -- cycle (227,720) -- (241.81,720) (227,760) -- (241.81,760) (295.15,740) -- (307,740) (286.26,740) .. controls (286.26,736.69) and (288.25,734) .. (290.7,734) .. controls (293.16,734) and (295.15,736.69) .. (295.15,740) .. controls (295.15,743.31) and (293.16,746) .. (290.7,746) .. controls (288.25,746) and (286.26,743.31) .. (286.26,740) -- cycle ;
		%Straight Lines [id:da8946513776513207] 
		\draw    (227,720) -- (227,699.82) ;
		%Straight Lines [id:da18628896771843717] 
		\draw    (227,781.82) -- (227,760) ;
		%Shape: Circle [id:dp7101985809723244] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (223.59,699.82) .. controls (223.59,697.94) and (225.12,696.41) .. (227,696.41) .. controls (228.88,696.41) and (230.41,697.94) .. (230.41,699.82) .. controls (230.41,701.71) and (228.88,703.24) .. (227,703.24) .. controls (225.12,703.24) and (223.59,701.71) .. (223.59,699.82) -- cycle ;
		%Shape: Circle [id:dp5329679334825808] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (223.59,781.82) .. controls (223.59,779.94) and (225.12,778.41) .. (227,778.41) .. controls (228.88,778.41) and (230.41,779.94) .. (230.41,781.82) .. controls (230.41,783.71) and (228.88,785.24) .. (227,785.24) .. controls (225.12,785.24) and (223.59,783.71) .. (223.59,781.82) -- cycle ;
		%Shape: Or Gate [id:dp009113048067728391] 
		\draw   (569.37,95) -- (589.37,95) .. controls (603.32,95.54) and (615.79,107.23) .. (621.37,125) .. controls (615.79,142.77) and (603.32,154.46) .. (589.37,155) -- (569.37,155) .. controls (577.94,136.44) and (577.94,113.56) .. (569.37,95) -- cycle (557.37,105) -- (573.37,105) (557.37,145) -- (573.37,145) (621.37,125) -- (637.37,125) ;
		%Shape: And Gate [id:dp4189594373723775] 
		\draw   (323,135) -- (347,135) .. controls (360.25,135) and (371,148.44) .. (371,165) .. controls (371,181.56) and (360.25,195) .. (347,195) -- (323,195) -- (323,135) -- cycle (307,145) -- (323,145) (307,185) -- (323,185) (371,165) -- (387,165) ;
		%Straight Lines [id:da07497751101786965] 
		\draw    (227,221.1) -- (557.37,221.1) ;
		%Shape: And Gate [id:dp3224708347543235] 
		\draw   (424.37,135.28) -- (448.37,135.28) .. controls (461.62,135.28) and (472.37,148.72) .. (472.37,165.28) .. controls (472.37,181.84) and (461.62,195.28) .. (448.37,195.28) -- (424.37,195.28) -- (424.37,135.28) -- cycle (408.37,145.28) -- (424.37,145.28) (408.37,185.28) -- (424.37,185.28) (472.37,165.28) -- (488.37,165.28) ;
		%Straight Lines [id:da09566108620476554] 
		\draw    (488.37,130.28) -- (575.37,130.28) ;
		%Straight Lines [id:da48036299252382886] 
		\draw    (488.37,130.28) -- (488.37,165.28) ;
		%Shape: Or Gate [id:dp09501451965568375] 
		\draw   (569.37,254) -- (589.37,254) .. controls (603.32,254.54) and (615.79,266.23) .. (621.37,284) .. controls (615.79,301.77) and (603.32,313.46) .. (589.37,314) -- (569.37,314) .. controls (577.94,295.44) and (577.94,272.56) .. (569.37,254) -- cycle (557.37,264) -- (573.37,264) (557.37,304) -- (573.37,304) (621.37,284) -- (637.37,284) ;
		%Straight Lines [id:da42431252711100464] 
		\draw    (557.37,264) -- (557.37,221.28) ;
		%Shape: And Gate [id:dp30246238897895306] 
		\draw   (323,253) -- (347,253) .. controls (360.25,253) and (371,266.44) .. (371,283) .. controls (371,299.56) and (360.25,313) .. (347,313) -- (323,313) -- (323,253) -- cycle (307,263) -- (323,263) (307,303) -- (323,303) (371,283) -- (387,283) ;
		%Straight Lines [id:da5584230733172557] 
		\draw    (259.63,185) -- (307,185) ;
		%Straight Lines [id:da8732258245585267] 
		\draw    (259.63,185) -- (259.63,302.28) ;
		%Straight Lines [id:da8927095366492199] 
		\draw    (387,120.28) -- (387,165) ;
		%Straight Lines [id:da8287728048257916] 
		\draw    (387,120.28) -- (575.37,120.28) ;
		%Straight Lines [id:da2654731311668126] 
		\draw    (307,343) -- (307,303) ;
		%Straight Lines [id:da3222701064973301] 
		\draw    (303,263) -- (303,145) ;
		%Straight Lines [id:da7862853239610361] 
		\draw    (387,283) -- (396.37,283) ;
		%Straight Lines [id:da8640746101246224] 
		\draw    (408.37,145.28) -- (396.37,145.28) ;
		%Straight Lines [id:da8332333461507906] 
		\draw    (396.37,283) -- (396.37,145.28) ;
		%Shape: And Gate [id:dp7666345261157439] 
		\draw   (429.37,289.28) -- (453.37,289.28) .. controls (466.62,289.28) and (477.37,302.72) .. (477.37,319.28) .. controls (477.37,335.84) and (466.62,349.28) .. (453.37,349.28) -- (429.37,349.28) -- (429.37,289.28) -- cycle (413.37,299.28) -- (429.37,299.28) (413.37,339.28) -- (429.37,339.28) (477.37,319.28) -- (493.37,319.28) ;
		%Straight Lines [id:da9042750209784696] 
		\draw    (315,238.28) -- (315,145) ;
		%Straight Lines [id:da7055001822799647] 
		\draw    (315,238.28) -- (413.37,238.28) ;
		%Straight Lines [id:da9453216581359252] 
		\draw    (413.37,299.28) -- (413.37,238.28) ;
		%Straight Lines [id:da4820190494060337] 
		\draw    (493.37,319.28) -- (493.37,276.28) ;
		%Straight Lines [id:da40589685560362976] 
		\draw    (575.37,276.28) -- (493.37,276.28) ;
		%Shape: And Gate [id:dp5545241116904616] 
		\draw   (472.37,353.28) -- (496.37,353.28) .. controls (509.62,353.28) and (520.37,366.72) .. (520.37,383.28) .. controls (520.37,399.84) and (509.62,413.28) .. (496.37,413.28) -- (472.37,413.28) -- (472.37,353.28) -- cycle (456.37,363.28) -- (472.37,363.28) (456.37,403.28) -- (472.37,403.28) (520.37,383.28) -- (536.37,383.28) ;
		%Straight Lines [id:da2371290212580488] 
		\draw    (536.37,383.28) -- (536.37,288.28) ;
		%Straight Lines [id:da2905576991515666] 
		\draw    (576.37,288.28) -- (536.37,288.28) ;
		%Straight Lines [id:da11921516052408654] 
		\draw    (332.37,339.28) -- (413.37,339.28) ;
		%Straight Lines [id:da8564332462104522] 
		\draw    (332.37,339.28) -- (332.37,385.28) ;
		%Straight Lines [id:da03324082223826452] 
		\draw    (396.37,283) -- (396.37,363.28) ;
		%Straight Lines [id:da4159214500811599] 
		\draw    (396.37,363.28) -- (456.37,363.28) ;
		%Straight Lines [id:da4027162459784299] 
		\draw    (403.37,501) -- (403.37,185.28) ;
		%Straight Lines [id:da1934162338457761] 
		\draw    (326.37,403.28) -- (456.37,403.28) ;
		%Straight Lines [id:da031988288553281174] 
		\draw    (326.37,583) -- (326.37,403.28) ;
		%Shape: And Gate [id:dp5508629912437475] 
		\draw   (414.37,421.28) -- (438.37,421.28) .. controls (451.62,421.28) and (462.37,434.72) .. (462.37,451.28) .. controls (462.37,467.84) and (451.62,481.28) .. (438.37,481.28) -- (414.37,481.28) -- (414.37,421.28) -- cycle (398.37,431.28) -- (414.37,431.28) (398.37,471.28) -- (414.37,471.28) (462.37,451.28) -- (478.37,451.28) ;
		%Straight Lines [id:da635300766382568] 
		\draw    (307,541) -- (354.37,541) ;
		%Straight Lines [id:da23006105258430853] 
		\draw    (354.37,540.73) -- (354.37,471.28) ;
		%Straight Lines [id:da11720708065111807] 
		\draw    (379.37,431.28) -- (379.37,283.73) ;
		%Straight Lines [id:da37352140139092316] 
		\draw    (404.37,431.28) -- (379.37,431.28) ;
		%Straight Lines [id:da8093941770239494] 
		\draw    (404.37,471.28) -- (354.37,471.28) ;
		%Shape: And Gate [id:dp5970504635609033] 
		\draw   (494.37,441.28) -- (518.37,441.28) .. controls (531.62,441.28) and (542.37,454.72) .. (542.37,471.28) .. controls (542.37,487.84) and (531.62,501.28) .. (518.37,501.28) -- (494.37,501.28) -- (494.37,441.28) -- cycle (478.37,451.28) -- (494.37,451.28) (478.37,491.28) -- (494.37,491.28) (542.37,471.28) -- (558.37,471.28) ;
		%Straight Lines [id:da36966067694141214] 
		\draw    (557.37,471.28) -- (557.37,304) ;
		%Straight Lines [id:da9259493630721556] 
		\draw    (464.37,781.73) -- (464.37,515.73) -- (464.37,491.28) ;
		%Shape: And Gate [id:dp6707214740133565] 
		\draw   (500.37,602.28) -- (524.37,602.28) .. controls (537.62,602.28) and (548.37,615.72) .. (548.37,632.28) .. controls (548.37,648.84) and (537.62,662.28) .. (524.37,662.28) -- (500.37,662.28) -- (500.37,602.28) -- cycle (484.37,612.28) -- (500.37,612.28) (484.37,652.28) -- (500.37,652.28) (548.37,632.28) -- (564.37,632.28) ;
		%Straight Lines [id:da8729647840147676] 
		\draw    (307,740) -- (378.37,740) ;
		%Straight Lines [id:da5208588617454322] 
		\draw    (377.37,740) -- (377.37,652.28) ;
		%Straight Lines [id:da32455012954724904] 
		\draw    (377.37,652.28) -- (484.37,652.28) ;
		%Straight Lines [id:da31761885868990825] 
		\draw    (484.37,612.28) -- (484.37,451.28) ;
		%Straight Lines [id:da7769577830011876] 
		\draw    (478.37,491.28) -- (464.37,491.28) ;
		%Straight Lines [id:da10912805744071052] 
		\draw    (564.37,632.28) -- (636.37,632.28) ;
		%Shape: And Gate [id:dp35889505816314804] 
		\draw   (392.37,554.28) -- (416.37,554.28) .. controls (429.62,554.28) and (440.37,567.72) .. (440.37,584.28) .. controls (440.37,600.84) and (429.62,614.28) .. (416.37,614.28) -- (392.37,614.28) -- (392.37,554.28) -- cycle (376.37,564.28) -- (392.37,564.28) (376.37,604.28) -- (392.37,604.28) (440.37,584.28) -- (456.37,584.28) ;
		%Straight Lines [id:da9826712175082335] 
		\draw    (328.37,604.28) -- (376.37,604.28) ;
		%Straight Lines [id:da2882294054725041] 
		\draw    (328.37,604.28) -- (328.37,699.73) ;
		%Straight Lines [id:da3210149034155083] 
		\draw    (549.37,584.28) -- (456.37,584.28) ;
		%Straight Lines [id:da3474080087179121] 
		\draw    (549.37,145.73) -- (549.37,584.28) ;
		%Straight Lines [id:da515512437238892] 
		\draw    (557.37,145) -- (549.37,145) ;
		%Straight Lines [id:da15228730141330216] 
		\draw    (471.37,534.73) -- (471.37,451.28) ;
		%Straight Lines [id:da8685597597932027] 
		\draw    (471.37,534.73) -- (376.37,534.73) ;
		%Straight Lines [id:da6297602521715668] 
		\draw    (376.37,564.28) -- (376.37,534.73) ;
		%Straight Lines [id:da9748581870563258] 
		\draw    (408.37,185.28) -- (403.37,185.28) ;
		%Shape: Circle [id:dp7253394298857727] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (299.59,145) .. controls (299.59,143.12) and (301.12,141.59) .. (303,141.59) .. controls (304.88,141.59) and (306.41,143.12) .. (306.41,145) .. controls (306.41,146.88) and (304.88,148.41) .. (303,148.41) .. controls (301.12,148.41) and (299.59,146.88) .. (299.59,145) -- cycle ;
		%Shape: Circle [id:dp4706244841157545] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (311.59,145) .. controls (311.59,143.12) and (313.12,141.59) .. (315,141.59) .. controls (316.88,141.59) and (318.41,143.12) .. (318.41,145) .. controls (318.41,146.88) and (316.88,148.41) .. (315,148.41) .. controls (313.12,148.41) and (311.59,146.88) .. (311.59,145) -- cycle ;
		%Straight Lines [id:da2558316250635757] 
		\draw    (307,263) -- (303,263) ;
		%Shape: Circle [id:dp07630877414632664] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (375.96,283.73) .. controls (375.96,281.85) and (377.48,280.32) .. (379.37,280.32) .. controls (381.25,280.32) and (382.78,281.85) .. (382.78,283.73) .. controls (382.78,285.62) and (381.25,287.14) .. (379.37,287.14) .. controls (377.48,287.14) and (375.96,285.62) .. (375.96,283.73) -- cycle ;
		%Shape: Circle [id:dp8211551162930011] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (480.96,451.28) .. controls (480.96,449.39) and (482.48,447.87) .. (484.37,447.87) .. controls (486.25,447.87) and (487.78,449.39) .. (487.78,451.28) .. controls (487.78,453.16) and (486.25,454.69) .. (484.37,454.69) .. controls (482.48,454.69) and (480.96,453.16) .. (480.96,451.28) -- cycle ;
		%Shape: Circle [id:dp5702329586447616] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (467.96,451.28) .. controls (467.96,449.39) and (469.48,447.87) .. (471.37,447.87) .. controls (473.25,447.87) and (474.78,449.39) .. (474.78,451.28) .. controls (474.78,453.16) and (473.25,454.69) .. (471.37,454.69) .. controls (469.48,454.69) and (467.96,453.16) .. (467.96,451.28) -- cycle ;
		
		% Text Node
		\draw (14,69.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$A_{3}$};
		% Text Node
		\draw (15.5,190.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$B_{3}$};
		% Text Node
		\draw (621,111.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$A< B$};
		% Text Node
		\draw (621,271.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$A=B$};
		% Text Node
		\draw (621,616.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$A >B$};
		% Text Node
		\draw (14,267.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$A_{2}$};
		% Text Node
		\draw (15.5,388.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$B_{2}$};
		% Text Node
		\draw (14,465.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$A_{1}$};
		% Text Node
		\draw (15.5,586.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$B_{1}$};
		% Text Node
		\draw (14,664.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$A_{0}$};
		% Text Node
		\draw (15.5,785.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$B_{0}$};
		\end{tikzpicture}
		}
		\caption{$4$-bits magnitude comparator}
	\end{figure}
	Notice that as far as we know it doesn't seem to exist a simple reducible case to any number of bits. That's why computer systems have a limit depending on their bus width (system that transfers data between components inside a computer) like those in the early 121st century (holocene calendar) have a $32$ bits or $64$ bits bus and hence a $32$ bits or $64$ bits magnitude comparator.
	
	The generic method for implementing if-else statement:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,833); %set diagram left start at 0, and has height of 833
		
		%Flowchart: Terminator [id:dp461136743375818] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 167; green, 213; blue, 122 }  ,fill opacity=1 ] (148.61,53) -- (211.39,53) .. controls (219.55,53) and (226.16,61.95) .. (226.16,73) .. controls (226.16,84.05) and (219.55,93) .. (211.39,93) -- (148.61,93) .. controls (140.45,93) and (133.84,84.05) .. (133.84,73) .. controls (133.84,61.95) and (140.45,53) .. (148.61,53) -- cycle ;
		%Flowchart: Decision [id:dp018657644835142095] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 240; green, 176; blue, 70 }  ,fill opacity=1 ] (180,151) -- (236.16,192.21) -- (180,233.42) -- (123.84,192.21) -- cycle ;
		%Flowchart: Process [id:dp13516050147573933] 
		\draw   (106.34,292) -- (253.66,292) -- (253.66,345.28) -- (106.34,345.28) -- cycle ;
		%Flowchart: Terminator [id:dp20970964416189752] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 218; green, 54; blue, 73 }  ,fill opacity=1 ] (148.61,405.64) -- (211.39,405.64) .. controls (219.55,405.64) and (226.16,414.6) .. (226.16,425.64) .. controls (226.16,436.69) and (219.55,445.64) .. (211.39,445.64) -- (148.61,445.64) .. controls (140.45,445.64) and (133.84,436.69) .. (133.84,425.64) .. controls (133.84,414.6) and (140.45,405.64) .. (148.61,405.64) -- cycle ;
		%Straight Lines [id:da23055301842405118] 
		\draw    (180,233.42) -- (180,290.69) ;
		\draw [shift={(180,292.69)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da7368974895601346] 
		\draw    (180,345.69) -- (180,402.97) ;
		\draw [shift={(180,404.97)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da2772100393632366] 
		\draw    (180,93) -- (180,150.27) ;
		\draw [shift={(180,152.27)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Flowchart: Process [id:dp010857042970146225] 
		\draw   (312.34,292) -- (459.66,292) -- (459.66,345.28) -- (312.34,345.28) -- cycle ;
		%Straight Lines [id:da4540144964807331] 
		\draw    (386,192.21) -- (386,289.44) ;
		\draw [shift={(386,291.44)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da39982981854525823] 
		\draw    (236.16,192.21) -- (386,192.21) ;
		%Straight Lines [id:da5267814683727656] 
		\draw    (385.33,345.46) -- (385.33,425.64) ;
		%Straight Lines [id:da4283772844290956] 
		\draw    (385.33,425.64) -- (228.16,425.64) ;
		\draw [shift={(226.16,425.64)}, rotate = 360] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		
		% Text Node
		\draw (155,64) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,opacity=1 ] [align=left] {START};
		% Text Node
		\draw (143,182) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,opacity=1 ] [align=left] {expression};
		% Text Node
		\draw (107.5,311) node [anchor=north west][inner sep=0.75pt]   [align=left] {statement in \textit{if} branch};
		% Text Node
		\draw (159,416.64) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,opacity=1 ] [align=left] {STOP};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (165,242.64) -- (193,242.64) -- (193,267.64) -- (165,267.64) -- cycle  ;
		\draw (168,246.64) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\footnotesize true}};
		% Text Node
		\draw (326.5,301) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{82.66pt}\setlength\topsep{0pt}
		\begin{center}
		statement in \textit{else} \\branch
		\end{center}
		\end{minipage}};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{if-else statement flowchat}
	\end{figure}
	in hardware (using gates,mux-demux, flip flops) is given in Boolean Algebra by:
	
	where:
	\begin{itemize}
		\item $S$ is the input fed by the \textit{if} condition
		
		\item $A$ is the input fed by the \textit{then} subexpression
		
		\item $B$ is the input fed by the \textit{else} subexpression, and
		
		\item $Q$ is the output of the expression.
	\end{itemize}
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,511); %set diagram left start at 0, and has height of 511
		
		%Shape: And Gate [id:dp049768613876360446] 
		\draw   (123,46) -- (147,46) .. controls (160.25,46) and (171,59.44) .. (171,76) .. controls (171,92.56) and (160.25,106) .. (147,106) -- (123,106) -- (123,46) -- cycle (107,56) -- (123,56) (107,96) -- (123,96) (171,76) -- (187,76) ;
		%Straight Lines [id:da45033435782477804] 
		\draw    (107,56) -- (61.37,56) ;
		%Straight Lines [id:da11479430329303097] 
		\draw    (107,96) -- (107,160) ;
		%Straight Lines [id:da6268833226953712] 
		\draw    (60.37,160) -- (109,160) ;
		%Shape: Not/Inverter Gate [id:dp7357548905755724] 
		\draw   (122.81,130) -- (167.26,160) -- (122.81,190) -- (122.81,130) -- cycle (108,160) -- (122.81,160) (176.15,160) -- (188,160) (167.26,160) .. controls (167.26,156.69) and (169.25,154) .. (171.7,154) .. controls (174.16,154) and (176.15,156.69) .. (176.15,160) .. controls (176.15,163.31) and (174.16,166) .. (171.7,166) .. controls (169.25,166) and (167.26,163.31) .. (167.26,160) -- cycle ;
		%Shape: Circle [id:dp48300522703357296] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (103.59,160) .. controls (103.59,158.12) and (105.12,156.59) .. (107,156.59) .. controls (108.88,156.59) and (110.41,158.12) .. (110.41,160) .. controls (110.41,161.88) and (108.88,163.41) .. (107,163.41) .. controls (105.12,163.41) and (103.59,161.88) .. (103.59,160) -- cycle ;
		%Straight Lines [id:da44313579125171176] 
		\draw    (187,103) -- (267,103) ;
		%Shape: Or Gate [id:dp7203486340394807] 
		\draw   (279,93) -- (299,93) .. controls (312.95,93.54) and (325.42,105.23) .. (331,123) .. controls (325.42,140.77) and (312.95,152.46) .. (299,153) -- (279,153) .. controls (287.57,134.44) and (287.57,111.56) .. (279,93) -- cycle (267,103) -- (283,103) (267,143) -- (283,143) (331,123) -- (347,123) ;
		%Straight Lines [id:da5010666560273218] 
		\draw    (267,143) -- (267,223) ;
		%Straight Lines [id:da9147332749626795] 
		\draw    (187,76) -- (187,103) ;
		%Shape: And Gate [id:dp6181835752837836] 
		\draw   (203,193) -- (227,193) .. controls (240.25,193) and (251,206.44) .. (251,223) .. controls (251,239.56) and (240.25,253) .. (227,253) -- (203,253) -- (203,193) -- cycle (187,203) -- (203,203) (187,243) -- (203,243) (251,223) -- (267,223) ;
		%Straight Lines [id:da5976643888325124] 
		\draw    (187,243) -- (61.37,243) ;
		%Straight Lines [id:da8073744999057035] 
		\draw    (188,160) -- (188,203) ;
		
		% Text Node
		\draw (60,36.4) node [anchor=north west][inner sep=0.75pt]    {$A$};
		% Text Node
		\draw (61.5,142.4) node [anchor=north west][inner sep=0.75pt]    {$S$};
		% Text Node
		\draw (61.5,223.4) node [anchor=north west][inner sep=0.75pt]    {$B$};
		% Text Node
		\draw (339,103.4) node [anchor=north west][inner sep=0.75pt]    {$Q$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{If-else statement in hardware based on boolean gates}
	\end{figure}
	
	\pagebreak
	\subsection{Fuzzy logic}\label{fuzzy logic}
	Classical logic only permits conclusions which are either true or false. However, there are also propositions with variable answers, such as one might find when asking a group of people to identify a color. In such instances, the truth appears as the result of reasoning from inexact or partial knowledge in which the sampled answers are mapped on a spectrum.
	
	Humans and animals often operate using fuzzy evaluations in many everyday situations. In the case where someone is tossing an object into a container from a distance, the person does not compute exact values for the object weight, density, distance, direction, container height and width, and air resistance to determine the force and angle to toss the object. Instead the person instinctively applies quick "fuzzy" estimates, based upon previous experience, to determine what output values of force, direction and vertical angle to use to make the toss.
Both degrees of truth and probabilities range between 0 and 1 and hence may seem similar at first, but fuzzy logic uses degrees of truth as a mathematical model of vagueness, while probability is a mathematical model of ignorance.

	Take, for example, the concepts of "empty" and "full". The meaning of each of them can be represented by a certain fuzzy set. The concept of emptiness would be subjective and thus would depend on the observer or designer. A $100$ [ml] glass containing $30$ [ml] of water may be defined as being $0.7$ empty and $0.3$ full, but another designer might, equally well, design a set membership function where the glass would be considered full for all values down to $50$ [ml].
	
	Most of the problems encountered are certainly mathematically modelizable. But these models often require overly restrictive assumptions, making application to the real world tricky. The problems of this world must take into account imprecise, uncertain information. Let us take the example of air conditioning: if we want to obtain a cool temperature, we can ask ourselves what temperature range will be appropriate (the demand is imprecise); Furthermore the reliability of the sensors comes into play (the measurement of the ambient temperature is uncertain). We see the difficulty of interpreting the linguistic variables as fresh, hot, ... and the processing of these uncertainties.
	
	An approach was developed from 11965 (holocene calendar) by Loft. A. Zadeh, a professor at the University of California at Berkeley, based on the theory of fuzzy sets, generalizing the theory of classical sets. In the new Zadeh theory, an element can more or less belong to a certain set. Inaccuracies and uncertainties can thus be modeled, and reasonings acquire a flexibility that is not allowed by classical logic: "\NewTerm{fuzzy logic}\index{fuzzy logic}" was born. Many applications have developed in various domains, where no deterministic model exists or is practically implementable, as well as in situations where data imprecision makes control by conventional methods impossible.

	In the following, we will first develop the basics of the theory of fuzzy subsets, then we will clarify the reasoning in fuzzy logic, we will examine the methods of exploitation of the results obtained, and finally we will see an effective application (and if possible an application with MATLAB™ and/or \texttt{R}).

	Before turning to the formal side of the thing (mathematically speaking), it may be preferable (since it is still a technique of the engineer mainly) to briefly present the concepts of fuzzy logic in a pictorial way.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The fuzzy logic is a technique used for real in fields as varied as automatism (ABS brakes), robotics (pattern recognition), road traffic management (red lights), air traffic control, environment (meteorology, climatology, seismology), medicine (diagnostic aid), psychology, data mining, Machine Learning, and many others.
	\end{tcolorbox}
	Consider, for example, the speed of a vehicle on a national highway. The normal speed is $90\;[\text{km}\cdot\text{h}^{-1}]$. A speed can be considered as high above  $100\;[\text{km}\cdot\text{h}^{-1}]$, and as low below $80\;[\text{km}\cdot\text{h}^{-1}]$. Boolean logic would look at a thing like this:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
		
		%Shape: Axis 2D [id:dp9550200477185562] 
		\draw  (147,251.3) -- (455.5,251.3)(177.85,29) -- (177.85,276) (448.5,246.3) -- (455.5,251.3) -- (448.5,256.3) (172.85,36) -- (177.85,29) -- (182.85,36) (224.85,246.3) -- (224.85,256.3)(271.85,246.3) -- (271.85,256.3)(318.85,246.3) -- (318.85,256.3)(365.85,246.3) -- (365.85,256.3)(412.85,246.3) -- (412.85,256.3)(172.85,204.3) -- (182.85,204.3)(172.85,157.3) -- (182.85,157.3)(172.85,110.3) -- (182.85,110.3)(172.85,63.3) -- (182.85,63.3) ;
		\draw   ;
		%Straight Lines [id:da464995688274662] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (413.5,61) -- (444.5,61) ;
		%Straight Lines [id:da6034694467918995] 
		\draw    (226.5,61) -- (226.5,251) ;
		%Straight Lines [id:da7056028089093695] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (413.5,61) -- (413.5,251) ;
		%Straight Lines [id:da8451102424920396] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (177.85,251.3) -- (413.5,251) ;
		
		% Text Node
		\draw (146,232.4) node [anchor=north west][inner sep=0.75pt]    {$0\%$};
		% Text Node
		\draw (129,55.4) node [anchor=north west][inner sep=0.75pt]    {$100\%$};
		% Text Node
		\draw (214,260.4) node [anchor=north west][inner sep=0.75pt]    {$80$};
		% Text Node
		\draw (309,260.4) node [anchor=north west][inner sep=0.75pt]    {$90$};
		% Text Node
		\draw (399,260.4) node [anchor=north west][inner sep=0.75pt]    {$100$};
		% Text Node
		\draw (457,255.4) node [anchor=north west][inner sep=0.75pt]    {$\left[\text{km} \cdot \text{h}^{-1}\right]$};
		
		\end{tikzpicture}
	\end{figure}
	We see above that therefore the speed is considered $100\%$ as high starting from $100\;[\text{km}\cdot\text{h}^{-1}]$, and $0\%$ below.

	Fuzzy logic, on the other hand, allows degrees of verification of the condition "Is speed high?" according to:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
		
		%Shape: Axis 2D [id:dp9550200477185562] 
		\draw  (147,251.3) -- (455.5,251.3)(177.85,29) -- (177.85,276) (448.5,246.3) -- (455.5,251.3) -- (448.5,256.3) (172.85,36) -- (177.85,29) -- (182.85,36) (224.85,246.3) -- (224.85,256.3)(271.85,246.3) -- (271.85,256.3)(318.85,246.3) -- (318.85,256.3)(365.85,246.3) -- (365.85,256.3)(412.85,246.3) -- (412.85,256.3)(172.85,204.3) -- (182.85,204.3)(172.85,157.3) -- (182.85,157.3)(172.85,110.3) -- (182.85,110.3)(172.85,63.3) -- (182.85,63.3) ;
		\draw   ;
		%Straight Lines [id:da464995688274662] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (413.5,61) -- (444.5,61) ;
		%Straight Lines [id:da6034694467918995] 
		\draw    (225.5,61) -- (225.5,251) ;
		%Straight Lines [id:da8451102424920396] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (177.85,251.3) -- (225.5,251.3) ;
		%Straight Lines [id:da2138408130280709] 
		\draw    (413.5,61) -- (413.5,251) ;
		%Straight Lines [id:da7491317430467677] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (225.5,251) -- (413.5,61) ;
		
		% Text Node
		\draw (146,232.4) node [anchor=north west][inner sep=0.75pt]    {$0\%$};
		% Text Node
		\draw (129,55.4) node [anchor=north west][inner sep=0.75pt]    {$100\%$};
		% Text Node
		\draw (214,260.4) node [anchor=north west][inner sep=0.75pt]    {$80$};
		% Text Node
		\draw (309,260.4) node [anchor=north west][inner sep=0.75pt]    {$90$};
		% Text Node
		\draw (399,260.4) node [anchor=north west][inner sep=0.75pt]    {$100$};
		% Text Node
		\draw (457,255.4) node [anchor=north west][inner sep=0.75pt]    {$\left[\text{km} \cdot \text{h}^{-1}\right]$};
		
		\end{tikzpicture}
	\end{figure}
	The situation is better here as the speed is considered as not high at all  below $80\;[\text{km}\cdot\text{h}^{-1}]$. We can therefore say that below $80\;[\text{km}\cdot\text{h}^{-1}]$, the speed is high at $0\%$. The speed is considered to be high above $100\;[\text{km}\cdot\text{h}^{-1}]$. The speed is therefore high at $100\%$ above $100\;[\text{km}\cdot\text{h}^{-1}]$. The speed is thus high at $50\%$ when at $90\;[\text{km}\cdot\text{h}^{-1}]$ and high at $25\%$ at $85\;[\text{km}\cdot\text{h}^{-1}]$.

	Similarly, the function "Is the speed low?" Will be addressed typically as following by most humans:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
		
		%Shape: Axis 2D [id:dp9550200477185562] 
		\draw  (147,251.3) -- (455.5,251.3)(177.85,29) -- (177.85,276) (448.5,246.3) -- (455.5,251.3) -- (448.5,256.3) (172.85,36) -- (177.85,29) -- (182.85,36) (224.85,246.3) -- (224.85,256.3)(271.85,246.3) -- (271.85,256.3)(318.85,246.3) -- (318.85,256.3)(365.85,246.3) -- (365.85,256.3)(412.85,246.3) -- (412.85,256.3)(172.85,204.3) -- (182.85,204.3)(172.85,157.3) -- (182.85,157.3)(172.85,110.3) -- (182.85,110.3)(172.85,63.3) -- (182.85,63.3) ;
		\draw   ;
		%Straight Lines [id:da464995688274662] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (413.5,251) -- (444.5,251) ;
		%Straight Lines [id:da6034694467918995] 
		\draw    (225.5,64) -- (225.5,251) ;
		%Straight Lines [id:da8451102424920396] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (177.85,64) -- (225.5,64) ;
		%Straight Lines [id:da2138408130280709] 
		\draw    (413.5,61) -- (413.5,251) ;
		%Straight Lines [id:da7491317430467677] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (413.5,251) -- (225.5,64) ;
		
		% Text Node
		\draw (146,232.4) node [anchor=north west][inner sep=0.75pt]    {$0\%$};
		% Text Node
		\draw (129,55.4) node [anchor=north west][inner sep=0.75pt]    {$100\%$};
		% Text Node
		\draw (214,260.4) node [anchor=north west][inner sep=0.75pt]    {$80$};
		% Text Node
		\draw (309,260.4) node [anchor=north west][inner sep=0.75pt]    {$90$};
		% Text Node
		\draw (399,260.4) node [anchor=north west][inner sep=0.75pt]    {$100$};
		% Text Node
		\draw (457,255.4) node [anchor=north west][inner sep=0.75pt]    {$\left[\text{km} \cdot \text{h}^{-1}\right]$};
		
		\end{tikzpicture}
	\end{figure}
	Therefore following the above figure the speed is considered low under $80\;[\text{km}\cdot\text{h}^{-1}]$. It is therefore $100\%$ low. The speed is considered not at all low above $100\;[\text{km}\cdot\text{h}^{-1}]$. It is therefore $0\%$ low. The speed is thus a $50\%$ (bit low) when at $90\;[\text{km}\cdot\text{h}^{-1}]$ and at $75\%$ (quite low) when at $85\;[\text{km}\cdot\text{h}^{-1}]$.
	
	We can also define a function "Is the speed average?" by:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
		
		%Shape: Axis 2D [id:dp9550200477185562] 
		\draw  (147,251.3) -- (455.5,251.3)(177.85,29) -- (177.85,276) (448.5,246.3) -- (455.5,251.3) -- (448.5,256.3) (172.85,36) -- (177.85,29) -- (182.85,36) (224.85,246.3) -- (224.85,256.3)(271.85,246.3) -- (271.85,256.3)(318.85,246.3) -- (318.85,256.3)(365.85,246.3) -- (365.85,256.3)(412.85,246.3) -- (412.85,256.3)(172.85,204.3) -- (182.85,204.3)(172.85,157.3) -- (182.85,157.3)(172.85,110.3) -- (182.85,110.3)(172.85,63.3) -- (182.85,63.3) ;
		\draw   ;
		%Straight Lines [id:da8451102424920396] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (177.85,251.3) -- (226.5,251) ;
		%Straight Lines [id:da6876906538948762] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (226.5,251) -- (319.5,60) ;
		%Straight Lines [id:da3756565684132618] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (413.5,251) -- (319.5,60) ;
		%Straight Lines [id:da464995688274662] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (413.5,251) -- (444.5,251) ;
		%Straight Lines [id:da031121596678874486] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (319.5,60) -- (319.5,250) ;
		%Straight Lines [id:da6034694467918995] 
		\draw    (226.5,61) -- (226.5,251) ;
		%Straight Lines [id:da8926692151631135] 
		\draw    (413.5,61) -- (413.5,251) ;
		
		% Text Node
		\draw (146,232.4) node [anchor=north west][inner sep=0.75pt]    {$0\%$};
		% Text Node
		\draw (129,55.4) node [anchor=north west][inner sep=0.75pt]    {$100\%$};
		% Text Node
		\draw (214,260.4) node [anchor=north west][inner sep=0.75pt]    {$80$};
		% Text Node
		\draw (309,260.4) node [anchor=north west][inner sep=0.75pt]    {$90$};
		% Text Node
		\draw (399,260.4) node [anchor=north west][inner sep=0.75pt]    {$100$};
		% Text Node
		\draw (457,255.4) node [anchor=north west][inner sep=0.75pt]    {$\left[\text{km} \cdot \text{h}^{-1}\right]$};
		
		\end{tikzpicture}
	\end{figure}
	Once the input value evaluated ("Is the speed high?"), a value can be determined for an output function. Consider the function "If the fever is strong, then administer aspirin". Such a function is named "\NewTerm{fuzzy control}". It is composed of two parts:
	\begin{enumerate}
		\item An input: "Is the fever strong?". We consider that a fever is not strong below $38\;[^\circ\text{C}]$, and that it is high above $40\;[^\circ\text{C}]$.
		
		\item An output: "Administer a given number of aspirin tablets!"
	\end{enumerate}
	These two parts are related. We can represent them together as below:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1,scale=0.9]
		%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
		
		%Shape: Axis 2D [id:dp9550200477185562] 
		\draw  (23,239.3) -- (331.5,239.3)(53.85,17) -- (53.85,264) (324.5,234.3) -- (331.5,239.3) -- (324.5,244.3) (48.85,24) -- (53.85,17) -- (58.85,24) (100.85,234.3) -- (100.85,244.3)(147.85,234.3) -- (147.85,244.3)(194.85,234.3) -- (194.85,244.3)(241.85,234.3) -- (241.85,244.3)(288.85,234.3) -- (288.85,244.3)(48.85,192.3) -- (58.85,192.3)(48.85,145.3) -- (58.85,145.3)(48.85,98.3) -- (58.85,98.3)(48.85,51.3) -- (58.85,51.3) ;
		\draw   ;
		%Straight Lines [id:da464995688274662] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (289.5,49) -- (320.5,49) ;
		%Straight Lines [id:da6034694467918995] 
		\draw    (101.5,52) -- (101.5,239) ;
		%Straight Lines [id:da8451102424920396] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (53.85,239) -- (101.5,239) ;
		%Straight Lines [id:da2138408130280709] 
		\draw    (289.5,49) -- (289.5,239) ;
		%Straight Lines [id:da7491317430467677] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (289.5,49) -- (101.5,239) ;
		%Shape: Axis 2D [id:dp8687820296455193] 
		\draw  (340,239.3) -- (611.5,239.3)(367.15,17) -- (367.15,264) (604.5,234.3) -- (611.5,239.3) -- (604.5,244.3) (362.15,24) -- (367.15,17) -- (372.15,24) (414.15,234.3) -- (414.15,244.3)(461.15,234.3) -- (461.15,244.3)(508.15,234.3) -- (508.15,244.3)(555.15,234.3) -- (555.15,244.3)(362.15,192.3) -- (372.15,192.3)(362.15,145.3) -- (372.15,145.3)(362.15,98.3) -- (372.15,98.3)(362.15,51.3) -- (372.15,51.3) ;
		\draw   ;
		%Straight Lines [id:da36416021902306084] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (554.85,49.3) -- (585.85,49.3) ;
		%Straight Lines [id:da4329000307190114] 
		\draw    (554.85,49.3) -- (554.85,239.3) ;
		%Straight Lines [id:da2753147741377966] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (554.85,49.3) -- (367.15,239.3) ;
		
		% Text Node
		\draw (22,220.4) node [anchor=north west][inner sep=0.75pt]    {$0\%$};
		% Text Node
		\draw (5,43.4) node [anchor=north west][inner sep=0.75pt]    {$100\%$};
		% Text Node
		\draw (90,247.4) node [anchor=north west][inner sep=0.75pt]    {$38$};
		% Text Node
		\draw (185,247.4) node [anchor=north west][inner sep=0.75pt]    {$39$};
		% Text Node
		\draw (275,247.4) node [anchor=north west][inner sep=0.75pt]    {$40$};
		% Text Node
		\draw (303,248.4) node [anchor=north west][inner sep=0.75pt]    {$\left[^{\circ }\text{C}\right]$};
		% Text Node
		\draw (375,247.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (457,247.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (551,247.4) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (578,251) node [anchor=north west][inner sep=0.75pt]   [align=left] {\#tablets};
		
		\end{tikzpicture}
	\end{figure}
	There are several empirical techniques for determining the output value (in the example: the number of aspirin tablets to be administered):

	An example consists in taking the horizontal passing through the corresponding ordinate point on the starting curve at the abscissa of the value of the input and of looking at where this horizontal section intersects the output curve. The abscissa of this point of intersection is a possible output value as shown below:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1,scale=0.9]
		%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
		
		%Shape: Axis 2D [id:dp9550200477185562] 
		\draw  (23,239.3) -- (331.5,239.3)(53.85,17) -- (53.85,264) (324.5,234.3) -- (331.5,239.3) -- (324.5,244.3) (48.85,24) -- (53.85,17) -- (58.85,24) (100.85,234.3) -- (100.85,244.3)(147.85,234.3) -- (147.85,244.3)(194.85,234.3) -- (194.85,244.3)(241.85,234.3) -- (241.85,244.3)(288.85,234.3) -- (288.85,244.3)(48.85,192.3) -- (58.85,192.3)(48.85,145.3) -- (58.85,145.3)(48.85,98.3) -- (58.85,98.3)(48.85,51.3) -- (58.85,51.3) ;
		\draw   ;
		%Straight Lines [id:da464995688274662] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (289.5,49) -- (320.5,49) ;
		%Straight Lines [id:da6034694467918995] 
		\draw    (101.5,52) -- (101.5,239) ;
		%Straight Lines [id:da8451102424920396] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (53.85,239) -- (101.5,239) ;
		%Straight Lines [id:da2138408130280709] 
		\draw    (289.5,49) -- (289.5,239) ;
		%Straight Lines [id:da7491317430467677] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (289.5,49) -- (101.5,239) ;
		%Shape: Axis 2D [id:dp8687820296455193] 
		\draw  (340,239.3) -- (611.5,239.3)(367.15,17) -- (367.15,264) (604.5,234.3) -- (611.5,239.3) -- (604.5,244.3) (362.15,24) -- (367.15,17) -- (372.15,24) (414.15,234.3) -- (414.15,244.3)(461.15,234.3) -- (461.15,244.3)(508.15,234.3) -- (508.15,244.3)(555.15,234.3) -- (555.15,244.3)(362.15,192.3) -- (372.15,192.3)(362.15,145.3) -- (372.15,145.3)(362.15,98.3) -- (372.15,98.3)(362.15,51.3) -- (372.15,51.3) ;
		\draw   ;
		%Straight Lines [id:da36416021902306084] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (554.85,49.3) -- (585.85,49.3) ;
		%Straight Lines [id:da4329000307190114] 
		\draw    (554.85,49.3) -- (554.85,239.3) ;
		%Straight Lines [id:da2753147741377966] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (554.85,49.3) -- (367.15,239.3) ;
		%Straight Lines [id:da9136655981828958] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (148,239) -- (148,195) ;
		%Straight Lines [id:da011873387714540273] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (412.5,195) -- (148,195) ;
		%Straight Lines [id:da4642763121981026] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (412.5,239) -- (412.5,195) ;
		
		% Text Node
		\draw (22,220.4) node [anchor=north west][inner sep=0.75pt]    {$0\%$};
		% Text Node
		\draw (5,43.4) node [anchor=north west][inner sep=0.75pt]    {$100\%$};
		% Text Node
		\draw (90,247.4) node [anchor=north west][inner sep=0.75pt]    {$38$};
		% Text Node
		\draw (185,247.4) node [anchor=north west][inner sep=0.75pt]    {$39$};
		% Text Node
		\draw (275,247.4) node [anchor=north west][inner sep=0.75pt]    {$40$};
		% Text Node
		\draw (303,248.4) node [anchor=north west][inner sep=0.75pt]    {$\left[^{\circ}\text{C}\right]$};
		% Text Node
		\draw (375,247.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (457,247.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (551,247.4) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (578,251) node [anchor=north west][inner sep=0.75pt]   [align=left] {\#tablets};
		% Text Node
		\draw (131,247.4) node [anchor=north west][inner sep=0.75pt]    {$38.5$};
		
		\end{tikzpicture}
	\end{figure}
	A second empirical possible choice consists in taking as output value of the center of gravity of the gray trapezoid delimited by the horizontal and the output curve as shown in the figure below:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1,scale=0.9]
		%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
		
		%Shape: Polygon [id:ds25789896080162755] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 206; green, 206; blue, 206 }  ,fill opacity=1 ] (554.85,239.3) -- (367.15,239.3) -- (412.5,195) -- (555.5,195) -- cycle ;
		%Shape: Axis 2D [id:dp9550200477185562] 
		\draw  (23,239.3) -- (331.5,239.3)(53.85,17) -- (53.85,264) (324.5,234.3) -- (331.5,239.3) -- (324.5,244.3) (48.85,24) -- (53.85,17) -- (58.85,24) (100.85,234.3) -- (100.85,244.3)(147.85,234.3) -- (147.85,244.3)(194.85,234.3) -- (194.85,244.3)(241.85,234.3) -- (241.85,244.3)(288.85,234.3) -- (288.85,244.3)(48.85,192.3) -- (58.85,192.3)(48.85,145.3) -- (58.85,145.3)(48.85,98.3) -- (58.85,98.3)(48.85,51.3) -- (58.85,51.3) ;
		\draw   ;
		%Straight Lines [id:da464995688274662] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (289.5,49) -- (320.5,49) ;
		%Straight Lines [id:da6034694467918995] 
		\draw    (101.5,52) -- (101.5,239) ;
		%Straight Lines [id:da8451102424920396] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (53.85,239) -- (101.5,239) ;
		%Straight Lines [id:da2138408130280709] 
		\draw    (289.5,49) -- (289.5,239) ;
		%Straight Lines [id:da7491317430467677] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (289.5,49) -- (101.5,239) ;
		%Shape: Axis 2D [id:dp8687820296455193] 
		\draw  (340,239.3) -- (611.5,239.3)(367.15,17) -- (367.15,264) (604.5,234.3) -- (611.5,239.3) -- (604.5,244.3) (362.15,24) -- (367.15,17) -- (372.15,24) (414.15,234.3) -- (414.15,244.3)(461.15,234.3) -- (461.15,244.3)(508.15,234.3) -- (508.15,244.3)(555.15,234.3) -- (555.15,244.3)(362.15,192.3) -- (372.15,192.3)(362.15,145.3) -- (372.15,145.3)(362.15,98.3) -- (372.15,98.3)(362.15,51.3) -- (372.15,51.3) ;
		\draw   ;
		%Straight Lines [id:da36416021902306084] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (554.85,49.3) -- (585.85,49.3) ;
		%Straight Lines [id:da4329000307190114] 
		\draw    (554.85,49.3) -- (554.85,239.3) ;
		%Straight Lines [id:da2753147741377966] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (554.85,49.3) -- (367.15,239.3) ;
		%Straight Lines [id:da9136655981828958] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (148,239) -- (148,195) ;
		%Straight Lines [id:da011873387714540273] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (412.5,195) -- (148,195) ;
		%Straight Lines [id:da4642763121981026] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (412.5,239) -- (412.5,195) ;
		%Flowchart: Summing Junction [id:dp6881980304678648] 
		\draw   (470,216.75) .. controls (470,214.13) and (472.13,212) .. (474.75,212) .. controls (477.37,212) and (479.5,214.13) .. (479.5,216.75) .. controls (479.5,219.37) and (477.37,221.5) .. (474.75,221.5) .. controls (472.13,221.5) and (470,219.37) .. (470,216.75) -- cycle ; \draw   (471.39,213.39) -- (478.11,220.11) ; \draw   (478.11,213.39) -- (471.39,220.11) ;
		%Straight Lines [id:da8716980344713459] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (474.75,239) -- (474.75,216.75) ;
		
		% Text Node
		\draw (22,220.4) node [anchor=north west][inner sep=0.75pt]    {$0\%$};
		% Text Node
		\draw (5,43.4) node [anchor=north west][inner sep=0.75pt]    {$100\%$};
		% Text Node
		\draw (90,247.4) node [anchor=north west][inner sep=0.75pt]    {$38$};
		% Text Node
		\draw (185,247.4) node [anchor=north west][inner sep=0.75pt]    {$39$};
		% Text Node
		\draw (275,247.4) node [anchor=north west][inner sep=0.75pt]    {$40$};
		% Text Node
		\draw (303,248.4) node [anchor=north west][inner sep=0.75pt]    {$\left[^{\circ}\text{C}\right]$};
		% Text Node
		\draw (375,247.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (457,247.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (551,247.4) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (578,251) node [anchor=north west][inner sep=0.75pt]   [align=left] {\#tablets};
		% Text Node
		\draw (131,247.4) node [anchor=north west][inner sep=0.75pt]    {$38.5$};
		% Text Node
		\draw (482,205.4) node [anchor=north west][inner sep=0.75pt]    {$G$};
		
		\end{tikzpicture}
	\end{figure}
	From these two non-software oriented examples, we see that we are at the frontier of pure science and engineering frontier since there is a technical and/or statistical choice to be made in the method to be chosen.
	
	Let us now do a computer aided example as engineers in practice use such tools to speed up their research and development! The case below in inspired by a textbook and many softwares take it now as basis example named the "Basic Tipping Problem". 
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Given a number between $0$ and $10$ that represents the quality of service and food at a restaurant (where $10$ is excellent), what should the tip be? \\
	
	This problem is based on tipping as it is typically practiced in the United States. An average tip for a meal in the U.S. is $15\%$, though the actual amount may vary depending on the quality of the service provided. But because service and food is rated on a scale of $0$ to $10$, we might have the tip go linearly from $5\%$ if the service is bad to $25\%$ if the service is excellent.\\
	
	If we denote $S$ for the service and $F$ for food quality, we will chose the tip as being given by:
	
	Here is a copy/paste of the MATLAB™ 2013a script for the original url\footnote{See \url{https://ch.mathworks.com/help/fuzzy/an-introductory-example-fuzzy-versus-nonfuzzy-logic.html}}:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/computing/fuzzy_logic_tip_matlab_01_script.jpg}
	\end{figure}
	In this case, the results look satisfactory, but when you look at them closely, they do not seem quite right:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.9]{img/computing/fuzzy_logic_tip_matlab_01_plot.jpg}
	\end{figure}
	Suppose you want the service to be a more important factor than the food quality. Specify that service accounts for $80\%$ of the overall tipping grade and the food makes up the other $20\%$. Try this equation:
	
	Thus in MATLAB™ 2013a:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/computing/fuzzy_logic_tip_matlab_02_script.jpg}
	\end{figure}
	The response is still some how too uniformly linear. Suppose you want more of a flat response in the middle, i.e., you want to give a $15\%$ tip in general, but want to also specify a variation if the service is exceptionally good or bad. This factor, in turn, means that the previous linear mappings no longer apply. You can still use the linear calculation with a piecewise linear construction. Now, return to the one-dimensional problem of just considering the service. You can create a simple conditional tip assignment using logical indexing:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/computing/fuzzy_logic_tip_matlab_03_script.jpg}
	\end{figure}
	that gives the following plot:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/computing/fuzzy_logic_tip_matlab_02_plot.jpg}
	\end{figure}
	Suppose you extend this to two dimensions, where we take food into account again:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/computing/fuzzy_logic_tip_matlab_04_script.jpg}
	\end{figure}
	that gives the following plot:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/computing/fuzzy_logic_tip_matlab_03_plot.jpg}
	\end{figure}
	The plot looks good, but the function is surprisingly complicated. It was a little difficult to code this correctly, and it is definitely not easy to modify this code in the future. Moreover, it is even less apparent how the algorithm works to someone who did not see the original design process.
	\end{tcolorbox}
	In practice we don't use a fuzzy function like the one above as there are sharp angles (imagine a car having speed settings wit abrupt changes as above...).
	
	Therefore rather than using linear functions a input we can choose among a class of continuous and smooth functions. For example the "\NewTerm{Gaussian curve membership function}" defined by:
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
		
		%Shape: Axis 2D [id:dp9494165001496875] 
		\draw  (130,307.38) -- (516.5,307.38)(159.04,18) -- (159.04,341) (509.5,302.38) -- (516.5,307.38) -- (509.5,312.38) (154.04,25) -- (159.04,18) -- (164.04,25) (186.04,302.38) -- (186.04,312.38)(213.04,302.38) -- (213.04,312.38)(240.04,302.38) -- (240.04,312.38)(267.04,302.38) -- (267.04,312.38)(294.04,302.38) -- (294.04,312.38)(321.04,302.38) -- (321.04,312.38)(348.04,302.38) -- (348.04,312.38)(375.04,302.38) -- (375.04,312.38)(402.04,302.38) -- (402.04,312.38)(429.04,302.38) -- (429.04,312.38)(456.04,302.38) -- (456.04,312.38)(483.04,302.38) -- (483.04,312.38)(154.04,280.38) -- (164.04,280.38)(154.04,253.38) -- (164.04,253.38)(154.04,226.38) -- (164.04,226.38)(154.04,199.38) -- (164.04,199.38)(154.04,172.38) -- (164.04,172.38)(154.04,145.38) -- (164.04,145.38)(154.04,118.38) -- (164.04,118.38)(154.04,91.38) -- (164.04,91.38)(154.04,64.38) -- (164.04,64.38)(154.04,37.38) -- (164.04,37.38)(154.04,334.38) -- (164.04,334.38) ;
		\draw   ;
		%Curve Lines [id:da22819829366986233] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (159.5,296) .. controls (266.5,238) and (276.5,38) .. (334.5,37) .. controls (392.5,36) and (407.5,239) .. (513.5,296) ;
		
		% Text Node
		\draw (143,310) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
		% Text Node
		\draw (128,271.4) node [anchor=north west][inner sep=0.75pt]    {$0.1$};
		% Text Node
		\draw (128,244.16) node [anchor=north west][inner sep=0.75pt]    {$0.2$};
		% Text Node
		\draw (128,216.94) node [anchor=north west][inner sep=0.75pt]    {$0.3$};
		% Text Node
		\draw (128,189.72) node [anchor=north west][inner sep=0.75pt]    {$0.4$};
		% Text Node
		\draw (128,162.5) node [anchor=north west][inner sep=0.75pt]    {$0.5$};
		% Text Node
		\draw (128,135.28) node [anchor=north west][inner sep=0.75pt]    {$0.6$};
		% Text Node
		\draw (128,108.06) node [anchor=north west][inner sep=0.75pt]    {$0.7$};
		% Text Node
		\draw (128,80.84) node [anchor=north west][inner sep=0.75pt]    {$0.8$};
		% Text Node
		\draw (128,53.62) node [anchor=north west][inner sep=0.75pt]    {$0.9$};
		% Text Node
		\draw (139,26.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (290,345) node [anchor=north west][inner sep=0.75pt]   [align=left] {gaussmf, P=[2,6.5]};
		% Text Node
		\draw (182.44,313.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (207.04,313.4) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (234.64,313.4) node [anchor=north west][inner sep=0.75pt]    {$3$};
		% Text Node
		\draw (261.24,313.4) node [anchor=north west][inner sep=0.75pt]    {$4$};
		% Text Node
		\draw (288.84,313.4) node [anchor=north west][inner sep=0.75pt]    {$5$};
		% Text Node
		\draw (314.44,313.4) node [anchor=north west][inner sep=0.75pt]    {$6$};
		% Text Node
		\draw (343.04,313.4) node [anchor=north west][inner sep=0.75pt]    {$7$};
		% Text Node
		\draw (369.64,313.4) node [anchor=north west][inner sep=0.75pt]    {$8$};
		% Text Node
		\draw (396.24,313.4) node [anchor=north west][inner sep=0.75pt]    {$9$};
		% Text Node
		\draw (419.84,313.4) node [anchor=north west][inner sep=0.75pt]    {$10$};
		% Text Node
		\draw (447.44,313.4) node [anchor=north west][inner sep=0.75pt]    {$11$};
		% Text Node
		\draw (476.08,313.4) node [anchor=north west][inner sep=0.75pt]    {$12$};
		\end{tikzpicture}
	\end{figure}
	or the "\NewTerm{Trapezoidal-shaped membership function}" defined by:
	
	The parameters $a$ and $d$ locate the "feet" of the trapezoid and the parameters $b$ and $c$ locate the "shoulders."
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
		
		%Shape: Axis 2D [id:dp9494165001496875] 
		\draw  (130,307.38) -- (516.5,307.38)(159.04,18) -- (159.04,341) (509.5,302.38) -- (516.5,307.38) -- (509.5,312.38) (154.04,25) -- (159.04,18) -- (164.04,25) (186.04,302.38) -- (186.04,312.38)(213.04,302.38) -- (213.04,312.38)(240.04,302.38) -- (240.04,312.38)(267.04,302.38) -- (267.04,312.38)(294.04,302.38) -- (294.04,312.38)(321.04,302.38) -- (321.04,312.38)(348.04,302.38) -- (348.04,312.38)(375.04,302.38) -- (375.04,312.38)(402.04,302.38) -- (402.04,312.38)(429.04,302.38) -- (429.04,312.38)(456.04,302.38) -- (456.04,312.38)(483.04,302.38) -- (483.04,312.38)(154.04,280.38) -- (164.04,280.38)(154.04,253.38) -- (164.04,253.38)(154.04,226.38) -- (164.04,226.38)(154.04,199.38) -- (164.04,199.38)(154.04,172.38) -- (164.04,172.38)(154.04,145.38) -- (164.04,145.38)(154.04,118.38) -- (164.04,118.38)(154.04,91.38) -- (164.04,91.38)(154.04,64.38) -- (164.04,64.38)(154.04,37.38) -- (164.04,37.38)(154.04,334.38) -- (164.04,334.38) ;
		\draw   ;
		%Straight Lines [id:da8246870444428189] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (159.04,307.38) -- (187.5,307.38) ;
		%Straight Lines [id:da1278554298183281] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (187.5,307.38) -- (294,38) ;
		%Straight Lines [id:da3823444408288643] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (294,38) -- (348,38) ;
		%Straight Lines [id:da6836866386739784] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (348,38) -- (375,307) ;
		%Straight Lines [id:da638412251163812] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (375,307) -- (485.5,307) ;
		
		% Text Node
		\draw (143,310) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
		% Text Node
		\draw (128,271.4) node [anchor=north west][inner sep=0.75pt]    {$0.1$};
		% Text Node
		\draw (128,244.16) node [anchor=north west][inner sep=0.75pt]    {$0.2$};
		% Text Node
		\draw (128,216.94) node [anchor=north west][inner sep=0.75pt]    {$0.3$};
		% Text Node
		\draw (128,189.72) node [anchor=north west][inner sep=0.75pt]    {$0.4$};
		% Text Node
		\draw (128,162.5) node [anchor=north west][inner sep=0.75pt]    {$0.5$};
		% Text Node
		\draw (128,135.28) node [anchor=north west][inner sep=0.75pt]    {$0.6$};
		% Text Node
		\draw (128,108.06) node [anchor=north west][inner sep=0.75pt]    {$0.7$};
		% Text Node
		\draw (128,80.84) node [anchor=north west][inner sep=0.75pt]    {$0.8$};
		% Text Node
		\draw (128,53.62) node [anchor=north west][inner sep=0.75pt]    {$0.9$};
		% Text Node
		\draw (139,26.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (290,345) node [anchor=north west][inner sep=0.75pt]   [align=left] {trapmf, P=[1,5,7,8]};
		% Text Node
		\draw (182.44,313.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (207.04,313.4) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (234.64,313.4) node [anchor=north west][inner sep=0.75pt]    {$3$};
		% Text Node
		\draw (261.24,313.4) node [anchor=north west][inner sep=0.75pt]    {$4$};
		% Text Node
		\draw (288.84,313.4) node [anchor=north west][inner sep=0.75pt]    {$5$};
		% Text Node
		\draw (314.44,313.4) node [anchor=north west][inner sep=0.75pt]    {$6$};
		% Text Node
		\draw (343.04,313.4) node [anchor=north west][inner sep=0.75pt]    {$7$};
		% Text Node
		\draw (369.64,313.4) node [anchor=north west][inner sep=0.75pt]    {$8$};
		% Text Node
		\draw (396.24,313.4) node [anchor=north west][inner sep=0.75pt]    {$9$};
		% Text Node
		\draw (419.84,313.4) node [anchor=north west][inner sep=0.75pt]    {$10$};
		% Text Node
		\draw (447.44,313.4) node [anchor=north west][inner sep=0.75pt]    {$11$};
		% Text Node
		\draw (476.08,313.4) node [anchor=north west][inner sep=0.75pt]    {$12$};
		
		\end{tikzpicture}
	\end{figure}
	Or also the "\NewTerm{Triangular-shaped membership function}" defined by:
	
	The parameters $a$ and $c$ locate the "feet" of the triangle and the parameter $b$ locates the peak:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
		
		%Shape: Axis 2D [id:dp9494165001496875] 
		\draw  (130,307.38) -- (516.5,307.38)(159.04,18) -- (159.04,341) (509.5,302.38) -- (516.5,307.38) -- (509.5,312.38) (154.04,25) -- (159.04,18) -- (164.04,25) (186.04,302.38) -- (186.04,312.38)(213.04,302.38) -- (213.04,312.38)(240.04,302.38) -- (240.04,312.38)(267.04,302.38) -- (267.04,312.38)(294.04,302.38) -- (294.04,312.38)(321.04,302.38) -- (321.04,312.38)(348.04,302.38) -- (348.04,312.38)(375.04,302.38) -- (375.04,312.38)(402.04,302.38) -- (402.04,312.38)(429.04,302.38) -- (429.04,312.38)(456.04,302.38) -- (456.04,312.38)(483.04,302.38) -- (483.04,312.38)(154.04,280.38) -- (164.04,280.38)(154.04,253.38) -- (164.04,253.38)(154.04,226.38) -- (164.04,226.38)(154.04,199.38) -- (164.04,199.38)(154.04,172.38) -- (164.04,172.38)(154.04,145.38) -- (164.04,145.38)(154.04,118.38) -- (164.04,118.38)(154.04,91.38) -- (164.04,91.38)(154.04,64.38) -- (164.04,64.38)(154.04,37.38) -- (164.04,37.38)(154.04,334.38) -- (164.04,334.38) ;
		\draw   ;
		%Straight Lines [id:da8246870444428189] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (159.04,307.38) -- (241.5,307.38) ;
		%Straight Lines [id:da1278554298183281] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (241.5,307.38) -- (321.5,37) ;
		%Straight Lines [id:da6836866386739784] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (321.5,37) -- (375,307) ;
		%Straight Lines [id:da638412251163812] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (375,307) -- (485.5,307) ;
		
		% Text Node
		\draw (143,310) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
		% Text Node
		\draw (128,271.4) node [anchor=north west][inner sep=0.75pt]    {$0.1$};
		% Text Node
		\draw (128,244.16) node [anchor=north west][inner sep=0.75pt]    {$0.2$};
		% Text Node
		\draw (128,216.94) node [anchor=north west][inner sep=0.75pt]    {$0.3$};
		% Text Node
		\draw (128,189.72) node [anchor=north west][inner sep=0.75pt]    {$0.4$};
		% Text Node
		\draw (128,162.5) node [anchor=north west][inner sep=0.75pt]    {$0.5$};
		% Text Node
		\draw (128,135.28) node [anchor=north west][inner sep=0.75pt]    {$0.6$};
		% Text Node
		\draw (128,108.06) node [anchor=north west][inner sep=0.75pt]    {$0.7$};
		% Text Node
		\draw (128,80.84) node [anchor=north west][inner sep=0.75pt]    {$0.8$};
		% Text Node
		\draw (128,53.62) node [anchor=north west][inner sep=0.75pt]    {$0.9$};
		% Text Node
		\draw (139,26.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (290,345) node [anchor=north west][inner sep=0.75pt]   [align=left] {trimf, P=[3,6,8]};
		% Text Node
		\draw (182.44,313.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (207.04,313.4) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (234.64,313.4) node [anchor=north west][inner sep=0.75pt]    {$3$};
		% Text Node
		\draw (261.24,313.4) node [anchor=north west][inner sep=0.75pt]    {$4$};
		% Text Node
		\draw (288.84,313.4) node [anchor=north west][inner sep=0.75pt]    {$5$};
		% Text Node
		\draw (314.44,313.4) node [anchor=north west][inner sep=0.75pt]    {$6$};
		% Text Node
		\draw (343.04,313.4) node [anchor=north west][inner sep=0.75pt]    {$7$};
		% Text Node
		\draw (369.64,313.4) node [anchor=north west][inner sep=0.75pt]    {$8$};
		% Text Node
		\draw (396.24,313.4) node [anchor=north west][inner sep=0.75pt]    {$9$};
		% Text Node
		\draw (419.84,313.4) node [anchor=north west][inner sep=0.75pt]    {$10$};
		% Text Node
		\draw (447.44,313.4) node [anchor=north west][inner sep=0.75pt]    {$11$};
		% Text Node
		\draw (476.08,313.4) node [anchor=north west][inner sep=0.75pt]    {$12$};
		
		\end{tikzpicture}
	\end{figure}
	Let us see with \texttt{R} the tipper problem managed with the fuzzy functions above (for more details see the \texttt{R} companion book!).
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Here is a copy/paste of the \texttt{R} script  given in \cite{wagner2011fuzzy} with detailed explanations:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/computing/fuzzy_tipping_problem_r.jpg}
	\end{figure}
	That gives:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.45]{img/computing/fuzzy_tipping_problem_r_plot.jpg}
	\end{figure}
	\end{tcolorbox}
	
	\subsubsection{Fuzzy set}
	\textbf{Definition (\#\thesection.\mydef):} Given $X$ a set. A "\NewTerm{fuzzy subset}" $A$ of $X$ is defined by a belonging function $f_A$ on $X$ with values in the interval $[0,1]$.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The belonging function $f_A$ can be set arbitrarily. A practical application problem is for the engineer to define these functions (we usually use statistical data or the opinion of an expert to make the least worst choice...).
	\end{tcolorbox}
	The notion of fuzzy subset encompasses that of classical subset for which $f_A$ is the indicator function given for recall by:
	
	\textbf{Definition (\#\thesection.\mydef):} If $A$ and $B$ are two sets, such that $A$ is included in $B$ (i.e. $A\subset B$), we name "\NewTerm{indicator function}\index{indicator function}" of $A$ (relatively to $B$), the function $1_A$ defined in $\{0,1\}$, and such that:
	
	This is for a classical set. Obviously such indicator functions are often very practical technical intermediaries!
	
	But for fuzzy logic we have scnerio such like this:
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	A possible characteristic function to define the fuzzy subset $A$ "to be twenty years" on the set $X$ of the real positive numbers:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
		
		%Shape: Axis 2D [id:dp9494165001496875] 
		\draw  (130,326.38) -- (516.5,326.38)(159.04,37) -- (159.04,360) (509.5,321.38) -- (516.5,326.38) -- (509.5,331.38) (154.04,44) -- (159.04,37) -- (164.04,44) (186.04,321.38) -- (186.04,331.38)(213.04,321.38) -- (213.04,331.38)(240.04,321.38) -- (240.04,331.38)(267.04,321.38) -- (267.04,331.38)(294.04,321.38) -- (294.04,331.38)(321.04,321.38) -- (321.04,331.38)(348.04,321.38) -- (348.04,331.38)(375.04,321.38) -- (375.04,331.38)(402.04,321.38) -- (402.04,331.38)(429.04,321.38) -- (429.04,331.38)(456.04,321.38) -- (456.04,331.38)(483.04,321.38) -- (483.04,331.38)(154.04,299.38) -- (164.04,299.38)(154.04,272.38) -- (164.04,272.38)(154.04,245.38) -- (164.04,245.38)(154.04,218.38) -- (164.04,218.38)(154.04,191.38) -- (164.04,191.38)(154.04,164.38) -- (164.04,164.38)(154.04,137.38) -- (164.04,137.38)(154.04,110.38) -- (164.04,110.38)(154.04,83.38) -- (164.04,83.38)(154.04,56.38) -- (164.04,56.38)(154.04,353.38) -- (164.04,353.38) ;
		\draw   ;
		%Straight Lines [id:da8246870444428189] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (159.04,326.38) -- (294.5,326.38) ;
		%Straight Lines [id:da1278554298183281] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (294.5,326.38) -- (339.5,56) ;
		%Straight Lines [id:da6836866386739784] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (360.5,56) -- (403.5,326) ;
		%Straight Lines [id:da638412251163812] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (403.5,326) -- (499.5,326) ;
		%Straight Lines [id:da644922407571465] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (159,56) -- (412.5,56) ;
		%Straight Lines [id:da1857690923698605] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (339.5,56) -- (360.5,56) ;
		
		% Text Node
		\draw (143,329) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
		% Text Node
		\draw (128,290.4) node [anchor=north west][inner sep=0.75pt]    {$0.1$};
		% Text Node
		\draw (128,263.16) node [anchor=north west][inner sep=0.75pt]    {$0.2$};
		% Text Node
		\draw (128,235.94) node [anchor=north west][inner sep=0.75pt]    {$0.3$};
		% Text Node
		\draw (128,208.72) node [anchor=north west][inner sep=0.75pt]    {$0.4$};
		% Text Node
		\draw (128,181.5) node [anchor=north west][inner sep=0.75pt]    {$0.5$};
		% Text Node
		\draw (128,154.28) node [anchor=north west][inner sep=0.75pt]    {$0.6$};
		% Text Node
		\draw (128,127.06) node [anchor=north west][inner sep=0.75pt]    {$0.7$};
		% Text Node
		\draw (128,99.84) node [anchor=north west][inner sep=0.75pt]    {$0.8$};
		% Text Node
		\draw (128,72.62) node [anchor=north west][inner sep=0.75pt]    {$0.9$};
		% Text Node
		\draw (139,45.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (182.44,332.4) node [anchor=north west][inner sep=0.75pt]    {$5$};
		% Text Node
		\draw (234.64,332.4) node [anchor=north west][inner sep=0.75pt]    {$10$};
		% Text Node
		\draw (284.84,332.4) node [anchor=north west][inner sep=0.75pt]    {$15$};
		% Text Node
		\draw (339.04,332.4) node [anchor=north west][inner sep=0.75pt]    {$20$};
		% Text Node
		\draw (392.24,332.4) node [anchor=north west][inner sep=0.75pt]    {$25$};
		% Text Node
		\draw (447.44,332.4) node [anchor=north west][inner sep=0.75pt]    {$30$};
		% Text Node
		\draw (119,17.4) node [anchor=north west][inner sep=0.75pt]    {$f_{A}(x)$};
		% Text Node
		\draw (495,334) node [anchor=north west][inner sep=0.75pt]   [align=left] {age};
		
		\draw (506,305) node [anchor=north west][inner sep=0.75pt]    {$x$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Centered linear fuzzy function}
	\end{figure}
	\end{tcolorbox}
	The following concepts are characteristic of $A$ in the field of fuzzy sets:
	
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] Support of $A$:
		

		\item[D2.] Height of $A$:
		

		\item[D3.] A is said to be normalized if $h(A)=1$
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		A possible characteristic function to define the fuzzy subset $A$ "to be twenty years" on the set $X$ of the real positive numbers:
		\begin{figure}[H]
			\centering
	
			\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
			%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
			
			% Pattern Info
 
			\tikzset{
			pattern size/.store in=\mcSize, 
			pattern size = 5pt,
			pattern thickness/.store in=\mcThickness, 
			pattern thickness = 0.3pt,
			pattern radius/.store in=\mcRadius, 
			pattern radius = 1pt}
			\makeatletter
			\pgfutil@ifundefined{pgf@pattern@name@_ytxno0otv}{
			\pgfdeclarepatternformonly[\mcThickness,\mcSize]{_ytxno0otv}
			{\pgfqpoint{0pt}{0pt}}
			{\pgfpoint{\mcSize+\mcThickness}{\mcSize+\mcThickness}}
			{\pgfpoint{\mcSize}{\mcSize}}
			{
			\pgfsetcolor{\tikz@pattern@color}
			\pgfsetlinewidth{\mcThickness}
			\pgfpathmoveto{\pgfqpoint{0pt}{0pt}}
			\pgfpathlineto{\pgfpoint{\mcSize+\mcThickness}{\mcSize+\mcThickness}}
			\pgfusepath{stroke}
			}}
			
			% Pattern Info
			 
			\tikzset{
			pattern size/.store in=\mcSize, 
			pattern size = 5pt,
			pattern thickness/.store in=\mcThickness, 
			pattern thickness = 0.3pt,
			pattern radius/.store in=\mcRadius, 
			pattern radius = 1pt}
			\makeatletter
			\pgfutil@ifundefined{pgf@pattern@name@_dvrmp2e2h}{
			\pgfdeclarepatternformonly[\mcThickness,\mcSize]{_dvrmp2e2h}
			{\pgfqpoint{0pt}{0pt}}
			{\pgfpoint{\mcSize+\mcThickness}{\mcSize+\mcThickness}}
			{\pgfpoint{\mcSize}{\mcSize}}
			{
			\pgfsetcolor{\tikz@pattern@color}
			\pgfsetlinewidth{\mcThickness}
			\pgfpathmoveto{\pgfqpoint{0pt}{0pt}}
			\pgfpathlineto{\pgfpoint{\mcSize+\mcThickness}{\mcSize+\mcThickness}}
			\pgfusepath{stroke}
			}}
			
			%Shape: Axis 2D [id:dp9494165001496875] 
			\draw  (130,326.38) -- (516.5,326.38)(159.04,37) -- (159.04,360) (509.5,321.38) -- (516.5,326.38) -- (509.5,331.38) (154.04,44) -- (159.04,37) -- (164.04,44) (186.04,321.38) -- (186.04,331.38)(213.04,321.38) -- (213.04,331.38)(240.04,321.38) -- (240.04,331.38)(267.04,321.38) -- (267.04,331.38)(294.04,321.38) -- (294.04,331.38)(321.04,321.38) -- (321.04,331.38)(348.04,321.38) -- (348.04,331.38)(375.04,321.38) -- (375.04,331.38)(402.04,321.38) -- (402.04,331.38)(429.04,321.38) -- (429.04,331.38)(456.04,321.38) -- (456.04,331.38)(483.04,321.38) -- (483.04,331.38)(154.04,299.38) -- (164.04,299.38)(154.04,272.38) -- (164.04,272.38)(154.04,245.38) -- (164.04,245.38)(154.04,218.38) -- (164.04,218.38)(154.04,191.38) -- (164.04,191.38)(154.04,164.38) -- (164.04,164.38)(154.04,137.38) -- (164.04,137.38)(154.04,110.38) -- (164.04,110.38)(154.04,83.38) -- (164.04,83.38)(154.04,56.38) -- (164.04,56.38)(154.04,353.38) -- (164.04,353.38) ;
			\draw   ;
			%Straight Lines [id:da8246870444428189] 
			\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (159.04,326.38) -- (294.5,326.38) ;
			%Straight Lines [id:da638412251163812] 
			\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (403.5,326) -- (499.5,326) ;
			%Straight Lines [id:da644922407571465] 
			\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (159,56) -- (412.5,56) ;
			%Straight Lines [id:da1857690923698605] 
			\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (339.5,56) -- (360.5,56) ;
			%Shape: Rectangle [id:dp4817652441541729] 
			\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (293.83,322.67) -- (402.83,322.67) -- (402.83,329.67) -- (293.83,329.67) -- cycle ;
			%Shape: Rectangle [id:dp7257555220945182] 
			\draw  [draw opacity=0][pattern=_ytxno0otv,pattern size=6pt,pattern thickness=0.75pt,pattern radius=0pt, pattern color={rgb, 255:red, 0; green, 0; blue, 0}] (333.83,322.83) -- (362.83,322.83) -- (362.83,329.5) -- (333.83,329.5) -- cycle ;
			%Shape: Rectangle [id:dp860515113041624] 
			\draw  [draw opacity=0][pattern=_dvrmp2e2h,pattern size=6pt,pattern thickness=0.75pt,pattern radius=0pt, pattern color={rgb, 255:red, 0; green, 0; blue, 0}] (481.17,69.33) -- (510.17,69.33) -- (510.17,76) -- (481.17,76) -- cycle ;
			%Shape: Rectangle [id:dp47536158937385187] 
			\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (480.83,92.67) -- (509.83,92.67) -- (509.83,99.67) -- (480.83,99.67) -- cycle ;
			%Straight Lines [id:da1278554298183281] 
			\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (294.5,326.38) -- (339.5,56) ;
			%Straight Lines [id:da6836866386739784] 
			\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (360.5,56) -- (403.5,326) ;
			
			% Text Node
			\draw (143,329) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
			% Text Node
			\draw (128,290.4) node [anchor=north west][inner sep=0.75pt]    {$0.1$};
			% Text Node
			\draw (128,263.16) node [anchor=north west][inner sep=0.75pt]    {$0.2$};
			% Text Node
			\draw (128,235.94) node [anchor=north west][inner sep=0.75pt]    {$0.3$};
			% Text Node
			\draw (128,208.72) node [anchor=north west][inner sep=0.75pt]    {$0.4$};
			% Text Node
			\draw (128,181.5) node [anchor=north west][inner sep=0.75pt]    {$0.5$};
			% Text Node
			\draw (128,154.28) node [anchor=north west][inner sep=0.75pt]    {$0.6$};
			% Text Node
			\draw (128,127.06) node [anchor=north west][inner sep=0.75pt]    {$0.7$};
			% Text Node
			\draw (128,99.84) node [anchor=north west][inner sep=0.75pt]    {$0.8$};
			% Text Node
			\draw (128,72.62) node [anchor=north west][inner sep=0.75pt]    {$0.9$};
			% Text Node
			\draw (139,45.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
			% Text Node
			\draw (182.44,332.4) node [anchor=north west][inner sep=0.75pt]    {$5$};
			% Text Node
			\draw (234.64,332.4) node [anchor=north west][inner sep=0.75pt]    {$10$};
			% Text Node
			\draw (284.84,332.4) node [anchor=north west][inner sep=0.75pt]    {$15$};
			% Text Node
			\draw (339.04,332.4) node [anchor=north west][inner sep=0.75pt]    {$20$};
			% Text Node
			\draw (392.24,332.4) node [anchor=north west][inner sep=0.75pt]    {$25$};
			% Text Node
			\draw (447.44,332.4) node [anchor=north west][inner sep=0.75pt]    {$30$};
			% Text Node
			\draw (119,17.4) node [anchor=north west][inner sep=0.75pt]    {$f_{A}( x)$};
			% Text Node
			\draw (495,334) node [anchor=north west][inner sep=0.75pt]   [align=left] {age};
			% Text Node
			\draw (514.83,61) node [anchor=north west][inner sep=0.75pt]   [align=left] {ker$\displaystyle ( A)$};
			% Text Node
			\draw (514.17,85) node [anchor=north west][inner sep=0.75pt]   [align=left] {supp$\displaystyle ( A)$};
			% Text Node
			\draw (482.17,111.73) node [anchor=north west][inner sep=0.75pt]    {$h(A) =1$};
			\draw (506,305) node [anchor=north west][inner sep=0.75pt]    {$x$};
			
			\end{tikzpicture}
			\vspace*{3mm}
			\caption{Centered linear fuzzy function}
		\end{figure}
		\end{tcolorbox}
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		The fuzzy subsets considered will all be assumed normalized, in extenso of height equal to $1$.
		\end{tcolorbox}
		
		\item[D4.] The kernel of $A$:
		
	
		\item[D5.] Cardinality of $A$:
		
		
		\item[D6.] $A$ is "more specific" than $B$ if:
		
		
		\item[D7.] $A$ is "more precise" than $B$ if:
		
		
		\item[D8.] There is equality between two fuzzy subsets if and only if:
		
		\item[D9.] There is equality between two fuzzy subsets if and only if:
		
		
		\item[D10.] There is inclusion between two fuzzy subsets if and only if:
		
		
		\item[D11.] The intersection $A\cap B$ is defined by:
		
		
		\item[D12.] The union $A\cup B$ is defined by:
		
	\end{enumerate}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us return to the case already envisaged. We consider the "twenty-year-old" people and those with "being in age" (ie, those who are in age to vote/drink alcohol) (dotted in the figure: we consider it as a non-fuzzy sub-set!):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
		
		%Shape: Axis 2D [id:dp9494165001496875] 
		\draw  (130,326.38) -- (516.5,326.38)(159.04,37) -- (159.04,360) (509.5,321.38) -- (516.5,326.38) -- (509.5,331.38) (154.04,44) -- (159.04,37) -- (164.04,44) (186.04,321.38) -- (186.04,331.38)(213.04,321.38) -- (213.04,331.38)(240.04,321.38) -- (240.04,331.38)(267.04,321.38) -- (267.04,331.38)(294.04,321.38) -- (294.04,331.38)(321.04,321.38) -- (321.04,331.38)(348.04,321.38) -- (348.04,331.38)(375.04,321.38) -- (375.04,331.38)(402.04,321.38) -- (402.04,331.38)(429.04,321.38) -- (429.04,331.38)(456.04,321.38) -- (456.04,331.38)(483.04,321.38) -- (483.04,331.38)(154.04,299.38) -- (164.04,299.38)(154.04,272.38) -- (164.04,272.38)(154.04,245.38) -- (164.04,245.38)(154.04,218.38) -- (164.04,218.38)(154.04,191.38) -- (164.04,191.38)(154.04,164.38) -- (164.04,164.38)(154.04,137.38) -- (164.04,137.38)(154.04,110.38) -- (164.04,110.38)(154.04,83.38) -- (164.04,83.38)(154.04,56.38) -- (164.04,56.38)(154.04,353.38) -- (164.04,353.38) ;
		\draw   ;
		%Straight Lines [id:da8246870444428189] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (159.04,326.38) -- (294.5,326.38) ;
		%Straight Lines [id:da638412251163812] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (403.5,326) -- (499.5,326) ;
		%Straight Lines [id:da644922407571465] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=3]  [dash pattern={on 7.88pt off 4.5pt}]  (159.04,326.38) -- (333.5,326.38) ;
		%Straight Lines [id:da1857690923698605] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (339.5,56) -- (360.5,56) ;
		%Straight Lines [id:da1278554298183281] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (294.5,326.38) -- (339.5,56) ;
		%Straight Lines [id:da6836866386739784] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (360.5,56) -- (403.5,326) ;
		%Straight Lines [id:da9957277008629206] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=3]  [dash pattern={on 7.88pt off 4.5pt}]  (333.5,56) -- (333.5,326.38) ;
		%Straight Lines [id:da31230060195569753] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=3]  [dash pattern={on 7.88pt off 4.5pt}]  (333.5,56) -- (500.5,56) ;
		
		% Text Node
		\draw (143,329) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
		% Text Node
		\draw (128,290.4) node [anchor=north west][inner sep=0.75pt]    {$0.1$};
		% Text Node
		\draw (128,263.16) node [anchor=north west][inner sep=0.75pt]    {$0.2$};
		% Text Node
		\draw (128,235.94) node [anchor=north west][inner sep=0.75pt]    {$0.3$};
		% Text Node
		\draw (128,208.72) node [anchor=north west][inner sep=0.75pt]    {$0.4$};
		% Text Node
		\draw (128,181.5) node [anchor=north west][inner sep=0.75pt]    {$0.5$};
		% Text Node
		\draw (128,154.28) node [anchor=north west][inner sep=0.75pt]    {$0.6$};
		% Text Node
		\draw (128,127.06) node [anchor=north west][inner sep=0.75pt]    {$0.7$};
		% Text Node
		\draw (128,99.84) node [anchor=north west][inner sep=0.75pt]    {$0.8$};
		% Text Node
		\draw (128,72.62) node [anchor=north west][inner sep=0.75pt]    {$0.9$};
		% Text Node
		\draw (139,45.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (182.44,332.4) node [anchor=north west][inner sep=0.75pt]    {$5$};
		% Text Node
		\draw (234.64,332.4) node [anchor=north west][inner sep=0.75pt]    {$10$};
		% Text Node
		\draw (284.84,332.4) node [anchor=north west][inner sep=0.75pt]    {$15$};
		% Text Node
		\draw (339.04,332.4) node [anchor=north west][inner sep=0.75pt]    {$20$};
		% Text Node
		\draw (392.24,332.4) node [anchor=north west][inner sep=0.75pt]    {$25$};
		% Text Node
		\draw (447.44,332.4) node [anchor=north west][inner sep=0.75pt]    {$30$};
		% Text Node
		\draw (119,17.4) node [anchor=north west][inner sep=0.75pt]    {$f_{A}( x)$};
		% Text Node
		\draw (495,334) node [anchor=north west][inner sep=0.75pt]   [align=left] {age};
		% Text Node
		\draw (506,305) node [anchor=north west][inner sep=0.75pt]    {$x$};
		
		\end{tikzpicture}
	\end{figure}
	According on the definitions of the intersection ("logical AND" or logical multiplication according to the Boolean Algebra) and the union ("logical OR" or logical addition according to Boolean Algebra), we can characterize the subsets (first figure below), as well as those "being in their twenties or being in age" (second figure below):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
		
		%Shape: Axis 2D [id:dp9494165001496875] 
		\draw  (130,326.38) -- (516.5,326.38)(159.04,37) -- (159.04,360) (509.5,321.38) -- (516.5,326.38) -- (509.5,331.38) (154.04,44) -- (159.04,37) -- (164.04,44) (186.04,321.38) -- (186.04,331.38)(213.04,321.38) -- (213.04,331.38)(240.04,321.38) -- (240.04,331.38)(267.04,321.38) -- (267.04,331.38)(294.04,321.38) -- (294.04,331.38)(321.04,321.38) -- (321.04,331.38)(348.04,321.38) -- (348.04,331.38)(375.04,321.38) -- (375.04,331.38)(402.04,321.38) -- (402.04,331.38)(429.04,321.38) -- (429.04,331.38)(456.04,321.38) -- (456.04,331.38)(483.04,321.38) -- (483.04,331.38)(154.04,299.38) -- (164.04,299.38)(154.04,272.38) -- (164.04,272.38)(154.04,245.38) -- (164.04,245.38)(154.04,218.38) -- (164.04,218.38)(154.04,191.38) -- (164.04,191.38)(154.04,164.38) -- (164.04,164.38)(154.04,137.38) -- (164.04,137.38)(154.04,110.38) -- (164.04,110.38)(154.04,83.38) -- (164.04,83.38)(154.04,56.38) -- (164.04,56.38)(154.04,353.38) -- (164.04,353.38) ;
		\draw   ;
		%Straight Lines [id:da8246870444428189] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (159.04,326.38) -- (324.5,326.38) ;
		%Straight Lines [id:da638412251163812] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (403.5,326) -- (499.5,326) ;
		%Straight Lines [id:da1857690923698605] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (339.5,56) -- (360.5,56) ;
		%Straight Lines [id:da1278554298183281] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (324.5,326.38) -- (324.5,139) ;
		%Straight Lines [id:da6836866386739784] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (360.5,56) -- (403.5,326) ;
		%Straight Lines [id:da14059483852435561] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (339.5,56) -- (324.5,139) ;
		
		% Text Node
		\draw (143,329) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
		% Text Node
		\draw (128,290.4) node [anchor=north west][inner sep=0.75pt]    {$0.1$};
		% Text Node
		\draw (128,263.16) node [anchor=north west][inner sep=0.75pt]    {$0.2$};
		% Text Node
		\draw (128,235.94) node [anchor=north west][inner sep=0.75pt]    {$0.3$};
		% Text Node
		\draw (128,208.72) node [anchor=north west][inner sep=0.75pt]    {$0.4$};
		% Text Node
		\draw (128,181.5) node [anchor=north west][inner sep=0.75pt]    {$0.5$};
		% Text Node
		\draw (128,154.28) node [anchor=north west][inner sep=0.75pt]    {$0.6$};
		% Text Node
		\draw (128,127.06) node [anchor=north west][inner sep=0.75pt]    {$0.7$};
		% Text Node
		\draw (128,99.84) node [anchor=north west][inner sep=0.75pt]    {$0.8$};
		% Text Node
		\draw (128,72.62) node [anchor=north west][inner sep=0.75pt]    {$0.9$};
		% Text Node
		\draw (139,45.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (182.44,332.4) node [anchor=north west][inner sep=0.75pt]    {$5$};
		% Text Node
		\draw (234.64,332.4) node [anchor=north west][inner sep=0.75pt]    {$10$};
		% Text Node
		\draw (284.84,332.4) node [anchor=north west][inner sep=0.75pt]    {$15$};
		% Text Node
		\draw (339.04,332.4) node [anchor=north west][inner sep=0.75pt]    {$20$};
		% Text Node
		\draw (392.24,332.4) node [anchor=north west][inner sep=0.75pt]    {$25$};
		% Text Node
		\draw (447.44,332.4) node [anchor=north west][inner sep=0.75pt]    {$30$};
		% Text Node
		\draw (119,17.4) node [anchor=north west][inner sep=0.75pt]    {$f_{A}( x)$};
		% Text Node
		\draw (495,334) node [anchor=north west][inner sep=0.75pt]   [align=left] {age};
		% Text Node
		\draw (506,305) node [anchor=north west][inner sep=0.75pt]    {$x$};
		
		\end{tikzpicture}
	\end{figure}
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
		
		%Shape: Axis 2D [id:dp9494165001496875] 
		\draw  (130,326.38) -- (516.5,326.38)(159.04,37) -- (159.04,360) (509.5,321.38) -- (516.5,326.38) -- (509.5,331.38) (154.04,44) -- (159.04,37) -- (164.04,44) (186.04,321.38) -- (186.04,331.38)(213.04,321.38) -- (213.04,331.38)(240.04,321.38) -- (240.04,331.38)(267.04,321.38) -- (267.04,331.38)(294.04,321.38) -- (294.04,331.38)(321.04,321.38) -- (321.04,331.38)(348.04,321.38) -- (348.04,331.38)(375.04,321.38) -- (375.04,331.38)(402.04,321.38) -- (402.04,331.38)(429.04,321.38) -- (429.04,331.38)(456.04,321.38) -- (456.04,331.38)(483.04,321.38) -- (483.04,331.38)(154.04,299.38) -- (164.04,299.38)(154.04,272.38) -- (164.04,272.38)(154.04,245.38) -- (164.04,245.38)(154.04,218.38) -- (164.04,218.38)(154.04,191.38) -- (164.04,191.38)(154.04,164.38) -- (164.04,164.38)(154.04,137.38) -- (164.04,137.38)(154.04,110.38) -- (164.04,110.38)(154.04,83.38) -- (164.04,83.38)(154.04,56.38) -- (164.04,56.38)(154.04,353.38) -- (164.04,353.38) ;
		\draw   ;
		%Straight Lines [id:da8246870444428189] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (159.04,326.38) -- (295.5,326.38) ;
		%Straight Lines [id:da1857690923698605] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (339.5,56) -- (493.5,56) ;
		%Straight Lines [id:da1278554298183281] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (295.5,326.38) -- (339.5,127) ;
		%Straight Lines [id:da14059483852435561] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (339.5,56) -- (339.5,127) ;
		
		% Text Node
		\draw (143,329) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
		% Text Node
		\draw (128,290.4) node [anchor=north west][inner sep=0.75pt]    {$0.1$};
		% Text Node
		\draw (128,263.16) node [anchor=north west][inner sep=0.75pt]    {$0.2$};
		% Text Node
		\draw (128,235.94) node [anchor=north west][inner sep=0.75pt]    {$0.3$};
		% Text Node
		\draw (128,208.72) node [anchor=north west][inner sep=0.75pt]    {$0.4$};
		% Text Node
		\draw (128,181.5) node [anchor=north west][inner sep=0.75pt]    {$0.5$};
		% Text Node
		\draw (128,154.28) node [anchor=north west][inner sep=0.75pt]    {$0.6$};
		% Text Node
		\draw (128,127.06) node [anchor=north west][inner sep=0.75pt]    {$0.7$};
		% Text Node
		\draw (128,99.84) node [anchor=north west][inner sep=0.75pt]    {$0.8$};
		% Text Node
		\draw (128,72.62) node [anchor=north west][inner sep=0.75pt]    {$0.9$};
		% Text Node
		\draw (139,45.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (182.44,332.4) node [anchor=north west][inner sep=0.75pt]    {$5$};
		% Text Node
		\draw (234.64,332.4) node [anchor=north west][inner sep=0.75pt]    {$10$};
		% Text Node
		\draw (284.84,332.4) node [anchor=north west][inner sep=0.75pt]    {$15$};
		% Text Node
		\draw (339.04,332.4) node [anchor=north west][inner sep=0.75pt]    {$20$};
		% Text Node
		\draw (392.24,332.4) node [anchor=north west][inner sep=0.75pt]    {$25$};
		% Text Node
		\draw (447.44,332.4) node [anchor=north west][inner sep=0.75pt]    {$30$};
		% Text Node
		\draw (119,17.4) node [anchor=north west][inner sep=0.75pt]    {$f_{A}( x)$};
		% Text Node
		\draw (495,334) node [anchor=north west][inner sep=0.75pt]   [align=left] {age};
		% Text Node
		\draw (506,305) node [anchor=north west][inner sep=0.75pt]    {$x$};
		
		\end{tikzpicture}
	\end{figure}
	\end{tcolorbox}
	
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{80} & \pbox{20cm}{\score{3}{5} \\ {\tiny 10 votes,  66.00\%}} 
	\end{tabular} 
	\end{flushright}


	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Error-Correcting Codes}
	\lettrine[lines=4]{\color{BrickRed}I}{f} the first half of the 120th century (holocene calendar) was that of the analogue revolution by radio and television, the second half of this century is that of the digital revolution and the systematic use of algebra in the data transmission. It is also the emergence automated error handling, where an "\NewTerm{Error}\index{error}" is a condition when the output information does not match with the input information. During transmission, digital signals suffer from noise that can introduce errors in the binary bits travelling from one system to other.
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
		
		% Gradient Info
  
		\tikzset {_apupibcje/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{-90 }  \pgftransformscale{2 }  }}}
		\pgfdeclarehorizontalshading{_i269gd8gd}{150bp}{rgb(0bp)=(0.84,0.91,1);
		rgb(37.589285714285715bp)=(0.84,0.91,1);
		rgb(62.5bp)=(0.58,0.76,0.99);
		rgb(100bp)=(0.58,0.76,0.99)}
		
		% Gradient Info
		  
		\tikzset {_9w76ps6af/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{-90 }  \pgftransformscale{2 }  }}}
		\pgfdeclarehorizontalshading{_2gsqvm6ls}{150bp}{rgb(0bp)=(0.84,0.91,1);
		rgb(37.589285714285715bp)=(0.84,0.91,1);
		rgb(62.5bp)=(0.58,0.76,0.99);
		rgb(100bp)=(0.58,0.76,0.99)}   
		
		%Shape: Trapezoid [id:dp2307814419791674] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (125,261) -- (137,221) -- (183,221) -- (195,261) -- cycle ;
		%Shape: Trapezoid [id:dp17258731010031525] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (492,261) -- (504,221) -- (550,221) -- (562,261) -- cycle ;
		%Rounded Rect [id:dp084080784437923] 
		\path  [shading=_i269gd8gd,_apupibcje] (100,142.2) .. controls (100,131.04) and (109.04,122) .. (120.2,122) -- (201.3,122) .. controls (212.46,122) and (221.5,131.04) .. (221.5,142.2) -- (221.5,202.8) .. controls (221.5,213.96) and (212.46,223) .. (201.3,223) -- (120.2,223) .. controls (109.04,223) and (100,213.96) .. (100,202.8) -- cycle ; % for fading 
		 \draw   (100,142.2) .. controls (100,131.04) and (109.04,122) .. (120.2,122) -- (201.3,122) .. controls (212.46,122) and (221.5,131.04) .. (221.5,142.2) -- (221.5,202.8) .. controls (221.5,213.96) and (212.46,223) .. (201.3,223) -- (120.2,223) .. controls (109.04,223) and (100,213.96) .. (100,202.8) -- cycle ; % for border 
		
		%Rounded Rect [id:dp24505216223463733] 
		\path  [shading=_2gsqvm6ls,_9w76ps6af] (467,142.2) .. controls (467,131.04) and (476.04,122) .. (487.2,122) -- (568.3,122) .. controls (579.46,122) and (588.5,131.04) .. (588.5,142.2) -- (588.5,202.8) .. controls (588.5,213.96) and (579.46,223) .. (568.3,223) -- (487.2,223) .. controls (476.04,223) and (467,213.96) .. (467,202.8) -- cycle ; % for fading 
		 \draw   (467,142.2) .. controls (467,131.04) and (476.04,122) .. (487.2,122) -- (568.3,122) .. controls (579.46,122) and (588.5,131.04) .. (588.5,142.2) -- (588.5,202.8) .. controls (588.5,213.96) and (579.46,223) .. (568.3,223) -- (487.2,223) .. controls (476.04,223) and (467,213.96) .. (467,202.8) -- cycle ; % for border 
		
		%Left Right Arrow [id:dp7782918589618455] 
		\draw   (221.5,202.4) -- (242.91,193) -- (242.91,197.7) -- (445.59,197.7) -- (445.59,193) -- (467,202.4) -- (445.59,211.8) -- (445.59,207.1) -- (242.91,207.1) -- (242.91,211.8) -- cycle ;
		%Shape: Pulse Wave Form [id:dp3286246992349766] 
		\draw  [line width=1.5]  (232,183) -- (246.31,183) -- (246.31,123) -- (269.19,123) -- (269.19,183) -- (283.5,183) ;
		%Shape: Pulse Wave Form [id:dp1854706304294722] 
		\draw  [line width=1.5]  (269.19,183) -- (283.5,183) -- (283.5,123) -- (306.39,123) -- (306.39,183) -- (320.69,183) ;
		%Straight Lines [id:da021684622732878855] 
		\draw    (261.5,115) -- (289.5,76) ;
		%Curve Lines [id:da7801985077239109] 
		\draw [line width=1.5]    (352.5,180) .. controls (372.5,203) and (366.5,119) .. (372.5,136) .. controls (378.5,153) and (381.17,133.09) .. (382.5,130) .. controls (383.83,126.91) and (389.5,175) .. (395.5,177) .. controls (401.5,179) and (406.68,126.39) .. (410.5,124) .. controls (414.32,121.61) and (421.5,163) .. (426.5,155) .. controls (431.5,147) and (434.5,112) .. (450.5,182) ;
		%Straight Lines [id:da3484924039986681] 
		\draw    (410.5,116) -- (387.5,77) ;
		
		% Text Node
		\draw (120.75,164) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{Computer 1}};
		% Text Node
		\draw (487.75,164) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{Computer 2}};
		% Text Node
		\draw (250,54) node [anchor=north west][inner sep=0.75pt]   [align=left] {binary signal};
		% Text Node
		\draw (372,54) node [anchor=north west][inner sep=0.75pt]   [align=left] {noise};
		
		\end{tikzpicture}
	\end{figure}
	The "\NewTerm{error correction codes}\index{error correction codes}" (ECC) are used to add redundancy to the data to make it tolerant to the transmission errors (at least to a given degree). Basically, the idea is to encode one way or another an information sequence and add encoding the original data as a control data integrity. So even if some of the information is corrupted, but not too much... redundancy will identify the incorrect parts of the message.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	ECC are not only used for data transmission but also to give unique identifiers to bills in some countries and also in some factories to build a nomenclature system (naming) of pieces and to control afterwards the input of employees in computer systems! For example, bar-codes and QR-codes contains ECC.
	\end{tcolorbox}
	
	Thus, after the appearance of audio CD in the early 11980s (holocene calendar), we must take into account the development of broadcasting satellites  and new means of communication such as the fax, the Minitel, Internet or digital phone using any correcting codes errors (CCE). Even photography, radio and books are becoming digital (and will probably only be in this format in a hundred years).
	
	Image or sound reproduction techniques are related to the transmission and to the correct reading of many digital messages, also known as "words". A message consists of words themselves made up of symbols (a particular example being the "bit" which for reminder is contraction of "BInary digiT"), taken from an alphabet. If the alphabet is binary so each symbol will be a bit.
	
	Let us take the message $00101$ formed of $5$ bits each worth $0$ or $1$. If we send the message as is, a transmission error or of reading can take place and make the message unintelligible (or the bill/piece unique identifier). Let us decide to repeat the message three times and send it:
	
	If the received message contains an error, this error can be corrected. If there are two errors, the receiver is able to detect that there was a mistake but can not always recover the original message. Finally, if it occurs more than two errors during transmission, the receiver can not detect them.
	
	We have seen just now a first example of ECC, named "\NewTerm{repeatedly code}\index{repeatedly code}". This code, which corrects errors and detects two, was used in some Audio CD player having three heads. The signal $0$ or $1$ is read independently by each of these three heads to give a word of three digits, and a reading error can be sometimes corrected.
	
	Note that it is natural to extend a message to protect it. Let us consider the words of a language. They are usually very far from each other, two words differs according to their lengths and in the letters and syllables used. Thus it will be difficult to confound the words "library" and "cabinet" although these words are mispronounced or misheard and we will naturally reconstitute the message in a conversation even when some letters would be deleted or distorted. The military meanwhile spell some information by saying "Alpha Zulu" for "AZ" to avoid errors...
	
	A second example widely used in computing science of error detection is the addition of a "\NewTerm{parity bit}\index{parity bit}". Let $00101$ be the original message and add it a last bit obtained by adding five bits of departure modulo $2$. The message is $001010$ and we can therefore detect errors but can not correct it.... For this, we make the sum of all the bits to obtain $0$ if there is no error, and $1$ otherwise. This code named "\NewTerm{parity code}\index{parity code}" is used everywhere: in the social security numbers where we add a key, in those of bank accounts or in bar-code of supermarkets where it is the 13th digit that is the control key (the space probe Voyager II is one of the many users of parity codes to communicate almost in a reliable way and also the $8$th bit in the ASCII system which is used as a parity bit).
	
	For many years, the DRAM sockets managed words with one bit only; it was then necessary to put the $8$ memory card sockets for working with bytes ($8$ bits). But at this time many cards included not $8$ but $9$ sockets! The ninth socket was designed to store a parity bit at each start of a memory byte. When reading a byte, we checked, if between the time of writing and that of reading, parity had not been changed (due to a parasite, for example).
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/computing/ecc_ram.jpg}
		\caption{ECC RAM vs Non-ECC RAM}
	\end{figure}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	We would like to point out that ECC memory is considered server-grade. They are sold to a market which pays much closer attention to reliability than the consumer market. As a result, ECC RAM is usually subjected to much stricter testing and validation before shipping. This is a big factor in why ECC modules see much lower failure rates.
	\end{tcolorbox}
	Finally, let us see a third example used on some computer servers that use  in RAID4 or RAID6 parallel disks, this latter using Hamming codes that we will see later (see page \pageref{Hamming distance}).
	
	Suppose we have $3$ hard drives, and the content of the first byte of each disc is the following:
	
	So it is sufficient take each column and count the number $p$ of $1$ in the column. The value of the parity bit is then $p$ modulo $2$. We have for the first column in the example above $p$ which is $2$. Therefore, the parity bit is equal to $0$, etc. Then we have on the control disc (CD):
	
	These three examples are fundamental to the basic coding theory and show that we can control the appearance of error by deliberately lengthening the message before transmission or reading. More sophisticated algebraic techniques are then used to improve the performance of coding, thanks to:
	
	\begin{enumerate}
		\item Know if any errors occurred (detection problems)
		
		\item Find the initial correct message from the message received (correction of problem)
		
		\item Correct the most  possible mistakes while using the least possible additional bits (the problem of performance encoding)
	\end{enumerate}
	From the mathematical point of view, one of the interests of coding theory is to show that algebra applies once again well in our everyday life when we listen to music, or we settle in front of our televisions, and that such abstract notions as those of vector spaces or polynomials over finite fields allow us to read messages, listen to music or watch movies in optimum conditions!
	
	We distinguish the following two classes of ECC: 
	\begin{enumerate}
		\item Block codes
		\item Treillis codes
	\end{enumerate}
	The figure below provides a simple overview of the error correcting codes family\footnote{For example "Quantum Error Correction" is not indicated}. In the first class (right in the figure), we have the most popular codes such as BCH codes, Reed-Solomon and Goppa, Golay and Hamming. The second class (left in the figure) is less rich in variety but has a lot more flexibility, especially in the choice of parameters and decoding algorithms available. Let us cite for example the binary systematic recursive convolutional codes widely used in coded modulation and the parallel concatenated codes (Turbo Codes).
	\begin{figure}[H]
    	\centering
    	%run compilation always at least three times to get the final tree
    	 \begin{forest}
                for tree={
                    forked edges,
                    grow'=0,
                    draw,
                    rounded corners,
                    node options={align=center,},
                    text width=3cm,
                    inner sep=2pt
                },
                [\textbf{Error Correcting Codes}, fill=gray!45, parent
                %
                    [Treillis Codes, for tree={fill=brown!45, child}
                        [Modulation Coded into Treillis, fill=brown!30, grandchild]
                        [Convolutive Codes, fill=brown!30, grandchild
                        	[Recursive Codes, fill=brown!30, greatgrandchild]                        
                        	[Non-Recursive Codes, fill=brown!30, greatgrandchild]
                        ]
                    ]
                    %
                    [Blocks Codes, for tree={fill=red!45,child}
                        [Non-Linear Codes, fill=red!30, grandchild]
                        [Linear Codes, fill=red!30, grandchild
                        	[Cyclic Codes, fill=red!20, greatgrandchild
                        		[Hamming Codes, fill=red!10, referenceblock]
                        		[Parity check Codes, fill=red!10, referenceblock]
                        		[Repetition Codes, fill=red!10, referenceblock]
                        		[BCH Codes, fill=red!10, referenceblock]
                        		[Reed Solomon Codes, fill=red!10, referenceblock]
                        	]
							[Non-Cyclic Codes, fill=red!20, greatgrandchild]
						]
                        [Modulation Coded into Blocks, fill=red!30, grandchild]
                    ]
                ]
            \end{forest}
   		\vspace*{3mm}
		\caption{Non-exhaustive orgchart of correcting codes}
    \end{figure}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	To introduce the foundations of the theory of error correcting codes, we recommend strongly the reader to have read through first the section of Set Theory (see page \pageref{set theory}), after this of  Statistical Mechanics (where the information theory is at page \pageref{statistical information theory}) after of Numerical Methods (see page \pageref{numerical methods}) and finally of Topology (see page \pageref{topology}).
	\end{tcolorbox}
	
	When possible we will show the reader how to put quickly in practice ECC using native MATLAB™ Communication Toolbox functions.
	
	\subsection{CheckSum}\label{checksum}
	Before starting the part of pure mathematics, we would like to make a small introduction to the "\NewTerm{checksum}\index{checksum}" (control sum) that is a tool frequently used in the business when exchanging files over several Giga Bytes between two computers or when downloading on the Internet.
	
	The checksum, also sometimes named "fingerprint" is a basic concept of coding theory used for correcting codes. It corresponds to a particular case of "redundancy control code". It is widely used in computer and digital telecommunications as already said.
	
	One basic technique (among a dozens more or less sophisticated) is to take the sum of a given length of bits (byte, word, or other ...) and calculate the modulo 255 (FF in hexadecimal). For example, if we take two words and we rely on their hexadecimal ASCII code (you can find ASCII tables almost everywhere on the internet):
	\begin{figure}[H]
		\centering
		\resizebox{\columnwidth}{!}{
  		 \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
		
		%Shape: Square [id:dp26194721614926686] 
		\draw   (40,55) -- (93,55) -- (93,108) -- (40,108) -- cycle ;
		%Shape: Square [id:dp08647708768988438] 
		\draw   (93,55) -- (146,55) -- (146,108) -- (93,108) -- cycle ;
		%Shape: Square [id:dp801822849659124] 
		\draw   (146,55) -- (199,55) -- (199,108) -- (146,108) -- cycle ;
		%Shape: Square [id:dp8358835222674759] 
		\draw   (199,55) -- (252,55) -- (252,108) -- (199,108) -- cycle ;
		%Shape: Square [id:dp07885595489043506] 
		\draw   (252,55) -- (305,55) -- (305,108) -- (252,108) -- cycle ;
		%Straight Lines [id:da8320068393993385] 
		\draw    (172,108) -- (172,194) ;
		\draw [shift={(172,196)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Square [id:dp01399404136511695] 
		\draw   (40,198) -- (93,198) -- (93,251) -- (40,251) -- cycle ;
		%Shape: Square [id:dp06106768884704428] 
		\draw   (93,198) -- (146,198) -- (146,251) -- (93,251) -- cycle ;
		%Shape: Square [id:dp21473446246804895] 
		\draw   (146,198) -- (199,198) -- (199,251) -- (146,251) -- cycle ;
		%Shape: Square [id:dp23077809918081793] 
		\draw   (199,198) -- (252,198) -- (252,251) -- (199,251) -- cycle ;
		%Shape: Square [id:dp7574499603445806] 
		\draw   (252,198) -- (305,198) -- (305,251) -- (252,251) -- cycle ;
		%Straight Lines [id:da9224707503350611] 
		\draw    (172,251) -- (172,310) ;
		\draw [shift={(172,312)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Square [id:dp014395196262842136] 
		\draw   (119,311) -- (172,311) -- (172,364) -- (119,364) -- cycle ;
		%Shape: Square [id:dp2660733024369375] 
		\draw   (172,311) -- (225,311) -- (225,364) -- (172,364) -- cycle ;
		%Shape: Rectangle [id:dp6877318713874725] 
		\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]  (119,311) -- (225,311) -- (225,364) -- (119,364) -- cycle ;
		%Shape: Square [id:dp9118055062963795] 
		\draw   (334,55) -- (387,55) -- (387,108) -- (334,108) -- cycle ;
		%Shape: Square [id:dp06787700083881831] 
		\draw   (387,55) -- (440,55) -- (440,108) -- (387,108) -- cycle ;
		%Shape: Square [id:dp6652578498512605] 
		\draw   (440,55) -- (493,55) -- (493,108) -- (440,108) -- cycle ;
		%Shape: Square [id:dp3876584582394407] 
		\draw   (493,55) -- (546,55) -- (546,108) -- (493,108) -- cycle ;
		%Shape: Square [id:dp055947873336195775] 
		\draw   (546,55) -- (599,55) -- (599,108) -- (546,108) -- cycle ;
		%Straight Lines [id:da22334722627864867] 
		\draw    (466,108) -- (466,194) ;
		\draw [shift={(466,196)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Square [id:dp09315933317430458] 
		\draw   (334,198) -- (387,198) -- (387,251) -- (334,251) -- cycle ;
		%Shape: Square [id:dp9890658649519408] 
		\draw   (387,198) -- (440,198) -- (440,251) -- (387,251) -- cycle ;
		%Shape: Square [id:dp8813655429029887] 
		\draw   (440,198) -- (493,198) -- (493,251) -- (440,251) -- cycle ;
		%Shape: Square [id:dp4472901141631602] 
		\draw   (493,198) -- (546,198) -- (546,251) -- (493,251) -- cycle ;
		%Shape: Square [id:dp5123285129794324] 
		\draw   (546,198) -- (599,198) -- (599,251) -- (546,251) -- cycle ;
		%Straight Lines [id:da32842941848121576] 
		\draw    (466,251) -- (466,310) ;
		\draw [shift={(466,312)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Square [id:dp039352877602889746] 
		\draw   (413,311) -- (466,311) -- (466,364) -- (413,364) -- cycle ;
		%Shape: Square [id:dp6583563045329732] 
		\draw   (466,311) -- (519,311) -- (519,364) -- (466,364) -- cycle ;
		%Shape: Rectangle [id:dp2424035801513329] 
		\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]  (413,311) -- (519,311) -- (519,364) -- (413,364) -- cycle ;
		
		% Text Node
		\draw (51.75,69) node [anchor=north west][inner sep=0.75pt]  [font=\huge] [align=left] {H \ \ \ e \ \ \ \ l \ \ \ \ l \ \ \ \ o};
		% Text Node
		\draw (175,139) node [anchor=north west][inner sep=0.75pt]   [align=left] {ASCII};
		% Text Node
		\draw (50,211) node [anchor=north west][inner sep=0.75pt]  [font=\huge] [align=left] {$\displaystyle 48\ \ \ 65\ \ 6C\ 6C\ \ 6F$};
		% Text Node
		\draw (175,273) node [anchor=north west][inner sep=0.75pt]   [align=left] {Hash};
		% Text Node
		\draw (123,323) node [anchor=north west][inner sep=0.75pt]  [font=\huge] [align=left] {$\displaystyle F4\ \ 31$};
		% Text Node
		\draw (10,380) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$ \begin{array}{l}
		48+65+6C+6C+6F=F4\ \text{mod} \ FF\\
		48\cdot 1+65\cdot 2+6C\cdot 3+6C\cdot 4+6F\cdot 5=31\ \text{mod} \ FF
		\end{array}$};
		% Text Node
		\draw (283,329) node [anchor=north west][inner sep=0.75pt]   [align=left] {Fingerprint};
		% Text Node
		\draw (469,139) node [anchor=north west][inner sep=0.75pt]   [align=left] {ASCII};
		% Text Node
		\draw (469,273) node [anchor=north west][inner sep=0.75pt]   [align=left] {Hash};
		% Text Node
		\draw (416,323) node [anchor=north west][inner sep=0.75pt]  [font=\huge] [align=left] {$\displaystyle E6\ EB$};
		% Text Node
		\draw (337,380) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$ \begin{array}{l}
		48+65+6C+6C+6F=F4\ \text{mod} \ FF\\
		48\cdot 1+65\cdot 2+6C\cdot 3+6C\cdot 4+61\cdot 5=EB\ \text{mod} \ FF
		\end{array}$};
		% Text Node
		\draw (344.75,69) node [anchor=north west][inner sep=0.75pt]  [font=\huge] [align=left] {H \ \ \ e \ \ \ \ l \ \ \ \ l \ \ \ \ \textcolor[rgb]{0.29,0.56,0.89}{\textbf{a}}};
		% Text Node
		\draw (341,211) node [anchor=north west][inner sep=0.75pt]  [font=\huge] [align=left] {$\displaystyle 48\ \ \ 65\ \ 6C\ \ 6C\ \ \textcolor[rgb]{0.29,0.56,0.89}{61}$};
		\end{tikzpicture}
		}
		\vspace*{3mm}
		\caption{Basic Checksum principle}
	\end{figure}
	Some tools also use the MD5 algorithm (Message Digest 5) or SHA (Secure Hash Algorithm) to have a checksum of a message (\SeeChapter{see section Cryptography page \pageref{md5} and page \pageref{sha 1}}).
	
	\subsubsection{Luhn algorithm}
	The "\NewTerm{Luhn algorithm}\index{Luhn algorithm}" or "\NewTerm{Luhn formula}", also known as the "modulus 10" or "mod 10" algorithm, is a simple checksum formula used to validate a variety of identification numbers, such as Credit card numbers, IMEI numbers, National Provider Identifier numbers in the US, and Canadian Social Insurance Numbers (see examples further below). It was created by IBM scientist Hans Peter Luhn and described in U.S. Patent No. 2,950,048, filed on January 6, 11954 (holocene calendar), and granted on August 23, 11960 (holocene calendar).

	The algorithm is in the public domain and is in wide use today. It is specified in ISO/IEC 7812-1 and is completely described in the following figure:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/computing/luhn_algorithm.jpg}
		\caption{Luhn algorithm for checksum principle}
	\end{figure}
	The Luhn algorithm will detect any single-digit error, as well as almost all transpositions of adjacent digits. It will not, however, detect transposition of the two-digit sequence $09$ to $90$ (or vice versa). It will detect $7$ of the $10$ possible twin errors (it will not detect $22 \leftrightarrow 55$, $33 \leftrightarrow 66$ or $44 \leftrightarrow 77$).
	
	A picture is worth a thousand words let us see how this apply to VISA cards:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/computing/luhn_algorithm_visa.jpg}
	\end{figure}

	Other, more complex check-digit algorithms (such as the Verhoeff algorithm and the Damm algorithm) can detect more transcription errors.
	
	\pagebreak
	\subsection{Check Digit}
	Also before starting the part of pure mathematics, we would like to make a small introduction to some common check digits (in fact any professionally driven government or company should have all objects or individuals identifiers with at least one check digit). We will only focus on example that have been asked to me by my students or people that contacted me on the Internet (otherwise I can dedicate a whole book only for examples on check digits...)!
	
	\textbf{Definition (\#\thesection.\mydef):} A "\NewTerm{check digit}\index{check digit}" is a form of redundancy check used for error detection on identification numbers, such as bank account numbers, which are used in an application where they will at least sometimes be input manually. It is analogous to a binary parity bit used to check for errors in computer-generated data. It consists of one or more digits computed by an algorithm from the other digits (or letters) in the sequence input.
	
	\subsubsection{European Article Numbering (EAN-13)}
	Let us see first the check digit of EAN-13 bar codes (Cyclic Redundancy Check\footnote{type of checksum, specifically a position dependent checksum algorithm} (CRC) ECC type):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/computing/ean_13_barcod.jpg}
		\caption{EAN-13 bar-code example}
	\end{figure}
	There are various different errors that can occur when numbers are written, printed or transferred in any manner. Different methods of assigning check digits are better at detecting certain kinds of errors than others. The most common types of errors that occur in practice and their frequencies, according to one study, are as follows:
	\begin{table}[H]
	\begin{center}
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|l|c|l|}
				\hline
				\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Error type}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Form}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Relative frequency}} \\ \hline
				 Single error & $a$ replaced by $b$ & $79.1\%$ \\ \hline
				 Transposition of adjacent digits & $ab$ replaced by $ba$ & $10.2\%$ \\ \hline
				 Jump transposition & $abc$ replaced by $cba$ & $0.8\%$ \\ \hline
				 Twin error & $aa$ replaced by $bb$ & $0.5\%$\\ \hline
				 Phonetic error & $a0$ swapped with $1a$, $a = 2,\ldots,9$ & $0.5\%$ \\ \hline
				 Jump twin error & $aca$ replaced by $bcb$ & $0.3\%$ \\ \hline
		\end{tabular}
	\end{center}
	\caption[]{Various costs}
	\end{table}
	Another common type of error not mentioned here is accidental insertion or deletion of characters. In the cases we will consider, the number will have a fixed length, so insertions and deletions will be automatically detected.
	
	The EAN-13 format uses a modulus $10$ scheme, with check digit ($a_ c$) defined by:
	
	For example, if we start with the number $1234567$ in the EAN-8 scheme, then our check digit is:
	
	which makes the full bar code number $12345670$.
	
	When dealing with ECC we must worry about how effective a scheme is in detecting errors!
	
	Let us consider only two cases:
	\begin{enumerate}
		\item Single error detection rate:

		If a digit $d$ whose weight is $1$ is changed to $c$, the weighted sum will change by $d-c$. The error will go undetected only if $d-c=0 \mod 10$. But this happens only when $d=c$, in which case there has not been an error after all, so all errors of this kind are caught.

		What if the weight were $3$? Then the error would be undetected if $3(d-c)=0 \mod 10$. But again, this cannot happen unless $d=c$. Thus, this method has a $100\%$ single error detection rate (SEDR).

		\item Transposition of adjacent digits detection rate:

		Suppose two adjacent digits, $cd$, are transposed to $dc$. If $c$’s weight is $3$ (hence $d$’s weight is $1$), the weighted sum is changed by:
		
		which will be detected unless $2(c-d)=0 \mod 10$, which can happen only if $c$ and $d$ differ by $5$. The same would have applied if $c$ had been weighted by $1$ and $d$ by $3$.

		As a result, the transpositions that will go undetected must involve $0 \leftrightarrow 5, 1\leftrightarrow 6, 2 \leftrightarrow 7, 3 \leftrightarrow 8$ and $4 \leftrightarrow 9$. So, $10$ transpositions are undetectable.

		There are 100 possibilities for each pairing, and the transposition of 90 of these would result in an error. Therefore the detection rate is $ 80/90 = 88.9\% $ transposition error detection rate (TEDR).
	\end{enumerate}
	
	\subsubsection{Swiss Post Payment slip}
	Another famous example for my country is the Swiss Post payment slip where numerous numbers have a check digit.
	
	Here is a sample preview of such a payment slip:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{img/computing/swiss_payment_slip.jpg}
		\caption{Swiss payment slip}
	\end{figure}
	and below is the technical description of the bottom right number with in red the check digit on which we will focus here:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/computing/swiss_payment_slip_check_digit.jpg}
		\caption{Swiss payment slip check digit for Reference Number}
	\end{figure}
	All the digit check in this Swiss payment slip are based on a recursive modulo $10$ computation.
	
	Let us give a small example on how to calculate the check digit based on the Swiss method. For this let us consider first the following matrix:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.55]{img/computing/swiss_payment_slip_matrix.jpg}
	\end{figure}
	So what would be the check digit of the number 70004152 (it's a typical 8 digit Swiss Post account)?
	
	The process to determine that latter is simply the following:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.74]{img/computing/swiss_payment_slip_check_digit_calculation_procedure.jpg}
	\end{figure}
	
	\subsubsection{International Bank Account Number (IBAN)}
	The International Bank Account Number (IBAN) is an internationally agreed system of identifying bank accounts across national borders to facilitate the communication and processing of cross border transactions with a reduced risk of transcription errors. It was originally adopted by the European Committee for Banking Standards (ECBS), and later as an international standard under ISO 13616.

	The IBAN consists of up to 34 alphanumeric characters comprising: a country code; two check digits; and a number that includes the domestic bank account number, branch identifier, and potential routing information. The check digits enable a sanity check of the bank account number to confirm its integrity before submitting a transaction.

	Before IBAN errors of transcription were not detectable and it was not possible for a sending bank to validate the routing information prior to submitting the payment. Routing errors caused delayed payments and incurred extra costs to the sending and receiving banks and often to intermediate routing banks.

	The IBAN should not contain spaces when transmitted electronically. When printed it is expressed in groups of four characters separated by a single space, the last group being of variable length. Here is an example of how typically Switzerland write IBAN numbers:
	\begin{center}
		\texttt{CH93 0076 2011 6238 5295 7}
	\end{center}
	Permitted IBAN characters are the digits $0$ to $9$ and the $26$ upper-case Latin alphabetic characters $A$ to $Z$. This applies even in countries (e.g., Thailand) where these characters are not used in the national language.
	
	An IBAN is validated by converting it into an integer and performing a basic modulo $97$ operation (as described in ISO 7064: \textit{Security techniques - Check character systems}) on it. If the IBAN is valid, the remainder equals $1$.

	The procedure is a follows:
	\begin{itemize}
		\item Check that the total IBAN length is correct as per the country. If not, the IBAN is invalid;

		\item Move the four initial characters to the end of the string;

		\item Replace each letter in the string with two digits, thereby expanding the string, where A $= 10$, B $= 11$, ..., Z $= 35$;
	
		\item Interpret the string as a decimal integer and compute the remainder of that number on division by $97$.
	\end{itemize}
	If the remainder is $1$, the check digit test is passed and the IBAN might be valid.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Given a fictitious United Kingdom bank, sort code \texttt{12-34-56}, account number \texttt{98765432}. For the sanity check we follow the above procedure:
	\begin{itemize}
		\item The IBAN is therefore:
		\begin{center}
			\texttt{GB82 WEST12345698765432}
		\end{center}

		\item We rearrange:
		\begin{center}
			\texttt{WEST12345698765432GB82}
		\end{center}

		\item We convert to integer:
		\begin{center}
			\texttt{3214282912345698765432161182}
		\end{center}

		\item We compute the modulo $97$:
		\begin{center}
			\texttt{3214282912345698765432161182} $\mod 97=1$
		\end{center}
	\end{itemize}
	\end{tcolorbox}
	
	\subsubsection{UIC wagon numbers}
	Wagon numbers (or coach numbers) are key data for railway operations. They enable a railway wagon or coach to be positively identified and form a common language between railway operators, infrastructure companies and the state authorities. The system of wagon numbering has been laid down by the International Union of Railways (Union Internationale des Chemins de fer or UIC) and is similar to that used for the locomotives and multiple units.
	\begin{figure}[H]
		\centering
		\begin{subfigure}{0.4\textwidth}
			\includegraphics[width=\textwidth]{img/computing/uic_wagon_slovakia.jpg}
			\caption{Slovak UIC wagon number}
		\end{subfigure}
		\begin{subfigure}{0.4\textwidth}
			\includegraphics[width=\textwidth]{img/computing/uic_wagon_switzerland.jpg}
			\caption{Swiss UIC wagon number}
		\end{subfigure}				
	\end{figure}
	The complete wagon number comprises 12 digits. The individual digits have the following meaning:
	\begin{itemize}
		\item Digit 1-2: Type of vehicle and indication of the interoperability capacity

		\item Digit 3-4: Country Code (Switzerland (CH) = $85$)

		\item Digit 5-8: Vehicle type information

		\item Digit 9-11: Individual running number (serial number)
		
		\item Digit 12: Self-check digit
	\end{itemize}
	The digits are multiplied individually from right to left alternately by $2$ and $1$, and digit summed. The difference between this sum and the next multiple of ten is the check digit, placed after the eleventh digit, separated by a dash.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Given a fictitious Wagon UIC \texttt{21-81-24 7121 7}. For the sanity check we follow the above procedure:
	\begin{itemize}
		\item The UIC is therefore:
		\begin{center}
			 \texttt{2 1 8 1 2 4 7 1 2 1 7}
		\end{center}

		\item We multiply:
		\begin{center}
			 \texttt{2} $\cdot 2\quad$ \texttt{1}$\cdot 1\quad$ \texttt{8}$\cdot 2\quad$ \texttt{1}$\cdot 1\quad$ \texttt{2}$\cdot 2\quad$ \texttt{4}$\cdot 1\quad$ \texttt{7}$\cdot 2\quad$ \texttt{1}$\cdot 1\quad$ \texttt{2}$\cdot 2\quad$ \texttt{1}$\cdot 1\quad$ \texttt{7}$\cdot 2$
		\end{center}
		It gives:
		\begin{center}
			$4 \; 1\; 16 \; 1\; 4\; 4\; 14\; 1\; 4\; 1\; 14$
		\end{center}

		\item We sum up the digits using:
		\begin{gather*}
			4+1+1+6+1+4+4+1+4+1+4+1+1+4=37
		\end{gather*}
		
		\item We take the next multiple of $10$ that $40$ so the check digit is equal to three $3$.
		
		\item Finally we get:
		\begin{center}
			\texttt{21-81-24 7121 7-3}
		\end{center}
	\end{itemize}
	\end{tcolorbox}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The swiss "Unternehmens-Identifikationsnummer" (company identification number) also has a check digit and so on...
	\end{tcolorbox}
	
	
	\pagebreak
	\subsection{Permutations}
	Also before starting the part of pure mathematics, we would make a small introduction to the check digit using permutations in VISA card numbers.
	
	Credit cards use an error-detecting scheme that was developed by IBM. It uses the permutation:
	
	In other words (for more details see the section of Set Algebra page \pageref{set algebra}) $\sigma (0)=0$, $\sigma (1)=2$, $\sigma (2)=4$, etc.
	
	Notice also that:
	

	In a $16$ digit credit card number, the final digit is the check digit. Let the credit card number be $(a_1, a_2,\ldots , a_{15}, a_{16})$, with $a_{16}$ being the check digit. Then:
	
	Note that in this example the permutation was applied to $a_ i$ where $i$ is odd ($a_1, a_3$, etc), because there is an odd number of digits excluding the check digit. Had this scheme been used on a number with an even number of digits excluding the check digit, the permutation would have been applied to $a_ i$ where $i$ was even.
	
	As we know form the previous example, dealing we ECC must worry about how effective a scheme is in detecting errors!
	
	Let us consider again only two cases:
	\begin{enumerate}
		\item Single error detection rate:

		This scheme catch all single-digit errors ($100\%$ SEDR). Indeed, for example if digit $a_ i$ is changed from $c$ to $d$, and $i$ is even, the remainder will change by $c-d$, which is non-zero (and is, of course, smaller than the modulus $N=10$). If $i$ is odd, it will change by $\sigma (c)-\sigma (d)$. This is again non-zero: $\sigma (c)$ cannot be equal to $\sigma (d)$ if $\sigma $ is a permutation.

		\item Transposition of adjacent digits detection rate:
		
		If two adjacent digits $c$ and $d$ are transposed, one of them must have the permutation applied - say $c$. The remainder will be unchanged only if $\sigma (c)+d = \sigma (d)+c$. Since $\sigma (x) = 2x \mod 9$, this happens only when $c = d \mod 9$, that is, only when $c$ and $d$ are $0$ and $9$ (in either order).
		
		Therefore, for each pair of adjacent digits, of the $90$ possible transposition errors, two will be undetectable. So the detection rate for transpositions is $88/90 = 97.8\% $ (TEDR).
	\end{enumerate}
	
	\subsection{Encoders}
	Given $Q$ a finite set of $q$ elements (bits, alphabets). Given $k$ and $n$ two  non-zero integers with $k\leq n$. The set of messages will be a part $E$ of $Q^k$ and we introduce a bijective application (at least that is the goal!):
	
	named "\NewTerm{encoding application}\index{encoding application}" or "\NewTerm{encoder}\index{encoder}". The message or word is an element  of $E$ that is to say $Q^k$. It is modified to provide the word: 
	
	It is the word $c$ that will be transmitted and read by any system to give a message received $x=(x_1,\ldots ,x_n)$ possibly flawed.
	
	Let us now denote $C=f(E)$ the image of $f$. Since $f$ is subjective by definition it is also injective, $f$ realized a bijection of $E$ on $C$ and $C$ can be considered as the set of all possible error coding messages. $C$ is named the "\NewTerm{code of length $n$}\index{lengths of code}", and the elements of $C$ are named the "\NewTerm{words}\index{words}" of the code. The cardinal of the code is by definition that of $C$ that is to say $\text{Card}(C)$. To measure the degree of difference between two words $x$ and $y$ of $Q^n$, we use the "\NewTerm{Hamming distance}\index{Hamming distance}\label{Hamming distance}" $d_H$ defined by:
	
	For binary strings, the Hamming distance is equal to the number of ones in XOR (exclusive OR) operation on the strings.
	
	\begin{theorem}
	On any set $Q$, we therefore define the application $d:Q^n\times Q^n \rightarrow \mathbb{R}$ by:
	
	If we denote by $\delta_x$ the characteristic function of $x$:
	
	then:
	
	is a distance.
	\end{theorem}
	Let us now prove that following the topological axioms of a distance (\SeeChapter{see section Topology page \pageref{distance}}) that this is really a distance:
	\begin{dem}
	Ok let us prove the five axioms of a distance:
	\begin{enumerate}
		\item We will suppose that for the reader:
		
		is obvious (if not send us a request).
		
		\item We will also suppose that:
		
		is obvious (if not send us a request).
		
		\item We will also suppose that:
		
		is obvious (if not send us a request).
		
		\item We will also suppose that:
		
		where $d_H(x,y)=0$ mean that $x_i=y_i$ for $i=1\ldots n$ and therefore that $x=y$.
		
		\item Finally:
		
		Indeed:
		
		but as:
		
		as $(1-\delta_{x_i}(z_i))$ is equal to $1$ if $x_i\neq z_i$ and $0$ otherwise, then:
		
	\end{enumerate}
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	The Hamming distance between the word "\textbf{ramer}" and "\textbf{cases}" or between "\textbf{0100}" and "\textbf{1001}" is equal to $3$.
	\end{tcolorbox}
	
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Caution!!! Vectors will be denoted without the arrow in respect for tradition for this study field.
	\end{tcolorbox}
	
	The Hamming distance $d_H$ is therefore indeed a metric (\SeeChapter{see section Topology page \pageref{metric}}) as we just proved above and then we name "\NewTerm{Hamming space}\index{Hamming space}" on $Q$ the set $Q^n$ equipped with the metric $d_H$.
		
	\textbf{Definition (\#\thesection.\mydef):} If $Q$ is a group, the "\NewTerm{Hamming weight}\index{Hamming weight}" $w_H(x)$ of a word $x\in Q^n$ is the number of non-zero components:
	
	where $0$ is the word (vector) of $Q^n$ with all its components equal to the neutral element of $Q$. Furthermore, we have the following trivial property:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	When $Q=\{0,1\}$ we will talk about "\NewTerm{binary code}\index{binary code}" (we will see soon later an another form of writing for this binary set) of dimension $n$ equal to $2$.
	\end{tcolorbox}
	The "\NewTerm{minimum distance}\index{minimum distance}" of the code $C$ is the minimum distance between two distinct words of this code. We denote that integer by $d(C)$ or simply $d$ and therefore:
	
	or using the property of Hamming weight $d_H(x,y)=w(x-y)=w(x)$:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us consider the following redundant code denoted $(5, 4)$ for $4$ coded words of length $5$:
	\begin{table}[H]
	\centering
		\begin{tabular}{|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\textbf{Original Word} & \textbf{Coded Word} & \textbf{Weight} & \textbf{Code Identifier} \\ \hline
		$00$ & $\pmb{00}000$ & $0$ & $C_1$ \\ \hline
		$01$ & $\pmb{01}110$ & $3$ & $C_2$ \\ \hline
		$10$ & $\pmb{10}011$ & $3$ & $C_3$ \\ \hline
		$11$ & $\pmb{11}101$ & $4$ & $C_4$ \\ \hline
		\end{tabular}
	\end{table}
	The Hamming distance of each of the pairs of code are:
	
	The smallest non-zero minimum weight is therefore $3$ and the smallest Hamming distance is $3$.
	\end{tcolorbox}
	
	\pagebreak
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{itemize}
		\item[D1.] A code $C$ of length $n$, of cardinal $M$ and of minimum distance $d$ is named a "\NewTerm{$(n, M, d)$ code}". The numbers $n$, $M$, $d$ are the "code parameters". Thus, the code $(7, 4, 3)$ is a code of length seven, that is to say that the receiver receives seven bits, of length four, that is to say that once decoded, the message contains four symbols (letters) and the minimum distance between each codeword is three.

		\item[D2.] We name "\NewTerm{minimum weight}" of a $C$ code the integer:
		
	\end{itemize}
	The parameter $d$ plays an important role because it is closely related to the number of errors that can be corrected. Suppose that the encoded message is $c=(c_1,\ldots,c_n)$ and that there were at least $e$ errors of transmission or reading. The resulting message obtained $x=(x_1,\ldots,x_n)$ satisfies $d(x,c)\le e$. We can fall back on $c$ from $x$ if, and only if, there exists a single code word located at a distance of $x$ less than or equal to $e$ (i.e. the center-to-center distance between two balls is equal to $2e$). In other words, it is necessary and sufficient that the closed balls of radius $e$ and centered on the elements of the code $C$ are disjoint. A code will correct $e$ errors if this condition is true.

	Therefore, a code $C$ of minimum distance $d$ corrects at most:
	
	where $[]$ represents the integer part of a real number.

	Indeed, if a message of the code is at $d/2$ we will not be able to know to which message of the code (center of ball) it belongs since being (in a pictorial way) at the tangent of two balls. This is the reason why we will take $\dfrac{d-1}{2}$ which is then the "\NewTerm{safe distance}\index{safe distance}" to correct as much as possible an erroneous message of the code. Moreover, since the number of errors is an integer, it comes the previous notation with the square brackets.
	
	It is clear that the code can detect at most $d-1$ errors. Indeed, however how to distinguish an incorrect code from a correct code (code = coded message)? Apart from the fact that each element of the code is different (injective application of the set of messages in the set of encoded messages), it is also necessary to be able to differentiate among them those which are erroneous codes from those which do not Are not. Hence the $d-1$!!!
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us consider the following redundant code denoted $(5, 4)$ for $4$ coded words of length $5$ whose minimal distance was therefore $d=3$:
	\begin{table}[H]
	\centering
		\begin{tabular}{|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\textbf{Original Word} & \textbf{Coded Word} & \textbf{Weight} & \textbf{Code Identifier} \\ \hline
		$00$ & $\pmb{00}000$ & $0$ & $C_1$ \\ \hline
		$01$ & $\pmb{01}110$ & $3$ & $C_2$ \\ \hline
		$10$ & $\pmb{10}011$ & $3$ & $C_3$ \\ \hline
		$11$ & $\pmb{11}101$ & $4$ & $C_4$ \\ \hline
		\end{tabular}
	\end{table}
	This code therefore allows and make it possible to detect at most:
	
	errors and to correct at most:
	
	of them.
	\end{tcolorbox}
	
	\subsubsection{Block code}
	In coding theory, a block code is any member of the large and important family of error-correcting codes that encode data in blocks. There is a vast number of examples for block codes, many of which have a wide range of practical applications. Block codes are conceptually useful because they allow coding theorists, mathematicians, and computer scientists to study the limitations of all block codes in a unified way. Such limitations often take the form of bounds that relate different parameters of the block code to each other, such as its rate and its ability to detect and correct errors.

	\textbf{Definition (\#\thesection.\mydef):} A "\NewTerm{block code}\index{block code}" of size $M$ and length $n$ defined on an alphabet of $q$ symbols ($1$ and $0$ for the binary language for example) is a set of $M$ vectors named the "\NewTerm{code words}\index{code word}". The idea is that each information word composed of $k$ symbols is associated with a single codeword composed of $n$ symbols. The vectors are therefore of length $n\ge k$ and their components are $q$-ary (thus "$2$-ary" in the case of the binary language).
	
	Examples of block codes are Reed–Solomon codes, Hamming codes, Hadamard codes, Expander codes, Golay codes, and Reed–Muller codes. These examples also belong to the class of linear codes, and hence they are named "linear block codes". More particularly, these codes are known as algebraic block codes, or cyclic block codes, because they can be generated using boolean polynomials.
	
	\textbf{Definition (\#\thesection.\mydef):} A "\NewTerm{linear coding}\index{linear code}" means a set of code in which any linear combination (modular 2 sum most of time) of two codes within the set results in a code which also belong to the original set. Let's assume that you have a set of codes as in the figure below:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,810); %set diagram left start at 0, and has height of 810
		
		%Shape: Cross [id:dp6437823358479489] 
		\draw  [color={rgb, 255:red, 93; green, 223; blue, 192 }  ,draw opacity=1 ][fill={rgb, 255:red, 201; green, 255; blue, 242 }  ,fill opacity=1 ][line width=1.5]  (296.5,123) -- (305,123) -- (305,134.5) -- (316.5,134.5) -- (316.5,144.5) -- (305,144.5) -- (305,156) -- (296.5,156) -- (296.5,144.5) -- (285,144.5) -- (285,134.5) -- (296.5,134.5) -- cycle ;
		%Shape: Cross [id:dp42914529654322964] 
		\draw  [color={rgb, 255:red, 93; green, 223; blue, 192 }  ,draw opacity=1 ][fill={rgb, 255:red, 201; green, 255; blue, 242 }  ,fill opacity=1 ][line width=1.5]  (296.5,361) -- (305,361) -- (305,372.5) -- (316.5,372.5) -- (316.5,382.5) -- (305,382.5) -- (305,394) -- (296.5,394) -- (296.5,382.5) -- (285,382.5) -- (285,372.5) -- (296.5,372.5) -- cycle ;
		%Shape: Arc [id:dp2456519120852032] 
		\draw  [draw opacity=0] (239.84,116) .. controls (256.6,116.1) and (271.79,120.18) .. (282.89,126.76) -- (239.25,153.5) -- cycle ; \draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ]   (239.84,116) .. controls (255.77,116.09) and (270.26,119.78) .. (281.19,125.79) ; \draw [shift={(282.89,126.76)}, rotate = 206.14] [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ; 
		%Shape: Arc [id:dp39894931935234657] 
		\draw  [draw opacity=0] (239.84,162) .. controls (256.6,161.9) and (271.79,157.82) .. (282.89,151.24) -- (239.25,124.5) -- cycle ; \draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ]   (239.84,162) .. controls (255.77,161.91) and (270.26,158.22) .. (281.19,152.21) ; \draw [shift={(282.89,151.24)}, rotate = 153.86] [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ; 
		%Curve Lines [id:da2949331998242777] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ]   (578,139) .. controls (648.15,67.36) and (310.9,78.88) .. (241.53,78.01) ;
		\draw [shift={(240.5,78)}, rotate = 0.86] [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da17181689546985046] 
		\draw    (321,139) -- (338.5,139) ;
		\draw [shift={(340.5,139)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Curve Lines [id:da5426118365371109] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ]   (243.5,195) .. controls (308.84,188.07) and (298.71,305.61) .. (300.44,352.6) ;
		\draw [shift={(300.5,354)}, rotate = 267.51] [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Curve Lines [id:da7950480204031629] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ]   (240.5,431) .. controls (288.52,431) and (301.01,430.04) .. (300.54,402.71) ;
		\draw [shift={(300.5,401)}, rotate = 88.03] [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da7770387941410462] 
		\draw    (323,377) -- (340.5,377) ;
		\draw [shift={(342.5,377)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Curve Lines [id:da6364708144585975] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ]   (580.5,377) .. controls (599.86,377) and (623.5,467) .. (569.5,525) .. controls (516.85,581.55) and (270.95,580.11) .. (244.12,580) ;
		\draw [shift={(242.5,580)}, rotate = 360] [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		
		% Text Node
		\draw (59,32.4) node [anchor=north west][inner sep=0.75pt]    {$ \begin{array}{l}
		( 0\quad 0\quad 0\quad 0\quad 0\quad 0\quad 0)\\
		\\
		( 1\quad 1\quad 0\quad 1\quad 0\quad 0\quad 0)\\
		\\
		( 0\quad 1\quad 1\quad 0\quad 1\quad 0\quad 0)\\
		\\
		( 1\quad 0\quad 1\quad 1\quad 1\quad 0\quad 0)\\
		\\
		( 1\quad 1\quad 1\quad 0\quad 0\quad 1\quad 0)\\
		\\
		( 0\quad 0\quad 1\quad 1\quad 0\quad 1\quad 0)\\
		\\
		( 1\quad 0\quad 0\quad 0\quad 1\quad 1\quad 0)\\
		\\
		( 0\quad 1\quad 0\quad 1\quad 1\quad 1\quad 0)\\
		\\
		( 1\quad 0\quad 1\quad 0\quad 0\quad 0\quad 1)\\
		\\
		( 0\quad 1\quad 1\quad 1\quad 0\quad 0\quad 1)\\
		\\
		( 1\quad 1\quad 0\quad 0\quad 1\quad 0\quad 1)\\
		\\
		( 0\quad 0\quad 0\quad 1\quad 1\quad 0\quad 1)\\
		\\
		( 0\quad 1\quad 0\quad 0\quad 0\quad 1\quad 1)\\
		\\
		( 1\quad 0\quad 0\quad 1\quad 0\quad 1\quad 1)\\
		\\
		( 0\quad 0\quad 1\quad 0\quad 1\quad 1\quad 1)\\
		\\
		( 1\quad 1\quad 1\quad 1\quad 1\quad 1\quad 1)
		\end{array}$};
		% Text Node
		\draw (348,129.4) node [anchor=north west][inner sep=0.75pt]    {$( 1\quad 1\quad 2\quad 1\quad 2\quad 0\quad 0) \ \bmod 2$};
		% Text Node
		\draw (348,367.4) node [anchor=north west][inner sep=0.75pt]    {$( 2\quad 2\quad 1\quad 0\quad 1\quad 1\quad 1) \ \bmod 2$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Principle of a Linear code}
	\end{figure}
	Take out any two codes from the set and take modular $2$ sum of them. The result is also a member of the set as shown below. Take any other two codes and try yourself.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The linearity of the block codes also mean that the $n$ symbols of the code word are obtained by a linear combination of the $k$ symbols of the information word.
	\end{tcolorbox}
	Let us see an example!
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us start from the following $M$ vectors based on $q = 3$ binary symbols (hence $k = 2$). In this case, we have $M=q^k$:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}\textbf{Vectors}} \\ \hline
		$000$ \\ \hline
		$100$ \\ \hline
		$010$ \\ \hline
		$110$ \\ \hline
		$001$ \\ \hline
		$101$ \\ \hline
		$011$ \\ \hline
		$111$ \\ \hline
		\end{tabular}
	\end{table}
	We then choose $n$ as equal to $6$ and we define a bijective mapping such that:
	% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}\textbf{Vectors}} & \textbf{Code Vectors} \\ \hline
		$000$ & $000\pmb{000}$ \\ \hline
		$100$ & $110\pmb{100}$ \\ \hline
		$010$ & $011\pmb{010}$ \\ \hline
		$110$ & $101\pmb{110}$ \\ \hline
		$001$ & $101\pmb{001}$ \\ \hline
		$101$ & $011\pmb{101}$ \\ \hline
		$011$ & $110\pmb{011}$ \\ \hline
		$111$ & $000\pmb{111}$ \\ \hline
		\end{tabular}
	\end{table}
	As shows this example, of the particular block code conventionally denoted $(n, k)_q = (6, 3)_2$, the code has no particular structure. The decoding operation involves making an exhaustive comparison of the word received at the output of the channel with all the code modes before determining the most likely code word. This simple and stupid approach explains why many times this error correcting code is faster than many others...
	\end{tcolorbox}
	Thus, according to the above definition, a block code $C$ is the result of a injective application which associates with each vector formed by $k$ $q$-ary symbols ($k$ information symbols), an image vector of length $n$ with components in the same alphabet ($n$ encoded symbols):
	
	 The encoding adds to the initial information $n-k$ additional symbols. The quantity:
	
	is named the "\NewTerm{rate of $C$}\index{rate of error correcting code}", or "\NewTerm{coding rate}\index{cording rate}". The block encoding operation is "without memory", in extenso the blocks are coded independently without any correlation between two consecutive blocks.
	
	Now it is convenient to come back a little bit on Boole's Algebras (\SeeChapter{see section Logical Systems page \pageref{boolean algebra}}). To the $5$ axioms which define a Boolean Algebra let us add a sixth one which gives it a structure of a field:
	\begin{enumerate}
		\item[A6.] The Boolean Algebra (extension of a unitary ring by an axiom) with the law $*$ (or $\wedge$) is a field.
	\end{enumerate}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Let us recall that a field is a non-zero ring in which every non-zero element is invertible.
	\end{tcolorbox}
	If we take the Boolean Algebra formed by the $q=2$ elementary elements $\{0,1\}$ forming a binary set (alphabet), we actually have $1$ which is invertible since there exists $x$ such that:
	
	which is $1$ itself!

	This field is denoted $\mathbb{F}_q=\mathbb{F}_2$. In the area of error correcting codes, we often work in $\mathbb{F}_2$ (single field with two elements) where for recall the addition is defined by:
	
	The multiplication being defined by:
	
	
	To return to our theory of codes: the set of messages $E=\mathbb{F}_q^k$ can be equipped with a vector space structure of dimension $k$ on $\mathbb{F}_q$ (\SeeChapter{see section Set Theory page \pageref{vector space}}). Indeed, it suffices for this that $(E, +)$ to an Abelian group and $*$ an external law defined by $\mathbb{F}_q^k\times \mathbb{F}_q\mapsto \mathbb{F}_q^k$. If we decide to use only encoders that are linear (applications), the code $C=f(\mathbb{F}_q^k)$ becomes a vector subspace of $\mathbb{F}_q^n$ (because even if the application is bijective, since the body of the coded messages is finite, we necessarily have vector subspace of the vector space of all possible encoded messages).
	
	\textbf{Definition (\#\thesection.\mydef):} A "\NewTerm{linear code}\index{linear code}" of dimension $k$ and length $n$ is a vector subspace of dimension $k$ of $\mathbb{F}_q^n$ (it is the way this is said..). If the minimum distance of $C$ is $d$, we say that $C$ is a "\NewTerm{$[n, k, d]_q$ code}" or more simply "\NewTerm{code $[n,k]$}".
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Linear codes are therefore a special case of block codes as shown in the hierarchical scheme at the beginning of this section.
	\end{tcolorbox}
	The addition of the linearity constraint could undermine the quality of the code sought, but fortunately the performance study shows that the linear codes are very close to the best Block codes. Thus, linearity facilitates the study of block codes and allows the use of very powerful algebraic tools without reducing the class of linear blocks to an inefficient class.
	
	Let us denote by $G$ the matrix of the linear application $f:\mathbb{F}_q^k \mapsto \mathbb{F}_q^n$. $G$ is a matrix of obviously dimension $n\times k$ type and every word $c$ of $C$ is obtained from every word $x$ of $E$ by:
	
	where $c=(c_1,\ldots,c_n)\in\mathbb{F}_q^n$ and $x=(x_1,\ldots,x_k)\in\mathbb{F}_q^k$ are line vectors with always $n\ge k$. Thus $f(\mathbb{F}_q^k)=C$.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The bases of $\mathbb{F}_2^k$, $\mathbb{F}_2^n$ are the common canonical bases (those we have often used in the section of Vector Calculus).
	\end{tcolorbox}
	
	\textbf{Definition (\#\thesection.\mydef):} Let $C$ be a linear code $[n, k]$ and given $\{g_1,g_2,\ldots,g_k\}$ the basis of $C$. A "\NewTerm{generating matrix}\index{generating matrix}" $G$ of $C$ is therefore a matrix $n\times k$ whose columns are formed by the vectors $g_i$ of the basis (see the example further below).

	Given $u=(u_1,\ldots,u_k)$ the information word, in extenso the vector containing the $k$ information symbols. Then we can write the matrix relation linking the code word $c$ and the information word $u$ by:
	
	
	\textbf{Definition (\#\thesection.\mydef):} Let $C$ be a block code $[n, k]$. This code is named "\NewTerm{systematic code}\index{systematic code}", if the set of code words contains the $k$ unmodified symbols of the original information (we will return on this type of code further below). The remaining $n-k$ symbols are named "\NewTerm{parity symbols}\index{parity symbols}".
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The "Hamming Code" is such a code! In addition, systematic codes are special cases of block codes and we will return to their study further below.
	\end{tcolorbox}
	
	\textbf{Definition (\#\thesection.\mydef):} Let $H$ be an $(n-k)\times n$ matrix with elements in $\mathbb{F}_q$, which satisfies $Hc=0$ for every word $c$ of a linear code $C$ (in other words: whose kernel is $C$). Then, $H$ is named the "\NewTerm{control matrix}\index{control matrix}" of the code $C$. Conversely, $c$ belongs to the code if and only if $Hc=0$. Otherwise there is a mistake!
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	It is easy to find $H$ because it is "orthogonal" to $G$ since the above definition implies:
	
	of course we must not take $H = 0$ in practice...
	\end{tcolorbox}
	Let's see a companion example of all this with the Hamming code which is a systematic block code (caution!! it seem that there exist several definitions of a "Hamming code!"):

	This method consists of doubling the information, sending as many parity bits as data bits. A first matrix for this purpose is:
	
	The coding matrix $G$ above is of dimension $n\times k$, where $n$ is the number of bits received per packet, and $k$ is the number of bits per message containing the information (here $n=8$ and $k=4$). It automatically generates the parity bits specific to a message. For example, in order to send the message $1101$, in order to respect the matrix multiplication rule, consider this quartet as a column vector:
	
	So by multiplying, we get:
	
	We will therefore send a byte $11010110$, whose the first four bits form the message $u$ and the last four bits the parity bits, which are used to check the veracity / integrity of the message.

	The corresponding control matrix $H$ is:
	
	Thus, when the receiver receives the byte $11000110$ instead of $11010110$, the decoding gives as "syndrome":
	
	The resulting column vector is therefore not zero. So there is an error! With the control matrix, the theory (see proof below) makes it possible to assert that as the vector obtained is the same as that which is in fourth position in the decoding matrix, the error is due to the fourth bit. As we are in base $2$, it is enough to change the $0$ into a $1$. This coding of the information is expensive, because it occupies twice as much bandwidth. However this is one of the most effective ways to secure information.

	\begin{theorem}
	The syndrome $s$ of a Hamming code corresponds to one of the column of the control matrix $H$.
	\end{theorem}
	\begin{dem}
	To show that the syndrome of a Hamming code corresponds to one of the columns of the control matrix, we denote by $e_i$ the vectors-columns of the canonical basis on $\mathbb{F}_2^n$, $e_i=(0,\ldots,1,0,1,\ldots,0)$ with $1$ in the $i$-th place. Given $c$ a code word. We thus have by the definition of $H$: $Hc=0$. Let us suppose that the received word, which we will denote by $\tilde{c}$, is tainted by a single error and that this error is on the $j$-th bit. Therefore we have:
	
	and:
	
	Therefore it comes that:
	
	but $He_j$ is the $j$-th column vector of the matrix $H$.
	
	This shows us that when we $\tilde{c}$ and we compute $H\tilde{c}$ we get the column vector of the matrix $H$ located exactly at the location of the error (in this case $j$).
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	A null syndrome does not mean the absence of error(s). There are therefore undetectable error configurations!!!
	\end{tcolorbox}
	Let us now write:
	
	Then we will notice that the matrices $G$ and $H$ of our companion example above are formed by the blocks $\mathds{1}_4$ and $A$ in the following way:
	
	named the "\NewTerm{(canonical) generator matrix of a linear $(n,k)$ code}", and:
	
	The  "\NewTerm{parity-check matrix}\index{parity-check matrix}".
	
	Therefore:
	
	Because $1 + 1 = 0$ in $\mathbb{F}_2$.

	In general, if we work with the alphabet $\mathbb{F}_2$ and if $G=\begin{pmatrix}\mathds{1}_k\\ A\end{pmatrix}$ where $A$ is $(n-k)\times k$ matrix then $H=(A\quad -\mathds{1}_k$ is also a control matrix because again:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In $\mathbb{F}_2$, we have $\mathds{1}_k=-\mathds{1}_k$, since $1=-1$ this is why we wrote $H=(A\quad -\mathds{1}_k)$ in the previous companion example.
	\end{tcolorbox}
	
	\subsubsection{Systematic codes}
	As we have mention it earlier above, let us come back on "systematic codes" that we have already defined.
	
	Constructing a systematic code consists as we already know in adding to each word $x=(x_1,\ldots,x_k)$ of the message $n-k$ symbols $(c_{k+1},\ldots,c_{k+n})$ linearly depending of the $x_i$ to get the code word $c=f(x)$.

	We know already that symbols are named "\NewTerm{control bits}\index{control bits}" and (we will see another example just below):
	
	where, for recall, $(\mathds{1}_k|A)$ denotes the matrix $n\times k$ obtained by writing one below the other, the identity matrix $\mathds{1}_k$ of size $k$ and any matrix $A$.
	
	We will say that a code $C$ is "systematic" if it has a generating matrix of the form $G=(\mathds{1}_k|A)$.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	We propose to construct a systematic linear code with $n = k = 3$ as example. We will denote by $a_1,a_2,a_3$ the information bits. The control bits $a_4,a_5,a_6$ will be defined by:
	
	The generating matrix $G$ is such that its upper part is the identity matrix of dimension $3$ (we had the same thing for the Hamming code). The first line $(110)$ of the matrix $A$ corresponds to the expression of the control bit $a_4$:
	
	etc. For each control bit.\\

	The generating matrix $G$ is then written:
	
	By multiplying this matrix by the $2^3=8$ possible vectors (the words consisting of three bits of information), we get the following code words:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		$\pmb{a_1}$ & $\pmb{a_2}$ & $\pmb{a_3}$ & $\pmb{a_4}$ & $\pmb{a_5}$ & $\pmb{a_6}$ \\ \hline
		 $0$ & $0$ & $0$ & $0$ & $0$ & $0$ \\ \hline
		 $0$ & $0$ & $1$ & $0$ & $1$ & $1$ \\ \hline
		 $0$ & $1$ & $0$ & $1$ & $1$ & $0$ \\ \hline
		 $1$ & $0$ & $0$ & $1$ & $0$ & $1$ \\ \hline
		 $1$ & $0$ & $1$ & $1$ & $1$ & $0$ \\ \hline
		 $1$ & $1$ & $0$ & $0$ & $1$ & $1$ \\ \hline
		 $1$ & $1$ & $1$ & $0$ & $0$ & $0$ \\ \hline
		\end{tabular}
	\end{table}
	We thus find that the minimum weight of the code words is $3$. Therefore the code detects $3-1 = 2$ errors and can correct of them $\left[\dfrac{3-1}{2}\right]=1$.
	\end{tcolorbox}
	The reader interested can refer to out MATLAB™ companion book to see how to handle or generate error correcting codes.

	\begin{flushright}
	\begin{tabular}{l c}
	\circled{60} & \pbox{20cm}{\score{3}{5} \\ {\tiny 11 votes,  58.18\%}} 
	\end{tabular} 
	\end{flushright}

	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Automata Theory}\label{automata theory}
	\lettrine[lines=4]{\color{BrickRed}T}{he} purpose of this section is to study the theoretical aspect of the computer/machine concept. We are located here at the level of mathematics and logic, regardless of any reference to a real specific computer/machine (or software). We will look at how this theoretical machine will make acquisition of digital data, of whatever nature, to do treatment on it or to solve a general problem. We will then be taken to see that, from this point of view, any theoretical machine is reducible in its operating principle, to an ideal machine. Thus, we can say that all computers, or all programs are equivalent to each other, as the purpose of computer, in its theoretical definition, is universal, that is to say the capacity to treat all actually treatable problems.
	
	Modern computing is the result of research undertaken in the early 120th century (holocene calendar) by Bertrand Russell and Alfred North Whitehead to constitute a formal mathematical system where any proposal could be proved by a logical calculation \SeeChapter{see section Proof Theory page \pageref{proof theory}}. David Hilbert and Kurt Gödel accomplished decisive steps in the exploration of this program. In 11931 (holocene calendar) Gödel proved that (recall):
	\begin{enumerate}
		\item It may be that in some cases we can prove one thing and its opposite (inconsistency).
		
		\item In any formal mathematical system there are mathematical truths that can not be proved (incompleteness)
	\end{enumerate}
	Gödel's theorem thus ruin the dream to make from mathematics a perfectly coherent deductive system, but from the intellectual activity around the \textit{Principia Mathematica} project of Russel and Whitehead will release the founding ideas of computer science. This brings Alan Turing in 11936 (holocene calendar), after Gödel, to tackle the problem of decidability.
	
	\textbf{Definition (\#\thesection.\mydef):} A system is named "\NewTerm{decidable system}\index{decidable system}" if there is an effective procedure for distinguishing provable proposals of others. To define more rigorously the concept of effective procedure, Alan Turing developed the concept of "automata", hereinafter named "\NewTerm{Turing machine}\index{Turing machine}" (see example below), which allows him to clarify the concept of implementation of an "\NewTerm{algorithm}\index{algorithm}" (\SeeChapter{see section Numerical Methods page \pageref{algorithm}}).
	
	\textbf{Definition (\#\thesection.\mydef):} A "\NewTerm{Turing Complete language}\index{Turing Complete language}" is a language with at least a conditional and a while-loop construct - such a language can be used to implement a Turing machine that is "powerful" enough to perform any realizable algorithm.
	
	Inventing effective procedures (algorithms) is to determine a sequence of elementary operations that perform the calculations/treatments necessary to solve problems for which there are computable solutions (there are unsolved problems and incalculable solutions as we have seen during our study of complexity in the section of Theoretical Computing). Turing also proved that its calculation model is universal, that is to say that all Turing machines are equivalent (we will prove this below). It makes the assumption that any algorithm can be computed by a Turing machine. These ideas underpin the theory of computer programming.
	
	These ideas underlying the theory of computer programming were one important factor of winning the second World War thanks to the machine developed by Turing to uncrypt the Nazi Enigma cypher machine (a romance movie has been made about this subject in 12015 (holocene calendar) with Alan Turing as major role).
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	This section should have normally been placed at the first position of this chapter but it seemed wiser to do first hand on concrete examples of theoretical computing before moving to the abstract formalism of their executions. This is one of the reasons why we will return here briefly on the concepts of algorithms, complexity, formal logical systems, proof theory and information (see all sections with the corresponding name). Furthermore, for this section, an experience in the development of computer software is a big plus for understanding certain concepts (or to imagine practical applications).
	\end{tcolorbox}
	Before we begin, it should be useful for the reader to have a non-exhaustive overview of the applications of Language and Automata Theory: specification of programming languages, compilation, pattern matching (in a text, in a database on the web, in the genes .. .), text compression, program verification, electronic computers, encoding for transmission, encryption, decoding of the genome, language, cognitive science, etc.
	

	\subsection{Von Neumann machine}
	We due to John von Neumann to conceive in 11945 (holocene calendar) the general architecture of the concrete apparatuses which will carry out the calculations according to the Turing model, an architecture so efficient and elegant that the computers this early 121st century (holocene calendar) are still constructed on these principles.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In a way, we can say that this decade between 11936 and 11945, according to holocene calendar (corresponding well to the Second World War), saw the birth of computer science, which went from the mathematical and logical intellectual construction stage to the application of these ideas to the realization of concrete physical systems.
	\end{tcolorbox}
	Here is the diagram of the of a von Neumann architecture:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,638); %set diagram left start at 0, and has height of 638
		
		%Shape: Rectangle [id:dp03193991580091904] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (359.5,225.66) -- (467.5,225.66) -- (467.5,278.6) -- (359.5,278.6) -- cycle ;
		%Shape: Rectangle [id:dp735624915856995] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (352.5,218.66) -- (460.5,218.66) -- (460.5,271.6) -- (352.5,271.6) -- cycle ;
		%Shape: Rectangle [id:dp5402495518069099] 
		\draw   (122.5,67.6) -- (288.5,67.6) -- (288.5,119.6) -- (122.5,119.6) -- cycle ;
		%Shape: Rectangle [id:dp4816386608920693] 
		\draw   (340.5,67.6) -- (461.5,67.6) -- (461.5,119.6) -- (340.5,119.6) -- cycle ;
		%Shape: Rectangle [id:dp3841792322211759] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}] (102.5,47.6) -- (479.5,47.6) -- (479.5,137.6) -- (102.5,137.6) -- cycle ;
		%Straight Lines [id:da7631547941874681] 
		\draw    (50.5,155.6) -- (534.5,155.6) ;
		%Straight Lines [id:da05034898243318486] 
		\draw [line width=1.5]    (70.5,175.6) -- (541.5,175.6) ;
		%Straight Lines [id:da6253025922195101] 
		\draw    (148,119) -- (148,211) ;
		%Straight Lines [id:da9464773603036152] 
		\draw    (262,120) -- (262,211) ;
		%Straight Lines [id:da6082862538524161] 
		\draw    (359.5,119.6) -- (359.5,211.6) ;
		%Straight Lines [id:da1974350648414389] 
		\draw    (438.5,119.6) -- (438.5,210.6) ;
		%Shape: Circle [id:dp10989763272390451] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (144,155.5) .. controls (144,153.57) and (145.57,152) .. (147.5,152) .. controls (149.43,152) and (151,153.57) .. (151,155.5) .. controls (151,157.43) and (149.43,159) .. (147.5,159) .. controls (145.57,159) and (144,157.43) .. (144,155.5) -- cycle ;
		%Shape: Circle [id:dp6150651708580606] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (258,175.5) .. controls (258,173.57) and (259.57,172) .. (261.5,172) .. controls (263.43,172) and (265,173.57) .. (265,175.5) .. controls (265,177.43) and (263.43,179) .. (261.5,179) .. controls (259.57,179) and (258,177.43) .. (258,175.5) -- cycle ;
		%Shape: Circle [id:dp2465807257082855] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (356,155.5) .. controls (356,153.57) and (357.57,152) .. (359.5,152) .. controls (361.43,152) and (363,153.57) .. (363,155.5) .. controls (363,157.43) and (361.43,159) .. (359.5,159) .. controls (357.57,159) and (356,157.43) .. (356,155.5) -- cycle ;
		%Shape: Circle [id:dp4501906280597725] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (435,175.5) .. controls (435,173.57) and (436.57,172) .. (438.5,172) .. controls (440.43,172) and (442,173.57) .. (442,175.5) .. controls (442,177.43) and (440.43,179) .. (438.5,179) .. controls (436.57,179) and (435,177.43) .. (435,175.5) -- cycle ;
		%Shape: Rectangle [id:dp23115008953933436] 
		\draw   (121.5,210.6) -- (287.5,210.6) -- (287.5,262.6) -- (121.5,262.6) -- cycle ;
		%Shape: Rectangle [id:dp21089206288155515] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (346.5,210.66) -- (454.5,210.66) -- (454.5,263.6) -- (346.5,263.6) -- cycle ;
		
		% Text Node
		\draw (122,78) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{120pt}\setlength\topsep{0pt}
		\begin{center}
		Arithmetic-Logical Unit\\(ALU)
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (359,77) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{65pt}\setlength\topsep{0pt}
		\begin{center}
		Control Unit\\(CU)
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (197,28) node [anchor=north west][inner sep=0.75pt]   [align=left] {Control Processing Unit (CPU)};
		% Text Node
		\draw (47,138) node [anchor=north west][inner sep=0.75pt]   [align=left] {Data};
		% Text Node
		\draw (64,178) node [anchor=north west][inner sep=0.75pt]   [align=left] {Address};
		% Text Node
		\draw (133,220) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{110pt}\setlength\topsep{0pt}
		\begin{center}
		Primary Memory\\(Executable Memory)
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (373,234) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{39pt}\setlength\topsep{0pt}
		\begin{center}
		Devices
		\end{center}
		
		\end{minipage}};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Principle of the von Neumann machine}
	\end{figure}
	The Control Units, Arithmetic-Logic Unit (ALU), and Primary Memory constitute all three the "Central Unit", or "Processor", of the computer. The processor consists of electronic circuits that can perform actions. The set of actions wired in the processor constitutes the instruction set of the processor and determines the basic language of its use, named "\NewTerm{machine language}".

	The role of the Control Unit is to enable the desired action (instruction) to be triggered at the desired moment. This instruction can belong to the Arithmetic-Logic Unit, to the Memory Unit or to the Control Unit itself. An instruction can also consult the contents of the Primary Memory unit (the "read") or modify the contents of that memory (the "write"). In general, an action consists in either consulting or modifying the state of the memory or one of the registers (which are special memory elements incorporated in the central processing unit), or triggering an input-output operation (communication with the outside world and in particular a human user).
	
	The memory is made up of elements that can take states. A basic element of the memory can take two distinct states and can be used to represent an elementary data item, or "bit" (\SeeChapter{see section Logical Systems \pageref{bit}}). This representation of a data element by a memory element is named a "\NewTerm{code}". A memory with many bits allows the coding of complex data, within the limit of the size of the memory.

	The way in which the central unit, memory and I/O devices (input/output) communicate is generically named a "\NewTerm{bus}" (it is, in a way, the highway where data flows from one point to another Using addresses). In a somewhat formal way, a bus is a complete connected graph (\SeeChapter{see section Graph Theory page \pageref{complete graph}}), which means in common language that all the elements connected to the bus can communicate with one another.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The "code" makes bits and groups of symbols match together. The simplest symbols are numbers and letters. To represent complex data, you can define methods, rules for grouping symbols, and associate a data element with a symbol group constructed according to the rules.
	\end{tcolorbox}
	\textbf{Definition (\#\thesection.\mydef):} We will name "\NewTerm{language}\index{language}" a set of symbols or groups of symbols, constructed according to certain rules, and which are the "\NewTerm{words}\index{word}" of that language. A "\NewTerm{language syntax}\index{language syntax}" is the set of construction rules for language words.
	
	The memory of the computer (this is the basic idea of von Neumann) contains information of two types: Programs and Data. The programs and the data are represented with the same symbols, only the semantics allows to interpret their respective texts. Moreover, the text of a program can sometimes be considered as data for another program, for example a program of translation from one language to another.
	
	\begin{fquote}[John von Neumann]Computers are like humans - they do everything except think.
 	\end{fquote}
	
	\subsection{Turing machine}
	It is important to be convinced (it will perhaps not be done in one day...) that all the programs we can write in different languages are equivalent!!! The Turing machine is a model of an automata whose description is very low-level (before going on to a much more formal definition). The von Neumann architecture, designed to efficiently perform the processes described by a Turing machine, generates imperative languages (see definition in remark R1 below). Any program, functional or imperative, intended to be executed, will be translated into an imperative language, the "machine language" of the computer used. The coherence of computer science and the semantic equivalence of programs written in various languages which ensure the validity of this operation are the result not of chance but of a common original theoretical conception. Gödel, Church, von Neumann and Turing were all in Princeton in 11936 (holocene calendar)...!
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} The first evolved languages that have appeared are so-named "\NewTerm{imperative languages}", based on the notion of the state of memory (it is the "\NewTerm{assembly language}\index{assembly}" by the way!). These languages, inspired by John von Neumann's model, include, like machine language, instructions that produce changes in memory (assignment instruction). The writing of a program in imperative language consists in writing the sequence of instructions which will cause the successive states by which the memory will have to pass so that, starting from an initial state allowing the initialization of the program, it arrives in a state providing final results.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.91]{img/computing/assembly_demo.jpg}
		\caption{Assembly language "Hello World" demo}
	\end{figure}
	\textbf{R2.} In addition to "computational languages", we distinguish in computer science "sequential languages", "interpreted languages", "description languages", "functinal languages" and "compiled languages".
	\end{tcolorbox}
	A formal model for an effective procedure (to describe an algorithm) must possess certain properties. First, each procedure must receive a finite definition. Secondly, the procedure must consist of separate steps, each of which must be capable of being accomplished mechanically. In its simplicity, the Turing machine composed of the following elements answers to this program:
	\begin{enumerate}
		\item An infinite memory represented by a ribbon divided into boxes. Each square of the ribbon may be given a symbol of the alphabet defined for the machine;
	
		\item A reading head capable of traversing the tape in both directions;
	
		\item A finite set of states among which we distinguish an initial state and the other states, named "\NewTerm{accepting states}"
	
		\item A transition function which, for each state of the machine and each symbol under the read head, specifies: the next state, the character that will be written on the ribbon instead of the one that was under the head, the direction of the next playback of the playback head.
	\end{enumerate}
	One can equip his Turing Machine with the finite alphabet of his choice. Its ribbon can be infinite in either direction or in one. It may even have several ribbons. It is shown that these various machines are equivalent.
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/turing_machine_principle.pdf}
		\vspace*{3mm}
		\caption[Turing Machine principle]{Turing Machine principle (author: Sebastian Sardina)}
	\end{figure}
	We are then led to the following simplistic definition:
	
	\textbf{Definition (\#\thesection.\mydef):}
	A "\NewTerm{finite automate}" is a mathematical model of systems having a finite number of states and that actions (external or internal) can move from one state to another. The external actions are represented by the symbols of an alphabet $\mathcal{A}$; The internal actions (invisible, silent, or spontaneous) are represented by a symbol not belonging to the aforementioned alphabet.
	
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Caution! For example, any real computer is finite so it isn't an abstract model like a Turing Machine. The finite version of the Turing Machine has a confusing name: "\NewTerm{Linear Bounded Automaton}". This is basically what real computers are. However, LBA are not seen or used in discussion nearly as much as Turing Machine.
	\end{tcolorbox}

	An automate is represented by a graph (\SeeChapter{see section Graph Theory page \pageref{graph theory}}) whose vertices are states and with each arc is associated the recognition of one or more letters.

	Finite automata are used to model and control finite-state systems and to solve common problems: lexical analysis, search of patterns in text, genome analysis, etc.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. A finite and deterministic automate which recognizes all integers whose writing is normalized (regular language), that is to say not starting with $0$ (the numbers in the circles are just there to describe the order in which the controller performs the operation):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1215); %set diagram left start at 0, and has height of 1215
		
		%Shape: Circle [id:dp7689158996175625] 
		\draw   (239,193) .. controls (239,179.19) and (250.19,168) .. (264,168) .. controls (277.81,168) and (289,179.19) .. (289,193) .. controls (289,206.81) and (277.81,218) .. (264,218) .. controls (250.19,218) and (239,206.81) .. (239,193) -- cycle ;
		%Shape: Circle [id:dp5381955201203776] 
		\draw   (320.5,118) .. controls (320.5,104.19) and (331.69,93) .. (345.5,93) .. controls (359.31,93) and (370.5,104.19) .. (370.5,118) .. controls (370.5,131.81) and (359.31,143) .. (345.5,143) .. controls (331.69,143) and (320.5,131.81) .. (320.5,118) -- cycle ;
		%Shape: Circle [id:dp3031578670372417] 
		\draw   (320.5,268) .. controls (320.5,254.19) and (331.69,243) .. (345.5,243) .. controls (359.31,243) and (370.5,254.19) .. (370.5,268) .. controls (370.5,281.81) and (359.31,293) .. (345.5,293) .. controls (331.69,293) and (320.5,281.81) .. (320.5,268) -- cycle ;
		%Shape: Arc [id:dp7246909727579529] 
		\draw  [draw opacity=0] (264,168) .. controls (266.65,138.81) and (290.21,115.97) .. (318.88,115.97) .. controls (318.98,115.97) and (319.08,115.97) .. (319.19,115.97) -- (318.88,173.49) -- cycle ; \draw    (264,168) .. controls (266.65,138.81) and (290.21,115.97) .. (318.88,115.97) ; \draw [shift={(319.19,115.97)}, rotate = 173.89] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ; 
		%Shape: Arc [id:dp3844252333352851] 
		\draw  [draw opacity=0] (264.01,216.7) .. controls (266.81,245.9) and (290.95,268.76) .. (320.4,269) -- (320.87,211.01) -- cycle ; \draw    (264.01,216.7) .. controls (266.76,245.32) and (290,267.84) .. (318.64,268.96) ; \draw [shift={(320.4,269)}, rotate = 186.63] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ; 
		%Notched Right Arrow [id:dp26469011240669027] 
		\draw   (157,183) -- (199,183) -- (199,173) -- (227,193) -- (199,213) -- (199,203) -- (157,203) -- (167,193) -- cycle ;
		%Shape: Arc [id:dp2279294859373302] 
		\draw  [draw opacity=0] (331.86,97.14) .. controls (328.24,93.52) and (326,88.52) .. (326,83) .. controls (326,71.95) and (334.95,63) .. (346,63) .. controls (357.05,63) and (366,71.95) .. (366,83) .. controls (366,87.83) and (364.29,92.26) .. (361.44,95.71) -- (346,83) -- cycle ; \draw    (331.86,97.14) .. controls (328.24,93.52) and (326,88.52) .. (326,83) .. controls (326,71.95) and (334.95,63) .. (346,63) .. controls (357.05,63) and (366,71.95) .. (366,83) .. controls (366,87.13) and (364.75,90.96) .. (362.61,94.15) ; \draw [shift={(361.44,95.71)}, rotate = 292.85] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ; 
		%Notched Right Arrow [id:dp6238270877660173] 
		\draw   (377,106) -- (419,106) -- (419,96) -- (447,116) -- (419,136) -- (419,126) -- (377,126) -- (387,116) -- cycle ;
		%Notched Right Arrow [id:dp48315761906782884] 
		\draw   (377,261) -- (419,261) -- (419,251) -- (447,271) -- (419,291) -- (419,281) -- (377,281) -- (387,271) -- cycle ;
		
		% Text Node
		\draw (258,185.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (339.5,110.4) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (339.5,260.4) node [anchor=north west][inner sep=0.75pt]    {$3$};
		% Text Node
		\draw (231,106.4) node [anchor=north west][inner sep=0.75pt]    {$1\dotsc 9$};
		% Text Node
		\draw (257,257.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (368,61.4) node [anchor=north west][inner sep=0.75pt]    {$0\dotsc 9$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[A simple finite automata]{First example of a finite automata (pedagogical version)}
	\end{figure}
	Description: The automata receives an integer in input \circledtext{1}, it looks at whether this number starts with a $0$ or it is a number between $1$ and $9$. If the number starts with zero, the controller exits and stop at \circledtext{3}. Otherwise, the controller goes to \circledtext{2} and analyses the numbers one after the other until it reaches the end and then stops and goes out at \circledtext{3}.\\
	
	Here is a version of the same automata but respecting the traditional design and notation in academics:
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[shorten >=1pt,node distance=2cm,on grid,auto]
		  % first automaton
		  \node[state,initial]   (sA)               {$1$};
		  \node[state,accepting] (sB) [above right of=sA] {$2$};
		  \node[state,accepting] (sC) [below right of=sA] {$3$};
		  \path[->] (sB) edge[loop above] node {$0\ldots 9$} ()
		            (sA) edge[bend left]  node {$1$} (sB)
		            (sA) edge[bend right]  node [swap] {$0$} (sC);
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[]{Finite automata with academic official notation and design}
	\end{figure}
	
	
	E2. A finite and deterministic automata which recognizes a numerical input in a regular language spreadsheet (for example: $+12,3$ or $08$ or $-15$ or $5\text{E}12$ or $14\text{E}-3$):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1077); %set diagram left start at 0, and has height of 1077
		
		%Shape: Circle [id:dp25623893669199216] 
		\draw   (134,267) .. controls (134,253.19) and (145.19,242) .. (159,242) .. controls (172.81,242) and (184,253.19) .. (184,267) .. controls (184,280.81) and (172.81,292) .. (159,292) .. controls (145.19,292) and (134,280.81) .. (134,267) -- cycle ;
		%Shape: Circle [id:dp01982744791388158] 
		\draw   (215.5,192) .. controls (215.5,178.19) and (226.69,167) .. (240.5,167) .. controls (254.31,167) and (265.5,178.19) .. (265.5,192) .. controls (265.5,205.81) and (254.31,217) .. (240.5,217) .. controls (226.69,217) and (215.5,205.81) .. (215.5,192) -- cycle ;
		%Shape: Circle [id:dp07513610632948398] 
		\draw   (215.5,342) .. controls (215.5,328.19) and (226.69,317) .. (240.5,317) .. controls (254.31,317) and (265.5,328.19) .. (265.5,342) .. controls (265.5,355.81) and (254.31,367) .. (240.5,367) .. controls (226.69,367) and (215.5,355.81) .. (215.5,342) -- cycle ;
		%Shape: Arc [id:dp7783120652021116] 
		\draw  [draw opacity=0] (159,242) .. controls (161.65,212.81) and (185.21,189.97) .. (213.88,189.97) .. controls (213.98,189.97) and (214.08,189.97) .. (214.19,189.97) -- (213.88,247.49) -- cycle ; \draw    (159,242) .. controls (161.65,212.81) and (185.21,189.97) .. (213.88,189.97) ; \draw [shift={(214.19,189.97)}, rotate = 173.89] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ; 
		%Shape: Arc [id:dp20606444811644287] 
		\draw  [draw opacity=0] (159.13,291.68) .. controls (162.02,319.8) and (186.12,341.78) .. (215.52,342) -- (215.97,286) -- cycle ; \draw    (159.13,291.68) .. controls (161.96,319.24) and (185.17,340.9) .. (213.76,341.96) ; \draw [shift={(215.52,342)}, rotate = 186.8] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ; 
		%Notched Right Arrow [id:dp9301903386937884] 
		\draw   (52,257) -- (94,257) -- (94,247) -- (122,267) -- (94,287) -- (94,277) -- (52,277) -- (62,267) -- cycle ;
		%Shape: Arc [id:dp37164217502103525] 
		\draw  [draw opacity=0] (226.86,171.14) .. controls (223.24,167.52) and (221,162.52) .. (221,157) .. controls (221,145.95) and (229.95,137) .. (241,137) .. controls (252.05,137) and (261,145.95) .. (261,157) .. controls (261,161.83) and (259.29,166.26) .. (256.44,169.71) -- (241,157) -- cycle ; \draw    (226.86,171.14) .. controls (223.24,167.52) and (221,162.52) .. (221,157) .. controls (221,145.95) and (229.95,137) .. (241,137) .. controls (252.05,137) and (261,145.95) .. (261,157) .. controls (261,161.13) and (259.75,164.96) .. (257.61,168.15) ; \draw [shift={(256.44,169.71)}, rotate = 292.85] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ; 
		%Notched Right Arrow [id:dp836869980628568] 
		\draw   (205.65,182.98) -- (173.24,156.26) -- (166.88,163.98) -- (157.99,130.74) -- (192.32,133.11) -- (185.96,140.83) -- (218.37,167.55) -- (204.29,168.9) -- cycle ;
		%Straight Lines [id:da515637884563025] 
		\draw    (240.5,317) -- (240.5,219) ;
		\draw [shift={(240.5,217)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Arc [id:dp10635360701166796] 
		\draw  [draw opacity=0] (265.5,192) .. controls (268.15,162.81) and (291.71,139.97) .. (320.38,139.97) .. controls (320.48,139.97) and (320.58,139.97) .. (320.69,139.97) -- (320.38,197.49) -- cycle ; \draw    (265.5,192) .. controls (268.15,162.81) and (291.71,139.97) .. (320.38,139.97) ; \draw [shift={(320.69,139.97)}, rotate = 173.89] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ; 
		%Shape: Arc [id:dp4092209116144341] 
		\draw  [draw opacity=0] (265.5,192) .. controls (268.3,221.2) and (292.44,244.06) .. (321.89,244.3) -- (322.36,186.31) -- cycle ; \draw    (265.5,192) .. controls (268.24,220.62) and (291.48,243.15) .. (320.13,244.26) ; \draw [shift={(321.89,244.3)}, rotate = 186.63] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ; 
		%Shape: Circle [id:dp9198526274551799] 
		\draw   (321.89,139.97) .. controls (321.89,126.16) and (333.09,114.97) .. (346.89,114.97) .. controls (360.7,114.97) and (371.89,126.16) .. (371.89,139.97) .. controls (371.89,153.78) and (360.7,164.97) .. (346.89,164.97) .. controls (333.09,164.97) and (321.89,153.78) .. (321.89,139.97) -- cycle ;
		%Shape: Circle [id:dp6554298370844307] 
		\draw   (321.89,244.3) .. controls (321.89,230.5) and (333.09,219.3) .. (346.89,219.3) .. controls (360.7,219.3) and (371.89,230.5) .. (371.89,244.3) .. controls (371.89,258.11) and (360.7,269.3) .. (346.89,269.3) .. controls (333.09,269.3) and (321.89,258.11) .. (321.89,244.3) -- cycle ;
		%Straight Lines [id:da763933064998358] 
		\draw    (346.89,164.97) -- (346.89,217.3) ;
		\draw [shift={(346.89,219.3)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Arc [id:dp6506804909577695] 
		\draw  [draw opacity=0] (333.86,118.14) .. controls (330.24,114.52) and (328,109.52) .. (328,104) .. controls (328,92.95) and (336.95,84) .. (348,84) .. controls (359.05,84) and (368,92.95) .. (368,104) .. controls (368,108.83) and (366.29,113.26) .. (363.44,116.71) -- (348,104) -- cycle ; \draw    (333.86,118.14) .. controls (330.24,114.52) and (328,109.52) .. (328,104) .. controls (328,92.95) and (336.95,84) .. (348,84) .. controls (359.05,84) and (368,92.95) .. (368,104) .. controls (368,108.13) and (366.75,111.96) .. (364.61,115.15) ; \draw [shift={(363.44,116.71)}, rotate = 292.85] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ; 
		%Notched Right Arrow [id:dp25422496768996283] 
		\draw   (373.03,118.61) -- (411.16,100.99) -- (406.96,91.91) -- (440.77,98.31) -- (423.75,128.22) -- (419.55,119.14) -- (381.43,136.76) -- (386.31,123.49) -- cycle ;
		%Shape: Arc [id:dp39251436919507987] 
		\draw  [draw opacity=0] (371.89,244.3) .. controls (374.54,215.11) and (398.1,192.27) .. (426.77,192.27) .. controls (426.87,192.27) and (426.98,192.27) .. (427.08,192.27) -- (426.77,249.79) -- cycle ; \draw    (371.89,244.3) .. controls (374.54,215.11) and (398.1,192.27) .. (426.77,192.27) ; \draw [shift={(427.08,192.27)}, rotate = 173.89] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ; 
		%Shape: Arc [id:dp5390383285523126] 
		\draw  [draw opacity=0] (371.89,244.3) .. controls (374.69,273.51) and (398.83,296.37) .. (428.29,296.6) -- (428.75,238.62) -- cycle ; \draw    (371.89,244.3) .. controls (374.64,272.92) and (397.88,295.45) .. (426.53,296.56) ; \draw [shift={(428.29,296.6)}, rotate = 186.63] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ; 
		%Shape: Circle [id:dp6439744697667458] 
		\draw   (427.08,192.27) .. controls (427.08,178.47) and (438.27,167.27) .. (452.08,167.27) .. controls (465.89,167.27) and (477.08,178.47) .. (477.08,192.27) .. controls (477.08,206.08) and (465.89,217.27) .. (452.08,217.27) .. controls (438.27,217.27) and (427.08,206.08) .. (427.08,192.27) -- cycle ;
		%Shape: Circle [id:dp346430125322295] 
		\draw   (428.29,296.6) .. controls (428.29,282.8) and (439.48,271.6) .. (453.29,271.6) .. controls (467.09,271.6) and (478.29,282.8) .. (478.29,296.6) .. controls (478.29,310.41) and (467.09,321.6) .. (453.29,321.6) .. controls (439.48,321.6) and (428.29,310.41) .. (428.29,296.6) -- cycle ;
		%Straight Lines [id:da5571251023116017] 
		\draw    (453.29,271.6) -- (453.29,219.27) ;
		\draw [shift={(453.29,217.27)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Arc [id:dp5396186782139945] 
		\draw  [draw opacity=0] (435.86,172.14) .. controls (432.24,168.52) and (430,163.52) .. (430,158) .. controls (430,146.95) and (438.95,138) .. (450,138) .. controls (461.05,138) and (470,146.95) .. (470,158) .. controls (470,162.83) and (468.29,167.26) .. (465.44,170.71) -- (450,158) -- cycle ; \draw    (435.86,172.14) .. controls (432.24,168.52) and (430,163.52) .. (430,158) .. controls (430,146.95) and (438.95,138) .. (450,138) .. controls (461.05,138) and (470,146.95) .. (470,158) .. controls (470,162.13) and (468.75,165.96) .. (466.61,169.15) ; \draw [shift={(465.44,170.71)}, rotate = 292.85] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ; 
		%Notched Right Arrow [id:dp8584573515007901] 
		\draw   (482,183) -- (524,183) -- (524,173) -- (552,193) -- (524,213) -- (524,203) -- (482,203) -- (492,193) -- cycle ;
		
		% Text Node
		\draw (122,192.4) node [anchor=north west][inner sep=0.75pt]    {$0\dotsc 9$};
		% Text Node
		\draw (152,331.4) node [anchor=north west][inner sep=0.75pt]    {$\pm $};
		% Text Node
		\draw (222,115.4) node [anchor=north west][inner sep=0.75pt]    {$0\dotsc 9$};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (216,257) -- (265,257) -- (265,281) -- (216,281) -- cycle  ;
		\draw (219,261.4) node [anchor=north west][inner sep=0.75pt]    {$0\dotsc 9$};
		% Text Node
		\draw (282,129.4) node [anchor=north west][inner sep=0.75pt]    {$,$};
		% Text Node
		\draw (282,238.4) node [anchor=north west][inner sep=0.75pt]    {$E$};
		% Text Node
		\draw (330,180.4) node [anchor=north west][inner sep=0.75pt]    {$E$};
		% Text Node
		\draw (326,62.4) node [anchor=north west][inner sep=0.75pt]    {$0\dotsc 9$};
		% Text Node
		\draw (364,177.4) node [anchor=north west][inner sep=0.75pt]    {$0\dotsc 9$};
		% Text Node
		\draw (379,288.4) node [anchor=north west][inner sep=0.75pt]    {$\pm $};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (428.75,235.62) -- (477.75,235.62) -- (477.75,259.62) -- (428.75,259.62) -- cycle  ;
		\draw (431.75,240.02) node [anchor=north west][inner sep=0.75pt]    {$0\dotsc 9$};
		% Text Node
		\draw (452,118.4) node [anchor=north west][inner sep=0.75pt]    {$0\dotsc 9$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Second example of a finite automata}
	\end{figure}
	In other words, it is enough to recognize a language of the form:
		
	which is indeed regular, where $\varepsilon$ is the empty word, $\mathcal{A}$ is the alphabet $\{0,1, \ldots, 9\}$ and equation the set of words (in extenso of numbers) that can be written with $\mathcal{A}$.\\
	
	E3.  A finite and deterministic automata recognizing all the multiples of $3$, regular language type (in other words if such a multiple is found, the automata gives an output, otherwise nothing):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1077); %set diagram left start at 0, and has height of 1077
		
		%Shape: Circle [id:dp25623893669199216] 
		\draw   (225,168) .. controls (225,154.19) and (236.19,143) .. (250,143) .. controls (263.81,143) and (275,154.19) .. (275,168) .. controls (275,181.81) and (263.81,193) .. (250,193) .. controls (236.19,193) and (225,181.81) .. (225,168) -- cycle ;
		%Shape: Circle [id:dp01982744791388158] 
		\draw   (339.78,92.83) .. controls (339.78,79.02) and (350.97,67.83) .. (364.78,67.83) .. controls (378.59,67.83) and (389.78,79.02) .. (389.78,92.83) .. controls (389.78,106.64) and (378.59,117.83) .. (364.78,117.83) .. controls (350.97,117.83) and (339.78,106.64) .. (339.78,92.83) -- cycle ;
		%Shape: Circle [id:dp07513610632948398] 
		\draw   (339.78,242.83) .. controls (339.78,229.02) and (350.97,217.83) .. (364.78,217.83) .. controls (378.59,217.83) and (389.78,229.02) .. (389.78,242.83) .. controls (389.78,256.64) and (378.59,267.83) .. (364.78,267.83) .. controls (350.97,267.83) and (339.78,256.64) .. (339.78,242.83) -- cycle ;
		%Shape: Arc [id:dp7783120652021116] 
		\draw  [draw opacity=0] (261,145) .. controls (266.07,115.56) and (299.26,92.83) .. (339.45,92.83) .. controls (339.56,92.83) and (339.67,92.83) .. (339.78,92.83) -- (339.45,152.84) -- cycle ; \draw    (261,145) .. controls (266.07,115.56) and (299.26,92.83) .. (339.45,92.83) ; \draw [shift={(339.78,92.83)}, rotate = 176.92] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ; 
		%Shape: Arc [id:dp20606444811644287] 
		\draw  [draw opacity=0] (260.96,190.94) .. controls (264.56,219.92) and (298.44,242.66) .. (339.78,242.83) -- (340.24,185.83) -- cycle ; \draw    (260.96,190.94) .. controls (264.5,219.49) and (297.43,241.98) .. (337.92,242.81) ; \draw [shift={(339.78,242.83)}, rotate = 184.62] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ; 
		%Notched Right Arrow [id:dp9301903386937884] 
		\draw   (149,149) -- (191,149) -- (191,139) -- (219,159) -- (191,179) -- (191,169) -- (149,169) -- (159,159) -- cycle ;
		%Shape: Arc [id:dp37164217502103525] 
		\draw  [draw opacity=0] (244.7,192.11) .. controls (244.16,197.2) and (241.68,202.08) .. (237.39,205.56) .. controls (228.79,212.5) and (216.2,211.16) .. (209.26,202.57) .. controls (202.32,193.98) and (203.65,181.39) .. (212.24,174.44) .. controls (216,171.41) and (220.52,169.96) .. (225,170) -- (224.81,190) -- cycle ; \draw    (244.7,192.11) .. controls (244.16,197.2) and (241.68,202.08) .. (237.39,205.56) .. controls (228.79,212.5) and (216.2,211.16) .. (209.26,202.57) .. controls (202.32,193.98) and (203.65,181.39) .. (212.24,174.44) .. controls (215.46,171.85) and (219.23,170.41) .. (223.05,170.08) ; \draw [shift={(225,170)}, rotate = 163.91] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ; 
		%Notched Right Arrow [id:dp5944112257138927] 
		\draw   (233.73,137.38) -- (223.61,96.62) -- (213.9,99.02) -- (226.57,67.03) -- (252.72,89.39) -- (243.02,91.8) -- (253.14,132.56) -- (241.02,125.26) -- cycle ;
		%Shape: Arc [id:dp26120417666755147] 
		\draw  [draw opacity=0] (338.03,92.84) .. controls (333.02,121.93) and (300.55,144.47) .. (261,145) -- (259.58,84.99) -- cycle ; \draw    (338.03,92.84) .. controls (333.1,121.49) and (301.52,143.79) .. (262.77,144.96) ; \draw [shift={(261,145)}, rotate = 354.75] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ; 
		%Shape: Arc [id:dp8827163579941142] 
		\draw  [draw opacity=0] (339.78,242.83) .. controls (336.19,213.85) and (302.31,191.1) .. (260.97,190.93) -- (260.5,247.93) -- cycle ; \draw    (339.78,242.83) .. controls (336.24,214.29) and (303.32,191.79) .. (262.83,190.95) ; \draw [shift={(260.97,190.93)}, rotate = 4.63] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ; 
		%Shape: Arc [id:dp6647262336414204] 
		\draw  [draw opacity=0] (364.78,217.83) .. controls (389.52,196.21) and (390.56,153.16) .. (366.34,118.61) -- (313.96,155.09) -- cycle ; \draw    (364.78,217.83) .. controls (389.15,196.53) and (390.53,154.44) .. (367.41,120.17) ; \draw [shift={(366.34,118.61)}, rotate = 59.47] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ; 
		%Shape: Arc [id:dp40806760624671057] 
		\draw  [draw opacity=0] (364.78,117.83) .. controls (339.76,139.13) and (338.16,182.16) .. (361.94,217.02) -- (414.78,181.22) -- cycle ; \draw    (364.78,117.83) .. controls (340.13,138.81) and (338.21,180.88) .. (360.89,215.45) ; \draw [shift={(361.94,217.02)}, rotate = 240.21] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ; 
		%Shape: Arc [id:dp08229967740246069] 
		\draw  [draw opacity=0] (364.78,67.83) .. controls (364.25,62.74) and (365.67,57.45) .. (369.15,53.16) .. controls (376.11,44.59) and (388.71,43.28) .. (397.28,50.25) .. controls (405.86,57.21) and (407.16,69.81) .. (400.2,78.38) .. controls (397.15,82.13) and (393.03,84.49) .. (388.64,85.37) -- (384.67,65.77) -- cycle ; \draw    (364.78,67.83) .. controls (364.25,62.74) and (365.67,57.45) .. (369.15,53.16) .. controls (376.11,44.59) and (388.71,43.28) .. (397.28,50.25) .. controls (405.86,57.21) and (407.16,69.81) .. (400.2,78.38) .. controls (397.59,81.59) and (394.2,83.77) .. (390.53,84.9) ; \draw [shift={(388.64,85.37)}, rotate = 331.94] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ; 
		%Shape: Arc [id:dp04768462403529217] 
		\draw  [draw opacity=0] (389.04,250.85) .. controls (394.03,251.98) and (398.59,255.01) .. (401.54,259.68) .. controls (407.43,269.03) and (404.63,281.38) .. (395.29,287.27) .. controls (385.94,293.16) and (373.59,290.36) .. (367.7,281.02) .. controls (365.13,276.93) and (364.21,272.27) .. (364.78,267.83) -- (384.62,270.35) -- cycle ; \draw    (389.04,250.85) .. controls (394.03,251.98) and (398.59,255.01) .. (401.54,259.68) .. controls (407.43,269.03) and (404.63,281.38) .. (395.29,287.27) .. controls (385.94,293.16) and (373.59,290.36) .. (367.7,281.02) .. controls (365.5,277.52) and (364.51,273.61) .. (364.63,269.77) ; \draw [shift={(364.78,267.83)}, rotate = 80.62] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ; 
		
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (289,121) -- (338,121) -- (338,145) -- (289,145) -- cycle  ;
		\draw (292,125.4) node [anchor=north west][inner sep=0.75pt]    {$2,5,8$};
		% Text Node
		\draw (180,219.4) node [anchor=north west][inner sep=0.75pt]    {$0,3,6,9$};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (264,90) -- (313,90) -- (313,114) -- (264,114) -- cycle  ;
		\draw (267,94.4) node [anchor=north west][inner sep=0.75pt]    {$1,4,7$};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (263,218) -- (312,218) -- (312,242) -- (263,242) -- cycle  ;
		\draw (266,222.4) node [anchor=north west][inner sep=0.75pt]    {$2,5,8$};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (292,187) -- (341,187) -- (341,211) -- (292,211) -- cycle  ;
		\draw (295,191.4) node [anchor=north west][inner sep=0.75pt]    {$1,4,7$};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (317,163) -- (366,163) -- (366,187) -- (317,187) -- cycle  ;
		\draw (320,167.4) node [anchor=north west][inner sep=0.75pt]    {$2,5,8$};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (366,140) -- (415,140) -- (415,164) -- (366,164) -- cycle  ;
		\draw (369,144.4) node [anchor=north west][inner sep=0.75pt]    {$1,4,7$};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (376,20) -- (441,20) -- (441,44) -- (376,44) -- cycle  ;
		\draw (379,24.4) node [anchor=north west][inner sep=0.75pt]    {$0,3,6,9$};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (376,295) -- (441,295) -- (441,319) -- (376,319) -- cycle  ;
		\draw (379,299.4) node [anchor=north west][inner sep=0.75pt]    {$0,3,6,9$};
		
		\end{tikzpicture}
		\caption{Third example of a finite automata}
	\end{figure}
	\end{tcolorbox}
	I strongly recommend any reader that want the be familiar with complete Turing Machines to practice the challenge that Google made with the Turing Doodle (it is probably "complete" but not sure):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/computing/turing_doodle.jpg}
		\caption[]{Turing Doodle (source: Google)}
	\end{figure}
	available here:
	\begin{center}
		\url{http://www.sciences.ch/htmlen/turing_doodle/}
	\end{center}
	The purpose of that Doodle is to change the algorithm so that when it is executed and the machine stops the content of the tape is compared to the content of the display on the right top corner. When the comparison is successful the player will go to the next level with another algorithm to change... and so on...
	
	Some passionate of Turing Machines have build real life small and beautiful electronic Turing machines as the one visible in the picture below (perhaps they build it as a hobby or sell it to schools for education purposes???):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/computing/turing_machine_photo.jpg}
		\caption[Turing Machine]{Turing Machine (source: ?)}
	\end{figure}
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} Relatively to a common question...: YES! Artificial intelligence are Turing Machines. However at the day we write these lines (early 121st century according to holocene calendar), we still have no evidence based likelihood to claim that Homo Sapiens brain is a Linear Bounded Automaton or not (we let this work to future generations!).\\
	
	\textbf{R2.} Markov chains can be represented by finite states machines. The main thing to keep in mind is that the transitions in a Markov chain are probabilistic rather than deterministic. which means that you can't always say with perfect certainty what will happen at time $t+1$. The brain is closer to a Markov chain than a Turing machine.
	\end{tcolorbox}
	
	\pagebreak
	\subsection{Chomsky hierarchy}
	The "\NewTerm{Chomsky hierarchy}\index{Chomsky hierarchy}" is a classification of the languages described by the formal grammars proposed in 11956 (holocene calendar) by the linguist Noam Chomsky. It is widely used today in computing, especially for the design of interpreters or compilers, or for the analysis of natural languages.
	
	It is necessary before to define certain concepts!
	
	\subsubsection{Formal language}
	\textbf{Definition (naive version \#\mydef):} In a broad range of contexts (scientific, legal, etc.) we designate naively by "\NewTerm{formal language $\mathcal{L}$}\index{formal language}" a more formalized and more precise form of expression than the everyday natural language (the two do not necessarily go together).
	
	In mathematics, logic and computer science, a formal language is formed by:
	\begin{enumerate}
	 	\item A set of words obeying to strict logical rules (formal grammar or syntax).

		\item Possibly of an underlying semantics (the strength of formal languages is to be able to disregard such semantics, which makes theories reusable in several models).
	\end{enumerate}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Thus, while a particular payroll or inverse matrix calculation will always remain a payroll or inverse matrix calculation, a group theorem will apply both to the set of integers and to the transformations of the Rubik cube.
	\end{tcolorbox}
	The formal language of a scientific discipline is therefore actually a language obeying strict formal syntax and which will serve to expose statements precisely, if possible concisely and without ambiguity, and is in opposition to natural language.

	Formal language has the advantage of making easy the manipulation and transformations these statements. Indeed, we will generally have precise transformation rules (development of logical formulas, normal forms, contrapositions, commutativity, associativity, etc.) that can be applied without even knowing the meaning of the statements to be transformed or the meaning of the transformation. It is therefore a powerful exploration tool, and it is the only language that allows machines to "do mathematics".

	The disadvantage is obvious: not knowing the meaning of the statement prevents us from knowing which are the relevant transformations and hurts the intuition of the reasoning. Thus, it is good to know how to quickly read a statement in formal language and to translate it just as quickly into one or more natural language statements.
	
	This is where the limit of what we name "proof-aid software" lies: of course, the computer has (for now...) no intuition. The skill of the designer of such a program is to find ways for the computer to understand.

	Giving relevant meaning to a programming language in order to run its programs is relatively easy, because these formal languages have been designed to mean sequences of elementary actions of the machine. To prove a program (to prove that the algorithm ends in a finite number of times) or a mathematical theorem (which is almost the same thing), there is, on the other hand, no infallible method, the correction of a program being an undecidable decision problem. Thus, the prover must simply apply certain heuristics (a technique consisting in learning little by little taking into account what has been done beforehand) and often calling for help to the human user (same as humans do in fact...!). However, thanks to its heuristics and computing power, the computer explores thousands of ways that the human user would not have been able to test in several years, thus accelerating the work of the mathematician, physicist or engineer.
	
	\textbf{Definition (\#\thesection.\mydef):} As an object of study, a "\NewTerm{formal language $\mathcal{L}$}\index{formal language}" is defined as a set $\mathcal{W}$ of finite-length words $w_i$ (i.e. strings) deduced from a certain finite alphabet $\mathcal{A}$, that is to say a free monoid (the set of words formed on an alphabet, provided with the internal law of concatenation - which is a law of composition - is a monoid which we name "free monoid", which empty word is the neutral element) on this alphabet.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Thus, while a particular payroll or inverse matrix calculation will always remain a payroll or inverse matrix calculation, a group theorem will apply both to the set of integers and to the transformations of the Rubik cube.
	\end{tcolorbox}
	
	\subsubsection{Syntax}
	\textbf{Definition (\#\thesection.\mydef):} The "\NewTerm{syntax}\index{syntax}" is the branch of linguistics that studies the way in which "free morphemes" (words) combine to form "syntagmas" (nominal or verbal) that can lead to propositions that can combine in turn to form statements.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	The syntagma (sentence): \textit{a modest house of red bricks} is encompassed in the upper syntagma, that is, the complete sentence. But this same sentence \textit{a modest house of red bricks} includes among its elements, the lower syntagma \textit{of red bricks}, complement of the name house.
	\end{tcolorbox}
	
	\textbf{Definitions (\#\thesection.\mydef):} 
	\begin{enumerate}
		\item[D1.] In grammar school, a "\NewTerm{proposition}\index{proposition}" is a syntagma articulated around a verb. This notion is mainly used in language learning.

		\item[D2.] A "\NewTerm{statement}\index{statement}" in linguistics is everything that is pronounced by a speaker between two breaks. Syntactically, the statement can therefore extend from the simple word to the length of a sentence (even to a discourse), through the syntagma.
	\end{enumerate}
	The term "syntax" is also used in computer science, where its definition is similar, modulo a different terminology... Thus the syntax is the respect, or the non-respect, of the formal grammar of a computer language, that is to say of the rules of arrangement of the lexemes (which in computer science are only lexical entities) in more complex terms, often: "programs". In the theory of formal languages, what plays the role of lexeme is usually named "\NewTerm{letter}\index{letter}" or "\NewTerm{symbol}\index{symbol}", and the product terms are named "\NewTerm{words}\index{words}".
	
	From a purely grammatical point of view, the study of syntax concerns three kinds of units:
	\begin{itemize}
		\item The "\NewTerm{sentence}\index{sentence}", which is the upper limit of the syntax.

		\item The "\NewTerm{word}\index{word}", which is its basic constituent, sometimes named "\NewTerm{terminal element}\index{terminal elements}"

		\item The "\NewTerm{syntagma}\index{syntagma}", which is its intermediate unit
	\end{itemize}
	The syntactic relations between these different units can be of two kinds:
	\begin{itemize}
		\item The "\NewTerm{coordination}" when the elements are of the same status

		\item The "\NewTerm{subordination}" in the opposite case (when there is subordination, the subordinate element fulfils a syntactic function determined with respect to the higher level unit)
	\end{itemize}
	The study of syntax will take into account, in particular, the nature (or category or species) of the words, their form (morphology) and their function. It is why we speak more generally of "\NewTerm{morphosyntactic relations}".
	
	\subsubsection{Grammar}
	\textbf{Definition (naive version \#\mydef):} A "\NewTerm{formal grammar $\mathcal{G}$}\index{formal grammar}" is a formalism used to define a syntax and therefore a formal language $\mathcal{L}$, that is to say a set $\mathcal{W}$ of words $w_i$ on a given alphabet $\mathcal{A}$.
	
	The concept of formal grammar is particularly used in the following fields:
	\begin{itemize}
		\item Programs compilation (syntactic analysis)

		\item The analysis and processing of natural languages

		\item Calculation models (automata, circuits, Turing machines, etc.)
	\end{itemize}
	To define a grammar, we need (see the example below to understand):
	\begin{enumerate}
		\item An alphabet of non-terminal items

		\item An alphabet of terminals item;

		\item An initial symbol (the axiom) taken among the non-terminals items;

		\item A set of production rules.
	\end{enumerate}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. We can define arithmetic expressions in the following way (writings that we often find in the Proof theory) where $|$ is the symbol commonly use for the logical "OR":
	\begin{center}
		\texttt{exp}$\rightarrow$ \texttt{exp+exp}$|$\texttt{exp*exp}$|$\texttt{(exp)}$|$\texttt{num}
	\end{center}
	where \texttt{exp} means "expression" or:
	\begin{center}
		\texttt{num}$\rightarrow 0$\texttt{num}$|1$\texttt{num}$|2$\texttt{num}$|\ldots|9$\texttt{num}$|1|2|\ldots|9$
	\end{center}
	The non-terminals here are explicitly \texttt{exp} and \texttt{num}, the terminals are \texttt{+}, \texttt{*}, (\texttt{,}) and the digits. The axiom is \texttt{exp}.\\
	
	E2. The syntax of classical propositional logic can be defined in the following way (\SeeChapter{see section Proof Theory page \pageref{grammar}}):
	\begin{gather*}
		\mathcal{F}=\text{Atom}|F\vee F|F\wedge F|F\rightarrow|F\neg F|\exists x F|\forall xF
	\end{gather*}
	\end{tcolorbox}
	The most commonly used types of grammars are:
	\begin{enumerate}
		\item Left linear grammars that produce the same languages as regular expressions (this is what we are interested to in this section)

		\item Context-free grammar (example above)

		\item Contextual grammars (this type of grammar requires a mathematical formalism and can not be defined without it)
	\end{enumerate}
	A language is therefore a set of words $\mathcal{W}$, which are simply sequences of symbols chosen from a finite alphabet set $\mathcal{A}$. The languages of the Chomsky's hierarchy consist of words that respect a particular formal grammar. What distinguishes them within the framework of classification is the nature of the grammar.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Most often, the symbols that are considered are formed of several characters, so that they correspond rather to what "words" in the current language. When there is ambiguity, for example in lexical analysis (vocabulary) and syntactic analysis (part of the grammar that deals with the function and the disposition of words and propositions in the sentence), we speak of "characters" for the symbols of the alphabet used to encode the information, and of "lexemes" for the symbols of the abstract alphabet, which are the basic units of the language. Similarly, the "words" of the language correspond rather to "sentences" or "texts".
	\end{tcolorbox}
	The Chomsky hierarchy consists of the following $4$ levels, from the most restrictive to the broadest one:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}
		\node[above,ellipse,minimum height=2em,minimum width=4em,draw] (a) {regular};
		\node[above,ellipse,minimum height=4em,minimum width=8em,draw] (b) {};
		\node[above,ellipse,minimum height=6em,minimum width=12em,draw] (c) {};
		\node[above,ellipse,minimum height=8em,minimum width=16em,draw] (d) {};
		\path (a.north) node[above] {context-free}
		    (b.north) node[above] {context-sensitive}
		    (c.north) node[above] {recursively enumerable};
		\end{tikzpicture}
	\end{figure}
	\begin{enumerate}
		\item[L1.] The "\NewTerm{languages of type 3}" or "\NewTerm{regular language}\index{regular language}": these are the languages defined by a regular grammar or a regular expression, or the languages recognizable by a finite-state automata.

		\item[L2.]  The "\NewTerm{languages of type 2}" or "\NewTerm{algebraic languages}\index{algebraic language}" also named "\NewTerm{context-free languages}\index{context-free languages}": these are the languages defined by a context-free grammar, or the languages recognizable by a non-deterministic stack automata. In this category are for example the computer programming languages.

		\item[L3.]  The "\NewTerm{languages of type 1}"  or "\NewTerm{context sensitive languages}\index{context sensitive languages}": these are the languages defined by a contextual grammar, or the languages recognizable by a non-deterministic Turing machine with a length bounded by a fixed multiple of the word length's (these types of languages require a mathematical formalism and can not be defined without it).

		\item[L4.]  The "\NewTerm{languages of type 0}", or "\NewTerm{recursively enumerable languages}\index{recursively enumerable languages}": This set includes all languages defined by a formal grammar. It is also the set of languages acceptable by a Turing machine (which is allowed to loop on a word that is not of the language).
	\end{enumerate}	
	\begin{figure}[H]
		\centering
		\resizebox{8cm}{!}{\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1215); %set diagram left start at 0, and has height of 1215
		
		% Gradient Info
		  
		\tikzset {_2xr45jvsq/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0.5 bp } { 0 bp }  }  \pgftransformrotate{-90 }  \pgftransformscale{2 }  }}}
		\pgfdeclarehorizontalshading{_8nu33r0hz}{150bp}{rgb(0bp)=(1,1,1);
		rgb(45.714285714285715bp)=(1,1,1);
		rgb(62.5bp)=(0.45,0.73,0.12);
		rgb(100bp)=(0.45,0.73,0.12)}
		
		% Gradient Info
		  
		\tikzset {_qyz95hnn0/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{0 }  \pgftransformscale{3.4 }  }}}
		\pgfdeclarehorizontalshading{_10ot86d4z}{150bp}{rgb(0bp)=(0.45,0.73,0.12);
		rgb(37.5bp)=(0.45,0.73,0.12);
		rgb(49.82142857142857bp)=(1,1,1);
		rgb(62.5bp)=(0.45,0.73,0.12);
		rgb(100bp)=(0.45,0.73,0.12)}
		
		% Gradient Info
		  
		\tikzset {_3d8a9q5f3/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{89.1 bp } { -128.7 bp }  }  \pgftransformscale{1.32 }  }}}
		\pgfdeclareradialshading{_zut4w70wj}{\pgfpoint{-72bp}{104bp}}{rgb(0bp)=(1,1,1);
		rgb(0bp)=(1,1,1);
		rgb(25bp)=(0.45,0.73,0.12);
		rgb(400bp)=(0.45,0.73,0.12)}
		
		% Gradient Info
		  
		\tikzset {_da67dl1m0/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{89.1 bp } { -128.7 bp }  }  \pgftransformscale{1.32 }  }}}
		\pgfdeclareradialshading{_9dj9p4lod}{\pgfpoint{-72bp}{104bp}}{rgb(0bp)=(1,1,1);
		rgb(0bp)=(1,1,1);
		rgb(25bp)=(0.45,0.73,0.12);
		rgb(400bp)=(0.45,0.73,0.12)}
		
		% Gradient Info
		  
		\tikzset {_uft1jzd2w/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{89.1 bp } { -128.7 bp }  }  \pgftransformscale{1.32 }  }}}
		\pgfdeclareradialshading{_09iisz9v2}{\pgfpoint{-72bp}{104bp}}{rgb(0bp)=(1,1,1);
		rgb(0bp)=(1,1,1);
		rgb(25bp)=(0.45,0.73,0.12);
		rgb(400bp)=(0.45,0.73,0.12)}
		
		% Gradient Info
		  
		\tikzset {_nwv0bhqbk/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{0 }  \pgftransformscale{3.4 }  }}}
		\pgfdeclarehorizontalshading{_3e1v8shiy}{150bp}{rgb(0bp)=(0.45,0.73,0.12);
		rgb(37.5bp)=(0.45,0.73,0.12);
		rgb(49.82142857142857bp)=(1,1,1);
		rgb(62.5bp)=(0.45,0.73,0.12);
		rgb(100bp)=(0.45,0.73,0.12)}
		
		% Gradient Info
		  
		\tikzset {_ki6691wmq/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{89.1 bp } { -128.7 bp }  }  \pgftransformscale{1.32 }  }}}
		\pgfdeclareradialshading{_bdqt82me6}{\pgfpoint{-72bp}{104bp}}{rgb(0bp)=(1,1,1);
		rgb(0bp)=(1,1,1);
		rgb(25bp)=(0.45,0.73,0.12);
		rgb(400bp)=(0.45,0.73,0.12)}
		
		% Gradient Info
		  
		\tikzset {_6z6wkabyo/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{89.1 bp } { -128.7 bp }  }  \pgftransformscale{1.32 }  }}}
		\pgfdeclareradialshading{_0laww7zxx}{\pgfpoint{-72bp}{104bp}}{rgb(0bp)=(1,1,1);
		rgb(0bp)=(1,1,1);
		rgb(25bp)=(0.45,0.73,0.12);
		rgb(400bp)=(0.45,0.73,0.12)}
		
		% Gradient Info
		  
		\tikzset {_2d5aa7hfi/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{89.1 bp } { -128.7 bp }  }  \pgftransformscale{1.32 }  }}}
		\pgfdeclareradialshading{_h9k3wkyz1}{\pgfpoint{-72bp}{104bp}}{rgb(0bp)=(1,1,1);
		rgb(0bp)=(1,1,1);
		rgb(25bp)=(0.45,0.73,0.12);
		rgb(400bp)=(0.45,0.73,0.12)}
		
		% Gradient Info
		  
		\tikzset {_83azb99i1/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{89.1 bp } { -128.7 bp }  }  \pgftransformscale{1.32 }  }}}
		\pgfdeclareradialshading{_dwmw0u2u7}{\pgfpoint{-72bp}{104bp}}{rgb(0bp)=(1,1,1);
		rgb(0bp)=(1,1,1);
		rgb(25bp)=(0.45,0.73,0.12);
		rgb(400bp)=(0.45,0.73,0.12)}
		
		% Gradient Info
		  
		\tikzset {_u5u2fghqw/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{89.1 bp } { -128.7 bp }  }  \pgftransformscale{1.32 }  }}}
		\pgfdeclareradialshading{_f1foykxer}{\pgfpoint{-72bp}{104bp}}{rgb(0bp)=(1,1,1);
		rgb(0bp)=(1,1,1);
		rgb(25bp)=(0.45,0.73,0.12);
		rgb(400bp)=(0.45,0.73,0.12)}
		
		% Gradient Info
		  
		\tikzset {_lexs7a7s1/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{89.1 bp } { -128.7 bp }  }  \pgftransformscale{1.32 }  }}}
		\pgfdeclareradialshading{_jvvx1wl5s}{\pgfpoint{-72bp}{104bp}}{rgb(0bp)=(1,1,1);
		rgb(0bp)=(1,1,1);
		rgb(25bp)=(0.45,0.73,0.12);
		rgb(400bp)=(0.45,0.73,0.12)}
		
		% Gradient Info
		  
		\tikzset {_7gaa1jrn8/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{89.1 bp } { -128.7 bp }  }  \pgftransformscale{1.32 }  }}}
		\pgfdeclareradialshading{_1ey1oldow}{\pgfpoint{-72bp}{104bp}}{rgb(0bp)=(1,1,1);
		rgb(0bp)=(1,1,1);
		rgb(25bp)=(0.45,0.73,0.12);
		rgb(400bp)=(0.45,0.73,0.12)}
		
		% Gradient Info
		  
		\tikzset {_0pg7zcg8r/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{89.1 bp } { -128.7 bp }  }  \pgftransformscale{1.32 }  }}}
		\pgfdeclareradialshading{_cnf6fzl9r}{\pgfpoint{-72bp}{104bp}}{rgb(0bp)=(1,1,1);
		rgb(0bp)=(1,1,1);
		rgb(25bp)=(0.45,0.73,0.12);
		rgb(400bp)=(0.45,0.73,0.12)}
		
		% Gradient Info
		  
		\tikzset {_ellnzgb6o/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{89.1 bp } { -128.7 bp }  }  \pgftransformscale{1.32 }  }}}
		\pgfdeclareradialshading{_14mpgvr93}{\pgfpoint{-72bp}{104bp}}{rgb(0bp)=(1,1,1);
		rgb(0bp)=(1,1,1);
		rgb(25bp)=(0.45,0.73,0.12);
		rgb(400bp)=(0.45,0.73,0.12)}
		
		%Shape: Rectangle [id:dp7452706549736479] 
		\draw  [draw opacity=0][shading=_8nu33r0hz,_2xr45jvsq] (477,422) -- (510.5,422) -- (510.5,474) -- (477,474) -- cycle ;
		%Straight Lines [id:da043557862155363836] 
		\draw    (107,33) -- (528.5,33) ;
		%Straight Lines [id:da7489920394213792] 
		\draw    (107,85) -- (528.5,85) ;
		%Shape: Rectangle [id:dp7247298491525076] 
		\path  [shading=_10ot86d4z,_qyz95hnn0] (311,119) -- (511.5,119) -- (511.5,159) -- (311,159) -- cycle ; % for fading 
		 \draw   (311,119) -- (511.5,119) -- (511.5,159) -- (311,159) -- cycle ; % for border 
		
		%Shape: Square [id:dp5878338116098154] 
		\draw   (311,119) -- (351,119) -- (351,159) -- (311,159) -- cycle ;
		%Shape: Square [id:dp8194114211358814] 
		\draw   (351,119) -- (391,119) -- (391,159) -- (351,159) -- cycle ;
		%Shape: Square [id:dp2906324768449038] 
		\draw   (391,119) -- (431,119) -- (431,159) -- (391,159) -- cycle ;
		%Shape: Square [id:dp7304994488859256] 
		\draw   (431,119) -- (471,119) -- (471,159) -- (431,159) -- cycle ;
		%Shape: Square [id:dp8778321546587797] 
		\draw   (471,119) -- (511,119) -- (511,159) -- (471,159) -- cycle ;
		%Straight Lines [id:da5970967745030147] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (275,119) -- (546.5,119) ;
		%Straight Lines [id:da6185917052140149] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (275,159) -- (546.5,159) ;
		%Shape: Circle [id:dp17998486578776807] 
		\path  [shading=_zut4w70wj,_3d8a9q5f3] (312,212) .. controls (312,203.16) and (319.16,196) .. (328,196) .. controls (336.84,196) and (344,203.16) .. (344,212) .. controls (344,220.84) and (336.84,228) .. (328,228) .. controls (319.16,228) and (312,220.84) .. (312,212) -- cycle ; % for fading 
		 \draw   (312,212) .. controls (312,203.16) and (319.16,196) .. (328,196) .. controls (336.84,196) and (344,203.16) .. (344,212) .. controls (344,220.84) and (336.84,228) .. (328,228) .. controls (319.16,228) and (312,220.84) .. (312,212) -- cycle ; % for border 
		
		%Shape: Circle [id:dp8879633488109282] 
		\path  [shading=_9dj9p4lod,_da67dl1m0] (396,212) .. controls (396,203.16) and (403.16,196) .. (412,196) .. controls (420.84,196) and (428,203.16) .. (428,212) .. controls (428,220.84) and (420.84,228) .. (412,228) .. controls (403.16,228) and (396,220.84) .. (396,212) -- cycle ; % for fading 
		 \draw   (396,212) .. controls (396,203.16) and (403.16,196) .. (412,196) .. controls (420.84,196) and (428,203.16) .. (428,212) .. controls (428,220.84) and (420.84,228) .. (412,228) .. controls (403.16,228) and (396,220.84) .. (396,212) -- cycle ; % for border 
		
		%Shape: Circle [id:dp9343537949257319] 
		\path  [shading=_09iisz9v2,_uft1jzd2w] (478,212) .. controls (478,203.16) and (485.16,196) .. (494,196) .. controls (502.84,196) and (510,203.16) .. (510,212) .. controls (510,220.84) and (502.84,228) .. (494,228) .. controls (485.16,228) and (478,220.84) .. (478,212) -- cycle ; % for fading 
		 \draw   (478,212) .. controls (478,203.16) and (485.16,196) .. (494,196) .. controls (502.84,196) and (510,203.16) .. (510,212) .. controls (510,220.84) and (502.84,228) .. (494,228) .. controls (485.16,228) and (478,220.84) .. (478,212) -- cycle ; % for border 
		
		%Straight Lines [id:da9933186832270473] 
		\draw    (345,212) -- (392.5,212) ;
		\draw [shift={(394.5,212)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da2904634633997112] 
		\draw    (429,212) -- (476.5,212) ;
		\draw [shift={(478.5,212)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Arc [id:dp7050663058896796] 
		\draw  [draw opacity=0] (399.43,203.07) .. controls (396.44,199.87) and (394.61,195.58) .. (394.61,190.86) .. controls (394.61,180.99) and (402.61,172.99) .. (412.48,172.99) .. controls (422.35,172.99) and (430.35,180.99) .. (430.35,190.86) .. controls (430.35,195.83) and (428.33,200.32) .. (425.06,203.56) -- (412.48,190.86) -- cycle ; \draw    (399.43,203.07) .. controls (396.44,199.87) and (394.61,195.58) .. (394.61,190.86) .. controls (394.61,180.99) and (402.61,172.99) .. (412.48,172.99) .. controls (422.35,172.99) and (430.35,180.99) .. (430.35,190.86) .. controls (430.35,195.13) and (428.85,199.05) .. (426.36,202.13) ; \draw [shift={(425.06,203.56)}, rotate = 296.66] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ; 
		%Straight Lines [id:da059039799674217575] 
		\draw    (370.5,188) -- (370.5,162) ;
		\draw [shift={(370.5,159)}, rotate = 90] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Shape: Rectangle [id:dp8951390194657498] 
		\path  [shading=_3e1v8shiy,_nwv0bhqbk] (311,277) -- (511.5,277) -- (511.5,317) -- (311,317) -- cycle ; % for fading 
		 \draw   (311,277) -- (511.5,277) -- (511.5,317) -- (311,317) -- cycle ; % for border 
		
		%Shape: Square [id:dp36460803930005903] 
		\draw   (311,277) -- (351,277) -- (351,317) -- (311,317) -- cycle ;
		%Shape: Square [id:dp15055079710694774] 
		\draw   (351,277) -- (391,277) -- (391,317) -- (351,317) -- cycle ;
		%Shape: Square [id:dp42163813867372224] 
		\draw   (391,277) -- (431,277) -- (431,317) -- (391,317) -- cycle ;
		%Shape: Square [id:dp8468864661257283] 
		\draw   (431,277) -- (471,277) -- (471,317) -- (431,317) -- cycle ;
		%Shape: Square [id:dp5852481308092827] 
		\draw   (471,277) -- (511,277) -- (511,317) -- (471,317) -- cycle ;
		%Shape: Circle [id:dp7631943909121099] 
		\path  [shading=_bdqt82me6,_ki6691wmq] (312,370) .. controls (312,361.16) and (319.16,354) .. (328,354) .. controls (336.84,354) and (344,361.16) .. (344,370) .. controls (344,378.84) and (336.84,386) .. (328,386) .. controls (319.16,386) and (312,378.84) .. (312,370) -- cycle ; % for fading 
		 \draw   (312,370) .. controls (312,361.16) and (319.16,354) .. (328,354) .. controls (336.84,354) and (344,361.16) .. (344,370) .. controls (344,378.84) and (336.84,386) .. (328,386) .. controls (319.16,386) and (312,378.84) .. (312,370) -- cycle ; % for border 
		
		%Shape: Circle [id:dp72502983568363] 
		\path  [shading=_0laww7zxx,_6z6wkabyo] (396,370) .. controls (396,361.16) and (403.16,354) .. (412,354) .. controls (420.84,354) and (428,361.16) .. (428,370) .. controls (428,378.84) and (420.84,386) .. (412,386) .. controls (403.16,386) and (396,378.84) .. (396,370) -- cycle ; % for fading 
		 \draw   (396,370) .. controls (396,361.16) and (403.16,354) .. (412,354) .. controls (420.84,354) and (428,361.16) .. (428,370) .. controls (428,378.84) and (420.84,386) .. (412,386) .. controls (403.16,386) and (396,378.84) .. (396,370) -- cycle ; % for border 
		
		%Shape: Circle [id:dp026246247935124423] 
		\path  [shading=_h9k3wkyz1,_2d5aa7hfi] (478,370) .. controls (478,361.16) and (485.16,354) .. (494,354) .. controls (502.84,354) and (510,361.16) .. (510,370) .. controls (510,378.84) and (502.84,386) .. (494,386) .. controls (485.16,386) and (478,378.84) .. (478,370) -- cycle ; % for fading 
		 \draw   (478,370) .. controls (478,361.16) and (485.16,354) .. (494,354) .. controls (502.84,354) and (510,361.16) .. (510,370) .. controls (510,378.84) and (502.84,386) .. (494,386) .. controls (485.16,386) and (478,378.84) .. (478,370) -- cycle ; % for border 
		
		%Straight Lines [id:da27626537978824706] 
		\draw    (345,370) -- (392.5,370) ;
		\draw [shift={(394.5,370)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da6764494092979643] 
		\draw    (429,370) -- (476.5,370) ;
		\draw [shift={(478.5,370)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Arc [id:dp7569368082703505] 
		\draw  [draw opacity=0] (399.43,361.07) .. controls (396.44,357.87) and (394.61,353.58) .. (394.61,348.86) .. controls (394.61,338.99) and (402.61,330.99) .. (412.48,330.99) .. controls (422.35,330.99) and (430.35,338.99) .. (430.35,348.86) .. controls (430.35,353.83) and (428.33,358.32) .. (425.06,361.56) -- (412.48,348.86) -- cycle ; \draw    (399.43,361.07) .. controls (396.44,357.87) and (394.61,353.58) .. (394.61,348.86) .. controls (394.61,338.99) and (402.61,330.99) .. (412.48,330.99) .. controls (422.35,330.99) and (430.35,338.99) .. (430.35,348.86) .. controls (430.35,353.13) and (428.85,357.05) .. (426.36,360.13) ; \draw [shift={(425.06,361.56)}, rotate = 296.66] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ; 
		%Straight Lines [id:da8965011282934572] 
		\draw    (370.5,346) -- (370.5,320) ;
		\draw [shift={(370.5,317)}, rotate = 90] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Shape: Circle [id:dp48778559625949214] 
		\path  [shading=_dwmw0u2u7,_83azb99i1] (312,500) .. controls (312,491.16) and (319.16,484) .. (328,484) .. controls (336.84,484) and (344,491.16) .. (344,500) .. controls (344,508.84) and (336.84,516) .. (328,516) .. controls (319.16,516) and (312,508.84) .. (312,500) -- cycle ; % for fading 
		 \draw   (312,500) .. controls (312,491.16) and (319.16,484) .. (328,484) .. controls (336.84,484) and (344,491.16) .. (344,500) .. controls (344,508.84) and (336.84,516) .. (328,516) .. controls (319.16,516) and (312,508.84) .. (312,500) -- cycle ; % for border 
		
		%Shape: Circle [id:dp14225218078229318] 
		\path  [shading=_f1foykxer,_u5u2fghqw] (396,500) .. controls (396,491.16) and (403.16,484) .. (412,484) .. controls (420.84,484) and (428,491.16) .. (428,500) .. controls (428,508.84) and (420.84,516) .. (412,516) .. controls (403.16,516) and (396,508.84) .. (396,500) -- cycle ; % for fading 
		 \draw   (396,500) .. controls (396,491.16) and (403.16,484) .. (412,484) .. controls (420.84,484) and (428,491.16) .. (428,500) .. controls (428,508.84) and (420.84,516) .. (412,516) .. controls (403.16,516) and (396,508.84) .. (396,500) -- cycle ; % for border 
		
		%Shape: Circle [id:dp7457899927851219] 
		\path  [shading=_jvvx1wl5s,_lexs7a7s1] (478,500) .. controls (478,491.16) and (485.16,484) .. (494,484) .. controls (502.84,484) and (510,491.16) .. (510,500) .. controls (510,508.84) and (502.84,516) .. (494,516) .. controls (485.16,516) and (478,508.84) .. (478,500) -- cycle ; % for fading 
		 \draw   (478,500) .. controls (478,491.16) and (485.16,484) .. (494,484) .. controls (502.84,484) and (510,491.16) .. (510,500) .. controls (510,508.84) and (502.84,516) .. (494,516) .. controls (485.16,516) and (478,508.84) .. (478,500) -- cycle ; % for border 
		
		%Straight Lines [id:da32970402337652627] 
		\draw    (345,500) -- (392.5,500) ;
		\draw [shift={(394.5,500)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da6009355128588956] 
		\draw    (429,500) -- (476.5,500) ;
		\draw [shift={(478.5,500)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Arc [id:dp6043965029681371] 
		\draw  [draw opacity=0] (399.43,491.07) .. controls (396.44,487.87) and (394.61,483.58) .. (394.61,478.86) .. controls (394.61,468.99) and (402.61,460.99) .. (412.48,460.99) .. controls (422.35,460.99) and (430.35,468.99) .. (430.35,478.86) .. controls (430.35,483.83) and (428.33,488.32) .. (425.06,491.56) -- (412.48,478.86) -- cycle ; \draw    (399.43,491.07) .. controls (396.44,487.87) and (394.61,483.58) .. (394.61,478.86) .. controls (394.61,468.99) and (402.61,460.99) .. (412.48,460.99) .. controls (422.35,460.99) and (430.35,468.99) .. (430.35,478.86) .. controls (430.35,483.13) and (428.85,487.05) .. (426.36,490.13) ; \draw [shift={(425.06,491.56)}, rotate = 296.66] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ; 
		%Straight Lines [id:da2892877566687204] 
		\draw    (477,422) -- (477,473) ;
		%Straight Lines [id:da24061454833925744] 
		\draw    (511.5,473) -- (477.5,473) ;
		%Straight Lines [id:da9128317332199252] 
		\draw    (511,422) -- (511,473) ;
		%Straight Lines [id:da7691292778287333] 
		\draw    (494.5,406) -- (494.5,425) ;
		\draw [shift={(494.5,428)}, rotate = 270] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Shape: Circle [id:dp3236597985917802] 
		\path  [shading=_1ey1oldow,_7gaa1jrn8] (312,619) .. controls (312,610.16) and (319.16,603) .. (328,603) .. controls (336.84,603) and (344,610.16) .. (344,619) .. controls (344,627.84) and (336.84,635) .. (328,635) .. controls (319.16,635) and (312,627.84) .. (312,619) -- cycle ; % for fading 
		 \draw   (312,619) .. controls (312,610.16) and (319.16,603) .. (328,603) .. controls (336.84,603) and (344,610.16) .. (344,619) .. controls (344,627.84) and (336.84,635) .. (328,635) .. controls (319.16,635) and (312,627.84) .. (312,619) -- cycle ; % for border 
		
		%Shape: Circle [id:dp5861179437463531] 
		\path  [shading=_cnf6fzl9r,_0pg7zcg8r] (396,619) .. controls (396,610.16) and (403.16,603) .. (412,603) .. controls (420.84,603) and (428,610.16) .. (428,619) .. controls (428,627.84) and (420.84,635) .. (412,635) .. controls (403.16,635) and (396,627.84) .. (396,619) -- cycle ; % for fading 
		 \draw   (396,619) .. controls (396,610.16) and (403.16,603) .. (412,603) .. controls (420.84,603) and (428,610.16) .. (428,619) .. controls (428,627.84) and (420.84,635) .. (412,635) .. controls (403.16,635) and (396,627.84) .. (396,619) -- cycle ; % for border 
		
		%Shape: Circle [id:dp5285225229412145] 
		\path  [shading=_14mpgvr93,_ellnzgb6o] (478,619) .. controls (478,610.16) and (485.16,603) .. (494,603) .. controls (502.84,603) and (510,610.16) .. (510,619) .. controls (510,627.84) and (502.84,635) .. (494,635) .. controls (485.16,635) and (478,627.84) .. (478,619) -- cycle ; % for fading 
		 \draw   (478,619) .. controls (478,610.16) and (485.16,603) .. (494,603) .. controls (502.84,603) and (510,610.16) .. (510,619) .. controls (510,627.84) and (502.84,635) .. (494,635) .. controls (485.16,635) and (478,627.84) .. (478,619) -- cycle ; % for border 
		
		%Straight Lines [id:da5482356087881073] 
		\draw    (345,619) -- (392.5,619) ;
		\draw [shift={(394.5,619)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da8764201306212254] 
		\draw    (429,619) -- (476.5,619) ;
		\draw [shift={(478.5,619)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Arc [id:dp09688311293092666] 
		\draw  [draw opacity=0] (399.43,610.07) .. controls (396.44,606.87) and (394.61,602.58) .. (394.61,597.86) .. controls (394.61,587.99) and (402.61,579.99) .. (412.48,579.99) .. controls (422.35,579.99) and (430.35,587.99) .. (430.35,597.86) .. controls (430.35,602.83) and (428.33,607.32) .. (425.06,610.56) -- (412.48,597.86) -- cycle ; \draw    (399.43,610.07) .. controls (396.44,606.87) and (394.61,602.58) .. (394.61,597.86) .. controls (394.61,587.99) and (402.61,579.99) .. (412.48,579.99) .. controls (422.35,579.99) and (430.35,587.99) .. (430.35,597.86) .. controls (430.35,602.13) and (428.85,606.05) .. (426.36,609.13) ; \draw [shift={(425.06,610.56)}, rotate = 296.66] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ; 
		
		% Text Node
		\draw (124,44.5) node [anchor=north west][inner sep=0.75pt]  [font=\LARGE] [align=left] {\textbf{Language}};
		% Text Node
		\draw (338,44.5) node [anchor=north west][inner sep=0.75pt]  [font=\LARGE] [align=left] {\textbf{Automaton}};
		% Text Node
		\draw (139,136) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{56.59pt}\setlength\topsep{0pt}
		\begin{center}
		Recursively\\enumerable\\languages
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (144,299) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{49.8pt}\setlength\topsep{0pt}
		\begin{center}
		Context-\\sensitive\\languages
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (144,457) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{49.8pt}\setlength\topsep{0pt}
		\begin{center}
		Context-\\free\\languages
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (144,585) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{49.8pt}\setlength\topsep{0pt}
		\begin{center}
		Regular\\languages
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (359.5,91) node [anchor=north west][inner sep=0.75pt]   [align=left] {Turing machine};
		% Text Node
		\draw (359,249) node [anchor=north west][inner sep=0.75pt]   [align=left] {Linear bounded};
		% Text Node
		\draw (374,415) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{53.19pt}\setlength\topsep{0pt}
		\begin{center}
		Pushdown \\(stack)
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (374.5,544) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{62pt}\setlength\topsep{0pt}
		\begin{center}
		Finite state\\automaton
		\end{center}
		\end{minipage}};
		\end{tikzpicture}}
		\vspace*{3mm}
		\caption{Chomsky hierarchy}
	\end{figure}
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} In addition to the $4$ types of the Chomsky hierarchy, there are remarkable intermediate classes! For example between types $3$ and $2$: deterministic non-contextual languages, recognizable by a deterministic stack automata and languages between levels $1$ and $0$: recursive languages, that is to say, recognizable by a Turing machine (the latter must refuse words which are not in the language).\\
	
	\textbf{R2.} The $4$ types of languages and $2$ of intermediate languages above are strictly included in each other.
	\end{tcolorbox}
	A parser for a formal language is a computer program that decides whether a given input word belongs or not to the language, and possibly constructs a derivation of it.

	There are systematic methods for writing type $2$ or $3$ language analysis programs (parsers). Interpreters or compilers almost always include a phase of lexical analysis, which consists of recognizing type $3$ languages, followed by a phase of syntactic analysis that is a in fact a type $2$ language analysis.

	We can now finally in a vulgarize way (always with the aim of paving the way) define what an automata is in the Chomsky hierarchy.
	
	\subsubsection{Associated automata}
	\textbf{Definition (naive version \#\mydef):}  In the field of theoretical computing, an "\NewTerm{automata}\index{automata}" is a machine to process information by a formal model (a Turing machine) on a given language. So:
	\begin{itemize}
		\item On a "\NewTerm{finite language}\index{finite language}" (language containing a finite number of words), the associated automata is a machine comparing a text with that which is stored in a read-only memory. The grammar associated with a finite language is a list of the words of the language.
		
		\item On a "\NewTerm{regular language}\index{regular language}" (language where the syntactic correction is verified by storing only a finite number of information), the associated automata is the "deterministic finite automaton" (that is, for each word entered, there is only one possible path of the graph) or the "non-deterministic finite automata". The grammar associated with a regular language is a left linear grammar.
		
		\item On an "\NewTerm{algebraic language}" (language where the main syntactic constraint are the parenthesis), the associated automata is the "pushdown (stack) non-deterministic automata". The associated grammar is the algebraic grammar.
		
		\item On a "\NewTerm{bounded language}" (description requiring a mathematical formalism), the associated automata is the  "linearly bounded automata". The associated grammar is the contextual grammar.
		
		\item On a "\NewTerm{decidable language}" (an intelligent being manages to find a process to know whether or not one he is in the language), the associated automata is a Turing machine that stops on all data. There is no grammar associated to it.
		
		\item On a "\NewTerm{semi-decidable language}" (an intelligent being manages to find a process to know whether or not one he is in the language), the associated  automata is the Turing machine (thus contains conditional structures and loops). The associated grammars are the "semi-Turing grammar", the "de Vangarden grammar" or the "affixed grammars".
	\end{itemize}
	
	\pagebreak
	\subsection{Terminology}
	The automata therefore work mainly on letters, words, sentences and languages. In order to construct rigorous and optimal analysis methods and treatment on the subject it is interesting to formalize the treated objects. This is what we will do initially by defining these latter and their mathematical properties (which are very intuitive).
	
	\subsubsection{Words}
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] An "\NewTerm{alphabet $\mathcal{A}$}\index{alphabet}" is a set whose elements are the "\NewTerm{letter $\ell$}". The alphabets are always supposed to be finished.
		
		\item[D2.] A "\NewTerm{word $w_i$}\index{word}" is a finite sequence of "letters" which we denote by juxtaposition:
		
	
		\item[D3.] The "\NewTerm{empty word}", denoted $\varepsilon$, is the only word composed of no letters.
	
		\item[D4.] The "\NewTerm{length}" of a word $w$ is the number of letters that compose it, and is denoted $|w|$ (the empty word $\varepsilon$ is the only word of length $0$).
	
		The "\NewTerm{concatenation product}\index{concatenation product}" of two words $w_1=a_1a_2\ldots a_n$ and $w_i=b_1b_2\ldots b_m$ is the word $w_1w_2$ obtained by juxtaposition (concatenation):
		
		Of course (trivial), we have:
		
		We denote by $\mathcal{A}^{*}$ the set of words on $\mathcal{A}$.
		
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		Genes are words on the ACGT alphabet, proteins are words on a $20$-letter alphabet. The natural integers, written in base $10$, are words on the alphabet of the decimal digits...
		\end{tcolorbox}
		Let $\mathcal{A}$ be an alphabet. Let $\mathcal{B}$ be a subset of $\mathcal{A}$. For any word $w\in \mathcal{A}$, the length in  $\mathcal{B}$ of $w$ is the number of occurrences of letters of $\mathcal{B}$ in the word $w$. This number will be denoted $|w|_{\mathcal{B}}$.
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		In particular, we have trivially $|w|=|w|_{\mathcal{A}}$.
		\end{tcolorbox}
		For every letter $\ell\in\mathcal{A}$, $|w|_{\ell}$ is the number of occurrences of $\ell$ in $w$. We have:
		
		
		\item[D5.] Given $w=\ell_1\ell_2\ldots\ell_n$, with $\ell_1\ell_2\ldots\ell_n\in\mathcal{A}$. The "\NewTerm{mirror word}\index{mirror word}" of $w$ is the word denoted $\widetilde{w}$ defined by:
		
		Obviously:
		
		
		\item[D6.] A word $u$ is a "\NewTerm{prefix}" or "\NewTerm{left factor}" of a word $v$ if there is a word $x$ such that $ux=v$. The word $u$ is moreover a "\NewTerm{strict prefix}" or "\NewTerm{eigen-prefix}" if $u\neq v$. Symmetrically, $u$ is a "\NewTerm{suffix}" or "\NewTerm{right factor}" of $v$ if $xu=v$ a word $x$. If $u\neq v$, then $u$ is "\NewTerm{strict suffix}" or "\NewTerm{eigen-suffix}". The number of prefixes of a non-empty word $v$ is $1+|v|$ (the empty word always being a prefix, we always have any non-empty word that has at least the empty word as a prefix).
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		The word $w=aabab$ on $\mathcal{A}=\{a,b\}$ has $12$ different possible factors:
		
		\end{tcolorbox}
	\end{enumerate}
	\begin{lemma}[Levy's lemma]
	Given $x$, $y$, $z$, $t$ be words such as $xy=zt$. Then there exists a word $w$ such that:
	
	with obviously:
	
	or:
	
	with also by extension:
		
	\end{lemma}
	It results logically in particular that if $|x|=|y|$, the word $w$ is empty, and therefore $x=z$ and $y=t$. In other words:
	\begin{theorem}
	A free monoid (see the reminder below) can be simplified on the left and on the right.
	\end{theorem}
	\begin{dem}
	Let us put:
	
	with $a_i\in\mathcal{A}$, similarly:
	
	with $b_i\in\mathcal{A}$.
	
	As:
	
	We have:
	
	(but not necessarily $n=p$) and:
	
	for $i=1,\ldots,m$ so that:
	
	If $|z|=p\le n=|x|$, let us put $w=x_{p+1}\ldots x_n$. Therefore:
	
	If $|z|>|x|$, let us put $w=x_{n+1}\ldots x_p$. Therefore:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Let us just do a recall of a parallel of the section of Set Theory... In the framework of the study automata a "\NewTerm{free monoid}\index{free monoid}" is a set $\mathcal{A}$ (the alphabet), whose elements are the letters $\ell_i$. Therefore the composition law $\cdot$ is denoted:
	
	In the Set Theory section, we were simply talking about the concept of "monoid". The monoid $(\mathcal{A},\cdot)=(\mathcal{A},\text{concatenation})$.
	
	\pagebreak
	\subsubsection{Languages}
	The subsets of $\mathcal{A}$ are named "\NewTerm{formal languages}\index{formal language}". For example, for $\mathcal{A}=\{a,b\}$, the set $\mathcal{A}^{+}=\{a^nb^n|n\ge 0\}$ is a language.

	We define on the languages several operations. The set operations are the union, intersection, complementarity and the resulting difference (\SeeChapter{see section Set Theory page \pageref{set operations}}). If $X$ and $Y$ are two parts of $\mathcal{A}^{*}$ then for recall each of this operation is defined by:
	\begin{itemize}
		\item Union:
		

		\item Intersection:
		

		\item Complementarity:
		

		\item Difference:
		
	\end{itemize}
	The also have for operation the product (of concatenation) of two languages $X$ and $Y$ is the language:
	
	and we have for recall:
	
	and also the operation of left quotient of $Y$ by $X$:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us consider three languages:
	
	Then we have for the union:
	
	for the concatenation:
	
	for a given quotient:
	
	another quotient:
	
	and a last quotient:
	
	and a stupid difference example:
	
	Following the request of a reader we will detail:
	
	by recalling the definition:
	
	We then have explicitly:
	
	and in the concatenation product of the two languages $X$ and $Y$, the only words where we find the elements $\{0,1\}$ of the language $Z$ as a prefix are:
	
	and as by definition $Z^{-1}(XY)$ is the unique set of terms $w$ which follow the prefixes that constitute the terms of $Z$, then there remains only:
	
	\end{tcolorbox}
	We have the following properties:
	\begin{enumerate}
		\item[P1.] Obviously:
		

		\item[P2.] Less obvious:
		
		where the inclusion is generally strict. To conceptualize this property, we must not forget that $X$ is a set of words and that $Y$, $Z$ do not necessarily have words of the same length!
	\end{enumerate}	
	The powers of $X$ are defined by:
	
	for $n\geq 1$

	In particular, if $\mathcal{A}$ is an alphabet, $\mathcal{A}^n$ is the set of words of length $n$.
	
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] The "\NewTerm{Kleene star}\index{}" (or "\NewTerm{Kleene operator}" or "\NewTerm{Kleene closure}") of $X$ is the set:
		
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		Given $X=\{a,ba\}$. The words of $X^{*}$, classified by length are:
		\begin{table}[H]
			\begin{tabular}{cc}
			$0$      & $\varepsilon$              \\
			$1$      & $a$                        \\
			$2$      & $aa,ba$                    \\
			$3$      & $aaa,aba,baa$              \\
			$4$      & $aaaa,aaba,abaa,baaa,baba$ \\
			$\ldots$ &                           
			\end{tabular}
		\end{table}
		\end{tcolorbox}
	
		\item[D2.] The operator "$+$" is defined similarly:
		
	\end{enumerate}
	
	
	\subsubsection{Equations}
	First let us see a little something we will need later: given $u$ and $v$ two non-empty words. The three following conditions are equivalent (without proof because quite trivial):
	\begin{enumerate}
		\item[C1.] $uv=vu$
		\item[C2.] $\exists\, n,m>1:\quad u^n=v^m$
		\item[C3.] $\exists\, w\neq\varepsilon, k,l\ge 1:\quad u=w^k,v=w^l$ 
	\end{enumerate}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Let us recall again that we do not necessarily have $|u|=|v|$ but that we can very well have $|u|>|v|$.
	\end{tcolorbox}
	Let us now turn to interesting things (some fuzzy points of the section of Proof Theory can be clarified here sometimes...)!
	
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] Let $\mathcal{V}$ and $\mathcal{A}$ be two disjoint alphabets (you can imagine them as the set of variables and respectively of the constants for example...). An "\NewTerm{equation in words}" with constants on $\mathcal{A}$ is a couple $e=(\alpha,\beta)$ of words $(\mathcal{V}\cup \mathcal{A})^{*}$. Such an equation is represented by $\alpha=\beta$. It is therefore necessary to see the two chosen words as the left and right members respectively of an equation.
		
		\item[D2.]  An equation is said to be "\NewTerm{non-trivial equation}" if $\alpha\neq \beta$.
		
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		Given $\mathcal{V}=\{x\}$ and $\mathcal{A}=\{a\}$ and let us define:
		
		then we have the following equation in words:
		
		\end{tcolorbox}

		\item[D3.] An equation $e$ is said to be an "\NewTerm{equation without constant}" if $\alpha,\beta\in\mathcal{V}^{*}$.

		\item[D4.] A "\NewTerm{solution}" of the equation $e$ is a monoid homomorphism (\SeeChapter{see section Set Theory page \pageref{homomorphism of monoid}}):
		
		invariant (because every letter on $\mathcal{A}$ is sent on $\mathcal{A}^{*}$ and therefore every word on $\mathcal{A}$ is sent on $\mathcal{A}$) on $\mathcal{A}$ such that:
		
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		Let us recall that the definition of the homomorphism is such that if $\alpha=xy$ then:
		
		\end{tcolorbox}
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		Given $\mathcal{A}=\{a,b\}$ and $\mathcal{V}=\{x,y\}$. Let us wow consider the following words:
		
		let us define $h$ such that it sends $x$ on $b$, $y$ on $a$, $a$ on $a$, $b$ on $b$. Therefore we have well:
		
		and we will always have for every couple:
		
		\end{tcolorbox}
		
		\item[D5.] A solution $h$ is said to be a "\NewTerm{cyclic solution}" if there exists a word $w$ (belonging to $\mathcal{A}$) such that $h(x)\in w^{E}$ (considering the word itself as an alphabet therefore) for any variable $x$.
	\end{enumerate}
	
	
	\subsubsection{Codes}
	\textbf{Definition (\#\thesection.\mydef):} We name "\NewTerm{code}\index{code}" any part $\mathcal{C}$ of a free monoid $\mathcal{A}^{*}$ that satisfies the following condition for any (word) $x_1,\ldots,x_n,y_1,\ldots,y_m\in \mathcal{C}$:
	
	In other words, $\mathcal{C}$ is a code if every word of $\mathcal{C}^{*}$ (word composed of words) \underline{uniquely factorize} into a product words of $\mathcal{C}$. When a set is not a code, general we see it quite easily.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\	
	E1. The set (language) $\{a,ab,ba\}$ is not a code since the word $aba$ can written both as product $a\cdot ba$ and as product $ab\cdot a$.\\
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The simplest codes are the "\NewTerm{uniform codes}\index{uniform codes}". These are sets whose all words have the same length (which means that since each word is different, the combination of words can hardly differ).
	\end{tcolorbox}
	\phantom \\
	E2. The set $\mathcal{A}^{*}$ of the words of length $n$ is a code, if $n\ge 1$. The ASCII code that associates to some characters binary words of length $7$ (see ASCII table) with some characters is an example of uniform code.
	\end{tcolorbox}
	
	\pagebreak
	\paragraph{Prefix codes}\mbox{}\\\\
	\textbf{Definition (\#\thesection.\mydef):} A "\NewTerm{prefix code}\index{prefix code}" is a type of code system (typically a variable-length code) distinguished by its possession of the "prefix property", which requires that there is no whole code word in the system that is a prefix (initial segment) of any other code word in the system. 
	
	Prefix codes are also known as "\NewTerm{prefix-free codes}", "\NewTerm{prefix condition codes}" and "\NewTerm{instantaneous codes}". Although Huffman coding is just one of many algorithms for deriving prefix codes, prefix codes are also widely referred to as "Huffman codes" (see further below), even when the code was not produced by a Huffman algorithm. 
	
	Using prefix codes, a message can be transmitted as a sequence of concatenated code words, without any out-of-band markers or (alternatively) special markers between words to frame the words in the message. The recipient can decode the message unambiguously, by repeatedly finding and removing sequences that form valid code words. This is not generally possible with codes that lack the prefix property
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} The variable-length Huffman codes, country calling codes, the country and publisher parts of ISBNs, the Secondary Synchronization Codes used in the UMTS W-CDMA 3G Wireless Standard, and the instruction sets (machine language) of most computer microarchitectures are prefix codes.\\
	
	\textbf{R2.} Prefix codes are not error-correcting codes. In practice, a message might first be compressed with a prefix code, and then encoded again with channel coding (including error correction) before transmission.
	\end{tcolorbox}
	The Morse codes encodes the letter \texttt{E}, the most frequent, with a '.' And the letter \texttt{Y}, more rare, by '-.--': this is an example of a variable length code, which makes it possible to represent the most frequent letters or words by shorter words. 
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,661); %set diagram left start at 0, and has height of 661
		
		%Straight Lines [id:da860390153517328] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (144,91.5) -- (144,109.78) ;
		%Straight Lines [id:da42586715638068684] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (185,91.5) -- (185,109.78) ;
		%Straight Lines [id:da599162606261634] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (254,91.5) -- (254,109.78) ;
		%Straight Lines [id:da4708610553351851] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (321,91.5) -- (321,109.78) ;
		%Straight Lines [id:da5397017278274907] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (405,91.5) -- (405,109.78) ;
		%Straight Lines [id:da7439479197031826] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (518,91.5) -- (518,109.78) ;
		%Straight Lines [id:da13306036377636765] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (133,140) -- (133,158.28) ;
		%Straight Lines [id:da7649294018931261] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (206,140) -- (206,158.28) ;
		%Straight Lines [id:da10575337343986768] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (289,140) -- (289,158.28) ;
		%Straight Lines [id:da6292631899532686] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (351,140) -- (351,158.28) ;
		%Straight Lines [id:da6355626390599387] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (419,140) -- (419,158.28) ;
		%Shape: Circle [id:dp4354658097045718] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (100.81,83.16) .. controls (100.81,82.14) and (101.64,81.31) .. (102.66,81.31) .. controls (103.67,81.31) and (104.5,82.14) .. (104.5,83.16) .. controls (104.5,84.17) and (103.67,85) .. (102.66,85) .. controls (101.64,85) and (100.81,84.17) .. (100.81,83.16) -- cycle ;
		%Shape: Circle [id:dp7947443528497782] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (107.48,83.16) .. controls (107.48,82.14) and (108.31,81.31) .. (109.32,81.31) .. controls (110.34,81.31) and (111.17,82.14) .. (111.17,83.16) .. controls (111.17,84.17) and (110.34,85) .. (109.32,85) .. controls (108.31,85) and (107.48,84.17) .. (107.48,83.16) -- cycle ;
		%Shape: Circle [id:dp03917831905212199] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (114.15,83.16) .. controls (114.15,82.14) and (114.97,81.31) .. (115.99,81.31) .. controls (117.01,81.31) and (117.84,82.14) .. (117.84,83.16) .. controls (117.84,84.17) and (117.01,85) .. (115.99,85) .. controls (114.97,85) and (114.15,84.17) .. (114.15,83.16) -- cycle ;
		%Shape: Circle [id:dp044301091306574] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (120.81,83.16) .. controls (120.81,82.14) and (121.64,81.31) .. (122.66,81.31) .. controls (123.67,81.31) and (124.5,82.14) .. (124.5,83.16) .. controls (124.5,84.17) and (123.67,85) .. (122.66,85) .. controls (121.64,85) and (120.81,84.17) .. (120.81,83.16) -- cycle ;
		%Shape: Circle [id:dp9380875187290303] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (163.81,83.16) .. controls (163.81,82.14) and (164.64,81.31) .. (165.66,81.31) .. controls (166.67,81.31) and (167.5,82.14) .. (167.5,83.16) .. controls (167.5,84.17) and (166.67,85) .. (165.66,85) .. controls (164.64,85) and (163.81,84.17) .. (163.81,83.16) -- cycle ;
		%Shape: Circle [id:dp8538537121274692] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (226.56,83.16) .. controls (226.56,82.14) and (227.39,81.31) .. (228.41,81.31) .. controls (229.42,81.31) and (230.25,82.14) .. (230.25,83.16) .. controls (230.25,84.17) and (229.42,85) .. (228.41,85) .. controls (227.39,85) and (226.56,84.17) .. (226.56,83.16) -- cycle ;
		%Shape: Circle [id:dp41654207249789255] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (203.06,83.16) .. controls (203.06,82.14) and (203.89,81.31) .. (204.91,81.31) .. controls (205.92,81.31) and (206.75,82.14) .. (206.75,83.16) .. controls (206.75,84.17) and (205.92,85) .. (204.91,85) .. controls (203.89,85) and (203.06,84.17) .. (203.06,83.16) -- cycle ;
		%Shape: Circle [id:dp5416264929251682] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (233.81,83.16) .. controls (233.81,82.14) and (234.64,81.31) .. (235.66,81.31) .. controls (236.67,81.31) and (237.5,82.14) .. (237.5,83.16) .. controls (237.5,84.17) and (236.67,85) .. (235.66,85) .. controls (234.64,85) and (233.81,84.17) .. (233.81,83.16) -- cycle ;
		%Shape: Circle [id:dp8724933082691888] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (301.56,83.16) .. controls (301.56,82.14) and (302.39,81.31) .. (303.41,81.31) .. controls (304.42,81.31) and (305.25,82.14) .. (305.25,83.16) .. controls (305.25,84.17) and (304.42,85) .. (303.41,85) .. controls (302.39,85) and (301.56,84.17) .. (301.56,83.16) -- cycle ;
		%Shape: Circle [id:dp6215374595304275] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (295.31,83.16) .. controls (295.31,82.14) and (296.14,81.31) .. (297.16,81.31) .. controls (298.17,81.31) and (299,82.14) .. (299,83.16) .. controls (299,84.17) and (298.17,85) .. (297.16,85) .. controls (296.14,85) and (295.31,84.17) .. (295.31,83.16) -- cycle ;
		%Shape: Circle [id:dp8692354404919738] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (271.31,83.16) .. controls (271.31,82.14) and (272.14,81.31) .. (273.16,81.31) .. controls (274.17,81.31) and (275,82.14) .. (275,83.16) .. controls (275,84.17) and (274.17,85) .. (273.16,85) .. controls (272.14,85) and (271.31,84.17) .. (271.31,83.16) -- cycle ;
		%Shape: Circle [id:dp8939915320867995] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (458.31,83.16) .. controls (458.31,82.14) and (459.14,81.31) .. (460.16,81.31) .. controls (461.17,81.31) and (462,82.14) .. (462,83.16) .. controls (462,84.17) and (461.17,85) .. (460.16,85) .. controls (459.14,85) and (458.31,84.17) .. (458.31,83.16) -- cycle ;
		%Shape: Circle [id:dp2692067225617245] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (464.81,83.16) .. controls (464.81,82.14) and (465.64,81.31) .. (466.66,81.31) .. controls (467.67,81.31) and (468.5,82.14) .. (468.5,83.16) .. controls (468.5,84.17) and (467.67,85) .. (466.66,85) .. controls (465.64,85) and (464.81,84.17) .. (464.81,83.16) -- cycle ;
		%Straight Lines [id:da6210216270474729] 
		\draw    (211.91,83.16) -- (221.75,83.16) ;
		%Straight Lines [id:da444454982885381] 
		\draw    (280.16,83.16) -- (290,83.16) ;
		%Straight Lines [id:da7793529274853159] 
		\draw    (341.91,83.16) -- (351.75,83.16) ;
		%Straight Lines [id:da1423223883740783] 
		\draw    (358.91,83.16) -- (368.75,83.16) ;
		%Straight Lines [id:da8025262160713686] 
		\draw    (376.16,83.16) -- (386,83.16) ;
		%Straight Lines [id:da5838950547005635] 
		\draw    (425.66,83.16) -- (435.5,83.16) ;
		%Straight Lines [id:da8791666218452365] 
		\draw    (442.66,83.16) -- (452.5,83.16) ;
		%Straight Lines [id:da7613981085727601] 
		\draw    (473.91,83.16) -- (483.75,83.16) ;
		%Straight Lines [id:da8207062995864816] 
		\draw    (490.41,83.16) -- (500.25,83.16) ;
		%Shape: Circle [id:dp14460187943203184] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (147.15,130.66) .. controls (147.15,129.64) and (147.97,128.81) .. (148.99,128.81) .. controls (150.01,128.81) and (150.83,129.64) .. (150.83,130.66) .. controls (150.83,131.67) and (150.01,132.5) .. (148.99,132.5) .. controls (147.97,132.5) and (147.15,131.67) .. (147.15,130.66) -- cycle ;
		%Shape: Circle [id:dp3010434279469836] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (304.15,130.66) .. controls (304.15,129.64) and (304.97,128.81) .. (305.99,128.81) .. controls (307.01,128.81) and (307.83,129.64) .. (307.83,130.66) .. controls (307.83,131.67) and (307.01,132.5) .. (305.99,132.5) .. controls (304.97,132.5) and (304.15,131.67) .. (304.15,130.66) -- cycle ;
		%Shape: Circle [id:dp7356169235257342] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (327.81,130.66) .. controls (327.81,129.64) and (328.64,128.81) .. (329.66,128.81) .. controls (330.67,128.81) and (331.5,129.64) .. (331.5,130.66) .. controls (331.5,131.67) and (330.67,132.5) .. (329.66,132.5) .. controls (328.64,132.5) and (327.81,131.67) .. (327.81,130.66) -- cycle ;
		%Shape: Circle [id:dp4027352492122036] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (366.15,130.66) .. controls (366.15,129.64) and (366.97,128.81) .. (367.99,128.81) .. controls (369.01,128.81) and (369.83,129.64) .. (369.83,130.66) .. controls (369.83,131.67) and (369.01,132.5) .. (367.99,132.5) .. controls (366.97,132.5) and (366.15,131.67) .. (366.15,130.66) -- cycle ;
		%Shape: Circle [id:dp3115854843991348] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (389.81,130.66) .. controls (389.81,129.64) and (390.64,128.81) .. (391.66,128.81) .. controls (392.67,128.81) and (393.5,129.64) .. (393.5,130.66) .. controls (393.5,131.67) and (392.67,132.5) .. (391.66,132.5) .. controls (390.64,132.5) and (389.81,131.67) .. (389.81,130.66) -- cycle ;
		%Shape: Circle [id:dp14971060557713534] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (396.48,130.66) .. controls (396.48,129.64) and (397.3,128.81) .. (398.32,128.81) .. controls (399.34,128.81) and (400.17,129.64) .. (400.17,130.66) .. controls (400.17,131.67) and (399.34,132.5) .. (398.32,132.5) .. controls (397.3,132.5) and (396.48,131.67) .. (396.48,130.66) -- cycle ;
		%Shape: Circle [id:dp9899047238948935] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (451.81,130.66) .. controls (451.81,129.64) and (452.64,128.81) .. (453.66,128.81) .. controls (454.67,128.81) and (455.5,129.64) .. (455.5,130.66) .. controls (455.5,131.67) and (454.67,132.5) .. (453.66,132.5) .. controls (452.64,132.5) and (451.81,131.67) .. (451.81,130.66) -- cycle ;
		%Shape: Circle [id:dp15183975260335503] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (457.81,130.66) .. controls (457.81,129.64) and (458.64,128.81) .. (459.66,128.81) .. controls (460.67,128.81) and (461.5,129.64) .. (461.5,130.66) .. controls (461.5,131.67) and (460.67,132.5) .. (459.66,132.5) .. controls (458.64,132.5) and (457.81,131.67) .. (457.81,130.66) -- cycle ;
		%Straight Lines [id:da5832868167918452] 
		\draw    (156.82,130.66) -- (166.67,130.66) ;
		%Straight Lines [id:da7108679496808981] 
		\draw    (174.16,130.66) -- (184,130.66) ;
		%Straight Lines [id:da6206127925088571] 
		\draw    (225.49,130.66) -- (235.33,130.66) ;
		%Straight Lines [id:da9334930653851792] 
		\draw    (241.49,130.66) -- (251.33,130.66) ;
		%Straight Lines [id:da26635391799436214] 
		\draw    (257.82,130.66) -- (267.67,130.66) ;
		%Straight Lines [id:da9059829213091444] 
		\draw    (312.49,130.66) -- (322.33,130.66) ;
		%Straight Lines [id:da19493457039547968] 
		\draw    (375.16,130.66) -- (385,130.66) ;
		%Straight Lines [id:da05294065834563333] 
		\draw    (437.16,130.66) -- (447,130.66) ;
		
		% Text Node
		\draw (104.66,91.5) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 192; green, 188; blue, 188 }  ,opacity=1 ] [align=left] {{\fontfamily{pcr}\selectfont \textbf{{\LARGE H}}}};
		% Text Node
		\draw (157,91.5) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 192; green, 188; blue, 188 }  ,opacity=1 ] [align=left] {{\fontfamily{pcr}\selectfont \textbf{{\LARGE E}}}};
		% Text Node
		\draw (209.91,91.5) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 192; green, 188; blue, 188 }  ,opacity=1 ] [align=left] {{\fontfamily{pcr}\selectfont \textbf{{\LARGE L}}}};
		% Text Node
		\draw (279.16,91.5) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 192; green, 188; blue, 188 }  ,opacity=1 ] [align=left] {{\fontfamily{pcr}\selectfont \textbf{{\LARGE L}}}};
		% Text Node
		\draw (353.75,91.5) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 192; green, 188; blue, 188 }  ,opacity=1 ] [align=left] {{\fontfamily{pcr}\selectfont \textbf{{\LARGE O}}}};
		% Text Node
		\draw (455.5,105) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 192; green, 188; blue, 188 }  ,opacity=1 ] [align=left] {{\fontfamily{pcr}\selectfont \textbf{{\LARGE ,}}}};
		% Text Node
		\draw (239.49,140.5) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 192; green, 188; blue, 188 }  ,opacity=1 ] [align=left] {{\fontfamily{pcr}\selectfont \textbf{{\LARGE O}}}};
		% Text Node
		\draw (375,140.5) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 192; green, 188; blue, 188 }  ,opacity=1 ] [align=left] {{\fontfamily{pcr}\selectfont \textbf{{\LARGE L}}}};
		% Text Node
		\draw (158.82,140.5) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 192; green, 188; blue, 188 }  ,opacity=1 ] [align=left] {{\fontfamily{pcr}\selectfont \textbf{{\LARGE W}}}};
		% Text Node
		\draw (308,140.5) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 192; green, 188; blue, 188 }  ,opacity=1 ] [align=left] {{\fontfamily{pcr}\selectfont \textbf{{\LARGE R}}}};
		% Text Node
		\draw (440,140.5) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 192; green, 188; blue, 188 }  ,opacity=1 ] [align=left] {{\fontfamily{pcr}\selectfont \textbf{{\LARGE D}}}};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{'Hello World' in Morse code}
	\end{figure}
	An important property is the uniqueness of the decoding (injective application), a problem that does not arise for codes of constant length. It can be solved, but too costly, when a special symbol separates two successive words of the code (the "blank" in the case of the Morse code). We can therefore not use such a non-constant length code if no word is the prefix of another code word. And as the reader has probably understand it now, a code with this property is named a "prefix code".
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\	
	Suppose we decide on a variable-size code convention, which matches, among other things, the following values:
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|}
		\hline
		\rowcolor[gray]{0.75} 
		\textbf{Character} & \textbf{Code} \\ \hline
		$0$ & $11$ \\ \hline
		$2$ & $11010$ \\ \hline
		$12$ & $00$ \\ \hline
		$127$ & $0111100$ \\ \hline
		$255$ & $0100$ \\ \hline
		\end{tabular}
	\end{table}
	Let us suppose that we have to decode the sequence: 
	\begin{center}
		$1101000111100$
	\end{center}

	Several interpretations (factorization) are then possible:
	\begin{center}
		$1101000111100 = 11\; 0100\; 0111100 = 0\; 255\; 127$
	\end{center}
	or:
	\begin{center}
		$1101000111100 = 11010\; 00\; 11\; 11\; 00 = 2\; 12\; 0\; 0\; 12$
	\end{center}
	And now we are very embarrassed! With several equivalent possibilities between which one can not decide, one is incapable of retranscribing the initial code.\\

	The problem that has arisen here is that some codes are the beginning of other codes. Here, "$11$" is the code of the number $0$, but it is also the beginning of "$11010$", code of the number $2$. Hence the ambiguity!
	\end{tcolorbox}
	We then better understand the purpose and the name of "prefix codes". Thus, in order for there to be no ambiguity at the time of the decoding, we must absolutely have a prefix code if the code is not of constant length.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\	
	The set:
	\begin{gather*}
		X=\{ab,ababa,baa\}
	\end{gather*}
	is a code. Here, knowledge of the beginning of a possible code $abababa$ does not yet make it possible to know whether the decomposition begins by $ab\cdot ab\cdot ab\cdot a$ or by $ababa\cdot ba$. It is only after reading the next letter (not indicated in this example) that we know if the decomposition starts with $ab$ (if the letter is $b$) or by $ababa$ (if the letter is $a$)
	\end{tcolorbox}
	
	\pagebreak
	\textbf{Definition (\#\thesection.\mydef):} A code is with "\NewTerm{finite decryption delay}" if there exists an integer $d$ such that, whenever a message $w$ begins with $p=x_1x_2\ldots x_{d+1}$ with $x_1,x_2,\ldots, x_{d+1}\in X$ then the complete factorization of $w$ begins with $x_d$. It is therefore after a "delay" of $d$ code words that we can affirm that the first word found is the right one (as in the example above that is a $8$ finite decryption delay code).
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\	
	E1. The code:
	
	has therefore a delay $d=0$.\\

	E2. The code:
	
	has therefore a delay $d=2$.
	\end{tcolorbox}
	
	\subsection{Linguistic algorithms}
	Let us put into practice what has already been seen so far in order to support us a little on "useful" concrete stuff!
	
	\subsubsection{Huffman algorithm}\label{huffman algorithm}
	Let us first recall that in computing, we decide to encode an integer that has a value between $0$ and $255$ by a sequence of $8$ binary digits (or "bits" in English, valued $0$ or $1$), also named "byte" (whose maximum value is equal to $2^8$).
	
	Even if there is a mathematical logic in the way of associating an $8$-bit binary number to an integer between $0$ and $255$ (\SeeChapter{see section Numerical Methods page \pageref{computer representation of numbers}}) we can imagine any coding of the type:
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|c|}{\cellcolor[gray]{0.75}Integer} & \multicolumn{1}{c|}{\cellcolor[gray]{0.75}Corresponding unique byte} \\ \hline
		$\ldots$ & $\ldots$ \\ \hline
		$138$ & $10001010$ \\ \hline
		$139$ & $10001011$ \\ \hline
		$\ldots$ & $\ldots$ \\ \hline
		\end{tabular}
	\end{table}
	in fact the correspondence can be any one, dictated by our imagination, as long as each integer between $0$ and $255$ is assigned to a fixed length binary code and only one. Once a correspondence is fixed, it is enough to take it as a convention.
	
	The byte defined according to this convention is the basic unit of data storage. Any computer file is a sequence of bytes arranged in a defined order. The size of the file is simply the number of bytes that constitute it. The kilobyte (KB) corresponds to $1024$ (not $1000$) bytes, the megabyte (MB) to $1024\times 1024$ bytes.

	It should be noticed that this representation in base $2$ is only a convention! Other conventions are possible, which would be equally appropriate if everyone agrees to use the same convention.

	The problem we are asking ourselves is: would there be another way of coding the numbers, perhaps less logical but more judicious, in such a way that the size of the same file rewritten according to the new convention would be smaller?

	The binary encoding convention is ultimately very democratic: whether you are a $0$ or a $255$, we allocate you $8$ bits anyway to be able to code you. In other words, each possible input (a number between $0$ and $255$) is encoded on $8$ bits. This is a fixed size encoding.

	From the point of view of our problem (data compression), it would not matter if each of the possible values ($0$...$255$) were represented as frequently as the others. But in general this is not the case.

	For example, see below the parsing of Wordpad.exe file (see your Accessories folder in your Microsoft Windows operating system\footnote{This plot dates from they year 12001 (holocene calendar)... so its rendering quality is very bad. We will do it again when we will have the time}...). In this plot, on the abscissa as on the ordinate are the possible values of a given octet (hence $0$ ... $255$). On the diagonal, at the abscissa and corresponding ordinate, the size of the circle is proportional to the number of bytes having this value in the file:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/wordpad_byte_parsing.jpg}
		\caption[]{Analysis of the byte distribution of a the Wordpad.exe file}
	\end{figure}
	We see clearly that the values $0$, $128$ and $255$ are much more frequent than the others!

	As an indication, here are some values:
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|l|}
		\hline
		\rowcolor[gray]{0.75} 
		\textbf{Value} & \textbf{Count} & \textbf{Frequency} \\ \hline
		$0$ & $71891$ & $34.4\%$ \\ \hline
		$2$ & $1119$ & $0.53\%$ \\ \hline
		$128$ & $1968$ & $0.94\%$ \\ \hline
		$130$ & $79$ & $0.038\%$ \\ \hline
		$255$ & $10422$ & $4.99\%$ \\ \hline
		\end{tabular}
	\end{table}
	We will now decide of a variable-sized coding convention, which represents a value that is frequent by a small number of bits, and an uncommon value by a large number of bits.
	
	We will now decide on a variable-sized coding convention, which represents a value that is frequent by a small number of bits, and an uncommon value by a large number of bits.

	For example, $0$ will now be represented by the sequence "$11$" (when before it was "$00000000$"), $128$ by "$1011010$" (when before it was "$10000000$"), $255$ by "$0100$" (when before it was "$11111111$"), etc.

	Given that $0$ represents almost one third of the file, we have gained a considerable place by coding it on two bits instead of eight! And same for the other frequent values...

	Therefore "\NewTerm{Huffman algorithm}\index{Huffman algorithm}" is a recipe for generating a variable-length prefix code from the frequency table of a sequence of values. So, if you have followed the theory so far, it is a solution to our problem.

	Suppose that our file is extremely simple, consisting of a single french word (\textit{unconstitutionally}):
	\begin{center}
		\texttt{anticonstitutionnellement}
	\end{center}
	There are $25$ characters in this file. Each character being encoded by an $8$-bit octet (ASCII encoding), this means $25$ bytes, or $200$ bits! Let's see what we can do with that.

	First let us render the table of frequencies:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}\textbf{Letter}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Count}} \\ \hline
		\texttt{a} & $1$ \\ \hline
		\texttt{c} & $1$ \\ \hline
		\texttt{s} & $1$ \\ \hline
		\texttt{u} & $1$ \\ \hline
		\texttt{m} & $1$ \\ \hline
		\texttt{o} & $2$ \\ \hline
		\texttt{l} & $2$ \\ \hline
		\texttt{i} & $3$ \\ \hline
		\texttt{e} & $3$ \\ \hline
		\texttt{n} & $5$ \\ \hline
		\texttt{t} & $5$ \\ \hline
		\end{tabular}
	\end{table}
	All other bytes (strings) have a null frequency: they are not represented in the file.

	Now we create a "terminal node" for each entry of the array:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.9]{img/computing/huffman_step_0.jpg}
	\end{figure}
	What makes for us now $11$ trees containing only one knot each.

	We now start an iteration: each time we delete the two trees on the left and replace them with a "sum tree". The new tree is inserted in the list in ascending order, and is repeated until there is only one tree left. Therefore we get:
	\begin{itemize}
		\item First Iteration:
		\begin{figure}[H]
			\centering
			\includegraphics[scale=1]{img/computing/huffman_step_1.jpg}
		\end{figure}
		
		\item Second iteration:
		\begin{figure}[H]
			\centering
			\includegraphics[scale=0.9]{img/computing/huffman_step_2.jpg}
		\end{figure}
		
		\item Third iteration:
		\begin{figure}[H]
			\centering
			\includegraphics[scale=1]{img/computing/huffman_step_3.jpg}
		\end{figure}
		
		\item ....
	\end{itemize}
	...and the final tree is:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/computing/huffman_step_final.jpg}
	\end{figure}
	And that's it!

	Now, the associated code to each letter is none other than the path to the corresponding terminal node from the root, noting $0$ for each left branch and $1$ for each right branch.

	Finally:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}\textbf{Letter}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Huffman binary code}} \\ \hline
		\texttt{n} & $00$ \\ \hline
		\texttt{t} & $01$ \\ \hline
		\texttt{i} & $100$ \\ \hline
		\texttt{e} & $101$ \\ \hline
		\texttt{a} & $11000$ \\ \hline
		\texttt{c} & $11001$ \\ \hline
		\texttt{o} & $1101$ \\ \hline
		\texttt{l} & $1110$ \\ \hline
		\texttt{m} & $11110$ \\ \hline
		\texttt{s} & $111110$ \\ \hline
		\texttt{u} & $111111$ \\ \hline
		\end{tabular}
	\end{table}
	And here is now, transcribed with our new code, the starting word:
	\begin{center}
	{\small \texttt{110000001100110011101001111100110001111111011001101000010111101110101111101010001}}
	\end{center}
	which makes $81$ bits, instead of $200$ at the beginning! This corresponds to a compression ratio of almost $60\%$.
	
	The fact of having generated code using a binary tree ensures that no code can be the prefix of another one. You can verify that using the encoding table, there is no ambiguity possible to decode our compressed word! This is why the Huffman algorithm is in many compressed formats (for example the famous MP3 for audio encoding).
	
	\subsubsection{Sardinas and Patterson algorithm}
	When we have to deal with long codes, the difficulty is to check whether the code is really one... In order to do this, we can use the Sardinas and Patterson algorithm (the proof of this algorithm will be done during the next update of this section of the book).

	In coding theory, the Sardinas–Patterson algorithm is a classical algorithm for determining in polynomial time whether a given variable-length code is uniquely decodable, named after August Albert Sardinas and George W. Patterson, who published it in 11953 (holocene calendar).

	To do this check, we construct a graph $G(X)=(P,U)$, where $P$ is the set of non-empty prefixes (according to the definition of "prefixes" given earlier above) of words of $X$, and $U$ the set of pairs $(u, v)$ such that one of the following possibilities is met:
	\begin{enumerate}
		\item[P1.] $uv\in X$ (by eliminating duplicate pairs if necessary)

		\item[P2.] $v\notin X$ and it exist $x\in X$ such that $ux=v$
	\end{enumerate}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\	
	For $X=\{a,bb,abbba,babab\}$, the set $P$ contains, in addition to $X$ (which are prefixes of $\varepsilon$), the words $\{b,ab,abb,abbb,b,ba,bab,baba\}$ (respectively prefixes of $\{b,bba,ba,a,abab,bab,ab,b\}$).\\

	First we see immediately that the set of $X=\{a,bb,abbba,babab\}$ is not a code because:
	
	Now the pairs of $U$ are for the first possibility P1:
	
	and for the second possibility P2:
	
	for which the $x$ which is used to form the $v$ is respectively $bb$, $bb$, $a$, $a$.\\
	
	The Sardinas and Patterson graph will therefore be:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1107); %set diagram left start at 0, and has height of 1107
		
		%Shape: Donut [id:dp0671815903868791] 
		\draw   (44.58,129.5) .. controls (44.58,120.7) and (51.7,113.57) .. (60.5,113.57) .. controls (69.3,113.57) and (76.43,120.7) .. (76.43,129.5) .. controls (76.43,138.3) and (69.3,145.43) .. (60.5,145.43) .. controls (51.7,145.43) and (44.58,138.3) .. (44.58,129.5)(41,129.5) .. controls (41,118.73) and (49.73,110) .. (60.5,110) .. controls (71.27,110) and (80,118.73) .. (80,129.5) .. controls (80,140.27) and (71.27,149) .. (60.5,149) .. controls (49.73,149) and (41,140.27) .. (41,129.5) ;
		%Shape: Donut [id:dp4045746640850667] 
		\draw   (44.58,214.5) .. controls (44.58,205.7) and (51.7,198.58) .. (60.5,198.58) .. controls (69.3,198.58) and (76.43,205.7) .. (76.43,214.5) .. controls (76.43,223.3) and (69.3,230.43) .. (60.5,230.43) .. controls (51.7,230.43) and (44.58,223.3) .. (44.58,214.5)(41,214.5) .. controls (41,203.73) and (49.73,195) .. (60.5,195) .. controls (71.27,195) and (80,203.73) .. (80,214.5) .. controls (80,225.27) and (71.27,234) .. (60.5,234) .. controls (49.73,234) and (41,225.27) .. (41,214.5) ;
		%Shape: Donut [id:dp2832420026677631] 
		\draw   (373,128.5) .. controls (373,117.73) and (381.73,109) .. (392.5,109) .. controls (403.27,109) and (412,117.73) .. (412,128.5) .. controls (412,139.27) and (403.27,148) .. (392.5,148) .. controls (381.73,148) and (373,139.27) .. (373,128.5) .. controls (373,117.73) and (381.73,109) .. (392.5,109) .. controls (403.27,109) and (412,117.73) .. (412,128.5) .. controls (412,139.27) and (403.27,148) .. (392.5,148) .. controls (381.73,148) and (373,139.27) .. (373,128.5) -- cycle ;
		%Shape: Donut [id:dp5980761861970574] 
		\draw   (373,300.5) .. controls (373,289.73) and (381.73,281) .. (392.5,281) .. controls (403.27,281) and (412,289.73) .. (412,300.5) .. controls (412,311.27) and (403.27,320) .. (392.5,320) .. controls (381.73,320) and (373,311.27) .. (373,300.5) .. controls (373,289.73) and (381.73,281) .. (392.5,281) .. controls (403.27,281) and (412,289.73) .. (412,300.5) .. controls (412,311.27) and (403.27,320) .. (392.5,320) .. controls (381.73,320) and (373,311.27) .. (373,300.5) -- cycle ;
		%Shape: Donut [id:dp6021213030459314] 
		\draw   (542.58,129.5) .. controls (542.58,120.7) and (549.7,113.57) .. (558.5,113.57) .. controls (567.3,113.57) and (574.42,120.7) .. (574.42,129.5) .. controls (574.42,138.3) and (567.3,145.43) .. (558.5,145.43) .. controls (549.7,145.43) and (542.58,138.3) .. (542.58,129.5)(539,129.5) .. controls (539,118.73) and (547.73,110) .. (558.5,110) .. controls (569.27,110) and (578,118.73) .. (578,129.5) .. controls (578,140.27) and (569.27,149) .. (558.5,149) .. controls (547.73,149) and (539,140.27) .. (539,129.5) ;
		%Rounded Rect [id:dp5479422428807381] 
		\draw   (28.5,290) .. controls (28.5,286.13) and (31.63,283) .. (35.5,283) -- (86.5,283) .. controls (90.37,283) and (93.5,286.13) .. (93.5,290) -- (93.5,311) .. controls (93.5,314.87) and (90.37,318) .. (86.5,318) -- (35.5,318) .. controls (31.63,318) and (28.5,314.87) .. (28.5,311) -- cycle ;
		%Rounded Rect [id:dp3774454557624156] 
		\draw   (34,292.7) .. controls (34,289.83) and (36.33,287.5) .. (39.2,287.5) -- (82.8,287.5) .. controls (85.67,287.5) and (88,289.83) .. (88,292.7) -- (88,308.3) .. controls (88,311.17) and (85.67,313.5) .. (82.8,313.5) -- (39.2,313.5) .. controls (36.33,313.5) and (34,311.17) .. (34,308.3) -- cycle ;
		%Rounded Rect [id:dp4470358926425373] 
		\draw   (28.5,377) .. controls (28.5,373.13) and (31.63,370) .. (35.5,370) -- (86.5,370) .. controls (90.37,370) and (93.5,373.13) .. (93.5,377) -- (93.5,398) .. controls (93.5,401.87) and (90.37,405) .. (86.5,405) -- (35.5,405) .. controls (31.63,405) and (28.5,401.87) .. (28.5,398) -- cycle ;
		%Rounded Rect [id:dp7607220337205098] 
		\draw   (34,379.7) .. controls (34,376.83) and (36.33,374.5) .. (39.2,374.5) -- (82.8,374.5) .. controls (85.67,374.5) and (88,376.83) .. (88,379.7) -- (88,395.3) .. controls (88,398.17) and (85.67,400.5) .. (82.8,400.5) -- (39.2,400.5) .. controls (36.33,400.5) and (34,398.17) .. (34,395.3) -- cycle ;
		%Rounded Rect [id:dp24170090672864375] 
		\draw   (206.5,120) .. controls (206.5,116.13) and (209.63,113) .. (213.5,113) -- (242.5,113) .. controls (246.37,113) and (249.5,116.13) .. (249.5,120) -- (249.5,141) .. controls (249.5,144.87) and (246.37,148) .. (242.5,148) -- (213.5,148) .. controls (209.63,148) and (206.5,144.87) .. (206.5,141) -- cycle ;
		%Rounded Rect [id:dp5467032194538861] 
		\draw   (206.5,291) .. controls (206.5,287.13) and (209.63,284) .. (213.5,284) -- (242.5,284) .. controls (246.37,284) and (249.5,287.13) .. (249.5,291) -- (249.5,312) .. controls (249.5,315.87) and (246.37,319) .. (242.5,319) -- (213.5,319) .. controls (209.63,319) and (206.5,315.87) .. (206.5,312) -- cycle ;
		%Rounded Rect [id:dp5756632078990858] 
		\draw   (371.5,203) .. controls (371.5,199.13) and (374.63,196) .. (378.5,196) -- (407.5,196) .. controls (411.37,196) and (414.5,199.13) .. (414.5,203) -- (414.5,224) .. controls (414.5,227.87) and (411.37,231) .. (407.5,231) -- (378.5,231) .. controls (374.63,231) and (371.5,227.87) .. (371.5,224) -- cycle ;
		%Rounded Rect [id:dp7532899651753766] 
		\draw   (537.5,203) .. controls (537.5,199.13) and (540.63,196) .. (544.5,196) -- (573.5,196) .. controls (577.37,196) and (580.5,199.13) .. (580.5,203) -- (580.5,224) .. controls (580.5,227.87) and (577.37,231) .. (573.5,231) -- (544.5,231) .. controls (540.63,231) and (537.5,227.87) .. (537.5,224) -- cycle ;
		%Straight Lines [id:da09980684324204891] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (206,301) -- (113.73,237.66) -- (81.65,215.63) ;
		\draw [shift={(80,214.5)}, rotate = 34.47] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da5142549232647189] 
		\draw    (80,214.5) -- (205.35,128.63) ;
		\draw [shift={(207,127.5)}, rotate = 145.59] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da22498346089164767] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (250,128.5) -- (371,128.5) ;
		\draw [shift={(373,128.5)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da14971399299911692] 
		\draw    (373,300.5) -- (252.5,300.5) ;
		\draw [shift={(250.5,300.5)}, rotate = 360] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da6624980123015134] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (392.5,148) -- (392.5,194) ;
		\draw [shift={(392.5,196)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da7014551100888746] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (392.5,233) -- (392.5,279) ;
		\draw [shift={(392.5,281)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da0007136111006607049] 
		\draw    (414.5,215) -- (497.5,215) -- (533.5,215) ;
		\draw [shift={(535.5,215)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da16717738760217293] 
		\draw    (539,129.5) -- (416.5,129.5) ;
		\draw [shift={(414.5,129.5)}, rotate = 360] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da11737895459057657] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (557.5,196) -- (557.5,151) ;
		\draw [shift={(557.5,149)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Shape: Arc [id:dp32219610026373724] 
		\draw  [draw opacity=0][dash pattern={on 4.5pt off 4.5pt}] (544.86,116.14) .. controls (541.24,112.52) and (539,107.52) .. (539,102) .. controls (539,90.95) and (547.95,82) .. (559,82) .. controls (570.05,82) and (579,90.95) .. (579,102) .. controls (579,106.83) and (577.29,111.26) .. (574.44,114.71) -- (559,102) -- cycle ; \draw [dash pattern={on 4.5pt off 4.5pt}] [dash pattern={on 4.5pt off 4.5pt}]  (544.86,116.14) .. controls (541.24,112.52) and (539,107.52) .. (539,102) .. controls (539,90.95) and (547.95,82) .. (559,82) .. controls (570.05,82) and (579,90.95) .. (579,102) .. controls (579,106.13) and (577.75,109.96) .. (575.61,113.15) ; \draw [shift={(574.44,114.71)}, rotate = 292.85] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][dash pattern={on 4.5pt off 4.5pt}][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ; 
		
		% Text Node
		\draw (51.5,123) node [anchor=north west][inner sep=0.75pt]    {$bb$};
		% Text Node
		\draw (55.5,210) node [anchor=north west][inner sep=0.75pt]    {$a$};
		% Text Node
		\draw (384,119.4) node [anchor=north west][inner sep=0.75pt]    {$ba$};
		% Text Node
		\draw (385,292.4) node [anchor=north west][inner sep=0.75pt]    {$ab$};
		% Text Node
		\draw (553,121.4) node [anchor=north west][inner sep=0.75pt]    {$b$};
		% Text Node
		\draw (41.2,292.4) node [anchor=north west][inner sep=0.75pt]    {$abbba$};
		% Text Node
		\draw (41.2,380) node [anchor=north west][inner sep=0.75pt]    {$babab$};
		% Text Node
		\draw (112.5,265) node [anchor=north west][inner sep=0.75pt]    {$abbba$};
		% Text Node
		\draw (120,155) node [anchor=north west][inner sep=0.75pt]    {$bb$};
		% Text Node
		\draw (216.5,121.4) node [anchor=north west][inner sep=0.75pt]    {$abb$};
		% Text Node
		\draw (212.5,293.4) node [anchor=north west][inner sep=0.75pt]    {$abbb$};
		% Text Node
		\draw (287.5,104.4) node [anchor=north west][inner sep=0.75pt]    {$abbba$};
		% Text Node
		\draw (305,278.4) node [anchor=north west][inner sep=0.75pt]    {$bb$};
		% Text Node
		\draw (381,204.9) node [anchor=north west][inner sep=0.75pt]    {$bab$};
		% Text Node
		\draw (404.5,242.4) node [anchor=north west][inner sep=0.75pt]    {$babab$};
		% Text Node
		\draw (404.5,159.4) node [anchor=north west][inner sep=0.75pt]    {$babab$};
		% Text Node
		\draw (562.5,167.4) node [anchor=north west][inner sep=0.75pt]    {$babab$};
		% Text Node
		\draw (478.75,132.9) node [anchor=north west][inner sep=0.75pt]    {$a$};
		% Text Node
		\draw (478.75,217.9) node [anchor=north west][inner sep=0.75pt]    {$a$};
		% Text Node
		\draw (551,58.4) node [anchor=north west][inner sep=0.75pt]    {$bb$};
		% Text Node
		\draw (542.5,204.9) node [anchor=north west][inner sep=0.75pt]    {$baba$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Sardinas and Patterson example graph}
	\end{figure}
	where the vertices corresponding to the words of $X$ are doubly circled. The label of each arc is a word of the set $X$. The "crossed arcs" are traced in dotted lines and the "front arcs" in solid lines. If the arc $(u,v)$ is crossed, then the label is $uv$, otherwise it is the word $x$ such that $ux = v$.
	\end{tcolorbox}
	In our example above, there is a path from $a$ to $a$. By virtue of the Sardinas and Patterson theorem (which we will proved in the next update of this section), the set $X$ is not a code (a single and unique path between any two vertices of $X$ is enough).
	
	The set $X$ is a code if and only if there is no non trivial path in $G(x)$ of a vertex of $X$ to a vertex of $X$ (in other words, a set $X$ is a code if and only if there is only the one and only trivial path leading from one vertex of $X$ to another one - this vertex may be the same as in the previous example).
	
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{60} & \pbox{20cm}{\score{2}{5} \\ {\tiny 10 votes,  46.00\%}} 
	\end{tabular} 
	\end{flushright}

	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Cryptography}\label{cryptography}
	\lettrine[lines=4]{\color{BrickRed}C}{ryptography} is one of the disciplines of cryptology endeavouring to protect messages (ensuring confidentiality and / or authenticity) that two people wish to share through an insecure channel often thanks to secrets or keys.\\
	
	The history of cryptography is already long and exciting (since it is a kind of "game"). We report its first use in Egypt 4,000 years ago. However, for centuries, the methods used were often remained very primitive. Moreover, its implementation was limited to the needs of the army and diplomacy. Thus, encryption methods and cryptanalysis (code breaking) experienced an important development during the Second World War and had a profound influence on the course of it.
	
	At the end (especially!) of the 120th century (holocene calendar), with the proliferation of computers and electronic communications media, it became increasingly important to use secret codes for transmitting data between the military or private organizations. Thus, engineers have had to look at this same time solid numerical methods whose implementation and use was within reach of almost everyone (nation, enterprise and individual) while ensuring that external attacks required tools out of reach of an individual or group of individuals equipped with standard and high-performance IT tools (in computing power). Engineers and researchers then plunged into the mathematical tools to search for satisfying these specifications and the most known systems, mathematical theories which were adopted had over 200 years old (apart quantum cryptography).
	
	The growth of cryptographic technology has raised a number of legal issues in the information age. Cryptography's potential for use as a tool for espionage and sedition has led many governments to classify it as a weapon and to limit or even prohibit its use and export. In some jurisdictions where the use of cryptography is legal, laws permit investigators to compel the disclosure of encryption keys for documents relevant to an investigation. Cryptography also plays a major role in digital rights management and piracy of digital media.
	
	Steganography techniques (art of concealing a message in another one or in an image) however must be preserved because nothing tells us that computing power will still be available in times of war. It should be noted also that the steganography deployed wealth of imagination. Note for example: the permutations of letters, special and subtle formatting of characters, use of synonyms, hidden messages in text or comma behind a stamp, inside shots chess (hence the fact that these games were banned by the USA for some years after the attack on Pearl Harbor), in pictures / drawings, musical scores, etc. All of these techniques make that during the Second World War, the office of censorship in the United States occupied 10,000 full-time employees that analysed the mail of citizens, classified ads, radio text, etc.
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} To address the foundations of the theory of cryptography, we advise the reader to have read at first and at least in diagonal the section on Number Theory, on Set Theory, on Numerical Methods (especially the subsection on computational complexity), on Numerical Systems , on Statistical Mechanics (where information theory can be found) and for the part about quantum cryptography: the section of on Quantum Computing.\\
	
	\textbf{R2.} We must remain aware that cryptography is more an engineer science than physicist science (except with quantum cryptography) and we must then not be so surprised to see some algorithms like fallen just from nowhere and adopted by industry because they just work almost well... Furthermore, it is also certain that only a few years after writing this text it will already be considered as obsolete (that is the art of engineering ... planned obsolescence).
	\end{tcolorbox}
	
	\subsection{Cryptographic systems}
	
	\textbf{Definitions (\#\thesection.\mydef):}
	
	A "\NewTerm{cryptographic system}\index{cryptographic system}" is composed of:
	\begin{enumerate}
		\item[D1.] A finite set $P$ named "\NewTerm{space of clear texts}\index{space of clear texts}".
		
		\item[D2.] A finite set $C$ named "\NewTerm{space of encrypted texts}\index{space of encrypted texts}".
		
		\item[D3.] A finite set $K$ named  the "\NewTerm{space of keys}\index{space of keys}".
	\end{enumerate}
	For each key $k$, we seek an encryption function $e_k$:
	
	and a deciphering (decryption) function $d_k$:
	
	such as (\SeeChapter{see section Set Theory page \pageref{identity application}}):
	
	In other words, these two functions must be injective!
	
	To achieve this, two types of cryptographic techniques are mainly distinguished, encompassing almost all known modern encryption methods of the 120th century (for mathematical details see below):
	\begin{enumerate}
		\item The first concern cryptosystems with "\NewTerm{symmetrical secret key}\index{symmetrical secret key}".
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		Public keys often refer to the DES protocol (see below) for: Data Encryption System.
		\end{tcolorbox}
		
		\item The second concerning encryption systems with "\NewTerm{asymmetric public key}\index{asymmetric public key}".
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		This type of key often refers for example to the RSA protocol, the names of those to whom we awarded the Development: Rivest, Shamir and Adleman. They are widely used thanks to their rapid time encryption and decryption as well as their high entropy (see definition below).
		\end{tcolorbox}
	\end{enumerate}
	By nature, these two types of keys are very different. Let us try to understand the reasons:
		
		A symmetric encryption means a system where the key used in the encryption operation is that used in the deciphering operation. In this case, during a secure exchange (assumed to be as), both sides of the correspondence must share a same secret: the used key or "\NewTerm{session key}\index{session key}".
		
		An asymmetric encryption designates an encryption system in which the key used for encryption (private key of the sender) differs from that used for decryption (recipient's private key). The only exchange that exists between members of the group is the public key, which allows each member to adjust its encryption based on the private key of the other members (among the many who have asymmetric systems been proposed, one of the most widespread in the early 121st century is the RSA).
		
		The symmetric key ciphers are traditionally classified into two groups:  "\NewTerm{stream ciphers}\index{stream ciphers}" and "\NewTerm{block ciphers}\index{block cipher}".
		
		\textbf{Definitions (\#\thesection.\mydef):}
		\begin{enumerate}
			\item[D1.] A "\NewTerm{block cipher}\index{block cipher}" is an encryption algorithm that encrypts a fixed size of $n$-bits of data - known as a block - at one time. The usual sizes of each block are $64$ bits, $128$ bits, and $256$ bits. So for example, a $64$-bit block cipher will take in $64$ bits of plaintext and encrypt it into $64$ bits of ciphertext. In cases where bits of plaintext is shorter than the block size, padding schemes are called into play. Majority of the symmetric ciphers used today are actually block ciphers. DES, Triple DES, AES, IDEA, and Blowfish are some of the commonly used encryption algorithms that fall under this group.  
	
				\item[D2.] A "\NewTerm{stream cipher}\index{stream cipher}" is an encryption algorithm that encrypts $1$ bit or byte of plaintext at a time. It uses an infinite stream of pseudorandom bits as the key. For a stream cipher implementation to remain secure, its pseudorandom generator should be unpredictable and the key should never be reused. Stream ciphers are designed to approximate an idealized cipher, known as the One-Time Pad. RC4, which stands for Rivest Cipher 4, is the most widely used of all stream ciphers, particularly in software. The cypher Engima machine of the second World Was is also a famous application of stream cipher.
		\end{enumerate}
		
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Without going in the mechanical and electrical description of Enignma, the reader has just to know that the second version of the Enigma\label{enigma} cypher machine had first a box $5$ rotors with $26$ start positions! The user had to choose $3$ of theses $5$ rotors and put them in a given position (position order has an importance!) in the machine.\\

	So we have:
	
	combinations to put $3$ rotors in a given order choosing among $5$ rotors ($5$ rotors for the first  position, multiplied the $4$ remaining rotors for the second position and so on...).\\
	
	After the Enigma user had to choose among on of the $26$ position of each rotor. Then the number of starting positions possibilities is equal to:
	
	Finally the business and military version of the Enigma machine had something extra: plugboard.\\
	
	This plug-board has $10$ wires that connect two letter together among 26 letters (there $2\cdot 20$ letters combine together). The combination is therefore:
	
	Indeed, we have $26!$ combinations of letters, but as there are $10$ cables for therefore $20$ letters we don't care about the combinations of $6$ of them. Hence the division by $6!$. We divide by $10!$ as for all the remaining combination of letters we can use only $10!$ of them. Since we don't care about the direction of the cables (from $A$ to $B$ or $B$ to $C$) and that we have $10$ cables, we must divide $10$ times by $2$ and this is equivalent as dividing by $2^{10}$.\\
	
	So finally the total is:
	
	This is the total number of ways you can set the Enigma machine...
	\end{tcolorbox}
		
		\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		\textbf{R1.} In 12001 (holocene calendar), Microsoft Internet Explorer (Microsoft's web browser in this time) worked with a 1024-bit asynchronous system certified by a synchronous system and Adobe Acrobat (PDF) in 12004 (holocene calendar) with an AES (Advanced Encryption System) of 128 bits for the low protection as well in the years 12010-12015 (holocene calendar) the iPhone 4S and 5.\\
		
		\textbf{R2.} Microsoft Windows Enterprise and its E.F.S. system (Encrypting File System) uses a symmetric key (to encrypt the file) named  "File Encryption Key" and asymmetric cryptography to encrypt the symmetric key in the file header as shown below (the key being updated regularly via Windows Update root certificates):
		\begin{figure}[H]
			\centering
			\resizebox{\textwidth}{15cm}{\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
			%uncomment if require: \path (0,970); %set diagram left start at 0, and has height of 970
			
			% Gradient Info
			  
			\tikzset {_ni3g1evx6/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{0 }  \pgftransformscale{2 }  }}}
			\pgfdeclarehorizontalshading{_6iwqxdond}{150bp}{rgb(0bp)=(0.49,0.83,0.13);
			rgb(37.5bp)=(0.49,0.83,0.13);
			rgb(62.5bp)=(1,1,1);
			rgb(100bp)=(1,1,1)}
			
			% Gradient Info
			  
			\tikzset {_yqppptr5a/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{0 }  \pgftransformscale{2 }  }}}
			\pgfdeclarehorizontalshading{_yayb4f0jw}{150bp}{rgb(0bp)=(1,0.01,0.13);
			rgb(37.5bp)=(1,0.01,0.13);
			rgb(62.5bp)=(0.9,0.96,1);
			rgb(100bp)=(0.9,0.96,1)}
			
			% Gradient Info
			  
			\tikzset {_cfs7l0rd4/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{0 }  \pgftransformscale{2 }  }}}
			\pgfdeclarehorizontalshading{_30trd8skn}{150bp}{rgb(0bp)=(1,0.01,0.13);
			rgb(37.5bp)=(1,0.01,0.13);
			rgb(62.5bp)=(0.9,0.96,1);
			rgb(100bp)=(0.9,0.96,1)}
			
			% Gradient Info
			  
			\tikzset {_e3fcxwstm/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{0 }  \pgftransformscale{2 }  }}}
			\pgfdeclarehorizontalshading{_6kg1a9nie}{150bp}{rgb(0bp)=(0.98,0.76,0.2);
			rgb(37.5bp)=(0.98,0.76,0.2);
			rgb(62.5bp)=(0.98,0.98,0.09);
			rgb(100bp)=(0.98,0.98,0.09)}
			
			% Gradient Info
			  
			\tikzset {_lz7ilvb0x/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{0 }  \pgftransformscale{2 }  }}}
			\pgfdeclarehorizontalshading{_a9ns0ncfh}{150bp}{rgb(0bp)=(1,0.01,0.13);
			rgb(37.5bp)=(1,0.01,0.13);
			rgb(62.5bp)=(0.9,0.96,1);
			rgb(100bp)=(0.9,0.96,1)}
			
			% Gradient Info
			  
			\tikzset {_tyttowu20/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{0 }  \pgftransformscale{2 }  }}}
			\pgfdeclarehorizontalshading{_p5d5pwacw}{150bp}{rgb(0bp)=(0.49,0.83,0.13);
			rgb(37.5bp)=(0.49,0.83,0.13);
			rgb(62.5bp)=(1,1,1);
			rgb(100bp)=(1,1,1)}
			
			% Gradient Info
			  
			\tikzset {_po2eaq0ym/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{-8.5 bp } { 0 bp }  }  \pgftransformrotate{0 }  \pgftransformscale{2 }  }}}
			\pgfdeclarehorizontalshading{_98uwlpvm7}{150bp}{rgb(0bp)=(0.6,0,0.08);
			rgb(37.5bp)=(0.6,0,0.08);
			rgb(62.5bp)=(0.35,0.63,0);
			rgb(100bp)=(0.35,0.63,0)}
			
			% Gradient Info
			  
			\tikzset {_lnftl1tw9/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{0 }  \pgftransformscale{2 }  }}}
			\pgfdeclarehorizontalshading{_bd09ee0c8}{150bp}{rgb(0bp)=(0.49,0.83,0.13);
			rgb(37.5bp)=(0.49,0.83,0.13);
			rgb(62.5bp)=(0.97,0.91,0.11);
			rgb(100bp)=(0.97,0.91,0.11)}
			
			% Gradient Info
			  
			\tikzset {_q016pqicr/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{-8.5 bp } { 0 bp }  }  \pgftransformrotate{0 }  \pgftransformscale{2 }  }}}
			\pgfdeclarehorizontalshading{_hezbsifeu}{150bp}{rgb(0bp)=(0.6,0,0.08);
			rgb(37.5bp)=(0.6,0,0.08);
			rgb(62.5bp)=(0.35,0.63,0);
			rgb(100bp)=(0.35,0.63,0)}
			
			% Gradient Info
			  
			\tikzset {_9888uvihu/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{0 }  \pgftransformscale{2 }  }}}
			\pgfdeclarehorizontalshading{_d8muff2ya}{150bp}{rgb(0bp)=(0.49,0.83,0.13);
			rgb(37.5bp)=(0.49,0.83,0.13);
			rgb(62.5bp)=(0.97,0.91,0.11);
			rgb(100bp)=(0.97,0.91,0.11)}
			
			% Gradient Info
			  
			\tikzset {_r0hga75zb/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{0 }  \pgftransformscale{2 }  }}}
			\pgfdeclarehorizontalshading{_bofp2mhs0}{150bp}{rgb(0bp)=(1,0.01,0.13);
			rgb(37.5bp)=(1,0.01,0.13);
			rgb(62.5bp)=(0.9,0.96,1);
			rgb(100bp)=(0.9,0.96,1)}
			
			% Gradient Info
			  
			\tikzset {_jyvw1dyr9/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{-8.5 bp } { 0 bp }  }  \pgftransformrotate{0 }  \pgftransformscale{2 }  }}}
			\pgfdeclarehorizontalshading{_elqitw2j4}{150bp}{rgb(0bp)=(0.6,0,0.08);
			rgb(37.5bp)=(0.6,0,0.08);
			rgb(62.5bp)=(0.35,0.63,0);
			rgb(100bp)=(0.35,0.63,0)}
			
			% Gradient Info
			  
			\tikzset {_bb6hgvy86/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{0 }  \pgftransformscale{2 }  }}}
			\pgfdeclarehorizontalshading{_ne5x8rl2s}{150bp}{rgb(0bp)=(0.49,0.83,0.13);
			rgb(37.5bp)=(0.49,0.83,0.13);
			rgb(62.5bp)=(0.97,0.91,0.11);
			rgb(100bp)=(0.97,0.91,0.11)}
			
			% Gradient Info
			  
			\tikzset {_zki958xic/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{0 }  \pgftransformscale{2 }  }}}
			\pgfdeclarehorizontalshading{_8jy93bwvn}{150bp}{rgb(0bp)=(0.03,0.43,0.87);
			rgb(37.5bp)=(0.03,0.43,0.87);
			rgb(62.5bp)=(0.9,0.96,1);
			rgb(100bp)=(0.9,0.96,1)}
			
			% Gradient Info
			  
			\tikzset {_nfhy1av5w/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{0 }  \pgftransformscale{2 }  }}}
			\pgfdeclarehorizontalshading{_lbefbadh8}{150bp}{rgb(0bp)=(0.03,0.43,0.87);
			rgb(37.5bp)=(0.03,0.43,0.87);
			rgb(62.5bp)=(0.9,0.96,1);
			rgb(100bp)=(0.9,0.96,1)}
			
			% Gradient Info
			  
			\tikzset {_tdl09y8d3/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{-8.5 bp } { 0 bp }  }  \pgftransformrotate{0 }  \pgftransformscale{2 }  }}}
			\pgfdeclarehorizontalshading{_nxhi8srhm}{150bp}{rgb(0bp)=(0.6,0,0.08);
			rgb(37.5bp)=(0.6,0,0.08);
			rgb(62.5bp)=(0.35,0.63,0);
			rgb(100bp)=(0.35,0.63,0)}
			
			% Gradient Info
			  
			\tikzset {_gb84azk5z/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{0 }  \pgftransformscale{2 }  }}}
			\pgfdeclarehorizontalshading{_vyqct2yxf}{150bp}{rgb(0bp)=(0.49,0.83,0.13);
			rgb(37.5bp)=(0.49,0.83,0.13);
			rgb(62.5bp)=(0.97,0.91,0.11);
			rgb(100bp)=(0.97,0.91,0.11)}
			
			% Gradient Info
			  
			\tikzset {_1krdkiodq/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{0 }  \pgftransformscale{2 }  }}}
			\pgfdeclarehorizontalshading{_zd9ra24th}{150bp}{rgb(0bp)=(0.03,0.43,0.87);
			rgb(37.5bp)=(0.03,0.43,0.87);
			rgb(62.5bp)=(0.9,0.96,1);
			rgb(100bp)=(0.9,0.96,1)}
			
			% Gradient Info
			  
			\tikzset {_sq6udkphg/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{0 }  \pgftransformscale{2 }  }}}
			\pgfdeclarehorizontalshading{_zyzl7h7ht}{150bp}{rgb(0bp)=(0.12,0.45,0.84);
			rgb(37.5bp)=(0.12,0.45,0.84);
			rgb(62.5bp)=(1,1,1);
			rgb(100bp)=(1,1,1)}
			
			% Gradient Info
			  
			\tikzset {_zg1fy50we/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{0 }  \pgftransformscale{2 }  }}}
			\pgfdeclarehorizontalshading{_lvsslu7po}{150bp}{rgb(0bp)=(0.49,0.83,0.13);
			rgb(37.5bp)=(0.49,0.83,0.13);
			rgb(62.5bp)=(1,1,1);
			rgb(100bp)=(1,1,1)}
			
			% Gradient Info
			  
			\tikzset {_vnfflhw8o/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{0 }  \pgftransformscale{2 }  }}}
			\pgfdeclarehorizontalshading{_92qa1qrt2}{150bp}{rgb(0bp)=(0.03,0.43,0.87);
			rgb(37.5bp)=(0.03,0.43,0.87);
			rgb(62.5bp)=(0.9,0.96,1);
			rgb(100bp)=(0.9,0.96,1)}
			
			% Gradient Info
			  
			\tikzset {_v4755zuoo/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{0 }  \pgftransformscale{2 }  }}}
			\pgfdeclarehorizontalshading{_vbv5f4ca0}{150bp}{rgb(0bp)=(0.49,0.83,0.13);
			rgb(37.5bp)=(0.49,0.83,0.13);
			rgb(62.5bp)=(1,1,1);
			rgb(100bp)=(1,1,1)}
			
			% Gradient Info
			  
			\tikzset {_h90ufajx6/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{0 }  \pgftransformscale{2 }  }}}
			\pgfdeclarehorizontalshading{_tdhg8xud7}{150bp}{rgb(0bp)=(0.03,0.43,0.87);
			rgb(37.5bp)=(0.03,0.43,0.87);
			rgb(62.5bp)=(0.9,0.96,1);
			rgb(100bp)=(0.9,0.96,1)}
			
			%Shape: Rectangle [id:dp8266271141896586] 
			\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 173; green, 214; blue, 255 }  ,fill opacity=1 ] (38,77) -- (131.5,77) -- (131.5,197) -- (38,197) -- cycle ;
			%Straight Lines [id:da3248446670706733] 
			\draw    (45,123) -- (124.5,123) ;
			%Straight Lines [id:da22972570825819738] 
			\draw    (45,130) -- (124.5,130) ;
			%Straight Lines [id:da7902771163110134] 
			\draw    (45,137) -- (124.5,137) ;
			%Straight Lines [id:da9842421797419827] 
			\draw    (45,145) -- (124.5,145) ;
			%Straight Lines [id:da8892665402111777] 
			\draw    (45,155) -- (124.5,155) ;
			%Straight Lines [id:da1836189386743139] 
			\draw    (45,163) -- (124.5,163) ;
			%Straight Lines [id:da866160384737275] 
			\draw    (45,172) -- (124.5,172) ;
			%Straight Lines [id:da15973248752724567] 
			\draw    (45,179) -- (124.5,179) ;
			%Straight Lines [id:da9538603520668114] 
			\draw    (45,189) -- (124.5,189) ;
			%Shape: Path Data [id:dp28754981064877483] 
			\path  [shading=_6iwqxdond,_ni3g1evx6] (188.12,128.33) .. controls (195.81,128.33) and (202.32,133.24) .. (204.51,140) .. controls (212.37,139.97) and (261.03,139.35) .. (264.04,139.67) .. controls (267.25,140) and (265.35,140.75) .. (267.5,142) .. controls (269.65,143.25) and (272.75,141.5) .. (271.75,143.5) .. controls (270.75,145.5) and (265.5,148.75) .. (264.75,149.5) .. controls (264,150.25) and (264.67,149.65) .. (260.25,150) .. controls (255.83,150.35) and (258.33,148.93) .. (256.5,148.75) .. controls (254.67,148.57) and (254.79,147.53) .. (252.5,147.25) .. controls (250.21,146.97) and (252.34,149.24) .. (250.25,149) .. controls (248.16,148.76) and (250.54,150.63) .. (248,150.25) .. controls (245.46,149.87) and (246.39,151.14) .. (244.25,150.75) .. controls (242.11,150.36) and (243.18,147.91) .. (241.25,147.5) .. controls (239.32,147.09) and (241.09,146.94) .. (238.5,146.5) .. controls (235.91,146.06) and (235.72,148.61) .. (233.5,148.25) .. controls (231.28,147.89) and (231.49,147.36) .. (229,146.75) .. controls (226.51,146.14) and (227.38,148.43) .. (224.5,147.75) .. controls (221.62,147.07) and (222.04,151.83) .. (219.5,151) .. controls (217.29,150.28) and (208.54,151) .. (204.32,150.55) .. controls (201.96,157.02) and (195.6,161.67) .. (188.12,161.67) .. controls (178.64,161.67) and (170.96,154.2) .. (170.96,145) .. controls (170.96,135.8) and (178.64,128.33) .. (188.12,128.33) -- cycle (176,145.5) .. controls (176,148.54) and (178.46,151) .. (181.5,151) .. controls (184.54,151) and (187,148.54) .. (187,145.5) .. controls (187,142.46) and (184.54,140) .. (181.5,140) .. controls (178.46,140) and (176,142.46) .. (176,145.5) -- cycle ; % for fading 
			 \draw   (188.12,128.33) .. controls (195.81,128.33) and (202.32,133.24) .. (204.51,140) .. controls (212.37,139.97) and (261.03,139.35) .. (264.04,139.67) .. controls (267.25,140) and (265.35,140.75) .. (267.5,142) .. controls (269.65,143.25) and (272.75,141.5) .. (271.75,143.5) .. controls (270.75,145.5) and (265.5,148.75) .. (264.75,149.5) .. controls (264,150.25) and (264.67,149.65) .. (260.25,150) .. controls (255.83,150.35) and (258.33,148.93) .. (256.5,148.75) .. controls (254.67,148.57) and (254.79,147.53) .. (252.5,147.25) .. controls (250.21,146.97) and (252.34,149.24) .. (250.25,149) .. controls (248.16,148.76) and (250.54,150.63) .. (248,150.25) .. controls (245.46,149.87) and (246.39,151.14) .. (244.25,150.75) .. controls (242.11,150.36) and (243.18,147.91) .. (241.25,147.5) .. controls (239.32,147.09) and (241.09,146.94) .. (238.5,146.5) .. controls (235.91,146.06) and (235.72,148.61) .. (233.5,148.25) .. controls (231.28,147.89) and (231.49,147.36) .. (229,146.75) .. controls (226.51,146.14) and (227.38,148.43) .. (224.5,147.75) .. controls (221.62,147.07) and (222.04,151.83) .. (219.5,151) .. controls (217.29,150.28) and (208.54,151) .. (204.32,150.55) .. controls (201.96,157.02) and (195.6,161.67) .. (188.12,161.67) .. controls (178.64,161.67) and (170.96,154.2) .. (170.96,145) .. controls (170.96,135.8) and (178.64,128.33) .. (188.12,128.33) -- cycle (176,145.5) .. controls (176,148.54) and (178.46,151) .. (181.5,151) .. controls (184.54,151) and (187,148.54) .. (187,145.5) .. controls (187,142.46) and (184.54,140) .. (181.5,140) .. controls (178.46,140) and (176,142.46) .. (176,145.5) -- cycle ; % for border 
			
			%Shape: Rectangle [id:dp392918202705006] 
			\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 250; green, 241; blue, 156 }  ,fill opacity=1 ] (327,76) -- (420.5,76) -- (420.5,196) -- (327,196) -- cycle ;
			%Straight Lines [id:da9784696310985281] 
			\draw    (334,122) -- (413.5,122) ;
			%Straight Lines [id:da055488653885701344] 
			\draw    (334,129) -- (413.5,129) ;
			%Straight Lines [id:da9894633335161365] 
			\draw    (334,136) -- (413.5,136) ;
			%Straight Lines [id:da33823819734840277] 
			\draw    (334,144) -- (413.5,144) ;
			%Straight Lines [id:da8908483161745282] 
			\draw    (334,154) -- (413.5,154) ;
			%Straight Lines [id:da015629734189955835] 
			\draw    (334,162) -- (413.5,162) ;
			%Straight Lines [id:da5590755264585081] 
			\draw    (334,171) -- (413.5,171) ;
			%Straight Lines [id:da2227259511619868] 
			\draw    (334,178) -- (413.5,178) ;
			%Straight Lines [id:da9195386606692892] 
			\draw    (334,188) -- (413.5,188) ;
			%Right Arrow [id:dp20663664867185982] 
			\path  [shading=_yayb4f0jw,_yqppptr5a] (169.95,169.45) -- (234.5,169.45) -- (234.5,160) -- (272.95,173.5) -- (234.5,187) -- (234.5,177.55) -- (169.95,177.55) -- cycle ; % for fading 
			 \draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]  (169.95,169.45) -- (234.5,169.45) -- (234.5,160) -- (272.95,173.5) -- (234.5,187) -- (234.5,177.55) -- (169.95,177.55) -- cycle ; % for border 
			
			%Right Arrow [id:dp3403078578865706] 
			\path  [shading=_30trd8skn,_cfs7l0rd4] (435.07,149.73) -- (488.02,186.64) -- (493.43,178.89) -- (517.25,211.95) -- (477.99,201.04) -- (483.39,193.29) -- (430.44,156.37) -- cycle ; % for fading 
			 \draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]  (435.07,149.73) -- (488.02,186.64) -- (493.43,178.89) -- (517.25,211.95) -- (477.99,201.04) -- (483.39,193.29) -- (430.44,156.37) -- cycle ; % for border 
			
			%Shape: Rectangle [id:dp3773389952317283] 
			\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 250; green, 241; blue, 156 }  ,fill opacity=1 ] (531,194) -- (624.5,194) -- (624.5,314) -- (531,314) -- cycle ;
			%Straight Lines [id:da8823280365004387] 
			\draw    (538,240) -- (617.5,240) ;
			%Straight Lines [id:da15128930274599428] 
			\draw    (538,247) -- (617.5,247) ;
			%Straight Lines [id:da830283174217973] 
			\draw    (538,254) -- (617.5,254) ;
			%Straight Lines [id:da3956075419871852] 
			\draw    (538,262) -- (617.5,262) ;
			%Straight Lines [id:da7058008934345705] 
			\draw    (538,272) -- (617.5,272) ;
			%Straight Lines [id:da24909125928717035] 
			\draw    (538,280) -- (617.5,280) ;
			%Straight Lines [id:da8331917962775948] 
			\draw    (538,289) -- (617.5,289) ;
			%Straight Lines [id:da3477881790616908] 
			\draw    (538,296) -- (617.5,296) ;
			%Straight Lines [id:da06855828957103549] 
			\draw    (538,306) -- (617.5,306) ;
			%Shape: Path Data [id:dp5716802361636424] 
			\path  [shading=_6kg1a9nie,_e3fcxwstm] (188.12,291.33) .. controls (195.81,291.33) and (202.32,296.24) .. (204.51,303) .. controls (212.37,302.97) and (261.03,302.35) .. (264.04,302.67) .. controls (267.25,303) and (265.35,303.75) .. (267.5,305) .. controls (269.65,306.25) and (272.75,304.5) .. (271.75,306.5) .. controls (270.75,308.5) and (265.5,311.75) .. (264.75,312.5) .. controls (264,313.25) and (264.67,312.65) .. (260.25,313) .. controls (255.83,313.35) and (258.33,311.93) .. (256.5,311.75) .. controls (254.67,311.57) and (254.79,310.53) .. (252.5,310.25) .. controls (250.21,309.97) and (252.34,312.24) .. (250.25,312) .. controls (248.16,311.76) and (250.54,313.63) .. (248,313.25) .. controls (245.46,312.87) and (246.39,314.14) .. (244.25,313.75) .. controls (242.11,313.36) and (243.18,310.91) .. (241.25,310.5) .. controls (239.32,310.09) and (241.09,309.94) .. (238.5,309.5) .. controls (235.91,309.06) and (235.72,311.61) .. (233.5,311.25) .. controls (231.28,310.89) and (231.49,310.36) .. (229,309.75) .. controls (226.51,309.14) and (227.38,311.43) .. (224.5,310.75) .. controls (221.62,310.07) and (222.04,314.83) .. (219.5,314) .. controls (217.29,313.28) and (208.54,314) .. (204.32,313.55) .. controls (201.96,320.02) and (195.6,324.67) .. (188.12,324.67) .. controls (178.64,324.67) and (170.96,317.2) .. (170.96,308) .. controls (170.96,298.8) and (178.64,291.33) .. (188.12,291.33) -- cycle (176,308.5) .. controls (176,311.54) and (178.46,314) .. (181.5,314) .. controls (184.54,314) and (187,311.54) .. (187,308.5) .. controls (187,305.46) and (184.54,303) .. (181.5,303) .. controls (178.46,303) and (176,305.46) .. (176,308.5) -- cycle ; % for fading 
			 \draw   (188.12,291.33) .. controls (195.81,291.33) and (202.32,296.24) .. (204.51,303) .. controls (212.37,302.97) and (261.03,302.35) .. (264.04,302.67) .. controls (267.25,303) and (265.35,303.75) .. (267.5,305) .. controls (269.65,306.25) and (272.75,304.5) .. (271.75,306.5) .. controls (270.75,308.5) and (265.5,311.75) .. (264.75,312.5) .. controls (264,313.25) and (264.67,312.65) .. (260.25,313) .. controls (255.83,313.35) and (258.33,311.93) .. (256.5,311.75) .. controls (254.67,311.57) and (254.79,310.53) .. (252.5,310.25) .. controls (250.21,309.97) and (252.34,312.24) .. (250.25,312) .. controls (248.16,311.76) and (250.54,313.63) .. (248,313.25) .. controls (245.46,312.87) and (246.39,314.14) .. (244.25,313.75) .. controls (242.11,313.36) and (243.18,310.91) .. (241.25,310.5) .. controls (239.32,310.09) and (241.09,309.94) .. (238.5,309.5) .. controls (235.91,309.06) and (235.72,311.61) .. (233.5,311.25) .. controls (231.28,310.89) and (231.49,310.36) .. (229,309.75) .. controls (226.51,309.14) and (227.38,311.43) .. (224.5,310.75) .. controls (221.62,310.07) and (222.04,314.83) .. (219.5,314) .. controls (217.29,313.28) and (208.54,314) .. (204.32,313.55) .. controls (201.96,320.02) and (195.6,324.67) .. (188.12,324.67) .. controls (178.64,324.67) and (170.96,317.2) .. (170.96,308) .. controls (170.96,298.8) and (178.64,291.33) .. (188.12,291.33) -- cycle (176,308.5) .. controls (176,311.54) and (178.46,314) .. (181.5,314) .. controls (184.54,314) and (187,311.54) .. (187,308.5) .. controls (187,305.46) and (184.54,303) .. (181.5,303) .. controls (178.46,303) and (176,305.46) .. (176,308.5) -- cycle ; % for border 
			
			%Straight Lines [id:da24986785919224164] 
			\draw  [dash pattern={on 4.5pt off 4.5pt}]  (39,227) -- (422.5,227) ;
			%Right Arrow [id:dp7816543108107754] 
			\path  [shading=_a9ns0ncfh,_lz7ilvb0x] (169.95,338.45) -- (234.5,338.45) -- (234.5,329) -- (272.95,342.5) -- (234.5,356) -- (234.5,346.55) -- (169.95,346.55) -- cycle ; % for fading 
			 \draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]  (169.95,338.45) -- (234.5,338.45) -- (234.5,329) -- (272.95,342.5) -- (234.5,356) -- (234.5,346.55) -- (169.95,346.55) -- cycle ; % for border 
			
			%Shape: Path Data [id:dp1036753845888927] 
			\path  [shading=_p5d5pwacw,_tyttowu20] (59.34,325.83) .. controls (67.03,325.83) and (73.54,330.74) .. (75.72,337.5) .. controls (83.59,337.47) and (132.25,336.85) .. (135.26,337.17) .. controls (138.47,337.5) and (136.57,338.25) .. (138.72,339.5) .. controls (140.86,340.75) and (143.97,339) .. (142.97,341) .. controls (141.97,343) and (136.72,346.25) .. (135.97,347) .. controls (135.22,347.75) and (135.88,347.15) .. (131.47,347.5) .. controls (127.05,347.85) and (129.54,346.43) .. (127.72,346.25) .. controls (125.89,346.07) and (126,345.03) .. (123.72,344.75) .. controls (121.43,344.47) and (123.55,346.74) .. (121.47,346.5) .. controls (119.38,346.26) and (121.75,348.13) .. (119.22,347.75) .. controls (116.68,347.37) and (117.6,348.64) .. (115.47,348.25) .. controls (113.33,347.86) and (114.39,345.41) .. (112.47,345) .. controls (110.54,344.59) and (112.31,344.44) .. (109.72,344) .. controls (107.12,343.56) and (106.94,346.11) .. (104.72,345.75) .. controls (102.49,345.39) and (102.71,344.86) .. (100.22,344.25) .. controls (97.72,343.64) and (98.6,345.93) .. (95.72,345.25) .. controls (92.83,344.57) and (93.26,349.33) .. (90.72,348.5) .. controls (88.51,347.78) and (79.76,348.5) .. (75.53,348.05) .. controls (73.18,354.52) and (66.82,359.17) .. (59.34,359.17) .. controls (49.86,359.17) and (42.17,351.7) .. (42.17,342.5) .. controls (42.17,333.3) and (49.86,325.83) .. (59.34,325.83) -- cycle (47.22,343) .. controls (47.22,346.04) and (49.68,348.5) .. (52.72,348.5) .. controls (55.75,348.5) and (58.22,346.04) .. (58.22,343) .. controls (58.22,339.96) and (55.75,337.5) .. (52.72,337.5) .. controls (49.68,337.5) and (47.22,339.96) .. (47.22,343) -- cycle ; % for fading 
			 \draw   (59.34,325.83) .. controls (67.03,325.83) and (73.54,330.74) .. (75.72,337.5) .. controls (83.59,337.47) and (132.25,336.85) .. (135.26,337.17) .. controls (138.47,337.5) and (136.57,338.25) .. (138.72,339.5) .. controls (140.86,340.75) and (143.97,339) .. (142.97,341) .. controls (141.97,343) and (136.72,346.25) .. (135.97,347) .. controls (135.22,347.75) and (135.88,347.15) .. (131.47,347.5) .. controls (127.05,347.85) and (129.54,346.43) .. (127.72,346.25) .. controls (125.89,346.07) and (126,345.03) .. (123.72,344.75) .. controls (121.43,344.47) and (123.55,346.74) .. (121.47,346.5) .. controls (119.38,346.26) and (121.75,348.13) .. (119.22,347.75) .. controls (116.68,347.37) and (117.6,348.64) .. (115.47,348.25) .. controls (113.33,347.86) and (114.39,345.41) .. (112.47,345) .. controls (110.54,344.59) and (112.31,344.44) .. (109.72,344) .. controls (107.12,343.56) and (106.94,346.11) .. (104.72,345.75) .. controls (102.49,345.39) and (102.71,344.86) .. (100.22,344.25) .. controls (97.72,343.64) and (98.6,345.93) .. (95.72,345.25) .. controls (92.83,344.57) and (93.26,349.33) .. (90.72,348.5) .. controls (88.51,347.78) and (79.76,348.5) .. (75.53,348.05) .. controls (73.18,354.52) and (66.82,359.17) .. (59.34,359.17) .. controls (49.86,359.17) and (42.17,351.7) .. (42.17,342.5) .. controls (42.17,333.3) and (49.86,325.83) .. (59.34,325.83) -- cycle (47.22,343) .. controls (47.22,346.04) and (49.68,348.5) .. (52.72,348.5) .. controls (55.75,348.5) and (58.22,346.04) .. (58.22,343) .. controls (58.22,339.96) and (55.75,337.5) .. (52.72,337.5) .. controls (49.68,337.5) and (47.22,339.96) .. (47.22,343) -- cycle ; % for border 
			
			%Shape: Rectangle [id:dp596212774374349] 
			\draw  [draw opacity=0][shading=_98uwlpvm7,_po2eaq0ym] (303,316) -- (425.5,316) -- (425.5,366) -- (303,366) -- cycle ;
			%Shape: Path Data [id:dp7199570396043229] 
			\path  [shading=_bd09ee0c8,_lnftl1tw9] (332.34,322.83) .. controls (340.03,322.83) and (346.54,327.74) .. (348.72,334.5) .. controls (356.59,334.47) and (405.25,333.85) .. (408.26,334.17) .. controls (411.47,334.5) and (409.57,335.25) .. (411.72,336.5) .. controls (413.86,337.75) and (416.97,336) .. (415.97,338) .. controls (414.97,340) and (409.72,343.25) .. (408.97,344) .. controls (408.22,344.75) and (408.88,344.15) .. (404.47,344.5) .. controls (400.05,344.85) and (402.54,343.43) .. (400.72,343.25) .. controls (398.89,343.07) and (399,342.03) .. (396.72,341.75) .. controls (394.43,341.47) and (396.55,343.74) .. (394.47,343.5) .. controls (392.38,343.26) and (394.75,345.13) .. (392.22,344.75) .. controls (389.68,344.37) and (390.6,345.64) .. (388.47,345.25) .. controls (386.33,344.86) and (387.39,342.41) .. (385.47,342) .. controls (383.54,341.59) and (385.31,341.44) .. (382.72,341) .. controls (380.12,340.56) and (379.94,343.11) .. (377.72,342.75) .. controls (375.49,342.39) and (375.71,341.86) .. (373.22,341.25) .. controls (370.72,340.64) and (371.6,342.93) .. (368.72,342.25) .. controls (365.83,341.57) and (366.26,346.33) .. (363.72,345.5) .. controls (361.51,344.78) and (352.76,345.5) .. (348.53,345.05) .. controls (346.18,351.52) and (339.82,356.17) .. (332.34,356.17) .. controls (322.86,356.17) and (315.17,348.7) .. (315.17,339.5) .. controls (315.17,330.3) and (322.86,322.83) .. (332.34,322.83) -- cycle (320.22,340) .. controls (320.22,343.04) and (322.68,345.5) .. (325.72,345.5) .. controls (328.75,345.5) and (331.22,343.04) .. (331.22,340) .. controls (331.22,336.96) and (328.75,334.5) .. (325.72,334.5) .. controls (322.68,334.5) and (320.22,336.96) .. (320.22,340) -- cycle ; % for fading 
			 \draw   (332.34,322.83) .. controls (340.03,322.83) and (346.54,327.74) .. (348.72,334.5) .. controls (356.59,334.47) and (405.25,333.85) .. (408.26,334.17) .. controls (411.47,334.5) and (409.57,335.25) .. (411.72,336.5) .. controls (413.86,337.75) and (416.97,336) .. (415.97,338) .. controls (414.97,340) and (409.72,343.25) .. (408.97,344) .. controls (408.22,344.75) and (408.88,344.15) .. (404.47,344.5) .. controls (400.05,344.85) and (402.54,343.43) .. (400.72,343.25) .. controls (398.89,343.07) and (399,342.03) .. (396.72,341.75) .. controls (394.43,341.47) and (396.55,343.74) .. (394.47,343.5) .. controls (392.38,343.26) and (394.75,345.13) .. (392.22,344.75) .. controls (389.68,344.37) and (390.6,345.64) .. (388.47,345.25) .. controls (386.33,344.86) and (387.39,342.41) .. (385.47,342) .. controls (383.54,341.59) and (385.31,341.44) .. (382.72,341) .. controls (380.12,340.56) and (379.94,343.11) .. (377.72,342.75) .. controls (375.49,342.39) and (375.71,341.86) .. (373.22,341.25) .. controls (370.72,340.64) and (371.6,342.93) .. (368.72,342.25) .. controls (365.83,341.57) and (366.26,346.33) .. (363.72,345.5) .. controls (361.51,344.78) and (352.76,345.5) .. (348.53,345.05) .. controls (346.18,351.52) and (339.82,356.17) .. (332.34,356.17) .. controls (322.86,356.17) and (315.17,348.7) .. (315.17,339.5) .. controls (315.17,330.3) and (322.86,322.83) .. (332.34,322.83) -- cycle (320.22,340) .. controls (320.22,343.04) and (322.68,345.5) .. (325.72,345.5) .. controls (328.75,345.5) and (331.22,343.04) .. (331.22,340) .. controls (331.22,336.96) and (328.75,334.5) .. (325.72,334.5) .. controls (322.68,334.5) and (320.22,336.96) .. (320.22,340) -- cycle ; % for border 
			
			%Shape: Rectangle [id:dp5126213689497465] 
			\draw  [draw opacity=0][shading=_hezbsifeu,_q016pqicr] (539.5,199) -- (617.5,199) -- (617.5,228) -- (539.5,228) -- cycle ;
			%Shape: Path Data [id:dp1415542638634255] 
			\path  [shading=_d8muff2ya,_9888uvihu] (558.34,203.24) .. controls (563.23,203.24) and (567.38,206.08) .. (568.77,210.01) .. controls (573.78,209.99) and (604.76,209.63) .. (606.68,209.81) .. controls (608.72,210.01) and (607.51,210.44) .. (608.88,211.17) .. controls (610.25,211.89) and (612.22,210.88) .. (611.59,212.04) .. controls (610.95,213.2) and (607.61,215.08) .. (607.13,215.52) .. controls (606.65,215.95) and (607.08,215.6) .. (604.26,215.81) .. controls (601.45,216.01) and (603.04,215.19) .. (601.88,215.08) .. controls (600.71,214.97) and (600.78,214.37) .. (599.33,214.21) .. controls (597.87,214.05) and (599.22,215.37) .. (597.9,215.23) .. controls (596.57,215.08) and (598.08,216.17) .. (596.46,215.95) .. controls (594.85,215.73) and (595.44,216.47) .. (594.08,216.24) .. controls (592.71,216.01) and (593.39,214.59) .. (592.17,214.36) .. controls (590.94,214.12) and (592.06,214.03) .. (590.41,213.78) .. controls (588.76,213.52) and (588.65,215) .. (587.23,214.79) .. controls (585.81,214.58) and (585.95,214.27) .. (584.37,213.92) .. controls (582.78,213.57) and (583.34,214.89) .. (581.5,214.5) .. controls (579.66,214.11) and (579.93,216.87) .. (578.32,216.39) .. controls (576.91,215.96) and (571.34,216.38) .. (568.65,216.12) .. controls (567.15,219.88) and (563.1,222.57) .. (558.34,222.57) .. controls (552.3,222.57) and (547.41,218.24) .. (547.41,212.91) .. controls (547.41,207.57) and (552.3,203.24) .. (558.34,203.24) -- cycle (550.62,213.2) .. controls (550.62,214.96) and (552.19,216.39) .. (554.12,216.39) .. controls (556.05,216.39) and (557.62,214.96) .. (557.62,213.2) .. controls (557.62,211.43) and (556.05,210.01) .. (554.12,210.01) .. controls (552.19,210.01) and (550.62,211.43) .. (550.62,213.2) -- cycle ; % for fading 
			 \draw   (558.34,203.24) .. controls (563.23,203.24) and (567.38,206.08) .. (568.77,210.01) .. controls (573.78,209.99) and (604.76,209.63) .. (606.68,209.81) .. controls (608.72,210.01) and (607.51,210.44) .. (608.88,211.17) .. controls (610.25,211.89) and (612.22,210.88) .. (611.59,212.04) .. controls (610.95,213.2) and (607.61,215.08) .. (607.13,215.52) .. controls (606.65,215.95) and (607.08,215.6) .. (604.26,215.81) .. controls (601.45,216.01) and (603.04,215.19) .. (601.88,215.08) .. controls (600.71,214.97) and (600.78,214.37) .. (599.33,214.21) .. controls (597.87,214.05) and (599.22,215.37) .. (597.9,215.23) .. controls (596.57,215.08) and (598.08,216.17) .. (596.46,215.95) .. controls (594.85,215.73) and (595.44,216.47) .. (594.08,216.24) .. controls (592.71,216.01) and (593.39,214.59) .. (592.17,214.36) .. controls (590.94,214.12) and (592.06,214.03) .. (590.41,213.78) .. controls (588.76,213.52) and (588.65,215) .. (587.23,214.79) .. controls (585.81,214.58) and (585.95,214.27) .. (584.37,213.92) .. controls (582.78,213.57) and (583.34,214.89) .. (581.5,214.5) .. controls (579.66,214.11) and (579.93,216.87) .. (578.32,216.39) .. controls (576.91,215.96) and (571.34,216.38) .. (568.65,216.12) .. controls (567.15,219.88) and (563.1,222.57) .. (558.34,222.57) .. controls (552.3,222.57) and (547.41,218.24) .. (547.41,212.91) .. controls (547.41,207.57) and (552.3,203.24) .. (558.34,203.24) -- cycle (550.62,213.2) .. controls (550.62,214.96) and (552.19,216.39) .. (554.12,216.39) .. controls (556.05,216.39) and (557.62,214.96) .. (557.62,213.2) .. controls (557.62,211.43) and (556.05,210.01) .. (554.12,210.01) .. controls (552.19,210.01) and (550.62,211.43) .. (550.62,213.2) -- cycle ; % for border 
			
			%Right Arrow [id:dp0475072332462827] 
			\path  [shading=_bofp2mhs0,_r0hga75zb] (435.07,348.27) -- (488.02,311.36) -- (493.43,319.11) -- (517.25,286.05) -- (477.99,296.96) -- (483.39,304.71) -- (430.44,341.63) -- cycle ; % for fading 
			 \draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]  (435.07,348.27) -- (488.02,311.36) -- (493.43,319.11) -- (517.25,286.05) -- (477.99,296.96) -- (483.39,304.71) -- (430.44,341.63) -- cycle ; % for border 
			
			%Straight Lines [id:da981084040774207] 
			\draw [line width=2.25]    (39,430) -- (643.5,430) ;
			%Shape: Rectangle [id:dp32036423023924576] 
			\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 250; green, 241; blue, 156 }  ,fill opacity=1 ] (38,515) -- (131.5,515) -- (131.5,635) -- (38,635) -- cycle ;
			%Straight Lines [id:da48792684806991904] 
			\draw    (45,561) -- (124.5,561) ;
			%Straight Lines [id:da4168198427051244] 
			\draw    (45,568) -- (124.5,568) ;
			%Straight Lines [id:da9855574267294602] 
			\draw    (45,575) -- (124.5,575) ;
			%Straight Lines [id:da5982918812954523] 
			\draw    (45,583) -- (124.5,583) ;
			%Straight Lines [id:da5308787685413321] 
			\draw    (45,593) -- (124.5,593) ;
			%Straight Lines [id:da1668467766002446] 
			\draw    (45,601) -- (124.5,601) ;
			%Straight Lines [id:da40706548345943716] 
			\draw    (45,610) -- (124.5,610) ;
			%Straight Lines [id:da039110694313422956] 
			\draw    (45,617) -- (124.5,617) ;
			%Straight Lines [id:da9502711973374369] 
			\draw    (45,627) -- (124.5,627) ;
			%Shape: Rectangle [id:dp895842088419649] 
			\draw  [draw opacity=0][shading=_elqitw2j4,_jyvw1dyr9] (46.5,520) -- (124.5,520) -- (124.5,549) -- (46.5,549) -- cycle ;
			%Shape: Path Data [id:dp9727855842091055] 
			\path  [shading=_ne5x8rl2s,_bb6hgvy86] (65.82,525.7) .. controls (70.71,525.7) and (74.86,528.55) .. (76.25,532.47) .. controls (81.26,532.45) and (112.24,532.1) .. (114.16,532.28) .. controls (116.2,532.47) and (114.99,532.9) .. (116.36,533.63) .. controls (117.73,534.36) and (119.7,533.34) .. (119.07,534.5) .. controls (118.43,535.66) and (115.09,537.54) .. (114.61,537.98) .. controls (114.13,538.42) and (114.56,538.07) .. (111.74,538.27) .. controls (108.93,538.47) and (110.52,537.65) .. (109.36,537.54) .. controls (108.19,537.44) and (108.27,536.84) .. (106.81,536.67) .. controls (105.35,536.51) and (106.7,537.83) .. (105.38,537.69) .. controls (104.05,537.55) and (105.56,538.64) .. (103.94,538.42) .. controls (102.33,538.19) and (102.92,538.93) .. (101.56,538.71) .. controls (100.19,538.48) and (100.87,537.06) .. (99.65,536.82) .. controls (98.42,536.58) and (99.55,536.49) .. (97.89,536.24) .. controls (96.24,535.99) and (96.13,537.46) .. (94.71,537.26) .. controls (93.29,537.05) and (93.43,536.74) .. (91.85,536.39) .. controls (90.26,536.03) and (90.82,537.36) .. (88.98,536.97) .. controls (87.14,536.57) and (87.41,539.33) .. (85.8,538.85) .. controls (84.39,538.43) and (78.82,538.85) .. (76.13,538.59) .. controls (74.63,542.34) and (70.58,545.04) .. (65.82,545.04) .. controls (59.78,545.04) and (54.89,540.71) .. (54.89,535.37) .. controls (54.89,530.03) and (59.78,525.7) .. (65.82,525.7) -- cycle (58.1,535.66) .. controls (58.1,537.42) and (59.67,538.85) .. (61.6,538.85) .. controls (63.53,538.85) and (65.1,537.42) .. (65.1,535.66) .. controls (65.1,533.9) and (63.53,532.47) .. (61.6,532.47) .. controls (59.67,532.47) and (58.1,533.9) .. (58.1,535.66) -- cycle ; % for fading 
			 \draw   (65.82,525.7) .. controls (70.71,525.7) and (74.86,528.55) .. (76.25,532.47) .. controls (81.26,532.45) and (112.24,532.1) .. (114.16,532.28) .. controls (116.2,532.47) and (114.99,532.9) .. (116.36,533.63) .. controls (117.73,534.36) and (119.7,533.34) .. (119.07,534.5) .. controls (118.43,535.66) and (115.09,537.54) .. (114.61,537.98) .. controls (114.13,538.42) and (114.56,538.07) .. (111.74,538.27) .. controls (108.93,538.47) and (110.52,537.65) .. (109.36,537.54) .. controls (108.19,537.44) and (108.27,536.84) .. (106.81,536.67) .. controls (105.35,536.51) and (106.7,537.83) .. (105.38,537.69) .. controls (104.05,537.55) and (105.56,538.64) .. (103.94,538.42) .. controls (102.33,538.19) and (102.92,538.93) .. (101.56,538.71) .. controls (100.19,538.48) and (100.87,537.06) .. (99.65,536.82) .. controls (98.42,536.58) and (99.55,536.49) .. (97.89,536.24) .. controls (96.24,535.99) and (96.13,537.46) .. (94.71,537.26) .. controls (93.29,537.05) and (93.43,536.74) .. (91.85,536.39) .. controls (90.26,536.03) and (90.82,537.36) .. (88.98,536.97) .. controls (87.14,536.57) and (87.41,539.33) .. (85.8,538.85) .. controls (84.39,538.43) and (78.82,538.85) .. (76.13,538.59) .. controls (74.63,542.34) and (70.58,545.04) .. (65.82,545.04) .. controls (59.78,545.04) and (54.89,540.71) .. (54.89,535.37) .. controls (54.89,530.03) and (59.78,525.7) .. (65.82,525.7) -- cycle (58.1,535.66) .. controls (58.1,537.42) and (59.67,538.85) .. (61.6,538.85) .. controls (63.53,538.85) and (65.1,537.42) .. (65.1,535.66) .. controls (65.1,533.9) and (63.53,532.47) .. (61.6,532.47) .. controls (59.67,532.47) and (58.1,533.9) .. (58.1,535.66) -- cycle ; % for border 
			
			%Right Arrow [id:dp5297482475592032] 
			\path  [shading=_8jy93bwvn,_zki958xic] (144.17,556.95) -- (208.72,556.95) -- (208.72,547.5) -- (247.17,561) -- (208.72,574.5) -- (208.72,565.05) -- (144.17,565.05) -- cycle ; % for fading 
			 \draw  [color={rgb, 255:red, 0; green, 87; blue, 190 }  ,draw opacity=1 ][line width=1.5]  (144.17,556.95) -- (208.72,556.95) -- (208.72,547.5) -- (247.17,561) -- (208.72,574.5) -- (208.72,565.05) -- (144.17,565.05) -- cycle ; % for border 
			
			%Right Arrow [id:dp6231320923574739] 
			\path  [shading=_lbefbadh8,_nfhy1av5w] (168.17,667.45) -- (221.12,704.37) -- (226.53,696.61) -- (250.35,729.67) -- (211.09,718.76) -- (216.49,711.01) -- (163.53,674.09) -- cycle ; % for fading 
			 \draw  [color={rgb, 255:red, 0; green, 87; blue, 190 }  ,draw opacity=1 ][line width=1.5]  (168.17,667.45) -- (221.12,704.37) -- (226.53,696.61) -- (250.35,729.67) -- (211.09,718.76) -- (216.49,711.01) -- (163.53,674.09) -- cycle ; % for border 
			
			%Shape: Rectangle [id:dp5764877515582407] 
			\draw  [draw opacity=0][shading=_nxhi8srhm,_tdl09y8d3] (258,536) -- (380.5,536) -- (380.5,586) -- (258,586) -- cycle ;
			%Shape: Path Data [id:dp9200065347905251] 
			\path  [shading=_vyqct2yxf,_gb84azk5z] (285.92,544.33) .. controls (293.61,544.33) and (300.12,549.24) .. (302.31,556) .. controls (310.17,555.97) and (358.83,555.35) .. (361.84,555.67) .. controls (365.05,556) and (363.15,556.75) .. (365.3,558) .. controls (367.45,559.25) and (370.55,557.5) .. (369.55,559.5) .. controls (368.55,561.5) and (363.3,564.75) .. (362.55,565.5) .. controls (361.8,566.25) and (362.47,565.65) .. (358.05,566) .. controls (353.63,566.35) and (356.13,564.93) .. (354.3,564.75) .. controls (352.47,564.57) and (352.59,563.53) .. (350.3,563.25) .. controls (348.01,562.97) and (350.13,565.24) .. (348.05,565) .. controls (345.96,564.76) and (348.34,566.63) .. (345.8,566.25) .. controls (343.26,565.87) and (344.19,567.14) .. (342.05,566.75) .. controls (339.91,566.36) and (340.98,563.91) .. (339.05,563.5) .. controls (337.12,563.09) and (338.89,562.94) .. (336.3,562.5) .. controls (333.71,562.06) and (333.52,564.61) .. (331.3,564.25) .. controls (329.07,563.89) and (329.29,563.36) .. (326.8,562.75) .. controls (324.31,562.14) and (325.18,564.43) .. (322.3,563.75) .. controls (319.42,563.07) and (319.84,567.83) .. (317.3,567) .. controls (315.09,566.28) and (306.34,567) .. (302.12,566.55) .. controls (299.76,573.02) and (293.4,577.67) .. (285.92,577.67) .. controls (276.44,577.67) and (268.76,570.2) .. (268.76,561) .. controls (268.76,551.8) and (276.44,544.33) .. (285.92,544.33) -- cycle (273.8,561.5) .. controls (273.8,564.54) and (276.26,567) .. (279.3,567) .. controls (282.34,567) and (284.8,564.54) .. (284.8,561.5) .. controls (284.8,558.46) and (282.34,556) .. (279.3,556) .. controls (276.26,556) and (273.8,558.46) .. (273.8,561.5) -- cycle ; % for fading 
			 \draw   (285.92,544.33) .. controls (293.61,544.33) and (300.12,549.24) .. (302.31,556) .. controls (310.17,555.97) and (358.83,555.35) .. (361.84,555.67) .. controls (365.05,556) and (363.15,556.75) .. (365.3,558) .. controls (367.45,559.25) and (370.55,557.5) .. (369.55,559.5) .. controls (368.55,561.5) and (363.3,564.75) .. (362.55,565.5) .. controls (361.8,566.25) and (362.47,565.65) .. (358.05,566) .. controls (353.63,566.35) and (356.13,564.93) .. (354.3,564.75) .. controls (352.47,564.57) and (352.59,563.53) .. (350.3,563.25) .. controls (348.01,562.97) and (350.13,565.24) .. (348.05,565) .. controls (345.96,564.76) and (348.34,566.63) .. (345.8,566.25) .. controls (343.26,565.87) and (344.19,567.14) .. (342.05,566.75) .. controls (339.91,566.36) and (340.98,563.91) .. (339.05,563.5) .. controls (337.12,563.09) and (338.89,562.94) .. (336.3,562.5) .. controls (333.71,562.06) and (333.52,564.61) .. (331.3,564.25) .. controls (329.07,563.89) and (329.29,563.36) .. (326.8,562.75) .. controls (324.31,562.14) and (325.18,564.43) .. (322.3,563.75) .. controls (319.42,563.07) and (319.84,567.83) .. (317.3,567) .. controls (315.09,566.28) and (306.34,567) .. (302.12,566.55) .. controls (299.76,573.02) and (293.4,577.67) .. (285.92,577.67) .. controls (276.44,577.67) and (268.76,570.2) .. (268.76,561) .. controls (268.76,551.8) and (276.44,544.33) .. (285.92,544.33) -- cycle (273.8,561.5) .. controls (273.8,564.54) and (276.26,567) .. (279.3,567) .. controls (282.34,567) and (284.8,564.54) .. (284.8,561.5) .. controls (284.8,558.46) and (282.34,556) .. (279.3,556) .. controls (276.26,556) and (273.8,558.46) .. (273.8,561.5) -- cycle ; % for border 
			
			%Right Arrow [id:dp5849319359198968] 
			\path  [shading=_zd9ra24th,_1krdkiodq] (399.17,556.95) -- (463.72,556.95) -- (463.72,547.5) -- (502.17,561) -- (463.72,574.5) -- (463.72,565.05) -- (399.17,565.05) -- cycle ; % for fading 
			 \draw  [color={rgb, 255:red, 0; green, 87; blue, 190 }  ,draw opacity=1 ][line width=1.5]  (399.17,556.95) -- (463.72,556.95) -- (463.72,547.5) -- (502.17,561) -- (463.72,574.5) -- (463.72,565.05) -- (399.17,565.05) -- cycle ; % for border 
			
			%Shape: Path Data [id:dp0897487183089194] 
			\path  [shading=_zyzl7h7ht,_sq6udkphg] (411.34,506.83) .. controls (419.03,506.83) and (425.54,511.74) .. (427.72,518.5) .. controls (435.59,518.47) and (484.25,517.85) .. (487.26,518.17) .. controls (490.47,518.5) and (488.57,519.25) .. (490.72,520.5) .. controls (492.86,521.75) and (495.97,520) .. (494.97,522) .. controls (493.97,524) and (488.72,527.25) .. (487.97,528) .. controls (487.22,528.75) and (487.88,528.15) .. (483.47,528.5) .. controls (479.05,528.85) and (481.54,527.43) .. (479.72,527.25) .. controls (477.89,527.07) and (478,526.03) .. (475.72,525.75) .. controls (473.43,525.47) and (475.55,527.74) .. (473.47,527.5) .. controls (471.38,527.26) and (473.75,529.13) .. (471.22,528.75) .. controls (468.68,528.37) and (469.6,529.64) .. (467.47,529.25) .. controls (465.33,528.86) and (466.39,526.41) .. (464.47,526) .. controls (462.54,525.59) and (464.31,525.44) .. (461.72,525) .. controls (459.12,524.56) and (458.94,527.11) .. (456.72,526.75) .. controls (454.49,526.39) and (454.71,525.86) .. (452.22,525.25) .. controls (449.72,524.64) and (450.6,526.93) .. (447.72,526.25) .. controls (444.83,525.57) and (445.26,530.33) .. (442.72,529.5) .. controls (440.51,528.78) and (431.76,529.5) .. (427.53,529.05) .. controls (425.18,535.52) and (418.82,540.17) .. (411.34,540.17) .. controls (401.86,540.17) and (394.17,532.7) .. (394.17,523.5) .. controls (394.17,514.3) and (401.86,506.83) .. (411.34,506.83) -- cycle (399.22,524) .. controls (399.22,527.04) and (401.68,529.5) .. (404.72,529.5) .. controls (407.75,529.5) and (410.22,527.04) .. (410.22,524) .. controls (410.22,520.96) and (407.75,518.5) .. (404.72,518.5) .. controls (401.68,518.5) and (399.22,520.96) .. (399.22,524) -- cycle ; % for fading 
			 \draw   (411.34,506.83) .. controls (419.03,506.83) and (425.54,511.74) .. (427.72,518.5) .. controls (435.59,518.47) and (484.25,517.85) .. (487.26,518.17) .. controls (490.47,518.5) and (488.57,519.25) .. (490.72,520.5) .. controls (492.86,521.75) and (495.97,520) .. (494.97,522) .. controls (493.97,524) and (488.72,527.25) .. (487.97,528) .. controls (487.22,528.75) and (487.88,528.15) .. (483.47,528.5) .. controls (479.05,528.85) and (481.54,527.43) .. (479.72,527.25) .. controls (477.89,527.07) and (478,526.03) .. (475.72,525.75) .. controls (473.43,525.47) and (475.55,527.74) .. (473.47,527.5) .. controls (471.38,527.26) and (473.75,529.13) .. (471.22,528.75) .. controls (468.68,528.37) and (469.6,529.64) .. (467.47,529.25) .. controls (465.33,528.86) and (466.39,526.41) .. (464.47,526) .. controls (462.54,525.59) and (464.31,525.44) .. (461.72,525) .. controls (459.12,524.56) and (458.94,527.11) .. (456.72,526.75) .. controls (454.49,526.39) and (454.71,525.86) .. (452.22,525.25) .. controls (449.72,524.64) and (450.6,526.93) .. (447.72,526.25) .. controls (444.83,525.57) and (445.26,530.33) .. (442.72,529.5) .. controls (440.51,528.78) and (431.76,529.5) .. (427.53,529.05) .. controls (425.18,535.52) and (418.82,540.17) .. (411.34,540.17) .. controls (401.86,540.17) and (394.17,532.7) .. (394.17,523.5) .. controls (394.17,514.3) and (401.86,506.83) .. (411.34,506.83) -- cycle (399.22,524) .. controls (399.22,527.04) and (401.68,529.5) .. (404.72,529.5) .. controls (407.75,529.5) and (410.22,527.04) .. (410.22,524) .. controls (410.22,520.96) and (407.75,518.5) .. (404.72,518.5) .. controls (401.68,518.5) and (399.22,520.96) .. (399.22,524) -- cycle ; % for border 
			
			%Shape: Path Data [id:dp10891185399797387] 
			\path  [shading=_lvsslu7po,_zg1fy50we] (538.34,544.33) .. controls (546.03,544.33) and (552.54,549.24) .. (554.72,556) .. controls (562.59,555.97) and (611.25,555.35) .. (614.26,555.67) .. controls (617.47,556) and (615.57,556.75) .. (617.72,558) .. controls (619.86,559.25) and (622.97,557.5) .. (621.97,559.5) .. controls (620.97,561.5) and (615.72,564.75) .. (614.97,565.5) .. controls (614.22,566.25) and (614.88,565.65) .. (610.47,566) .. controls (606.05,566.35) and (608.54,564.93) .. (606.72,564.75) .. controls (604.89,564.57) and (605,563.53) .. (602.72,563.25) .. controls (600.43,562.97) and (602.55,565.24) .. (600.47,565) .. controls (598.38,564.76) and (600.75,566.63) .. (598.22,566.25) .. controls (595.68,565.87) and (596.6,567.14) .. (594.47,566.75) .. controls (592.33,566.36) and (593.39,563.91) .. (591.47,563.5) .. controls (589.54,563.09) and (591.31,562.94) .. (588.72,562.5) .. controls (586.12,562.06) and (585.94,564.61) .. (583.72,564.25) .. controls (581.49,563.89) and (581.71,563.36) .. (579.22,562.75) .. controls (576.72,562.14) and (577.6,564.43) .. (574.72,563.75) .. controls (571.83,563.07) and (572.26,567.83) .. (569.72,567) .. controls (567.51,566.28) and (558.76,567) .. (554.53,566.55) .. controls (552.18,573.02) and (545.82,577.67) .. (538.34,577.67) .. controls (528.86,577.67) and (521.17,570.2) .. (521.17,561) .. controls (521.17,551.8) and (528.86,544.33) .. (538.34,544.33) -- cycle (526.22,561.5) .. controls (526.22,564.54) and (528.68,567) .. (531.72,567) .. controls (534.75,567) and (537.22,564.54) .. (537.22,561.5) .. controls (537.22,558.46) and (534.75,556) .. (531.72,556) .. controls (528.68,556) and (526.22,558.46) .. (526.22,561.5) -- cycle ; % for fading 
			 \draw   (538.34,544.33) .. controls (546.03,544.33) and (552.54,549.24) .. (554.72,556) .. controls (562.59,555.97) and (611.25,555.35) .. (614.26,555.67) .. controls (617.47,556) and (615.57,556.75) .. (617.72,558) .. controls (619.86,559.25) and (622.97,557.5) .. (621.97,559.5) .. controls (620.97,561.5) and (615.72,564.75) .. (614.97,565.5) .. controls (614.22,566.25) and (614.88,565.65) .. (610.47,566) .. controls (606.05,566.35) and (608.54,564.93) .. (606.72,564.75) .. controls (604.89,564.57) and (605,563.53) .. (602.72,563.25) .. controls (600.43,562.97) and (602.55,565.24) .. (600.47,565) .. controls (598.38,564.76) and (600.75,566.63) .. (598.22,566.25) .. controls (595.68,565.87) and (596.6,567.14) .. (594.47,566.75) .. controls (592.33,566.36) and (593.39,563.91) .. (591.47,563.5) .. controls (589.54,563.09) and (591.31,562.94) .. (588.72,562.5) .. controls (586.12,562.06) and (585.94,564.61) .. (583.72,564.25) .. controls (581.49,563.89) and (581.71,563.36) .. (579.22,562.75) .. controls (576.72,562.14) and (577.6,564.43) .. (574.72,563.75) .. controls (571.83,563.07) and (572.26,567.83) .. (569.72,567) .. controls (567.51,566.28) and (558.76,567) .. (554.53,566.55) .. controls (552.18,573.02) and (545.82,577.67) .. (538.34,577.67) .. controls (528.86,577.67) and (521.17,570.2) .. (521.17,561) .. controls (521.17,551.8) and (528.86,544.33) .. (538.34,544.33) -- cycle (526.22,561.5) .. controls (526.22,564.54) and (528.68,567) .. (531.72,567) .. controls (534.75,567) and (537.22,564.54) .. (537.22,561.5) .. controls (537.22,558.46) and (534.75,556) .. (531.72,556) .. controls (528.68,556) and (526.22,558.46) .. (526.22,561.5) -- cycle ; % for border 
			
			%Shape: Rectangle [id:dp9047683845510976] 
			\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 250; green, 241; blue, 156 }  ,fill opacity=1 ] (276,676) -- (369.5,676) -- (369.5,796) -- (276,796) -- cycle ;
			%Straight Lines [id:da7905052195057931] 
			\draw    (283,722) -- (362.5,722) ;
			%Straight Lines [id:da8282633990919337] 
			\draw    (283,729) -- (362.5,729) ;
			%Straight Lines [id:da414082342916791] 
			\draw    (283,736) -- (362.5,736) ;
			%Straight Lines [id:da09743650582387375] 
			\draw    (283,744) -- (362.5,744) ;
			%Straight Lines [id:da1538875501532888] 
			\draw    (283,754) -- (362.5,754) ;
			%Straight Lines [id:da817026011056839] 
			\draw    (283,762) -- (362.5,762) ;
			%Straight Lines [id:da25956293393299057] 
			\draw    (283,771) -- (362.5,771) ;
			%Straight Lines [id:da057262255786314054] 
			\draw    (283,778) -- (362.5,778) ;
			%Straight Lines [id:da7304549171335268] 
			\draw    (283,788) -- (362.5,788) ;
			%Right Arrow [id:dp8612905292336022] 
			\path  [shading=_92qa1qrt2,_vnfflhw8o] (399.17,752.95) -- (463.72,752.95) -- (463.72,743.5) -- (502.17,757) -- (463.72,770.5) -- (463.72,761.05) -- (399.17,761.05) -- cycle ; % for fading 
			 \draw  [color={rgb, 255:red, 0; green, 87; blue, 190 }  ,draw opacity=1 ][line width=1.5]  (399.17,752.95) -- (463.72,752.95) -- (463.72,743.5) -- (502.17,757) -- (463.72,770.5) -- (463.72,761.05) -- (399.17,761.05) -- cycle ; % for border 
			
			%Shape: Path Data [id:dp13944054884926516] 
			\path  [shading=_vbv5f4ca0,_v4755zuoo] (411.34,702.83) .. controls (419.03,702.83) and (425.54,707.74) .. (427.72,714.5) .. controls (435.59,714.47) and (484.25,713.85) .. (487.26,714.17) .. controls (490.47,714.5) and (488.57,715.25) .. (490.72,716.5) .. controls (492.86,717.75) and (495.97,716) .. (494.97,718) .. controls (493.97,720) and (488.72,723.25) .. (487.97,724) .. controls (487.22,724.75) and (487.88,724.15) .. (483.47,724.5) .. controls (479.05,724.85) and (481.54,723.43) .. (479.72,723.25) .. controls (477.89,723.07) and (478,722.03) .. (475.72,721.75) .. controls (473.43,721.47) and (475.55,723.74) .. (473.47,723.5) .. controls (471.38,723.26) and (473.75,725.13) .. (471.22,724.75) .. controls (468.68,724.37) and (469.6,725.64) .. (467.47,725.25) .. controls (465.33,724.86) and (466.39,722.41) .. (464.47,722) .. controls (462.54,721.59) and (464.31,721.44) .. (461.72,721) .. controls (459.12,720.56) and (458.94,723.11) .. (456.72,722.75) .. controls (454.49,722.39) and (454.71,721.86) .. (452.22,721.25) .. controls (449.72,720.64) and (450.6,722.93) .. (447.72,722.25) .. controls (444.83,721.57) and (445.26,726.33) .. (442.72,725.5) .. controls (440.51,724.78) and (431.76,725.5) .. (427.53,725.05) .. controls (425.18,731.52) and (418.82,736.17) .. (411.34,736.17) .. controls (401.86,736.17) and (394.17,728.7) .. (394.17,719.5) .. controls (394.17,710.3) and (401.86,702.83) .. (411.34,702.83) -- cycle (399.22,720) .. controls (399.22,723.04) and (401.68,725.5) .. (404.72,725.5) .. controls (407.75,725.5) and (410.22,723.04) .. (410.22,720) .. controls (410.22,716.96) and (407.75,714.5) .. (404.72,714.5) .. controls (401.68,714.5) and (399.22,716.96) .. (399.22,720) -- cycle ; % for fading 
			 \draw   (411.34,702.83) .. controls (419.03,702.83) and (425.54,707.74) .. (427.72,714.5) .. controls (435.59,714.47) and (484.25,713.85) .. (487.26,714.17) .. controls (490.47,714.5) and (488.57,715.25) .. (490.72,716.5) .. controls (492.86,717.75) and (495.97,716) .. (494.97,718) .. controls (493.97,720) and (488.72,723.25) .. (487.97,724) .. controls (487.22,724.75) and (487.88,724.15) .. (483.47,724.5) .. controls (479.05,724.85) and (481.54,723.43) .. (479.72,723.25) .. controls (477.89,723.07) and (478,722.03) .. (475.72,721.75) .. controls (473.43,721.47) and (475.55,723.74) .. (473.47,723.5) .. controls (471.38,723.26) and (473.75,725.13) .. (471.22,724.75) .. controls (468.68,724.37) and (469.6,725.64) .. (467.47,725.25) .. controls (465.33,724.86) and (466.39,722.41) .. (464.47,722) .. controls (462.54,721.59) and (464.31,721.44) .. (461.72,721) .. controls (459.12,720.56) and (458.94,723.11) .. (456.72,722.75) .. controls (454.49,722.39) and (454.71,721.86) .. (452.22,721.25) .. controls (449.72,720.64) and (450.6,722.93) .. (447.72,722.25) .. controls (444.83,721.57) and (445.26,726.33) .. (442.72,725.5) .. controls (440.51,724.78) and (431.76,725.5) .. (427.53,725.05) .. controls (425.18,731.52) and (418.82,736.17) .. (411.34,736.17) .. controls (401.86,736.17) and (394.17,728.7) .. (394.17,719.5) .. controls (394.17,710.3) and (401.86,702.83) .. (411.34,702.83) -- cycle (399.22,720) .. controls (399.22,723.04) and (401.68,725.5) .. (404.72,725.5) .. controls (407.75,725.5) and (410.22,723.04) .. (410.22,720) .. controls (410.22,716.96) and (407.75,714.5) .. (404.72,714.5) .. controls (401.68,714.5) and (399.22,716.96) .. (399.22,720) -- cycle ; % for border 
			
			%Shape: Rectangle [id:dp2876729156857749] 
			\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][fill={rgb, 255:red, 173; green, 214; blue, 255 }  ,fill opacity=1 ] (531,676) -- (624.5,676) -- (624.5,796) -- (531,796) -- cycle ;
			%Straight Lines [id:da0336409695601767] 
			\draw    (538,722) -- (617.5,722) ;
			%Straight Lines [id:da35798670100754637] 
			\draw    (538,729) -- (617.5,729) ;
			%Straight Lines [id:da5051167991310435] 
			\draw    (538,736) -- (617.5,736) ;
			%Straight Lines [id:da48319371107329445] 
			\draw    (538,744) -- (617.5,744) ;
			%Straight Lines [id:da526617942461282] 
			\draw    (538,754) -- (617.5,754) ;
			%Straight Lines [id:da720355542863655] 
			\draw    (538,762) -- (617.5,762) ;
			%Straight Lines [id:da2782631486280276] 
			\draw    (538,771) -- (617.5,771) ;
			%Straight Lines [id:da4473845427033669] 
			\draw    (538,778) -- (617.5,778) ;
			%Straight Lines [id:da7391533295207624] 
			\draw    (538,788) -- (617.5,788) ;
			%Right Arrow [id:dp28900121256851175] 
			\path  [shading=_tdhg8xud7,_h90ufajx6] (531.35,608.65) -- (490.56,658.68) -- (497.88,664.65) -- (463.12,685.91) -- (476.96,647.58) -- (484.28,653.56) -- (525.08,603.53) -- cycle ; % for fading 
			 \draw  [color={rgb, 255:red, 0; green, 87; blue, 190 }  ,draw opacity=1 ][line width=1.5]  (531.35,608.65) -- (490.56,658.68) -- (497.88,664.65) -- (463.12,685.91) -- (476.96,647.58) -- (484.28,653.56) -- (525.08,603.53) -- cycle ; % for border 
			
			
			% Text Node
			\draw (35,26) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\LARGE FILE ENCRYPTION}};
			% Text Node
			\draw (73.38,204.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {File};
			% Text Node
			\draw (148.45,97) node [anchor=north west][inner sep=0.75pt]   [align=left] {Symmetric Key (FEK)};
			% Text Node
			\draw (184.95,204.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {Encryption};
			% Text Node
			\draw (324.42,204.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {Encrypted File};
			% Text Node
			\draw (519.42,317.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{88.9pt}\setlength\topsep{0pt}
			\begin{center}
			Encrypted File\\with FEK in header
			\end{center}
			
			\end{minipage}};
			% Text Node
			\draw (184.95,374.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {Encryption};
			% Text Node
			\draw (35,458) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\LARGE FILE DECRYPTION}};
			% Text Node
			\draw (26.42,638.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{88.9pt}\setlength\topsep{0pt}
			\begin{center}
			Encrypted File\\with FEK in header
			\end{center}
			
			\end{minipage}};
			% Text Node
			\draw (144.95,262.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {User's Public Key (PK)};
			% Text Node
			\draw (318.95,374.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {Encrypted FEK};
			% Text Node
			\draw (267.75,594.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {Encrypted FEK};
			% Text Node
			\draw (410.75,592.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {Decryption};
			% Text Node
			\draw (273.42,804.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {Encrypted File};
			% Text Node
			\draw (410.75,804.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {Decryption};
			% Text Node
			\draw (566.38,803.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {File};
			\end{tikzpicture}}
			\vspace*{3mm}
			\caption[Principle of F.E.K. in Microsoft Windows XP]{Principle of F.E.K. in Microsoft Windows XP (source: Wikipedia)}
		\end{figure}
		However, the cryptography keys for EFS are in practice protected by the user account password, and are therefore susceptible to most password attacks. In other words, encryption of files is only as strong as the password to unlock the decryption key.
		\end{tcolorbox}
		These methods are still decipherable, provided that the interceptor has enough time and paper/money (excepted at this date for quantum encryption).
		
		Here is a small summary table of broken keys and their respective size for both conventional systems (the years are given based on the holocene calendar):
		\begin{table}[H]
	\begin{center}
		\begin{tabular}{|c|c|}
			\hline
			\multicolumn{2}{|c|}{\cellcolor[gray]{0.75}\textbf{Secret key (symmetric system)}} \\
			\multicolumn{2}{|c|}{\cellcolor[gray]{0.75}exhaustive search} \\
			\hline
			\cellcolor[gray]{0.75}Number of bits & \cellcolor[gray]{0.75}Year \\
			\hline
			$40$ & Broken in 11995 \\
			\hline
			$56$ & Broken in 11998 \\
			\hline
			$64$ & Brokable \\
			\hline
			$128$ & Brokable in $\sim$12100 \\
			\hline
			$256$ & ? \\
			\hline
		\end{tabular}
		\begin{tabular}{|c|c|}
			\hline
			\multicolumn{2}{|c|}{\cellcolor[gray]{0.75}\textbf{RSA public key (asymmetric system)}} \\
			\multicolumn{2}{|c|}{\cellcolor[gray]{0.75}exhaustive search} \\
			\hline
			\cellcolor[gray]{0.75}Number of bits & \cellcolor[gray]{0.75}Year \\
			\hline
			$256$ & Broken in 11985 \\
			\hline
			$512$ & Broken in 11999 \\
			\hline
			$1024$ & Broken in 12010 \\
			\hline
			$2048$ & Brokable in $\sim$2100 \\
			\hline
			$4096$ & ? \\
			\hline
		\end{tabular}
		\caption{Key systems and recent broke}
	\end{center}
	\end{table}
	
	\subsubsection{Kerckhoffs' principle}
	The primary function of cryptography is therefore to ensure the confidentiality of information exchange. Two parts of a confidential exchange will first agree on a secret convention to write their messages, and if they have carefully chosen, no one else should be able to enter their exchange.
	
	If the secrecy of such agreements is possible from a few isolated individuals for a limited period, it is inconceivable at  large scale and for a fairly long period. This is what Auguste Kerckhoffs understood when establishing the basic principles of practical cryptography which requires a fundamental principle encryption system "that does not require secrecy, and which can conveniently fall into the hands the enemy".
	
	The six postulates of Kerchoffs are:
	\begin{enumerate}
		\item The system must be practically, if not mathematically, indecipherable;
		\item It should not require secrecy, and it should not be a problem if it falls into enemy hands;
		\item It must be possible to communicate and remember the key without using written notes, and correspondents must be able to change or modify it at will;
		\item It must be applicable to telegraph communications;
		\item It must be portable, and should not require several persons to handle or operate;
		\item Lastly, given the circumstances in which it is to be used, the system must be easy to use and should not be stressful to use or require its users to know and comply with a long list of rules.
	\end{enumerate}
	The second postulate, known today as the "\NewTerm{Kerckhoffs principle}\index{Kerckhoffs principle}"  states that the security of an encryption system is not based on the secrecy of the procedure, but only on one parameter used when its implemented: the key. This key is the only secret of the Exchange Agreement.
	
	This principle, however, was reformulated by Claude Shannon: "the enemy knows the system". This formulation is known as the "Shannon's maxim". This is the principle usually adopted by cryptologists, as opposed to the security through obscurity.
	
	\subsection{Traps}
	Sometimes there are what we name "trap doors" in public and secret keys. This is because when generating the key, which has to be done randomly within certain predefined theoretical constraints, the random generator may have an issue (the issue is sometimes voluntary on the part of the supplier of the material for spy purpose...).
	
	In the secret keys, the traps are located at the level of the "\NewTerm{key's entropy}\index{key's entropy}" (\SeeChapter{see section Statistical Mechanics page \pageref{entropy}}), directly linked to the entropy of the random generator. We can simplistically define the entropy of a key generator by the average optimal binary questions (that is to say giving rise to the type of answers Yes / No) that we need to ask someone knowing a key produced by this generator to determine it. More the entropy of a key generator is high, higher is the number of questions we need to determine the key. Conversely, the smaller is the entropy, the lower are the questions, so that the search of a key is facilitated.
	
	The introduction of traps in the asymmetric key systems is much more difficult, since this type of key already has intrinsic mathematical structure: their construction is not due to chance but is the result of mathematical rules. Chance is here in the choice of large prime numbers used. The fact that asymmetric systems can be easily calculated, but they are difficult to reverse are sometimes named "trapdoor functions".

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	An example of a simple mathematical trapdoor is "$6895601$ is the product of two prime numbers. What are those numbers?" A typical solution would be to try dividing $6895601$ by several prime numbers until finding the answer. However, if one is told that $1931$ is one of the numbers, one can find the answer by entering "$6895601\div 1931$" into any calculator. This example is not a sturdy trapdoor function – modern computers can guess all of the possible answers within a second – but this sample problem could be improved by using the product of two much larger primes.\\
	
	Therefore if a random generator that generates prime numbers is biased (\SeeChapter{see section Statistics page \pageref{likelihood estimators}}), this bias will facilitate the research of a trapdoor.
	\end{tcolorbox}
	
	\textbf{Definition (\#\thesection.\mydef):} A "\NewTerm{trapdoor function}\index{trapdoor function}" is a function that is easy to compute in one direction, yet difficult to compute in the opposite direction (finding its inverse) without special information.
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} Functions related to the hardness of the discrete logarithm problem (either modulo a prime or in a group defined over an elliptic curve) are not known to be trapdoor functions, because there is no known "trapdoor" information about the group that enables the efficient computation of discrete logarithms.\\
	
	\textbf{R2.} Trapdoor must not to be confused with a "\NewTerm{backdoor}\index{backdoor}" as this latter is a deliberate mechanism that is added to a cryptographic algorithm or operating system, for example, that permits one or more unauthorized parties to bypass or subvert the security of the system in some fashion.
	\end{tcolorbox}
	
	\subsection{Secret-key encryption system}
	\textbf{Definition (\#\thesection.\mydef):} The "\NewTerm{single-use encryption}\index{single-use encryption}" is a secret key encryption algorithm proved unconditionally secure. Properly used (and that's an important point), it provides an unbreakable encryption in reasonable time.
	
	The theoretical basis of this encryption system are:
	
	Given a message $M$ in binary form to be transmitted between people $A$ (creator and originator of the message $M$) and $B$ (reader and receiver). We generate a large amount of bits if possible "truly randomly" forming a secret key $K$ of same size as the message to be transmitted (computer programs, deterministic by nature, can not generate truly random bits).
	
	This key will be sent to $B$ by a channel supposedly safe ... A given time after the transmission of this key $A$ will encode his message into $C$ by performing the operation:
	
	where $\star$ is an operator that must satisfy to a group law (\SeeChapter{see section Set Theory page \pageref{group law}}) on a finite set (that contains a limited number of items or "letters").
	
	The idea in computing science is to use the XOR law (exclusive OR) denoted $\oplus$ for what will follow (\SeeChapter{see section Logical Systems page \pageref{boolean operators}}) as it is enough as a group law (remember that a group is the smallest structure having an opposite - and is associative and having neutral element - that gives therefore the possibility to reverse the encoding process). Therefore:
	
	Finally, the sender $A$ transmits the encrypted version of his message $C$ by a route not necessarily secure. $B$ can read the original message $M$ by using the inverse operator $\oplus^{-1}$ (the XOR operator is its own inverse as we have proved it thanks to its truth table in the section of Logical Systems!!!!). So receiver $B$ will do the following:
	
	Provided that the $K$ has been generated totally randomly and that each bit of it has been used only once to encrypt the message, an interceptor gets no information about the clear message $M$ if he intercepts $C$. Indeed, in these conditions, we can not establish any correlation between $M$ and $C$ without the knowledge of $K$.
	
	Even with future ultra-powerful quantum computers, the problem is insoluble, because nothing connects the information which is available and the problem to solve. Consequently, the "single-use encoding" is an encryption algorithm "unconditionally secure". The proof of its security does not rely on unproven mathematical conjectures and decryption attempts of an interceptor with infinite computing power are futile.
	
	However, each stage of encryption is a source of possible errors. Indeed, the key $K$ may have been poorly developed. The slightest statistical deviation of $K$ compared to the "real" random provides information on the clear message $M$ from its encrypted version. This is why the $K$ bits are to be used only once if possible.
	
	Indeed, suppose that same key is used to encrypt messages of French language $M_1$ and $M_2$ an attacker manages to intercept the two corresponding encrypted messages. From $C_1$ and $C_2$ the interceptor and can easily obtain information about $K$ and this because of language peculiarities (same for English). Indeed, since:
	
	then the interceptor knows a simple result that involves $M_1$ and $M_2$ without the key $K$:
	
	as:
	
	(if necessary make the truth table to be convinced of this relation). Now, if $M_1$ and $M_2$ are in the same language, we will know, usually due to language redundancies (e.g. the letter "e" often appears in French), found from $C_1\oplus C_2$ each of the two original messages (the work is though laborious without statistical automated tools).
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Imagine that we want to send a little message $M$ binary coded by $1101$ and we generated a random key $K$ that gave $0101$.\\
	
	Then we have:
	
	and therefore:
	
	\end{tcolorbox}
	Obviously in this kind of small situations we can guess $M$ without much difficulty just by having $C$ if there such like here only a single encryption step. This is why there are encoding patterns as we shall see now.
	
	The main problem with this technique is the creation of a key as random as possible. To overcome this, mathematicians do pass the key through a series of nested functions, the result after many iterations, becomes "pseudo-random".
	
	Building a pseudo-random iteration is one thing, building a pseudo-random bijection is yet another!!! Indeed, we need to decrypt the message later, which is why we absolutely need a bijective system (which has everything arrival element - encrypted message - matches a single starting element - decrypted message - and vice versa).
	
	\subsubsection{Feistel Schemes}
	Even if encryption algorithms, in this late 120th century (holocene calendar) and early 121st century (holocene calendar), are considered sufficient with a key having a finite number of bits, the goal remains the development that from a message $M$ and a random sequence of digits, or at least that looks like, to build a key $K$ to send an encrypted message $C$ that can be decipher easily only by people knowing the key. Specifically, this target application is to construct or identify a function which, firstly, do correspond to each digit of $M$ a digit $C$ that seems to look random (but whose value depends in reality on the deterministic key) and, secondly, authorizing the reverse path (inverse function by the property of bijection), that is to say that from a digit $C$, we can uniquely trace back to the corresponding digit of $M$. We therefore would like to find a pseudo-random bijection function.
	
	In the years 11950s (holocene calendar), the mathematician Horst Feistel has shown that a pseudo-random function transformed itself, by a relatively simple method, in bijection function. Today, the "\NewTerm{Feistel cypher}\index{Feistel cypher}" is most commonly used in the secret key encryption systems and is also the basis of the DES (Data Encryption System). How does it work?
	
	Here is the principle:
	
	The initial message to be encrypted has a size of $2n$ bits. The split the original message $M$ into two blocks (thus the Feistel Schemes belongs ton the family of "\NewTerm{block cipher}\index{block cipher}"), $G$ and $D$, of equal length ($G$ includes the first $n$ bits and $D$ the following) and we build the transformation $\varphi$ that associates to $G$ and $D$ the numbers $T$ and $S$ such as:
	
	where for reminder the $\oplus$ still represents the bit by bit XOR operation and where $f_1$ is any function, non-necessarily bijective, from $n$ bits to $n$ bit using the secret key $K$.
	
	The transformation $\varphi(G,D)=(S,T)$ is indeed bijective, as we can go back in a unambiguous way starting form $S$ and $T$ to $G$ and $D$ by the operations:
	
	Obviously we must not stop here, since the right side of the message, $D$, has not been encrypted, it is simply passed to the left. However, as $\varphi$ is bijective, we can repeat the process. A Feistel scheme where we apply $n$ times the function $\varphi$ is named a "\NewTerm{$n$-step pattern}\index{$n$-step pattern}".
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	We will encrypt by the a two-step Feistel cypher a message consisting of $4$ bits (thus $2^4=16$ possibilities of messages), what is equivalent to building a bijection from $4$ bits to $4$bits from two functions $f_1,f_2$ of two bits to two bits. The functions $f_1,f_2$ have in input both: the message to encrypt and the secret key. We will assume that for some input key, these functions are:
	\begin{table}[H]
	\centering
		\begin{tabular}{|c|c|c|l|c|c|c|}
		\cline{1-3} \cline{5-7}
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}{\textbf{Input}}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}{\textbf{$f_1$}}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}{\textbf{Output}}} &  & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}{\textbf{Input}}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}{\textbf{$f_2$}}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}{\textbf{Output}}} \\ \cline{1-3} \cline{5-7} 
		$00$ & $\rightarrow$ & $01$ &  & $00$ & $\rightarrow$ & $11$ \\ \cline{1-3} \cline{5-7} 
		$01$ & $\rightarrow$ & $11$ &  & $01$ & $\rightarrow$ & $00$ \\ \cline{1-3} \cline{5-7} 
		$10$ & $\rightarrow$ & $10$ &  & $10$ & $\rightarrow$ & $00$ \\ \cline{1-3} \cline{5-7} 
		$11$ & $\rightarrow$ & $01$ &  & $11$ & $\rightarrow$ & $01$ \\ \cline{1-3} \cline{5-7} 
		\end{tabular}
		\caption{Input/Output key matches by functions}
	\end{table}
	Let us notice that neither $f_1$ nor $f_2$ are bijections ($f_1(00)=f_1(11)=01$,$f_2(01)=f_2(10)=00$). For example, encrypt the message 1101. $G$ designates the left part of the message to be encrypted, $D$ the right part:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,638); %set diagram left start at 0, and has height of 638
		
		%Shape: Rectangle [id:dp6247820827867736] 
		\draw   (268,32) -- (402.4,32) -- (402.4,69.2) -- (268,69.2) -- cycle ;
		%Shape: Rectangle [id:dp8136715478392649] 
		\draw   (204,143) -- (296.4,143) -- (296.4,177.2) -- (204,177.2) -- cycle ;
		%Shape: Rectangle [id:dp36275129293669073] 
		\draw   (376,143) -- (468.4,143) -- (468.4,177.2) -- (376,177.2) -- cycle ;
		%Straight Lines [id:da512249648493261] 
		\draw    (326,72) -- (260.52,137.48) ;
		\draw [shift={(258.4,139.6)}, rotate = 315] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da3224728869025322] 
		\draw    (343,72) -- (409.28,138.28) ;
		\draw [shift={(411.4,140.4)}, rotate = 225] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da12392064280201831] 
		\draw    (256.7,183.6) -- (434.54,240.29) ;
		\draw [shift={(437.4,241.2)}, rotate = 197.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da7990450841473362] 
		\draw    (423.4,181.2) -- (260.22,240.18) ;
		\draw [shift={(257.4,241.2)}, rotate = 340.13] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Shape: Rectangle [id:dp20985354289283764] 
		\draw   (204,245) -- (296.4,245) -- (296.4,279.2) -- (204,279.2) -- cycle ;
		%Shape: Rectangle [id:dp42507735908265665] 
		\draw   (339.4,245) -- (556.4,245) -- (556.4,279.2) -- (339.4,279.2) -- cycle ;
		%Straight Lines [id:da20563121182987354] 
		\draw    (250.7,286.6) -- (428.54,343.29) ;
		\draw [shift={(431.4,344.2)}, rotate = 197.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da8331201713122618] 
		\draw    (448.4,285.2) -- (254.28,342.35) ;
		\draw [shift={(251.4,343.2)}, rotate = 343.59] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Shape: Rectangle [id:dp09620680334960241] 
		\draw   (186.4,349) -- (310.4,349) -- (310.4,383.2) -- (186.4,383.2) -- cycle ;
		%Shape: Rectangle [id:dp9814712937911543] 
		\draw   (341.4,350) -- (578.4,350) -- (578.4,406) -- (341.4,406) -- cycle ;
		
		% Text Node
		\draw (316,43.4) node [anchor=north west][inner sep=0.75pt]    {$1101$};
		% Text Node
		\draw (150,42) node [anchor=north west][inner sep=0.75pt]   [align=left] {Message};
		% Text Node
		\draw (224.2,152.5) node [anchor=north west][inner sep=0.75pt]    {$G=11$};
		% Text Node
		\draw (396.2,152.5) node [anchor=north west][inner sep=0.75pt]    {$D=01$};
		% Text Node
		\draw (150,152) node [anchor=north west][inner sep=0.75pt]   [align=left] {Split};
		% Text Node
		\draw (224.2,254.5) node [anchor=north west][inner sep=0.75pt]    {$D=01$};
		% Text Node
		\draw (343,254.5) node [anchor=north west][inner sep=0.75pt]    {$G\oplus f_{1}( D) =( 11) \oplus ( 11) =00$};
		% Text Node
		\draw (116,251) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 1^{\text{st}}$ step};
		% Text Node
		\draw (116,355) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 2^{\text{nd}}$ step};
		% Text Node
		\draw (191.2,357.5) node [anchor=north west][inner sep=0.75pt]    {$G\oplus f_{1}( D) =00$};
		% Text Node
		\draw (339,359.5) node [anchor=north west][inner sep=0.75pt]    {$ \begin{array}{l}
		D\oplus f_{2}( G\oplus f_{1}( D)) =D\oplus f_{2}( 00)\\
		=( 01) \oplus ( 11) =10
		\end{array}$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Encryption of $1101$ using the Feistel method}
	\end{figure}
	The result is $0010$. We will compute the image of the other $15$ other possible messages and verify that there is an unambiguous correspondence between each message and its image by the Feistel scheme: we have constructed a bijection from two functions that are not bijective.
	\end{tcolorbox}

	Quite complex theoretical results guarantee the cryptographic security of Feistel schemes starting from $4$ steps when $n$ is large enough and when the functions $f_i$ are indistinguishable from truly random functions. In practice, rather than using $4$ steps and functions $f_i$ that look like random, it is generally preferred to use more steps and more simple functions $f_i$. After a few steps, the obtained bijection often becomes very difficult to distinguish from random bijections. And for parameters well chosen, we no longer know at all how to distinguish them from truly random bijections!!!

	Most of the secret key encryption algorithms currently in this end of 120th century (holocene calendar) used in the civilian world are Feistel schemas. In particular, the DES (Data Encryption System) algorithm  which is a $16$-step Feistel scheme as shown in the figure below and the Triple DES (TDES) algorithm which is a $48$-step Feistal scheme and the Blowfish algorithm that will not be discussed here).
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	For example, there are, in some bank cards (at least in the beginning of this 121st century... according to holocene calendar), a DES key (or TDES since October 12001 according to holocene calendar) which provides proof of the legitimacy of the card between the bank's control center and the merchant's terminal in addition to Public part of a RSA key to make sure the user code is entered (control done by an internal chip on the card, which must then be manufactured in very secure premises).
	\end{tcolorbox}
	Rigorously the Feistel scheme is a bit different because it involves keys, which we did not use in the example presented before. Here is a more detailed figure in what this Feistel scheme consists of (see figures below).

	Principle of the diagram: A message to be encrypted is divided into blocks of $64$ bits, each of which is divided into two $32$-bit sub-blocks, the left block ($G$) and the right-hand block ($D$). At each iteration, the old right block becomes the new left block and the new right block results from the XOR operation of the old right block, whose bits are mixed by a confusion function, and of the previous left block. The iteration is repeated $16$ times.
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1215); %set diagram left start at 0, and has height of 1215
		
		%Shape: Rectangle [id:dp7600178477828723] 
		\draw  [fill={rgb, 255:red, 255; green, 247; blue, 172 }  ,fill opacity=1 ] (176,27) -- (351.87,27) -- (351.87,48.19) -- (176,48.19) -- cycle ;
		%Shape: Rectangle [id:dp20839942060854044] 
		\draw  [fill={rgb, 255:red, 240; green, 248; blue, 255 }  ,fill opacity=1 ] (351.87,27) -- (527.74,27) -- (527.74,48.19) -- (351.87,48.19) -- cycle ;
		%Rounded Rect [id:dp24435436772472507] 
		\draw  [fill={rgb, 255:red, 224; green, 255; blue, 197 }  ,fill opacity=1 ] (309,75.46) .. controls (309,72.44) and (311.44,70) .. (314.46,70) -- (382.42,70) .. controls (385.43,70) and (387.87,72.44) .. (387.87,75.46) -- (387.87,91.82) .. controls (387.87,94.84) and (385.43,97.28) .. (382.42,97.28) -- (314.46,97.28) .. controls (311.44,97.28) and (309,94.84) .. (309,91.82) -- cycle ;
		%Curve Lines [id:da42506318886269034] 
		\draw    (168.87,38.28) .. controls (96.24,52.21) and (98.84,143.36) .. (166.84,162) ;
		\draw [shift={(167.87,162.28)}, rotate = 194.62] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Flowchart: Or [id:dp047523744617595565] 
		\draw   (256.22,83.63) .. controls (256.22,79.69) and (259.41,76.5) .. (263.35,76.5) .. controls (267.29,76.5) and (270.48,79.69) .. (270.48,83.63) .. controls (270.48,87.57) and (267.29,90.76) .. (263.35,90.76) .. controls (259.41,90.76) and (256.22,87.57) .. (256.22,83.63) -- cycle ; \draw   (256.22,83.63) -- (270.48,83.63) ; \draw   (263.35,76.5) -- (263.35,90.76) ;
		%Straight Lines [id:da188055336126985] 
		\draw    (263.35,48.05) -- (263.35,74.5) ;
		\draw [shift={(263.35,76.5)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da2386920514239983] 
		\draw    (308.65,83.63) -- (272.48,83.63) ;
		\draw [shift={(270.48,83.63)}, rotate = 360] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da03565915146872256] 
		\draw    (437.65,84.13) -- (390.48,84.13) ;
		\draw [shift={(388.48,84.13)}, rotate = 360] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da11713659616302596] 
		\draw    (437.65,48.4) -- (437.65,110.4) -- (263.65,132.4) -- (263.65,149.9) ;
		\draw [shift={(263.65,151.9)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da8314026942383932] 
		\draw    (263.35,90.76) -- (263.15,111.9) -- (436.15,132.4) -- (436.15,150.4) ;
		\draw [shift={(436.15,152.4)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Rectangle [id:dp7138552771118623] 
		\draw  [fill={rgb, 255:red, 255; green, 247; blue, 160 }  ,fill opacity=1 ] (176,152) -- (351.87,152) -- (351.87,173.19) -- (176,173.19) -- cycle ;
		%Shape: Rectangle [id:dp8359014700552085] 
		\draw  [fill={rgb, 255:red, 216; green, 235; blue, 255 }  ,fill opacity=1 ] (351.87,152) -- (527.74,152) -- (527.74,173.19) -- (351.87,173.19) -- cycle ;
		%Rounded Rect [id:dp8219462266019615] 
		\draw  [fill={rgb, 255:red, 224; green, 255; blue, 197 }  ,fill opacity=1 ] (309,200.46) .. controls (309,197.44) and (311.44,195) .. (314.46,195) -- (382.42,195) .. controls (385.43,195) and (387.87,197.44) .. (387.87,200.46) -- (387.87,216.82) .. controls (387.87,219.84) and (385.43,222.28) .. (382.42,222.28) -- (314.46,222.28) .. controls (311.44,222.28) and (309,219.84) .. (309,216.82) -- cycle ;
		%Flowchart: Or [id:dp18179406650005547] 
		\draw   (256.22,208.63) .. controls (256.22,204.69) and (259.41,201.5) .. (263.35,201.5) .. controls (267.29,201.5) and (270.48,204.69) .. (270.48,208.63) .. controls (270.48,212.57) and (267.29,215.76) .. (263.35,215.76) .. controls (259.41,215.76) and (256.22,212.57) .. (256.22,208.63) -- cycle ; \draw   (256.22,208.63) -- (270.48,208.63) ; \draw   (263.35,201.5) -- (263.35,215.76) ;
		%Straight Lines [id:da8484109816201268] 
		\draw    (263.35,173.05) -- (263.35,199.5) ;
		\draw [shift={(263.35,201.5)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da020595186853328062] 
		\draw    (308.65,208.63) -- (272.48,208.63) ;
		\draw [shift={(270.48,208.63)}, rotate = 360] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da6587237663748018] 
		\draw    (437.65,209.13) -- (390.48,209.13) ;
		\draw [shift={(388.48,209.13)}, rotate = 360] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da6723290691046757] 
		\draw    (437.65,173.4) -- (437.65,235.4) -- (263.65,257.4) -- (263.65,274.9) ;
		\draw [shift={(263.65,276.9)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da9091499950769961] 
		\draw    (263.35,215.76) -- (263.15,236.9) -- (436.15,257.4) -- (436.15,275.4) ;
		\draw [shift={(436.15,277.4)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Rectangle [id:dp08152397595286986] 
		\draw  [fill={rgb, 255:red, 254; green, 246; blue, 144 }  ,fill opacity=1 ] (176,277) -- (351.87,277) -- (351.87,298.19) -- (176,298.19) -- cycle ;
		%Shape: Rectangle [id:dp11326684817368093] 
		\draw  [fill={rgb, 255:red, 178; green, 217; blue, 254 }  ,fill opacity=1 ] (351.87,277) -- (527.74,277) -- (527.74,298.19) -- (351.87,298.19) -- cycle ;
		%Rounded Rect [id:dp3721691777021867] 
		\draw  [fill={rgb, 255:red, 224; green, 255; blue, 197 }  ,fill opacity=1 ] (309,325.46) .. controls (309,322.44) and (311.44,320) .. (314.46,320) -- (382.42,320) .. controls (385.43,320) and (387.87,322.44) .. (387.87,325.46) -- (387.87,341.82) .. controls (387.87,344.84) and (385.43,347.28) .. (382.42,347.28) -- (314.46,347.28) .. controls (311.44,347.28) and (309,344.84) .. (309,341.82) -- cycle ;
		%Flowchart: Or [id:dp10884730163570344] 
		\draw   (256.22,333.63) .. controls (256.22,329.69) and (259.41,326.5) .. (263.35,326.5) .. controls (267.29,326.5) and (270.48,329.69) .. (270.48,333.63) .. controls (270.48,337.57) and (267.29,340.76) .. (263.35,340.76) .. controls (259.41,340.76) and (256.22,337.57) .. (256.22,333.63) -- cycle ; \draw   (256.22,333.63) -- (270.48,333.63) ; \draw   (263.35,326.5) -- (263.35,340.76) ;
		%Straight Lines [id:da4511301777792356] 
		\draw    (263.35,298.05) -- (263.35,324.5) ;
		\draw [shift={(263.35,326.5)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da8548946019623567] 
		\draw    (308.65,333.63) -- (272.48,333.63) ;
		\draw [shift={(270.48,333.63)}, rotate = 360] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da5434501332797741] 
		\draw    (437.65,334.13) -- (390.48,334.13) ;
		\draw [shift={(388.48,334.13)}, rotate = 360] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da4866326066368154] 
		\draw    (437.65,298.4) -- (437.65,360.4) -- (263.65,382.4) -- (263.65,399.9) ;
		\draw [shift={(263.65,401.9)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da5251389080140365] 
		\draw    (263.35,340.76) -- (263.15,361.9) -- (436.15,382.4) -- (436.15,400.4) ;
		\draw [shift={(436.15,402.4)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da14543445914512776] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (264.35,401.86) -- (264.35,443.96) ;
		%Straight Lines [id:da7195032975278342] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (436.15,402.4) -- (436.65,444.5) ;
		%Straight Lines [id:da72805809221081] 
		\draw    (436.65,444.5) -- (436.65,464.35) -- (262.65,486.35) -- (262.65,503.85) ;
		\draw [shift={(262.65,505.85)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da9294924324011071] 
		\draw    (264.35,443.96) -- (264.35,464.8) -- (436.15,485.6) -- (436.15,503.6) ;
		\draw [shift={(436.15,505.6)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Rectangle [id:dp29526616448049015] 
		\draw  [fill={rgb, 255:red, 255; green, 246; blue, 116 }  ,fill opacity=1 ] (176,505.2) -- (351.87,505.2) -- (351.87,526.39) -- (176,526.39) -- cycle ;
		%Shape: Rectangle [id:dp907780922020953] 
		\draw  [fill={rgb, 255:red, 141; green, 193; blue, 252 }  ,fill opacity=1 ] (351.87,505.2) -- (527.74,505.2) -- (527.74,526.39) -- (351.87,526.39) -- cycle ;
		%Rounded Rect [id:dp8494673017623788] 
		\draw  [fill={rgb, 255:red, 224; green, 255; blue, 197 }  ,fill opacity=1 ] (308,553.46) .. controls (308,550.44) and (310.44,548) .. (313.46,548) -- (381.42,548) .. controls (384.43,548) and (386.87,550.44) .. (386.87,553.46) -- (386.87,569.82) .. controls (386.87,572.84) and (384.43,575.28) .. (381.42,575.28) -- (313.46,575.28) .. controls (310.44,575.28) and (308,572.84) .. (308,569.82) -- cycle ;
		%Flowchart: Or [id:dp7441679406894628] 
		\draw   (255.22,561.63) .. controls (255.22,557.69) and (258.41,554.5) .. (262.35,554.5) .. controls (266.29,554.5) and (269.48,557.69) .. (269.48,561.63) .. controls (269.48,565.57) and (266.29,568.76) .. (262.35,568.76) .. controls (258.41,568.76) and (255.22,565.57) .. (255.22,561.63) -- cycle ; \draw   (255.22,561.63) -- (269.48,561.63) ; \draw   (262.35,554.5) -- (262.35,568.76) ;
		%Straight Lines [id:da5882702664755826] 
		\draw    (262.35,526.05) -- (262.35,552.5) ;
		\draw [shift={(262.35,554.5)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da9237011647211455] 
		\draw    (307.65,561.63) -- (271.48,561.63) ;
		\draw [shift={(269.48,561.63)}, rotate = 360] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da10665194766439723] 
		\draw    (436.65,562.13) -- (389.48,562.13) ;
		\draw [shift={(387.48,562.13)}, rotate = 360] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da6542194263323611] 
		\draw    (436.65,526.4) -- (436.65,588.4) -- (262.65,610.4) -- (262.65,627.9) ;
		\draw [shift={(262.65,629.9)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da9845654071455301] 
		\draw    (262.35,568.76) -- (262.15,589.9) -- (435.15,610.4) -- (435.15,628.4) ;
		\draw [shift={(435.15,630.4)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Rectangle [id:dp8516301519463569] 
		\draw  [fill={rgb, 255:red, 252; green, 243; blue, 76 }  ,fill opacity=1 ] (175,630) -- (350.87,630) -- (350.87,651.19) -- (175,651.19) -- cycle ;
		%Shape: Rectangle [id:dp7182188713144879] 
		\draw  [fill={rgb, 255:red, 74; green, 160; blue, 254 }  ,fill opacity=1 ] (350.87,630) -- (526.74,630) -- (526.74,651.19) -- (350.87,651.19) -- cycle ;
		
		% Text Node
		\draw (251,30.4) node [anchor=north west][inner sep=0.75pt]    {$G1$};
		% Text Node
		\draw (427,30.4) node [anchor=north west][inner sep=0.75pt]    {$D1$};
		% Text Node
		\draw (339,76.4) node [anchor=north west][inner sep=0.75pt]    {$f1$};
		% Text Node
		\draw (53,89) node [anchor=north west][inner sep=0.75pt]   [align=left] {iteration};
		% Text Node
		\draw (251,155.4) node [anchor=north west][inner sep=0.75pt]    {$G2$};
		% Text Node
		\draw (426,155.4) node [anchor=north west][inner sep=0.75pt]    {$D2$};
		% Text Node
		\draw (339,201.4) node [anchor=north west][inner sep=0.75pt]    {$f2$};
		% Text Node
		\draw (252,280.4) node [anchor=north west][inner sep=0.75pt]    {$G3$};
		% Text Node
		\draw (425,280.4) node [anchor=north west][inner sep=0.75pt]    {$D3$};
		% Text Node
		\draw (339,326.4) node [anchor=north west][inner sep=0.75pt]    {$f3$};
		% Text Node
		\draw (247,508.6) node [anchor=north west][inner sep=0.75pt]    {$G16$};
		% Text Node
		\draw (421,508.6) node [anchor=north west][inner sep=0.75pt]    {$D16$};
		% Text Node
		\draw (338,554.4) node [anchor=north west][inner sep=0.75pt]    {$f16$};
		% Text Node
		\draw (247,633.4) node [anchor=north west][inner sep=0.75pt]    {$G17$};
		% Text Node
		\draw (420,633.4) node [anchor=north west][inner sep=0.75pt]    {$D17$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[Feistel's scheme]{Feistel's scheme a little more realistic (source: ?)}
	\end{figure}
	The confusion function (\textbf{f}), which acts on the $32$-bit blocks, mixes the bits according to the following processes (see the figure below):
	\begin{itemize}
		\item First, it transforms the $32$-bit block into a $48$-bit block by duplicating certain bits ("expansion"). 

		\item Then, it adds to this block a $48$-bit ("token key") subkey extracted from the $56$-bit secret key 

		\item And then transforms each $6$-bit set into $4$ bits by local transformations (\textbf{S} transform)
	\end{itemize}
	The result is a $32$-bit block which is finally mixed according to a fixed permutation.
	
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/feistel_confusion_function_diagram.jpg}
		\caption{Feistel's scheme confusion function}
	\end{figure}
	
	\pagebreak
	\subsection{Public key encryption}
	In 11975 (holocene calendar), W. Diffie and M. E. Hellman revolutionized the science of cryptography by proving the existence of a protocol that could not be deciphered by an interceptor unless the interceptor had large computer resources. The most fascinating in their method - the principle of which is still in use in this early 121st century (holocene calendar) - is that the code used does not require to hide the chosen method and can be applied repeatedly without any modification (Kerckhoffs principle). At their time, they simply created the concept of "public-key cryptography", or "asymmetric cryptography" (which we mentioned earlier in this section), an invention that sparked the emergence of a dynamic academic and industrial community.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Contrary to what one might think, public key cryptography has not relegated secret key cryptography to oblivion, on the contrary: these two types of cryptography are most often used in hybrid cryptosystems where the authentication of published keys is performed by a "certification authority".
	\end{tcolorbox}
	Before describing the Diffie-Hellman protocol  in detail, let us recall that the protocol of exchange of the "secret keys" was not reliable at that time (and is still not today) as it was transiting between the interlocutors, the element making it possible to encrypt and therefore decrypt the messages. In addition, even if only one key were to travel, anyone with sufficient computing power could break the code. Hence the need to change (misfortune more!) Periodically the keys (cryptoperiod). At least two solution are therefore available to us:
	\begin{enumerate}
		\item Do not exchange any key (it is possible but it is quite long as we will see in the figure below)

		\item Exchange a secret key using a non-invertible mathematical function or at least very difficult to inverse (this is the Diffie-Hellman protocol that we will also see in a figure below).
	\end{enumerate}
	Therefore Public key cryptography systems often rely on cryptographic algorithms based on mathematical problems that currently admit no efficient solution—particularly those inherent in certain integer factorization, discrete logarithm, and elliptic curve relationships. Public key algorithms, unlike symmetric key algorithms, do not require a secure channel for the initial exchange of one (or more) secret keys between the parties.

	Because of the computational complexity of asymmetric encryption, it is usually used only for small blocks of data, typically the transfer of a symmetric encryption key (e.g. a session key). This symmetric key is then used to encrypt the rest of the potentially long message sequence. The symmetric encryption/decryption is based on simpler algorithms and is much faster.
	
	Let us see what the first solution is and its blatant disadvantage:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/public_key_principle.jpg}
		\caption[Principle of public key encryption]{Principle of public key encryption (source: ?)}
	\end{figure}
	Explanation: Alice and Bernard want to transmit a message on an unsecured line and without exchanging keys. To do this, Alice puts her letter in a chest that she closes with her key and sends it to Bernard. The latter returns the chest to Alice where he added his own padlock which he closed with his own key. When Alice receives the chest, she takes off her padlock and sends Bernard a chest that no longer includes Bernard's padlock closed with Bernard's key. The latter then only has to open the chest to read the letter. This operation is safe and does not require exchange of keys. On the other hand, it requires several paths (the process is represented by the first 4th transaction of the figure above).
	
	The principle of the public key must allow secure exchanges, without a secret key, in a single path. Bernard distributes widely copies of his public padlock. Alice gets one, but anyone could do the same. Alice places the message in the trunk and closes it with the Bernard code lock, then sends it the trunk (represented by the 5th transaction in the figure above). On receiving the trunk, Bernard can open the trunk, since he alone holds the key that opens this lock. The transfer is safe in one trip. In cryptography, the public key is equivalent to the code lock, which is available for example in directories, while the key that opens this lock is the private key, owned solely by their owner and never disclosed. The private and public keys (the so-named "key trousseau") are constructed from a supposed "one-way" mathematical function.

	Let's now see the second solution making use of public key according to the Diffie-Hellman protocol:
	
	\subsubsection{Diffie-Hellman protocol}
	As the name implies, a one-way function gives easily a result, but the reverse operation is very difficult. Finding such functions in the mathematical world seemed very arduous to mathematicians. How to imagine a function that is one-way for everyone, except for its creator who can reverse it through the knowledge of a particular information. Thus, W. Diffie and Hellman were the first to publicly propose a one-way function to solve the problem of agreeing on a common secret. The basic idea is to calculate values of the type:
	
	where $\alpha$ and $a$ are imposed as being integers and $p$ is a prime number.
	
	Mathematicians name this kind of operation a "\NewTerm{modular exponentiation}\index{modular exponentiation}" or "\NewTerm{discrete exponential}" and it is customary to denote the finite field of integers modulo $p$ (where $p$ is a prime number) by $\text{GF}(p)$ in honour of Évariste Galois.
	
	To explicate such a calculation (as a reminder of what was seen in the section of Number Theory ...), we raise a number $\alpha$ to the power of $a$, and then divide the result by a large prime number $p$ and we keep finally the remainder of this division (operation modulo $p$). If this remainder is denoted $r$ then we write this:
	
	Modular exponentiation similar to the one described above are considered easy to compute, even when the numbers involved are enormous. On the other hand, computing the "\NewTerm{discrete logarithm}\index{discrete logarithm}" – that is, the task of finding the exponent $a$ when given $\alpha$, $p$, and $r$ (i.e. $\alpha^a \mod p$) – is believed to be difficult. This one-way function behaviour makes modular exponentiation a candidate for use in cryptographic algorithms! In addition, one-way functions such as the one above from the modular arithmetic behave very irregularly as is shown in the table with the particular example below:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}$\pmb{a}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{\alpha^a=3^a}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{\alpha^3 \mod p=3^a \mod 7}$} \\ \hline
		$0$ & $1$ & $1$ \\ \hline
		$1$ & $3$ & $3$ \\ \hline
		$2$ & $9$ & $2$ \\ \hline
		$3$ & $27$ & $6$ \\ \hline
		$4$ & $81$ & $4$ \\ \hline
		$5$ & $243$ & $5$ \\ \hline
		$6$ & $729$ & $1$ \\ \hline
		$7$ & $217$ & $3$ \\ \hline
		$8$ & $6561$ & $2$ \\ \hline
		\end{tabular}
		\caption{Examples of modular exponentiation applications}
	\end{table}
	So even if it is easy to compute a discrete exponential, it is almost impossible to find the starting number $a$ from the result, especially when this modular function is applied to very large primes $p$.

	The reader can check this by playing with Maple 4.00b that can calculate the discrete logarithm as following:
	
	\texttt{>with(numtheory):\\
	>mlog(r,alpha,p);}
	
	Ok we know how to crypt message... But now how can we communicate messages to someone that should be able to uncrypt them? This is named the "\NewTerm{Diffie–Hellman key exchange}\index{Diffie–Hellman key exchange}" that is a specific method of securely exchanging cryptographic keys over a public channel and was one of the first public-key protocols as originally conceptualized by Ralph Merkle and named after Whitfield Diffie and Martin Hellman. D–H is one of the earliest practical examples of public key exchange implemented within the field of cryptography. Here is the idea of the protocol:
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|l|}
		\hline
		\cellcolor[HTML]{FFCCC9}\textbf{ALICE} & \cellcolor[gray]{0.75}\textbf{Public (Internet)} & \cellcolor[HTML]{34FF34}\textbf{BERNARD} \\ \hline
		\multicolumn{3}{|l|}{\parbox{13cm}{We choose an arbitrary common prime number $p=419$ and a common random number smaller than $p$: $\alpha=7$. These two values are assumed to be secret.}} \\ \hline
		\parbox{5cm}{Alice chooses a secret\\ random number: $a=178$} &  & \parbox{5cm}{Bernard chooses a secret\\ random number: $b=344$} \\ \hline
		\parbox{5cm}{With the number $a$ Alice generates the public element:\\
		$k_a=\alpha^a \mod(p)=181$} &  &  \parbox{5cm}{With the number $b$ Bob generates the public element:\\
		$k_b=\alpha^b \mod(p)=351$}\\ \hline
		\parbox{5cm}{The result is sent to Bernard:\\ $k_a=181$} & $\xrightarrow{\makebox[2cm]{}}$ & $k_a=181$ \\ \hline
		$k_b=351$  & $\xleftarrow{\makebox[2cm]{}}$ &  \parbox{5cm}{The result is sent to Alice:\\ $k_b=351$} \\ \hline
		\parbox{5cm}{The shared secret is then:\\ $K=(k_b)^a \mod(p)=493$} &  & \parbox{5cm}{The shared secret is then:\\ $K=(k_a)^b \mod(p)=493$} \\ \hline
		 \multicolumn{3}{|c|}{$\leftarrow$ The exchanges are then encrypted with the secret key $K$ $\rightarrow$} \\ \hline
		\end{tabular}
		 \caption{Example of key exchange following Diffie-Hellmann protocol}
	\end{table}
	The security of this protocol is computational. It is based on the assumption that with limited computing power and time, an opponent (spy) can not reverse the modular exponential function (by making use of the properties of the logarithms with the exponential functions as we saw in the section of Functional Analysis) and therefore can not find the secret $a$ from the exchanged elements. This computational difficulty is due to the fact that the computation time necessary for the inversion of a one-way function does not have an algorithmic complexity (\SeeChapter{see section of Numerical Methods page \pageref{algorithm complexity}}) polynomial but exponential with $p$.
	
	Alice and Bernard have calculated the same common secret: $493$. Then $493$ is used to encrypt the exchanged data (in practice, much larger numbers are used). The spy is supposed to be able to intervene only after the exchange of the common choice of $p$ and $\alpha$ (no man-in-the-middle attack!).

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	This protocol is vulnerable to a "man-in-the-middle attack" that is an attack where the attacker secretly relays and possibly alters the communication between two parties who believe they are directly communicating with each other. One example of man-in-the-middle attacks is active eavesdropping, in which the attacker makes independent connections with the victims and relays messages between them to make them believe they are talking directly to each other over a private connection, when in fact the entire conversation is controlled by the attacker. The attacker must be able to intercept all relevant messages passing between the two victims and inject new ones.
	\end{tcolorbox}
	The key $K$ is obtained by the fact that the power operation is compatible with the relation of equivalence modulo $p$ (\SeeChapter{see section Number Theory page \pageref{congruence}}) such that:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	We have:
	
	when with $w=2$ we have:
	
	but:
	
	If it is not clear let us write it differently:
	
	Indeed, for the first one: $5-2=3$ can be divided by $3$ and for the second one $25-4=21$ can be divided by $3$.
	\end{tcolorbox}
	Thus, since $x<p$, the second modulo below has no meaning, so we can write:
	
	identically:
	
	and therefore:
	
	Diffie-Hellman is a cornerstone of modern cryptography used for VPNs, HTTPS websites, email, and many other protocols. Bad implementation choices combined with advances in number theory mean real-world users of Diffie-Hellman are likely vulnerable to state-level attackers. 
	
	Despite these precautions, experts established a record at the beginning of the 121st century (holocene calendar) using a new algorithm, they succeeded in reversing the modular exponential function for a $p$-number of $120$ digits (about $400$ bits), using a Computer with four $525$ [MHz] processors. This record shows that the security of the protocol depends greatly on the constant progress made in the field of algorithmic complexity. Researchers estimate that breaking a single, common $1024$-bit prime would allow NSA (USA National Security Agency) to passively decrypt connections to two-thirds of VPNs and a quarter of all SSH servers globally. Breaking a second 1024-bit prime would allow passive eavesdropping on connections to nearly $20\%$ of the top million HTTPS websites. In other words, a one-time colossal investment in power-lifting computation would make it possible to eavesdrop on trillions of encrypted connections.
	
	The clever schema of Diffie-Hellman remains a schema of principle. Its main disadvantage is that it does not make it possible to provide the traditional security services: authentication of the two interveners, control of the integrity of the key and anti-replay (verification that information already transmitted is not re-transmitted ). It follows that an attacker can, for example, impersonate Alice by replacing the public element of Alice with her own public element. To overcome this disadvantage, secure versions of this generic protocol have been published, for example a protocol named "STS" (Station To Station), which uses, in particular, the electronic signature to ensure the authentication of the interveners (see below). This policy is the basis of the secured Internet connection (IPSec).
	
	The Diffie-Hellman protocol paved the way for a whole series of algorithms, that of "public key encryption" being the first. The idea was to break the symmetry of encryption and decryption by using one-way functions.
	
	\pagebreak
	\subsubsection{R.S.A system}
	Curiously, the first "\NewTerm{R.S.A. encryption system}\index{RSA encryption system}" is conceptually quite different from the Diffie-Hellman protocol: it does not use the discrete exponential, but the factorization of large numbers. This public key system was invented in 11977 (holocene calendar) by Ron Rivest, Adi Shamir, and Leonard Adleman (hence the abbreviation "R.S.A."). Having quickly become an international standard, the R.S.A. technique has been marketed by more than $400$ companies and we estimate that more than 400 million softwares use it. It is implemented in web browsers, such as Netscape Navigator, Microsoft Internet Explorer, or some bank smart cards, such as VISA cards.

	The R.S.A. system is based on the difficulty of factorizing large numbers and the one-way function used is a "power" function. The R.S.A. encryption protocol is divided into three phases:
	\begin{enumerate}
		\item Creation of keys (public and private)

		\item Encryption using the recipient's public key

		\item Decryption using the private key
	\end{enumerate}
	Its concept is based on a famous theorem named "\NewTerm{Euler's theorem}\index{Euler's theorem}" (nothing to do with the theorem of the same name seen in the section of Graph Theory or in the section of Geometric Shapes). Let's see what it is (be careful it is relatively long!).
	
	\paragraph{Euler's theorem}\mbox{}\\\\
	Before we see what Euler's theorem consists of, we must define two elements that are included in it. Apart the concept of congruence which we have already studied in the section of Number Theory (see page \pageref{euler indicator function}), there remains a special function named the "\NewTerm{Euler indicator}\index{Euler indicator}\label{euler indicator function cryptography}" or also named "\NewTerm{totient function}\index{totient function}" and defined in general by:
	
	In other words, the function $\phi$ of the integer $m$ results in a number $n$ strictly less than $m$, given by the number of elements between $1$ and $m$ whose greatest common divisor (\SeeChapter{see section Number Theory page \pageref{greatest common divisor}}) with $m$ is $1$. We have already given a practical example of the utility of this indicator function in the section of Number Theory in the framework of the reduced systems of residues and which are at the center of the proof of Euler's theorem.
	
	This can be formulated in the following form: the indicator $\phi$ of the integer $m$ is defined as the number of positive integers less than or equal to $m$ and prime with $m$.

	This function therefore has the remarkable property of counting the number of positive integers smaller than $m$ and "relatively prime" (ie, having greater common divider equal to $1$) with $m$.

	Here are some values of $\phi(m)$ for $m$ that range from $0$ to $19$:
	\begin{table}[]
		\centering
		\begin{tabular}{|l|l|c|c|c|c|c|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		$\pmb{\phi(m)}$ & $\pmb{0}$ & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{1}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{2}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{3}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{4}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{5}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{6}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{7}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{8}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{9}$} \\ \hline
		\cellcolor[gray]{0.75}$\pmb{0+}$ &  & $1$ & $1$ & $2$ & $2$ & $4$ & $2$ & $6$ & $4$ & $6$ \\ \hline
		\cellcolor[gray]{0.75}$\pmb{10+}$ & \multicolumn{1}{c|}{$4$} & $10$ & $4$ & $12$ & $6$ & $8$ & $8$ & $16$ & $6$ & $18$ \\ \hline
		\end{tabular}
		\caption{Some values of the Euler indicator $\phi(m)$}
	\end{table}
	Let us now introduce two properties of $\phi(m)$:
	\begin{enumerate}
		\item[P1.] We notice the (trivial) property of this function when we denote any prime number (remember that $1$ is not a prime number!) by the letter $p$ then:
		
		as it is highlighted by the table above.
	
		\item[P2.] The Euler indicator can also be written in the following form if $p$ and $q$ are relatively prime (this is the padlock of the R.S.A system which is more complicated than the simple multiplication of $p$ and $q$):
		
		this last relation can easily be verified (without proof) by taking some values from the preceding table (if we do it like Ramanujan...).
	\end{enumerate}
	\begin{theorem}
	This done, given $(a,m)=1$ (the greatest common divisor of $a$ and $m$, ie $a$ and $m$ are relatively prime), the "\NewTerm{Euler's theorem}\index{Euler's theorem}" says that if $m$ is a natural number and $a$ is relatively prime with $m$ then we have:
	
	in which we see the Euler indicator defined above. It is a rather surprising relation. Let's see if it works with $7$ and $2$ which are relatively prime between them:
	
	the remainder being indeed therefore equal to $1$ when we compute $64$ modulo $7$.
	\end{theorem}
	\begin{dem}
	Let us first recall (\SeeChapter{see section Number Theory page \pageref{system of reduced residue}}) that a reduced system of residuals modulo $m$ is a set of integers equation that satisfy the three properties:
	\begin{enumerate}
		\item[P1.] The remainder $r_i$ and $m$ are relatively prime, ie $(r_i,m)=1$

		\item[P2.] $r_i$ is not congruate $r_j$ modulo $m$ when $i\neq j$

		\item[P3.] Each integer $x$ relatively prime with $m$ is congruent to some $r_i$ modulo $m$
	\end{enumerate}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	For example, the set $\{1,5\}$ is a reduced system of residuals modulo $6$ or another example, $\{1,2,3,4,5,6\}$ is a reduced system of residuals modulo $7$. \\

	We also check for the first example that $1$ is not congruent $5$ modulo $6$ (indeed, $6$ does not divide $(5-1)$) and that $5$ which is relatively prime to $6$ is congruent to itself.\\

	For the second set, we notice that the cardinal of the set of residuals corresponds to the value of the Euler indicator for the number $7$.
	\end{tcolorbox}
	\begin{lemma}
	Thus, given $\{r_1,r_2,\ldots,r_{\phi(m)}\}$ a reduced system of residuals modulo $m$. We need for the proof of Euler's theorem, to prove beforehand the lemma that $\{ar_1,ar_2,\ldots,ar_{\phi(m)}\}$ is also a reduced system of residuals modulo $m$.
	\end{lemma}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	As we have already mentioned in the previous example, you can observe that the cardinality of the set of residuals corresponds, for a given prime modulo $m$, to the result defined by the property P1 of the Euler indicator function $\phi(m)$. This property is to this day only a "conjecture", that is to say, an assumption based on probabilities (because it seems it has not be proven so far!).
	\end{tcolorbox}
	For this, let us recall that by the property of a reduced system:
	
	and that by hypothesis:
	
	then we want the lemma that:
	
	is also satisfied.
	
	Let us put for this $d=1$ (by tradition ...). We then have since $(r_i,m)=d$ that $d|r_i$ and $d|m$ and identically for $(a,m)=d$ that $d|a$ and $d|m$. Now if $d$ divides well $a$ or $r_i$ in this case we have $d|r_i(a)$ or (equivalently) $d|a(r_i)$. Therefore $d|ar_i$ and $d|m$ which allows us to write:
	
	Let us return to our Euler's theorem... if the reader stills follow ... We have just proved that there is bijection between the two sets of residues. That is to say that for each residue $r_i$ of the reduced system modulo $m$, we will have a residue $ar_i$ of the reduced system modulo $m$ according to the fundamental property of the congruence which we recall says that: we can multiply the two members of a congruence by the same integer number and it will remain congruent modulo $m$ and modulo $m$ multiplied by this integer number.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us take:
	
	indeed:
	
	because the remainder of the division of $30$ by $6$ is indeed equal to zero. If we take for example:
	
	then we have:
	
	and the remainder is also zero...
	\end{tcolorbox}
	Let us make a recall on bijection (\SeeChapter{see section Set Theory page \pageref{bijection}}): We say that we have a "bijection", if to each element of a starting set corresponds one and only one element in the arrival set (if there was for every man on Earth only one woman - in equal proportions therefore - there would be a bijection between the set of Men and Women).

	In short, since there is bijection between the two sets of residues, we can write:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	The set $\{1,5\}$ is a reduced system of residuals modulo $6$ as we have already seen. So we have:
	
	We have then:
	
	If we take an $a$ such that $(a,m)=1$, for example $a=7$ because indeed $(7,6)=1$, then:
	
	because $6|(35-5=30)$. Indeed, $6$ divides well $30$ with a remainder equal to $0$.
	\end{tcolorbox}
	So let us return to our bijection, which can be written by the elementary rules of algebra:
	
	Since:
	
	(you can verify, but this is the very definition of a set of residues!), we are then obliged to conclude that:
	
	and anyway, even if it does not seem obvious to you, you just need to multiply each of the members of the equality of the congruence by:
	
	as permit us one of the intrinsic properties of congruence previously proved.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	This theoretical interlude being done, let us consider a number $N$ of which we wish to decide whether it is prime number or not.

	We know from the Euler theorem and of the property P1 of the Euler indicator that if $N$ is a prime number and if $a\in\mathbb{N}$, where $a<N$, then:
	
	which is named the "\NewTerm{Fermat's little theorem}\index{Fermat's little theorem}".
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	This relation follows from the properties we have presented in our proof of Euler's theorem:
	
	and of the property P1 of the function $\phi(m)$ for a prime number $p$:
	
	\end{tcolorbox}
	The Fermat's little theorem is however, also valid for some numbers $N$ which are not prime. But the numbers which check this without being prime are rare, and it is worthwhile to look for a more sophisticated algorithm to know if $N$ is really prime or not (we say that in this case, $N$ is a good candidate for primality and is then named "\NewTerm{pseudo-prime number}"). To test whether the number non-prime number $N$ is "sufficiently prime", we try with an algorithm to test the Fermat's little theorem a maximum number $a\in\mathbb{N}$ with $a<N$.
	
	According to the property of congruence (see above), we also have:
	
	We can apply this last theorem to a number $N$ on which we would like to know at best whether it is prime or not.

	There are a large number of other non-optimal methods for determining whether $N$ is prime; including preliminary division trials by $2$, $3$, $5$, $7$, $11$, $\ldots$ and small prime numbers up to $p\leq\sqrt{N}$ according to the method of the Eratosthenes screening which is best known method in high schools.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In fact, with the help of a fairly powerful computer, we can decide whether a natural number of the order of $10^{300}$ ($10$ followed by $300$ zeros) is first or not within a few minutes or seconds. What is important to know is that, given a natural number $N$, one can decide in relatively short time whether it is prime or not, without knowing however its prime factors!!
	\end{tcolorbox}
	However, according to the fundamental theorem of arithmetic we have that:
	\begin{theorem}
	Any natural number $N$ can be written as a product of prime numbers, and this representation is unique, apart from the order in which the prime factors are arranged.
	\end{theorem}
	The proof is already in the section of Number Theory but exceptionally we will reproduce it here as it is quite a short proof:
	\begin{dem}
	The proof uses Euclid's lemma (\SeeChapter{see section Number Theory page \pageref{euclid lemma}}): if a prime $p$ divides the product of two natural numbers $a$ and $b$, then either $p$ divides $a$ or $p$ divides $b$ (or both).
	
	If $N$ is prime, and therefore product of a unique prime integer, namely itself, the result is true and the proof is complete (say that a prime number is product of itself is obviously a misnomer!). Suppose that $n$ is not prime and therefore strictly greater than $1$ and consider the set:
	
	So, $D\subset \mathbb{N}$ and since $N$ is composite, we have that $D\neq \varnothing$. According to the principle of good order, $D$ has a smaller element $p_1$ that is prime, otherwise the minimum choice of $p_1$ is contradicted. We can the write $N=p_1N_1$. If $n_1$ is prime, then the proof is complete. If $n_1$ is also composite, then we repeat the same argument as before and we deduce the existence of a prime number $p_2$ and of an integer $N_2<N_1$, such as $N=p_1p_2N_2$. By continuing we come inevitably to the conclusion that $N_k$ will be prime.
	
	So finally we well show that any number can be decomposed into prime numbers factors with the principle of good order.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	So finally we have proved that any number is decomposable into prime factors using the principle of good order. There exist in the set of natural numbers $\mathbb{N}$, some which can be expressed by (or only by) two prime factors traditionally denoted $p$ and $q$. These are the numbers we use in public key cryptography according to the R.S.A. protocol.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	We do not know to this day a law that makes it possible to easily and quickly calculate the $i$-th prime factor $p_i$ of a number. In fact, even with the most powerful computers we had in year 12002 (holocene calendar) when we wrote these lines, it would take several years to find the two prime factors $p$ and $q$ of a "\NewTerm{RSA number}\index{RSA number}" $N=pq$ where $p$ and $q$ are of the order of $10^{100}$ each. And it seems unlikely that we will discover in the near future an algorithm sufficiently effective to improve appreciably this computing time. Note that it is possible to determine in less than $5$ minutes (in year 12002 according to holocene calendar) whether a number of $200$ digits is prime or not. However, to factorize a number of $200$ digits into two prime numbers, it would take at least $100$ years. Wonderful thing: the theories that allow these exploits are very deep and were developed partly long ago in a very different setting.\\
	
	Now in year 12009 (holocene calendar) a RSA number of $232$ digits (i.e. $768$ bit RSA number) was factorized ($7$ years after we wrote the lines above) in half a year on eighty 2.2 GHz AMD Opteron processors...
	\end{tcolorbox}
	The fact that it is much more difficult to find the prime factors of a number $N$ than to find out if $N$ is prime or compound is precisely what made it possible to develop this very ingenious method of encoding and decoding messages according to the RSA protocol.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us consider now a group of individuals who regularly transmit messages by e-mail and for which it is important that the messages are known only to the sender and the recipient. Then, the group member (here Alice) who wants to receive encrypted information, choose two very large prime numbers $p$ and $q$ of the order of $10^{100}$. To find such prime numbers, we randomly choose a number of $100$ digits and we check by one of the known algorithms whether it is prime or not and we repeat the experiment until we get a prime number. Once this is done with these two prime numbers, we compute the expression:
	
	named the "\NewTerm{modulus}".\\
	
	Then, Alice (who is the only one in possession of the number $N$ for the moment) who wants to receive the encrypted informations chooses a positive integer $a$ such (p.g.c.d.) that:
	
	So $a$ (often denoted $e$ in the literature) is a prime integer with $\phi(N)$ sometimes named the "\NewTerm{generator}".\\
	
	And as:
	
	Suppose a Alice wants to receive a message from Bob, one of her friends.\\

	Alice has therefore the "\NewTerm{public key}\index{public RSA key}", defined by the couple:
	
	to Bob.\\
	
	Bob receives the public key and wishes to send the french message: \textit{déclencher l'opération rouge}\footnote{In English: \textit{trigger the red operation}}. To do this, Bob first transforms the message into numbers by using the convention that each letter is replaced by its corresponding position in the alphabet starting counting from $01$ (the character "space" will be encrypted "$27$").\\

	Thus the clear message denoted $M$ afterwards becomes:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	For technical reason, $M$ and $N$ must have no common divisor other than $1$ (otherwise, a possible spy could reduce the problem of two very large numbers difficult to manipulate to that of smaller numbers, easier to manipulate). Otherwise, at the end of $M$, we add numbers without value, such as $01$ (for example), to finally have $M$ and $N$ without common divisor other than $1$.
	\end{tcolorbox}
	We can also break $M$ into pieces $M_i$ whose number of digits does not exceed $99$ (remember that we set a lower limit of a power of $100$ for $p$ and $q$ and that it would therefore suffice that one of the two prime numbers to be $1$ and the other exactly a number with an exponent $100$ to be at the limit of the number $N$ then comprising at worst 100 digits, even if this extreme example is quite bad for technical reasons as more easy to crack), in which case one will always have:
	
	We cut $M$ into pieces, each being smaller than $N$:
	
	and we work successively with each piece $M_1,M_2,\ldots,M_{12}$ of the message.\\
	
	We consider the power $a$ of $M_1$, that is, $M_1â$. We replace $M_1$ by the number $\bar{M}_1$, which is the remainder of the division by $N$ of the number $M_1^a$. The same procedure is followed for all other $M_i$ pieces such as:
	
	Then Bob sends the encoded message to Alice:
	
	An interceptor of the encoded message and of the public key, knowing the encryption algorithm, would therefore have to solve the problem of one equation with two unknowns (equation obtained simply from the mathematical expression of the encryption rule):
	
	Obviously unspecified problem!
	\end{tcolorbox}
	To see how the receiver decrypts the message, we need an additional mathematical tool.

	Let us recall that the receiver chooses $a$ such that $(a,\phi(N))=1$, which implies, according to the Bézout's theorem\index{Bézout's theorem} (\SeeChapter{see section Number Theory page \pageref{bezout theorem}}), that if $a$ and $\phi(N)$ are relatively prime (that is to say for recall that their greatest common divisor is $1$) there exist integers $x$ and $y$ such that (we can assume that $x>0$, in which case $y<0$):
	
	or otherwise written:
	
	This is how we will determine the value of $x$ (we must use algorithms to find the solution $x$ to this equation).
	
	Which means:
	\begin{enumerate}
		\item If $a$ is prime with $\phi(N)$ then by the properties of congruence it is also prime with $p-1$ and $q-1$.
	
		\item That $a$ is invertible modulo $\phi(N)$
	
		Indeed, because:
		
		And according to the definition of congruence ($m|(a-b)$) we have:
		
		since $\phi(N)$ divides the right-hand side of $ax-1=\phi(N)y$ and therefore by the equality, the left-hand member. Therefore:
		
	\end{enumerate}
	Only the receiver of the message, can easily calculate the number $x\le a$ used above. In order to do this, it is necessary to be able to calculate the value of $\phi(N)$ and thus know $p$ and $q$.

	If $M_i$ is the original message (its numerical value) and $\bar{M}_i$ is the received encoded message (its numeric value), then we have the following relation:
	
	This is completely logical since the difference $M_i^a-\bar{M}_i$, where for recall, $\bar{M}_i$ is the remainder of the division of $M_i^a$ by $N$, can therefore only be divisible by $N$.
	
	Alice thus receives the coded message $\bar{M}$ and raise to the power of $x$ the numbers $\bar{M}_i$ and thus obtains the initial message.

	Indeed, she will apply for each $\bar{M}_i$ the following mathematical property of congruence:
	
	The "\NewTerm{private key}\index{private RSA Key}" (allowing to decrypt the message and which can be easily known only by the Alice) is thus defined by the couple:
	
	Let us give more indeed explanations about what we have stated just above! We have showed that:
	
	and from the property of symmetry of congruence (\SeeChapter{see section Numbers page \pageref{congruence}}), we can write:
	
	Now we can write:
	
	according to the second principal property of congruence, which says for recall that the two members of a congruence can be elevated to the same power! That latter relation can also be written (application of Bézout's theorem):
	
	Remains to prove that:
	
	where we can write $M_i^{1-\phi(N)y}$ under the form:
	
	Now, remember that we have proved Euler's theorem:
	
	and that one of the properties of congruence gives us the right to elevate to any power the two members of the congruence such as:
	
	But as 1 raised to any power makes $1$, we have:
	
	This last relation allows us to verify that we can authorize ourselves to write:
	
	since the two left members are well modulos $N$. 
	
	So if we sum up all this, Alice receives a piece $\bar{M}_i$ and raises it automatically to the power $x$ to obtain a number which according to her should be the true $M_i$. To be sure, it applies the verification:
	
	It is easy to see that any interceptor can not decode and in addition verify if the decoding is indeed the right one, because for this it should know the value of $x$, which in turn depends on $\phi(N)$, that it does not know either, because he does not know the prime factors of $N$ that are $p$ and $q$.

	It is customary to say that the RSA system uses the numbers $p$, $q$ (secrets), $N$ (public), $a$ (public) and $x$ (secret). The whole being summed up by the triplet $\{n, a, x\}$ denoted sometimes in the literature $\{n, e, d\}$.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/arithmetics/rsa_detailed_cyphering.jpg}
		\caption{Principle of RSA public key encryption}
	\end{figure}
	And here is a small practical application with Maple 4.00b:
	
	\texttt{> \#Initialization of the Maple 4.00b random generator\\
	> randomize():\\
	> \#definition of the desired size for N (this is an even number)\\
	> t:=30:\\
	> \#Generation of two integers of t/2 bits size\\
	> x:=rand(2\string^(t/2-1)..2\string^(t/2))();\\
	> y:=rand(2\string^(t/2-1)..2\string^(t/2))();\\
	> \#Calculation of the following prime numbers\\
	> p:=nextprime(x);\\
	> q:=nextprime(y);\\
	> \#Generation of the RSA key\\
	> n:=p*q;\\
	> phi:=(p-1)*(q-1);\\
	> \#We choose "a" empirically\\
	> a:=65537;\\
	> \#We check that it is prime with phi\\
	> igcd(a,phi);\\
	> \#we calculate the inverse of "a" modulo phi\\
	> x:=1/a mod phi;\\
	> \#we choose a message a being "1234"\\
	> m:=1234;\\
	> \#we cypher\\
	> c:=m\&\string^a mod n;\\
	> \#we decode\\
	> c\&\string^x mod n;\\
	}

	Following the request of a reader here is a literal summary of what we have seen so far for the first steps of the algorithm above with practical value and a given message:
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	We want to cypher the message $M=314158$.

	\begin{enumerate} 
		\item We choose $p$ and $q$ prime and sufficiently large:
		
		we then have:
		
	
		\item We compute the Euler indicator:
		
	
		\item We choose the generator $a$ such that:
		
		and for this we will take $a=5$. The pair $(a,N)$ is the public key (can be distributed to everyone for a specified time).
	
		\item Then we calculate:
		
		So the pair $(x, a)$ is the private key (to be kept secret).
		
		\item Now we cypher with:
		
		Therefore:
		
		
		\item Now to decipher (the exponent calculation cannot be done with sample spreadsheet softwares or simple online scientific calculator):
		
	\end{enumerate}
	\end{tcolorbox}
	For security reasons, public key cryptography is used in conjunction with secret key cryptography. For example, at the time of writing these lines, the SSL protocol for Internet pages uses the RSA to exchange a secret key (symmetric system) and then encrypts the data using a conventional symmetric algorithm.

	Let us conclude this brief presentation of the messages cyphering by informing the reader that the US government (and not only...!) closely monitors the activities of mathematicians who work on the factorization of large numbers. Indeed, if one of them could find an algorithm allowing to factorize in a short time a number of two hundred digits (greater than $524$ bits unsigned), this would jeopardize the secret nature of several communications of a military order. In fact, this surveillance has raised a protest by the scientists in the United States, who see their professional freedom undermined (Notices of American Mathematical Society, January 11983 according to holocene calendar).

	For technical information, the software PGP (Pretty Good Privacy) published by the MIT (Massachusetts Institute of Technology), uses a RSA encryption system.
	
	\pagebreak

	
	\pagebreak
	\subsection{Hash functions}
	A "\NewTerm{hash function}\index{hash function}" is a function that associates to a big set a much smaller set (of the order of a few hundred bits) that is characteristic of the starting. This property makes it very used in computing, in particular for quick access to data thanks to "hash tables" or to check the result of huge data transmission (downloads). Indeed, a hash function makes it possible to associate a particular integer with a string. Thus, if we know the fingerprint of the stored character strings, we can quickly check whether a string is in this table (in $\mathcal{O}(1)$ if the hash function is good enough). Hash functions are also extremely useful in cryptography to speed up encryption.

	The two most commonly used condensation algorithms at the beginning of the 121st century (holocene calendar) are the "Secure Hash Algorithm (SHA)", which calculates a $160$-bit summary, and the MD5 (Message Digest 5 - Run Rivest 11992 according to holocene calendar), which calculates a $128$-bit summary named "Message Digest".
	
	\subsubsection{MD5 message digest condensation function}\label{md5}
	This "\NewTerm{Message Digest MD5}\index{message Digest MD5}" algorithm is (was) used mainly for digital signatures (notion used, when validating certificates of authenticity as we will see later) but as it has been found to suffer from extensive vulnerabilities\footnote{In 12004 (holocene calendar) it was shown that MD5 is not collision-resistant. As such, MD5 is not suitable for applications like SSL certificates or digital signatures that rely on this property for digital security.}. It can still be used as a checksum to verify data integrity, but only against unintentional corruption.
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,896); %set diagram left start at 0, and has height of 896
		
		%Shape: Rectangle [id:dp990747207019278] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 224; green, 224; blue, 224 }  ,fill opacity=1 ][line width=0.75]  (95,43) -- (527.5,43) -- (527.5,135) -- (95,135) -- cycle ;
		%Shape: Rectangle [id:dp4654094302446825] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 224; green, 224; blue, 224 }  ,fill opacity=1 ][line width=0.75]  (95,148) -- (527.5,148) -- (527.5,240) -- (95,240) -- cycle ;
		%Shape: Rectangle [id:dp3428274709712049] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 224; green, 224; blue, 224 }  ,fill opacity=1 ][line width=0.75]  (95,253) -- (527.5,253) -- (527.5,345) -- (95,345) -- cycle ;
		%Shape: Rectangle [id:dp9871353127261975] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 104; green, 197; blue, 3 }  ,fill opacity=1 ] (202,53.5) -- (344.5,53.5) -- (344.5,122.5) -- (202,122.5) -- cycle ;
		%Shape: Rectangle [id:dp9847592612206215] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 104; green, 197; blue, 3 }  ,fill opacity=1 ] (202,265) -- (344.5,265) -- (344.5,334) -- (202,334) -- cycle ;
		%Shape: Rectangle [id:dp5836228209757133] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 104; green, 197; blue, 3 }  ,fill opacity=1 ] (369.13,265) -- (511.63,265) -- (511.63,334) -- (369.13,334) -- cycle ;
		%Shape: Rectangle [id:dp14495841043696012] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 104; green, 197; blue, 3 }  ,fill opacity=1 ] (369.13,53.5) -- (511.63,53.5) -- (511.63,122.5) -- (369.13,122.5) -- cycle ;
		%Flowchart: Preparation [id:dp9897571022095042] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 95; green, 167; blue, 255 }  ,fill opacity=1 ] (203.13,194) -- (229.42,160.5) -- (317.08,160.5) -- (343.38,194) -- (317.08,227.5) -- (229.42,227.5) -- cycle ;
		%Flowchart: Preparation [id:dp8724354746273861] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 95; green, 167; blue, 255 }  ,fill opacity=1 ] (370.25,194) -- (396.55,160.5) -- (484.2,160.5) -- (510.5,194) -- (484.2,227.5) -- (396.55,227.5) -- cycle ;
		%Straight Lines [id:da042872822665175114] 
		\draw    (273.25,122) -- (273.25,160) ;
		\draw [shift={(273.25,162)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da6283846960592683] 
		\draw    (273.25,228) -- (273.25,264) ;
		\draw [shift={(273.25,266)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da9558412027436478] 
		\draw    (440.38,121) -- (440.38,158) ;
		\draw [shift={(440.38,160)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da5454887711671814] 
		\draw    (440.38,227) -- (440.38,264) ;
		\draw [shift={(440.38,266)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		
		% Text Node
		\draw (102,81) node [anchor=north west][inner sep=0.75pt]   [align=left] {INPUT};
		% Text Node
		\draw (102,188) node [anchor=north west][inner sep=0.75pt]   [align=left] {PROCESS};
		% Text Node
		\draw (102,295) node [anchor=north west][inner sep=0.75pt]   [align=left] {OUPUT};
		% Text Node
		\draw (211,65) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{100pt}\setlength\topsep{0pt}
		\begin{center}
		\textcolor[rgb]{1,1,1}{John Smith agrees}\\\textcolor[rgb]{1,1,1}{to pay \$ $500$/month}\\\textcolor[rgb]{1,1,1}{for rent}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (370,64) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{110pt}\setlength\topsep{0pt}
		\begin{center}
		\textcolor[rgb]{1,1,1}{John Smith agrees}\\\textcolor[rgb]{1,1,1}{to pay }\textcolor[rgb]{1,1,1}{\$$5,000$}\textcolor[rgb]{1,1,1}{/month}\\\textcolor[rgb]{1,1,1}{for rent}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (239,180) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{60pt}\setlength\topsep{0pt}
		\begin{center}
		\textcolor[rgb]{1,1,1}{MD5 hash}\\\textcolor[rgb]{1,1,1}{algorithm}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (405,180) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{60pt}\setlength\topsep{0pt}
		\begin{center}
		\textcolor[rgb]{1,1,1}{MD5 hash}\\\textcolor[rgb]{1,1,1}{algorithm}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (211,287) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,opacity=1 ] [align=left] {\begin{minipage}[lt]{87.82pt}\setlength\topsep{0pt}
		\begin{center}
		ac49e74434a64c2\\47aa129bef83f204
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (378,287) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,opacity=1 ] [align=left] {\begin{minipage}[lt]{87.82pt}\setlength\topsep{0pt}
		\begin{center}
		b68e2f019ef60266\\8f8ebf4eb6e3a69b
		\end{center}
		\end{minipage}};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Illustrated result of MD5 algorithm}
	\end{figure}
	Here are the different stages of its operation:
	\begin{enumerate}
		\item Completion:
		
		The message consists of $b$ bits. The message is completed with a $1$, and sufficiently enough $0$ for the extended message to have a multiple length of $512$ bits. After this initial processing, the input text is manipulated in blocks of $512$ bits divided into $16$ sub-blocks \texttt{M[i]} of $32$ bits.
		
		\item Initialization:
		
		We define the $32$-bit "chaining variables" \texttt{A}, \texttt{B}, \texttt{C} and \texttt{D} initialized as follows (the digits are hexadecimal):
		\begin{center}
			\texttt{A=01234567}, \texttt{B=89ABCDEF}, \texttt{C=FEDCBA98}, \texttt{D=76543210}
		\end{center}
		We also define four non-linear functions \texttt{F}, \texttt{G}, \texttt{H} and \texttt{I} which take arguments coded on $32$ bits, and return a value on $32$ bits, the operations taking place bit by bit.
	
		\texttt{F(X,Y,Z) = (X AND Y) OR (NOT (X) AND Z)}\\
		\texttt{G(X,Y,Z) = (X AND Z) OR (Y AND NOT (Z))}\\
		\texttt{H(X,Y,Z) = X XOR Y XOR Z}\\
		\texttt{I(X,Y,Z) = Y XOR (X OR NOT (Z))}
	
		What is important with these four functions is that if the bits of their arguments \texttt{X}, \texttt{Y} and \texttt{Z} are independent, the resulting bits are also independent.
		
		\item Iterative calculation:
		
		The main loop has $4$ rounds (see figure below) which each use a different non-linear function (hence the fact that there are $4$ rounds). Each round therefore consists of $16$ executions of an operation (because there are $16$ sub-blocks).

		Each operation calculates a non-linear function of three of the variables \texttt{A}, \texttt{B}, \texttt{C} and \texttt{D}, adds to it a sub-block $M[i]$ of the text to be encrypted, a predefined constant $s$ (encoded on $32$ bits) and to a circular shift on the left of a variable number of bits $n$. Here is the example for \texttt{A}:
		\begin{itemize}
			\item \texttt{A = B + A + F(B,C,D) + M[i] + s} circularly offseted from $n$ bits to the left
			\item \texttt{A = B + A + G(B,C,D) + M[i] + s} circularly offseted from $n$ bits to the left
			\item \texttt{A = B + A + H(B,C,D) + M[i] + s} circularly offseted from $n$ bits to the left
			\item \texttt{A = B + A + I(B,C,D) + M[i] + s} circularly offseted from $n$ bits to the left
		\end{itemize}
		This new value of \texttt{A} is then summed with the old one.
		
		\item Writing of the summary (fingerprint):
		
		The $128$-bit summary is obtained by putting end-to-end the four $32$-bit chaining variables \texttt{A}, \texttt{B}, \texttt{C}, \texttt{D} obtained at the end of the iteration.
	\end{enumerate}
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1467); %set diagram left start at 0, and has height of 1467
		
		%Rounded Rect [id:dp8565073628577151] 
		\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (177.5,28.4) .. controls (177.5,25.42) and (179.92,23) .. (182.9,23) -- (427.1,23) .. controls (430.08,23) and (432.5,25.42) .. (432.5,28.4) -- (432.5,44.6) .. controls (432.5,47.58) and (430.08,50) .. (427.1,50) -- (182.9,50) .. controls (179.92,50) and (177.5,47.58) .. (177.5,44.6) -- cycle ;
		%Rounded Rect [id:dp8741863333887494] 
		\draw  [fill={rgb, 255:red, 216; green, 232; blue, 253 }  ,fill opacity=1 ] (197.5,81.4) .. controls (197.5,78.42) and (199.92,76) .. (202.9,76) -- (407.1,76) .. controls (410.08,76) and (412.5,78.42) .. (412.5,81.4) -- (412.5,97.6) .. controls (412.5,100.58) and (410.08,103) .. (407.1,103) -- (202.9,103) .. controls (199.92,103) and (197.5,100.58) .. (197.5,97.6) -- cycle ;
		%Rounded Rect [id:dp8724137694916627] 
		\draw  [fill={rgb, 255:red, 219; green, 219; blue, 219 }  ,fill opacity=1 ] (180.5,180) .. controls (180.5,173.92) and (185.42,169) .. (191.5,169) -- (418.5,169) .. controls (424.58,169) and (429.5,173.92) .. (429.5,180) -- (429.5,213) .. controls (429.5,219.08) and (424.58,224) .. (418.5,224) -- (191.5,224) .. controls (185.42,224) and (180.5,219.08) .. (180.5,213) -- cycle ;
		%Rounded Rect [id:dp26425802213232186] 
		\draw  [fill={rgb, 255:red, 219; green, 219; blue, 219 }  ,fill opacity=1 ] (180.5,270) .. controls (180.5,263.92) and (185.42,259) .. (191.5,259) -- (418.5,259) .. controls (424.58,259) and (429.5,263.92) .. (429.5,270) -- (429.5,303) .. controls (429.5,309.08) and (424.58,314) .. (418.5,314) -- (191.5,314) .. controls (185.42,314) and (180.5,309.08) .. (180.5,303) -- cycle ;
		%Rounded Rect [id:dp9116067577618263] 
		\draw  [fill={rgb, 255:red, 219; green, 219; blue, 219 }  ,fill opacity=1 ] (180.5,359) .. controls (180.5,352.92) and (185.42,348) .. (191.5,348) -- (418.5,348) .. controls (424.58,348) and (429.5,352.92) .. (429.5,359) -- (429.5,392) .. controls (429.5,398.08) and (424.58,403) .. (418.5,403) -- (191.5,403) .. controls (185.42,403) and (180.5,398.08) .. (180.5,392) -- cycle ;
		%Rounded Rect [id:dp7778583026468096] 
		\draw  [fill={rgb, 255:red, 219; green, 219; blue, 219 }  ,fill opacity=1 ] (180.5,449) .. controls (180.5,442.92) and (185.42,438) .. (191.5,438) -- (418.5,438) .. controls (424.58,438) and (429.5,442.92) .. (429.5,449) -- (429.5,482) .. controls (429.5,488.08) and (424.58,493) .. (418.5,493) -- (191.5,493) .. controls (185.42,493) and (180.5,488.08) .. (180.5,482) -- cycle ;
		%Rounded Rect [id:dp5921702170275418] 
		\draw  [fill={rgb, 255:red, 219; green, 219; blue, 219 }  ,fill opacity=1 ] (224.5,566.7) .. controls (224.5,562.72) and (227.72,559.5) .. (231.7,559.5) -- (379.3,559.5) .. controls (383.28,559.5) and (386.5,562.72) .. (386.5,566.7) -- (386.5,588.3) .. controls (386.5,592.28) and (383.28,595.5) .. (379.3,595.5) -- (231.7,595.5) .. controls (227.72,595.5) and (224.5,592.28) .. (224.5,588.3) -- cycle ;
		%Rounded Rect [id:dp9068092252666873] 
		\draw  [fill={rgb, 255:red, 80; green, 227; blue, 194 }  ,fill opacity=1 ] (201,666.6) .. controls (201,664.06) and (203.06,662) .. (205.6,662) -- (404.4,662) .. controls (406.94,662) and (409,664.06) .. (409,666.6) -- (409,680.4) .. controls (409,682.94) and (406.94,685) .. (404.4,685) -- (205.6,685) .. controls (203.06,685) and (201,682.94) .. (201,680.4) -- cycle ;
		%Rounded Rect [id:dp08815315571901272] 
		\draw  [fill={rgb, 255:red, 184; green, 233; blue, 134 }  ,fill opacity=1 ] (247.5,571.8) .. controls (247.5,569.7) and (249.2,568) .. (251.3,568) -- (263.7,568) .. controls (265.8,568) and (267.5,569.7) .. (267.5,571.8) -- (267.5,583.2) .. controls (267.5,585.3) and (265.8,587) .. (263.7,587) -- (251.3,587) .. controls (249.2,587) and (247.5,585.3) .. (247.5,583.2) -- cycle ;
		%Rounded Rect [id:dp4708880386740113] 
		\draw  [fill={rgb, 255:red, 184; green, 233; blue, 134 }  ,fill opacity=1 ] (280.83,571.8) .. controls (280.83,569.7) and (282.53,568) .. (284.63,568) -- (297.03,568) .. controls (299.13,568) and (300.83,569.7) .. (300.83,571.8) -- (300.83,583.2) .. controls (300.83,585.3) and (299.13,587) .. (297.03,587) -- (284.63,587) .. controls (282.53,587) and (280.83,585.3) .. (280.83,583.2) -- cycle ;
		%Rounded Rect [id:dp8809171896695771] 
		\draw  [fill={rgb, 255:red, 184; green, 233; blue, 134 }  ,fill opacity=1 ] (314.16,571.8) .. controls (314.16,569.7) and (315.86,568) .. (317.96,568) -- (330.36,568) .. controls (332.46,568) and (334.16,569.7) .. (334.16,571.8) -- (334.16,583.2) .. controls (334.16,585.3) and (332.46,587) .. (330.36,587) -- (317.96,587) .. controls (315.86,587) and (314.16,585.3) .. (314.16,583.2) -- cycle ;
		%Rounded Rect [id:dp6042629640777397] 
		\draw  [fill={rgb, 255:red, 184; green, 233; blue, 134 }  ,fill opacity=1 ] (347.5,571.8) .. controls (347.5,569.7) and (349.2,568) .. (351.3,568) -- (363.7,568) .. controls (365.8,568) and (367.5,569.7) .. (367.5,571.8) -- (367.5,583.2) .. controls (367.5,585.3) and (365.8,587) .. (363.7,587) -- (351.3,587) .. controls (349.2,587) and (347.5,585.3) .. (347.5,583.2) -- cycle ;
		%Straight Lines [id:da9540335415450065] 
		\draw    (177,37) -- (125.5,37) ;
		%Straight Lines [id:da16669254765372] 
		\draw    (125.5,459) -- (125.5,37) ;
		%Straight Lines [id:da3309498325673017] 
		\draw    (178,197) -- (125.5,197) ;
		\draw [shift={(180,197)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da35365561424278735] 
		\draw    (178,286) -- (125.5,286) ;
		\draw [shift={(180,286)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da3772966282477257] 
		\draw    (178,373) -- (125.5,373) ;
		\draw [shift={(180,373)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da9923246315267251] 
		\draw    (178,459) -- (125.5,459) ;
		\draw [shift={(180,459)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da9151349453460844] 
		\draw    (464,97.6) -- (412.5,97.6) ;
		%Straight Lines [id:da04178767397838534] 
		\draw    (464,573) -- (464,97.6) ;
		%Straight Lines [id:da9534849472767726] 
		\draw    (389.5,573) -- (438.5,573) -- (464,573) ;
		\draw [shift={(387.5,573)}, rotate = 0] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da1913866128710806] 
		\draw    (497.5,672.6) -- (408.5,672.6) ;
		%Straight Lines [id:da04009817036866892] 
		\draw    (497.5,672.6) -- (497.5,83) ;
		%Straight Lines [id:da5157550326477294] 
		\draw    (414.5,83) -- (497.5,83) ;
		\draw [shift={(412.5,83)}, rotate = 0] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da42560581173468814] 
		\draw    (256.5,165) -- (256.5,103) ;
		\draw [shift={(256.5,167)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da22981872915430301] 
		\draw    (251.5,142) -- (261.5,129) ;
		
		%Straight Lines [id:da6391154649211612] 
		\draw    (288.5,165) -- (288.5,103) ;
		\draw [shift={(288.5,167)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da2605445038726706] 
		\draw    (283.5,142) -- (293.5,129) ;
		
		%Straight Lines [id:da7924101316508347] 
		\draw    (320.5,165) -- (320.5,103) ;
		\draw [shift={(320.5,167)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da4152812811618698] 
		\draw    (315.5,142) -- (325.5,129) ;
		
		%Straight Lines [id:da9859021607071821] 
		\draw    (352.5,165) -- (352.5,103) ;
		\draw [shift={(352.5,167)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da5921835347406124] 
		\draw    (347.5,142) -- (357.5,129) ;
		%Straight Lines [id:da026555101903535494] 
		\draw    (256.5,255) -- (256.5,224) ;
		\draw [shift={(256.5,257)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da041479803358698275] 
		\draw    (288.83,255) -- (288.83,224) ;
		\draw [shift={(288.83,257)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da0727486541012583] 
		\draw    (321.16,255) -- (321.16,224) ;
		\draw [shift={(321.16,257)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da6003289167779573] 
		\draw    (353.5,255) -- (353.5,224) ;
		\draw [shift={(353.5,257)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da6302365950509461] 
		\draw    (256.5,344) -- (256.5,313) ;
		\draw [shift={(256.5,346)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da14700907558656495] 
		\draw    (288.83,344) -- (288.83,313) ;
		\draw [shift={(288.83,346)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da9075507431410927] 
		\draw    (321.16,344) -- (321.16,313) ;
		\draw [shift={(321.16,346)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da4762674216682572] 
		\draw    (353.5,344) -- (353.5,313) ;
		\draw [shift={(353.5,346)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da6324270629112831] 
		\draw    (257.5,434) -- (257.5,403) ;
		\draw [shift={(257.5,436)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da7236671015391656] 
		\draw    (289.83,434) -- (289.83,403) ;
		\draw [shift={(289.83,436)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da18313112401870146] 
		\draw    (322.16,434) -- (322.16,403) ;
		\draw [shift={(322.16,436)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da1782503994218949] 
		\draw    (354.5,434) -- (354.5,403) ;
		\draw [shift={(354.5,436)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da6858993773880988] 
		\draw    (256.5,555) -- (256.5,493) ;
		\draw [shift={(256.5,557)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da0963131670247741] 
		\draw    (251.5,532) -- (261.5,519) ;
		%Straight Lines [id:da6080989720210652] 
		\draw    (288.5,555) -- (288.5,493) ;
		\draw [shift={(288.5,557)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da9726538768107025] 
		\draw    (283.5,532) -- (293.5,519) ;
		%Straight Lines [id:da6007787364372505] 
		\draw    (320.5,555) -- (320.5,493) ;
		\draw [shift={(320.5,557)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da9575899947257001] 
		\draw    (315.5,532) -- (325.5,519) ;
		%Straight Lines [id:da5149450175375789] 
		\draw    (352.5,555) -- (352.5,493) ;
		\draw [shift={(352.5,557)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da8458543905290374] 
		\draw    (347.5,532) -- (357.5,519) ;
		%Straight Lines [id:da007523697880791991] 
		\draw    (256.5,658) -- (256.5,596) ;
		\draw [shift={(256.5,660)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da11460276921138357] 
		\draw    (251.5,635) -- (261.5,622) ;
		%Straight Lines [id:da3060806183508129] 
		\draw    (288.5,658) -- (288.5,596) ;
		\draw [shift={(288.5,660)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da6882032021602214] 
		\draw    (283.5,635) -- (293.5,622) ;
		%Straight Lines [id:da3973098526535015] 
		\draw    (320.5,658) -- (320.5,596) ;
		\draw [shift={(320.5,660)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da6624484639876218] 
		\draw    (315.5,635) -- (325.5,622) ;
		%Straight Lines [id:da8040254125168924] 
		\draw    (352.5,658) -- (352.5,596) ;
		\draw [shift={(352.5,660)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da48696864811686313] 
		\draw    (347.5,635) -- (357.5,622) ;
		
		% Text Node
		\draw (220.5,30) node [anchor=north west][inner sep=0.75pt]   [align=left] {Message block ($\displaystyle 512$ bits)};
		% Text Node
		\draw (242.5,82) node [anchor=north west][inner sep=0.75pt]   [align=left] {Initialization vector};
		% Text Node
		\draw (263,188) node [anchor=north west][inner sep=0.75pt]   [align=left] {16 iterations};
		% Text Node
		\draw (263,278) node [anchor=north west][inner sep=0.75pt]   [align=left] {16 iterations};
		% Text Node
		\draw (263,367) node [anchor=north west][inner sep=0.75pt]   [align=left] {16 iterations};
		% Text Node
		\draw (263,457) node [anchor=north west][inner sep=0.75pt]   [align=left] {16 iterations};
		% Text Node
		\draw (242,666) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 128$ bits fingerprint};
		% Text Node
		\draw (241,106.4) node [anchor=north west][inner sep=0.75pt]    {$A$};
		% Text Node
		\draw (244,130.4) node [anchor=north west][inner sep=0.75pt]  [font=\tiny]  {$32$};
		% Text Node
		\draw (273,106.4) node [anchor=north west][inner sep=0.75pt]    {$B$};
		% Text Node
		\draw (276,130.4) node [anchor=north west][inner sep=0.75pt]  [font=\tiny]  {$32$};
		% Text Node
		\draw (305,106.4) node [anchor=north west][inner sep=0.75pt]    {$C$};
		% Text Node
		\draw (308,130.4) node [anchor=north west][inner sep=0.75pt]  [font=\tiny]  {$32$};
		% Text Node
		\draw (241,226.4) node [anchor=north west][inner sep=0.75pt]    {$A$};
		% Text Node
		\draw (274,226.4) node [anchor=north west][inner sep=0.75pt]    {$B$};
		% Text Node
		\draw (306,226.4) node [anchor=north west][inner sep=0.75pt]    {$C$};
		% Text Node
		\draw (337,226.4) node [anchor=north west][inner sep=0.75pt]    {$D$};
		% Text Node
		\draw (241,315.4) node [anchor=north west][inner sep=0.75pt]    {$A$};
		% Text Node
		\draw (274,315.4) node [anchor=north west][inner sep=0.75pt]    {$B$};
		% Text Node
		\draw (306,315.4) node [anchor=north west][inner sep=0.75pt]    {$C$};
		% Text Node
		\draw (337,315.4) node [anchor=north west][inner sep=0.75pt]    {$D$};
		% Text Node
		\draw (242,405.4) node [anchor=north west][inner sep=0.75pt]    {$A$};
		% Text Node
		\draw (275,405.4) node [anchor=north west][inner sep=0.75pt]    {$B$};
		% Text Node
		\draw (307,405.4) node [anchor=north west][inner sep=0.75pt]    {$C$};
		% Text Node
		\draw (338,405.4) node [anchor=north west][inner sep=0.75pt]    {$D$};
		% Text Node
		\draw (238,533.4) node [anchor=north west][inner sep=0.75pt]    {$A$};
		% Text Node
		\draw (244,519.4) node [anchor=north west][inner sep=0.75pt]  [font=\tiny]  {$32$};
		% Text Node
		\draw (270,533.4) node [anchor=north west][inner sep=0.75pt]    {$B$};
		% Text Node
		\draw (276,520.4) node [anchor=north west][inner sep=0.75pt]  [font=\tiny]  {$32$};
		% Text Node
		\draw (302,533.4) node [anchor=north west][inner sep=0.75pt]    {$C$};
		% Text Node
		\draw (308,520.4) node [anchor=north west][inner sep=0.75pt]  [font=\tiny]  {$32$};
		% Text Node
		\draw (334,533.4) node [anchor=north west][inner sep=0.75pt]    {$D$};
		% Text Node
		\draw (340,520.4) node [anchor=north west][inner sep=0.75pt]  [font=\tiny]  {$32$};
		% Text Node
		\draw (238,636.4) node [anchor=north west][inner sep=0.75pt]    {$A$};
		% Text Node
		\draw (244,622.4) node [anchor=north west][inner sep=0.75pt]  [font=\tiny]  {$32$};
		% Text Node
		\draw (270,636.4) node [anchor=north west][inner sep=0.75pt]    {$B$};
		% Text Node
		\draw (276,623.4) node [anchor=north west][inner sep=0.75pt]  [font=\tiny]  {$32$};
		% Text Node
		\draw (302,636.4) node [anchor=north west][inner sep=0.75pt]    {$C$};
		% Text Node
		\draw (308,623.4) node [anchor=north west][inner sep=0.75pt]  [font=\tiny]  {$32$};
		% Text Node
		\draw (334,636.4) node [anchor=north west][inner sep=0.75pt]    {$D$};
		% Text Node
		\draw (340,623.4) node [anchor=north west][inner sep=0.75pt]  [font=\tiny]  {$32$};
		% Text Node
		\draw (420,651) node [anchor=north west][inner sep=0.75pt]   [align=left] {next block};
		% Text Node
		\draw (335,106.4) node [anchor=north west][inner sep=0.75pt]    {$D$};
		% Text Node
		\draw (340,130.4) node [anchor=north west][inner sep=0.75pt]  [font=\tiny]  {$32$};
		% Text Node
		\draw (249,569.5) node [anchor=north west][inner sep=0.75pt]  [font=\LARGE] [align=left] {+};
		% Text Node
		\draw (282.33,569.5) node [anchor=north west][inner sep=0.75pt]  [font=\LARGE] [align=left] {+};
		% Text Node
		\draw (315.66,569.5) node [anchor=north west][inner sep=0.75pt]  [font=\LARGE] [align=left] {+};
		% Text Node
		\draw (349,569.5) node [anchor=north west][inner sep=0.75pt]  [font=\LARGE] [align=left] {+};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[Illustrated MD5 algorithm flow]{Illustrated MD5 algorithm flow (source: Wikipedia, author: Dake)}
	\end{figure}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Normally we would put the MD5 algorithm pseudocode but... as it is a pain in the a.. to write it in LaTeX with the \texttt{algorithm2e} package we will for the moment not do it....
	\end{tcolorbox}
	The MD5 function as we have already mentioned it is not safe and not unique (two different inputs can give the same signature: we talk then of "collision"). However, the MD5 function is still widely used as a verification tool during downloads and the user can validate the integrity of the downloaded version thanks to the fingerprint. This can be done with a program for example named \texttt{md5sum} for MD5 and \texttt{sha1sum} for SHA-1 (see just below).
	
	Here is the fingerprint (abusively sometimes named "signature") obtained on a sentence\footnote{Made with the online tool \url{http://www.md5hashgenerator.com}} (which we took without accents):
	\begin{center}
	MD5("Wikipedia, the free encyclopedia") = f8aa0d3b1dae3f41d67c200688723c1b
	\end{center}
	By modifying a character, this impression drastically changes:
	\begin{center}
	MD5("Wikipedia, the free encyclopediA") = f9829ad9d4c2713140973520cad9206c
	\end{center}
	Specifically, the MD5 fingerprint can be performed as follows: when downloading a program, we write (copy) the character set indicated on the download page. When this download is complete, we launch one of the aforementioned software on the downloaded file.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.87]{img/computing/md5_cisco_hash.jpg}
		\caption{Illustrated MD5 download hash from CISCO}
	\end{figure}
	It must also be noticed that the main reason why using symmetric (or asymmetric) encryption is not advisable for protecting passwords is: key management. When using encryption, you must protect the encryption key (or the entropies from which the key is derived). And protecting the key is a very difficult task to solve. Hashing (with SHA, MD5, or any other algorithm) solves the problem of key protection, because you don't need to keep any secret value (other than salt, but salt is significantly less sensitive than encryption key; you can store salt in plain text). So if you only keep passwords for authentication purposes (performed by your app), there is absolutely no reason to use encryption; hashing would do just fine. 
	
	\pagebreak
	\subsubsection{SHA-1 Secure Hash Algorithm condensation function}\label{sha 1}
	The "\NewTerm{SHA-1}\index{SHA-1}" or "\NewTerm{secure hash algorithm-1}\index{Secure Hash Algorithm-1}" is used in competition with the MD5 for Digital Signature Algorithm as specified by the Digital Signature Standard (DSS). It was designed by the United States National Security Agency and is a U.S. Federal Information Processing Standard published by the United States (INST).

	SHA-1 is no longer considered secure against well-funded opponents. In 12005 (holocene calendar), cryptanalysts found attacks on SHA-1 suggesting that the algorithm might not be secure enough for ongoing use, and since 12010 (holocene calendar) many organizations have recommended its replacement by SHA-2 or SHA-3. Microsoft, Google, Apple and Mozilla have all announced that their respective browsers will stop accepting SHA-1 SSL certificates by 12017 (holocene calendar).

	On February 23, 12017 (holocene calendar) CWI Amsterdam and Google announced they had performed a collision attack against SHA-1, publishing two dissimilar PDF files which produce the same SHA-1 hash as proof of concept (\url{https://shattered.io}).

 	For a message of length less than $2^{64}$, the SHA-1 generates a $160$-bit digest of the message named also "hash" or "fingerprint". Again, identically to the MD5, a tiny modification of the original message must have a big impact on the condensed message and there must not be an identical Message Digest for two messages of different origin.

	As for the MD5, we work on messages whose length is a multiple of $512$ bits.
	\begin{enumerate}
		\item Completion:
		
		If the message does not have a length of $512$ bits, we add as many $1$ as necessary at the end of the message. The last $64$ bits of the $512$-bit block are used to set the original length of the message. The $512$-bit block is then transformed into sub-blocks \texttt{M[i]} of $32$ bits each expressed in hexadecimal ($0\ge i\ge 15$).
		
		\item Initialization:
		
		As for the MD5, this time we define $80$ chaining variables of $32$ bits $K[i]$ initialized as following (the digits are hexadecimal):
		\begin{itemize}
			\item \texttt{K[t]=01234567} for $0\ge t\ge 19$
			\item \texttt{K[t]=89ABCDEF} for $20\ge t\ge 39$
			\item \texttt{K[t]=FEDCBA98} for $40\ge t\ge 59$
			\item \texttt{K[t]=76543210} for $60\ge t\ge 79$
		\end{itemize}
		We also define $80$ non-linear functions \texttt{F[0]}, \texttt{F[1]}, \texttt{F[2]}, ..., \texttt{F[79]} which take $32$-bit arguments and return a $32$-bit value, the operation being done bit by bit:
		\begin{itemize}
			\item \texttt{F[t](X,Y,Z) = (X AND Y) OR (NOT(X) AND Z)} for $0\ge t\ge 19$
			\item \texttt{F[t](X,Y,Z) = (X XOR Y) XOR D } for $20\ge t\ge 39$
			\item \texttt{F[t](X,Y,Z) = (X AND Y) OR (X AND Z) OR (Y AND Z)} for $40\ge t\ge 59$
			\item \texttt{F[t](X,Y,Z) = X XOR Y XOR Z} for $60\ge t\ge 79$
		\end{itemize}
		What is important with these $80$ functions is that if the bits of their arguments \texttt{X}, \texttt{Y} and \texttt{Z} are independent, the bits of the result are also independent.
	
		\item Iterative calculation:
		
		The iteration uses two buffers, each consisting of the use of $5$ chaining variables. The chaining variables of the first buffer are denoted \texttt{A}, \texttt{B}, \texttt{C}, \texttt{D}, \texttt{E}. The second buffer contains the chaining variables \texttt{H[0]}, \texttt{H[1]}, \texttt{H[2]}, \texttt{H[3]}, \texttt{H[4]}.
	\end{enumerate}

	Moreover, let \texttt{S\string^n} denote the circular shift of \texttt{n} bits to the left, here is the SHA-1 algorithm (if we have the time in the future we will write it properly with the correct LaTeX package...):
	
	\begin{verbatim}
	For t = 16 to 79 Do
     M[t] = S^1(M[t-16] XOR M[t-15] XOR M [t-14] XOR M [t-13]);
	End For
	A = H[0];
	B = H[1]; 
	C = H[2];
	D = H[3]; 
	E = H[4]
	For t = 0 to 79 Do
	     TEMP = S^5(A) + F[t](B,C,D) + E + M[t] + K[t]
	     E = D; 
	     D = C; 
	     C = S^30(B); 
	     B = A; 
	     A = TEMP;
	End For
	H[0] = H[0] + A;
	H[1] = H[1] + B; 
	H[2] = H[2] + C; 
	H[3] = H[3] + D; 
	H[4] = H[4] + E;
	\end{verbatim}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Since we have written this text on SHA-1, new versions, sometimes significantly different of SHA-1 have been released. The SHA-0 was released in 11998 (holocene calendar), the SHA-1 presented above is a minor correction of the SHA-0, the SHA-2 was released in 12001 (holocene calendar) and finally the SHA-3 in 12012 (holocene calendar).
	\end{tcolorbox}
		
	To sum up a bit, it must be clear that MD5 (Message-Digest algorithm 5) is a cryptographic hash function, while Advanced Encryption Standard (AES) or RSA are symmetric-key encryption algorithms, so they are used for different purposes. A hash, like MD5 or SHA are used to verify passwords because they are hard to invert, that is, to obtain the password from the hash-string. An AES or RSA encryption, on the other hand, are invertible, the original message can be obtained if we know the key. 
	
	\begin{figure}[H]
		\centering
		\resizebox{\textwidth}{!}{%
		\begin{tikzpicture}
		\tikzset{
		  pics/media/.style ={
		    code = { %
		      \node[text width=2cm,minimum height=2cm,#1] (back) {};
		      \node[draw,anchor=center,fill=white] at ([yshift=5pt]back.center) {Media};
		      \draw[dashed] (back.north west) rectangle (back.south east);
		    }
		  },
		  pics/media/.default={pattern=north east lines},
		  aes/.style={
		    draw,
		    fill=red!30
		  },
		  rsa/.style={
		    draw,
		    rounded corners,
		    fill=blue!30
		  },
		  ar/.style={
		    ->,
		    >=latex,
		    shorten >= 3pt,
		    shorten <= 3pt,    
		  },
		  ar2/.style={
		    ->,
		    >=latex,
		    line width=2pt,
		    shorten >= 3pt,
		    shorten <= 3pt,    
		  }
		}
		
		\newcommand\mediaencryptedbox[3][1cm]{
		\node[
		  draw,
		  thick,
		  rounded corners,
		  #2,
		  text width=3.5cm,
		  minimum height=4.5cm,
		  anchor=north west,
		  yshift=#1
		  ]
		  (#3)
		  {};
		\node[
		  aes,
		  anchor=north
		  ]
		  at (#3.north) 
		  {AES key}; 
		\pic at (#3.center) (sm3) {media};
		\node[
		  anchor=south
		  ]
		  (rsa)  
		  at (sm3back.north) 
		  {Encrypted with RSA};
		\node[
		  anchor=north
		  ] 
		  at (sm3back.south) 
		  {Encrypted with AES};
		\draw
		  (#3.west|-rsa.south) -- (#3.east|-rsa.south);
		}
		
		% The Server
		\pic (sm1) {media={fill=gray!30}};
		\pic[right=of sm1back] (sm2) {media};
		\mediaencryptedbox{right=of sm2back}{box1}
		\node[
		  aes,
		  anchor=north,
		  above=of sm2back.north
		  ]
		  (aes1)
		  {AES key};  
		\draw[ar]
		  (aes1) -- (sm2back.north) ;  
		\draw[ar]
		  (aes1.south east) to[out=-60,in=180] coordinate (aux1) (box1.north west) ;
		\node[
		  anchor=west,
		  rsa
		  ]
		  at (aux1|-aes1)
		  (rsa1)
		  {RSA public key};    
		
		\draw[ar]
		  (rsa1) -- (aux1) ;  
		
		\draw[ar2]
		  (sm1back.east) -- (sm2back.west);
		\draw[ar2]
		  (sm2back.east) -- (box1.west|-sm2back.east);
		
		\node[
		  inner sep=10pt,
		  draw,
		  dashed,fit={(sm1back.north west) (box1.south east) (aes1)}
		  ]
		  (server) 
		  {};
		\node[
		  anchor=south west,
		  font=\Large
		  ]
		  at ([shift={(15pt,5pt)}]server.north west)
		  {Server};      
		
		% The Player
		\mediaencryptedbox[2.2cm]{right=6cm of box1}{box2}
		\node[
		  anchor=north west,
		  rsa,
		  above left=of box2
		  ]
		  (rsa2)
		  {RSA public key};    
		\draw[ar]
		  (rsa2.south) 
		    to[out=-80,in=160]
		    node[align=center,anchor=east,shift={(10pt,-20pt)}] {RSA decryption \\ (slow)} 
		  ([yshift=-20pt]box2.north west);
		\draw[ar]
		  ([yshift=-10pt]box2.north east) 
		    to[out=0,in=0]
		    node[align=center,anchor=west,shift={(5pt,0pt)}] (AESd) {AES decryption \\ (fast)} 
		  (sm3back.east);
		\node[
		  inner sep=10pt,
		  draw,
		  dashed,fit={(rsa2) (box2.south east) (AESd)}
		  ]
		  (player) 
		  {};
		\node[
		  anchor=south west,
		  font=\Large
		  ]
		  at ([shift={(15pt,5pt)}]player.north west)
		  {Player};
		
		\draw[ar2]
		  (server.east) -- (player.west|-server.east);        
		\draw[ar2]
		  ([yshift=10pt]sm3back.south east) -- ++(3cm,0) node[near end,anchor=south west] {Streaming};
		\end{tikzpicture}}
		\caption{Typical other use of AES and RSA combined together}
	\end{figure}
	
	
	\subsection{Certificate based authentication}
	We saw during our study of public key and secret key cryptography that there was an issue in the system of transmission of the keys at the beginning of the communication.

	Thus, in both systems, the issue lies in the fact that a malicious person ("man-in-the-middle" attack) can replace the real interlocutor and send either a false secret key or a false public key (depending on the case).

	Thus, a certificate of authenticity makes it possible to associate a key with an entity (a person, a machine, etc.) in order to ensure its validity (association with the "real person"). The certificate is in a way the identity card of the key or the "\NewTerm{digital signature}\index{digital signature}", issued by an organization named "\NewTerm{certification authority}\index{certification authority}".

	The technologies using digital signatures are part of a larger set known as "\NewTerm{Public Key Infrastructure (PKI)}\index{public key infrastructure}". The whole takes place by means of certificates which you can obtain from a Certification Authority (see example below). When you request your certificate, your computer creates the key pair consisting of a private key (the yellow on the schema) and a public key (the black one). Your private key is secret and it is only you who have access to it while the public key is freely available for everyone. Your public key will be attached to your certificate that you will get from the certification authority to whom you have submitted your certificate request.

	The PKI (on which the IPSec connection is based) essentially targets $4$ important points:
	\begin{enumerate}
		\item The authentication (the recipient of your email must be able to verify that it is you who sent the object and not another individual).

		\item Integrity (ensure that the content has not been changed along the way).

		\item Confidentiality (ensuring that the content is readable only by the recipient).

		\item Non-repudiation (arising from the first 3 points)
	\end{enumerate}
	The certification authority is responsible for issuing the certificates, assigning them a validity date, and possibly revoking certificates before that date if the key is compromised.
	
	Certificates are small files divided into two parts:
	\begin{itemize}
		\item The part containing the information
		
		\item The part containing the signature of the certification authority (see Microsoft Internet Explorer browser for an example)
	\end{itemize}

	The certificate structure is standardized by the International Telecommunication Unification (ITU) standard X.509, which defines the information contained in the certificate:
	\begin{itemize}
		\item The name of the certification authority (VeriSign for example)

		\item The name of the owner of the certificate (the UBS bank for example)

		\item The date of validity of the certificate ($X$ day from the current date)

		\item The encryption algorithm used (MD5RSA)

		\item The owner's public key
	\end{itemize}
	Here is a quite good schematic example:
	
	To sign the message you are sending (point \circledtext{5} in the figure below), it is sufficient to apply a hash function (point \circledtext{1}) which produces a summary (hash code) of the message (using MD5 or any version of SHA). The summary (fingerprint) obtained is (almost...) specific to each message, like the image of a fingerprint we know that a hash algorithm ensures that if a single bit of the original text is modified and a new hash is made, the latter will with a very high probability radically different from the first one, and the hashed code can then be encrypted using your private key (\circledtext{2} and \circledtext{3}), this results constitutes the "digital signature". The recipient of the message (point \circledtext{6}) can then verify that you are the sender by encrypting the digital signature (point \circledtext{7}), by means of your public key (point \circledtext{8}), that you transmitted to it automatically with the mail (point \circledtext{4}), to get the hashed code (point \circledtext{9}). The recipient then applies the same hash function to the received message (point \circledtext{10} in the diagram). If the two codes (points \circledtext{11} and \circledtext{12} on the schema) are identical, you are the sender of the message (authentication) and the message has not been altered (integrity).
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/computing/principle_of_digital_signatures.jpg}
		\caption{Principle of digital signatures}
	\end{figure}
	All this looks very complicated, but in practice, depending on the software, you just have to click on one or three buttons on the screen to start the whole process.

	Otherwise let's see another figure involving now a Certificate authority:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/computing/certificate_authority.jpg}
		\caption[Principle of certificate authority]{Principle of certificate authority (source: Pour la Science)}
	\end{figure}
	Where we have:
	\begin{enumerate}
		\item Alice uses a private key ($a$) as well as a public key ($b$) received from a certificate authority that has typically transmitted the private and public keys to Alice in a smart card containing a digital certificate ($c$). This certificate also includes the signature of the certificate authority, which can be verified by any person (or software) who knows or has access to the public key of this organization.
		\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/computing/quovadis_suisse_id.jpg}
		\caption[]{Example of digital certificate smart card as used\\ by the author of this chapter}
	\end{figure}
		
		\item The public key ($d$) of the certificate authority is provided to those who need it, for example Bob. This key can be included in the web browsers programs and in other software used for secure computer communications.
		
		\item Alice digitally signs the message she sends to Bob. First, she creates a digest of the message by applying a hash function to it. The digest thus created is then encrypted using the secret key of Alice which gives the digital signature of the message ($e$). This signature is sent to Bob at the same time as the encrypted message ($f$) and the public key.
		
		\item Bob uses the public key of the certificate authority to verify that the official digital signature on the certificate is authentic and that the accompanying public key is that of Alice. He then uses this key to decrypt Alice's digital signature and gets the digest of the message. Finally, Bernard applies the hash function to the message sent by Alice and thus gets a digest of the message. If this digest is identical to that obtained by Alice's numerical encryption, Bob is sure that the message comes from Alice and has not been altered by a third person.
	\end{enumerate}
	
	\pagebreak
	\subsection{Quantum cryptography}\label{quantum cryptography}
	"\NewTerm{Quantum cryptography}\index{quantum cryptography}" is a marketing expression, but somewhat misleading: it is not a question of encrypting a message using quantum physics, but of using quantum physics to ensure that the transmission of the key has not been spied. Currently used popular public-key encryption and signature schemes (RSA) can be broken by quantum adversaries. The advantage of quantum cryptography lies in the fact that it allows various cryptographic tasks that are proven or conjectured to be impossible to break using only classical (i.e. non-quantum) communication (see below for examples). For example, it is impossible to copy data encoded in a quantum state and the very act of reading data encoded in a quantum state changes the state. This is used to detect eavesdropping in quantum key distribution.

	Indeed, as we have already explained it in the section of Quantum Computing, the transmission of a message, encrypted or not, can be done using the two orthogonal linear polarization states of a photon, for example $|x\rangle$, $|y\rangle$. We can decide to assign by convention the value $1$ to the polarization $|x\rangle$ and the value $0$ to the polarization $|y\rangle$: each photon therefore carries one bit of information. Any encrypted or unencrypted message can then be written in binary language, such as a sequence of $0$ and $1$, and the message $1001110$ will be encoded by Alice thanks to the sequence of photons $|x\rangle |y\rangle |y\rangle |x\rangle |x\rangle |x\rangle |y\rangle$, which she will send to Bob for example by an optical fiber. Using a birefringent plate, Bob separates the photons with vertical and horizontal polarization and two detectors placed behind the slide allow him to decide whether the photon was polarized horizontally or vertically:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,638); %set diagram left start at 0, and has height of 638
		
		%Shape: Rectangle [id:dp21092714170233218] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (223,60) -- (283.3,60) -- (283.3,277.8) -- (223,277.8) -- cycle ;
		%Straight Lines [id:da420205265161931] 
		\draw    (109.3,168.8) -- (451.3,168.8) ;
		%Shape: Axis 2D [id:dp9809534641451305] 
		\draw  (113,169) -- (213,169)(123,79) -- (123,179) (206,164) -- (213,169) -- (206,174) (118,86) -- (123,79) -- (128,86)  ;
		%Straight Lines [id:da6628248286888274] 
		\draw    (123,169) -- (77.6,222.28) ;
		\draw [shift={(76.3,223.8)}, rotate = 310.44] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da025966209098285953] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (74.3,109.8) -- (140.3,188.8) ;
		%Shape: Arc [id:dp4582439197474846] 
		\draw  [draw opacity=0] (94.31,133.59) .. controls (99.8,126.54) and (108.37,122) .. (118,122) .. controls (119.96,122) and (121.88,122.19) .. (123.74,122.55) -- (118,152) -- cycle ; \draw   (94.31,133.59) .. controls (99.8,126.54) and (108.37,122) .. (118,122) .. controls (119.96,122) and (121.88,122.19) .. (123.74,122.55) ;  
		%Straight Lines [id:da6064077759381246] 
		\draw    (223.8,168.8) -- (309.3,168.8) ;
		\draw [shift={(311.3,168.8)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da17678803310922797] 
		\draw    (358.28,153.01) -- (330.32,183.59) ;
		\draw [shift={(328.3,185.8)}, rotate = 312.44] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		\draw [shift={(360.3,150.8)}, rotate = 132.44] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da08146118787516876] 
		\draw  [dash pattern={on 0.84pt off 2.51pt}]  (152.3,131.8) -- (305.3,63.8) ;
		%Straight Lines [id:da06827367783992644] 
		\draw    (287.3,128.8) -- (313.3,128.8) ;
		\draw [shift={(315.3,128.8)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da14419860685829722] 
		\draw    (343.3,111.8) -- (343.3,145.8) ;
		\draw [shift={(343.3,148.8)}, rotate = 270] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		\draw [shift={(343.3,108.8)}, rotate = 90] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da6798968780220347] 
		\draw    (223.8,168.8) -- (284.3,128.8) ;
		%Straight Lines [id:da4600194591037232] 
		\draw    (284.3,128.8) -- (451.3,128.8) ;
		%Straight Lines [id:da3210031769602666] 
		\draw    (81.3,168.8) -- (107.3,168.8) ;
		\draw [shift={(109.3,168.8)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Shape: Rectangle [id:dp46038864562105974] 
		\draw  [fill={rgb, 255:red, 128; green, 128; blue, 128 }  ,fill opacity=1 ] (452,109.2) -- (490.3,109.2) -- (490.3,147) -- (452,147) -- cycle ;
		%Shape: Rectangle [id:dp30851085826281355] 
		\draw  [fill={rgb, 255:red, 128; green, 128; blue, 128 }  ,fill opacity=1 ] (452,153) -- (490.3,153) -- (490.3,190.8) -- (452,190.8) -- cycle ;
		%Straight Lines [id:da8104771474935024] 
		\draw    (422,150) -- (533.3,150) ;
		\draw [shift={(535.3,150)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (12,-3) -- (0,0) -- (12,3) -- cycle    ;
		
		% Text Node
		\draw (118,61.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (78.3,227.2) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (100,235) node [anchor=north west][inner sep=0.75pt]   [align=left] {polariser};
		% Text Node
		\draw (98,105.4) node [anchor=north west][inner sep=0.75pt]    {$\theta $};
		% Text Node
		\draw (340,170) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
		% Text Node
		\draw (311,46) node [anchor=north west][inner sep=0.75pt]   [align=left] {optical axes};
		% Text Node
		\draw (339,82.4) node [anchor=north west][inner sep=0.75pt]    {$\vec{E}$};
		% Text Node
		\draw (125,151) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
		% Text Node
		\draw (494,107.4) node [anchor=north west][inner sep=0.75pt]    {$D_{x}$};
		% Text Node
		\draw (494,174.4) node [anchor=north west][inner sep=0.75pt]    {$D_{y}$};
		% Text Node
		\draw (538,140.4) node [anchor=north west][inner sep=0.75pt]    {$z$};
		% Text Node
		\draw (288,234) node [anchor=north west][inner sep=0.75pt]   [align=left] {Birefrengent waveplate};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Thought experiment for polarization measurement}
	\end{figure}
	
	The whole process and protocol can be summarized by the excellent following figure:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.65]{img/computing/quantum_cryptography.jpg}
		\caption{Quantum key distribution}
	\end{figure}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The protocol described above is named BB84, named after its inventors Bennett and Brassard.
	\end{tcolorbox}
	Let us now turn to the formal part (we strongly recommend the reader to first take a look the section of Quantum Computing at page \pageref{quantum computing}!).

	The states of the quantum system are the states of polarization of a photon: the measurements (of the observable) will also have its polarization states. Possible measures will include:
	
	we will denote the corresponding states $|0\rangle$ and $|1\rangle$ (orthonormal basis of the space of the states of polarization): it is the base H/V (Horizontal/Vertical).

	Let us consider several cases:
	\begin{enumerate}
		\item[C1.] Given a photon in the state $|\Psi\rangle=|0\rangle$ then as we have seen in the section Quantum Computing (see page \pageref{quantum computing}), we will have:
		

		\item[C2.] Or a photon in the state:
		
	\end{enumerate}
	And it customary to write the sequence of the key as following:
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} Let us recall that this (famous) value is chosen for normalization purposes such as that $\langle \Psi|\Psi\rangle$!!! Many people ask the question of where the square root comes from in Quantum Computing? The answer is simply for normalization as we have detailed it in the section of Quantum Computing\\

	\textbf{R2.} Let us also recall that the photons $|\Psi_{01}\rangle$ and $|\Psi_{11}\rangle$ are not polarized in the direction "$|0\rangle+|1\rangle$" (i.e. in the oblique direction) but are in a quantum superposition of these two polarizations!
	\end{tcolorbox}
	Then for example (we apply as we saw in the section of Quantum Computing, the test $|0\rangle$ to the state $|\Psi_{01}\rangle$):
	
	and:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Let us recall that in this book, we write in Quantum Physics the module of a complex number and the norm, indistinctly by the symbol $||{}\|$ therefore caution to the possible confusions!
	\end{tcolorbox}
	
	\subsection{Alternative cryptography}
	Mathematicians sometimes venture out of the beaten path of Number Theory: they invent cryptosystems based on braids or networks (see the corresponding sections of Knot Theory or Graph Theory). Physicists are not left behind and offer methods of encryption that use the theory of chaos or quantum physics. The latter would provide a definitive solution to the delicate problem of key exchange and jeopardize cryptosystems based on factorization.

	Most of these methods are outside the scope of this book for the moment but we can give however a non-exhaustive list:
	\begin{itemize}
		\item The LLL (Lenstra–Lenstra–Lovász) algorithm based on the mesh structure of sets of numbers and based on the Minkowski theorem ensuring that the content of a disc of given radius at a point contains at least one other point of the network

		\item The ultravariable cryptography in which the data pass through systems of superimposed quadratic equations.

		\item Optical hyperchaos, obtained by the passage of a LASER in a Ikeda ring in which a non-linear wavelength material is integrated.

		\item ...
	\end{itemize}
	The future will tell us the rest!
	
	
	


	

	\begin{flushright}
	\begin{tabular}{l c}
	\circled{50} & \pbox{20cm}{\score{2}{5} \\ {\tiny 12 votes,  50.00\%}} 
	\end{tabular} 
	\end{flushright}

	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Quantum Computing}\label{quantum computing}
	\lettrine[lines=4]{\color{BrickRed}Q}{uantum computing} (we should rather speak of "\NewTerm{quantum calculation}\index{quantum calculation}" because we are currently very far from an input / output system) is a beautiful example of the use of specific theoretical models of quantum physics for treatment and the transmission of information.\\
	
	However it must also be remembered that the behaviour of transistors etched on the chip in your computer could not be imagined in 11947 (holocene calendar) by Bardeen, Brattain and Shockley that from their knowledge of quantum physics. So all of our electronic devices already operating on the basis of semiconductors operate with developments achieved through quantum physics.
	
	The big news, since the early 11980s (holocene calendar), is the ability of physicists to manipulate and observe individual elementary quantum objects: photons, atoms, ions, etc. It is this ability to manipulate and observe basic quantum objects that is the cause of quantum information, where these elementary quantum objects will physically build the "\NewTerm{qubits}\index{qubits}" (for "Quantum Bit"). That said, no fundamentally new concept has been introduced since the 11930s (holocene calendar) and the founding fathers of quantum physics (Heisenberg, Schrödinger, Dirac, Planck, Einstein, etc.), if they revived today, would not be surprised by quantum computing, even if they would surely be surprised by the prowess of the experimenters who now realize experiences qualified in their era of "gedanken experiment" (imaginary experiment that was impossible to do in laboratory).
	
	It is also interesting to notice that the increasing miniaturization of electronics will find its limits because of quantum effects, which will become essential below the nanometer. Thus, we believe that Moore's Law (which assumption that the computing power of machines doubles roughly every $18$ months) may not be correct anymore by the years 12015-12020 (holocene calendar).
	
	It is likely that the trend of the study of quantum physics and its application to quantum information (and quantum electronics and quantum telecommunications) will explode in the coming decades (especially towards the end of the 121st century). Thus, engineering schools will integrate in all study field Quantum Physics in school curricula. What physicists study for soon already almost 100 years in their curriculum...
	
	Before moving to the formal side, we felt, however, interesting to make a small popularized passage because we noticed that it helps to understand the calculations that will be made thereafter.
	
	In the 70 and 80 (of the 120th century according to holocene calendar), the first quantum computers are born from the minds of physicists such as Richard Feynman, Paul Benioff, David Deutsch and Charles Bennett. Feynman's idea was that instead of complaining that the simulation of quantum phenomena demand enormous powers to our to days computers today, that we use the power of quantum phenomena to make the computers faster than classic computers.
	
	During long time physicists doubted that quantum computers can be used, and even that we can do something viable if they existed. But (all years are based on the holocene calendar):
	\begin{itemize}
		\item In 11994, Peter Shor, a scientist of AT\&T shows it is possible to factor large numbers in a reasonable time using a quantum computer. This discovery unlocks suddenly credits for quantum computers research.
		
		\item In 11996, Lov Grover, invented an algorithm based on quantum computers to find an entry in an unsorted database.
		
		\item In 11998, IBM was the first to present a $2$-qubit quantum calculator.
		
		\item In 11999, the IBM R\&D team used the Grover algorithm for fast quantum search on a database (quantum database search) on a calculator with $3$-qubits and beat their record the following year with a $5$-qubit computer.
		
		\item In 12001, IBM created a quantum computer with $7$-qubits and factored the number $15$... thanks to the Shor algorithm (\SeeChapter{see section Numerical Methods page \pageref{quantum computing}}). Calculators with $7$-qubits are built around chloroform molecules and their useful life is no more than a few minutes.
		
		\item In 12007, the Canadian company D-Wave during a demonstration presented a quantum computer with $16$-qubits.
		
		\item In May 12013, Google announced that it was launching the Quantum Artificial Intelligence Lab, hosted by NASA's Ames Research Center, with a $512$-qubit D-Wave quantum computer. 
		
		\item In October 12015 researchers at University of New South Wales built a quantum logic gate in silicon for the first time.
	\end{itemize}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	As a $1$ fermion spin qubit is equivalent to $2$ bits, we have then that $N$ fermion spin qubits are equal to $2^N$ classical bits. Therefore if one day we reach the $100$ fermion spin qubits computer we will have a potential memory of $2^{100}\cong 1\cdot 10^{15}$ Petabytes...
	\end{tcolorbox}
	The memory of a classical computer is made of bits (\SeeChapter{see section Numerical Methods page \pageref{bit}}). Each bit carries either a $1$ or a $0$ (bipolar mode). The machine computes by manipulating those bits. A quantum computer is working on a set of qubits. A qubit can wear either a $1$ or a $0$, or a superposition of a $1$ and a $0$ (or, more accurately, he wears a phase distribution). The quantum computer computes by manipulating those distributions as discussed in detail below.
	
	Query a qubit whose phase angle is not $0^{\circ}$ or $90^{\circ}$ ($\pi/2$) is not very useful: we will get the answer $0$ with a given probability and $1$ with another probability and ... it is possible to construct random generators much cheaper! However, if we manage to create an algorithm that systematically leads it to a phase $0^{\circ}$ or $90^{\circ}$, we get a deterministic result. But is also necessary that it corresponds to a sought response.
	
	A quantum computer could be implemented from any particles that can have two states. It can be built from photons, or from any particle or atom/molecule having a spin.
	
	As we know, a classical computer with one bit can only store numbers with one digit composed of a one or a zero (\SeeChapter{see section Numerical Methods page \pageref{bit}}) for a total of $2^1=2$ states that it must treat separately. At one point, he could contain the bits $1$ for example.
	
	A quantum computer with one qubit can actually store $3$ states as it can have a project state corresponding to $1$ or to $0$ and any superposition of the states $0$ and $1$ with a given probability (the third state!) before being observed (\SeeChapter{see section Wave Quantum Physics page \pageref{quantum superposition}}). When the calculator make the measurement the state superposition is cancelled as we know and therefore the third state can only be used (at least as far as we know) for intermediate and temporary calculations (as they cannot be read with our actual knowledge). This is why today qubits are specialized for given algorithms and we have "quantum calculators" but not "quantum computers".
	
	Before we study the mathematical aspects let us start by the study of one of the most famous cat in the world to better understand (we hope so):	
	
	\subsection{Schrödinger's Cat superposition}
	As we have studied it in details in the section of Quantum Sections, unlike in Classical Mechanics, essentially different states can mix in Quantum Mechanics (see page \pageref{collapse of the wave function}) at least until a specific property is measured!

	Now let us recall this famous thought experiment:
	
	\begin{enumerate}
		\item Put a (living) cat in box.
		\item Add a container with deadly poison, that can be remotely released.
		\item Close and seal the box.
		\item Connect the remote to a quantum randomness source (e.g. nuclear decay).
	\end{enumerate}
	In what state" is the cat? Is it alive or dead? Who knows?
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/schrodinger_cat_experiment.jpg}
		\caption[Schrödinger cat experiment]{Schrödinger cat experiment (source: Wikipedia)}
	\end{figure}
	As long as no one checks (!!!), it a sensible way to think about this to
consider the cat being in an intermediate state:
	
	As long as no one checks (!!!) the cat is in a superposition of two
states that are classically impossible to consider at the same time.

	Now let us start by understanding the underlying concepts of quantum theory and of quantum computing with the study of the polarization of the photon.
	
	Now if we open the box obviously, we will see ("measure") either a dead or alive cat.
	
	By "measuring" the system, we put it back in a classical, pure state! As we know it already: Measuring affects the system!
	
	The Cat set of states will be written:
	
	Take a second cat, same arrangement and consider a 2-cat system:
	
	That is to say with two cats:
	
	Therefore as we can see the central idea of quantum computing is manipulating the $N$-qubit system to change at the same time ALL $2^N$ pure states probabilities but when the $N$-qubit system will be measured,
only a single pure state will be picked out!

	A quantum algorithm must therefore modify the probability distribution of the quantum system, such that the correct "result" state has an (almost) $100\% $ probability.
	
	\subsection{Photon polarization}
	Since Albert Einstein, we know that light is composed of photons, or particles of light, and that it has a dual wave-particle appearance (\SeeChapter{see section Wave Optics page \pageref{young interference experiment}}). If we reduce the light intensity of a beam of photons, we should be able to study the polarization of individual photons, that we know perfectly how to detect using photomultipliers. Suppose that the experiment detects $N$ photons. When $N\rightarrow +\infty$, we must fall back on the results of wave optics (see section of the same name page \pageref{wave optics}).
	
	Let us perform by example the following experience:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,638); %set diagram left start at 0, and has height of 638
		
		%Shape: Rectangle [id:dp21092714170233218] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (223,60) -- (283.3,60) -- (283.3,277.8) -- (223,277.8) -- cycle ;
		%Straight Lines [id:da420205265161931] 
		\draw    (109.3,168.8) -- (451.3,168.8) ;
		%Shape: Axis 2D [id:dp9809534641451305] 
		\draw  (113,169) -- (213,169)(123,79) -- (123,179) (206,164) -- (213,169) -- (206,174) (118,86) -- (123,79) -- (128,86)  ;
		%Straight Lines [id:da6628248286888274] 
		\draw    (123,169) -- (77.6,222.28) ;
		\draw [shift={(76.3,223.8)}, rotate = 310.44] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da025966209098285953] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (74.3,109.8) -- (140.3,188.8) ;
		%Shape: Arc [id:dp4582439197474846] 
		\draw  [draw opacity=0] (94.31,133.59) .. controls (99.8,126.54) and (108.37,122) .. (118,122) .. controls (119.96,122) and (121.88,122.19) .. (123.74,122.55) -- (118,152) -- cycle ; \draw   (94.31,133.59) .. controls (99.8,126.54) and (108.37,122) .. (118,122) .. controls (119.96,122) and (121.88,122.19) .. (123.74,122.55) ;  
		%Straight Lines [id:da6064077759381246] 
		\draw    (223.8,168.8) -- (309.3,168.8) ;
		\draw [shift={(311.3,168.8)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da17678803310922797] 
		\draw    (358.28,153.01) -- (330.32,183.59) ;
		\draw [shift={(328.3,185.8)}, rotate = 312.44] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		\draw [shift={(360.3,150.8)}, rotate = 132.44] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da08146118787516876] 
		\draw  [dash pattern={on 0.84pt off 2.51pt}]  (152.3,131.8) -- (305.3,63.8) ;
		%Straight Lines [id:da06827367783992644] 
		\draw    (287.3,128.8) -- (313.3,128.8) ;
		\draw [shift={(315.3,128.8)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da14419860685829722] 
		\draw    (343.3,111.8) -- (343.3,145.8) ;
		\draw [shift={(343.3,148.8)}, rotate = 270] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		\draw [shift={(343.3,108.8)}, rotate = 90] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da6798968780220347] 
		\draw    (223.8,168.8) -- (284.3,128.8) ;
		%Straight Lines [id:da4600194591037232] 
		\draw    (284.3,128.8) -- (451.3,128.8) ;
		%Straight Lines [id:da3210031769602666] 
		\draw    (81.3,168.8) -- (107.3,168.8) ;
		\draw [shift={(109.3,168.8)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Shape: Rectangle [id:dp46038864562105974] 
		\draw  [fill={rgb, 255:red, 128; green, 128; blue, 128 }  ,fill opacity=1 ] (452,109.2) -- (490.3,109.2) -- (490.3,147) -- (452,147) -- cycle ;
		%Shape: Rectangle [id:dp30851085826281355] 
		\draw  [fill={rgb, 255:red, 128; green, 128; blue, 128 }  ,fill opacity=1 ] (452,153) -- (490.3,153) -- (490.3,190.8) -- (452,190.8) -- cycle ;
		%Straight Lines [id:da8104771474935024] 
		\draw    (422,150) -- (533.3,150) ;
		\draw [shift={(535.3,150)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (12,-3) -- (0,0) -- (12,3) -- cycle    ;
		
		% Text Node
		\draw (118,61.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (78.3,227.2) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (100,235) node [anchor=north west][inner sep=0.75pt]   [align=left] {polariser};
		% Text Node
		\draw (98,105.4) node [anchor=north west][inner sep=0.75pt]    {$\theta $};
		% Text Node
		\draw (340,170) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
		% Text Node
		\draw (311,46) node [anchor=north west][inner sep=0.75pt]   [align=left] {optical axes};
		% Text Node
		\draw (339,82.4) node [anchor=north west][inner sep=0.75pt]    {$\vec{E}$};
		% Text Node
		\draw (125,151) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
		% Text Node
		\draw (494,107.4) node [anchor=north west][inner sep=0.75pt]    {$D_{x}$};
		% Text Node
		\draw (494,174.4) node [anchor=north west][inner sep=0.75pt]    {$D_{y}$};
		% Text Node
		\draw (538,140.4) node [anchor=north west][inner sep=0.75pt]    {$z$};
		% Text Node
		\draw (288,234) node [anchor=north west][inner sep=0.75pt]   [align=left] {Birefrengent waveplate};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[]{Thought experiment for polarization measurement}
	\end{figure}
	A birefringent plate separates a light beam whose polarization is makes an angle $\theta$ with O$x$ in a subsequent beam polarized  following O$x$ and another polarized following O$y$, the intensities being respectively $I\cos^2(\theta)$ and $I\sin^2(\theta)$ (according to the proof of Malus Law made in the section of Wave Optics at page \pageref{malus law}).
	
	Let us reduce the intensity so that the photons arrive one by one, and let us place two photodetectors $D_x,D_y$ behind the blade. The experiment shows that $D_x,D_y$ never click together at the same time (except in cases of "dark count" when a counter is triggered spontaneously because of background noise): a photon comes entirely either on $D_x$, either on $D_y$, a photon therefore can not divide itself. On the other hand, the experiment shows that the probability $P_x$ (respectively $P_y$) of detecting a photon by $D_x$ (respectively $D_y$) is equal to $\cos^2(\theta)$ (respectively $\sin^2(\theta)$). So if the experiment detects $N$ photons, we will have $N_x$ (respectively $N_y$) photons detected by $D_x$ (respectively $D_y$):
	
	where the $\cong$ takes int account the statistical fluctuations. As the light intensity is proportional to the number of photons, we fall indeed back on the Malus law the limit $N\rightarrow +\infty$.
	
	However, we notice two problems:
	\begin{enumerate}
		\item Can we predict, for a given photon, it will trigger $D_x$ or $D_y$? The answer of quantum theory is: NO, statement has deeply shocked Albert Einstein (God does not play dice!). Some physicists (including Albert Einstein) have been tempted to assume that quantum theory was incomplete, and that there were "hidden variables" whose knowledge would provide the individual destiny of each photon. Under very reasonable assumptions on which we will come back, we now know that such hidden variables are excluded. The probabilities of quantum theory are, as we know (\SeeChapter{see section Wave Quantum Physics page \pageref{wave quantum physics}}), intrinsic! They are not related to an imperfect knowledge of the physical situation, as is the case for example in the game of heads or tails.

		\item If we combine the two beams of the first birefringent plate, using a second blade that is symmetrical to the first:
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
			%uncomment if require: \path (0,638); %set diagram left start at 0, and has height of 638
			
			%Shape: Rectangle [id:dp7038529285108361] 
			\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (358,59) -- (418.3,59) -- (418.3,276.8) -- (358,276.8) -- cycle ;
			%Shape: Rectangle [id:dp21092714170233218] 
			\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (223,60) -- (283.3,60) -- (283.3,277.8) -- (223,277.8) -- cycle ;
			%Straight Lines [id:da420205265161931] 
			\draw    (109.3,168.8) -- (488.3,168.8) ;
			%Shape: Axis 2D [id:dp9809534641451305] 
			\draw  (113,169) -- (213,169)(123,79) -- (123,179) (206,164) -- (213,169) -- (206,174) (118,86) -- (123,79) -- (128,86)  ;
			%Straight Lines [id:da6628248286888274] 
			\draw    (123,169) -- (77.6,222.28) ;
			\draw [shift={(76.3,223.8)}, rotate = 310.44] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
			%Shape: Axis 2D [id:dp8638319282216713] 
			\draw  (478,169) -- (578,169)(488,79) -- (488,179) (571,164) -- (578,169) -- (571,174) (483,86) -- (488,79) -- (493,86)  ;
			%Straight Lines [id:da5515196759596948] 
			\draw    (488,169) -- (442.6,222.28) ;
			\draw [shift={(441.3,223.8)}, rotate = 310.44] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
			%Straight Lines [id:da025966209098285953] 
			\draw  [dash pattern={on 4.5pt off 4.5pt}]  (74.3,109.8) -- (140.3,188.8) ;
			%Straight Lines [id:da03959419264061048] 
			\draw  [dash pattern={on 4.5pt off 4.5pt}]  (436.3,142.8) -- (545.3,198.8) ;
			%Shape: Arc [id:dp4582439197474846] 
			\draw  [draw opacity=0] (94.31,133.59) .. controls (99.8,126.54) and (108.37,122) .. (118,122) .. controls (119.96,122) and (121.88,122.19) .. (123.74,122.55) -- (118,152) -- cycle ; \draw   (94.31,133.59) .. controls (99.8,126.54) and (108.37,122) .. (118,122) .. controls (119.96,122) and (121.88,122.19) .. (123.74,122.55) ;  
			%Shape: Arc [id:dp6221251686562133] 
			\draw  [draw opacity=0] (458.31,154.59) .. controls (463.8,147.54) and (472.37,143) .. (482,143) .. controls (483.96,143) and (485.88,143.19) .. (487.74,143.55) -- (482,173) -- cycle ; \draw   (458.31,154.59) .. controls (463.8,147.54) and (472.37,143) .. (482,143) .. controls (483.96,143) and (485.88,143.19) .. (487.74,143.55) ;  
			%Shape: Trapezoid [id:dp8780677279275892] 
			\draw   (223.8,168.8) -- (284.3,128.8) -- (357.8,128.8) -- (418.3,168.8) -- cycle ;
			%Straight Lines [id:da6064077759381246] 
			\draw    (223.8,168.8) -- (309.3,168.8) ;
			\draw [shift={(311.3,168.8)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
			%Straight Lines [id:da6749954958271971] 
			\draw    (374.55,168.8) -- (460.05,168.8) ;
			\draw [shift={(462.05,168.8)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
			%Straight Lines [id:da17678803310922797] 
			\draw    (336.28,153.01) -- (308.32,183.59) ;
			\draw [shift={(306.3,185.8)}, rotate = 312.44] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
			\draw [shift={(338.3,150.8)}, rotate = 132.44] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
			%Straight Lines [id:da08146118787516876] 
			\draw  [dash pattern={on 0.84pt off 2.51pt}]  (152.3,131.8) -- (284.3,41.8) ;
			%Straight Lines [id:da14949669303096136] 
			\draw  [dash pattern={on 0.84pt off 2.51pt}]  (337.3,42.8) -- (463.3,119.8) ;
			%Straight Lines [id:da06827367783992644] 
			\draw    (315.3,128.8) -- (340.3,128.8) ;
			\draw [shift={(343.3,128.8)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
			%Straight Lines [id:da14419860685829722] 
			\draw    (315.3,111.8) -- (315.3,145.8) ;
			\draw [shift={(315.3,148.8)}, rotate = 270] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
			\draw [shift={(315.3,108.8)}, rotate = 90] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
			
			% Text Node
			\draw (118,61.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
			% Text Node
			\draw (482,61.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
			% Text Node
			\draw (443.3,227.2) node [anchor=north west][inner sep=0.75pt]    {$y$};
			% Text Node
			\draw (78.3,227.2) node [anchor=north west][inner sep=0.75pt]    {$y$};
			% Text Node
			\draw (100,235) node [anchor=north west][inner sep=0.75pt]   [align=left] {polarizer};
			% Text Node
			\draw (483,235) node [anchor=north west][inner sep=0.75pt]   [align=left] {analyzer};
			% Text Node
			\draw (98,105.4) node [anchor=north west][inner sep=0.75pt]    {$\theta $};
			% Text Node
			\draw (464,125.4) node [anchor=north west][inner sep=0.75pt]    {$\alpha $};
			% Text Node
			\draw (318,170) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
			% Text Node
			\draw (271,19) node [anchor=north west][inner sep=0.75pt]   [align=left] {optical axes};
			% Text Node
			\draw (311,82.4) node [anchor=north west][inner sep=0.75pt]    {$\vec{E}$};
			
			\end{tikzpicture}
			\vspace*{3mm}
			\caption[]{Imaginary experience recombining the two beams}
		\end{figure}
		and if we seek the probability that a photon passes through the analyser, a photon can choose the path $x$ with a probability $\cos(\theta)^2$, then it has a probability $\cos^2(\alpha)$ to cross the analyser thus a total probability of $\cos^2(\theta)\cos^2(\alpha)$. If it chooses the path $y$, they will have a probability $\sin^2(\theta)\sin^2(\alpha)$ to cross the analyser. The total probability is thus obtained by summing the probabilities of the two possible options:
		
		This result is FALSE! Indeed, classical optics tells us that the intensity is $I\cos^2(\theta-\alpha)$ (\SeeChapter{see section Wave Optics page \pageref{malus law}}) and the correct result is confirmed by experience:
		
		Which is not the same at all!
		
		In fact, to fall back on the results of optics wave, it must be remembered that the probability in quantum physics is obtained through the norm to the square of the probability amplitude (\SeeChapter{see section Wave Quantum Physics page \pageref{de broglie normalization condition}}). Therefore:
		
		and we must add the amplitudes for indistinguishable paths and using basic trigonometric relations, we get:
		
		which gives well:
		
	\end{enumerate}
	Suppose we have a way of knowing whether the photon takes the path $x$ or the path $y$ (impossible in our case, but analogue experiments answering to the question "What path?" were realized with atoms). We could then divide the photons into two classes, those who "chose" the path $x$ and those who "chose" the path $y$.
	
	For photons having chosen the path $x$, we may block the path $y$ by a cache without changing anything, and vice versa for photons having chosen the path $y$ we could block the path $x$. Obviously, the result can then only be ${P'}_\text{tot}$. If we can discriminate between the paths, the result is not the same anymore, the paths are no longer indistinguishable. Under the experimental conditions where it is impossible in principle to distinguish between the paths, we can say either:
	\begin{enumerate}
		\item Either the photon use the both paths at once (...)

		\item Or it make no sense to ask the question "Which path?", Since the experimental conditions do not allow to answer.
	\end{enumerate}
	It must be noticed that if the experience allows to decide between two paths, the result is ${P'}_\text{tot}$, even if we decide not to observe them. It is just enough that the experimental conditions allow, in principle, to distinguish between the two paths.
	
	\subsection{Qubit}
	Ultimately, the idea of quantum computing is therefore to connect
quantum gates in a suitable fashion while protecting the superposition
between the $N$ qubits from any external influence.

We can use the polarization of photons to transmit information, for example by an optical fiber. We decide quite arbitrarily, to assign the value of $1$ bit to a photon polarized along O$x$ and $0$ to a photon polarized along O$y$.

	To investigate the theory, it has become traditional to imagine that the two people who exchange information are conventionally named Alice (A) and Bob (B)... Alice sends to Bob for example the following sequence of polarized photons:
	
	Bob analyses the polarization of these photons with a birefringent plate and derives the message from Alice:
	
	This is obviously not a very efficient way to exchange messages, but it is the basis of quantum cryptography (\SeeChapter{see section Cryptography page \pageref{quantum cryptography}}). However, the interesting question now is: what is the value of the bit that we can attribute for example to a photon polarized at $45^\circ$...? According to the above results, a photon polarized at $45^\circ$. is a linear superposition of photon polarized along O$x$ and a photon polarized along O$y$. A qubit is therefore an entity much richer than a regular bit, which cannot take in the strict sense only the values $0$ and $1$.
	
	In a sense, a qubit can take all values between $0$ and $1$ and therefore contains an infinite amount of information! However, this optimistic statement was immediately denied when we realize that the measurement of qubit can give only the result $0$ or $1$, regardless of the chosen base. However, we can ask ourselves the question of this "hidden information" in the linear superposition and we will see that we can exploit it under certain conditions.
	
	To take into account for the possibility of linear superposition, it is natural to introduce for the mathematical description of the polarization a complex vector space (cause: phasers) in two dimensions corresponding to the polarization plane as we saw it in the section of Wave Optics (see page \pageref{polarization plane}). We denote this vector space $\mathcal{H}$ (we take again the notation of Hilbert spaces) and name the "\NewTerm{Hilbert space of polarization states}\index{Hilbert space of polarization states}".

	We may well decompose the vector corresponding to linear polarizations O$x$ and O$y$ in two ket vectors equation and equation such that any polarization state (whether linear, circular, or other) may decompose according to this basis:
	
	Thus, a linear polarization is described by real coefficients $\lambda,\mu$ but the description of a circular or elliptical polarization obviously require to use complex coefficients!

	The probability amplitudes will correspond to a scalar product on this space. So given two vectors corresponding to two different polarizations:
	
	The Hermitian dot product (\SeeChapter{see section Vector Calculus page \pageref{hermitian inner product}}) will be:
	
	Now a linear polarization state (\SeeChapter{see section Wave Optics page \pageref{linear polarization}}) following $\theta$ will be given logically  by (if we restrict ourselves to the linear case so!):
	
	where $|x\rangle$, $|y\rangle$ are vectors having unit norms. This is consistent with the mental representation:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,638); %set diagram left start at 0, and has height of 638
		
		%Shape: Parallelogram [id:dp49509882578038367] 
		\draw   (211.3,124) -- (211.3,334) -- (317.3,244) -- (317.3,34) -- cycle ;
		%Straight Lines [id:da48135887234948527] 
		\draw    (269.15,192.5) -- (436.3,254) ;
		%Straight Lines [id:da8797967087816945] 
		\draw    (269.15,192.5) -- (235.71,217.22) ;
		\draw [shift={(233.3,219)}, rotate = 323.53] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da8559919969302618] 
		\draw    (269.15,192.5) -- (269.15,143) ;
		\draw [shift={(269.15,140)}, rotate = 90] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da21916529894724568] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (246.22,168) -- (246.22,211.75) ;
		%Straight Lines [id:da21694259999671295] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (246.22,168) -- (269.3,151) ;
		%Straight Lines [id:da2600558309609251] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (225.3,148) -- (292.3,213) ;
		%Shape: Arc [id:dp06390644801297785] 
		\draw  [draw opacity=0] (260.8,182.5) .. controls (262.87,179.64) and (266.02,177.63) .. (269.66,177.06) -- (271.75,190.4) -- cycle ; \draw   (260.8,182.5) .. controls (262.87,179.64) and (266.02,177.63) .. (269.66,177.06) ;  
		
		% Text Node
		\draw (263,119.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (220,214.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (247,209.4) node [anchor=north west][inner sep=0.75pt]    {$E_{y}$};
		% Text Node
		\draw (271.15,143.4) node [anchor=north west][inner sep=0.75pt]    {$E_{x}$};
		% Text Node
		\draw (256,163.4) node [anchor=north west][inner sep=0.75pt]    {$\theta $};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[]{Reminder of field decomposition principle}
	\end{figure}
	where the amplitude is normalized to the unit.

	The probability amplitude for a polarized photon following $\theta$ goes through an analyser oriented following $\alpha$ can now be written:
	
	and the probability of passing through the analyser will always be given by the squared norm of this amplitude as we have prove it earlier above:
	
	In general, we define probability amplitudes, where $|\Phi\rangle$, $|\Psi\rangle$ are polarization states:
	
	and the corresponding probability will be:
	
	We are now ready to tackle the crucial issue of the measure as part of this quantum experiment. Let take again the polariser / analyser experiment, assuming that the analyser is oriented along O$x$. If the polariser is oriented along O$x$, an outgoing photon passes through the polariser analyser with a probability of $100\%$; if the polariser is oriented along O$y$, the probability is $0\%$. The analyser performs a (polarization) test, and the test result is $1$ or $0$. The test allow us to determine the polarization state of the photon.

	But this is not the general case!
	
	Let us assume that the polariser is oriented following the general direction $\theta$ or the orthogonal direction $\theta_{\perp}$ (there is a rotation of $\pi/2$). We then use the properties of the unitary trigonometric circle:
	
	and therefore if the polariser prepares for example the photon in the state $|\theta\rangle$ and the analyser is oriented along O$x$, the probability of success of the test will always be $\cos^2(\theta)$ whatever the type of polarization!! Let us recall that in this example, after the passing through the analyser, the polarization state of the photon is no longer $|\theta\rangle$, but $|x\rangle$. The measurement therefore changes the polarization state.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Of course, another way of seeing that the two vectors above are orthogonal is to make a scalar product and to see that the result is immediately equal to zero.
	\end{tcolorbox}
	We see a difference of principle between measurement in classical physics and measurement in quantum physics. In classical physics, the physical quantity to be measured predates the measurement: if a radar measures the speed of your car at $180$ [km$\cdot$h$^{-1}$] on the highway, this speed pre-existed to its measure by the policeman. On the contrary, in the measurement of polarization of a photon $|\theta\rangle$ by an analyser oriented along O$x$, the fact that the test gives a polarization according to O$x$ does not make it possible to conclude that the photon tested had previously its polarization following O$x$.

	So we have a device preparing the quantum system in the state $|\Phi\rangle$ and a second one capable of "preparing" it in the state $|\Psi\rangle$ that we will use as an analyser. After the test, the quantum system will therefore be in the state $|\Psi\rangle$, which means mathematically that we realize an orthogonal projection on $|\Psi\rangle$.

	Let $\mathcal{P}_\Psi$ this projector, then the vectorial orthogonal projection (\SeeChapter{see section Vector Calculus page \pageref{dot product}}) is given by:
	
	which consists (for recall) of a simple scalar product (scalar orthogonal projection) multiplied by the vector $|\Psi\rangle$. This is easily seen by judiciously placing the parentheses:
	
	and therefore:
	
	The projection of the state vector is named, as we have already seen it (in the section of Wave Quantum Physics), in the Copenhagen interpretation of quantum physics "state vector reduction", or, for historical reasons, "\NewTerm{reduction of the wave packet}\index{reduction of the wave packet}". This reduction of the state vector is a convenient fiction of the Copenhagen interpretation, which avoids having to ask questions about the measurement process ...

	The reader accustomed to Linear Algebra (see section of the same name page \pageref{linear algebra}) will have probably notice trivially that we can manipulate the notation convention of the projector as a matrix (linear mapping) such as in two simple particular cases (those of interest to us):
	
	A reader has asked us to explicit this matrix approach. Let us see how we arrive at this matrix aspect of the orthogonal projection with a particular example of two vectors in a real two-dimensional space. For this let us consider:
	
	and therefore (the procedure is the same for $y$):
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The matrix notation of the orthogonal projector is often presented as a definition of a mathematical tool named "\NewTerm{outer product}\index{outer product}".
	\end{tcolorbox}
	So finally, to get back to our previous subject, we have:
	
	and same for the other component.
	
	As:
	
	We then have:
	
	We will notice that the identity operator can be written as the sum of the two projectors $\mathcal{P}_x$,$\mathcal{P}_y$:
	
	relation named "\NewTerm{closure relation}", which can be generalized to an orthonormal basis of a Hilbert space $\mathcal{H}$ of dimension $N$:
	
	Moreover, the projectors $\mathcal{P}_x$,  $\mathcal{P}_y$ commute (trivial verification):
	
	Thus, the tests $|x\rangle$, $|y\rangle$ are compatible (whatever the direction of the measurement the result is independent). In contrast, the projectors $\mathcal{P}_\theta$, $\mathcal{P}_{\theta,\perp}$:
	
	which satisfy (trivial verification) to:
	
	as well as (trivial verification):
	
	Do not commute with $\mathcal{P}_x$,$\mathcal{P}_y$:
	
	and therefore the tests $|x\rangle$ and $|\theta\rangle$ are incompatible.

	For the next developments, it will be useful to notice that the knowledge of the probabilities of success of a $T$ test makes it possible to define an average value (expected mean):
	
	In analogy with the context, we can read this as following: the expected mean of the test is equal to the representative value of the photon oriented according to O$x$ (corresponding arbitrarily  to the value $1$) multiplied by the probability of passing through the analyser oriented also according to O$x$ (therefore test concluding at $100\%$) summed with the representative value of the photon oriented according to O$y$ (corresponding arbitrarily to the value $0$) multiplied by the probability of passing through the analyser always oriented along O$x$ (therefore $0\%$ of the photons O$y$ will pass the O$x$ test).

	For example, if the test $T$ is represented by the procedure $|\Phi\rangle$ and we apply it to a state $|\Phi\rangle$ (containing as we have seen above the  representative values of the linearly or non-linearly polarized photon...) then the test corresponds to a scalar product:
	
	And as we have seen it in the section of Wave Quantum Physics, we know that in fact (see page \pageref{fifth postulate of wave quantum physics}):
	
	is the mean value of an operator $M$ in the state $|\Phi\rangle$. Thus, to the test $T$ to which a procedure $|\Psi\rangle$ is associated, we can associate the projector $\mathcal{P}_{\Phi}$ whose mean value in the state $|\Phi$ gives the probability of success of the test.

	The generalization of this observation makes it possible to construct the physical properties of a quantum system. Let us give an example by returning to the case of polarization. Suppose that we construct (quite arbitrarily) a property $\mathcal{M}$ of a photon as follows:
	\begin{itemize}
		\item $\mathcal{M}$ is equal to $+1$ if the photon is polarized following O$x$

		\item $\mathcal{M}$ is equal to $-1$ if the photon is polarized according to O$y$
	\end{itemize}
	We can associate with the physical property $\mathcal{M}$ the Hermitian operator:
	
	Which satisfies (trivial) the relation between operator, eigenvalue and vector:
	
	The mean value (expected mean) of $M$ then being (by definition):
	
	Let us assume the photon in the linear polarization state of angle $\theta$, then the mean value $\rangle M \langle_\theta$ in the state $|\theta\rangle$ is (trivial):
	
	Before seeing, how can we construct such an operator $M$ with another object than the photon and with the same properties, let us introduce a mathematical tool generalizing the conditions and the configuration of any polarized wave:
	
	\subsubsection{Bloch sphere}
	The "\NewTerm{Bloch sphere}\index{Bloch sphere}" (11946 according to holocene calendar) is, as we will see, a geometrical representation of the pure states of the qubits (two-level quantum mechanical system) as points of the surface of a sphere, named after the physicist Felix Bloch

	A given number of elementary operations done in quantum computing can be carried out with this sphere under the choice of a suitable projector .

	We will see that a state of an arbitrary qubit (vector in the complex plane) can be written:
	
	where $\gamma\in\mathbb{R}$ and $0\le \phi\leq 2\pi$, define a point on the three-dimensional Bloch sphere and where we have the two basic vectors:
	
	for which sometimes the definition is reversed (but it does not matter as long as it forms an orthogonal basis!).

	The qubits represented by arbitrary values $\gamma$ (global gauge invariance according to $U(1)$ as seen in the section of Set Algebra page \pageref{set algebra}) are all represented by the same point on the Bloch sphere because we will show that the factor has no observable effect and that we can then write without losing in generality:
	
	which is represented as we will justify later by the figure below:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[line cap=round, line join=round, >=Triangle,scale=1.5]
		  \clip(-2.19,-2.49) rectangle (2.66,2.58);
		  \draw [shift={(0,0)}, lightgray, fill, fill opacity=0.1] (0,0) -- (56.7:0.4) arc (56.7:90.:0.4) -- cycle;
		  \draw [shift={(0,0)}, lightgray, fill, fill opacity=0.1] (0,0) -- (-135.7:0.4) arc (-135.7:-33.2:0.4) -- cycle;
		  \draw(0,0) circle (2cm);
		  \draw [rotate around={0.:(0.,0.)},dash pattern=on 3pt off 3pt] (0,0) ellipse (2cm and 0.9cm);
		  \draw (0,0)-- (0.70,1.07);
		  \draw [->] (0,0) -- (0,2);
		  \draw [->] (0,0) -- (-0.81,-0.79);
		  \draw [->] (0,0) -- (2,0);
		  \draw [dotted] (0.7,1)-- (0.7,-0.46);
		  \draw [dotted] (0,0)-- (0.7,-0.46);
		  \draw (-0.08,-0.3) node[anchor=north west] {$\phi$};
		  \draw (0.01,0.9) node[anchor=north west] {$\theta$};
		  \draw (-1.01,-0.72) node[anchor=north west] {${\vec{x}}$};
		  \draw (2.07,0.3) node[anchor=north west] {${\vec{y}}$};
		  \draw (-0.5,2.6) node[anchor=north west] {${\vec{z}=|0\rangle}$};
		  \draw (-0.4,-2) node[anchor=north west] {$-{\vec{z}=|1\rangle}$};
		  \draw (0.4,1.65) node[anchor=north west] {$|\Psi\rangle$};
		  \scriptsize
		  \draw [fill] (0,0) circle (1.5pt);
		  \draw [fill] (0.7,1.1) circle (0.5pt);
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Bloch-shpere (two dimensional one)}
	\end{figure}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Let us assume that we have a Quantum System with $k$ qubits:
	
	The qubit will be in a superimposed state:
	
	where $\alpha_i$ is a complex vector with the property $\sum \alpha_i =1$.
	\end{tcolorbox}	
	
	The Bloch sphere is a generalization of the representation of a complex number $z$ (\SeeChapter{see section Numbers page \pageref{complex numbers}}) with $|z|^2=1$ as point of the circle in the (Gauss-) plane.

	We also saw in this same section that a complex number could be represented by a complex exponential such that:
	
	and if the circle were unitary:
	
	Let us notice that the constraint $|z|^2=1$ eliminates a degree of freedom.

	We will now notice the decomposition of a state of polarization in the form:
	
	And this one in a more traditional form in quantum computing (logic when we see the bases ...):
	
	where $\alpha,\beta\in\mathbb{C}$ (yes! indeed we are not anymore in the simple case of a wave linearly polarized now ...!) without forgetting the condition of normalization:
	
	We can therefore write the qubit in the form:
	
	
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Caution! It is very important to have read the part dealing with the polarization of light in the section of Wave Optics (see page \pageref{light polarization}) to understand that it does not come from nowhere!!! With the difference that we do not work here with phasors because the solution of Schrödinger's equation of evolution involves complex exponentials as we saw in the context of the resolution of the latter for the eigen-mode of free particle.
	\end{tcolorbox}
	Adding an overall phase factor should have no influence on the coefficients $\alpha$, $\beta$ because:
	
	and similarly for $|\beta|^2$. Thus, we are free to multiply our polarized and standardized qubit:
	
	by the global phase $e^{-\mathrm{i}\phi_a}$ which gives:
	
	In addition, we always have the condition of normalization $\langle \Psi' | \Psi\rangle$ to respect (impose).
	
	Returning to the Cartesian coordinates, we have:
	
	And the normalization constraint then gives:
	
	what is the equation of a unit sphere in $\mathbb{R}^2$ space with the Cartesian coordinates $(x,y,r_\alpha)$. Hence the quantum origin of the Bloch sphere!
	
	We know (\SeeChapter{see section Vector Calculus page \pageref{spherical coordinates}}) that the Cartesian coordinates are connected to the spherical coordinates by the relations:
	
	therefore by putting $z:=r_\alpha$ and remembering that $r=1$, we can write:
	
	We now have only two useful parameters to know to define our point on the unit sphere (and always to the arbitrariness of phase).
	
	The reader will notice that unlike the linearly polarized qubit, the general case above adds a complex term and that inversely by removing this additional term, we fall back on the relation of the linearly polarized wave seen at the beginning of this section. However, notice (for general culture) that if we add the same complex term to the first term, then we also have the very common following representation of the linearly polarized wave whose writing is named "\NewTerm{Jones vector}\index{Jones vectors}":
	
	But we have not finished yet!

	Let us return to (we remove the apostrophe for the state of polarization):
	
	and notice that:
	
	and:
	
	without forgetting that $\phi\in\mathbb{R}$ and that in this case the exponential factor in front of the $|1\rangle$ is a global phase change without influence.

	All this suggests that $0\le \theta \le \pi/2 $ is sufficient to describe any state of polarization and hence all points of the Bloch sphere.

	On the other hand, we can see that in the system $(r,\theta,\phi)$ the point of coordinates $(1,\pi-\theta,\phi+\pi)$ is the point opposite to that of coordinates $(1,\theta,\pi)$:
	
	We also have (this is quite immediate but we can detail on request):
	
	and therefore opposite points on the Bloch's sphere correspond orthogonal qubits (states of polarization)!

	Thus we can consider only the upper hemisphere of the Bloch sphere since the opposite points differ only from a phase factor $-1$ and are therefore equivalent in the representation of the Bloch sphere.

	Thus, the relation:
	
	is sufficient to describe the whole Bloch sphere in a complex space of dimension $2$.

	By construction, each point given by the previous relation of dimension $2$ contains a double representation of a rotation in the real space of dimension $3$.

	We have also seen in the chapter of Spinor Calculus (see page \pageref{spinors}) that a rotation could be written in the form:
	
	With for recall the Pauli Matrices (\SeeChapter{see section Relativistic Quantum Physics page \pageref{pauli matrices}}):
	
	Either with the usual (traditional) writing of the field of Quantum Computing:
	
	Either after simplifying the last matrix:
	
	Now let us consider the rotation of our polarization state vector (due to a projector):
	
	To get a coefficient of $|0\rangle$ that is real (in order to have an observable in the projection of an axis), we multiply by a phase factor $e^{\mathrm{i}\alpha/2}$ giving:
	
	So to get a rotation around the $z$-axis it enough to change $\phi\rightarrow \phi+\alpha$.

	So if we come back to:
	
	it can be shown in the same way that in a general framework a unitary qubit operator can be written in the observable form:
	
	It is then necessary to choose the angles and the axis of rotation to define the operator completely.
	
	\paragraph{Qubit of polarization}\mbox{}\\\\
	We will come back here on the case of polarization of the photon but this time we will be able to generalize thanks to the formulation of the Bloch's sphere to any type of polarization.

	Let us consider for this a polariser that only passes photons polarized vertically followed by a photodetector, which do a "click" if a photon is detected and nothing else. This device allows us to detect vertically polarized photons.

	Let us translate this in the language of quantum mechanics: The states of the system are therefore the states of polarization of a photon. The measurements of the observable will also have its states of polarization.

	The possible measures are:
	
	We will denote the corresponding states $|x\rangle$, $|y\rangle$. In our configuration, it is then obvious that the couple $(\lambda_0,\lambda_1)$ represents the eigenvalues and $|x\rangle$, $|y\rangle$ the eigenvectors of an operator (which we do not know for know) and that we will therefore denote $\mathcal{P}$.

	As we know, $|x\rangle$, $|y\rangle$ is an orthonormal basis of the space of states (of polarization). This is the base named "H/V base" for "Horizontal/Vertical" and is denoted normally:
	
	Let us now take several cases:
	\begin{enumerate}
		\item Given a photon in the sate:
		
		then:
		
	
		\item Let a photon be in the semi-vertical / horizontal state, that is to say oblique (which can be assimilated to the quantum superposition of its two polarizations):
		
		where the root is just there to ensure the normalization condition:  $\langle \Psi|\Psi\rangle=1$. Indeed:
		
		Then since there is superposition (i.e. half of each in the total wave):
		
		
		\item Let us now take any polarization (and this is what we did not have before!):
		
		which is well-normalized as we know it. So:
		
		The sum of the two probabilities giving indeed $1$!
	\end{enumerate}
	Now, let's imagine that we turn the polariser of $\pi/4$. We will denote the new basis of this polariser $|0'\rangle$, $|1'\rangle$ determined by a rotation of angle $\pi/4$ with by construction:
	
	where the first base therefore corresponds to the diagonal polarization and the second is named "antidiagonal polarization". It is therefore the "\NewTerm{D/A base}" (Diagonal / Antidiagonal). In the form of the Jones vectors the last two relations are written:
	
	If we imagine that we have two polarisers that follow one another. The first having the D/A base and the second the H/V base, the first will prepare the photon polarized generally in a particular state (polarization) which will be by construction be the oblique state for the base H/V. Thus our second polariser will have only situations of the type:
	
	Thus, this shows that any measure obviously disturbs the state of polarization of the photon and thus disrupts the state of the system. This last result is used in quantum cryptography!

	We notice by the way that by using the spin operator introduced during our study of the section on Wave Quantum Physics (see page \pageref{spin operator}) we have:
	
	which is of the same form as the following relation (relation linking eigenvector and eigenvalue) obtained in the in the section of Wave Quantum Physics:
	
	And that indicates indeed that the state:
	
	seems to be necessarily associated with a $1/2$ spin particle. We also notice that this state is an eigenvector of the operator $S_x$ associated with the eigenvalue $\hbar/2$. Indeed:
	
	Since in the present case with the operator $S_x$ we have immediately the eigenvalue and the associated eigenvector without making any intermediate calculations, it comes that the probability of measuring this eigenvalue (refer for recall to the 4th postulate of Wave Quantum Physics which associates operator to a measurement through the eigenvalue) is easy to calculate because the eigenvector is equal to the state vector in this particular case. Therefore:
	
	
	\paragraph{$1/2$ spin Qubit}\mbox{}\\\\
	We will see here how to build a qubit based on a particle with a $1/2$ spin.

	In the study of the Bloch sphere, we have examined a qubit at a given instant and we have seen that in a Hilbert space $H$, this qubit is described (by choice) by a unit vector:
	
	decomposed into an orthonormal basis $(|0\rangle, |1\rangle)$.

	Let us consider the most general and minimum initial state corresponding to an arbitrary orientation of a spin:
	
	Which corresponds, as we know, to two opposite (and superposed) states on the Bloch sphere.
	
	Let us notice that we have indeed a normalized probability of the form:
	
	We have also seen that the projection along $z$ by the operator $R_z(\alpha)$ of the state $\Psi$ is given to the phase arbitrariness by:
	
	Now let u recall our example:
	
	where $M$ was a Hermitian operator. Now the Pauli matrices are simple Hermitian operators. Moreover, as we have demonstrated in the section of Spinor Calculus, the Hermitian operator $\sigma_z$ (assimilated to $M$) has by chance the same eigenvalues and eigenvectors corresponding to the two relations above. But it is then written in a traditional way as we saw it in the section of Spinor Calculus:
	
	or even more condensed:
	
	Moreover, this operator also satisfies the relation:
	
	And what is the physical property associated with $\sigma_z$? Well this is the spin (!) and we will come back to it a bit further below because that means we can use the spin $1/2$ as a qubit.

	Now let us introduce the evolution of the system on this projection because it is not static (but this would not change this particular case to consider it static).

	We have seen in the section of Wave Quantum Physics that this operation consisted in a simple case (as here) of introducing a phase term dependent on time of the type (see page \pageref{operator of evolution}):
	
	Therefore:
	
	What is written more soberly:
	
	Let us recall that the state of a $1/2$ spin particle is two-dimensional and described by the state matrix (\SeeChapter{see section Relativistic Quantum Physics page \pageref{emerging electron spin value}}):
	
	with:
	
	If we want to calculate the expected mean of the observable (physical property) along each axis, then we use the $5$th postulate (see page \pageref{fifth postulate of wave quantum physics}) in the case of the $x$-axis:
	
	Follows the $y$-axis:
	
	and finally following the $z$-axis:
	
	For thus fall back well for the component $z$ (because it is the only one that interests us here), the result which was imposed above in the form:
	
	with an angle difference which is a matter of substitution and an amplitude which makes it possible to match the particularity of the configuration of the system. We therefore have mathematical relations similar in all respects to the manipulation of qubits of oriented spins or qubits of polarized photons.

	The question we can ask ourselves about the spin is how to prepare it in the state $|\Psi\rangle$ In fact, we can do this with a magnetic field by copying the Stern-Gerlach experiment, which allows us to separate a beam of spin $1/2$ particles into two distinct beam.

	Since we now know all the eigenvalues and eigenvectors of the spin operator $S$, we can then determine the general form of the spin operator in any orientation (!):
	
	We have also:
	
	We have the chosen $|\Psi\rangle$ which is, in addition to being a state vector, an eigenvector of $S_n$ with the eigenvalue $\hbar/2$. Therefore the probability of measuring this particular state vector is equal to $1$.

	Now we know by using the $5$th postulate that the probability of finding the eigenvalue $\hbar/2$(of the operator $S_n$), in a measurement of the property $S$ along the $z$-axis performed at time $t$ on the quantum system prepared in the state $\Psi$, is given by the square of the module of the projection of the function or state vector $\Psi$ on the vector or eigenvector $\varphi$ associated with the eigenvalue $\hbar/2$ (and its operator along this axis).

	But the operator according to $z$ is:
	
	For the same operator, we have seen in the section of Spinor Calculus that it had as eigenvectors (see page \pageref{spinors}):
	
	Let us take the first eigenvector oriented according to $z$-axis on the Bloch sphere. We have then:
	
	And the probability according to the other eigenvector would give the same expression but with a sinus. The sum of the two probabilities would then give us well $1$!

	We thus notice that the relations are very similar between the photon and the spin in our case of study. This is normal since both are two-level systems, resulting in similar results.
	
	\subsection{Entangled qubit system}
	A $2$ qubit system is a superposition of all possible $2$ qubit states. A $2$ qubit system and can be represented as:
	
	Measuring the $2$ qubit system, as earlier, results in the collapse of the superposition and the result is one of 4 qubit states. The probability of the measure state is the square of the amplitude $|\alpha_{ij}|^2$.
	
	More specifically a 2 qubit system in which we have:
	
	the tensor product of the $2$ qubits is:
	
	where (notice that we have still a set of orthonormal vectors):
	
	
	\subsection{Quantum logic gates}
	In quantum computing and specifically the quantum circuit model of computation, a quantum gate (or quantum logic gate) is a basic quantum circuit operating on a small number of qubits. They are the building blocks of quantum circuits, like classical logic gates are for conventional digital circuits.
	
	Let us recall that we have stated above that the construction of a quantum qubit was done on the vector basis:
	
	or in extenso, by linearity (and this is all elementary trick!), any transformation that acts on these basic vectors, will therefore act on all qubit of the complex plane.

	Let us consider the particular case for this introduction the logic gate which modifies the state of the $1$-qubit, that is to say qubit which are collinear to one of the basis vectors, into the opposite state. That is to say the "\NewTerm{quantum inverter}":
	
	We quickly guess that the matrix that satisfies this relation (caution! the reader will notice that it is not a special case of the rotation matrix in the plane as seen in the section of Euclidean Geometry!):
	
	And which is sometimes written by specialists:
	
	And we see that this logical gate of negation of the $1$-qubit is nothing else than the first matrix of Pauli. So we can also write:
	
	Negation which is sometimes named "\NewTerm{$X$-Pauli quantum gate}" or "\NewTerm{NOT quantum gate}\index{NOT quantum gate}" and represented by the following symbol in "\NewTerm{quantum circuits}\index{quantum circuits}":
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,638); %set diagram left start at 0, and has height of 638

		%Straight Lines [id:da40928576288334284] 
		\draw    (454.3,125) -- (167.3,125) ;
		%Shape: Circle [id:dp7784874581849723] 
		\draw   (260.3,125) .. controls (260.3,96.56) and (283.36,73.5) .. (311.8,73.5) .. controls (340.24,73.5) and (363.3,96.56) .. (363.3,125) .. controls (363.3,153.44) and (340.24,176.5) .. (311.8,176.5) .. controls (283.36,176.5) and (260.3,153.44) .. (260.3,125) -- cycle ;
		%Straight Lines [id:da11570425324543421] 
		\draw    (311.8,73.5) -- (311.8,176.5) ;
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{NOT Quantum gate}
	\end{figure}
	Now let us look for the logic gate making the following orthogonal clockwise transformation:
	
	We guess quite quick that the matrix which satisfies this relation (caution! the reader will notice that this time is a special case of the rotation matrix in the plane as seen in the section of Euclidean Geometry!):
	
	And which is sometimes written by specialists (in a somewhat unfortunate way):
	
	
	And we see that this logical transformation gate in the clockwise direction of the $1$-qubit involves the second Pauli matrix. So we can also write:
	
	Negation which is sometimes named "\NewTerm{$Y$-Pauli quantum gate}\index{$Y$-Pauli gate}" and which has no classical equivalent.
	
	Now let us seek for the logical gate making the following strictly counter-clockwise orthogonal transformation:
	
	We guess quite quick that the matrix that satisfies this relation (caution! the reader will notice that it is not a special case of the rotation matrix in the plane seen in the section of Euclidean Geometry!):
	
	And which is sometimes written by specialists (in a somewhat unfortunate way):
	
	And we see that this logical transformation gate in the clockwise direction of $1$-qubit involves the second Pauli matrix. So we can also write:
	
	Negation then we sometimes name "\NewTerm{$Z$-Pauli quantum gate}\index{$Z$-Pauli quantum gate}".
	
	When describing a quantum gate on an individual qubit, any dynamical operation, $G$, is a member of the unitary group $\text{U}(2)$ (\SeeChapter{see section Set Algebra page \pageref{set algebra}}), which consists of all $2\times 2$ matrices where $G^\dagger=G^{-1}$. Up to a global (and unphysical) phase factor, any single qubit operation can be expressed as a linear combination of the generator of $\text{SU}(2)$ that are the Pauli matrices!
	
	Now let us return to the following transformation which we have deal with earlier:
	
	In other words, it is the transformation (diagonal for recall) that makes to qubit equiprobable\footnote{As the sum of square of the amplitudes is indeed equal to: $(1/\sqrt{2})^2+(1/\sqrt{2})^2=0.5+0.5=1$}:
	
	We quickly notice that the corresponding matrix is then:
	
	which is named "\NewTerm{Hadamard quantum gate}\index{Hadamard quantum gate}\label{hadamard quantum gate}" and thus corresponds to a counter-clockwise rotation of $\pi/4$. This gate is represented by the following symbol in quantum circuits:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,638); %set diagram left start at 0, and has height of 638
		
		%Shape: Square [id:dp48227530078484016] 
		\draw   (260,72) -- (361,72) -- (361,173) -- (260,173) -- cycle ;
		%Straight Lines [id:da40928576288334284] 
		\draw    (260.3,125) -- (167.3,125) ;
		%Straight Lines [id:da31991632627297495] 
		\draw    (454.3,124) -- (361.3,124) ;
		% Text Node
		\draw (296,110) node [anchor=north west][inner sep=0.75pt]  [font=\Huge]  {$H$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Hadamard Quantum gate}
	\end{figure}
	And we can continue like this for a long time to create empirical quantum logic gates ... so we will stop here.
	
	Now remember that as we have seen in the section of Boolean logic (see page \pageref{all gates from NAND}), with a NAND gate (or an AND gate and an INVERTER) we've got all the parts we need to build a modern computer!

	We have already seen the quantum inverter earlier above and its corresponding matrix. We need now and AND gate, however that latter has two input, ie entangles quantum bits. 

	We built a "\NewTerm{quantum AND gate}" as following:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1,scale=0.9]
		%uncomment if require: \path (0,1196); %set diagram left start at 0, and has height of 1196
		
		%Shape: And Gate [id:dp6088918615837711] 
		\draw   (66.1,69) -- (93.25,69) .. controls (108.23,69) and (120.4,82.44) .. (120.4,99) .. controls (120.4,115.56) and (108.23,129) .. (93.25,129) -- (66.1,129) -- (66.1,69) -- cycle (48,79) -- (66.1,79) (48,119) -- (66.1,119) (120.4,99) -- (138.5,99) ;
		%Right Arrow [id:dp8164253132294652] 
		\draw  [fill={rgb, 255:red, 182; green, 215; blue, 255 }  ,fill opacity=1 ] (172,84.75) -- (198.1,84.75) -- (198.1,71) -- (215.5,98.5) -- (198.1,126) -- (198.1,112.25) -- (172,112.25) -- cycle ;
		%Shape: And Gate [id:dp16385294130573813] 
		\draw   (264.1,69) -- (291.25,69) .. controls (306.23,69) and (318.4,82.44) .. (318.4,99) .. controls (318.4,115.56) and (306.23,129) .. (291.25,129) -- (264.1,129) -- (264.1,69) -- cycle (246,79) -- (264.1,79) (246,119) -- (264.1,119) (318.4,99) -- (336.5,99) ;
		%Right Arrow [id:dp5731302291647375] 
		\draw  [fill={rgb, 255:red, 182; green, 215; blue, 255 }  ,fill opacity=1 ] (369,84.75) -- (395.1,84.75) -- (395.1,71) -- (412.5,98.5) -- (395.1,126) -- (395.1,112.25) -- (369,112.25) -- cycle ;
		%Shape: And Gate [id:dp7677298230952041] 
		\draw   (66.1,182) -- (93.25,182) .. controls (108.23,182) and (120.4,195.44) .. (120.4,212) .. controls (120.4,228.56) and (108.23,242) .. (93.25,242) -- (66.1,242) -- (66.1,182) -- cycle (48,192) -- (66.1,192) (48,232) -- (66.1,232) (120.4,212) -- (138.5,212) ;
		%Right Arrow [id:dp7653590604262914] 
		\draw  [fill={rgb, 255:red, 182; green, 215; blue, 255 }  ,fill opacity=1 ] (172,197.75) -- (198.1,197.75) -- (198.1,184) -- (215.5,211.5) -- (198.1,239) -- (198.1,225.25) -- (172,225.25) -- cycle ;
		%Shape: And Gate [id:dp9842907174554025] 
		\draw   (264.1,182) -- (291.25,182) .. controls (306.23,182) and (318.4,195.44) .. (318.4,212) .. controls (318.4,228.56) and (306.23,242) .. (291.25,242) -- (264.1,242) -- (264.1,182) -- cycle (246,192) -- (264.1,192) (246,232) -- (264.1,232) (318.4,212) -- (336.5,212) ;
		%Right Arrow [id:dp47588673316796126] 
		\draw  [fill={rgb, 255:red, 182; green, 215; blue, 255 }  ,fill opacity=1 ] (369,197.75) -- (395.1,197.75) -- (395.1,184) -- (412.5,211.5) -- (395.1,239) -- (395.1,225.25) -- (369,225.25) -- cycle ;
		%Right Arrow [id:dp4563861749379525] 
		\draw  [fill={rgb, 255:red, 182; green, 215; blue, 255 }  ,fill opacity=1 ] (172,310.75) -- (198.1,310.75) -- (198.1,297) -- (215.5,324.5) -- (198.1,352) -- (198.1,338.25) -- (172,338.25) -- cycle ;
		%Shape: Not/Inverter Gate [id:dp0681520561305895] 
		\draw   (67.81,293) -- (112.26,323) -- (67.81,353) -- (67.81,293) -- cycle (53,323) -- (67.81,323) (121.15,323) -- (133,323) (112.26,323) .. controls (112.26,319.69) and (114.25,317) .. (116.7,317) .. controls (119.16,317) and (121.15,319.69) .. (121.15,323) .. controls (121.15,326.31) and (119.16,329) .. (116.7,329) .. controls (114.25,329) and (112.26,326.31) .. (112.26,323) -- cycle ;
		
		% Text Node
		\draw (20,69.4) node [anchor=north west][inner sep=0.75pt]    {$\ket{0}$};
		% Text Node
		\draw (20,109.4) node [anchor=north west][inner sep=0.75pt]    {$\ket{1}$};
		% Text Node
		\draw (71,90) node [anchor=north west][inner sep=0.75pt]   [align=left] {AND};
		% Text Node
		\draw (138,89.4) node [anchor=north west][inner sep=0.75pt]    {$\ket{0}$};
		% Text Node
		\draw (219,90.4) node [anchor=north west][inner sep=0.75pt]    {$\ket{01}$};
		% Text Node
		\draw (269,90) node [anchor=north west][inner sep=0.75pt]   [align=left] {AND};
		% Text Node
		\draw (338,89.4) node [anchor=north west][inner sep=0.75pt]    {$\ket{0}$};
		% Text Node
		\draw (415,56.4) node [anchor=north west][inner sep=0.75pt]    {$\begin{bmatrix}
		1 & 1 & 1 & 0\\
		0 & 0 & 0 & 1
		\end{bmatrix} \cdot \begin{bmatrix}
		0\\
		1\\
		0\\
		0
		\end{bmatrix} =\begin{bmatrix}
		1\\
		0
		\end{bmatrix} =\ket{0}$};
		% Text Node
		\draw (20,182.4) node [anchor=north west][inner sep=0.75pt]    {$\ket{1}$};
		% Text Node
		\draw (20,222.4) node [anchor=north west][inner sep=0.75pt]    {$\ket{1}$};
		% Text Node
		\draw (71,203) node [anchor=north west][inner sep=0.75pt]   [align=left] {AND};
		% Text Node
		\draw (138,202.4) node [anchor=north west][inner sep=0.75pt]    {$\ket{1}$};
		% Text Node
		\draw (219,203.4) node [anchor=north west][inner sep=0.75pt]    {$\ket{11}$};
		% Text Node
		\draw (269,203) node [anchor=north west][inner sep=0.75pt]   [align=left] {AND};
		% Text Node
		\draw (338,202.4) node [anchor=north west][inner sep=0.75pt]    {$\ket{0}$};
		% Text Node
		\draw (415,169.4) node [anchor=north west][inner sep=0.75pt]    {$\begin{bmatrix}
		1 & 1 & 1 & 0\\
		0 & 0 & 0 & 1
		\end{bmatrix} \cdot \begin{bmatrix}
		0\\
		0\\
		0\\
		1
		\end{bmatrix} =\begin{bmatrix}
		0\\
		1
		\end{bmatrix} =\ket{1}$};
		% Text Node
		\draw (20,313.4) node [anchor=north west][inner sep=0.75pt]    {$\ket{0}$};
		% Text Node
		\draw (138,313.4) node [anchor=north west][inner sep=0.75pt]    {$\ket{1}$};
		% Text Node
		\draw (224,303.4) node [anchor=north west][inner sep=0.75pt]    {$\begin{bmatrix}
		0 & 1\\
		1 & 0
		\end{bmatrix} \cdot \begin{bmatrix}
		1\\
		0
		\end{bmatrix} =\begin{bmatrix}
		0\\
		1
		\end{bmatrix} =\ket{1}$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{NAND Quantum Gate}
	\end{figure}

	\begin{flushright}
	\begin{tabular}{l c}
	\circled{60} & \pbox{20cm}{\score{2}{5} \\ {\tiny 22 votes,  53.64\%}} 
	\end{tabular} 
	\end{flushright}