%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Proof Theory}\label{proof theory}
	\lettrine[lines=4]{\color{BrickRed}W}{e} have chosen to begin the study of Applied Mathematics by the theory that seems to us the most fundamental and important in the field of pure and exact sciences: Proof Theory and its underlying formal systems! The proof theory and theory of propositional calculus (logic) have five main objectives through this book:
	\begin{enumerate}
		\item Teach to the reader how to reason and demonstrate (prove), and this independently of the specialization field.
		
		\item Show that the process of a demonstration (proof) is independent of the language used.
		
		\item Prepare the reader to the Logic Theory (\SeeChapter{see section Logic Systems page \pageref{logical systems}}).
		
		\item Prepare the path to Gödel's incompleteness theorem (main goal of this section!).
		
		\item Prepare the reader to the Automata Theory (\SeeChapter{see section Automata Theory page \pageref{automata theory}}).
	\end{enumerate} 
	
	Gödel's incompleteness theorem is probably the most exciting point of Proof Theory because if we define religion as a system of thought that contains unprovable statements, then it contains elements of faith, and Gödel tells us that mathematics is not only a religion, but that then it is the only religion that can prove it is one!
	
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Gödel's incompleteness theorem state that in any axiomatic system there are some statements that cannot be determined to be true or false. The incompleteness theorem seems like something horrible, and for mathematics, it is! However, it does not apply to physics (or any other experimental science), because physics does not rely on logical proof in axiomatic systems, it relies on empirical evidence!
	\end{tcolorbox}
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} It is (very) strongly advised to read this section in parallel with those on Automata Theory (page \pageref{automata theory}) and Logical Systems (page \pageref{logical systems}), including Boolean Algebra (page \pageref{boolean algebra}), available in Theoretical Computing (page \pageref{theoretical computing}) chapter of this book.\\

	\textbf{R2.} We must approach Proof Theory as a sympathetic curiosity but which basically brings nothing much except working/reasoning methods. Moreover, its purpose is not to show that everything is demonstrable but that any proof can be done on a common language starting from a finite number of rules.
	\end{tcolorbox}
	As illustrated in the figure below, taken from the book \cite{tegmark2014our}, formal system are at the root of the two main targets of the present book that are General Relativity and Quantum Field Theory:
	
	\begin{figure}[H]
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1,scale=0.98]
		%uncomment if require: \path (0,1029); %set diagram left start at 0, and has height of 1029
		
		%Shape: Rectangle [id:dp8145278330999122] 
		\draw  [dash pattern={on 6.75pt off 4.5pt}][line width=2.25]  (16,78.33) -- (86,78.33) -- (86,118.33) -- (16,118.33) -- cycle ;
		%Straight Lines [id:da23170618849604008] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (51,137) -- (51,120) ;
		\draw [shift={(51,117)}, rotate = 90] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Rectangle [id:dp5656662816638118] 
		\draw  [dash pattern={on 6.75pt off 4.5pt}][line width=2.25]  (107,78.33) -- (177,78.33) -- (177,118.33) -- (107,118.33) -- cycle ;
		%Straight Lines [id:da3066018646230011] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (104.4,137.45) -- (115.76,121.45) ;
		\draw [shift={(117.5,119)}, rotate = 125.38] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Rectangle [id:dp5927126904356437] 
		\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]  (89.33,8) -- (344.33,8) -- (344.33,71.47) -- (89.33,71.47) -- cycle ;
		%Shape: Rectangle [id:dp04331976968493256] 
		\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]  (351.33,8) -- (606.33,8) -- (606.33,71.47) -- (351.33,71.47) -- cycle ;
		%Shape: Rectangle [id:dp20033583375925335] 
		\draw  [line width=2.25]  (347.67,137.45) -- (451.4,137.45) -- (451.4,179.22) -- (347.67,179.22) -- cycle ;
		%Shape: Rectangle [id:dp343942197794602] 
		\draw  [line width=2.25]  (0.67,137.45) -- (104.4,137.45) -- (104.4,179.22) -- (0.67,179.22) -- cycle ;
		%Shape: Rectangle [id:dp9905128034527846] 
		\draw  [line width=2.25]  (108.67,137.45) -- (212.4,137.45) -- (212.4,179.22) -- (108.67,179.22) -- cycle ;
		%Shape: Rectangle [id:dp8108137153765336] 
		\draw  [line width=2.25]  (218.67,137.45) -- (322.4,137.45) -- (322.4,179.22) -- (218.67,179.22) -- cycle ;
		%Straight Lines [id:da9036804609391371] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (231.4,137) -- (231.4,76) ;
		\draw [shift={(231.4,73)}, rotate = 90] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Straight Lines [id:da011668517920057964] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (369.5,137.33) -- (327.19,75.48) ;
		\draw [shift={(325.5,73)}, rotate = 55.63] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Rectangle [id:dp2712343182131225] 
		\draw  [line width=2.25]  (459.67,137.45) -- (563.4,137.45) -- (563.4,179.22) -- (459.67,179.22) -- cycle ;
		%Image [id:dp04949818001530293] 
		\draw (107.93,24.16) node  {\includegraphics[width=24.89pt,height=20.64pt]{img/intro/general_relativity.jpg}};
		%Image [id:dp30570894189484177] 
		\draw (373.7,21.93) node  {\includegraphics[width=25.05pt,height=16.75pt]{img/intro/quantum_field_theory.png}};
		%Shape: Rectangle [id:dp6293753693310145] 
		\draw  [line width=2.25]  (69,204.5) -- (139,204.5) -- (139,244.5) -- (69,244.5) -- cycle ;
		%Straight Lines [id:da9431651838816042] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (127,204.35) -- (127,193) -- (127,182.15) ;
		\draw [shift={(127,179.15)}, rotate = 90] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Straight Lines [id:da967339864891964] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (278.5,137.23) -- (354.2,73.92) ;
		\draw [shift={(356.5,72)}, rotate = 140.1] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Curve Lines [id:da025245983142820227] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (178.5,203.33) .. controls (430.24,204) and (292.89,137.67) .. (372.29,71.99) ;
		\draw [shift={(373.5,71)}, rotate = 141.17] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Rectangle [id:dp03239767494229007] 
		\draw  [line width=2.25]  (148,204.5) -- (218,204.5) -- (218,244.5) -- (148,244.5) -- cycle ;
		%Shape: Rectangle [id:dp8647468281104325] 
		\draw  [line width=2.25]  (231,204.5) -- (301,204.5) -- (301,244.5) -- (231,244.5) -- cycle ;
		%Straight Lines [id:da5417186448832407] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (286.5,203.67) -- (344.88,180.33) ;
		\draw [shift={(347.67,179.22)}, rotate = 158.21] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Rectangle [id:dp7769377964618231] 
		\draw  [line width=2.25]  (311,204.5) -- (381,204.5) -- (381,244.5) -- (311,244.5) -- cycle ;
		%Straight Lines [id:da7954283908881117] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (339.5,203.67) -- (374.98,180.63) ;
		\draw [shift={(377.5,179)}, rotate = 147.01] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Rectangle [id:dp7365196575135502] 
		\draw  [line width=2.25]  (390,204.5) -- (460,204.5) -- (460,244.5) -- (390,244.5) -- cycle ;
		%Straight Lines [id:da22987049115523406] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (360.5,204) -- (479.56,179.6) ;
		\draw [shift={(482.5,179)}, rotate = 168.42] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Rectangle [id:dp23000863057195997] 
		\draw  [line width=2.25]  (468,204.5) -- (538,204.5) -- (538,244.5) -- (468,244.5) -- cycle ;
		%Shape: Rectangle [id:dp381653567280577] 
		\draw  [line width=2.25]  (544,204.5) -- (614,204.5) -- (614,244.5) -- (544,244.5) -- cycle ;
		%Shape: Rectangle [id:dp12744780301473035] 
		\draw  [dash pattern={on 6.75pt off 4.5pt}][line width=2.25]  (15,275.33) -- (85,275.33) -- (85,315.33) -- (15,315.33) -- cycle ;
		%Shape: Rectangle [id:dp6956837402231164] 
		\draw  [dash pattern={on 6.75pt off 4.5pt}][line width=2.25]  (15,367.33) -- (85,367.33) -- (85,407.33) -- (15,407.33) -- cycle ;
		%Shape: Rectangle [id:dp9963032868744282] 
		\draw  [line width=2.25]  (147,298.5) -- (217,298.5) -- (217,338.5) -- (147,338.5) -- cycle ;
		%Straight Lines [id:da537921779746819] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (147.5,323) -- (104.96,246.62) ;
		\draw [shift={(103.5,244)}, rotate = 60.88] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Straight Lines [id:da060152098822754185] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (183.5,298) -- (183.5,248) ;
		\draw [shift={(183.5,245)}, rotate = 90] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Straight Lines [id:da5972019813812797] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (206.5,298) -- (240.15,267.72) -- (264.27,246.01) ;
		\draw [shift={(266.5,244)}, rotate = 138.01] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Rectangle [id:dp06826927464965782] 
		\draw  [line width=2.25]  (312,276.5) -- (382,276.5) -- (382,316.5) -- (312,316.5) -- cycle ;
		%Straight Lines [id:da3168864287215285] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (216.5,318.5) -- (307.6,293.79) ;
		\draw [shift={(310.5,293)}, rotate = 164.82] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Straight Lines [id:da21128474829731236] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (346.5,276) -- (346.5,248) ;
		\draw [shift={(346.5,245)}, rotate = 90] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Rectangle [id:dp19822542621270167] 
		\draw  [line width=2.25]  (391,276.5) -- (461,276.5) -- (461,316.5) -- (391,316.5) -- cycle ;
		%Straight Lines [id:da6480255995951849] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (425.5,276) -- (425.5,248) ;
		\draw [shift={(425.5,245)}, rotate = 90] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Rectangle [id:dp5196671312005741] 
		\draw  [line width=2.25]  (469,297.5) -- (539,297.5) -- (539,337.5) -- (469,337.5) -- cycle ;
		%Straight Lines [id:da9942113456222434] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (217.67,332.5) -- (410.67,318.71) ;
		\draw [shift={(413.67,318.5)}, rotate = 175.91] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Straight Lines [id:da06603376521768611] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (217.67,332.5) -- (464.5,328.38) ;
		\draw [shift={(467.5,328.33)}, rotate = 179.04] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Rectangle [id:dp2789358165912801] 
		\draw  [line width=2.25]  (545,297.5) -- (615,297.5) -- (615,337.5) -- (545,337.5) -- cycle ;
		%Straight Lines [id:da5343548384950316] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (503.67,297.5) -- (503.67,247.5) ;
		\draw [shift={(503.67,244.5)}, rotate = 90] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Straight Lines [id:da09996226615003989] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (560.67,297.5) -- (560.67,247.5) ;
		\draw [shift={(560.67,244.5)}, rotate = 90] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Straight Lines [id:da7005952638327568] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (36.5,368.33) -- (36.5,323.33) -- (36.5,319.33) ;
		\draw [shift={(36.5,316.33)}, rotate = 90] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Rectangle [id:dp5922122338351559] 
		\draw  [line width=2.25]  (149,398.5) -- (219,398.5) -- (219,438.5) -- (149,438.5) -- cycle ;
		%Straight Lines [id:da846754881453541] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (184.5,398.33) -- (184.5,343) ;
		\draw [shift={(184.5,340)}, rotate = 90] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Straight Lines [id:da32961093568286826] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (148.5,430.33) -- (87.82,408.35) ;
		\draw [shift={(85,407.33)}, rotate = 19.91] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Rectangle [id:dp6114059881666289] 
		\draw  [line width=2.25]  (151,482.5) -- (221,482.5) -- (221,522.5) -- (151,522.5) -- cycle ;
		%Straight Lines [id:da6032353459574948] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (184.5,482.33) -- (184.5,441) ;
		\draw [shift={(184.5,438)}, rotate = 90] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Straight Lines [id:da4457885210161967] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (150.5,505.67) -- (77.32,248.55) ;
		\draw [shift={(76.5,245.67)}, rotate = 74.11] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Rectangle [id:dp646190203177371] 
		\draw  [line width=2.25]  (152,568.5) -- (222,568.5) -- (222,608.5) -- (152,608.5) -- cycle ;
		%Straight Lines [id:da147052727902949] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (185.5,568.33) -- (185.5,527) ;
		\draw [shift={(185.5,524)}, rotate = 90] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Rectangle [id:dp3613966517530143] 
		\draw  [line width=2.25]  (22,526.5) -- (92,526.5) -- (92,566.5) -- (22,566.5) -- cycle ;
		%Shape: Rectangle [id:dp014073252422288851] 
		\draw  [line width=2.25]  (5,587.5) -- (75,587.5) -- (75,627.5) -- (5,627.5) -- cycle ;
		%Straight Lines [id:da020447853297336005] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (33.5,587.33) -- (33.5,571) ;
		\draw [shift={(33.5,568)}, rotate = 90] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Straight Lines [id:da6709080457407384] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (11.5,586.67) -- (11.5,184) ;
		\draw [shift={(11.5,181)}, rotate = 90] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Rectangle [id:dp04351739392735343] 
		\draw  [dash pattern={on 6.75pt off 4.5pt}][line width=2.25]  (6,680.33) -- (108.5,680.33) -- (108.5,720.33) -- (6,720.33) -- cycle ;
		%Straight Lines [id:da3869101872738756] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (39.5,629.33) -- (39.5,679.33) ;
		\draw [shift={(39.5,682.33)}, rotate = 270] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Rectangle [id:dp595488903931781] 
		\draw  [line width=2.25]  (154,654.5) -- (224,654.5) -- (224,694.5) -- (154,694.5) -- cycle ;
		%Straight Lines [id:da22837737815310977] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (188,654.5) -- (188,613.17) ;
		\draw [shift={(188,610.17)}, rotate = 90] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Straight Lines [id:da4234436823021863] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (154,675.5) -- (77.54,629.1) ;
		\draw [shift={(75,627.5)}, rotate = 32.28] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Rectangle [id:dp4018777550165664] 
		\draw  [line width=2.25]  (294.5,372.5) -- (380.5,372.5) -- (380.5,412.5) -- (294.5,412.5) -- cycle ;
		%Straight Lines [id:da7471313087851801] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (318.5,372.67) -- (341.34,318.43) ;
		\draw [shift={(342.5,315.67)}, rotate = 112.83] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Straight Lines [id:da8788593234282329] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (477.5,372.67) -- (424.6,318.81) ;
		\draw [shift={(422.5,316.67)}, rotate = 45.52] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Straight Lines [id:da9553562742366883] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (218.5,419.67) -- (289.66,395.63) ;
		\draw [shift={(292.5,394.67)}, rotate = 161.33] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Straight Lines [id:da4401602847507713] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (380.5,393) -- (404.5,393) -- (416.5,393) ;
		\draw [shift={(419.5,393)}, rotate = 180] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Curve Lines [id:da15605587661153697] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (218.5,406) .. controls (226.46,379.8) and (335.9,291.39) .. (441.41,371.28) ;
		\draw [shift={(443,372.5)}, rotate = 217.72] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Rectangle [id:dp8248223299500097] 
		\draw  [line width=2.25]  (418.5,373.5) -- (504.5,373.5) -- (504.5,413.5) -- (418.5,413.5) -- cycle ;
		%Shape: Rectangle [id:dp17995763741586268] 
		\draw  [line width=2.25]  (533.5,372.5) -- (619.5,372.5) -- (619.5,412.5) -- (533.5,412.5) -- cycle ;
		%Straight Lines [id:da5632125785109918] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (504.5,393) -- (530.5,393) ;
		\draw [shift={(533.5,393)}, rotate = 180] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Rectangle [id:dp25257452599636365] 
		\draw  [line width=2.25]  (335.5,489.5) -- (421.5,489.5) -- (421.5,529.5) -- (335.5,529.5) -- cycle ;
		%Straight Lines [id:da7181071042824219] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (359.5,489) -- (340.28,417.4) ;
		\draw [shift={(339.5,414.5)}, rotate = 74.97] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Rectangle [id:dp4562006717594007] 
		\draw  [line width=2.25]  (479.5,489.5) -- (565.5,489.5) -- (565.5,529.5) -- (479.5,529.5) -- cycle ;
		%Straight Lines [id:da6874494081635656] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (513.5,490) -- (513.5,343.5) ;
		\draw [shift={(513.5,340.5)}, rotate = 90] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Rectangle [id:dp11425102753327865] 
		\draw  [line width=2.25]  (533.5,435.5) -- (619.5,435.5) -- (619.5,475.5) -- (533.5,475.5) -- cycle ;
		%Straight Lines [id:da8789867861336977] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (565.5,511.33) -- (581.17,479.69) ;
		\draw [shift={(582.5,477)}, rotate = 116.34] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Rectangle [id:dp7682037825657075] 
		\draw  [line width=2.25]  (481.5,559.5) -- (567.5,559.5) -- (567.5,599.5) -- (481.5,599.5) -- cycle ;
		%Straight Lines [id:da9248822888014887] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (521.5,558.33) -- (521.5,534) ;
		\draw [shift={(521.5,531)}, rotate = 90] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Rectangle [id:dp6671622978769793] 
		\draw  [line width=2.25]  (335.5,555.5) -- (421.5,555.5) -- (421.5,595.5) -- (335.5,595.5) -- cycle ;
		%Straight Lines [id:da731980772466569] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (378.5,556.33) -- (378.5,534.17) ;
		\draw [shift={(378.5,531.17)}, rotate = 90] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Rectangle [id:dp34151760087201843] 
		\draw  [line width=2.25]  (335.5,621.5) -- (421.5,621.5) -- (421.5,661.5) -- (335.5,661.5) -- cycle ;
		%Straight Lines [id:da14171896741964152] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (379.5,621.33) -- (379.5,599.17) ;
		\draw [shift={(379.5,596.17)}, rotate = 90] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Rectangle [id:dp18727962431440814] 
		\draw  [line width=2.25]  (477.5,692.33) -- (528.5,692.33) -- (528.5,718.5) -- (477.5,718.5) -- cycle ;
		%Straight Lines [id:da0811292348315662] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (481.5,691.83) -- (481.5,602.5) ;
		\draw [shift={(481.5,599.5)}, rotate = 90] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Rectangle [id:dp8389386401375247] 
		\draw  [line width=2.25]  (532.5,623.5) -- (618.5,623.5) -- (618.5,663.5) -- (532.5,663.5) -- cycle ;
		%Straight Lines [id:da05586205529902122] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (514.5,691.33) -- (530.87,666.02) ;
		\draw [shift={(532.5,663.5)}, rotate = 122.89] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Rectangle [id:dp5326387408926125] 
		\draw  [line width=2.25]  (546.5,692.33) -- (619.5,692.33) -- (619.5,718.5) -- (546.5,718.5) -- cycle ;
		%Straight Lines [id:da5135023464583397] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (588.5,692.33) -- (588.5,668.33) ;
		\draw [shift={(588.5,665.33)}, rotate = 90] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Straight Lines [id:da006406683249325251] 
		\draw [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ][line width=1.5]    (529.5,703.33) -- (544.5,703.33) ;
		\draw [shift={(547.5,703.33)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Rectangle [id:dp7263796745103868] 
		\draw  [line width=2.25]  (507,782.33) -- (580,782.33) -- (580,808.5) -- (507,808.5) -- cycle ;
		%Straight Lines [id:da27696570845942703] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (503.5,718.33) -- (523.56,779.48) ;
		\draw [shift={(524.5,782.33)}, rotate = 251.83] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Straight Lines [id:da014618105041813045] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (592.5,718.33) -- (566.68,778.58) ;
		\draw [shift={(565.5,781.33)}, rotate = 293.2] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Rectangle [id:dp9335643387533226] 
		\draw  [line width=2.25]  (337.5,701.5) -- (423.5,701.5) -- (423.5,764.33) -- (337.5,764.33) -- cycle ;
		%Straight Lines [id:da5334975365492509] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (380.5,701.33) -- (380.5,665.17) ;
		\draw [shift={(380.5,662.17)}, rotate = 90] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Straight Lines [id:da8183220833842637] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (424.5,735.33) -- (473.87,708.44) ;
		\draw [shift={(476.5,707)}, rotate = 151.42] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Rectangle [id:dp617555588770373] 
		\draw  [line width=2.25]  (337.5,788.5) -- (423.5,788.5) -- (423.5,830.67) -- (337.5,830.67) -- cycle ;
		%Straight Lines [id:da9952605754183002] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (381.5,789.33) -- (381.5,767.17) ;
		\draw [shift={(381.5,764.17)}, rotate = 90] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Circle [id:dp86012437024173] 
		\draw  [line width=1.5]  (346.83,906) .. controls (346.83,886.3) and (362.8,870.33) .. (382.5,870.33) .. controls (402.2,870.33) and (418.17,886.3) .. (418.17,906) .. controls (418.17,925.7) and (402.2,941.67) .. (382.5,941.67) .. controls (362.8,941.67) and (346.83,925.7) .. (346.83,906) -- cycle ;
		%Straight Lines [id:da025433460227610727] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (382.5,870.33) -- (382.5,832.33) ;
		\draw [shift={(382.5,829.33)}, rotate = 90] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Circle [id:dp8708594269382692] 
		\draw  [line width=1.5]  (351.83,906) .. controls (351.83,889.25) and (365.41,875.67) .. (382.17,875.67) .. controls (398.92,875.67) and (412.5,889.25) .. (412.5,906) .. controls (412.5,922.75) and (398.92,936.33) .. (382.17,936.33) .. controls (365.41,936.33) and (351.83,922.75) .. (351.83,906) -- cycle ;
		%Straight Lines [id:da8109285279884149] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (406,878) -- (504.16,798.88) ;
		\draw [shift={(506.5,797)}, rotate = 141.13] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Straight Lines [id:da7197793033026469] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (492.5,137) -- (476.25,73.91) ;
		\draw [shift={(475.5,71)}, rotate = 75.56] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Curve Lines [id:da42879608084801246] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (525.5,204) .. controls (556.5,183) and (576,187.5) .. (584,153.5) .. controls (591.76,120.52) and (568.47,100.71) .. (553.38,74.91) ;
		\draw [shift={(552,72.5)}, rotate = 60.95] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Curve Lines [id:da3137086735834893] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (323.5,205) .. controls (354.5,184) and (229.5,193) .. (210.5,190) .. controls (191.98,187.08) and (201.02,189.85) .. (166.26,179.8) ;
		\draw [shift={(163.5,179)}, rotate = 16.14] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Curve Lines [id:da7897861137174416] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (323.5,205) .. controls (354.5,184) and (165.5,200) .. (145.5,199) .. controls (126,198.03) and (85.59,195.15) .. (55.77,180.18) ;
		\draw [shift={(53.5,179)}, rotate = 28.07] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Straight Lines [id:da9096110488435885] 
\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (219,438.5) -- (443.52,413.99) ;
\draw [shift={(446.5,413.67)}, rotate = 173.77] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Straight Lines [id:da46656939100288586] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (219,438.5) -- (331.96,509.08) ;
		\draw [shift={(334.5,510.67)}, rotate = 212] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Straight Lines [id:da135740471069713] 
\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (222.5,591.67) -- (331.52,578.04) ;
\draw [shift={(334.5,577.67)}, rotate = 172.87] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Straight Lines [id:da6699472605357921] 
\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (336.5,732.67) -- (227.17,677.03) ;
\draw [shift={(224.5,675.67)}, rotate = 26.97] [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		
		% Text Node
		\draw (17,150.83) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {Lie Groups};
		% Text Node
		\draw (232,701.17) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {{\scriptsize {add $\displaystyle =$ and associative}}\\{\scriptsize {binary operations with}}\\{\scriptsize {identity}}};
		% Text Node
		\draw (124.67,91.33) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {SU$(2)$};
		% Text Node
		\draw (55,123) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {{\scriptsize specialize}};
		% Text Node
		\draw (209.83,19.23) node  [font=\small] [align=left] {\begin{minipage}[lt]{85.91pt}\setlength\topsep{0pt}
		\textbf{General Relativity}
		\end{minipage}};
		% Text Node
		\draw (474,19.67) node  [font=\small] [align=left] {\begin{minipage}[lt]{108.35pt}\setlength\topsep{0pt}
		\begin{center}
		\textbf{Quantum Field Theory}
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (216.67,47.8) node  [font=\scriptsize] [align=left] {\begin{minipage}[lt]{173.63pt}\setlength\topsep{0pt}
		\begin{center}
		$3+1$ dimensional pseudo-Riemmannian\\manifols with tensor fields obeying PDE's of, say,\\Einstein-Maxwell theory with perfect fluid component
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (479,45.8) node  [font=\scriptsize] [align=left] {\begin{minipage}[lt]{173.63pt}\setlength\topsep{0pt}
		\begin{center}
		operator-valued fields on $\displaystyle \mathbb{R}^{4}$ obeying certain Lorentz\\\mbox{-}invariant PDE's and commutation relationships,\\acting on an abstract Hilbert space
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (121.33,150.83) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {Lie Algebras};
		% Text Node
		\draw (399.53,158.33) node  [font=\small] [align=left] {\begin{minipage}[lt]{69.18pt}\setlength\topsep{0pt}
		\begin{center}
		Manifolds with\\tensor fields
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (270.4,158.33) node  [font=\small] [align=left] {\begin{minipage}[lt]{70.72pt}\setlength\topsep{0pt}
		\begin{center}
		Differential\\Operators
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (51.3,99.22) node  [font=\small] [align=left] {\begin{minipage}[lt]{47.19pt}\setlength\topsep{0pt}
		\begin{center}
		Lorentz\\Group
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (511.53,158.33) node  [font=\small] [align=left] {\begin{minipage}[lt]{69.18pt}\setlength\topsep{0pt}
		\begin{center}
		Metric\\manifolds
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (105,224.5) node  [font=\small] [align=left] {\begin{minipage}[lt]{47.6pt}\setlength\topsep{0pt}
		\begin{center}
		Algebras
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (183,224.57) node  [font=\small] [align=left] {\begin{minipage}[lt]{47.6pt}\setlength\topsep{0pt}
		\begin{center}
		Linear\\operators
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (266,224.5) node  [font=\small] [align=left] {\begin{minipage}[lt]{47.6pt}\setlength\topsep{0pt}
		\begin{center}
		Tensor\\spaces
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (346,224.5) node  [font=\small] [align=left] {\begin{minipage}[lt]{47.6pt}\setlength\topsep{0pt}
		\begin{center}
		Real\\manifolds
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (425,224.5) node  [font=\small] [align=left] {\begin{minipage}[lt]{47.6pt}\setlength\topsep{0pt}
		\begin{center}
		Complex\\manifolds
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (503,224.5) node  [font=\small] [align=left] {\begin{minipage}[lt]{47.6pt}\setlength\topsep{0pt}
		\begin{center}
		Hilbert\\spaces
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (579,224.5) node  [font=\small] [align=left] {\begin{minipage}[lt]{47.6pt}\setlength\topsep{0pt}
		\begin{center}
		Distri-\\butions
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (50.3,296.22) node  [font=\small] [align=left] {\begin{minipage}[lt]{47.19pt}\setlength\topsep{0pt}
		\begin{center}
		Triple\\fields
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (40,327.5) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {{\scriptsize add 4th}\\{\scriptsize binary op.}\\{\scriptsize etc.}};
		% Text Node
		\draw (50.3,388.22) node  [font=\small] [align=left] {\begin{minipage}[lt]{47.19pt}\setlength\topsep{0pt}
		\begin{center}
		Double\\fields
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (182,318.5) node  [font=\small] [align=left] {\begin{minipage}[lt]{47.6pt}\setlength\topsep{0pt}
		\begin{center}
		Vector\\spaces
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (124,255.33) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=right] {{\scriptsize define linear}\\{\scriptsize vector }\\{\scriptsize mappings}};
		% Text Node
		\draw (348,296.5) node  [font=\small] [align=left] {\begin{minipage}[lt]{47.6pt}\setlength\topsep{0pt}
		\begin{center}
		$\displaystyle \mathbb{R}^{n}$
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (426,296.5) node  [font=\small] [align=left] {\begin{minipage}[lt]{47.6pt}\setlength\topsep{0pt}
		\begin{center}
		$\displaystyle \mathbb{C}^{n}$
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (507,265.33) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {{\scriptsize add inner}\\{\scriptsize product}};
		% Text Node
		\draw (352,250) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {{\scriptsize define maps }\\{\scriptsize and atlases}};
		% Text Node
		\draw (504,317.31) node  [font=\small] [align=left] {\begin{minipage}[lt]{47.6pt}\setlength\topsep{0pt}
		\begin{center}
		Banach\\spaces
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (580,317.31) node  [font=\small] [align=left] {\begin{minipage}[lt]{47.6pt}\setlength\topsep{0pt}
		\begin{center}
		Real\\functions
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (565,260) node [anchor=north west][inner sep=0.1pt]  [font=\scriptsize] [align=left] {{\scriptsize define}\\{\scriptsize linear}\\{\scriptsize functionals}};
		% Text Node
		\draw (145,347) node [anchor=north west][inner sep=0.5pt]  [font=\scriptsize] [align=center] {{\scriptsize add new class}\\{\scriptsize (vectors), define}\\{\scriptsize vector sum and}\\{\scriptsize scalar product}};
		% Text Node
		\draw (184,418.5) node  [font=\small] [align=left] {\begin{minipage}[lt]{47.6pt}\setlength\topsep{0pt}
		\begin{center}
		Abelian\\fields
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (72,421.5) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {{\scriptsize add 3rd}\\{\scriptsize binary }\\{\scriptsize operation,}\\{\scriptsize etc.}};
		% Text Node
		\draw (186,500) node  [font=\small] [align=left] {\begin{minipage}[lt]{47.6pt}\setlength\topsep{0pt}
		\begin{center}
		Fields
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (186.5,462) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {{\scriptsize add commutativity}};
		% Text Node
		\draw (187,590) node  [font=\small] [align=left] {\begin{minipage}[lt]{47.6pt}\setlength\topsep{0pt}
		\begin{center}
		Rings
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (187.5,541.17) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {{\scriptsize add multiplicative}\\{\scriptsize inverse}};
		% Text Node
		\draw (57,546.25) node  [font=\small] [align=left] {\begin{minipage}[lt]{47.6pt}\setlength\topsep{0pt}
		\begin{center}
		Abelian\\Groups
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (40,608) node  [font=\small] [align=left] {\begin{minipage}[lt]{47.6pt}\setlength\topsep{0pt}
		\begin{center}
		Groups
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (40,573.5) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {{\scriptsize add commutativity}};
		% Text Node
		\draw (57.25,701.22) node  [font=\small] [align=left] {\begin{minipage}[lt]{69.7pt}\setlength\topsep{0pt}
		\begin{center}
		Dodecahedron\\Group
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (189,673.75) node  [font=\small] [align=left] {\begin{minipage}[lt]{47.6pt}\setlength\topsep{0pt}
		\begin{center}
		Semi-\\groups
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (94.05,627.37) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize,rotate=-31.12] [align=left] {{\scriptsize add inverse}};
		% Text Node
		\draw (41,646.33) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {{\scriptsize specialize}};
		% Text Node
		\draw (192,615) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {{\scriptsize add commutativity and 2nd}\\{\scriptsize associ. and distrib, binary}\\ {\scriptsize operation}};
		% Text Node
		\draw (338,392.5) node  [font=\small] [align=left] {\begin{minipage}[lt]{57.8pt}\setlength\topsep{0pt}
		\begin{center}
		Real \\numbers
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (231.5,416.17) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {{\scriptsize add ordering}\\{\scriptsize least upper bound}\\{\scriptsize property}};
		% Text Node
		\draw (383.5,400) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {{\scriptsize define }\\{\scriptsize $=,+$ and $\displaystyle \cdot $}\\{\scriptsize for pairs}};
		% Text Node
		\draw (462,393.5) node  [font=\small] [align=left] {\begin{minipage}[lt]{57.8pt}\setlength\topsep{0pt}
		\begin{center}
		Complex\\numbers
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (577,392.5) node  [font=\small] [align=left] {\begin{minipage}[lt]{57.8pt}\setlength\topsep{0pt}
		\begin{center}
		Complex\\functions
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (379,509.5) node  [font=\small] [align=left] {\begin{minipage}[lt]{57.8pt}\setlength\topsep{0pt}
		\begin{center}
		Rational\\numbers
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (360,455) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {{\scriptsize define $=,+$ and $\displaystyle \cdot $}\\{\scriptsize for Dedekind cuts}};
		% Text Node
		\draw (523,509.5) node  [font=\small] [align=left] {\begin{minipage}[lt]{57.8pt}\setlength\topsep{0pt}
		\begin{center}
		Metric\\spaces
		\end{center}
		\end{minipage}};
		% Text Node
		\draw (462.5,450) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {\begin{minipage}[lt]{45pt}\setlength\topsep{0pt}
		\begin{flushright}
		{\scriptsize add norm and}\\{\scriptsize completeness}
		\end{flushright}
		
		\end{minipage}};
		% Text Node
		\draw (577,455.5) node  [font=\small] [align=left] {\begin{minipage}[lt]{57.8pt}\setlength\topsep{0pt}
		\begin{center}
		Measurable\\spaces
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (573.5,494) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {\begin{minipage}[lt]{45pt}\setlength\topsep{0pt}
		\begin{flushright}
		{\scriptsize add measure}
		\end{flushright}
		
		\end{minipage}};
		% Text Node
		\draw (525,579.5) node  [font=\small] [align=left] {\begin{minipage}[lt]{57.8pt}\setlength\topsep{0pt}
		\begin{center}
		Topological\\spaces
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (527.5,542.17) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {{\scriptsize add metric}};
		% Text Node
		\draw (379,575.5) node  [font=\small] [align=left] {\begin{minipage}[lt]{57.8pt}\setlength\topsep{0pt}
		\begin{center}
		Integer\\numbers
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (385.5,539) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {{\scriptsize define $=,+$ and $\displaystyle \cdot $ for pairs}};
		% Text Node
		\draw (379,641.5) node  [font=\small] [align=left] {\begin{minipage}[lt]{57.8pt}\setlength\topsep{0pt}
		\begin{center}
		Natural\\numbers
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (386,598) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {{\scriptsize define $=,+$ and $\displaystyle \cdot $ for}\\{\scriptsize negative numbers}};
		% Text Node
		\draw (503,705.42) node  [font=\small] [align=left] {\begin{minipage}[lt]{34.68pt}\setlength\topsep{0pt}
		\begin{center}
		Sets
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (484,617.17) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {{\scriptsize add open}\\{\scriptsize set axioms}};
		% Text Node
		\draw (576,643.5) node  [font=\small] [align=left] {\begin{minipage}[lt]{57.8pt}\setlength\topsep{0pt}
		\begin{center}
		Abstract\\Geometries
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (582.5,705.42) node  [font=\small] [align=left] {\begin{minipage}[lt]{48.96pt}\setlength\topsep{0pt}
		\begin{center}
		Relations
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (519.5,717.17) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {\begin{minipage}[lt]{25.9pt}\setlength\topsep{0pt}
		\begin{center}
		{\scriptsize add}\\{\scriptsize cartesian}\\{\scriptsize products}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (543,795.42) node  [font=\small] [align=left] {\begin{minipage}[lt]{48.96pt}\setlength\topsep{0pt}
		\begin{center}
		Models
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (381,733.42) node  [font=\small] [align=left] {\begin{minipage}[lt]{57.8pt}\setlength\topsep{0pt}
		\begin{center}
		Lower\\predicate\\calculus
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (382.5,674) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {{\scriptsize add number theory}\\{\scriptsize symbols and axioms}};
		% Text Node
		\draw (426.5,738.33) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {{\scriptsize add symbols and}\\{\scriptsize axioms for}\\{\scriptsize $\displaystyle =$, $\displaystyle \in $, $\displaystyle \cup $, $\displaystyle \cap $}};
		% Text Node
		\draw (380,809.08) node  [font=\small] [align=left] {\begin{minipage}[lt]{57.8pt}\setlength\topsep{0pt}
		\begin{center}
		Boolean\\algebra
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (315,772) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {{\scriptsize add quantifiers}};
		% Text Node
		\draw (382.5,905) node  [font=\small] [align=left] {\begin{minipage}[lt]{48.51pt}\setlength\topsep{0pt}
		\begin{center}
		Formal\\systems
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (295,835) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {{\scriptsize add symbols and}\\{\scriptsize axioms for brackets,}\\{\scriptsize not, and, or, etc.}};
		
		% Text Node
		\draw (231,264) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {{\scriptsize define linear}\\{\scriptsize functions of}\\{\scriptsize $\displaystyle n$ vectors}};
		\end{tikzpicture}
		\vspace*{1mm}
		\caption[Relations between various important mathematical structures for physics]{Oversimplified relations between various important mathematical structures for physics (author: Max Tegmark)}
	\end{figure}

	Often when a student arrives in a graduate class he learned how to calculate or use algorithms but almost only a little or even not at all to reason. For all the reasoning the visual media is a powerful tool (a picture is worth a thousand words) and people who do not see that in tracing a given curve or straight line the solution appears or who do not see in space are really penalized.

	During high school we already manipulate unknown objects but especially to make calculations and when we reason about objects represented by letters, we can replace them visually by a real number, a vector, etc. At a given level we ask people to reason on more abstract structures and therefore to work on unknown objects which are elements of a set itself unknown, for example elements of any group (\SeeChapter{see section Set Theory page \pageref{groups}}). This visual support thus doesn't exist anymore.

We ask so often to students to reason, to demonstrate the properties, but almost no one has ever taught them to reason properly, writing proofs, control proofs. If we ask a graduate student what is a proof, it most likely he will have some difficulty to answer. He can say that it is a text in which there are keywords like "therefore", "because", "if", "if and only if", "take a $x$ such that", "assume", "lemma", "theorem", "let us look for a contradiction", etc. But he will probably be unable to provide the grammar of these texts nor their basics, and besides, its teachers, if they have not taken a course in Proof Theory, would probably be unable too.

To understand this situation, remember that to speak a child does not need to know the grammar. He imitates his surroundings and it works very well: most of time a six year old child know to use complicated sentences without ever having done grammar. Most teachers also do not know the grammar of reasoning but, for them, the imitation process has work well and thus they reason correctly. The experience of the majority of university teachers shows that this process of imitation works well for very good students, and then it is enough, but it works much less, if not at all, for many others.

As the complexity level is low (especially during an "equational" type reasoning), grammar is almost useless but when the level increase or when we do not understand why something is wrong, it becomes necessary to do some grammar to progress. Teachers and students are familiar with the following situation: in a school assignment the corrector barred whole page of a large red line and write "false" in the margin. When the student asks what is wrong, the corrector can only say things like "this has no relation with the requested proof", "nothing is right", ..., which help obviously not the student to understand. This is partly because the text written by the student uses the appropriate words but in a more or less random way and can not give meaning to the assembly of these words. In addition, the teacher does not have the tools to explain what is wrong. We must therefore give them to him!

These tools exist but are fairly recent. The proof theory is a branch of mathematical logic whose origin is the crisis of the foundations: there was a doubt about what we had the "right" to do in a mathematical reasoning (see the "foundations crisis" further below). Paradoxes appeared, and it was then necessary to clarify the rules of proof and to verify that these rules are not contradictory. This theory appeared in the early 120th century (holocene calendar), which is very new since most of the mathematics taught in the first half of the university is known since the 116th-117th century (holocene calendar).

	\subsection{Foundations Crisis}
	For the Greeks philosophers geometry was considered the highest form of knowledge, a powerful key to the metaphysical mysteries of the universe. It was rather a mystical belief and the link between mysticism and religion was made explicit in cults like those of the Pythagoreans.

	No culture has since challenged a man for having discovered a geometrical theorem! Later, mathematics was regarded as the model of a priori knowledge in the Aristotelian tradition of rationalism.
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.4]{img/intro/rationalism_vs_empiricism.jpg}
	\end{figure}

	The astonishment of the Greeks philosophers for mathematics has not left us, we find it in the traditional metaphor of mathematics as "Queen of Science". It was strengthened by the spectacular success of mathematical models in science, success that the Greeks (even ignoring the simple algebra) had not anticipated. Since the discovery by Isaac Newton's of integral calculus and the inverse square law of gravity in the late 1600s the phenomenal sciences and higher mathematics remained in close symbiosis - to the point that a predictive mathematical formalism was became the hallmark of a "hard science".

	After Newton, during the next two centuries, science aspired to that kind of rigour and purity that seemed inherent in mathematics. The metaphysical question seemed simple: mathematics seemed to have a perfect a priori knowledge, and among all sciences, those that were able to mathematize most perfectly were the most effective for predicting phenomena. The perfect knowledge therefore, was in a mathematical formalism that, once reached by science and embracing all aspects of reality, could found a posteriori empirical knowledge on an a priori rational logic. It was in this spirit that Marie Jean-Antoine Nicolas de Caritat, Marquis de Condorcet (French philosopher and mathematician), undertook to imagine describing the entire Universe as a set of partial differential equations being solved one after the other.

	The first break in this inspiring picture appeared in the second half of the 119th century (holocene calendar), when Riemann and Lobachevsky separately proved that Euclid's parallel axiom could be replaced by other geometries that produced "consistent" (we will come back more on this word further below). Riemannian geometry was modelled on a sphere, these of Lobatschewsky, on rotation of a hyperboloid.

	The impact of this discovery was later obscured by great upheaval, but at the time it made a thunderclap in the intellectual world. The existence of mutually inconsistent axiomatic systems, each of which could be a model for the phenomenal Universe, relied entirely into question the relation between mathematics and theoretical physics.

	When we knew only Euclid, there was only one possible geometry. One could believe that the Euclid's postulates (\SeeChapter{see section Euclidean Geometry page \pageref{euclid's postulates}}) were a kind of knowledge a priori perfect on the geometry in the phenomenal world. But suddenly we had three geometries, embarrassing for metaphysical subtleties.

	Why would we choose between the axioms of plane geometry, spherical and hyperbolic geometry as real descriptions? Because all three are consistent, we can not choose any a priori as a foundation - the choice must be empirical, based on their predictive power in a given situation.

	Of course, the theoretical physicists have long been accustomed to choose a formalism to study a scientific problem. But it was already accepted widely, if not unconsciously, that the need to do so was based on human ignorance, and with logic or good enough mathematics, one could infer the right choice from principles first, and produce a priori descriptions of reality that had to be confirmed afterwards by empirical verification.

	However, Euclidean geometry, seen for hundreds of years as the model of axiomatic perfection of mathematics, had been dethroned. If we could not know a priori something as basic as the geometry in space, what hope was there for a pure rational theory that would encompass all of nature? Psychologically, Riemann and Lobachevsky had struck at the heart the mathematical enterprise as it had been designed before.

	Moreover, Riemann and Lobachevsky have pushed the nature of mathematical intuition into question. It was easy to believe implicitly that mathematical intuition was a form of perception - a way to glimpse the Platonic world behind reality. But with two other geometries pushing the Euclid one in it's limit, no one could never be sure to know what the world really looks like.

	Mathematicians responded to this dual problem with excessive rigour, trying to apply the axiomatic method in all mathematics. In the pre-axiomatic period, the proofs were often based on commonly accepted intuitions of the "reality" of mathematics, which could not automatically be regarded as valid.

	The new way of thinking about mathematics led to a series of spectacular success. Yet this had also a price. The axiomatic method made the connection between mathematics and the phenomenal reality increasingly close. Meanwhile, discoveries suggested that mathematical axioms that appeared to be consistent with phenomenal experience could lead to dizzying contradictions with this experience.

	Most mathematicians quickly became "formalist" arguing that pure mathematics could only be regarded as a kind of elaborate philosophy game that was played with symbols on paper (that's the theory that is behind the mathematical prophetic qualification "zero content system" by Robert Heinlein). The "Platonic" belief in the reality of mathematical objects, in the old-fashioned way, seemed good 20th the trash, despite the fact that mathematicians still feel like platonicians during the process of discovery of mathematics.

	Philosophically, then, the axiomatic method led most mathematicians to abandon previous beliefs in the metaphysical specificity of mathematics. It also produced the contemporary rupture between pure and Applied Mathematics. Most of the great mathematicians of the early modern period - Newton, Leibniz, Fourier, Gauss and others - also occupied phenomenal science. The axiomatic method had hatched the modern idea of the pure mathematician as a great aesthete, heedless of physics. Ironically, formalism gave the pure mathematicians a bad addiction to the Platonic attitude. The researchers in Applied Mathematics ceased to meet physicists and learned to put themselves in their behind.

	This takes us to the early 120th century (holocene calendar). For the beleaguered minority of Platonists, the worst was yet to come. Cantor, Frege, Russell and Whitehead showed that all pure mathematics could be built on the simple foundation of the Set Theory axiomatic. This suited well the formalists: the mathematics were reunifying, at least in principle, from a small set of rules detached of a big one. Platonicians also were satisfied, if a great structure appeared, consistent keystone for the whole mathematics, the metaphysical specificity of mathematics could still be saved.

	In a negative way, though, a Platonist had the last word. Kurt Gödel put his grain of sand in the program of axiomatization formalism when he proved that any sufficiently powerful axiom system to include integers numbers had to be either inconsistent (contain contradictions) or incomplete (too weak to decide the rightness or the falsity of some statements of the system). And that's more or less where things stand today. Mathematicians know that many attempts to advance mathematics as a priori knowledge of the Universe must face numerous paradoxes and unable to decide which axiom system describes the real mathematics. They have been reduced to hope that standards axiomatizations are not inconsistent but just incomplete, and wondering anxiously what contradictions or unprovable theorems are waiting to be discovered elsewhere.

	However, on the front of empiricism, mathematics was always a spectacular success as a theoretical construction tool. The great success of physics in the 120th century (General Relativity and Quantum physics) pushed so far out of the realm of physical intuition, they could only be understood by meditating deeply on their mathematical formalism, and extending their logical conclusions, even when those findings seemed wildly bizarre. What irony! Just as the mathematical perception were to appear always less reliable in pure mathematics, it became more and more indispensable in phenomenal science.

	In contrast to this background, the applicability of mathematics to phenomenal science poses a more difficult problem than at first appears. The relation between the mathematical models and prediction of phenomena is complex, not only in practice but also in principle. Even more complex, as we now know, there are ways to axiomatize mathematics that mutually exclude themselves!

	But why is there only one good choice of mathematical model? That is, why is there a mathematical formalism, for example for quantum physics, so productive that it predicts the discovery of new observable particles?

	To answer this question we will can observe that, as well, works as a kind of definition. For many phenomenal systems, such exact predictive formalism has not been found, and none seem plausible. You can easily find such examples: climate or the behaviour of a superior economy to that of a town - systems so chaotically interdependent that exact prediction is actually impossible (not only in practice but in principle).

	\pagebreak
	\subsection{Paradoxes}
	Since ancient times, some logicians had noticed the presence of many paradoxes within rationality. In fact, we can say that despite their number, these paradoxes are merely illustrations of a few paradoxical structures. Let us look to for general culture to the most famous which constitute the class of "\NewTerm{undecidable propositions}"\index{undecidable propositions}.
	
	Let us see one of the most well known example: The paradox of the class of classes (Russell)\label{russell paradox}!

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	There are two types of classes: those that contain themselves (or reflexive classes: the class of non-empty sets, the class of classes, ...) and those who do not contains themselves (or non-reflexives classes: the class of work to be returned, the class of blood oranges, ...). The question is the following: is the class of non-reflexives classes itself reflexive or non-reflexive? If it is reflexive, it contains itself and is thus in the class of non-reflexives classes that it represents, which is contradictory. If it is non-reflexive, it must be included in the class of non-reflexives classes and becomes ipso facto reflexive, we are facing again a contradiction.\\\\
This Russell's paradox is often known mainly under the two following variants:
	\begin{itemize}
		\item Does the set of all sets that do not contain themselves contain himself?\\\\
		The answer is: If "Yes", then "No" and if "No" then "Yes"...
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		A religious funny version of this paradox is the following: Can the Almighty (whatever the deity of one of the 45 monotheistic religions) create an entity more powerful than Him? No! Then he is not all-powerful. Yes, then he is not all-powerful.
		\end{tcolorbox}
	
		\item Those who do not shave themselves are shaved by the barber but not those who shave themselves. So who shaves the barber?\\\\
		The answer is: If the barber shave himself he enters in the category of people that shave themselves so he does not shave himself because he is the barber... But if he does not shave himself he enters in the category of people that are shaved by the barber... The answer is also undecidable...
	\end{itemize}
	The Universe is a reflexive class (it contains itself). Therefore the Russell paradox doesn't apply to it by construction!
	\end{tcolorbox}

	Russell's paradox challenges the notion of a set as a collection defined by common ownership! In one shot it destroys the logic (undecidable proposition) and set theory... because the overall concept of all sets is an impossibility!!! The self-reference is the center of this logical problem!

	This paradox also returns to the question whether a math question correctly formulated (logical) necessarily admits an answer? Said in another way: is any mathematical statement provable... and it is Gödel that many years after the statement of Russell's paradox proved mathematically that the answer is No !!!!!! In other words, there will always be questions unanswered because any system (living language or mathematical tool) based itself is necessarily incomplete! This is the famous impact of Gödel's incompleteness theorem that is technically written as following (and that we will have to prove):
	
	that means that given a proposition $P$, whatever the set of axioms, there exist proposition that we will be never able to prove as true or false.
	
	Let us see another application of the Russell's Paradox:
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	In a library, there are two types of catalogues: Those who mention themselves and those who does not mention themselves. A librarian must draw up a catalogue of all catalogues that do not mention themselves. Having completed its work, our librarian asks whether or not to mention the catalogue that is precisely drafting. At this point, he is struck perplexity. If he does not mention this catalogue it will be a catalogue that is not mentioned and which should therefore be included in the list of catalogues that does not mention themselves. On the other hand, if he mentions the catalogue, this catalogue will become a catalogue that is mentioned and must therefore not be included in this catalogue, since it is the catalogue of catalogues which does not mention themselves.
	\end{tcolorbox}
	
	A variations of the previous paradox is the well-known liar paradox:

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us provisionally define lying as the work of making a false proposition. The Cretan poet Epimenides said: "All Cretans are liars", this is the proposal $P$. How to decide the truthfulness of $P$? If $P$ is true, as Epimenides is Cretan, $P$ must be false. $P$ must therefore be false to be true, which is contradictory.
	\end{tcolorbox}

As would have made understand the logician Ludwig Wittgenstein, these paradoxes ultimately show that mathematics is a pretty good tool to show the logic but not to talk about it. Give with mathematics an independent existence to this algebraic entities is madness and it is this that produces monsters like the set of all the sets... The logic is empty and can not tell the reality, it restrict to be just a picture of it.

	The reader may be always careful to distinguish paradoxes, sophisms and ill-defined (ill-specified) problems or popular myths (provided without evidence for that latter obviously...). That's one of the biggest source of misunderstanding in many debates.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	If you choose an answer to this question at random, what is the chance you will be correct?
	\begin{enumerate}
		\item[A)] $25\%$
	
		\item[B)] $50\%$
	
		\item[C)] $60\%$
	
		\item[D)] $25\%$
	\end{enumerate}
	This is an ill-defined question. There are no solution and lead therefore to a paradox.
	\end{tcolorbox}

	\pagebreak
	\subsubsection{Hypothetical-Deductive Reasoning}
	The hypothetical-deductive reasoning is, we know (see the Introduction of the book), the ability of the learner to deduce conclusions from pure hypotheses and not only of a real observation. It is a thought process that seeks to identify a causal explanation of any phenomenon (we will come back on this during our first steps in physics). The learner who uses this type of reasoning begins with a hypothesis and then tries to prove or disprove his hypothesis following the block diagram below:
	\begin{figure}[H]
		\centering		
		\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt
		
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,673); %set diagram left start at 0, and has height of 673
		
		%Rounded Rect [id:dp450071796993355] 
		\draw   (79,93.4) .. controls (79,89.87) and (81.87,87) .. (85.4,87) -- (167.6,87) .. controls (171.13,87) and (174,89.87) .. (174,93.4) -- (174,112.6) .. controls (174,116.13) and (171.13,119) .. (167.6,119) -- (85.4,119) .. controls (81.87,119) and (79,116.13) .. (79,112.6) -- cycle ;
		%Rounded Rect [id:dp3123365770289972] 
		\draw   (282,94.4) .. controls (282,90.87) and (284.87,88) .. (288.4,88) -- (370.6,88) .. controls (374.13,88) and (377,90.87) .. (377,94.4) -- (377,113.6) .. controls (377,117.13) and (374.13,120) .. (370.6,120) -- (288.4,120) .. controls (284.87,120) and (282,117.13) .. (282,113.6) -- cycle ;
		%Straight Lines [id:da7011194954020175] 
		\draw    (126,119) -- (126,147.5) ;
		%Straight Lines [id:da8062705329606052] 
		\draw    (328,119) -- (328,147.5) ;
		%Straight Lines [id:da854985017000603] 
		\draw    (126,147.5) -- (232,162.5) ;
		%Straight Lines [id:da5150789261033402] 
		\draw    (232,162.5) -- (328,147.5) ;
		%Straight Lines [id:da2480035607158091] 
		\draw    (232,163) -- (232,191.5) ;
		%Rounded Rect [id:dp37324890998767835] 
		\draw   (162,240.4) .. controls (162,236.87) and (164.87,234) .. (168.4,234) -- (297.6,234) .. controls (301.13,234) and (304,236.87) .. (304,240.4) -- (304,259.6) .. controls (304,263.13) and (301.13,266) .. (297.6,266) -- (168.4,266) .. controls (164.87,266) and (162,263.13) .. (162,259.6) -- cycle ;
		%Straight Lines [id:da44914926894106055] 
		\draw    (231,266) -- (231,294.5) ;
		%Straight Lines [id:da48173791567363056] 
		\draw    (232,218.5) -- (232,232.5) ;
		\draw [shift={(232,234.5)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da174728449205225] 
		\draw    (232,365) -- (192.54,401.18) -- (178,414.5) ;
		%Straight Lines [id:da20423642856626523] 
		\draw    (282,413.5) -- (232,365.5) ;
		%Straight Lines [id:da5612344595082037] 
		\draw    (160,437.5) -- (136.44,460.11) ;
		\draw [shift={(135,461.5)}, rotate = 316.17] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da8929748054583597] 
		\draw    (302,438.5) -- (329.51,463.16) ;
		\draw [shift={(331,464.5)}, rotate = 221.88] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da720094639073076] 
		\draw    (331,464.5) -- (303.28,497.96) ;
		\draw [shift={(302,499.5)}, rotate = 309.64] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da3810332730221839] 
		\draw    (420,464.5) -- (420,280.5) ;
		%Straight Lines [id:da48862321771791284] 
		\draw    (331,464.5) -- (420,464.5) ;
		%Straight Lines [id:da920299974696865] 
		\draw    (377,104.5) -- (420,104.5) ;
		%Straight Lines [id:da014842651548173658] 
		\draw    (420,231.5) -- (420,136.5) -- (420,104.5) ;
		%Straight Lines [id:da589053840154339] 
		\draw    (232,354.75) -- (232,365.5) ;
		
		% Text Node
		\draw (89,95) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{definitions}};
		% Text Node
		\draw (289,95) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{hypothesis}};
		% Text Node
		\draw (170,195) node [anchor=north west][inner sep=0.75pt]   [align=left] {deduction process};
		% Text Node
		\draw (166.5,242) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{theoretical results}};
		% Text Node
		\draw (160.5,300.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{104.2pt}\setlength\topsep{0pt}
		\begin{center}
		empirical observations\\process
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (141,418) node [anchor=north west][inner sep=0.75pt]   [align=left] {confirm};
		% Text Node
		\draw (235,418) node [anchor=north west][inner sep=0.75pt]   [align=left] {does not confirm};
		% Text Node
		\draw (58,467) node [anchor=north west][inner sep=0.75pt]   [align=left] {provisional acceptance};
		% Text Node
		\draw (283,505) node [anchor=north west][inner sep=0.75pt]   [align=left] {reject};
		% Text Node
		\draw (385,235) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{51.49pt}\setlength\topsep{0pt}
		\begin{center}
		change in \\hypothesis
		\end{center}
		
		\end{minipage}};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Hypothetical-Deductive Reasoning block diagram}
	\end{figure}
	The deductive procedure is to hold as true, provisionally, this first proposal that we name, in logic a "\NewTerm{predicate}\index{predicate}" (see further below for more details) and to draw all the consequences logically necessary, that is to say to look for its implications.

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Consider the proposition (antecedent) $P$: "X is a man", it implies the following proposition (consequent) $Q$: "X is mortal".\\
	
	The expression $P \Rightarrow Q$ (if it is a human it is necessarily mortal) is a predicative implication (hence the term "predicate"). There is no case in this example where we can state $P$ without $Q$. This example is that of strict implication, as we find in the "syllogism" (logical reasoning figure).
	\end{tcolorbox}
	
	In logic, a "\NewTerm{syllogism}\index{syllogism}" is a logical reasoning relating at least three propositions: two or more of them, named "\NewTerm{premises}\index{premises}",  lead to a "\NewTerm{conclusion}\index{conclusion}".

	A very well-known example of syllogism is: «\textit{All men are mortal, but Socrates is a man; therefore Socrates is mortal}»: the two premises (named "major premise" and "minor premise") are propositions given and assumed to be true, the syllogism makes it possible to establish the formal validity of the conclusion, which is necessarily true if the premises are true.

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
Experts have shown that the hypothetical-deductive reasoning develops gradually by children from six to seven years old and that this kind of reasoning is used systematically starting with a strict propositional function until the age of eleven-twelve.
	\end{tcolorbox}
	
	\subsection{Propositional Calculus}
	The "\NewTerm{propositional calculus}\index{propositional calculus}" (or "propositional logic"\index{propositional logic}) is an absolutely indispensable preliminary to tackle a background in science, philosophy, law, politics, economics, etc. This type of calculation allows for decisions or testing procedures. These help to determine when a logical expression (proposition) is true and especially if it is always true.

	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] An expression that is always true whatever the content language of the variables that compose it is named a "\NewTerm{valid expression}"\index{valid expression}, a "\NewTerm{tautology}"\index{tautology} or a "\NewTerm{law of propositional logic}"\index{law of propositional logic}. In other words a tautology is a statement that is always true because it includes all logical possibilities.
		
		\item[D2.] An expression that is always false is named a "\NewTerm{contradiction}"\index{contradiction} or "{antilogy}"\index{antilogy}.
		
		\item[D3.] An expression that is sometimes true, sometimes false is named a "\NewTerm{contingent expression}"\index{contingent expression} (cela est souvent lié à l'analogie selon laquelle quelque chose peut exister mais ne doit pas nécessairement exister).
		
		\item[D4.] We name "\NewTerm{assertion}"\index{assertion} an expression that we can say unambiguously whether it is true or false.
		
		\item[D5.] The "\NewTerm{object language}"\index{object language} is the language used to write logical expressions.
		
		\item[D6.] The "\NewTerm{meta-language}"\index{meta language} is the language used to talk about the object language in everyday language.
	\end{enumerate}

	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
\textbf{R1.} There are expressions that are actually not assertions. For example, the statement "this statement is false" is a paradox that can be neither true nor false.\\\\
\textbf{R2.} Consider a logical expression $A$. If it is a tautology, we denote it by $\models A$ and by $A \models$ or $\not\models A$ if it is a contradiction.\\\\
\textbf{R3.} In mathematics we can try to prove in a general way that an assertion is true, but not that it is false (if this is the case we give just one example).
	\end{tcolorbox}

	\pagebreak
	\subsubsection{Propositions (premises)}

	\textbf{Definition (\#\thesection.\mydef):} In logic, a "\NewTerm{proposition}"\index{proposition} is a statement that has meaning. That means we can say unambiguously whether this statement is true ($T$) or false ($F$). This is what we name the "\NewTerm{law of excluded middle}"\index{law of excluded middle}.

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. "I lie" is not a proposition (premise). If we assume that this statement is true, it is an affirmation of his own disability, so we should conclude that it is false. But if we assume that it is false, then the author of this statement does not lie, so he told the truth, thus the proposal would be true...\\
	
	E2. Another funny example is:
	\begin{itemize}
		\item Everything has a creator
		\item God is that creator
		\item God does not have creator
	\end{itemize}
	It's a solution that fails since it violates its own premise...
	\end{tcolorbox}

	\textbf{Definition (\#\thesection.\mydef):} A proposition in binary logic (where the proposals are either true or false) is therefore never true and false at the same time. This is what we name the "\NewTerm{principle of non-contradiction}"\index{principle of non-contradiction}.

	Thus, a property on the set of propositions $E$ is an application $P$ from $E$ to the set of "\NewTerm{truth values True, False}\index{truth values (True, False)}" $\left\lbrace T,F\right\rbrace$:
	
	We speak about "\NewTerm{associated subset}"\index{associated subset} , when the proposition only generates a portion $E'$ of $E$ and vice versa.

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	In $E=\mathbb{N}$, if $P(x)$ states "$x$ is even", then $P=\{0,2,4,\ldots,2k,\ldots\}$ which is indeed only an associated subset of $E$ but of same cardinal (\SeeChapter{see section Set Theory page \pageref{cardinal}}).	
	\end{tcolorbox}
	
	\pagebreak
	\textbf{Definition (\#\thesection.\mydef):} Let $P$ be a property of the set $E$. A property $Q$ on $E$ is a "\NewTerm{negation}"\index{negation} of $P$ if and only if, for any $ x \in E$:
	\begin{itemize}
		\item $Q(x)$ is $F$ (false) if $P(x)$ is $T$ (true)
		\item $Q(x)$ is $T$ (true) if $P(x)$ is $F$ (false)
	\end{itemize}
	We can gather these conditions in a table named "\NewTerm{truth table}"\index{truth table}:
	\begin{table}[H]
	\begin{center}
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|p{2cm}|p{2cm}|}
				\hline
				\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$P$}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$Q$}} \\ \hline
				\centering\arraybackslash\ $T$ & \centering\arraybackslash\ $F$ \\ \hline
				\centering\arraybackslash\ $F$ & \centering\arraybackslash\ $T$  \\ \hline
		\end{tabular}
	\end{center}
	\caption{Truth table of values}
	\end{table}	
Table that we can also find or also write in the most explicit following form:

	\begin{table}[H]
	\begin{center}
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|p{2cm}|p{2cm}|}
				\hline
				\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$P$}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$Q$}} \\ \hline
				\centering\arraybackslash\ True & \centering\arraybackslash\ False \\ \hline
				\centering\arraybackslash\ False & \centering\arraybackslash\ True \\ \hline
		\end{tabular}
	\end{center}
	\caption{Truth table of explicit values}
	\end{table}

or in binary form:

	\begin{table}[H]
	\begin{center}
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|p{2cm}|p{2cm}|}
				\hline
				\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$P$}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$Q$}} \\ \hline
				\centering\arraybackslash\ 1 & \centering\arraybackslash\ 0 \\ \hline
				\centering\arraybackslash\ 0 & \centering\arraybackslash\ 1 \\ \hline
		\end{tabular}
	\end{center}
	\caption{Truth table of binary values}
	\end{table}	

	In other words, $P$ and $Q$ always have opposite truth values. We denote this kind of statement "$Q$ is a negation of $P$":
	
	where the symbol $\neg$ is the "\NewTerm{negation connector}"\index{negation connector}.

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The expressions must be well-formed expressions (often abbreviated "WFE"). By definition, any variable is a well-formed expression, thus $\neg P$ is a well-formed expression. If $P, Q$ are well-formed formulas, then $P \Rightarrow Q$ is a well-formed expression (the expression "I am lying" is not well-formed because it contradicts itself).
	\end{tcolorbox}

	\pagebreak
	\subsubsection{Connectors}
	There are other types of logical connectors:

	\textbf{Definition (\#\thesection.\mydef):} Let $P$ and $Q$ two properties set defined on the same set $E$. $P\vee Q $ (read "$P$ \texttt{OR} $Q$") is a statement (property) on $E$ defined by:
	\begin{itemize}
		\item $P\vee Q$ is true if at least $P$ or $Q$ are true
		\item $P\vee Q$ is false otherwise
	\end{itemize}

	We can create the truth table of the "\NewTerm{\texttt{OR} connector}"\index{OR (inclusive) connector} or "\NewTerm{disjunction connector}"\index{disjonction connector} $\vee$:
	\begin{table}[H]
	\begin{center}
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|p{2cm}|p{2cm}|p{2cm}|}
				\hline
				\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$P$}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$Q$}}  & \multicolumn{1}{c}{\textbf{$P \vee Q$}} \\ \hline
				\centering\arraybackslash\ $T$ & \centering\arraybackslash\ $T$ & \centering\arraybackslash\ $T$ \\ \hline
				\centering\arraybackslash\ $T$ & \centering\arraybackslash\ $F$ & \centering\arraybackslash\ $T$  \\ \hline
				\centering\arraybackslash\ $F$ & \centering\arraybackslash\ $T$ & \centering\arraybackslash\ $T$  \\ \hline
				\centering\arraybackslash\ $F$ & \centering\arraybackslash\ $F$ & \centering\arraybackslash\ $F$  \\ \hline
		\end{tabular}
	\end{center}
	\caption{\texttt{OR} inclusive truth table}
	\end{table}	
	The reader must me careful not to confound the Exclusive \texttt{OR}\index{exclusive OR} that means "\textit{either this or that, but not both}" with the Inclusive \texttt{OR}\index{inclusive OR} that means "\textit{either this, or that, or both}". The truth table of that first one, denoted most of times with the symbol $\oplus$ (and very rarely denoted with the symbol $\veebar$), is obviously a bit different\label{exclusive or}:
	\begin{table}[H]
	\begin{center}
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|p{2cm}|p{2cm}|p{2cm}|}
				\hline
				\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$P$}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$Q$}}  & \multicolumn{1}{c}{\textbf{$P \oplus Q$}} \\ \hline
				\centering\arraybackslash\ $T$ & \centering\arraybackslash\ $T$ & \centering\arraybackslash\ $F$ \\ \hline
				\centering\arraybackslash\ $T$ & \centering\arraybackslash\ $F$ & \centering\arraybackslash\ $T$  \\ \hline
				\centering\arraybackslash\ $F$ & \centering\arraybackslash\ $T$ & \centering\arraybackslash\ $T$  \\ \hline
				\centering\arraybackslash\ $F$ & \centering\arraybackslash\ $F$ & \centering\arraybackslash\ $F$  \\ \hline
		\end{tabular}
	\end{center}
	\caption{\texttt{OR} exclusive (XOR) truth table}
	\end{table}

	Every natural number is either even or odd, but not both. So every natural number is even xor odd (that's short for exclusive or).
	
	It should be easy to convince yourself that if the parts $P, Q$ of $E$ are respectively associated with the properties $P, Q$ thus $P \cup Q$ (\SeeChapter{see section Set Theory page \pageref{union}}) is associated to $P \vee Q$:
	
	The connector $\vee$ is associative (no doubt about the fact that it is commutative!). For proof, just do a truth table where you can check that:
	
	\textbf{Definition (\#\thesection.\mydef):} There is also the "\NewTerm{\texttt{AND} connector}"\index{AND connector} or also named "\NewTerm{conjunction connector}"\index{conjunction connector} $\wedge$ for whatever are $P, Q$ two properties defined on $E$, $P \wedge Q$ is a property defined on $E$ by:
	\begin{itemize}
		\item $P \wedge Q$ is true if both properties $P, Q$ are true (the famous syllogism: \textit{All men are mortal, Socrates is a man, therefore Socrates is mortal} is a famous example).
		
		\item $P \wedge Q$ if false otherwise
	\end{itemize}
	We can create the truth table of the "\NewTerm{\texttt{AND} connector}"\index{AND connector} or "\NewTerm{conjunction connector}"\index{conjunction connector} $\vee$:
	\begin{table}[H]
	\begin{center}
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|p{2cm}|p{2cm}|p{2cm}|}
				\hline
				\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$P$}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$Q$}}  & \multicolumn{1}{c}{\textbf{$P \wedge Q$}} \\ \hline
				\centering\arraybackslash\ $T$ & \centering\arraybackslash\ $T$ & \centering\arraybackslash\ $T$ \\ \hline
				\centering\arraybackslash\ $T$ & \centering\arraybackslash\ $F$ & \centering\arraybackslash\ $F$  \\ \hline
				\centering\arraybackslash\ $F$ & \centering\arraybackslash\ $T$ & \centering\arraybackslash\ $F$  \\ \hline
				\centering\arraybackslash\ $F$ & \centering\arraybackslash\ $F$ & \centering\arraybackslash\ $F$  \\ \hline
		\end{tabular}
	\end{center}
	\caption{\texttt{AND} truth table}
	\end{table}
	It should be also almost easy to convince yourself that if the parts $P, Q$ of $E$ are respectively associated with the properties $P, Q$ thus $P \cap Q$ (\SeeChapter{see section Set Theory page \pageref{intersection}}) is associated to $P \wedge Q$:
	
	The connector $\wedge$ is associative (no doubt about the fact that it is commutative!). For proof, just do a truth table where you can check that:
	
	The  connectors $\vee,\wedge$ are distributive one on the other. Using a simple truth table, we can show that (ask me if you want a put the truth table):
	
as well as:
	
	
	\textbf{Definition (\#\thesection.\mydef):} The "\NewTerm{negation}"\index{negation} operator $\lnot$ transforms a True value into a False value such that:
	
	So in logic, negation, also named "\NewTerm{logical complement}"\index{legal complement}, is an operation that takes a proposition $P$ to another proposition "not $P$", denoted $\lnot P$ or sometimes $\bar{P}$, which is interpreted intuitively as being True when $P$ is false and False when $P$ is True. Negation is thus a unary (single-argument) logical connective.

	As we will prove it in detail in the section of Logic System page \pageref{de morgan theorem} (using a simple truth table), the "\NewTerm{De Morgan's laws}"\index{De Morgan's laws} provide a way of distributing negation over disjunction and conjunction:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	To see the details of all logical operators, the reader should read the section of Logical Systems page \pageref{logical systems} where the identity, the double negative, the idempotence, associativity, the distributive properties, the De Morgan relations are presented more formally and with full details.
	\end{tcolorbox}

	Let us now come back on the "\NewTerm{logical implication connector}" \index{logical implication connector} sometimes also named just the "\NewTerm{conditional}"\index{conditional} denoted by the symbol $\Rightarrow$.

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
In some books on propositional calculus, this connector is denoted by the symbol $\supset$ and as part of the proof theory we often prefer the symbol $\rightarrow$.
	\end{tcolorbox}

	Let $P, Q$ two properties given on $E$. $P \Rightarrow Q$ is a property on $E$ defined by:
	\begin{enumerate}
		\item[P1.] $P \Rightarrow Q$ is False if $P$ is True and $Q$ is False.
		\item[P2.] $P \Rightarrow Q$ is True otherwise.
	\end{enumerate}
	In other words, the fact that $P$ logically implies $Q$ means that $Q$ is True for any assessment for which $P$ is True. The implication is therefore the famous "if... then ...".

	If we write the truth table of the implication (caution with the before last line!!!):

	\begin{table}[H]
	\begin{center}
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|p{2cm}|p{2cm}|p{2cm}|}
				\hline
				\centering\arraybackslash\ \cellcolor[gray]{0.75}\textbf{$P$} & 
  \centering\arraybackslash\ \cellcolor[gray]{0.75}\textbf{$Q$}  & \centering\arraybackslash\ \textbf{$P \Rightarrow Q$} \\ \hline
				\centering\arraybackslash\ $T$ & \centering\arraybackslash\ $T$ & \centering\arraybackslash\ $T$ \\ \hline
				\centering\arraybackslash\ $T$ & \centering\arraybackslash\ $F$ & \centering\arraybackslash\ $F$  \\ \hline
				\centering\arraybackslash\ $F$ & \centering\arraybackslash\ $T$ & \centering\arraybackslash\ $T$  \\ \hline
				\centering\arraybackslash\ $F$ & \centering\arraybackslash\ $F$ & \centering\arraybackslash\ $T$  \\ \hline
		\end{tabular}
	\end{center}
	\caption{Implication truth table}
	\end{table}
	In other terms, a False proposition implies that any conclusion will always be True. This is named a "\NewTerm{informal fallacy}\index{informal fallacy}\index{informal fallacy}" and it is a very important concept in null hypothesis statistical tests (\SeeChapter{see section Statistics page \pageref{p value}}). If the proposition is True the implication can be True only if the result is True.
	
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe A given set of logical connectives is "\NewTerm{truth functionally complete}\index{truth functionally complete}" if and only if all possible connectives can be expressed through suitable combinations of the ones in the given set.\\

	It turns out the set of five logical connectives introduced in this chapter is truth functionally complete. In fact, the subset containing only the three connectives AND, OR, and NOT is also truth functionally complete. This means that you don't need the other two connectives-implication and equivalence-because you can express them in terms of NOT, AND, and OR:
	
	Although the set of connectives {AND, OR, NOT} is indeed both sufficient and convenient to express all possible compound predicates, you can achieve the same goal with even a single logical connective: the NAND operator (not and).
	\end{tcolorbox}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Strictly speaking, the above table should use the symbol $\rightarrow$, and the symbol $\Rightarrow$ should be reserved only for implications that are true.
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Consider the proposition: "If you get your diploma, I buy you a computer".\\

	Of all cases, only one corresponds to a broken promise: the one where the child graduates, and still has no computer (second line in the table above).\\

	What means exactly this promise, that we will write as following:
	\begin{center}
	"You have your degree" $\Rightarrow$ "I buy you a computer"?
	\end{center}

	Exactly this:
	\begin{itemize}
		\item If you have get graduate, for sure, I will buy you a computer (I can not not buy it).
		
		\item If you do not get graduate, I said nothing.
	\end{itemize}
	\end{tcolorbox}
	The implication gives us that from any false proposition we can deduce any proposal (last two lines)!!!
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Informal fallacy is very interesting to apply in the case of debates on social networks. Indeed, if someone ask you to prove your premise (null hypothesis) that he assumes to be false, then he didn't understand anything to logic as he should know that from any false premise (null hypothesis) you can always have a true conclusion (you don't reject the null hypothesis) whatever the reasoning! What the guy must do is assume your null hypothesis as true and found at least one example making it false (we reject the null hypothesis in favour of the alternative hypothesis). This is how science works and also the fact that this is not obvious explains why so many people loose their time to argue in social networks using biased arguments.
	\end{tcolorbox}

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	In a course taught by Russell on the subject from a false proposition, any proposal can be inferred, a student asked him the following question (anecdote or legend?):
	\begin{itemize}
		\item "Are you saying that from the proposition $2 + 2 = 5$, it follows that you are the Pope?".
		\item "Yes", answered Russell.
		\item "And could you prove it!", asked the student sceptical...
		\item "Certainly", answered Russell, who immediately offered the following proof:
			\begin{enumerate}
				\item Suppose that $2 + 2 = 5$.
				\item Subtract $3$ from each member of the equality, we thus get $1 = 2$.
				\item By symmetry $2=1$.
				\item The Pope and I are $2$. Since $2 = 1$, Pope and I are $1$. It follow I'm the Pope.
			\end{enumerate}
	\end{itemize}
	\end{tcolorbox}
	The implication connector is essential in mathematics, philosophy, etc. It is a backbone of any proof, evidence or deduction. It has the following useful properties (normally easy to check with a small truth table):
	
	And we have from the last property (again verifiable by a truth table):
	
	The "\NewTerm{logical equivalence connector}"\index{logical equivalence connector} or "\NewTerm{biconditional connector}"\index{biconditional connector} denoted most of times by "$\Leftrightarrow$" or sometimes by "$\leftrightarrow$" meaning by definition:
	
	in other words, the first expression has the same value for all evaluation of the second. It is the same with the following relation that is more "atomic" as the logical equivalence is reduced only to the use of $\wedge,\vee$ and negation $\lnot$ (combination of what we have seen above):
	
	When we prove such equivalence of two expressions we can therefore say that: "we proves that the equivalence is a tautology".

	The truth table of the equivalence is logically given by:
	\begin{table}[H]
	\begin{center}
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
				\hline
				\centering\arraybackslash\ \cellcolor[gray]{0.75}\textbf{$P$} & 
  \centering\arraybackslash\ \cellcolor[gray]{0.75}\textbf{$Q$}  & \centering\arraybackslash\ \textbf{$P \Rightarrow Q$}  & \centering\arraybackslash\ \textbf{$Q \Rightarrow P$} & \centering\arraybackslash\ \textbf{$Q \Leftrightarrow P$}\\ \hline
				\centering\arraybackslash\ $T$ & \centering\arraybackslash\ $T$ & \centering\arraybackslash\ $T$ & \centering\arraybackslash\ $T$ & \centering\arraybackslash\ $T$ \\ \hline
				\centering\arraybackslash\ $T$ & \centering\arraybackslash\ $F$ & \centering\arraybackslash\ $F$ & \centering\arraybackslash\ $T$ & \centering\arraybackslash\ $F$ \\ \hline
				\centering\arraybackslash\ $F$ & \centering\arraybackslash\ $T$ & \centering\arraybackslash\ $T$ & \centering\arraybackslash\ $F$ & \centering\arraybackslash\ $F$\\ \hline
				\centering\arraybackslash\ $F$ & \centering\arraybackslash\ $F$ & \centering\arraybackslash\ $T$ & \centering\arraybackslash\ $T$ & \centering\arraybackslash\ $T$ \\ \hline
		\end{tabular}
	\end{center}
	\caption{Truth table of the equivalence}
	\end{table}

	$P \Leftrightarrow Q$ means (when its true!) that "$P$ and $Q$ always have the same truth value" or "$P$ and $Q$ are equivalent." This is True if $P$ and $Q$ have the same value, False otherwise.

	Of course (it is a tautology):
	
	The relation $P \Leftrightarrow Q$ is equivalent to that $P$ is a necessary and sufficient condition for $Q$ and that $Q$ is a necessary and sufficient condition for $P$.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Strictly speaking, the above table should use the symbol $\leftrightarrow$, and the symbol $\Leftrightarrow$ should be reserved only for equivalences that are true.
	\end{tcolorbox}
	The conclusion is that the conditions of the types: "necessary", "sufficient", "necessary and sufficient" can be reformulated with the terms "only if", "if", "if and only if".

	Therefore:
	\begin{enumerate}
		\item $P \Rightarrow Q$ reflects the fact that $Q$ is a \NewTerm{necessary}\index{condition!necessary} condition for $P$ or in other words, $P$ is True \NewTerm{only if}\index{condition!only if} $Q$ is True (in the truth table, when $P \Rightarrow Q$ is equal to $1$ we see that $P$ is $1$ \NewTerm{only if} $Q$ is also $1$). We also say that \NewTerm{if} $P$ is true \NewTerm{then} $Q$ is true.
		\item $P \Longleftarrow Q$ or what is the same $Q \Longrightarrow P$ reflects the fact that $Q$ is a \NewTerm{sufficient}\index{condition!sufficient} condition for $P$ or in other words, $P$ is True \NewTerm{if} $Q$ is True (in the truth table, when $Q \Longrightarrow P$ takes the value $1$ we see that $P$ is $1$ \NewTerm{if} $Q$ is $1$ too).
		\item $P \Leftrightarrow Q$ reflects the fact that $Q$ is a \NewTerm{necessary and sufficient}\index{necessary and sufficient}\index{condition!necessary and sufficient}  condition for $P$ or in other words, $P$ is True \NewTerm{if and only if}\index{condition: if and only if}  $Q$ is True (in the truth table, when $P \Leftrightarrow Q$ takes the value $1$ we see that $P$ is $1$ \NewTerm{if} $Q$ is $1$ and \NewTerm{if and only if} $Q$ is equal to $1$).
	\end{enumerate}

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The expression "if and only if" therefore corresponds to a logical equivalence and can only be used to describe a bi-implication !!
	\end{tcolorbox}
	
	False equivalence is a logical fallacy in which an equivalence is drawn between two subjects based on flawed or false reasoning. This fallacy is categorized as a fallacy of inconsistency. Colloquially, a false equivalence is often named "comparing apples and oranges".
	
	This fallacy is committed when one shared trait between two subjects is assumed to show equivalence, especially in order of magnitude, when equivalence is not necessarily the logical result. False equivalence is a common result when an anecdotal similarity is pointed out as equal, but the claim of equivalence does not bear scrutiny because the similarity is based on oversimplification or ignorance of additional factors. The pattern of the fallacy is often as such:
	
	If $\{x_1,\ldots,x_k,\ldots,x_n\}\in A$ and $\{x_1,\ldots,x_{k-m},\ldots,x_{k-m},\ldots,x_n\}\in B$ with $0<m<n \in \mathbb{N}$, then since they both contain $\{x_1,\ldots,x_{k-m},x_{k-m},\ldots,x_n\}$ then $A=B$. Since $\{x_{k-m},\ldots,x_{k-m}\}$ is not required to exist in both sets; only a passing similarity is required to cause this fallacy to be used. More formally if $0<m<n \in \mathbb{N}$:
	

	False equivalence arguments are often used on social networks, journalism, and in politics, where one strategical choice may be compared to flaws of a wholly different nature of another.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\textwidth]{img/intro/false_equivalence.jpg}
		\caption[]{False equivalence fallacy}
	\end{figure}
	
	The first stage of propositional calculus is the formalization of natural language statements. To make this work, the propositional calculus finally provides three types of tools:
	
	\begin{enumerate}
		 \item The "\NewTerm{propositional variables}\index{propositional variables}" ($P, Q, R, \ldots$) symbolize any simple proposals. If the same variable occurs multiple times, each time it symbolizes the same proposal.
		 
		 \item The five logical operators: $\neg , \wedge, \vee, \Leftrightarrow, \Rightarrow$.
		 
		 \item Punctuation are reduced to only opening and closing parentheses that organize reading so as to avoid ambiguity.
	\end{enumerate}
	Here is a summary table:
	\begin{table}[H]
	\begin{center}
		\definecolor{gris}{gray}{0.85}
			\resizebox{\textwidth}{!}{\begin{tabular}{|p{12cm}|p{1cm}|p{1.5cm}|}
				\hline
				\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Description}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Symbol}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Usage}}  \\ \hline
				The "\NewTerm{Negation}" is an operator that act only on one proposal; it is unary or monadic. "\textit{It is not raining}" will be written $\neg P$. This statement is true if and only if $P$ is false (in this case if it is false that it is raining). The conventional use of negation is characterized by the double negation law: $\neg \neg P$ is equivalent to $P$. & $\neg$ & $\neg P$ \\ \hline
				
				The "\NewTerm{conjunction}" or "\NewTerm{logical product}" is a binary operator; it connects two proposals. "\textit{Every man is mortal AND my car loses oil}" is written $P \wedge Q$. This latter expression is true if and only if $P$ is true and $Q$ is true. & $\wedge$ & $P \wedge Q$ \\ \hline
				
				The "\NewTerm{disconnection}\index{disconnection}" or "\NewTerm{logical sum}\index{logical sum}" is also a binary operator. $P \vee Q$ is true if and only if $P$ is true OR $Q$ is true or both are true. We can understand the OR into two ways: either inclusively or exclusively. In the first  case $P \vee Q$ is true if $P$ is true, if $Q$ is true or if $P$ and $Q$ are both true. In the second case, $P \vee Q$ is true if $P$ is true or if $Q$ is true but not if both are. The disjunction of propositional calculus is the inclusive OR and we give to the exclusive OR, that is to say the XOR, the name of "\NewTerm{alternative}\index{alternative}". &  $\vee$ & $P \vee Q$ \\ \hline
				
				The "\NewTerm{implication}\index{implication}" or "\NewTerm{inference}\index{inference}" is also a binary operator. It corresponds roughly to the linguistic pattern "\textit{If ... then ...}". "\textit{If I have time, I will go see this movie}" will be written $P \Rightarrow Q$. This latter relation is false if $P$ is true and $Q$ is false. If the result (here $Q$) is true, the implication $P \Rightarrow Q$ is true. When the antecedent (here $P$) is false, the implication is always true. This latter remark can be understood if one refers to statements of the type: «\textit{If we could put Paris in a bottle, the Eiffel Tower would be used as a plug}». In summary, an implication is false if and only if its antecedent is true and its consequent is false. & $\Rightarrow$ & $P \Rightarrow Q$ \\ \hline
				
				The "\NewTerm{bi-implication}\index{bi-implication}" or "\NewTerm{equivalence}\index{equivalence}" $\Rightarrow$ is, too, a binary operator: it symbolizes the terms "\textit{... if and only if ...}" and "\textit{... is equivalent to ...}". The equivalence of two propositions is true if they have the same truth value. The bi-implication therefore expressed as a form of identity, which is why it is often used in definitions. & $\Leftrightarrow$ & $P \Leftrightarrow Q$\\ \hline
		\end{tabular}}
	\end{center}
	\caption{Summary of logical core operators}
	\end{table}	
	And the corresponding summary truth table:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		$P$ & $Q$ & $P\wedge Q$ & $P\vee Q$ & $P\Rightarrow Q$ & $P\Leftrightarrow Q$ & $\neg P$ \\ \hline
		$T$ & $T$ & $T$ & $T$ & $T$ & $T$ & $F$ \\ \hline
		$T$ & $F$ & $F$ & $T$ & $F$ & $F$ & $F$ \\ \hline
		$F$ & $T$ & $F$ & $T$ & $T$ & $F$ & $T$ \\ \hline
		$F$ & $F$ & $F$ & $F$ & $T$ & $T$ & $T$ \\ \hline
		\end{tabular}
		\caption{Truth table of main logical operators}
	\end{table}
	The reader may read some authors that like to use at minimum the natural language in their books (...) and that use the symbol "$\therefore$" that is sometimes placed before a logical consequence, such as the conclusion of a syllogism. This symbol consists of three dots placed in an upright triangle and must be read as the word "\textit{therefore}". We can also make use of the symbol "$\because$" that must be read as the word "\textit{because}". 
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	$\because$ All men are mortal.\\
	$\because$ Socrates is a man.\\
	$\therefore$ Socrates is mortal.
	\end{tcolorbox}
	In this book we will however avoid using this notation as the engineers don't make use of that latter a lot. 
	
	It is possible to establish equivalences between these operators presented earlier. We have already seen how the biconditional could be defined as a product of reciprocal conditional, let us see now other equivalences:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The classical operators $\wedge, \vee, \Leftrightarrow$ can therefore be defined using the canonical operators $\neg, \Rightarrow$ through equivalence laws between them.
	\end{tcolorbox}
	Also notice the two relations of De Morgan (see section of Boolean Algebra page \pageref{de morgan theorem} for the proof):
	
	that allow us to transform the disjunction into conjunction and vice versa:
	
	To summarize the most classical laws of logic:
	\begin{table}[H]
		\centering
		\resizebox{\textwidth}{!}{\begin{tabular}{|l|l|}
		\hline
		\rowcolor[HTML]{C0C0C0} 
		\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\textbf{Basic Law of FOL}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{Formalism}} \\ \hline
		Detachment (aka Modus Ponens) & $(p \rightarrow q) \land p\Rightarrow q$ \\ \hline
		Indirect reasoning (aka Modus Tollens) & $(p \to q) \land \neg q \Rightarrow \neg p$ \\ \hline
		Disjunctive addition & $p\Rightarrow (p\lor q)$ \\ \hline
		Conjunctive simplification & $(p \land q) \Rightarrow p$ \\ \hline
		Disjunctive simplification & $(p \lor q) \land \neg p \Rightarrow q$ \\ \hline
		Chain rule & $(p \to q) \land ( q \rightarrow r) \Rightarrow (p\to r)$ \\ \hline
		Conditional equivalence & $p \rightarrow q \Leftrightarrow \neg p \lor q$ \\ \hline
		Biconditional equivalences & $(p \leftrightarrow q) \Leftrightarrow (p\rightarrow q) \land (q \rightarrow p)\Leftrightarrow (p \land q) \lor (\neg p \land \neg q)$ \\ \hline
		Contrapositive & $(p\to q) \Leftrightarrow (\neg q \to \neg p)$ \\ \hline
		\end{tabular}}
		\caption{Basic Logical Laws - Common Implications and Equivalences}
	\end{table}
	
	\subsubsection{Decision procedures}
	We have previously introduced the basic elements allowing us to operate on expressions from properties (propositional variables) without saying much about the handling of such expressions. So now you need to know that in propositional calculus there are two ways to establish that a proposition is a law of propositional logic. We can either:
	
	\begin{enumerate}
		\item Use non-automated procedures
		
		\item Use axiomatic and demonstrative procedures
	\end{enumerate}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In many books these procedures are presented before the structure of the propositional language. We chose here to do the opposite thinking that the approach would be easier.
	\end{tcolorbox}
	
	\paragraph{Non-axiomatic procedural decisions}\mbox{}\\\\
	Several of these methods exist, but we will limit ourselves here to the simplest of them, that of the matrix calculation, often referred to as "\NewTerm{methods of truth tables}\index{method of truth tables}".
	
	The construction procedure is as we have already seen quite simple. Indeed, the truth value of a complex expression is a function of the truth value of the simple statements that compose it, and finally based on the truth value of propositional variables that makes it. Considering all possible combinations of truth values of propositional variables, we can determine the truth values of the complex expression.
	
	Truth tables, as we have seen it, gives us the possibility to decide, about any proposition, if this latter is a tautology (always true), a contradiction (always false) or a contingent expression (sometimes true, sometimes false).
	
	We can distinguish at least four ways to combine propositional variables, brackets and connectors:
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|c|l|c|c|}
				\hline
				\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Name}}  & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Description}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Example}} \\ \hline
				\cellcolor[gray]{0.75}\textbf{1} & Malformed statement  & Nonsense. Neither true nor false & \centering\arraybackslash\ $(\vee P)Q$ \\ \hline
				\cellcolor[gray]{0.75}\textbf{2} & Tautology & Statement always true & \centering\arraybackslash\ $P \vee \neg P$  \\ \hline
				\cellcolor[gray]{0.75}\textbf{3} & Contradiction & Statement always false & \centering\arraybackslash\ $P \wedge \neg P$  \\ \hline
				\cellcolor[gray]{0.75}\textbf{4}  & Contingent statement & Statement sometimes true, sometimes false & \centering\arraybackslash\ $P\vee Q$  \\ \hline
		\end{tabular}
		\caption{Combination of propositional variables}
	\end{table}
	The method of truth tables helps to determine the type of expression that are well-formed to which we have to face. It requires, in principle, no invention, it is "only" a mechanical procedure. Axiomatized procedures, however, are not entirely mechanical. Inventing a proof as part of an axiomatized system requires sometimes hability, practice or luck. Regarding to truth tables, here is the protocol to follow:
	
	When facing a well-formed expression, or function of truth, we first determine how many distinct propositional variables we are dealing with. We then examine the various arguments that form this expression. We then construct a table with $2^n$ columns ($n$ being the number of variables and without forgetting that they are binary variables!) and a number of columns equal to the number of arguments plus columns for the expression itself and its other components (see previous examples). Then we assign to the variables the various combinations of True ($1$) and False ($0$) values that may be conferred upon them. Each row corresponds to a possible outcome and all of the rows is the set of all possible outcomes. There is, for example, a possible outcome wherein $P$ is a true statement while $Q$ is false.
	
	\paragraph{Axiomatic procedural decisions}\mbox{}\\\\
	The axiomatization of a theory implies, besides its formalization, that we start form a finite number of axioms and that through the controlled transformation of these, we can get all the theorems of this theory. So we start from a few axioms whose truth is a statement (not proven). We determine afterwards deduction rules for manipulating the axioms or expression obtained from these. The sequence of these deductions is a proof that leads to a theorem, a law or lemma.
	
	We will now briefly present two axiomatic systems, each consisting of axioms using two specific rules named "\NewTerm{inference rules}\index{inference rules}" (intuitive rules):
	
	\begin{enumerate}
		\item The "\NewTerm{modus ponens}\index{modus ponens}": If we prove $A$ and $A\Rightarrow B$, then we can deduce $B$. $A$ is named the "\NewTerm{minor premise}\index{minor premise}" and $A\Rightarrow B$ the "\NewTerm{major premise}\index{major premise}" of the modus ponens rule.
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		From:
		\begin{gather*}
			x>y
		\end{gather*}
		and:
		\begin{gather*}
			(x>y)\Rightarrow (y<x)
		\end{gather*}
		we can deduce that:
		\begin{gather*}
			y<x
		\end{gather*}
		\end{tcolorbox}
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		Humans typically communicate in a way that resists shallow logical analysis. In a real conversation, people use words rather than terms, make utterances rather than sentences, and employ a wider variety of inference methods than modus ponens. A great deal of what is communicated and inferred in a conversation depends on context, the speakers and audience, their history, their shared knowledge and confidences, the feelers they lay out to establish mutual trust and rapport.
		\end{tcolorbox}
		
		\item The "\NewTerm{substitution}\index{substitution}": we can in a schema of axioms replace a letter by any formula, at the condition that all identical letters are replaced by identical formulas!
		
		Let us give as an example, two axiomatic systems: the axiomatic system of Whitehead and Russell, the axiomatic system of Lukasiewicz.
		\begin{enumerate}
			\item The axiomatic system of Russell and Whitehead adopts $\neg, \vee$ as primitive symbols and define $\Rightarrow, \wedge ,\Leftrightarrow$ from these latter as follows (easily verifiable relations with truth tables):
			
			This system includes $5$ axioms, somewhat quite obvious plus two rules of inference. Axioms are given here using non-primitive symbols, as did Whitehead and Russell:
			\begin{enumerate}
				\item[A1.] $(A \vee A)\Rightarrow A$
				\item[A2.] $B \Rightarrow (A\vee B)$
				\item[A3.] $(A\vee B) \Rightarrow (B\vee A)$
				\item[A4.] $(A\vee (B\vee C)) \Rightarrow (B\vee (A\vee C))$
				\item[A5.] $(B\Rightarrow C)\Rightarrow \left((A\vee B)\Rightarrow(A\vee C)\right)$
			\end{enumerate}
			we have already presented above some of these.
			\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
			These five axioms are not independent of each other. The fourth can be obtained from the other four.
			\end{tcolorbox}
			For example, to justify that $\neg A\vee A$ has a sense\footnote{It's named the "\NewTerm{law of the excluded middle}": A proposition is either true or false. This has been widely held to be self-evident for thousands of years}, we can proceed as following (in this system we only prove logical truths, meaning that every line need to be a logical truth by itself):
			\begin{table}[H]
			\begin{center}
				\definecolor{gris}{gray}{0.85}
					\begin{tabular}{ccr}
						(1) &  $B\Rightarrow (A\vee B)$ & Axiom A2 \\ 
						(2) &  $A\Rightarrow (A\vee A)$  & (1) and substitution \\
						(3) &  $(B\Rightarrow C)\Rightarrow \left((A\vee B)\Rightarrow (A\vee C)\right)$  & Axiom A5 (necessary)\\
						(4) &  $(B\Rightarrow C)\Rightarrow ((\neg A\vee B)\Rightarrow (\neg A\vee C))$ & (3) and substitution \\
						(5) &  $(B \Rightarrow C)\Rightarrow((A\Rightarrow B)\Rightarrow (A\Rightarrow C))$  & (4) and property of $\Rightarrow$ \\
						(6) &  $((A\vee A)\Rightarrow A)\Rightarrow((A\Rightarrow (A\vee A))\Rightarrow (A\Rightarrow A))$  & (5) and substitution \\
						(7) &  $(A\Rightarrow (A\vee A)) \Rightarrow (A\Rightarrow A)$  & (6) (modus ponens)\\
						(8) &  $(A \Rightarrow A)\Rightarrow (A\Rightarrow A)$  & (7) and axiom A1 \\
						(9) &  $A \Rightarrow A$  & (8) and modus ponens \\
						(10) &  $\neg A \vee A$  & (9) and property of $\Rightarrow$ \\
				\end{tabular}
			\end{center}
			\end{table}
			
			\item The axiomatic system of Lukasiewicz includes three axioms, plus the two rules of inference (modus ponens and substitution):
			\begin{enumerate}
				\item[A1.] $(A\Rightarrow B) \Rightarrow ((B\Rightarrow C) \Rightarrow (A\Rightarrow C))$
				\item[A2.] $A\Rightarrow  (\neg A\Rightarrow B)$
				\item[A3.] $(\neg A \Rightarrow A)\Rightarrow  A$
			\end{enumerate}
			Here is the proof of the first two axioms in the system of Russell and Whitehead. These are the formulas (6) and (16) of the following derivation:
			\begin{table}[H]
			\begin{center}
				\definecolor{gris}{gray}{0.85}
					\begin{tabular}{ccr}
						(1) &  $(A\vee (B\vee C))\Rightarrow (B\vee (A\vee C))$ & Axiom A4 \\ 
						(2) &  \parbox{6cm}{$(\neg (B\Rightarrow C)\vee (\neg(A\vee B)\vee (A\vee C)))$\\ $\Rightarrow (\neg (A\vee B)\vee (\neg(B\Rightarrow C)\vee(A\vee C)))$}  & (1) and substitution \\
						(3) &  $\neg(A\vee B)\vee (\neg(B\Rightarrow C)\vee (A\vee C))$  & A4 on (2) and modus ponens\\
						(4) &  $(A\vee B)\Rightarrow ((B\Rightarrow C)\Rightarrow (A\vee C))$ & (3) by the property of $\Rightarrow$ \\
						(5) & $(\neg A\vee B)\Rightarrow ((B\Rightarrow C)\Rightarrow(\neg A\vee C))$ & (4) and substitution \\ 
						(6) & $(A\Rightarrow B)\Rightarrow ((B\Rightarrow C)\Rightarrow (A\Rightarrow C))$ & (5) and property of $\Rightarrow$\\
						(7) & $(B\Rightarrow (A\vee B))\Rightarrow \begin{pmatrix}((A\vee B)\Rightarrow (B\vee A))\\ \Rightarrow (B\Rightarrow (B\vee A))\end{pmatrix}$ & (6) and substitution\\
						(8) & $((A\vee B)\Rightarrow (B\vee A))\Rightarrow (B\Rightarrow (B\vee A))$ & (7) modus ponens\\
						(9) & $B\Rightarrow (B\vee A)$ & (8) modus ponens\\
						(10) & $\neg B\Rightarrow (\neg B\vee A)$ & (9) and substitution\\
						(11) & $\neg\neg B\vee (\neg B \vee A)$ & (10) and property of $\Rightarrow$\\
						(12) & $\neg\neg B\vee (\neg B \vee A) \Rightarrow (\neg B\vee (\neg\neg B\vee A))$ & A4 + substitution with (11)\\
						(13) & $\neg B\vee (\neg\neg B\vee A)$ & (12) and modus ponens\\
						(14) & $B\Rightarrow (\neg\neg B\vee A)$ & (19) and property of $\Rightarrow$\\
						(15) & $B\Rightarrow (\neg B\vee A)$ & (14) and property of $\Rightarrow$\\
						(16) & $A\Rightarrow (\neg A\Rightarrow B)$ & (15) and substitution\\
				\end{tabular}
			\end{center}
			\end{table}
		\end{enumerate}
	\end{enumerate}
	These axiomatizations let us found as theorems all tautologies or laws of the propositional logic. From everything that has been said so far, we can try to define what is a proof!!!!
	
	\textbf{Definition (\#\thesection.\mydef):} A finite sequence of formulas $B_1,B_2,\ldots,B_m$ is named a "\NewTerm{proof}\index{proof}" from the assumptions/hypothesis $A_1,A_2,\ldots,A_n$ if for each $i$:
	\begin{itemize}
		\item $B_i$ is one of the assumptions/hypothesis $A_1,A_2,\ldots,A_n$
		
		\item or $B_i$ is a variant of an axiom
		
		\item or $B_i$ is inferred (by the application of the modus ponens rule) from the major premise $B_j$ and minor premise $B_k$ where $j,k<i$
		
		\item or $B_i$ is inferred (by the application of the substitution rule) from an anterior premise $B_j$, the replaced variable not appearing in $A_1,A_2,\ldots,A_n$
	\end{itemize}
	Such a sequence of formulas, $B_m$ being the final formula of the sequence, is more explicitly named "\NewTerm{proof of $B_m$}" from the assumptions/hypothesis (or axioms) $A_1,A_2,\ldots,A_n$, what we also write:
	
	 More explicitly a proof is a deductive argument for a mathematical statement. In the argument, other previously established statements, such as theorems, can be used. In principle, a proof can be traced back to self-evident or assumed statements, known as axioms.

	Proofs employ logic but usually include some amount of natural language which usually admits some ambiguity. In fact, the vast majority of proofs in written mathematics can be considered as applications of rigorous informal logic. Purely formal proofs, written in symbolic language instead of natural language, are considered in proof theory. 
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Note that when we try to prove a result from a number of assumptions, we do not try to prove the assumptions themselves!
	\end{tcolorbox}
	
	\pagebreak	
	\subsubsection{Quantifiers}
	We have to complete the use of the connectors of propositional calculus by what we name "\NewTerm{quantifiers}\index{quantifiers}" if we wish to solve some problems. Indeed, the propositional calculus does not allow us to state general things about the elements of a set, for example. In that sense, propositional logic is only part of the reasoning. The calculus of predicates on the contrary allows to formally handle statements such as "there exists an $x$ such that [$x$ has an American car]" or "for all $x$ [if $x$ is a dachshund, then $x$ is small]". In short, we extend the composed formulas in order to assert existential quantifiers ( "there...") and universal quantifiers ( "for every..."). The examples we just gave involve a bit special proposals like "$x$ has an American car." This is proposition with a variable. These proposals are in fact the application of a function to $x$. This function, is this that associates "$x$ has an American car" with $x$. We will denote this function by "... has an American car" and we say that is a propositional function because it is a function whose value is a proposal. Or a "predicate" as we already know.
	
	The existential and universal quantifiers go hand in hand with the use of propositional functions. The predicate calculus is however limited in the existential and universal formulas. Thus, we prohibit ourselves to use sentences like "there is an affirmation of $x$ such that ...". In fact, we allow ourselves to quantify only "individuals". This is why predicate logic is named "\NewTerm{first-order logic}\index{first order logic}" (FOL) or "\NewTerm{first-order predicate logic}\index{first order predicate logic}\label{first order predicate}" (FOPL) because it uses variables as basic mathematical objects (while in the second-order logic they can also be sets).
	\begin{figure}[H]
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,414); %set diagram left start at 0, and has height of 414
		
		%Shape: Rectangle [id:dp3812758200296995] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]  (51,25) -- (229.5,25) -- (229.5,74) -- (51,74) -- cycle ;
		%Shape: Rectangle [id:dp39379972928868456] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]  (347,24) -- (525.5,24) -- (525.5,73) -- (347,73) -- cycle ;
		%Shape: Circle [id:dp7818413921916829] 
		\draw  [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] (45,179.75) .. controls (45,127.97) and (86.97,86) .. (138.75,86) .. controls (190.53,86) and (232.5,127.97) .. (232.5,179.75) .. controls (232.5,231.53) and (190.53,273.5) .. (138.75,273.5) .. controls (86.97,273.5) and (45,231.53) .. (45,179.75) -- cycle ;
		%Shape: Circle [id:dp7502029730107302] 
		\draw  [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] (76.5,179.75) .. controls (76.5,145.37) and (104.37,117.5) .. (138.75,117.5) .. controls (173.13,117.5) and (201,145.37) .. (201,179.75) .. controls (201,214.13) and (173.13,242) .. (138.75,242) .. controls (104.37,242) and (76.5,214.13) .. (76.5,179.75) -- cycle ;
		%Shape: Circle [id:dp8652441033091895] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (131,141.5) .. controls (131,140.12) and (132.12,139) .. (133.5,139) .. controls (134.88,139) and (136,140.12) .. (136,141.5) .. controls (136,142.88) and (134.88,144) .. (133.5,144) .. controls (132.12,144) and (131,142.88) .. (131,141.5) -- cycle ;
		%Shape: Circle [id:dp42920444000040114] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (107,201.5) .. controls (107,200.12) and (108.12,199) .. (109.5,199) .. controls (110.88,199) and (112,200.12) .. (112,201.5) .. controls (112,202.88) and (110.88,204) .. (109.5,204) .. controls (108.12,204) and (107,202.88) .. (107,201.5) -- cycle ;
		%Shape: Circle [id:dp1020727784421045] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (163,191.5) .. controls (163,190.12) and (164.12,189) .. (165.5,189) .. controls (166.88,189) and (168,190.12) .. (168,191.5) .. controls (168,192.88) and (166.88,194) .. (165.5,194) .. controls (164.12,194) and (163,192.88) .. (163,191.5) -- cycle ;
		%Rounded Rect [id:dp2188008027124999] 
		\draw  [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] (259.5,126.15) .. controls (259.5,106.05) and (275.8,89.75) .. (295.9,89.75) -- (576.1,89.75) .. controls (596.2,89.75) and (612.5,106.05) .. (612.5,126.15) -- (612.5,235.35) .. controls (612.5,255.45) and (596.2,271.75) .. (576.1,271.75) -- (295.9,271.75) .. controls (275.8,271.75) and (259.5,255.45) .. (259.5,235.35) -- cycle ;
		%Shape: Circle [id:dp2567703597375819] 
		\draw  [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] (275.5,184.75) .. controls (275.5,150.37) and (303.37,122.5) .. (337.75,122.5) .. controls (372.13,122.5) and (400,150.37) .. (400,184.75) .. controls (400,219.13) and (372.13,247) .. (337.75,247) .. controls (303.37,247) and (275.5,219.13) .. (275.5,184.75) -- cycle ;
		%Shape: Circle [id:dp24566761417596972] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (330,146.5) .. controls (330,145.12) and (331.12,144) .. (332.5,144) .. controls (333.88,144) and (335,145.12) .. (335,146.5) .. controls (335,147.88) and (333.88,149) .. (332.5,149) .. controls (331.12,149) and (330,147.88) .. (330,146.5) -- cycle ;
		%Shape: Circle [id:dp3153197988418879] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (306,206.5) .. controls (306,205.12) and (307.12,204) .. (308.5,204) .. controls (309.88,204) and (311,205.12) .. (311,206.5) .. controls (311,207.88) and (309.88,209) .. (308.5,209) .. controls (307.12,209) and (306,207.88) .. (306,206.5) -- cycle ;
		%Shape: Circle [id:dp2622513982896748] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (362,196.5) .. controls (362,195.12) and (363.12,194) .. (364.5,194) .. controls (365.88,194) and (367,195.12) .. (367,196.5) .. controls (367,197.88) and (365.88,199) .. (364.5,199) .. controls (363.12,199) and (362,197.88) .. (362,196.5) -- cycle ;
		%Shape: Circle [id:dp3085707273672935] 
		\draw  [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] (450,188.25) .. controls (450,146.69) and (483.69,113) .. (525.25,113) .. controls (566.81,113) and (600.5,146.69) .. (600.5,188.25) .. controls (600.5,229.81) and (566.81,263.5) .. (525.25,263.5) .. controls (483.69,263.5) and (450,229.81) .. (450,188.25) -- cycle ;
		
		% Text Node
		\draw (82,41) node [anchor=north west][inner sep=0.75pt]   [align=left] {First-Order Logic};
		% Text Node
		\draw (368,40) node [anchor=north west][inner sep=0.75pt]   [align=left] {Second-Order Logic};
		% Text Node
		\draw (112,99) node [anchor=north west][inner sep=0.75pt]   [align=left] {Objects:};
		% Text Node
		\draw (114,128) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle A$};
		% Text Node
		\draw (113,185) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle B$};
		% Text Node
		\draw (159,173) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle C$};
		% Text Node
		\draw (108,278) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \forall x:x=x$};
		% Text Node
		\draw (311,104) node [anchor=north west][inner sep=0.75pt]   [align=left] {Objects:};
		% Text Node
		\draw (313,133) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle A$};
		% Text Node
		\draw (312,190) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle B$};
		% Text Node
		\draw (358,178) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle C$};
		% Text Node
		\draw (308,278) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \forall x\forall y:\left(( \forall P:P( x) =P( y))\rightarrow ( x=y)\right)$};
		% Text Node
		\draw (490,95) node [anchor=north west][inner sep=0.75pt]   [align=left] {Properties:};
		% Text Node
		\draw (455,154) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle  \begin{array}{{>{\displaystyle}l}}
		\{C\} \ \{A,C\}\\
		\{B\} \ \{A\} \ \{\} \ \{B,C\}\\
		\{A,B,C\} \ \{A,B\}
		\end{array}$};
		\end{tikzpicture}
	\end{figure}
	
	Before going back to the study of the predicate calculus we define:
	\begin{enumerate}
		\item[D1.] The "\NewTerm{universal quantifier}\index{universal quantifier}": $\forall$ (for all)

		\item[D2.]  The "\NewTerm{existential quantifier}\index{existential quantifier}": $\exists$ (exists)
	\end{enumerate}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	If any complex number is the product of a non-negative number and a number of modulus $1$ we will write:
	
	\end{tcolorbox}
	
	The order of quantifiers is critical to meaning, as illustrated by the following two propositions:
	\begin{center}
	For every natural number $n$, there exists a natural number $s$ such that $s$ = $n^2$.
	\end{center}

	This is clearly true! It just asserts that every natural number has a square. The meaning of the assertion in which the quantifiers are turned around is different:

	\begin{center}
	There exists a natural number $s$ such that for every natural number $n$, $s = n^2$.
	\end{center}
	This is clearly false! It asserts that there is a single natural number $s$ that is at the same time the square of every natural number. 
		
	A frequent question in physics and mathematics is to know if the universal quantifier has to be before of after the predicates they refer to. In fact, strictly in terms of formal logic, quantifiers are always at the beginning of any formula. However, almost no one gives a proof that is written in the formal language. Even simple proofs would be very long and unreadable. But anyone, regardless of what natural language they speak, will interpret a sentence in the formal language in the same way. The price for this clarity of course is readability. Natural languages, because of their inherent ambiguity, are subject to many more limitations.
		
	Obviously the proper usage of a formal notation or of a more informal one depends particularly on the context of presentation. It is essential to whom we communicate an idea and this should guide us to use a suitable level of formal notation.
	
	We use the sometimes the symbol $\exists !$ to say briefly: "there is one and only one". 
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	A famous example is the way to explicit that the logarithm is a bijective function:
	
	\end{tcolorbox}
	Suppose now we claim that there is no smallest number in $\mathbb{R}$. We can translate this into symbols as:
	
	Literally: "it is not true that there is a real number $x$ such tat for all real numbers $y$, $x$ is less than or equal to $y$".
	
	However, we can negate quantifiers: we can pass a negation over a quantifier by switching the quantifier type (between universal and existential). So the statement above should be logically equivalent to:
	
	Notice that $y<x$ is the negation of $x\leq y$. This literally says: "for every real number $x$ there is a real number $y$ which is small than $x$". We see that this is another way to make our original claim.

	Before moving to predicate calculus, let us have a non exhaustive summary that is common in the literature and uses negation:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|}
			\hline 
			\cellcolor[gray]{0.75}\textbf{Expression} & & \cellcolor[gray]{0.75}\textbf{Is equivalent to}\\ \hline
			 $\neg\neg A $& $\Leftrightarrow$ & A\\ \hline
			 $\neg A \Rightarrow \neg B$&$\Leftrightarrow$ &$B\Rightarrow A$ \\ \hline
			$\neg A \Rightarrow B$&$\Leftrightarrow$ &$\neg B\Rightarrow A$ \\ \hline
			$A\Rightarrow \neg B$ & $\Leftrightarrow$ & $(A \wedge B )\Rightarrow F$\\ \hline
			$\neg A \wedge \neg B$ & $\Leftrightarrow$ & $\neg(A\vee B)$ \\ \hline
			$A \wedge\neg B$ & $\Leftrightarrow$ & $\neg(\neg A \vee B)$ \\ \hline
			$\wedge A \neg B$ &$\Leftrightarrow$ & $\neg(A\vee\neg B)$\\ \hline
			$\neg A \vee\neg B$ &$\Leftrightarrow$ & $\neg(A\wedge B)$\\ \hline
			$A \vee \neg B$ & $\Leftrightarrow$ & $\neg(\neg A\wedge
			 B)$\\ \hline
			$\neg A \vee B$ &$\Leftrightarrow$& $\neg(A\wedge\neg B)$\\ \hline
			$\neg (\exists x \in X: \neg P(x))$ &$\Leftrightarrow$& $(\forall x \in X: P(x))$\\ \hline
			$(\exists x \in X:\neg P(x))$ &$\Leftrightarrow$& $\neg(\forall x \in X: P(x))$ \\ \hline
			$(\exists x \in X: P(x))$ &$\Leftrightarrow$& $(\forall x \in X:\neg P(x))$ \\ \hline
			$(\exists x \in X: P(x))$ &$\Leftrightarrow$& $\neg(\forall x \in X: \neg P(x))$\\ \hline
		\end{tabular}
	\end{table}
	We will see now that the Proof Theory and Set Theory are the exact transcription of the principles and results of Logic (the one with a capitalized "L"!!!).
	
	\pagebreak
	\subsection{Predicate Calculus}
	In mathematics courses (algebra, analysis, geometry, ...), we prove the properties of different object types (integer, real, matrices, sequences, continuous functions, curves, triangles, ...). To prove these properties, we need of course that the objects on which we work are clearly defined (what is a set, what is a real, what is point, ...?).
	
	In first-order logic and, in particular, in proof theory, the objects we study are the formulas and proofs. We must therefore give a precise definition of what are these objects. The terms and formulas are the grammar of a language, oversimplified and calculated exactly to say what we want without ambiguity and without unnecessary detour.
	
	\subsubsection{Grammar}\label{grammar}
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] The "\NewTerm{terms}\index{terms}" designate items for which we want to prove some properties (we will discussed the latter much more in details further below):
		\begin{itemize}
			\item In algebra, the terms refer to the elements of a set (group, ring, field, vector space, etc.). We also manipulate sets of objects (subgroups, subrings, subfields, etc.). The terms which will designate the objects are named "\NewTerm{second-order terms}\index{second-order terms}".

			\item In analysis, the terms refer most of time to real numbers (for example, if we put ourselves in functional spaces) or functions.
		\end{itemize}
		
		\item[D2.] The "\NewTerm{formulas}\index{formulas}", are the properties of objects we study (we will discussed the latter also much more in details further below):
		\begin{itemize}
			\item In algebra, we can write formulas to express that two elements commute, that a subspace is of dimension $3$, etc.

			\item In analysis, we will write formulas to express the continuity of a function, the convergence of a sequence, etc.

			\item In set theory, formulas can express inclusion of two sets, membership of an element in a set, etc.
		\end{itemize}
		
		\item[D3.] The "\NewTerm{proof}\index{proof}", enable to check if a formula is true. The precise meaning of this word will also need to be defined. More precisely, they are deductions under assumptions, they allow to "lead from truth to truth", the question of the truth of the conclusion is then returned to that of the hypothesis, which does not look at the logic but is based on the knowledge we have on things we talk about.
	\end{enumerate}
	
	\pagebreak
	\subsubsection{Languages}
	In mathematics we use, depending on the area, different languages that are distinguished by the symbols used. The definition below simply expresses that it is sufficient to only have to give the list of symbols to specify the language.
	
	\textbf{Definition (\#\thesection.\mydef):} A "\NewTerm{language}\index{language}" is the content of a family (not necessarily finite) of symbols. We distinguish three kinds of languages: symbols, terms and formulas.
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} We use sometimes the word "vocabulary" or "signature" instead of the word "language".\\
	
	\textbf{R2.} We already know that the word "predicate" is used instead of the word "relation". We speak then of "predicate calculus" instead of "first-order logic").
	\end{tcolorbox}
	
	\paragraph{Symbols}\mbox{}\\\\
	There are different types of symbols we will try to define!
	
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] The "\NewTerm{constant symbols}\index{constant symbols}" (see note below)
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		The neutral element $n$ in Set Theory (\SeeChapter{see section Set Theory page \pageref{neutral element}})
		\end{tcolorbox}
	
		\item[D2.] The "\NewTerm{function symbols}\index{functions symbols}" or "\NewTerm{functors}\index{functors}". To each function symbol is assigned a strictly positive integer that we name the "\NewTerm{ary}": this is the number of arguments of the function arguments. If the arity is $1$ (resp. $2, \ldots, n$), we say then that the function is unary (resp binary., ..., $n$-ary).
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		The binary functor of multiplication $\times$ or $\cdot$ in group theory (\SeeChapter{see section Set Theory page \pageref{multiplication binary operator}}) is $2$-ary or more commonly a "binary" operator.
		\end{tcolorbox}
	
		\item[D3.] The "\NewTerm{relation symbol}\index{relation symbol}". Similarly to the previous definition, every relation symbol is associated with a positive or null integer (its arity) that corresponds to its number of arguments and we talk then of unary, binary, $n$-ary relation.
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		The relation $=$ is a binary relation (\SeeChapter{see section Operators page \pageref{equality}})
		\end{tcolorbox}
	
		\item[D4.] The "\NewTerm{individual variables}\index{individual variables}". In what will follow we will give us an infinite set $\mathcal{V}$ of variables. The variables will be recorded as it is traditional by the latin lowercase letters: $x$, $y$, $z$ (possibly indexed: $x_1,x_2,x_3$).
	
		\item[D5.] To this we should add the connectors $\neg, \Rightarrow, \vee,\wedge$ and quantifiers $\forall,\exists,\exists!$ that we extensively discussed above, on which it is now useless to return.
	\end{enumerate}
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} A constant symbol can be seen as a function symbol with $0$ argument (arity zero).\\
	
	\textbf{R2.} We consider (unless otherwise stated) that each language contains the binary relation symbol $=$ (read "equal") and the relation symbol with zero argument denoted $\perp$ (read "bottom" or "absurd/contradiction") represents the value FALSE and $\top$ (read "top or "tautology") represents the value TRUE. In the description of a language, we will often omit to mention them. The symbol $\perp$ is often redundant. We can indeed, without using it, write a formula that is always false. However, it can represent the FALSE in a canonical way and therefore gives the possibility to write general proof rules. \\
	
	\textbf{R3.} The role of functions and relations is very different. As we will see, the function symbols are used to construct the terms (of language objects) and the relation symbols to build formulas (the properties of these objects).
	\end{tcolorbox}
	
	\paragraph{Terms}\mbox{}\\\\
	The terms, we also say the "first order terms", are the objects associated with the language. 
	
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] Given $\mathcal{L}$ a language, the set $\mathcal{T}$ of terms on $\mathcal{L}$ is the smallest set containing the variables, the constants and stable (we do not go out of the set) by applying function symbols of $\mathcal{L}$ to the terms.

		\item[D2.] A "\NewTerm{closed term}\index{closed term}" is a term that does not contain variables (by extension, only constants).

		\item[D3.] For a more formal definition, we can write:
		
		where $t$ is a variable or constant symbol and, for any $k\in \mathbb{N}$:
		
		where $f$ is obviously a function of arity $n$ (let us recall that the arity is the number of function arguments). Thus, for each arity, there is a degree of set of terms. We have finally:
		

		\item[D4.] We name "\NewTerm{height}\index{height of a term}" of a term $t$ the smallest $k$ such that $t\in \mathcal{T}_k$.
		
		This definition means that variables and constants are terms and that if $f$ is a $n$-ary function symbol and $t_1,\ldots,t_n$ are terms then $f(t_1,\ldots,t_n)$ is also a term itself. The set $\mathcal{T}$ of terms is defined by the grammar:
		
		This expression must be read as follows: a element of the set $\mathcal{T}$ we are defining is either an element of $V$ (variable) or an element of $S_c$ (the set of symbols of constant), or the application of a $n$-ary function symbol $f\in S_f$ (constants or variable) of $\mathcal{T}$.
		
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Caution! The fact that $f$ is of the good arity is only implicit in this notation. Moreover, writing $S_f(\mathcal{T},\ldots,\mathcal{T})$ does not mean that all function arguments are identical, but simply that these arguments are elements of $\mathcal{T}$.
	\end{tcolorbox}
		
		
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		It is often convenient to see a term (expression) as a tree, where each node is labelled with a function symbol (operator or function) and each sheet by a variable or constant.
		\end{tcolorbox}
	\end{enumerate}
	In what follows, we will almost always define concepts (or prove results) "by recurrence\label{proof by recurrence}" on the structure or the size of a term.

	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] To prove a property $P$ on the terms, it suffices to prove $P$ for the variables and constants and to prove $P(f(t_1,\ldots,t_n))$ from $P(t_1),\ldots,P(t_n)$. We do then here a "\NewTerm{proof by induction on the height of a term}\index{proof by induction on the height of a term}". It is a technique that we will find in the following chapters.

		Mathematical induction as an inference rule can be formalized as a second-order axiom. The axiom of induction is, in logical symbols:
		
		In words, the basis $P(0)$ and the inductive step (namely, that the inductive hypothesis $P(k)$ implies $P(k + 1)$) together imply that $P(n)$ for any natural number $n$. The axiom of induction asserts that the validity of inferring that $P(n)$ holds for any natural number $n$ from the basis and the inductive step.
		
		Induction can be compared to falling dominoes: whenever one domino falls, the next one also falls. The first step, proving that $P(1)$ is true, starts the infinite chain reaction.
	\begin{center}
	\includegraphics[scale=0.3]{img/arithmetics/dominoes.jpg}
	\end{center}

		\item[D2.] To define a function $\Phi$ based on the terms, it is enough to define it on the variables and constants and tell how we get $\Phi(f(t_1,\ldots,t_n))$ from $\Phi(t_1),\ldots,\Phi(t_n)$. We do here again a "\NewTerm{definition by induction on the height of a term}\index{definition by induction on the height of a term}".
	\end{enumerate}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	The size (we also say the "length") of a term $t$ (size denoted $\tau(t)$) is the number of function symbols occurring in $t$. formally:
	\begin{itemize}
		\item $\tau(x)=\tau(c)=0$ if $x$ is a variable and $c$ is a constant.

		\item $\tau(f(t_1,\ldots,t_n))=1+\displaystyle\sum_{i\leq i\leq n}\tau(t_i)$
	\end{itemize}
	where the $1$ in the last relation represents the term $f$ itself.
	\end{tcolorbox}
	
	\paragraph{Formulas}\mbox{}\\\\
	\textbf{Definition (\#\thesection.\mydef):} A "\NewTerm{well-formed formula}\index{well-formed formula}" (WFF), often simply "\NewTerm{formula}" is a word (i.e. a finite sequence of symbols from a given alphabet) that is part of a formal language. A formal language can be considered to be identical to the set containing all and only its formulas.

	The formulas of propositional calculus, also named "\NewTerm{propositional formulas}\index{propositional formulas}", are expressions such as $(A \land (B \lor C))$.

	An atomic formula is a formula that contains no logical connectives nor quantifiers, or equivalently a formula that has no strict sub-formulas. The precise form of atomic formulas depends on the formal system under consideration; for propositional logic, for example, the atomic formulas are the propositional variables. For predicate logic, the atoms are predicate symbols together with their arguments, each argument being a term.
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,414); %set diagram left start at 0, and has height of 414
		
		%Rounded Rect [id:dp48241420596904483] 
		\draw  [line width=1.5]  (196,65.6) .. controls (196,47.6) and (210.6,33) .. (228.6,33) -- (381.9,33) .. controls (399.9,33) and (414.5,47.6) .. (414.5,65.6) -- (414.5,163.4) .. controls (414.5,181.4) and (399.9,196) .. (381.9,196) -- (228.6,196) .. controls (210.6,196) and (196,181.4) .. (196,163.4) -- cycle ;
		%Rounded Rect [id:dp6468462193430167] 
		\draw  [line width=1.5]  (203.5,107.8) .. controls (203.5,96.86) and (212.36,88) .. (223.3,88) -- (388.7,88) .. controls (399.64,88) and (408.5,96.86) .. (408.5,107.8) -- (408.5,167.2) .. controls (408.5,178.14) and (399.64,187) .. (388.7,187) -- (223.3,187) .. controls (212.36,187) and (203.5,178.14) .. (203.5,167.2) -- cycle ;
		%Rounded Rect [id:dp11543665858899321] 
		\draw  [line width=1.5]  (212.2,139.8) .. controls (212.2,134.39) and (216.59,130) .. (222,130) -- (387.9,130) .. controls (393.31,130) and (397.7,134.39) .. (397.7,139.8) -- (397.7,169.2) .. controls (397.7,174.61) and (393.31,179) .. (387.9,179) -- (222,179) .. controls (216.59,179) and (212.2,174.61) .. (212.2,169.2) -- cycle ;
		
		% Text Node
		\draw (245,38) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{90pt}\setlength\topsep{0pt}
		\begin{center}
		Symbols and\\strings of symbols
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (235,102) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{110pt}\setlength\topsep{0pt}
		\begin{center}
		Well-formed formulas
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (270,149) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{48.64pt}\setlength\topsep{0pt}
		\begin{center}
		Theorems
		\end{center}
		
		\end{minipage}};
		\end{tikzpicture}
	\end{figure}
	\pagebreak
	\textbf{Definition (\#\thesection.\mydef):} Formulas are built from "\NewTerm{atomic formulas}\index{atomic formulas}" using connectors and quantifiers. We will use the following connectors and quantifiers (which we already known):
	\begin{itemize}
		\item Unary negation connector: $\neg$
		
		\item Binary connectors of conjunction and disjunction and implication: $\wedge, \vee, \rightarrow$
		
		\item Quantifiers: $\exists$ which must be read "it exists" and $\forall$ that must be read "for all"
	\end{itemize}
	This notation of the connectors is almost (it should at least). It is used to avoid confusion between the formulas and the current language (metalanguage).
	
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] Given $\mathcal{L}$ a language, the "\NewTerm{atomic formulas}" of $\mathcal{L}$ are the formulas of the form $R(t_1,\ldots,t_n)$ where $R$ is an $n$-ary relation symbol of $\mathcal{L}$ of and $t_1,\ldots,t_n$ are terms of $\mathcal{L}$. We denote by "Atom" all atomic formulas. If we denote by $S_r$ the set of relation symbols, we can write all terms related between them by the expression:
		
		The set $\mathcal{F}$ of formulas of the first order logic of $\mathcal{L}$ is thus defined by the grammar (where $x$ is a variable):
		
		where should be read: the set of formulas is the smallest set containing formulas and such that if $F_1$ and $F_2$ are formulas then $F_1\vee F_2$, etc. are formulas and can be related to each other. 
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		The relations symbols of the propositional language are relations of $0$-arity (even the symbol "$=$" is absent), the quantifiers are then useless (since a propositional formula can not contain variables). We then obtain the propositional calculus $\mathcal{C}_P$ defined by:
		
		Note the presence of the symbol "bottom" $\bot$ meaning the "false" that we did not mention in our previous study of propositional logic.
		\end{tcolorbox}
		The reader must be careful to not to confuse terms and formulas. $\sin(x)$ is a term (function), $x=3$ is a formula. But $\sin(x)\wedge (x=3)$ is nothing: we can not, in fact, put a connector between a term and a formula (meaningless).
		\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		\textbf{R1.} To define a function $\Phi$ based on formulas, we simply need to define $\Phi$ on atomic formulas.\\

		\textbf{R2.} To prove a property $P$ on formulas, it suffices to prove $P$ for the atomic formulas.\\
		
		\textbf{R3.} To prove a property $P$ on the formulas, it is enough assume that the property holds for all formulas of size $p<n$ and to prove the property for formulas of size $n$.\\
		\end{tcolorbox}
		
		\item[D2.] A "\NewTerm{sub-formula}\index{sub-formula}" of a formula (or expression) $F$ is one of its components, verbatim a formula from which $F$ is built. Formally, we define the set $\mathrm{SF}(F)$ of the sub-formulas $F$ by:
		\begin{itemize}
			\item If $F$ is atomic: 
			
			\item If $F=F_1\oplus F_2$ (that is to say a composition) with $\oplus\in\{\vee,\wedge,\rightarrow\}$:
			
			\item If $F=\neg F$ or $\mathrm{Q}\in F_1$ with $\mathrm{Q}\in \{\forall,\exists\}$:
			
		\end{itemize}

		\item[D3.] A formula $F$ of $\mathcal{L}$ uses only a finite number of symbols of $\mathcal{L}$. This subset is named the "\NewTerm{language of the formula}\index{language of the formula}" and is denoted by $\mathcal{L}(F)$.

		\item[D4.] The "\NewTerm{size (or length) of a formula $F$}\index{size or length of a formula}", denoted by $\tau(F)$ is the number of connectors or quantifiers occurring in $F$. Formally:
		\begin{itemize}
			\item $\tau(F)=0$ if $F$ is an atomic formula

			\item $\tau(F_1\oplus F_2)=1+\tau(F_1)+\tau(F_2)$ where once again  $\oplus\in\{\vee,\wedge,\rightarrow\}$

			\item $\tau(\neg F_1)=\tau(\mathrm{Q}xF_1)=1\tau(F_1)$ with once again $\mathrm{Q}\in \{\forall,\exists\}$
		\end{itemize}

		\item[D5.] The "\NewTerm{main operator}\index{main operator}" (we also say the "main connector") of a formula is defined as:
		\begin{itemize}
			\item If $A$ is atomic, so it has then it has no main operator
	
			\item If $A=\neg B$, the $\neg$ is the main operator of $A$
	
			\item If $A=B\oplus C$ where once again  $\oplus\in\{\vee,\wedge,\rightarrow\}$ then $\oplus$ is the main operator of $A$
	
			\item If $A=\mathrm{Q}xB$ where once again $\mathrm{Q}\in \{\forall,\exists\}$, then $\mathrm{Q}$ is the main operator of $A$
		\end{itemize}

		\item[D6.] Given $F$ a formula. The set $\mathrm{FV}(F)$ of free variables of $F$ and the set $\mathrm{DV}(F)$ of dummies variables (or "linked variables") of $F$ are defined by induction on $\tau(F)$.
		
		An occurrence of a given variable is named "\NewTerm{linked variable}\index{linked variable}" or "\NewTerm{dummy variable}\index{dummy variable}" in a formula $F$ in a formula $F$, if in this formula a quantifier refers to it. Otherwise, we say we have a "\NewTerm{free variable}\index{free variable}".
		
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		An occurrence of a variable $x$ in a formula $F$ is a position of the variable in the formula $F$. Do not confuse with the object that is the variable itself!
		\end{tcolorbox}
		To clarify the possible free variables of a formula $F$, we write $F[x_1,\ldots,x_n]$. This means that free variables of $F$ are among the variables $x_1,\ldots,x_n$, verbatim if $y$ is free in $F$, then is one of the $x_i$ but the $x_i$ do not necessarily appear in $F$.
		
		We can define the dummy or free variables more formally:
		\begin{enumerate}
			\item If $F=R(t_1,\ldots,t_n)$ is atomic then $\mathrm{FV}(F)$ is the set of free variables appearing in the $t_i$ and we have then for dummy variables $\mathrm{DV}(F)=\varnothing$.
	
			\item If $F=F_1\oplus F_2$ where $\oplus \in\{\vee,\wedge,\rightarrow\}:\mathrm{FV}(F)=\mathrm{FV}(F_1)\cup \mathrm{FV}(F_2)$ then $\mathrm{DV}(F)=\mathrm{DV}(F_1)\cup \mathrm{DV}(F_2)$.
	
			\item If $F=\neg F_1$ then $\mathrm{FV}(F)=\mathrm{FV}(F_1)$ and $\mathrm{DV}(F)=\mathrm{DV}(F_1)$.
	
			\item If $F=\mathrm{Q}xF_1$ with $\mathrm{Q}\in\{\forall,\exists\}:\mathrm{FV}(F)=\mathrm{FV}(F_1)-\{x\}$ and $\mathrm{DV}(F)=\mathrm{DV}(F_1)\cup\{x\}$.
		\end{enumerate}
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Examples:}\\\\
		E1. Given $F$: $\forall x\;(x\cdot y=y\cdot x)$ then $\mathrm{FV}(F)=\{y\}$ and $\mathrm{DV}(F)=\{x\}$\\
		
		E2. Given $G$: $\{\forall x\exists y(x\cdot z=z\cdot y)\}\wedge \{x=z\cdot z\}$ then $\mathrm{FV}(G)=\{x,z\}$ and $\mathrm{DV}(G)=\{x,y\}$.
		\end{tcolorbox}
		
		\item[D7.] We say that formulas $F$ and $G$ are "\NewTerm{$\alpha$-equivalent}\index{$\alpha$-equivalent formulas}" if they are (syntactically) identical only after the renaming of their related variables.

		\item[D8.] A "\NewTerm{closed formula}\index{closed formulas}" is a formula without free variables.

		\item[D9.] Given $F$ a formula, $x$ a variable and $t$ a term. $F[x:=t]$ is the formula obtained by replacing in $F$ all free occurrences of $x$ by $t$, after possible renaming of linked occurrences in $F$ which are free in $t$.

	\end{enumerate}
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} We will notice in the examples seen previously that a variable can have both free occurrences and linked occurrences. So we do not always have $\mathrm{FV}(F)\cap \mathrm{DV}(F)=\varnothing$.\\
	
	\textbf{R2.} We can not rename $y$ into $x$ in the formula $\forall y\;(x\cdot y=y\cdot x)$ of the previous example and get the formula $\forall x\;(x\cdot x=x\cdot x)$: the variable $x$ would be then "captured". We therefore can not rename variables without precautions: we must avoid capture of free occurrences.
	\end{tcolorbox}
	
	\subsection{Proofs}
	The proofs that we found in mathematical books or theoretical physics books are assemblies of mathematical symbols and sentences containing keywords such as: "So", "because", "if", "if and only if", "it is necessary", "just", "take an $x$ such that", "therefore", "assume", "seek a contradiction", etc. These words are assumed to be understood by all in the same way, which is in fact, not always the case.
	
	In any work, the purpose of a proof is to convince the reader of the truth of a statement by show him the intellectual path that gives him the possibility to control himself the truth and rigour of the statement. Depending on the level of the reader, this proof will be more or less detailed or formal: something that can be considered obvious in a graduate course may not be in a undergraduate level course.
	
	In a homework, the corrector know that the result given by the student is (normally) true and he knows the proof of it. The student must prove (correctly) the required result. The level of detail that the student must give will depends sometimes on the confidence possessed by the corrector: in a good copy, a "proof by an evident recurrence" will be accepted, while a copy where there previously had an "obvious" which was ... obviously false, will not pass!
	
	To properly manage the level of details, we should know what is a complete proof. This work of formalization has been done at the beginning of the 120th century only (holocene calendar)!!
	
	Several things may seem surprising:
	\begin{enumerate}
		\item There is only a finite number of rules: two for each of the connectors (and the equality) more three general rules. It was not at all clear before that a priori a finite number of rules were enough to prove all that is true. We will show this result (this is essentially the Completeness Theorem). The proof is not trivial at all.
	
		\item These are the same rules for all the mathematics and physics: algebra, analysis, geometry, etc. This means that we have managed to isolate what is general in reasoning!!! We will see later that a proof is an assembly of pairs $(\Gamma, A)$, where $\Gamma$ is a set of formulas (the assumptions) and $A$ a formula (the conclusion). When we do the arithmetic, geometry or real analysis, we use, in addition to rules, assumptions that are named as we know "axioms". These express the particular properties of objects that we manipulate (for details on the concept of axioms see the section Introduction section page \pageref{axiom}).
	\end{enumerate}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	It is important to study the abstract theory of Proofs, even if the "Münchhausen trilemma" (\SeeChapter{see section Principia page \pageref{Münchhausen trilemma}}) seems to give evidence in a rhetoric way (with all the weaknesses associated to this art...) the impossibility of proving any truth in the fields of logics and mathematics (keep in mind however that pure sciences are not similar to experimental sciences!).
	\end{tcolorbox}
	
	We prove therefore, in general, formulas using a set of assumptions, and this set can vary during the proof: when we say "suppose $F$ and let us prove $G$", $F$ is then a new hypothesis that we can use to prove $G$. to formalize this, we introduce the concept of "sequent".
	
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] A "\NewTerm{sequent}\index{sequent}" is a pair $\Gamma\vdash F$ where:
		\begin{enumerate}
			\item $\Gamma$ is a finite set of formulas that represents the assumptions that we can use. This set is also named the "\NewTerm{context of the sequent}\index{context of the sequent}".
	
			\item $F$ is a formula. This is the formula we want to prove. We say that this formula is the "\NewTerm{conclusion of the sequent}\index{conclusion of a sequent}".
		\end{enumerate}
		The sign "$\vdash$" must be read "\NewTerm{thesis}\index{symbol!thesis}" or "\NewTerm{prove that}\index{symbol!prove that}".
		\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		\textbf{R1.} If $\Gamma=\{A_1,\ldots,A_n\}$ we can then write $A_1,\ldots,A_n \vdash F$ instead of $\Gamma \vdash F$. \\
		
		\textbf{R2.} We write $\vdash F$ a sequent whose set of assumptions is empty and $\Gamma_1,\ldots,\Gamma_n \vdash F$ a sequent whose set of assumptions is $\bigcup_i \Gamma_i$.\\
		
		\textbf{R3.} We write $\Gamma \not\vdash F$ to say that "$F$ is not provable".
		\end{tcolorbox}
		
		\item[D2.] A sequent $\Gamma\vdash F$ is said to be "\NewTerm{provable}\index{provable}" (or \NewTerm{demonstrable}) if it can be obtained by a finite application of rules. A formula $F$ is provable if the sequent $\vdash F$ is provable.
	\end{enumerate}

	\subsubsection{Rules of Proofs}
	Proofs rules are the bricks used to build demonstration steps. A formal demonstration is a finite (and correct!) assembly of rules. This assembly is not linear (not a suite) but a "tree." Indeed, we are often forced to make connections.

	We will present a choice of rules. We could have introduced other (instead of or in addition) that would give the same notion of provability. Those that have been chosen are "natural" and correspond to the arguments that we usually made in mathematics. In the common practice we use, in addition to the rules below, many other rules, but these can be deduced from previous ones. We name them "\NewTerm{derived rules}\index{derived rules}".
	
	It is traditional to write the root of the tree (the sequent conclusion) at the bottom, the leaves at the top: the nature is done as this... As it is also tradition to write on a sheet of paper, from up to down, it would not be unreasonable to write the root at the top and the leaves at the bottom. We must make a choice!
	
	A rule consists of:
	\begin{itemize}
		\item a set of "premises" each is a sequent. There may be zero, one or more of them.

		\item the conclusion sequent of the rule

		\item a horizontal bar separating the premises (top) from the conclusion (bottom). On the right of the bar, we will indicate the name of the rule.
	\end{itemize}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us consider:
	
	This rule has two premises ($\Gamma\vdash A \rightarrow$ and $\Gamma\vdash A$) and a conclusion ($\Gamma\vdash B$) and is denoted in an abbreviated under the form $\rightarrow_e$. It can be read in two ways:
	\begin{itemize}
		\item from bottom to top: if we want to prove the conclusion, it suffices by using the rule to prove the premises. This is what we do when we seek a proof. This corresponds to the "analysis".

		\item from top to bottom: if we proved the premises, so we also proved the conclusion. This is what we do when we write a proof. This corresponds to the "synthesis".
	\end{itemize}
	\end{tcolorbox}
	For the proofs there is a finite number of 17 rules in number that we will define below:
	\begin{enumerate}
		\item Axiom:
		
		From bottom to top: if the conclusion of the sequent is one of the hypothesis, then the sequent is provable.
		
		\item Weakening:
		
		Explanations:
		\begin{itemize}
		\item From top to bottom: if we prove $A$ under the assumptions $\Gamma$, adding other hypotheses can still prove $A$.

		\item From bottom to top: there are assumptions that may not serve
		\end{itemize}

		\item Introduction of implication:
		
		From bottom to top: to prove that $A \rightarrow B$ we assume $A$ (that is to say, we add it to the assumptions) and we prove $B$.	
		
		\item Elimination of implication:
		
		From bottom to top: to prove $B$, if we know a theorem of the form $A\rightarrow B$ and if we can prove the lemma $A\rightarrow B$, it suffices to prove $A$.
		
		\item Introduction to the conjunction:
		
		From bottom to top: to prove $A\wedge B$, it suffices to prove $A$ and prove $B$.
		
		\item Elimination of the conjunction:
		
		From top to bottom: from $A\wedge B$, we can deduce $A$ (left elimination) and $B$ (right elimination).
		
		\item Introduction to the disjunction:
		
		From bottom to top: to prove $A\vee B$, it suffices to prove $A$ (left disjunction) or prove $B$ (right disjunction).
		
		\item Elimination of the disjunction:
		
		From bottom to top: if we want to prove $C$ and we know we have $A\vdash B$, it is enough to prove in first time by assuming $A$, and in a second time by assuming $B$. This is a case-based reasoning.
		
		\item Introduction of the negation:
		
		From bottom to top: to show $\neg A$, we assume $A$ and we prove the absurd ($\perp$).
		
		\item Elimination of the negation:
		
		From top to bottom: if we proved $\neg A$ and $A$, then we proved by absurdity ($\perp$).

		\item Classic absurdity (reductio ad absurdum):
		
		From bottom to top: to prove $A$, it suffices to prove the absurdity by assuming $\neg A$.
		
		This rule is equivalent to say: $A$ is true if and only if it is false that $A$ is false. This rule is not obvious: it is necessary to prove some results (there are results we can not prove if we do not have this rule). Contrary to many others, this rule may also be applied at any time. We can, in fact, always say: To prove $A$, I suppose $\neg A$ and I will seek for a contradiction.
		
		\item Introduction of the universal quantifier:
		
		From bottom to top: to prove $\forall x\; A$, it suffices to show $A$ doing no assumption about $x$.
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		For proofs this check (no assumption on $x$) is often a source of error.
		\end{tcolorbox}
		
		\item Elimination of the universal quantifier:
		
		From top to bottom: from $\forall x\; A$, we can deduce $A[x:0t]$ for any term $t$. What we can also say under the form: if we proved $A$ for all $x$, then we can use $A$ with any object $t$ (!!).
		
		\item Introduction of the existential quantifier:
		
		From bottom to top: to prove $\exists x\; A$, it suffices to find an object (verbatim a term $t$) for which we know how to prove $A[x:=t]$.
		
		\item Elimination of the existential quantifier:
		
		From bottom to top: we prove that there is indeed a set of assumptions such that $\exists x \; A$ and hence this result as new hypothesis, we prove $C$. This formula $C$ inherits then from the formula $\exists x \; A$  and therefore $x$ is not free in $C$ because it already was not in $\exists x \; A$.
		
		\item Introduction of equality:
		
		From bottom to top: we can always prove $t=t$. This rule means that equality is reflexive (\SeeChapter{see section Operators page \pageref{reflexive}}).
		
		\item Elimination of equality:
		
		From top to bottom: if we prove that $\Gamma\vdash A[x:=t]$ and $t = u$, then we have proved $A[x:=u]$. This rule expresses that equal objects have the same properties. We notice however that the formulas (or relations) $t =u$ and  $u = t$ are not, formally, identical. We will have to prove that equality is symmetric (we will benefit also to prove on the way that equality is transitive).
	\end{enumerate}
	Let us see now three examples by introducing them in the form of theorems as it should be in proof theory!

	\begin{theorem}
	The equality is symmetric (a little bit not trivial but quite good to begin with the subject):
	\end{theorem}
	\begin{dem}
	
	From top to bottom: we introduce the equality $=_i$ and prove from the assumption $x_1=x_2$ the formula $x_1=x_1$. At the same time, we define the axiom as what $x_1=x_2$. Then from these premises, we eliminate the equality $=_e$ by substituting the terms so that from the assumption $x_1=x_2$ (from the axiom) we get $x_2=x_1$. Then, the elimination of equality automatically implies without assumption that $x_1=x_2\rightarrow x_2=x-1$. Therefore, we simply insert the universal quantifier for each variable (i.e. twice) without any assumption to achieve that equality is symmetric.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\begin{theorem}
	The equality is transitive (that is to say if $x_1=x_2$ and $x_2=x_3$ then $x_1=x_3$). By denoting $F$ the formula $(x_1=x_2)\wedge (x_2=x_3)$:
	\end{theorem}

	\begin{dem}
	
	What do we do here? We first introduce the formula $F$ twice as axiom to "dissect" it latter left and right (we do not introduce the equality supposed already introduced as a rule). Once done, we eliminate on the left and on the right the conjunction on the formula to work on left and right terms only and we introduce the equality of the two terms which makes that from the formula we have the transitive equality. It follows without any assumption that automatically implies that equality is transitive and finally we say that this is valid for any value of the different variables (if the formula is true, then equality is transitive).
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	And now last big example always in the form of a theorem:
	\begin{theorem}
	Any involution is a bijection (\SeeChapter{see section Set Theory page \pageref{bijection}}).\label{proof involution is a bijection}
	\end{theorem}

	\begin{dem}
	Let $f$ be a unary function symbol (with one variable), we write (for the details see the section of Set Theory page \pageref{functions and applications}):
	\begin{itemize}
		\item $\text{Inj}[f]$ the formula:
		
		which means that $f$ is injective.
		
		\item $\text{Surj}[f]$ the formula:
		
		
		\item  $\text{Bij}[f]$ the formula:
		
		
		\item $\text{Inv}[f]$ the formula:
		
	\end{itemize}
	which means that $f$ is an involution (we also write this $f\circ f=\text{Id}$\label{identity application} that this is to say that the composition of $f$ is the identity).
	
	We would like to know if:\\
	
	
	We will present (trying this to be done as easy as possible) this proof in four (!!!) different ways: traditional (informal), classic (pseudo-formal), formal in tree and formal in-line.
	
	\begin{itemize}
		\item \textbf{Traditional method (informal):}
		
		We must prove that if $f$ is involutive then it is bijective. So we have two things to prove (and both must be satisfied simultaneously): the function is injective and surjective.
		
		\begin{enumerate}
			\item So we prove first that involution is injective. 
			
			We assume for this, because $f$ is an involution it therefore injective, such that:
			
			implies that:
			
			However, this assumption automatically comes from the definition of involution that:
			
			and to the application of $f$ to the relation:
			
			(thus three equalities so far) such that:
			
			we therefore have:
			
			
			\item Let us prove that involution is surjective. 
			
			If it is surjective, then we must have:
			
			But, let us define the variable $x$ by definition of the involution itself:
			
			as $y=f(x)$..., and a change of variable after we get:
			
			and therefore surjectivity is ensured.
			\end{enumerate}
			
			\item \textbf{Pseudo-formal method:}
			
			We take again the same and we inject in it the rules of proof theory:
			
			We must show that $f$ is involutive and therefore bijective. So we have two things to show ($\wedge_i$) (and both must be satisfied simultaneously): that the function injective and surjective:
			
			\begin{enumerate}
			\item Let us first prove that involution is injective. We assume for this, since $f$ is involutive and therefore injective, that:
			
			implies:
			
			However, this assumption automatically comes from the definition of involution that:
			
			and from the application of $f$ to the relation:
			
			(therefore three equalities $=_e \times 2$) such that:
			
			We therefore have:
			
			
			\item Let us prove that involution is surjective. If it is surjective, then we must have:
			
			Now, we define the variable $x$ by definition of the involution itself:
			
			since $y=f(x)$...., after a change of variables we get:
			
			and therefore:
			
			surjectivity is assured.
		\end{enumerate}
		\item \textbf{Formal tree method:}
			
		Let us do this now with the graphical method that we have presented above.
		\begin{enumerate}
			\item Let us prove that involution is injective:
			
			For this we must prove first that:
			
			Therefore:
			
			
			That bring us to write:
			
			\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
			The latter relation is abbreviated $=_c$ and named (as other existing) "derived rule" because it is an argument that is often made during proofs and a little time consuming to develop each time ...
			\end{tcolorbox}
			Therefore:
			
			
			\item Let us prove now that involution is surjective:
			
			It follows:
			
		\end{enumerate}
		
		\item \textbf{Formal in-line method:}
		
		We can do the same thing in a slightly less... wide form ... and more ... tabbed (it is no less indigestible):
		\begin{subequations}
		\begin{align}
			&\text{Inv}[f]\vdash \text{Bij}[f]&& \vee i\\
			&\quad 1:=\text{Inv}[f]\vdash \text{Inj}[f]&&\nonumber\\
			&\quad 2:=\text{Inv}[f]\vdash \text{Surj}[f]&&\nonumber\\[5pt]
			&(1)\text{Inv}[f]\vdash \text{Inj}[f]&& \forall i\\
			&\quad\text{Inv}[f]\vdash f(x)=f(y)\rightarrow x=y &&\rightarrow_i\nonumber\\
			&\quad\text{Inv}[f], f(x)=f(y)\vdash x=y&&=_e\times 2 (i)(ii)(iii)\nonumber\\
			&\quad(i)\quad\text{Inv}[f]\vdash f(f(x))=x &&\forall_e\nonumber\\
			&\quad\quad\quad\text{Inv}[f]\vdash \text{Inv}[f] &&\text{ax}\nonumber\\
			&\quad(ii)\quad\text{Inv}[f]\vdash f(f(y))=y &&\forall_e\nonumber\\
			&\quad\quad\quad\text{Inv}[f]\vdash \text{Inv}[f] &&\text{ax}\nonumber\\
			&\quad(iii)\quad f(x)=f(y)\vdash f(f(x))=f(f(y))&&=_c\nonumber\\
			&\quad\quad(1')f(x)=f(y)\vdash f(x)=f(y) &&\text{ax}\nonumber\\[5pt]
			&(2)\text{Inv}[f]\vdash \text{Surj}[f]&& \forall i\\
			&\quad\text{Inv}[f]\vdash \exists x \{f(x)=y\}&&\exists_i\nonumber\\
			&\quad\text{Inv}[f]\vdash \{f(x)=y\}[x:=f(y)]&&\forall_e\nonumber\\
			&\quad\text{Inv}[f]\vdash \forall x \{f(f(x))=x\} &&\text{ax}\nonumber
			\end{align}
		\end{subequations}
	\end{itemize}

	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The reader interested in going further in the topic can read the three famous volumes of \textit{Principia Mathematica} of Whitehead and Russel \cite{whitehead1910principia} (considered as the "bible" in Proof Theory by many specialists) that have an approximate total of 1900 pages where the first 380 pages of the first volume are dedicated to the proof that $1+1=2$...
	\end{tcolorbox}
	
	However... all this highly technical formalism don't make it always obvious to find where is the error in the following "pseudo-proof":
	
	Assume:
	
	Let us multiply both side by $a$:
	
	Let us subtract by $b^2$:
	
	We factorize:
	
	We divide by $(a-b)$:
	
	Since $a=b$, we have:
	
	Therefore:
	
	If you don't see the error, here is the analysis:
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|c|}{\cellcolor[gray]{0.75}\textbf{Written}} & \multicolumn{1}{c|}{\cellcolor[gray]{0.75}\textbf{Reality}} & \textbf{Judgement} \\ \hline
		$a=b$ & $a=a$ & True \\ \hline
		$a^2=ab$ & $a^2=aa$ & True \\ \hline
		$a^2-b^2=ab-b^2$ & $a^2-a^2=aa-a^2$ & True ($=0$) \\ \hline
		$(a+b)(a-b)=b(a-b)$ & $(a+a)(a-a)=a(a-a)$ & True ($=0$) \\ \hline
		$a+b=b$ & $a+a=a$ & \begin{tabular}[c]{@{}c@{}}False (unless $a=0$)\\ and forbidden as dividing by $a-b$ \\ is equivalent as dividing by $0$\end{tabular} \\ \hline
		$b+b=b$ & $a+a=a$ & False \\ \hline
		$2b=b$ & $2a=a$ & False \\ \hline
		$2=1$ & $2=1$ & False \\ \hline
		\end{tabular}
	\end{table}
	Obviously there are a lot of other kind of proofs as illustrated in the figure below... \Winkey :
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/proof.jpg}
	\end{figure}
	
	\subsection{Laws of thought}\label{laws of thought}
	The laws of thought are fundamental axiomatic rules upon which rational (or philosophical) discourse itself is often considered to be based. The formulation and clarification of such rules have a long tradition in the history of philosophy and logic. Generally they are taken as laws that guide and underlie everyone's thinking, thoughts, expressions, discussions, etc. However, such classical ideas are often questioned or rejected in more recent developments, such as intuitionistic logic, dialetheism and fuzzy logic.
	
	 \textbf{Definition (\#\mydef):}  The "\NewTerm{laws of thought}\index{laws of thought}" are axioms by which - or in accordance with which - valid thought proceeds, or that justify valid inference, or to which all valid rhetorical (i.e. semantic) deductions are reducible and not necessarily connected to evidence based science. Laws of thought are rules that apply without exception to any subject matter of thought, etc. however they can be applied \underline{only to sentences}, not to objects (!); sometimes they are said to be the "object of logic".
 	
	 \begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt,breakable]
	\bcbombe Caution! As already mentioned, as any axiomatic system, the laws of thought are submitted to the both Gödel’s incompleteness theorems (proven in year 11931 - holocene calendar) that are for recall among the most important results in modern logic, and have deep implications! They concern the limits of provability in formal axiomatic theories. The first \textbf{incompleteness} theorem states that \underline{in any} consistent formal system $F$ within which a certain amount of arithmetic can be carried out, there are statements of the language of $F$ which can neither be proved nor disproved in $F$. Formally:
	$$F \vdash \neg \mathrm{Provable}_F\left(\left\ulcorner G_F\right\urcorner\right)$$	
	 According to the second incompleteness theorem, such a formal system cannot prove that the system itself is \textbf{consistent} (assuming it is indeed consistent). Formally:
	 $$F \nvdash\mathrm{Cons} \equiv \neg \mathrm{Provable}_F\left(0=0^{\prime}\right)$$
	 Furthermore a proof in a given axiomatic system isn't necessarily true in another one!\\
	
	 In other words Gödel’s theorems does not disprove anything in mathematics. Rather, it proves that certain truths in a given axiomatic framework exist which cannot be proved using currently accepted framework or can be disproved in another axiomatic framework.\\
	 
	 Or again said otherwise (because some people have difficulties to understand), the theorem doesn't undermine logic and doesn't destroy the fabric of thought. It just says that certain axiomatic systems, satisfying various conditions, aren't complete, which means that certain statements can neither be proved nor disproved by those systems. Those very same "undecidable" statements may well be proved or disproved in other axiomatic systems.
	\end{tcolorbox}
	
	 \begin{fquote}[Bertrand Russell] Many logicians have been driven to the conclusion that there are unreal objects. [...] we can make true propositions of which these are the subjects; hence they must have some kind of logical being, since otherwise the propositions in which they occur would be meaningless. [...] Logic, I should maintain, must no more admit a unicorn than zoology can; for logic is concerned with the real world just as truly as zoology [...]
 	\end{fquote}
 	
 	Surprised? People usually come to hold, through their lack of any formal graduate or postgraduate training in logic, that logic is far more than it really is. Therefore, a further understanding of just what logic is, can be enhanced by delineating it from what it is not:
 	\begin{itemize}
 		\item Logic is not the study of what, if anything, comprises the "groundness of being for the universe" - That's metaphysics!
 		
 		\item Logic is not a set of laws that governs the universe - That's physics!
 		
 		\item Logic is not an immaterial "entity" that transcends reality - Such discussions belong to the realm of theology!
 		
 		\item Logic is not a method for "studying the world" - That's evidence based science!
 		
 		\item Logic is not the method for assessing axioms - That's a matter of pure reason!
 		
 		\item Logic is not a way of evaluating "truth" - That's philosophy!
 		
 		\item Logic is not a set of laws that governs human behavior - That's psychology!
 		
 		\item Logic is not even a study of how people reason - Fortunately there is more to human reason than just logic...
 	\end{itemize}
 	
 	\begin{tcolorbox}[enhanced,title=Remark,colframe=black,arc=10pt,drop lifted shadow,after skip=15pt plus 2pt]
	The reader muser also remember the Münchhausen trilemma (see page \pageref{Münchhausen trilemma}) that is a thought experiment to demonstrate the theoretical impossibility of proving any truth, even in the fields of logic and mathematics, without appealing to accepted assumptions like:  1) The circular argument, in which the proof of some proposition presupposes the truth of that very proposition 2) The regressive argument, in which each proof requires a further proof, ad infinitum 3) The dogmatic argument, which rests on accepted precepts which are merely asserted rather than defended.
	\end{tcolorbox}
	
	 These warnings (reminders) having been made, let us return to the laws of thought (that are in the framework of logic "tautologies"):
	 \begin{itemize}
	 	\item[A1.]  \textbf{The law of identity (ID):} everything is (i.e., is identical to) itself, or Whatever is, is (\textit{Metaphysics IV (Gamma)}, 3, Aristotle, year 9650 according to holocene calendar):
	 	
	 	
	 	\item[A2.]  \textbf{The law of contradiction (or non-contradiction; NC):} no thing having a given quality also has the negative of that quality (e.g., no even number is non-even), or Nothing can both be and not be (\textit{Metaphysics IV (Gamma)}, 3, Aristotle, year 9650 according to holocene calendar):
	 	
	 	
	 	\item[A3.]  \textbf{The law of excluded middle (EM):}  every thing either has a given quality or has the negative of that quality (e.g., every number is either even or non-even), or Everything must either be or not be (\textit{Metaphysics IV (Gamma)}, 6, Aristotle, year 9650 according to holocene calendar):
	 	
	 	While the law of non-contradiction tells us that no statement can be both true and false, the law of excluded middle tell us that they must all be one or the other.
	 	
	 	 \begin{tcolorbox}[enhanced,title=Remark,colframe=black,arc=10pt,drop lifted shadow,after skip=15pt plus 2pt]
		The law of excluded middle is removed in "intuitionistic logic" (also in "fuzzy logic"), sometimes more generally named "constructive logic", as at the opposite of "classical logic" it does not focus so much on whether statements are "really" true or false, in that sense - it focuses on whether we \underline{know} a statement is true, or \underline{know} it is false!  So, before someone working in constructive mathematics can assert a statement, he needs to have a concrete justification for the truth of the statement. Arguments such as proof by contradiction are not permitted!\\
		
		One kind of example of how excluded middle fails in this framework was named a "weak counterexample" by L. E. J. Brouwer (regarded as one of the greatest mathematicians of the 120th century according to holocene calendar). Here is one: Consider the number $\pi$. As a fact, even in classical mathematics we do not currently know whether there are infinitely many occurrences of the digit $5$ in the decimal expansion of $\pi$. Let $A$ be the statement «\textit{there are infinitely many occurrences of $5$ in the decimal expansion of $\pi$ }». Then someone working in constructive mathematics cannot assert «\textit{$A$ or not $A$}», because she does not know which of the two options holds. Of course «\textit{$A$ or not $A$}» is an instance of the law of the excluded middle, and is valid in classical mathematics.
		\end{tcolorbox}
	 \end{itemize}
	 More technically the three axioms (laws) above can be denoted:
	 

	Some people add a fourth law (axiom):
	\begin{enumerate}
		\item[A4.] \textbf{Sufficient Reason (SR):} Truth is the reference of a judgement to something outside it as its sufficient reason or ground. Of everything that is, it can be found why it is:
		
	\end{enumerate}
	Therefore the four axioms of thought (law) can be written as:
	
	The expression "laws of thought" gained added prominence through its use by George Boole (11815–11864) to denote theorems of his "algebra of logic"; in fact, he named his second logic book\textit{ An Investigation of the Laws of Thought on Which are Founded the Mathematical Theories of Logic and Probabilities} (11854). Modern logicians, in almost unanimous disagreement with Boole, take this expression to be a misnomer; none of the above propositions classed under "laws of thought" are explicitly about thought per se, a mental phenomenon studied by psychology, nor do they involve explicit reference to a thinker or knower as would be the case in pragmatics or in epistemology. The distinction between psychology (as a study of mental phenomena) and logic (as a study of valid inference) is widely accepted.
	
	 The reader must remember, as we have already mentioned it earlier (!), that logic isn't connected to evidence based science but only to sentences (rhetorical art, ie semantic) as already stated earlier in the definition! For example some people claim that the law of non-contradiction is violated by:
	 \begin{itemize}
	 	\item The photon has the property of being at the same time a particle and it's own opposite\footnote{Consider an annihilation reaction of some massive particle-antiparticle pair. As we know there is a good probability that this will produce a pair of photons. The inverse reaction will destroy the photons and produce a massive particle antiparticle pair. This proves that a photon with spin $+S$ is the antiparticle of a photon with spin $-S$ and the same frequency.} (anti-particle)
	 	
	 	\item There is a non-zero probability of a particle to be at multiple positions at the same time before its measurement (collapse of the wave function)
	 	
	 	\item The Klein bottle that has it's external surface that is at the same time its own opposite surface (the Möbius strip has the same property as the Klein bottle)
	 	
	 	\item The sphere that is endless and boundless is at the same time not infinite in size (hence its finite and infinite at the same time)
	 	
	 	\item The segment that has bounds and finite size contains at the same time infinitely many points (hence its finite and infinite at the same time)
	 	
	 	\item The Gabriel's horn that has infinite surface area but at the same time an finite volume
	 	
	 	\item Many mathematical series have infinite terms but at the same time finite value
	 	
	 	\item The number $0$ is an even number or odd number or both at the same?
	 	
	 	\item The circle is at the same time a square under a given topological transformation in advanced mathematics (for the same reason a cup of coffee is topologically a torus)
	 	
	 	\item At the quantum level, cause may occur after effect (retrocausality)
	 \end{itemize}
	 We see with these toy examples above that the three axioms of classical logic (i.e. laws of thoughts) are not always enough to have a rhetorical debate on the real world as modern science knows it!
	 
	 \begin{tcolorbox}[enhanced,title=Remark,colframe=black,arc=10pt,drop lifted shadow,after skip=15pt plus 2pt]
	The problem with Classic Logic is that some logicians claim that something that is "opposite" in real life does not make the difference logically necessary........!
	\end{tcolorbox}
	 
	 However, none of the examples violates the law of non-contradiction! The trivial reason is that the law of non-contradiction refers \underline{to the truth value of propositions} \underline{(sentences) but not to objects or entities} !
	 
	 \begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/arithmetics/science_greater_than_logic.jpg}
	\end{figure}
	 
	\begin{tcolorbox}[enhanced,title=Remark,colframe=black,arc=10pt,drop lifted shadow,after skip=15pt plus 2pt]
	Garrett Birkhoff and John Von Neuman proposed in the 11930s (holocene calendar) that the paradoxes of Quantum Mechanics can be explained if we abandoned classical logic and used some form of Quantum Logic instead (Birkhoff, Garrett; von Neumann, John. "\textit{The Logic of Quantum Mechanics}". Ann. Math. 37 (4): 823–843.). Such a Quantum Logic would change or abandon all together some of the rules of classical logic, and would be a perfect case of logical axioms arrived at by observation.
	\end{tcolorbox}
	 \textbf{Definitions (\#\thesection.\mydef):}
	 \begin{itemize}
	 	\item[D1.] A system is said to be "\NewTerm{consistent}" if we cannot prove both a statement and its opposite. We cannot prove something like $1=2$.
	
		\item[D2.] A system is said to be "\NewTerm{complete}" if every true statement can be proven. Every assertion can be either proved or disproved.
		
		\item[D3.] A system is said to be "\NewTerm{decidable}" if for every type of problem, there is a procedure that, in theory, will give a solution.
	 \end{itemize}
	 Here is a table that summarize some well known result of Gödel's theorems:
	 \begin{table}[H]
		\centering
		 \begin{tabular}{|c|c|c|c|}
			\rowcolor[gray]{0.75} \hline \textbf{Theory} & \textbf{Is it Consistent?} & \textbf{Is it Complete?} & \textbf{Is it Decidable?} \\
			\hline \begin{tabular}{c} 
			Propositional \\
			calculus
			\end{tabular} & Yes & Yes & Yes \\
			\hline \begin{tabular}{c} 
			Euclidean \\
			geometry
			\end{tabular} & Yes & Yes & Yes \\
			\hline $1^{\text {st }}$ order logic & Yes & Yes & No \\
			\hline \begin{tabular}{c} 
			Arithmetic \\
			$(+,-)$ only
			\end{tabular} & Yes & Yes & Yes \\
			\hline \begin{tabular}{c} 
			Arithmetic \\
			in full $(+,-, \times,\div)$
			\end{tabular} & $? ?$ & No & No \\
			\hline
		\end{tabular}
		\caption{Consistency, Completeness, Decidability of common axiomatic systems}
	\end{table}
	The fact that first-order logic is undecidable has significant implications for computer science, mathematics, and philosophy! In philosophy:
	\begin{itemize}
		\item The undecidability of first-order logic has implications for the nature of truth and knowledge. It suggests that there are certain truths that are beyond the reach of human reason.
		
		\item It also has implications for the nature of logic itself. It suggests that logic is not a complete and perfect system for reasoning about the world.
	\end{itemize}
	 The table above known thanks to Kurt Gödel is famous for the following corollaries:
	 \begin{enumerate}
	 	\item[T1.] Any formal system able to do elementary arithmetic is either inconsistent or incomplete
	 	
	 	\item[T2.] Any formal system able to express its own consistency can prove its own consistency if and only if it is inconsistent
	 \end{enumerate}
	The reader must remember that just because something is logical does not mean that the conclusion is true in the framework of evidenced based science as logic is a rhetorical (i.e. semantic) axiomatic system disconnected from reality! Especially in logic we can use unsupported premises or assumption or even worst, premises or assumption that are evidently shown wrong by evidence based science!
	
	Gödel's ontological proof (logical proof that a deity exist) is such a famous example:
	\begin{table}[H]
		\centering
		\begin{tabular}{ll}
		Ax. 1. & $(P(\varphi) \wedge \square \forall x(\varphi(x) \Rightarrow \psi(x))) \Rightarrow P(\psi)$  \\
		Ax. 2. & $P(\neg \varphi) \Leftrightarrow \neg P(\varphi)$  \\
		Th. 1. & $P(\varphi) \Rightarrow \diamond \exists x \varphi(x)$  \\
		Df. 1. & $G(x) \Leftrightarrow \forall \varphi(P(\varphi) \Rightarrow \varphi(x))$ \\
		Ax. 3. & $P(G)$ \\
		Th. 2. & $\diamond \exists x G(x)$ \\
		Df. 2. & $\varphi \; \mathrm{ess}\;  x \Leftrightarrow \varphi(x) \wedge \forall \psi(\psi(x) \Rightarrow \square \forall y(\varphi(y) \Rightarrow \psi(y)))$ \\
		Ax. 4. & $P(\varphi) \Rightarrow \square P(\varphi)$ \\
		Th. 3. & $G(x) \Rightarrow G\; \mathrm{ess}\; x$ \\
		Df. 3. & $E(x) \Leftrightarrow \forall \varphi(\varphi \; \mathrm{ess}\; x \Rightarrow \square \exists y \varphi(y))$ \\
		Ax. 5.  & $P(E)$ \\
		Th. 4. &  $\square \exists x G(x)$
		\end{tabular}
	\end{table}
	Most criticism of Gödel's proof is aimed at its axioms: as with any proof in any logical system, if the axioms of the proof depends on are doubted, then the conclusions can be doubted! It is particularly applicable to Gödel's proof – because it is based on five axioms which are considered questionable. A proof does not necessitate that the conclusion be correct, but rather that by accepting the axioms, the conclusion follows logically.
	
	We can also prove that most gods doesn't exist using logic:
	\begin{itemize}
		\item Premise: Everything that exists has a cause 
		\item Property: God is a concept without a cause
		\item Therefore: God has no cause therefore he does not exist 
	\end{itemize}
	Or formally:
	\begin{table}[H]
		\centering
		\begin{tabular}{ll}
		Ax. 1. & $\exists P \forall x P(x)$ \\
		Df. 1. & $\mathcal{G} \rightarrow \neg P(x)$ \\
		Th. 1. & $\square \neg P(\mathcal{G})\rightarrow \cancel{\exists}\mathcal{G}$
		\end{tabular}
	\end{table}
	
	Another famous similar example is the proof of the existence of unicorns...! Indeed, consider we want top prove that unicorns, i.e. horses with one horn, exist. The proof consist rhetorically on the following sequence:
	\begin{itemize}
		\item To prove: Unicorns, i.e. a horses with one horn exist
		
		\item (Df. 1.) Unicorns are as we all know horses with one horn!
		
		\item (Ax. 1.) Let us suppose the contrary, that unicorns do not exist!
		
		\item (Th. 1) Unicorns does not have a horn since, a non-existent entity cannot have anything...
		
		\item (Df. 2) But this contradicts the fact that unicorns have one horn by definition
		
		\item (Df. 3) This contradiction occurred for wrongly assuming that unicorns do not exist
		
		\item (Th. 4) Q.E.D. Hence, unicorns exist!
	\end{itemize}
	Formally the above can be rewritten:
	\begin{table}[H]
		\centering
		\begin{tabular}{ll}
		Df. 1. & $\forall x(\operatorname{Unicorn}(\mathrm{x}) \Rightarrow \operatorname{HasHorn}(\mathrm{x}))$ \\
		Ax. 1. & $\neg \exists x \operatorname{Unicorn}(x)$ \\
		Th. 1 & $\forall x(\operatorname{Unicorn}(x) \Rightarrow \neg \operatorname{HasHorn} (x))$ \\
		Df. 2 & $\left(\forall x(\operatorname{Unicorn}(\mathrm{x}) \Rightarrow \operatorname{HasHorn}(\mathrm{x})) \wedge \forall x(\operatorname{Unicorn}(x) \Rightarrow \neg \operatorname{HasHorn} (x))\right)\Rightarrow \perp$ \\
		Df. 3 & $\neg \exists x \operatorname{Unicorn}(x) \Rightarrow \perp$ \\
		Th. 4 & $\square \exists x \operatorname{Unicorn}(x)$
		\end{tabular}
	\end{table}
	The error in this example is the claim that a non-existent object cannot have attributes/parts, ie $\ne\exists(x) \wedge  \operatorname{Predicate} (\mathrm{Q}) \Rightarrow \neg Q(\mathrm{x})$. We cannot assert something about the non-existence of unicorns without evidence based science!
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{img/arithmetics/black_cat_analogy.jpg}
	\end{figure}
	
	\subsubsection{Some famous logical fallacies}\label{logical fallacies}
	\textbf{Definition (\#\thesection.\mydef):}  The "\NewTerm{principle of explosion}\index{principle of explosion}", or the "\NewTerm{principle of Pseudo-Scotus}\index{principle of Pseudo-Scotus}"  (falsely attributed to Duns Scotus), is the law according to which \underline{any statement can be proven from a contradiction}. That is, from a contradiction, any proposition (including its negation) can be inferred from it; this is known as "deductive explosion". Due to the principle of explosion, the existence of a contradiction (inconsistency) in a formal axiomatic system is disastrous; since any statement can be proven, it trivializes the concepts of truth and falsity...!
	
	In symbolic logic, the principle of explosion can be expressed schematically in the following way:
	
	For any statements $P$ and $Q$, if $P$ and $\neg P$ are both true, then it logically follows that $Q$ is true.
	\begin{dem}
	Below is a formal proof of the principle using symbolic logic:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|l|}
		\rowcolor[gray]{0.75} \hline \multicolumn{1}{|c|}{ Step } & \multicolumn{1}{|c|}{ Proposition } & \multicolumn{1}{c|}{ Derivation } \\
		\hline $1$ & $P$ & Premise \\
		\hline $2$ & $\neg P$ & Premise \\
		\hline $3$ & $P \vee Q$ & Disjunction introduction $(1)$ \\
		\hline $4$ & $Q$ & Disjunctive syllogism $(3,2)$ \\
		\hline
		\end{tabular}
	\end{table}
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\textbf{Definition (\#\thesection.\mydef):} The "\NewTerm{principle of sufficient reason}\index{principle of sufficient reason}" (PSR) states that everything must have a reason (explanation) or a cause. The principle was articulated and made prominent by Gottfried Wilhelm Leibniz, with many antecedents, and was further used and developed by Arthur Schopenhauer and Sir William Hamilton, 9th Baronet. It can be stated in various forms that are:
	\begin{itemize}
		\item For every entity $X$, if $X$ exists, then there is a sufficient explanation for why $X$ exists.
		
		\item For every event $E$, if $E$ occurs, then there is a sufficient explanation for why $E$ occurs.
		
		\item For every proposition $P$, if $P$ is true, then there is a sufficient explanation for why $P$ is true.
	\end{itemize}
	In symbolic logic, the principle of sufficient reason can be expressed schematically in the following way:
	
	It is an open question whether the principle of sufficient reason can be applied to axioms within a logic construction like a mathematical or a physical theory, because axioms are propositions accepted as having no justification possible within the system.  However, the reader must remember that Gödel has shown that for every sufficiently expressive deductive system a proposition exists that can neither be proved nor disproved ! The principle of sufficient reason demands that everything must have a reason. So what is the reason for the principle itself? There isn't one - it's simply stated as an axiom with all what that implies...
	
	The indeterministic interpretations of Quantum Mechanics\footnote{For example, the decay of a given radium atom has no cause, it is a probabilistic event. According to the Copenhagen interpretation of Quantum Mechanics there are no hidden parameters which give a sufficient cause - not to name a sufficient reason! Furthermore Bell showed that hidden variables weren't possible! } (Copenhagen and the like) that declare probabilistic predictions complete as to what there is to be known prior to the collapse contradicts the principle of sufficient reason. However other interpretation like the Many World one doesn't.
	
	\begin{tcolorbox}[enhanced,title=Remark,colframe=black,arc=10pt,drop lifted shadow,after skip=15pt plus 2pt]
	The philosopher Robert Nozick also said Quantum Mechanics does violate the principle of sufficient reason. Only a weakened form of the PSR can survive. The mathematician John Conway who wrote on quantum foundations also explicitly stated the original principle of sufficient reason contradicts Quantum Mechanics as such.
	\end{tcolorbox}
	
	Now that we have presented most famous logical fallacies, let us recall all symbols seen so far:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|}
		\rowcolor[HTML]{C0C0C0} \hline \textbf{Connective} & \textbf{Symbol} & \textbf{Statement} \\
		\hline TRUE value & $\top$ & Logical statement is true \\
		\hline FALSE value & $\perp$ & Logical statement is false \\
		\hline AND & $\wedge$ & Conjunction \\
		\hline OR & $\vee$ & \begin{tabular}{l} 
		Disjunction \\
		(inclusive or)
		\end{tabular} \\
		\hline XOR & $\oplus$ & Exclusive or \\
		\hline NOT & $\neg, \bar{p}$ & Negation \\
		\hline if...then & $\rightarrow, \Rightarrow$ & \begin{tabular}{c} 
		Implication \\
		(conditional statement) \\
		(necessary condition) \\
		\end{tabular} \\
		\hline if and only if & $\leftrightarrow, \Leftrightarrow$ & Biconditional \\
		\hline equivalent to & $\equiv$ & Equivalence \\
		\hline there exists & $\exists$ & Existence \\
		\hline there does not exists & $\nexists$ & Non-Existence \\
		\hline there is exactly & $\exists !$ & Unique Existence \\
		\hline for all & $\forall$ & Quantifier \\
		\hline because & $\because$ & Because / Since \\
		\hline therefore & $\therefore, \Longrightarrow$ & Therefore / Consequence \\
		\hline proves that & $\vdash$ &  \\
		\hline is not provable & $\not\vdash$ & \\
		\hline is a tautology & $\models$ & Tautology \\
		\hline is a contradiction & $\not\models$ & Contradiction \\
		\hline set of formulas & $\Gamma$ &  \\
		\hline possibly & $\lozenge$ & Non-obligatory assumption \\
		\hline end of proof & $\square, \blacksquare$ &  \\
		\hline code number of the Gödel sentence $G$ & $\ulcorner G\urcorner$ & \\
		\hline
		\end{tabular}
		\caption{Common symbols in Classic Logic and Proof Theory}
	\end{table}
	\begin{tcolorbox}[enhanced,title=Remark,colframe=black,arc=10pt,drop lifted shadow,after skip=15pt plus 2pt]
	The logical equivalence $\equiv$ is by tradition something that goes outside of a logical statement (between two of them in particular). If and only if $\leftrightarrow, \Leftrightarrow$ is a symbol that belongs inside a logical statement.
	\end{tcolorbox}
	To summarize, below we give a table with most common tautologies, inferences rules and fallacies:
	\begin{table}[H]
		\centering
		\resizebox{\textwidth}{!}{
		\begin{tabular}{|l|l|l|}
		\rowcolor[HTML]{C0C0C0}\hline \multicolumn{3}{|c|}{ {\Large \textbf{PROPOSITIONAL LOGIC RULES AND FALLACIES}}} \\
		\hline \rowcolor[HTML]{EFEFEF}  {\large \textbf{Name}} & {\large \textbf{Formula}} & {\large \textbf{Description}} \\
		\hline \begin{tabular}{l}Tautology \end{tabular} & \begin{tabular}{l}
		$\varphi=p(x) \therefore A \Rightarrow A $ (i.e. $\models A$)
		\end{tabular} & \begin{tabular}{l} 
		assertion that $A$ is true in every possible interpretation
		\end{tabular}  \\
		\hline \begin{tabular}{l}Law of Identity \end{tabular} & \begin{tabular}{l}
		$\forall A: A=A$
		\end{tabular} & \begin{tabular}{l} 
		for all $A$, $A$ is what it is
		\end{tabular}  \\
		\hline \begin{tabular}{l}Law of Non-Contradiction\end{tabular} & \begin{tabular}{l}
		$\neg(A \wedge \neg A)$
		\end{tabular} & \begin{tabular}{l} 
		$A$ cannot not be and be at the same type its opposite
		\end{tabular} \\
		\hline \begin{tabular}{l}Law of excluded middle\end{tabular} & \begin{tabular}{l}
		$A \vee \neg A$
		\end{tabular} & \begin{tabular}{l} 
		every thing has a given quality or has the  negative\\ of that quality
		\end{tabular} \\
		\hline \begin{tabular}{l}Modus Ponens\end{tabular} & \begin{tabular}{l}
		$[(p \rightarrow q) \wedge p] \Rightarrow q$
		\end{tabular} & \begin{tabular}{l} if $p$ then $q ; p$; therefore $q$
		\end{tabular} \\
		\hline \begin{tabular}{l}Modus Tollens\end{tabular} & \begin{tabular}{l}
		$[(p \rightarrow q) \wedge \neg q] \Rightarrow p$
		\end{tabular} & \begin{tabular}{l} if $p$ then $q$; not $q$; therefore not $p$
		\end{tabular}\\
		\hline \begin{tabular}{l}Hypothetical Syllogism\end{tabular} & \begin{tabular}{l} $[(p \rightarrow q) \wedge(q \rightarrow r)] \Rightarrow (p \rightarrow r)$
		\end{tabular} & \begin{tabular}{l} if $p$ then $q$; if $q$ then $r$; therefore, if $p$ then $r$
		\end{tabular} \\
		\hline \begin{tabular}{l}Disjunctive Syllogism\end{tabular} & \begin{tabular}{l} $(p \vee q ; \neg p) \therefore q$ \end{tabular} & \begin{tabular}{l} either $p$ or $q$; not $p$; therefore, $q$ 
		\end{tabular}\\
		\hline \begin{tabular}{l}Constructive Dilemma\end{tabular} & \begin{tabular}{l} $(p \rightarrow q) \wedge(r \rightarrow s) ; p \vee r \therefore q \vee s$ \end{tabular} & \begin{tabular}{l} 
		if $p$ then $q$; and if $r$ then $s$; but either $p$ or $r$; therefore \\
		either $q$ or $p$
		\end{tabular} \\
		\hline \begin{tabular}{l}Destructive Dilemma\end{tabular} & \begin{tabular}{l} $(p \Rightarrow q) \wedge(r \Rightarrow s) ; \neg q \vee \neg s \therefore \neg p \vee \neg r$ \end{tabular} & \begin{tabular}{l} 
		if $p$ then $q$; and if $r$ then $s$; but either not $q$ or not $s$; \\
		therefore rather not $p$ or not $r$
		\end{tabular} \\
		\hline \begin{tabular}{l}Simplification\end{tabular} & \begin{tabular}{l} $[(p= \top)\wedge (q = \top )] \therefore p = \top$ \end{tabular} & \begin{tabular}{l} $p$ and $q$ are true; therefore $p$ is true
		\end{tabular} \\
		\hline \begin{tabular}{l}Conjunction\end{tabular} & \begin{tabular}{l} $(p= \top, q = \top) \therefore p \wedge q$ \end{tabular} & \begin{tabular}{l}
		$p$ and $q$ are true separately; therefore they are true \\
		conjointly
		\end{tabular} \\
		\hline \begin{tabular}{l}Addition\end{tabular} & \begin{tabular}{l} $p = \top \therefore p \vee q$ \end{tabular} & \begin{tabular}{l} $p$ is true; therefore the disjunction ( $p$ or $q$ ) is true 
		\end{tabular}\\
		\hline \begin{tabular}{l}Composition\end{tabular} & \begin{tabular}{l} $[(p \rightarrow q) \wedge(p \rightarrow r)] \Rightarrow [p =\top  \rightarrow (q \wedge r)=\top]$ \end{tabular} & \begin{tabular}{l} 
		if $p$ then $q$; and if $p$ then $r$; therefore if $p$ is true then $q$ \\
		and $r$ are true
		\end{tabular} \\
		\hline \begin{tabular}{l}De Morgan's Theorems\end{tabular} & \begin{tabular}{l}
		$\neg(p \wedge q) \equiv(\neg p \vee \neg q)$ \\
		$\neg(p \vee q) \equiv(\neg p \wedge \neg q)$
		\end{tabular} & \begin{tabular}{l} 
		the negation of $(p$ and $q)$ is equiv. to (not $p$ or not $q$ ) \\
		the negation of $(p$ or $q)$ is equiv. to (not $p$ and not $q$ )
		\end{tabular} \\
		\hline \begin{tabular}{l}Commutation\end{tabular} & \begin{tabular}{l}
		$(p \vee q) \equiv(q \vee p)$ \\
		$(p \wedge q) \equiv(q \wedge p)$
		\end{tabular} & \begin{tabular}{l} 
		($p$ or $q)$ is equiv. to ( $q$ or $p)$ \\
		$(p$ and $q)$ is equiv. to $(q$ and $p$ )
		\end{tabular} \\
		\hline \begin{tabular}{l}Association\end{tabular} & \begin{tabular}{l}
		{$[p \vee(q \vee r)] \equiv[(p \vee q) \vee r]$} \\
		{$[p \wedge(q \wedge r)] \equiv[(p \wedge q) \wedge r]$}
		\end{tabular} & \begin{tabular}{l}
		$p$ or $(q$ or $r)$ is equiv. to $(p$ or $q)$ or $r$ \\
		$p$ and $(q$ and $r)$ is equiv. to $(p$ and $q)$ and $r$
		\end{tabular} \\
		\hline \begin{tabular}{l}Distribution\end{tabular} & \begin{tabular}{l}
		{$[p \wedge(q \vee r)] \equiv[(p \wedge q) \vee(p \wedge r)]$} \\
		{$[p \vee(q \wedge r)] \equiv[(p \vee q) \wedge(p \vee r)]$}
		\end{tabular} & \begin{tabular}{l}
		$p$ and $(q$ or $r)$ is equiv. to ( $p$ and $q$ ) or ( $p$ and ( $p$ or $r)$ \\
		$p$ or $(q$ and $r$ ) is equiv. to $(p$ or $q$ ) and ( $p$ or $r$ )
		\end{tabular} \\
		\hline \begin{tabular}{l}Double Negation\end{tabular} & \begin{tabular}{l} $p \equiv \neg \neg p$ \end{tabular} & \begin{tabular}{l} 
		$p$ is equivalent to the negation of not $p$ 
		\end{tabular} \\
		\hline \begin{tabular}{l}Transposition\end{tabular} & \begin{tabular}{l} $(p \rightarrow q) \equiv(\neg q \rightarrow \neg p)$ \end{tabular} &  \begin{tabular}{l} 
		if $p$ then $q$ is equiv. to if not $q$ then not $p$ 
		\end{tabular}\\
		\hline \begin{tabular}{l}Material Implication \end{tabular} & \begin{tabular}{l} $(p \rightarrow q) \equiv(\neg p \vee q)$ \end{tabular} & \begin{tabular}{l} 
		if $p$ then $q$ is equiv. to either not $p$ or $q$
		\end{tabular} \\
		\hline \begin{tabular}{l} Material Equivalence \end{tabular} & \begin{tabular}{l}
		$(p \equiv q) \equiv[(p \rightarrow q) \vee(q \rightarrow p)]$ \\
		$(p \equiv q) \equiv[(p \wedge q) \vee(\neg p \wedge \neg q)$
		\end{tabular} & \begin{tabular}{l} 
		($p$ is equiv. to $q$) means, either (if $p$ is true then $q$ is \\
		true) or (if $q$ is true then $p$ is true) \\
		($p$ is equiv. to $q$) means, either ( $p$ and $q$ are true) or \\
		(both $p$ and $q$ are false)
		\end{tabular} \\
		\hline \begin{tabular}{l}Exportation\end{tabular} &  \begin{tabular}{l} $[(p \wedge q) \rightarrow r] \equiv[p \rightarrow(q \rightarrow r)]$ \end{tabular} & \begin{tabular}{l} 
		(if $p$ and $q$ are true then $r$ is true) is equivalent to (if $q$ is \\
		true then $r$ is true, if $p$ is true)
		\end{tabular} \\
		\hline \begin{tabular}{l}Importation\end{tabular} & \begin{tabular}{l} $[p \rightarrow(q \rightarrow r)] \equiv[(p \wedge q) \rightarrow r]$ \end{tabular} & \begin{tabular}{l} 
		if $p$ is true then (if $q$ then $r$) is true is equivalent to \\
		$p$ and $q$ is true then $r$ is true
		\end{tabular} \\
		\hline \begin{tabular}{l}Principle of explosion\\(fallacy)\end{tabular} & \begin{tabular}{l} $\forall p,q: ( p \wedge \neg p = \top ) \vdash q$ \end{tabular} & \begin{tabular}{l} 
		for any statements $p$ and $q$, if $p$ and $\neg p$ are both \\
		true, then we can always prove that $q$ is true
		\end{tabular} \\
		\hline \begin{tabular}{l}Vacuous truth\\ (fallacy)\end{tabular} & \begin{tabular}{l} $\forall x \in P: Q(x) \;$ where $P=\varnothing$ \end{tabular} & \begin{tabular}{l} 
		statement that asserts that all members of the empty   \\
		set have a certain property
		\end{tabular} \\
		\hline \begin{tabular}{l} Existential fallacy\end{tabular} & \begin{tabular}{l} $A \Rightarrow B, B \therefore A$  \end{tabular} & \begin{tabular}{l} 
		 one presupposes that a class $B$ has members $A$ when \\
		 one is not supposed to do so
		\end{tabular} \\
		\hline \begin{tabular}{l} Existence property fallacy \end{tabular} & \begin{tabular}{l} $\exists A \neq P(A)$ \end{tabular} & \begin{tabular}{l} 
		 existence is a quantifier but not a predicate \\
		 (i.e. existence is not a property!)
		\end{tabular} \\
		\hline \begin{tabular}{l}Affirming the consequent\\(fallacy)\end{tabular} & \begin{tabular}{l} $(P = \top \wedge C = \top); C = \top !\therefore P = \top$ \end{tabular} & \begin{tabular}{l} 
		if the premise and conclusions are both true, the  \\
		conclusion is not a necessary consequence of the premise
		\end{tabular} \\
		\hline  \begin{tabular}{l}Denying the antecedent\\(fallacy)\end{tabular} & \begin{tabular}{l} $( P = \top \rightarrow C \Rightarrow \top); C = \top !\therefore P  = \top $ \end{tabular} & \begin{tabular}{l} 
		while the conclusion may be true, it does not follow   \\
		from the premise
		\end{tabular} \\
		\hline \begin{tabular}{l}Affirming a disjunct\\(fallacy)\end{tabular} & \begin{tabular}{l} $A \vee B = \top; B = \top !\therefore A = \perp$ \end{tabular} & \begin{tabular}{l} 
		the conclusion does not follow from the premise as   \\
		it could be the case that $A$ and $B$ are both true
		\end{tabular} \\
		\hline \begin{tabular}{l} Denying a conjunction \\(fallacy) \end{tabular} & \begin{tabular}{l} $(B \vee A ) = \perp !\therefore A = \top$ \end{tabular} & \begin{tabular}{l} 
		$A$ and $B$ cannot be both true \\
		the conclusion does not follow from the premise as   \\
		it could be the case that $A$ and $B$ are both false
		\end{tabular}  \\
		\hline \begin{tabular}{l} Undistributed middle \\(fallacy) \end{tabular} & \begin{tabular}{l} $Z \in B$, $Y\in B\therefore Y=Z$ \end{tabular} & \begin{tabular}{l} 
		it is not necessarily true that "all $B$s are $Z$s," which \\
		is ignored in the argument 
		\end{tabular} \\
		\hline \begin{tabular}{l} False dichotomy \\(fallacy) \end{tabular} & \begin{tabular}{l} $(p \rightarrow q),(r \rightarrow q),(p \vee q)\therefore q$ \end{tabular} & \begin{tabular}{l} 
		it is a false dichotomy as whatever the \\
		choice it leads to the same conclusion
		\end{tabular} \\
		\hline \begin{tabular}{l} A priori argument \\ (motivated reasoning)\\(fallacy) \end{tabular} & \begin{tabular}{l} $\overline{\Gamma, A \vdash A}^{\mathrm{ax}}$ \end{tabular} & \begin{tabular}{l} 
		using an axiom to prove the axiom itself \\
		doesn't make sense as the axiom is therefore\\
		beyond refutation
		\end{tabular}\\
		\hline
		\end{tabular}
		}
		\vspace*{1mm}
		\caption{Most well known rules of inference and fallacies in classic logic}
	\end{table}	
	With all these tools it's now easy to disprove any ontological argument for any of the gods of the 45 monotheistic revealed religions actually practised around the world! We already have for example explained the fallacies in Gödel's ontological argument earlier above. Another type of ontologic arguements are those used mainly by the eleven Abrahamic religions and named the "Thomas Aquinas' Five Ways\footnote{The reader must not forget that philosophers aren't skilled scientists and that’s a severe handicap when confronting a philosophical argument to evidence based science.}" (Thomas d'Aquinas was a Dominican friar and priest and influential philosopher and theologian from the 113th century according to holocene calendar):
	\begin{itemize}
		\item \textbf{The argument of the unmoved mover:} As particles pop-up from vacuum with non-zero speed in the quantum realm, they don't have any mover. They just extract, as we will prove during our study of Quantum Physics, their energy from the vacuum zero-energy level that has a non-zero value (at small scales principle of conservation of energy is still conserved as a photon is it's own anti-particle and other particles appear with opposite counterpart but at cosmological scales the principle of conservation of energy is violated as we will also prove it). Also light emitted from atoms caused by electrons levels changes aren't accelerated by a mover until they reach the speed of light, they emerge from vacuum instantaneously at the speed of light! Thomas d'Aquinas wasn't able to know that Quantum Physics would exist and most philosophers are illiterate in science and doesn't know nothing about vacuum fluctuations (typically they know nothing about the Euler–Heisenberg Lagrangian nor the Uehling potential).
		
		\item \textbf{Causation of existence:} Theoretically nothing forbids for example our universe to create itself. Indeed according to Roger Penrose speculative Conformal Cyclic Cosmology (CCC) model the end of the Universe generates it's own next beginning. However at the contrary of the science evidence based results that refutes Aquinas first way introduced previously, there are for the moment in the early 121st century (according to holocene calendar) no strong experimental evidence for Roger Penrose model.
		
		\item \textbf{Contingent and necessary objects:} This way of Aquinas is proven mathematically wrong by formal logic as it is based on three first order logic fallacies: 1) the a priori argument where Aquinas uses an axiom that leads to a conclusion that is the axiom itself (see the table of logical fallacies above) 2) the principle of explosion as Aquinas uses a contradiction to prove a statement (see the table of logical fallacies above) 3) the existence property fallacy as Aquinas uses a quantifier that is not a predicate (see the table of logical fallacies above).
		
		\item \textbf{Argument from degrees and perfection:} This one is different from the previous ways! It's primarily semantical nonsense based on solely linguistic definitions that doesn't prove a thing! However an nuclear chemist would very likely say that this argument is scientifically speaking pure nonsense and therefore wrong...
		
		\item \textbf{Argument from intelligent design:} This way of Aquinas uses a ill-defined concept in science. The "mind" is indeed a philosophical/religious construct, with a fluid definition that is difficult to pin down. No one can prove anything without clearly scientifically defining it and there isn't (sadly) any scientific consensus about its definition still in this early 121st century (holocene calendar). Nevertheless, we actually agree that some objects clearly are what the layman would say "mindless" (a stone for example) while others appear to have what we the layman commonly names a "mind" in vernacular usage.
	\end{itemize}
	Obviously we strongly advise the reader to check the original text of Thomas d'Aquinas because taking them out of their context can be misleading (some philosophers always argue that Aquinas five ways are wrongly interpreted when the latter are debunked by modern science) and we must also remember that nowadays someone like Thomas d'Aquinas wouldn't even obtain a high-school grade in modern science...
	
	Therefore, we can conclude that theist attempt to prove the existence of the god of one of the 45 revealed monotheistic religions based on rhetorical formal logic is easily exposed as vacuous!
	
	\subsubsection{Is logic a science?}
	\begin{fquote}[Joseph Wood Krutch]Logic is the art of going wrong with confidence.
 	\end{fquote}
	Let us recall that this question in the broad sense of the word "science" has obviously a positive answer. However as we have seen in the introduction of this book, there are different sciences!
	
	If we designate by "science" any field centered around the "scientific method": making hypotheses, running experiments in the purpose to refute (i.e. falsify) them and so on, then Logic is nothing like this (logic doesn't have for purpose to refute his premises). Experimental science is inherently based on the physical, observable world. While there is deep thinking involved, a posteriori assumptions, the ultimate arbiter of any experimental scientific idea is the sensible world, through experimentation. 
	
	Logic, on the other hand, is entirely self-contained. It's the study of formal systems: sets of axioms and rules on how to derive theorems. Logic is a game where all the rules are made up.
	
	Although logic is not an empirical science or a physical science, it is nonetheless a science in the broad sense of the term, i.e., a systematic body of rational knowledge.
	
	While any use of logic probably may falls back to the real world, logic itself doesn't. It's entirely self-contained. When we prove something logically, we are not making a direct statement about the universe: we are making a claim consistent with the assumptions of our logic system and that may furthermore not be refutable (i.e. not falsifiable) - or even worse - the major well known historical flaw/issue of logic (we have more than 2500 years of evidence of such flaws) is that the conclusion of any logical reasoning is always in the premises. The relationship between logic and the universe is separate: it's what makes logic useful, but it is not what makes logic.
	
	One of the most well known example in "modern" graduate level science of fallacious logic is the \textit{Kalâm cosmological argument} (an argument for the existence of a deity that created our Universe) the second one is very likely the Pascal's wager. The premise of the Kalâm cosmological argument, «\textit{Whatever begins has a cause}», may be kinda logic for an undergraduate or scientifically illiterate individual or a non-professional physicist. But for professionals postgraduate level physicists this is proven mathematically and experimentally wrong at least since the years 11950 (we will see the proofs in the section of Wave Quantum Physics page \pageref{wave quantum physics} and Quantum Gravity page \pageref{newton quantum gravity} much later in this book). So anyone using this argument as a logical claim just provide evidence for his ignorance of modern physics\footnote{Another such typical example are that because of existence of truth table, some people again argue that logic can lead to true conclusions. But truth tables depends again on the premise and if the premise is false... guess what!} and also of logics itself (as the conclusion is in the premise... facepalm). And furthermore as we have also seen during our introduction of this book, a claim or a quote is at maximum an evidence of level 02 (see page \pageref{evidence levels chart})...
	
	Very likely the only well known example in "modern" postgraduate level science of fallacious logic (where again the conclusion is in the premises\footnote{An argument is "circular" just in case there is a premise, either implicit or explicit, that is logically equivalent to the conclusion. In many cases, circularity is a problem and we name such circularity a "\NewTerm{vicious circularity}".}) is the \textit{Gödel's ontological proof} for the existence of (very likely...) the Christian deity... as it suits well Gödel's beliefs... Most theist share the following proof summary on social network this  without obviously recognizing all biases as they are for the huge majority of them scientifically illiterate:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{img/arithmetics/godel_ontological_proof.jpg}
	\end{figure}
	Most criticism of Gödel's proof (and other equivalent one) is aimed at its axioms (premises): as with any proof in any logical system, if the axioms of the proof depends on are doubted, then the conclusions can be doubted (that why in logic you can build any proof and that afterwards you need experimental science to sort the garbage from the evidence). It is particularly applicable to Gödel's proof – because it rests on five axioms, some of which are questionable (there are more than $10,000$ gods around the World for example and most of them don't have positive\footnote{And that concept wasn't defined by Gödel...} properties... especially in the Abrahamic religions) and some other are refuted theoretically and experimentally (the axiom A1 is wrong as we know from our actual knowledge of Quantum Physics as proven in the chapter on that topic at page \pageref{wave quantum physics}) since the 11950 at least... A proof does not necessitate that the conclusion be correct, but rather that by accepting the axioms, the conclusion follows logically. Also Gödel's proof is wrong because again... the conclusion is in the premises. And finally Gödel's ontological proof also suffer from the Gödel's incompleteness theorems!
	
	In a certain set, logic is pure "thought-stuff". It's pure reasoning distilled to a set of simple, verifiable rules. The ultimate arbiter for any logical proof is the logic system, not the universe at large. We can even have perfectly reasonable logic systems that do not map back to our universe at all! It would still be logic, but it probably wouldn't be very interesting or useful!	For all these reasons, you can't run an experiment on logic! On the one hand, there is nothing to experiment on, logic is only about validity (when science is about evidence). Again, logic is perfectly self-contained and not dependent on physical reality. On the other hand, logic does not have uncertainty: either you have a proof for something, or you don't. So you don't need to run experiments in the first place, since you have a more powerful notion of proof within the system itself.
	
	So logic doesn't involve any experimental configuration and doesn't describe our observable Universe. It is therefore definitely not a scientific method\footnote{There is a quite famous 700 pages book \cite{sobel2003logic} written by a philosopher that presents and criticize in details (in a horrible way because obviously most philosophers don't know how to write properly textbooks in \LaTeX) most existing ontological proofs and disproofs of gods but however without relying to experimental scientific evidence and focusing only on the main Abrahamic religions. Typical errors of most occidental philosophers by the way...}! Let us see a non-exhaustive logical/intuition errors list from some famous philosophers, engineers and scientists that prove the point:
	\begin{itemize}
		\item Aristotle wasn't the first to say that the Earth occupies the center of the universe, but he was the most dogmatic about it, and believed he had established it to be incontrovertibly true by using... logic.
		
		\item Euclid was sure by logic that the axiom of parallels (parallel postulate) should always have been true. However modern mathematics and modern experimental science proved he was wrong.
		
		\item According to Kepler's logic, planet orbits are ellipses because of his religious faith in perfection. We know now that in fact planets describe, from the point of view of a centered galactic observer, a helicoidal movement and describe a given ellipse whose parameters changes across billions years and may even become chaotic over time.
		
		\item According to Boyle's intuition (logic), light had to propagate in something named an "aether". This was later shown experimentally and theoretically wrong.
		
		\item According to Darwin's intuition (logic), characteristics of the mother and the father simply get blended in the offspring just as a can of black paint and a can of white paint blend to create gray when combined. This was proven wrong later by Mendel and modern genetics.
		
		\item According to Kelvin's reasoning method, Earth's age would have been 50 times younger than what modern radio-isotopic dating methods provide as evidence.
		
		\item According to Kelvin's logic, heavier than air flying machines must have been impossible...
		
		\item In the early 120th century (holocene calendar), the vast majority of geologists rejected Wegener’s idea of continental drift, arguing that ocean floors were fixed and continents could not logically move. 
		
		\item According to Einstein's intuition (logic), the Universe must have been static. So he introduced the cosmological constant in it's theory. Evidence proved his logic was flawed.
		
		\item According to Einstein's (and others!) intuition and logic, spooky action at a distance (entanglement) was something impossible and wrong.
		
		\item Einstein denied several times that Black Holes could form.
		
		\item In the early days of quantum mechanics, Einstein tried to show that the uncertainty principle was not valid. By 11927 (holocene calendar) he had become convinced of its utility, but he always opposed it
		
		\item Eddington's failed to take Chandrasekhar's theory of gravitational collapse seriously because for him it was something illogical.
		
		\item etc...
	\end{itemize}
	Obviously some people may argue that the errors above were technical errors and not logical errors. That latter point is a matter of semantic !
	
	But how is experimental science a logic?
	
	Well, the core concept in the scientific method is inductive reasoning. Roughly put, it's the idea that if we do $X$ and $Y$ happens $1,000$ times in a row, $X$ will probably lead to $Y$ next time too. If we do this enough times - and are sufficiently careful - we'll just conclude that $X$ causes $Y$. That's the reason experiments make sense and let us predict the future. As we will see during our study of statistical hypothesis test later, these latter are build on a kind of probabilistic inference and therefore on logic. So the scientific logic is based on logic but... logic is not a scientific method!
	
	Oh! And by the way... Some non-academic people argue that there is only one unique "scientific method". It is quite ambiguous to make such a statement as it is constantly evolving! The documented scientific method by Francis Bacon in 1620 has almost nothing to do anymore with the scientific method of the early 121st century (holocene calendar)!
	
	To finish, let us quote now a part of a text written by Donald E. Simanek on it's website, because it's worth it: 

	«Scientists do not arrive at models and theories by application of logic. They arrive at them by many processes lumped under the name "induction". Induction cannot be reduced to a set of logical rules (though many have tried). To see patterns (sometimes subtle and hidden ones) in data and observations requires creative ability. This is the ability to think ahead and say: «\textit{What model, set of statements (laws) or theoretical construct could I devise from which these observations and data might be deduced?}»
	
	We can't find, discover, or construct scientific laws and theories by mathematics and logic alone. But we can derive testable and useful results by application of mathematics and logic to laws and theories, and if those deduced results pass experimental tests, our confidence in the validity of the theory from which they were derived is strengthened.
	
	Some people are profoundly disturbed by the fact that reason alone can't generate truths. When the use of mathematics and logic in science is explained to them they respond, «\textit{If mathematics and logic can't produce absolute truths, then they produce only untruths or partial truths, and are therefore worthless.}» This sentence is itself an example of nonsense clothed in the appearance of logic.
	
	It must be admitted at the outset that science is not in the business of finding absolute truths. Science proceeds as if there are no absolute truths, or if there are such truths, we can never know what they are. As the pre-Socratic skeptics observed: If we were to stumble upon an absolute truth, we'd have no way to be certain it is an absolute truth. The models and theories of science are approximations to nature - never perfect. But in most cases we know rather well how good they are. We can state quantitatively the limits of uncertainty of numeric results, and their range of applicability. Yet there's always the possibility that we may find exceptions to one of our accepted laws, or may even find alternative theories that do a better job than older ones.
	
	[...]
	
	Some critics of science attack this process of science, on the grounds that it cannot produce absolute truths. Theirs is a black/white view of the scientific process. Never mind that they have not proposed any other process that is capable of producing anything near the power and comprehensiveness of present science. They say that "Theory X" isn't perfect therefore it is "wrong".
	
	[...]
	
	The fact that science claims no absolute truths is seized upon by people who hold strong religious beliefs and who dislike those conclusions of science that run counter to their emotional convictions. To them, if a thing is not absolutely and finally true, it is false, and therefore the methods used to formulate it must be flawed.»
	
	\begin{fquote}[Bill Flavell]Philosophy debates God, maths ignores him, climatology doesn't need him, physics can't find him, biology contradicts him, logic kills him!
 	\end{fquote}
	
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{80} & \pbox{20cm}{\score{4}{5} \\ {\tiny 64 votes, 80.94\%}} 
	\end{tabular} 
	\end{flushright}
	
	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Numbers}

	\lettrine[lines=4]{\color{BrickRed}T}{he} basis of mathematics, apart the reasoning (\SeeChapter{see section Proof Theory page \pageref{proof theory}}), is undoubtedly to ordinary people: arithmetic. It is therefore mandatory that we make a stop on it to study its origin, some of its properties and consequences.\\

	The numbers, like geometric figures are the basis of Geometry, are the basis of Arithmetic. These are also the historical basis because mathematics probably started with the study of these objects, but also the educational foundation, because it is by learning to count that we enter in the world of mathematics.

	The history of numbers, also sometimes named "\NewTerm{scalar}"\index{scalar}\label{scalar}, is far too long to be told here! But we can only advise you one of the best book on the subject: The Universal History of Numbers (approximately 2,000 pages in three volumes) Georges Ifrah, ISBN: 2221057791.

	But here's a little flange of this latter which seems fundamental to us:

	Our current decimal system, on base 10, uses the digits $0$ to $9$, named "Arabic numbers", but the fact of Indian origin (Hindus). The first numbers seems to have been created in the 97th (holocene calendar) in India by Brahmagupta, an Indian mathematician. he created the figures in Devanagari.

	Indeed, the Arabic numbers (of Indian origin...) in the table below are the first line and we see that they are significantly different from the "Indian numbers" of the second line:

	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/numbers.eps}
		\caption{Indo-Arabic numbers}
	\end{figure}
	
	You have to read this table as following from left to right: $0$ "\NewTerm{zero}", $1$ "\NewTerm{one}", $2$ "\NewTerm{two}", $3$ "\NewTerm{three}", $4$ "\NewTerm{four}", $5$ "\NewTerm{five}", $6$ "\NewTerm{six}", $7$ "\NewTerm{seven}", $8$ "\NewTerm{eight}, $9$ "\NewTerm{nine}". This system is much more efficient than the Roman numerals (try doing a calculation with Roman notation system you will see...).
	
	It is commonly accepted that these numbers were introduced in Europe only about the year 11000 (holocence calendar). Used in India, they were transmitted by Arabs to the Western world by the Pope Gerbert of Aurillac during his stay in Andalusia at the end of the 109th century (holocene calendar).

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
The French word "chiffre" (number) is a corruption of the Arabic word "sifr" meaning "zero". In Italian, "zero" is "zero", and seems to be a contraction of "zefiro", we again see here an Arabic root but the "zero" could also be of Indian origin... So the words "chiffre" and "zero" have the same origin.
	\end{tcolorbox}

The early use of a numerical symbol for the "nothing" in the sense of "no amount", i.e. our "\NewTerm{zero}"\index{zero} is because the Indians used a system named "\NewTerm{positional system}"\index{positional system}. In such a system, the position of a digit in the writing of a number expresses the power of $10$ and the number of times it occurs ... and the absence of a position in this system arise from huge proofreading  problems and could lead to large errors in calculations. The revolutionary and simple introduction of the concept of "nothing" allowed a proofreading without error of numbers.

The absence of a power is denoted by a small circle...: the zero. Our current system is thus the "\NewTerm{decimal and positional system}"\index{decimal and positional system}.

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\textbf{{\Large \ding{45}}Example:}\\\\
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,511); %set diagram left start at 0, and has height of 511
		
		%Straight Lines [id:da5890950478989929] 
		\draw    (275,96) -- (275,126) ;
		\draw [shift={(275,128)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da9472376832788985] 
		\draw    (386,96) -- (386,126) ;
		\draw [shift={(386,128)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da9610896107156215] 
		\draw    (485,96) -- (485,126) ;
		\draw [shift={(485,128)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da02390866720714091] 
		\draw [color={rgb, 255:red, 184; green, 233; blue, 134 }  ,draw opacity=1 ]   (128.5,161) -- (128.5,253) ;
		%Straight Lines [id:da5343153721688434] 
		\draw [color={rgb, 255:red, 184; green, 233; blue, 134 }  ,draw opacity=1 ]   (128.5,253) -- (258.5,253) ;
		%Straight Lines [id:da40466181761715503] 
		\draw [color={rgb, 255:red, 184; green, 233; blue, 134 }  ,draw opacity=1 ]   (258.5,253) -- (258.5,160) ;
		%Straight Lines [id:da7499247152416519] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ]   (147.5,161) -- (147.5,214) ;
		%Straight Lines [id:da43607343622482] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ]   (147.5,214) -- (379.5,214) ;
		%Straight Lines [id:da14353802746412847] 
		\draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ]   (379.5,161) -- (379.5,214) ;
		%Straight Lines [id:da7161401602122761] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ]   (162.5,161) -- (162.5,183) ;
		%Straight Lines [id:da9545221933397674] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ]   (485.5,183) -- (162.5,183) ;
		%Straight Lines [id:da25341996912792286] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ]   (485.5,161) -- (485.5,183) ;
		
		% Text Node
		\draw (122,135) node [anchor=north west][inner sep=0.75pt]  [font=\LARGE] [align=left] {$\displaystyle \textcolor[rgb]{0.82,0.01,0.11}{324}\textcolor[rgb]{0.29,0.56,0.89}{\;=\;}\textcolor[rgb]{0.72,0.91,0.53}{3\times 100}\,+\,\textcolor[rgb]{0.96,0.65,0.14}{2\times 10}\,+\,\textcolor[rgb]{0.29,0.56,0.89}{4\times 1}$};
		% Text Node
		\draw (263,73) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 10^{2}$};
		% Text Node
		\draw (375,74) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 10^{1}$};
		% Text Node
		\draw (473,74) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 10^{0}$};
		% Text Node
		\draw (160,256) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 184; green, 233; blue, 134 }  ,opacity=1 ] [align=left] {hundreds};
		% Text Node
		\draw (302,216) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 184; green, 233; blue, 134 }  ,opacity=1 ] [align=left] {\textcolor[rgb]{0.96,0.65,0.14}{tens}};
		% Text Node
		\draw (454,185) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,opacity=1 ] [align=left] {units};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Description of decimal and positional system}
	\end{figure}
	The number $324$ is written from left to right as three hundred: $3$ times $100$, two tens: $2$ times $10$ and four units: $4$ times $1$.
	\end{tcolorbox}

Thus a "\NewTerm{decimal number}\index{decimal number}" is thus a number that has a finite writing in base $10$.

	We sometimes see (and this is recommended) a thousands separator represented by a coma in United States (put all three numbers from the first from the right for the whole numbers). Thus, we write $1,034$ instead of $1034$ or $1,344,567,569$ instead of $1344567569$. Thousand separators permits to quickly quantify the magnitude of the read numbers.

	So: 
	\begin{itemize}
		\item If we see only one coma we know that the number is about thousands
		\item If we see two apostrophes we know that the number is about millions
		\item If we see three apostrophes we know that the number is about billions
		\item etc.
	\end{itemize}
	and so on... also with decimals this gives:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1467); %set diagram left start at 0, and has height of 1467
		
		%Straight Lines [id:da15642451068402186] 
		\draw    (159,189) -- (159,199) ;
		%Straight Lines [id:da6472293335790507] 
		\draw    (159,199) -- (177.5,199) ;
		%Straight Lines [id:da7240852059993093] 
		\draw    (177.5,189) -- (177.5,199) ;
		
		%Straight Lines [id:da7806819036595889] 
		\draw    (136,189) -- (136,199) ;
		%Straight Lines [id:da26109275847071745] 
		\draw    (136,199) -- (154.5,199) ;
		%Straight Lines [id:da17778487902103768] 
		\draw    (154.5,189) -- (154.5,199) ;
		
		%Straight Lines [id:da29858706043056715] 
		\draw    (182,189) -- (182,199) ;
		%Straight Lines [id:da3620884412100944] 
		\draw    (182,199) -- (200.5,199) ;
		%Straight Lines [id:da03709961182667687] 
		\draw    (200.5,189) -- (200.5,199) ;
		
		%Straight Lines [id:da9010776673875192] 
		\draw    (205,189) -- (205,199) ;
		%Straight Lines [id:da47071861910946855] 
		\draw    (205,199) -- (223.5,199) ;
		%Straight Lines [id:da21375377875821044] 
		\draw    (223.5,189) -- (223.5,199) ;
		
		%Straight Lines [id:da3096038842979896] 
		\draw    (228,189) -- (228,199) ;
		%Straight Lines [id:da017536874083286413] 
		\draw    (228,199) -- (246.5,199) ;
		%Straight Lines [id:da29065239491944284] 
		\draw    (246.5,189) -- (246.5,199) ;
		
		%Straight Lines [id:da10200164222169339] 
		\draw    (251,189) -- (251,199) ;
		%Straight Lines [id:da8810704647275363] 
		\draw    (251,199) -- (269.5,199) ;
		%Straight Lines [id:da020803212541443905] 
		\draw    (269.5,189) -- (269.5,199) ;
		
		%Straight Lines [id:da23945172059457742] 
		\draw    (274,189) -- (274,199) ;
		%Straight Lines [id:da896719737196622] 
		\draw    (274,199) -- (292.5,199) ;
		%Straight Lines [id:da08562490956721724] 
		\draw    (292.5,189) -- (292.5,199) ;
		
		%Straight Lines [id:da5985463429405113] 
		\draw    (297,189) -- (297,199) ;
		%Straight Lines [id:da16401737018414653] 
		\draw    (297,199) -- (315.5,199) ;
		%Straight Lines [id:da6119680611572536] 
		\draw    (315.5,189) -- (315.5,199) ;
		
		%Straight Lines [id:da8236487569873776] 
		\draw    (320,189) -- (320,199) ;
		%Straight Lines [id:da3073297067429739] 
		\draw    (320,199) -- (338.5,199) ;
		%Straight Lines [id:da15110799430249666] 
		\draw    (338.5,189) -- (338.5,199) ;
		
		%Straight Lines [id:da4967110440631104] 
		\draw    (343,189) -- (343,199) ;
		%Straight Lines [id:da9665382460694549] 
		\draw    (343,199) -- (361.5,199) ;
		%Straight Lines [id:da7117561392708107] 
		\draw    (361.5,189) -- (361.5,199) ;
		
		%Shape: Circle [id:dp002021455087769519] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (367,199) .. controls (367,196.79) and (368.79,195) .. (371,195) .. controls (373.21,195) and (375,196.79) .. (375,199) .. controls (375,201.21) and (373.21,203) .. (371,203) .. controls (368.79,203) and (367,201.21) .. (367,199) -- cycle ;
		%Straight Lines [id:da184180719256275] 
		\draw    (381,189) -- (381,199) ;
		%Straight Lines [id:da3196756255720361] 
		\draw    (381,199) -- (399.5,199) ;
		%Straight Lines [id:da6024706236995034] 
		\draw    (399.5,189) -- (399.5,199) ;
		
		%Straight Lines [id:da010218239613404512] 
		\draw    (403.5,189) -- (403.5,199) ;
		%Straight Lines [id:da5048858987274663] 
		\draw    (403.5,199) -- (422,199) ;
		%Straight Lines [id:da41565596019374773] 
		\draw    (422,189) -- (422,199) ;
		
		%Straight Lines [id:da4558602474524964] 
		\draw    (426,189) -- (426,199) ;
		%Straight Lines [id:da6582514484490563] 
		\draw    (426,199) -- (444.5,199) ;
		%Straight Lines [id:da09751948936727795] 
		\draw    (444.5,189) -- (444.5,199) ;
		
		%Straight Lines [id:da6434630462635926] 
		\draw    (448.5,189) -- (448.5,199) ;
		%Straight Lines [id:da621610306922388] 
		\draw    (448.5,199) -- (467,199) ;
		%Straight Lines [id:da8956408346140186] 
		\draw    (467,189) -- (467,199) ;
		
		%Straight Lines [id:da5183850761904651] 
		\draw    (471,189) -- (471,199) ;
		%Straight Lines [id:da6722586645434263] 
		\draw    (471,199) -- (489.5,199) ;
		%Straight Lines [id:da6685282604475573] 
		\draw    (489.5,189) -- (489.5,199) ;
		
		%Straight Lines [id:da6079130871740956] 
		\draw    (493.5,189) -- (493.5,199) ;
		%Straight Lines [id:da9657174878017369] 
		\draw    (493.5,199) -- (512,199) ;
		%Straight Lines [id:da43814636613224067] 
		\draw    (512,189) -- (512,199) ;
		
		
		% Text Node
		\draw (110,186.4) node [anchor=north west][inner sep=0.75pt]    {$\dotsc $};
		% Text Node
		\draw (517,186.4) node [anchor=north west][inner sep=0.75pt]    {$\dotsc $};
		% Text Node
		\draw (135.5,197) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Billions (Giga $10^{9})$};
		% Text Node
		\draw (160.5,197) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Hundreds of millions};
		% Text Node
		\draw (183,197) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Tens of millions};
		% Text Node
		\draw (204.5,197) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Millions (Mega $\displaystyle 10^{9})$};
		% Text Node
		\draw (229.5,197) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Hundreds of thousands};
		% Text Node
		\draw (253,197) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Tens of thousands};
		% Text Node
		\draw (273.5,197) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Thousands (Kilo $10^{3}$)};
		% Text Node
		\draw (298.5,197) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Hundreds};
		% Text Node
		\draw (322,197) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Tens};
		% Text Node
		\draw (346,197) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Units};
		% Text Node
		\draw (383,197) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Tenths};
		% Text Node
		\draw (404.5,197) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Hundreths};
		% Text Node
		\draw (426,197) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Thousands (milli $10^{-3}$)};
		% Text Node
		\draw (449.5,197) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Ten of thousandths};
		% Text Node
		\draw (473.5,197) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Hundred of thousandths};
		% Text Node
		\draw (492.5,197) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Millionth (micro $\displaystyle 10^{-6})$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Scale representation of the positional system}
	\end{figure}

	In fact, any integer other than the unit can be taken as the basis of a numbering system. We have for example the binary, ternary, quaternary, ..., decimal, duodecimal numbering systems which correspond respectively to the bases two, three, four, ..., ten, twelve.

	A generalization of what has been seen above, can be written as follows:

	\label{number power decomposition}Any positive integer $N$ can be represented in a base $b$ as a sum, where each coefficient $a_i$ are multiplied by their respective weight $b^i$. Such as:
	
More elegantly written:
	
with $a_i \in \left[0,b-1\right]$ and $b_i \in \left[1,b^{n-1}\right]$

	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} As frequently in mathematics, we will replace numbers with Latin or Greek letters in order to generalize their representation. So when we speak of a base $b$, the value of $b$ can take any positive integer value $1, 2, 3,\ldots$\\
	
	\textbf{R2.} When we take the value $2$ for $b$, the maximum value of $N$ will be $2^n-1$. The numbers that are written in this form are named "\NewTerm{Mersenne numbers}\index{Mersenne number}". These numbers can be prime numbers (see further below what a prime number is) if and only if $n$ is also a prime number.\\\\
	Indeed, if we take (for example) $b=10$ and $n=3$ the largest value we can get will be:
	
	\textbf{R3.} When a number is the same read from left to right or right to left, we name it a "\NewTerm{palindrome}\index{palindrome}".
	\end{tcolorbox}

\subsection{Digital Bases}

To write a number in base $b$ system, we must first adopt $b$ characters for representing the $b$ first numbers for example in the decimal system: $\left\lbrace 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\right\rbrace $. These characters are as we already defined them, the "\NewTerm{digits}\index{digits}" that we pronounce as usual $\left\lbrace \text{zero}, \text{one}, \text{two}, \text{three}, \text{four}, \text{five}, \text{six}, \text{seven}, \text{eigth}, \text{nine}\right\rbrace $.

For the written numbers, we make this convention that a digit, placed to the left of another represents the order units immediately above, or $b$ times larger. To take the place of units that may be lacking in certain orders, we use the zero "$0$" and consequently, the number of digits may vary.

\textbf{Definition (\#\thesection.\mydef):} For the spoken numbers, we agree to name "single unit", "ten", "hundred", "thousand", etc., units of the first, second, third, fourth order, etc. Thus the numbers $10, 11, \ldots, 19$ will be readed in the same way in all numbering systems. The numbers $1a, 1b, a0, b0, \ldots$ will be readed ten-$a$, ten-$b$, $a$-ten, $b$-ten, etc. Thus, the number 5b6a71c will be readed:
\begin{center}
five million be-hundred sixty-$a$ thousand seven hundred ten-$c$
\end{center}
This small example is relevant because it shows the general expression of the spoken language we use daily is intuitively in base ten (fault of our education).

	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
\textbf{R1.} The rules of mathematical operations defined for numbers written in the decimal system are the same for numbers written in any numbering system.\\

\textbf{R2.} To quickly operate in any numbering system, it is useful to know by heart all sums and products of two numbers of a single digit.\\

\textbf{R3.} The decimal seems has its origin in the fact that humans being have ten fingers.
	\end{tcolorbox}
Let's see how we convert a numbering system in another one:

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\textbf{{\Large \ding{45}}Examples:}\\\\
E1. In base ten we have seen above that $142,713$ will be written as:
	
E2. The number $0110$ that is in base two (binary base) would be written in base $10$:
	
	\end{tcolorbox}
	and so on... The reverse operation is often a little trickier (for example the case of the binary base):
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. Converting the decimal number $1,492$ in binary base is done by successive divisions by $2$ of residues and gives (the principle is about the same for all other bases):
	\begin{figure}[H]
		\centering		
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,510); %set diagram left start at 0, and has height of 510
		
		%Straight Lines [id:da4506461320143085] 
		\draw    (75,56) -- (75,335.5) ;
		%Straight Lines [id:da22476771259465744] 
		\draw    (2,79) -- (162,79) ;
		%Straight Lines [id:da23222318582797485] 
		\draw    (102,119.5) -- (329.56,339.11) ;
		\draw [shift={(331,340.5)}, rotate = 223.98] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da6316295791114859] 
		\draw    (102,139.5) -- (312.22,338.96) ;
		\draw [shift={(313.67,340.33)}, rotate = 223.5] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da5201055025462493] 
		\draw    (102,161.5) -- (294.53,339.14) ;
		\draw [shift={(296,340.5)}, rotate = 222.7] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da4821920614426827] 
		\draw    (102,183.67) -- (277.5,338.34) ;
		\draw [shift={(279,339.67)}, rotate = 221.39] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da6953023997720045] 
		\draw    (102,203.67) -- (262.8,338.38) ;
		\draw [shift={(264.33,339.67)}, rotate = 219.96] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da7337764197591361] 
		\draw    (102,224.33) -- (245.44,339.08) ;
		\draw [shift={(247,340.33)}, rotate = 218.66] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da21073668591749017] 
		\draw    (102,245) -- (229.4,339.81) ;
		\draw [shift={(231,341)}, rotate = 216.66] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da7055665901369654] 
		\draw    (102,267) -- (211.99,339.24) ;
		\draw [shift={(213.67,340.33)}, rotate = 213.29] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da9851330630990873] 
		\draw    (102,287.67) -- (194.59,339.36) ;
		\draw [shift={(196.33,340.33)}, rotate = 209.17] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da5412497222716586] 
		\draw    (102,307.67) -- (177.16,339.55) ;
		\draw [shift={(179,340.33)}, rotate = 202.99] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da25998799746669277] 
		\draw    (102,328.33) -- (156.37,339.27) ;
		\draw [shift={(158.33,339.67)}, rotate = 191.38] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		
		% Text Node
		\draw (6,57) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{Quotient}};
		% Text Node
		\draw (79,57) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{Residue}};
		% Text Node
		\draw (24,87) node [anchor=north west][inner sep=0.75pt]   [align=left] {$1492$};
		% Text Node
		\draw (32,109) node [anchor=north west][inner sep=0.75pt]   [align=left] {$746$};
		% Text Node
		\draw (32,131) node [anchor=north west][inner sep=0.75pt]   [align=left] {$373$};
		% Text Node
		\draw (33,153) node [anchor=north west][inner sep=0.75pt]   [align=left] {$186$};
		% Text Node
		\draw (41,175) node [anchor=north west][inner sep=0.75pt]   [align=left] {$93$};
		% Text Node
		\draw (41,196) node [anchor=north west][inner sep=0.75pt]   [align=left] {$46$};
		% Text Node
		\draw (41,217) node [anchor=north west][inner sep=0.75pt]   [align=left] {$23$};
		% Text Node
		\draw (41,237) node [anchor=north west][inner sep=0.75pt]   [align=left] {$11$};
		% Text Node
		\draw (48,257) node [anchor=north west][inner sep=0.75pt]   [align=left] {$5$};
		% Text Node
		\draw (48,279) node [anchor=north west][inner sep=0.75pt]   [align=left] {$2$};
		% Text Node
		\draw (48,299) node [anchor=north west][inner sep=0.75pt]   [align=left] {$1$};
		% Text Node
		\draw (48,319) node [anchor=north west][inner sep=0.75pt]   [align=left] {$0$};
		% Text Node
		\draw (88,109) node [anchor=north west][inner sep=0.75pt]   [align=left] {$0$};
		% Text Node
		\draw (88,131) node [anchor=north west][inner sep=0.75pt]   [align=left] {$0$};
		% Text Node
		\draw (88,153) node [anchor=north west][inner sep=0.75pt]   [align=left] {$1$};
		% Text Node
		\draw (88,175) node [anchor=north west][inner sep=0.75pt]   [align=left] {$0$};
		% Text Node
		\draw (88,196) node [anchor=north west][inner sep=0.75pt]   [align=left] {$1$};
		% Text Node
		\draw (89,217) node [anchor=north west][inner sep=0.75pt]   [align=left] {$0$};
		% Text Node
		\draw (88,238) node [anchor=north west][inner sep=0.75pt]   [align=left] {$1$};
		% Text Node
		\draw (88,259) node [anchor=north west][inner sep=0.75pt]   [align=left] {$1$};
		% Text Node
		\draw (88,279) node [anchor=north west][inner sep=0.75pt]   [align=left] {$1$};
		% Text Node
		\draw (89,300) node [anchor=north west][inner sep=0.75pt]   [align=left] {$0$};
		% Text Node
		\draw (88,320) node [anchor=north west][inner sep=0.75pt]   [align=left] {$1$};
		% Text Node
		\draw (145,343) node [anchor=north west][inner sep=0.75pt]   [align=left] {$($ \ $1$ \ $\;0$ \ $1$ \ $1$ \ $1$ \ $\;0$ \ $1$ \ $0$ \ $1$ \ $0$ \ $\;0$ \ $)_2$ = $(1492)_{10}$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Decimal to binary conversion}
	\end{figure}
	E2. To convert the number $142,713$ (decimal base) in duodecimal base (base twelve) we have (notation: $q$ is the "quotient", and $r$ is the "residue"):
	
	Thus we have the residues 6, 10, 7, 0, 9 which leads us to write:
	
	where we have chosen for this particular example the symbolism that we have previously defined ($a$-ten) to avoid any confusion.
	\end{tcolorbox}

	\pagebreak
	\subsection{Type of Numbers}\label{type of numbers}
	Now that we know that number is a mathematical object used to count, measure and label it must be know that it exists in mathematics a wide variety of numbers (natural, rational, real, irrational, complex, p-adic quaternions, transfinite, algebraic, constructibles, etc.) since any mathematician may at leisure create its own numbers just by defining axioms (rules) to manipulate them.
	
	However, there are a few of them that we find much more often than others through this book and some that serve as basic construction for others and which that should be defined sufficiently rigorously (without going to the extremes) in order to know what we will talk about when we will use them.

	\subsubsection{Natural Integer Numbers}\label{natural numbers}
	The idea of "\NewTerm{integer}\index{integer}" (the numbers for which there are no decimals) is the fundamental concept of mathematics and comes at the view of a group of objects of the same types (a sheep, another sheep, yet another sheep, etc.).

	When the amount of objects in a group is different from that of another group when the speak about a group that is numerically higher or lower regardless of the type of objects in these groups. When the amount of objects of one or multiples groups is equivalent, then we speak about "\NewTerm{equality}\index{equality}". 

	To each single object the number "\NewTerm{one}" or "\NewTerm{unit}\index{unit}" denoted by "$1$" in the decimal system will be used.

	To form groups of objects, we can operate as follows: to an object, add another object, then another, and so on... each of the clusters, from the point of view of its community, is characterized by a number. It follows from this that a number can be regarded as representing a group of units (single items) such that each unit corresponds to one single object of the collection.

\textbf{Definition (\#\thesection.\mydef):} Two numbers are said to be "\NewTerm{equal}\index{equal}" if each of the units of one we can match a unique unit of the other and vice versa (in a bijective way as seen in the section of Set Theory page \pageref{bijection}). If this does not hold true when we talk about "\NewTerm{inequality}\index{inequality}".

Let us take an object, then another, then to the formed group add again an object and so on. The groups thus formed are characterized by numbers which, taken in the same order as the groups successively obtained, are the "\NewTerm{natural sequence $\mathbb{N}$}\index{natural sequence}" or "\NewTerm{natural integers}\index{natural integers}", also sometimes named "\NewTerm{whole numbers}\index{whole numbers}", and denoted by:
	
To be unambiguous about whether $0$ is included or not, sometimes an index (or superscript) is added in the former case:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
The presence of the $0$ (zero) in our definition above of $\mathbb{N}$ is debatable since $0$ is neither positive nor negative. That is why in many books you will find a definition of $\mathbb{N}$ without the $0$ (hence corresponding to $\mathbb{Z}^+$) and therefore the set $\mathbb{N}\cup \{0\}$ (also sometimes denoted $\mathbb{Z}_{\geq 0}$ or $\mathbb{N}_0$) is named the "calculative set" or "whole numbers". But keep in mind that there seems to be no general agreement and it is actually not defined in the ISO 80000 norm series!!!
	\end{tcolorbox}

The components of this natural set can be defined by (we own this definition to the mathematician Frege Gottlob) the following the properties (having read first the section on Set Theory page \pageref{set theory} is strongly recommended...):
	\begin{enumerate}
		\item[P1.] $0$ (read "zero") is the number of elements (defined as an equivalence relation) of all sets equivalent to (in bijection with) the empty set.
		
		\item[P2.] $1$ (read "one") is the number of elements of all sets equivalent to the set whose only element is $1$.
		
		\item[P3.] $2$ (read "two") is the number of elements of all sets equivalent to the set whose only element are $1$ and $2$.
		
		\item[P4.] In general, an integer is the number of elements of all sets equivalent to the set of integers preceding it!
	\end{enumerate}
	The construction of the set of natural numbers is made of the most natural and consistent manner. Natural numbers get their name from what they were, in the beginnings of their existence, to count quantities and things of nature or intervened in human life. The originality of this set lies in the empirical way he has  been built since it does not actually the result of a mathematical definition, but more by awareness of the human by the concept of countable quantity, of number and operations that reflect the relations between them.

	The question about the origin of $\mathbb{N}$ is therefore the question of the origin of mathematics. And since thousands of years debates confronting the thoughts of the greatest philosophical minds have attempted to elucidate this deep mystery as to whether mathematics is a pure creation of the human mind or whether the man has only rediscovered a science that already existed in nature. Besides the many philosophical questions that the set of Natural numbers can generate, it is nonetheless interesting from an exclusively mathematical point of view. Because of its structure, it has remarkable properties that can be very useful when we practice some given reasoning or calculations.

	The sequence of natural numbers $\mathbb{N}$ is unlimited (\SeeChapter{see section Numbers page \pageref{natural numbers}}) but countable (we will this property in details below), because in a group of objects that is represented by a number $n$, it will be enough to add an object to get another group that will be defined by the integer $n + 1$.

\textbf{Definition (\#\thesection.\mydef):} Two integers that differs from a single positive unit are said to be "\NewTerm{consecutive}\index{consecutive numbers}".

	\pagebreak
	\paragraph{Peano axioms}\label{peano axioms}\mbox{}\\\\
During the crisis of foundations of mathematics, mathematicians have obviously sought to axiomatize the set $\mathbb{N}$ and we own the actual axiomatisation to Peano and Dedekind.

The axioms of this system include the symbols $<$ and $=$ to represent the relations "smaller than" and "equal to" (\SeeChapter{see section Operators page \pageref{comparators}}). They include also the symbols "$0$" for the number zero and $s$ to represent the "successor" number. In this system, $1$ is denoted by:
	
named "successor to zero" and $2$ is denoted by:
	
The Peano axioms that builds $\mathbb{N}$ are (see section of Proof Theory page \pageref{proof theory} for details on some of the symbols used below):
	\begin{enumerate}
		\item[A1.] $0$ is a natural number (this permits $\mathbb{0}$ to be not empty).
		\item[A2.] Every natural number $n$ has a successor, denoted by $s(n)$.
		
		Therefore $s$ is an injective application (\SeeChapter{see section Set Theory page \pageref{injective}}), that is to say:
		
		That is to say that if two successors are equal, they are the successors of the same number.
		\item[A3.] The successor of a natural number is never zero (therefore $\mathbb{N}$ has a first element):
		
		\item[A4.] If we prove a property $\varphi$ that is true for $x$ and its successor $s(x)$, then this property is true for any $x$ (this is named the "\NewTerm{axiom of recurrence}\index{axiom of recurrence}"):
		
		So the set of all the numbers satisfying the four above axioms is denoted by:
	\end{enumerate}
	So the set of all the numbers satisfying the four above axioms is denoted by:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The Peano axioms allow to build very rigorously the two basic operations of arithmetic in $\mathbb{N}$ that are addition and multiplication (\SeeChapter{see section Arithmetic Operators page \pageref{addition} and page \pageref{addition}}) and so all the other sets that we will see later (subtraction in $\mathbb{N}$ can not be applied because it can give negative numbers).
	\end{tcolorbox}
	
	\pagebreak
	\paragraph{Odd, Even and Perfect Numbers}\mbox{}\\\\
	In arithmetic, study the parity of an integer, its determiner if this integer is or not a multiple of $2$. An integer multiple of $2$ is an even integer, the others are odd integers.
	
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] The numbers obtained by counting by step of $2$ from zero (i.e.. $0, 2, 4, 6, 8, \ldots$) in the set of natural integer numbers $\mathbb{N}$ are named "\NewTerm{even numbers}\index{even numbers}".
		
		The $n^{\text{th}}$ even number is obviously given by the relation:
		
		Or more aesthetically:
		

		\item[D2.] The numbers we get by counting by step of $2$ starting from $1$ (i.e. $1, 3, 5, 7, \ldots$) in the set of natural integer numbers $\mathbb{N}$ are named "\NewTerm{odd numbers}\index{odd numbers}".
		
		The $(n+1)^{\text{th}}$ even number is almost obviously given by the relation:
		
		Or more aesthetically:
		
	\end{enumerate}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	We name "\NewTerm{perfect numbers}\index{perfect numbers}", numbers equal to the sum of their integer divisors strictly smaller than themselves (concept we will see in detail later) such as: $6 = 1 + 2 + 3$ and $28 + 1 = 2 + 4 + 7 + 14$.
	\end{tcolorbox}
	
	\paragraph{Prime Numbers}\mbox{}\label{prime number}\\\\
	\textbf{Definition (\#\thesection.\mydef):} A "\NewTerm{prime number}\index{prime number}" is an integer with exactly two positive divisors (these divisors are both: "1" and the number itself). In the case where there are more than two dividers it is named a "\NewTerm{composite number}\index{composite number}". The property of being prime (or not) is named "\NewTerm{primality}\index{primality}".
	
	The study of prime numbers is a huge subject in mathematics (see for a small example the section of Number Theory page \pageref{fundamental theorem of arithmetic} or of Cryptography page \pageref{cryptography}). There are books of thousands of pages on the subject and probable hundreds of research article per month even nowadays. Most theorems are largely out of the study of this book (and out of the interest of its main author...)!
	
	Here is the set of prime numbers less than $1000$:
	
	2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61,
	 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 
	 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 
	 223, 227, 229, 233, 239, 241, 251, 257, 263, 269, 271, 277, 281, 283, 
	 293, 307, 311, 313, 317, 331, 337, 347, 349, 353, 359, 367, 373, 379, 
	 383, 389, 397, 401, 409, 419, 421, 431, 433, 439, 443, 449, 457, 461, 
	 463, 467, 479, 487, 491, 499, 503, 509, 521, 523, 541, 547, 557, 563, 
	 569, 571, 577, 587, 593, 599, 601, 607, 613, 617, 619, 631, 641, 643, 
	 647, 653, 659, 661, 673, 677, 683, 691, 701, 709, 719, 727, 733, 739, 
	 743, 751, 757, 761, 769, 773, 787, 797, 809, 811, 821, 823, 827, 829, 
	 839, 853, 857, 859, 863, 877, 881, 883, 887, 907, 911, 919, 929, 937, 
	 941, 947, 953, 967, 971, 977, 983, 991, 997
	
	The whole set of prime numbers is sometimes denoted by $\mathbb{P}$.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Note that the primes numbers set does not include the number "$1$" because it has a only a single divider (himself) and not two as is the definition.
	\end{tcolorbox}
	
	We can ask ourselves if there are infinitely many prime numbers? The answer is YES and here is a proof (among others) by contradiction.
	
	\begin{dem}
	Suppose that there is a finite number of prime numbers that would be denoted by:
	
	We create a new number from the product of this prime number to which we add "1":
	
	According to our initial hypothesis and the fundamental theorem of arithmetic (\SeeChapter{see section Number Theory page \pageref{fundamental theorem of arithmetic}}) the new number $N$ should be divisible by one of the existing prime $p_i$ such that we can write:
	
	where $q$ is an integer. We can make the division:
	
	The first term is simplified as $p_i$ is in the product. Let us note the resulting integer $E$:
	
	But, $q$ and $E$ are integers, so $1/p_i$ should be an integer. But $p_i$ is by definition greater than $1$. So $1/p_i$ is not an integer and so is also $q$.
	
	Then there is contradiction, and we can conclude that the prime numbers are not finite but are infinite.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	
	\end{dem}
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} The product $p_n=p_1p_2\ldots p_n$ of the indexed prime numbers $\leq n$ is named the "\NewTerm{$n$-th primorial}\index{$n$-th primorial}".\\
	
	\textbf{R2.} We send the reader to the section Cryptography of the chapter on Theoretical Computing page \pageref{cryptography} (or Number Theory section of the chapter Arithmetic page \pageref{euler indicator function}) for the study of some remarkable properties of prime numbers including the famous Euler $\phi$ function (also named "indicator function") and a 120th-121th century (holocene calendar) industrial application of prime numbers.
	\end{tcolorbox}
	
	\subsubsection{Relative Integer Numbers}
	The set of natural integers $\mathbb{N}$ has a few issues that we did not set out earlier. For example, subtracting two numbers into $\mathbb{N}$ does not always have a result in $\mathbb{N}$ (negative numbers not existing in this set). Other issue... dividing two numbers in $\mathbb{N}$ also does not always have a result in $\mathbb{N}$ (fractional numbers - rational or irrational - not existing in this set). We then say in the language of set theory that: the subtraction and division is not an internal operation of $\mathbb{N}$.
	
	We can first resolve the problem of subtraction by adding to the set of natural numbers $\mathbb{N}$, negative integers (revolutionary concept for those who where behind this concept at their time!) to get the set of "\NewTerm{relative integers}\index{relative integers}" denoted by $\mathbb{Z}$ (for "Zahl" from German, meaning "Number"):
	
	
	The set of natural integers is therefore included in the set of relative integers. This is what we denote by (\SeeChapter{see section Set Theory page \pageref{subset}}):
	
	and we have by definition (it is a notations to be learned!!!):
	
	This set was originally created to make the natural numbers an object that we name a "\NewTerm{group}" (\SeeChapter{see section Set Theory page \pageref{groups}}) relatively to the addition.
	
	\textbf{Definition (\#\thesection.\mydef):} We say that a set $A$ is a "\NewTerm{countable set}\index{countable set}", if it is equipotent to $\mathbb{N}$. That is to say if there is a bijection (\SeeChapter{see section Set Theory page \pageref{bijection}}) of $S$ on $\mathbb{N}$. Thus, roughly said, two equipotent sets have the same number of elements in the meaning of their cardinal (\SeeChapter{see section Set Theory page \pageref{cardinal}}), or at least the same infinity.
	
	The purpose of this concept is to understand that the sets $\mathbb{N}$ and $\mathbb{Z}$ are countable.
	
	\begin{dem}
		Let us show that $\mathbb{Z}$ is countable by writing:
		
		for any integer $k\geq 0$. This gives the following ordered list:
		
		of all relative integers from natural integers only!
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
	\end{dem}

	\subsubsection{Rational Numbers}
	The set of relative integers $\mathbb{Z}$ also still has an issue. Dividing two numbers in $\mathbb{Z}$ also does not always have a result in $\mathbb{Z}$ (fraction numbers - rational or irrational - not existing in this set). We then say in the language of set theory that: the division is not an internal operation of $\mathbb{Z}$.
	
	We can thus define a new set that contains all the numbers which can be written as a "fraction" that is to say the ratio of a dividend (numerator) and a divider (denominator). When a number can be written in this form, we say that it is a "\NewTerm{fractional number}\index{fractional number}":
	
	A fraction can be used to express a part or fraction of something (of an object, of a distance, of a land, of an amount of money, of a cake...).
	
	To better understand rational number (fractions) let us consider two individuals: Andy and Bobby that bot love pizza. On Monday night, they share a pizza equally. How much of the pizza does each one get? Are you thinking that each boy gets half of the pizza? That's right. There is one whole pizza, evenly divided into two parts, so each boy gets one of the two equal parts. In math, we write  $\dfrac{1}{2}$ to mean one out of two parts:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/arithmetics/pizza_fraction_example_01.jpg}
		\caption[A $1/2$ pizza fraction example]{A $1/2$ pizza fraction example (source: OpenStax)}
	\end{figure}
	On Tuesday, Andy and Bobby share a pizza with their parents, Fred and Christy, with each person getting an equal amount of the whole pizza. How much of the pizza does each person get? There is one whole pizza, divided evenly into four equal parts. Each person has one of the four equal parts, so each has $\dfrac{1}{4}$ of the pizza:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/arithmetics/pizza_fraction_example_02.jpg}
		\caption[A $1/4$ pizza fraction example]{A $1/4$ pizza fraction example (source: OpenStax)}
	\end{figure}
	On Wednesday, the family invites some friends over for a pizza dinner. There are a total of $12$ people. If they share the pizza equally, each person would get $\dfrac{1}{12}$ of the pizza:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/arithmetics/pizza_fraction_example_03.jpg}
		\caption[A $1/12$ pizza fraction example]{A $1/12$ pizza fraction example (source: OpenStax)}
	\end{figure}
	
	By definition, the "\NewTerm{set of rational numbers}\index{rational numbers}\label{rational numbers}" is given by:
	
	
	In other words, any rational number is any number that can be expressed as the quotient or fraction $p/q$ of two integers, $p$ and $q$, with the denominator $q$ not equal to zero. Since $q$ may be equal to $1$, every integer is a rational number.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Pure mathematicians will argue that the definition above is wrong. Yes indeed it is not exact but good enough for engineers and physicists. So here is the right one:
	
	Where $q>0$ avoids the division by zero AND the repetition of the sign "$-$" above or below the fraction. We have also added an important property: the greatest common divisor (\SeeChapter{see section Number Theory page \pageref{greatest common divisor}}) between the both number must be equal to zero. Thus in the purpose that we consider, for example, $3/6$ and $1/2$ are not two distinct numbers. In fact we know that we can always bring a fraction to its irreducible form, where in the latter case we will have $\text{gcd}(p,q)=1$.
	\end{tcolorbox}
	We also assume as obvious that:
	
	The logic of the creation of the set of rational numbers $\mathbb{Q}$ is similar to that of relative integers $\mathbb{Z}$. Indeed, mathematicians wanted to make of the set of relative numbers $\mathbb{Z}$ a "group" with respect to the laws of multiplication and division (\SeeChapter{see section Set Theory page \pageref{multiplication} and page \pageref{division}}).
	
	Moreover, contrary to the intuition of most people, the set of natural integers $\mathbb{N}$ and rational numbers $\mathbb{Q}$ are equipotent\label{natural and rational numbers equipotence} (i.e. $\mathbb{Q}\simeq \mathbb{N}$). We can convince ourselves of this equipotence by ranking, as Cantor did, rational numbers in a first time as follows:
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}
	\matrix(m)[matrix of math nodes,column sep=0.5cm,row sep=0.5cm]{
	    1/1 & 2/1 & 3/1 & 4/1 & 5/1 & 6/1 & 7/1 & \cdots \\
	    1/2 & 2/2 & 3/2 & 4/2 & 5/2 & 6/2 & 7/2 & \cdots \\
	    1/3 & 2/3 & 3/3 & 4/3 & 5/3 & 6/3 & 7/3 & \cdots \\
	    1/4 & 2/4 & 3/4 & 4/4 & 5/4 & 6/4 & 7/4 & \cdots \\
	    1/5 & 2/5 & 3/5 & 4/5 & 5/5 & 6/5 & 7/5 & \cdots \\
	    1/6 & 2/6 & 3/6 & 4/6 & 5/6 & 6/6 & 7/6 & \cdots \\
	    1/7 & 2/7 & 3/7 & 4/7 & 5/7 & 6/7 & 7/7 & \cdots \\
	    \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \cdots \\
	};
	
	\draw[->]
	         (m-1-1)edge(m-1-2)
	         (m-1-2)edge(m-2-1)
	         (m-2-1)edge(m-3-1)
	         (m-3-1)edge(m-2-2)
	         (m-2-2)edge(m-1-3)
	         (m-1-3)edge(m-1-4)
	         (m-1-4)edge(m-2-3)
	         (m-2-3)edge(m-3-2)
	         (m-3-2)edge(m-4-1)
	         (m-1-5)edge(m-1-6)
	         (m-1-7)edge(m-1-8)
	         
	         (m-2-4)edge(m-1-5)
	         (m-3-3)edge(m-2-4)
	         (m-4-2)edge(m-3-3)
	         (m-5-1)edge(m-4-2)
	         
	         (m-5-2)edge(m-6-1)
	         (m-4-3)edge(m-5-2)
	         (m-3-4)edge(m-4-3)
	         (m-2-5)edge(m-3-4)
	         (m-1-6)edge(m-2-5)
	         
	         (m-2-6)edge(m-1-7)
	         (m-3-5)edge(m-2-6)
	         (m-4-4)edge(m-3-5)
	         (m-5-3)edge(m-4-4)
	         (m-6-2)edge(m-5-3)
	         (m-7-1)edge(m-6-2)
	         
	         (m-1-8)edge(m-2-7)
	         (m-2-7)edge(m-3-6)
	         (m-3-6)edge(m-4-5)
	         (m-4-5)edge(m-5-4)
	         (m-5-4)edge(m-6-3)
	         (m-6-3)edge(m-7-2)
	         (m-7-2)edge(m-8-1)
	         
	         (m-3-7)edge(m-2-8)
	         (m-4-6)edge(m-3-7)
	         (m-5-5)edge(m-4-6)
	         (m-6-4)edge(m-5-5)
	         (m-7-3)edge(m-6-4)
	         (m-8-2)edge(m-7-3)
	         
	         (m-3-8)edge(m-4-7)
	         (m-4-7)edge(m-5-6)
	         (m-5-6)edge(m-6-5)
	         (m-6-5)edge(m-7-4)
	         (m-7-4)edge(m-8-3)
	         
	         (m-5-7)edge(m-4-8)
	         (m-6-6)edge(m-5-7)
	         (m-7-5)edge(m-6-6)
	         (m-8-4)edge(m-7-5)
	         
	         (m-5-8)edge(m-6-7)
	         (m-6-7)edge(m-7-6)
	         (m-7-6)edge(m-8-5)
	         
	         (m-7-7)edge(m-6-8)
	         (m-8-6)edge(m-7-7)
	         
	         (m-7-8)edge(m-8-7)
	         
	         (m-4-1)edge(m-5-1)
	         (m-6-1)edge(m-7-1);
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Cantor diagonal method}
	\end{figure}
	This table is constructed so that each rational number appears only once (in the sense of its decimal value) by diagonal hence the name of the method: "\NewTerm{Cantor's diagonal}\index{Cantor's diagonal}\label{Cantor's diagonal}".
	
	If we eliminate from each diagonal the rational numbers that appear more than one time (the "\NewTerm{equivalent fractions}\index{equivalent fractions}") in order to keep only those who are irreducible (i.e. those with the greatest common divisor of the numerator and denominator is equal to $1$), then we can with this distinction define an application $f:\mathbb{N} \Rightarrow \mathbb{Q}$ that is injective (two distinct rational numbers have distinct ranks) and surjective (at any place will be written a rational number).
	
	The application $f$ is therefore bijective: $\mathbb{N}$ and $\mathbb{Q}$ are then effectively equipotent!
	
	The definition a little bit more rigorous (and therefore less funny) of $\mathbb{Q}$ from $\mathbb{Z}$ is as follows (it is interesting to see the notation used):
	
	On the set $\mathbb{Z}\times \mathbb{Z} \ \lbrace 0 \rbrace$, which should be read as the set constructed from two relative integer whose zero is excluded from the second one, we consider the relation $\mathcal{R}$ between two relative pairs of integers defined by:
	
	We then easily verify that $R$ is an equivalence relation (\SeeChapter{see section Operators page \pageref{equivalence relation}}) on $\mathbb{Z}\times \mathbb{Z} \setminus \lbrace 0 \rbrace$.
	
	The set of equivalence classes for this relation $\mathcal{R}$ denoted then $\left(\mathbb{Z}\times \mathbb{Z} \setminus \lbrace 0 \rbrace\right) / \mathcal{R}$ is by definition $\mathbb{Q}$. That is to say that we write therefore more rigorously:
	
	The equivalence class $(a,b) \in \mathbb{Z}\times\mathbb{Z}\setminus \{0\}$ is explicitly denoted by:
	
	in accordance with the notation that everyone is accustomed to use.

	We easily check the addition and multiplication operations that were operations defined on $\mathbb{Z}$ pass without problems to $\mathbb{Q}$ by writing:
	
	Moreover, these operations provide $\mathbb{Q}$ with the structure of a field (\SeeChapter{see section Set Theory page \pageref{field (set)}}) with $\dfrac{0}{1}$ as neutral element for the addition and $\dfrac{1}{1}$ as neutral element for the multiplication. Thus, any non-zero element of $\mathbb{Q}$ is reversible, in fact:
	
	what is written also more technically:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Even if we want to define $\mathbb{Q}$ as the being the set $\mathbb{Z}\times \mathbb{Z}\setminus{0}$ where $\mathbb{Z}$ represents the numerators and $\mathbb{Z}\setminus{0}$ the denominators of the rationals, this is not possible because otherwise we would for example $(1,2)\neq(2,4)$ while we expect for an equality.\\
	
	Hence the need to introduce an equivalence relation which enables us to identify, to return to the previous example, with $(1,2)$ and $(2,4)$. The relation $\mathcal{R}$ that we have defined does not fall from heaven, indeed the reader who handled the rational so far without ever having seen their formal definition knows that:
	
	It is therefore almost natural to define the relation $\mathcal{R}$ as we have done. In particular, regarding the above example, $\dfrac{1}{2}=\dfrac{2}{4}$ because $(1,2) \mathcal{R} (2,4)$ and the problem is solved.
	\end{tcolorbox}
	In addition to the historical circumstances of its establishment, this new entity (set) is distinguished from relative numbers because it induces the original and paradoxical concept of partial quantities. This notion that a priori does not make sense, find its place in the mind of man thanks to the geometry where the idea of fraction of length, of proportion are illustrated more intuitively.
	\pagebreak
	\subsubsection{Irrational Numbers}
	It can seem obvious to present irrational numbers before real number (see further below) but this can be explained by the fact that it is the order of the discovering in the human history and therefore it seems more pedagogical to us to present them in this order.
	
	So the set of rational $\mathbb{Q}$ is limited and sadly not sufficient too. Indeed, we may think that all mathematical computation with commonly known operations are reduced to this set but it is not the case!
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. Let us calculate the square root of two which we denote $\sqrt{2}$ (thing to Pythagorean theorem with a triangle of side $1$ and $1$ then the third one is of size $\sqrt{2}$). Suppose it is a rational root. So if this is truly a rational, we should be able to express it as $a / b$, where by the definition of a rational $a$ and $b$ are integers with no common factors. For this reason, $a$ and $b$ can not both be even numbers. There are three remaining possibilities:
	\begin{enumerate}
		\item $a$ is odd (then  $b$ is even)
		\item $a$ is even (then  $b$ is odd)
		\item $a$ is odd (then  $b$ is odd)
	\end{enumerate}
	By squaring, we have:
	
	That can be written:
	
	Since the square of an odd number is odd and the square of an even number is even, the case (1) is not possible because $a^2$ would be odd and $2b^2$ would be even.\\
	
	If case (2) is also impossible, because then we could write $a=2c$, where $c$ is any integer, and so if we take the square then we have $a^2=4c^2$ that is to say an even number on both sides of equality. Substituting in $2b^2=a^2$ we obtain after simplification that $b^2=2c^2$. Then $b^2$ would be odd while $2c^2$ would even.\\
	
	The case (3) is also impossible because $a^2$ is then odd and $2b^2$ is even (that $b$ is even or odd!).\\
	
	There is the no solutions! That is to say that the start assumption is false and there does not two integers $a$ and $b$ such that $\sqrt{2}=a/b$.\\

	E2. Let us prove by contradiction, that the famous Euler number $e$ is irrational. To do this, remember that $e$ (\SeeChapter{see section Functional Analysis page \pageref{Euler number}}) can also be defined by the Taylor series (\SeeChapter{see section Sequences and Series page \pageref{euler maclaurin expansion}}):
		
	Then if $e$ is rational, it could be written in the form $p / q$ (with $q>1$, because we know that is not an integer). Let us multiply both sides of the equality by $q!$:	
	
	The first member $q!e$ would then be an integer, because by definition of the factorial:
	
	is an integer.\\
	
	The first terms of the second member of the previous prior-previous relation, until the term $q! /q! = 1$ are also integer because $q! /m!$ is simplified if $q> m$. So by subtraction we find:
	
	when the right sequences should be an integer!\\
	
	After simplification, the second member of the equality becomes:
	
	the first term in this sum is strictly less than $1/2$, the second strictly less than $1/4$ second, the third strictly less than $1/8$, etc.\\
	
	So, since each term is strictly less than the following harmonic series which converges to $1$:
	
	then therefore the sequence is not an integer as being strictly less than $1$. This is a contradiction!
	\end{tcolorbox}
	Thus, the rational numbers do not satisfy the numerical expression of $\sqrt{2}$ and $e$ (to cite only these two particular examples).
	
	They must therefore be complemented by the set of all numbers that can not be written as a fraction (the ratio of an integer dividend and an integer divisor without common factors) and that we name "irrational numbers".
	Finally we can say that:
	
	\textbf{Definition (\#\thesection.\mydef):} In mathematics, an "\NewTerm{irrational number}\index{irrational number}" is any real number that cannot be expressed as a ratio of integers. Irrational numbers cannot be represented as terminating or repeating decimals.
	
	\subsubsection{Real Numbers}
	\textbf{Definition (\#\thesection.\mydef):} The union of rational and irrational numbers gives the set of "\NewTerm{real numbers}\index{real numbers}" that we denote by:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Mathematicians in their usual rigour have different techniques to define real numbers. They use the properties of topology (among others) and especially Cauchy sequences but that's another story that goes beyond the formal scope of this section. For a "set point of view" definition of $\mathbb{R}$ (the real numbers are often described as "the complete ordered field") the reader should report to the section on Set Theory  page \pageref{field (set)}.
	\end{tcolorbox}
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,525); %set diagram left start at 0, and has height of 525
		
		%Shape: Ellipse [id:dp9872399707179429] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (3,237) .. controls (3,112.46) and (142.69,11.5) .. (315,11.5) .. controls (487.31,11.5) and (627,112.46) .. (627,237) .. controls (627,361.54) and (487.31,462.5) .. (315,462.5) .. controls (142.69,462.5) and (3,361.54) .. (3,237) -- cycle ;
		%Shape: Ellipse [id:dp867987242775925] 
		\draw  [fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=1 ] (3,237) .. controls (3,136.21) and (141.57,54.5) .. (312.5,54.5) .. controls (483.43,54.5) and (622,136.21) .. (622,237) .. controls (622,337.79) and (483.43,419.5) .. (312.5,419.5) .. controls (141.57,419.5) and (3,337.79) .. (3,237) -- cycle ;
		%Shape: Ellipse [id:dp06632596810097846] 
		\draw  [fill={rgb, 255:red, 137; green, 162; blue, 243 }  ,fill opacity=1 ] (3,237) .. controls (3,163.55) and (140.45,104) .. (310,104) .. controls (479.55,104) and (617,163.55) .. (617,237) .. controls (617,310.45) and (479.55,370) .. (310,370) .. controls (140.45,370) and (3,310.45) .. (3,237) -- cycle ;
		%Shape: Circle [id:dp7416699092044479] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (497,240) .. controls (497,210.73) and (520.73,187) .. (550,187) .. controls (579.27,187) and (603,210.73) .. (603,240) .. controls (603,269.27) and (579.27,293) .. (550,293) .. controls (520.73,293) and (497,269.27) .. (497,240) -- cycle ;
		%Shape: Ellipse [id:dp14261940613389235] 
		\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (10,238.5) .. controls (10,175.26) and (114.09,124) .. (242.5,124) .. controls (370.91,124) and (475,175.26) .. (475,238.5) .. controls (475,301.74) and (370.91,353) .. (242.5,353) .. controls (114.09,353) and (10,301.74) .. (10,238.5) -- cycle ;
		%Shape: Circle [id:dp31660386163059195] 
		\draw  [fill={rgb, 255:red, 184; green, 233; blue, 134 }  ,fill opacity=1 ] (361,238) .. controls (361,208.73) and (384.73,185) .. (414,185) .. controls (443.27,185) and (467,208.73) .. (467,238) .. controls (467,267.27) and (443.27,291) .. (414,291) .. controls (384.73,291) and (361,267.27) .. (361,238) -- cycle ;
		%Shape: Ellipse [id:dp01584757368614631] 
		\draw  [fill={rgb, 255:red, 245; green, 166; blue, 35 }  ,fill opacity=1 ] (19,241) .. controls (19,189.64) and (92.43,148) .. (183,148) .. controls (273.57,148) and (347,189.64) .. (347,241) .. controls (347,292.36) and (273.57,334) .. (183,334) .. controls (92.43,334) and (19,292.36) .. (19,241) -- cycle ;
		%Shape: Ellipse [id:dp1262397713575285] 
		\draw  [fill={rgb, 255:red, 80; green, 227; blue, 194 }  ,fill opacity=1 ] (63,255.5) .. controls (63,214.91) and (113.59,182) .. (176,182) .. controls (238.41,182) and (289,214.91) .. (289,255.5) .. controls (289,296.09) and (238.41,329) .. (176,329) .. controls (113.59,329) and (63,296.09) .. (63,255.5) -- cycle ;
		%Shape: Ellipse [id:dp47042959394657524] 
		\draw  [fill={rgb, 255:red, 189; green, 16; blue, 224 }  ,fill opacity=1 ] (82,267.5) .. controls (82,236.85) and (123.41,212) .. (174.5,212) .. controls (225.59,212) and (267,236.85) .. (267,267.5) .. controls (267,298.15) and (225.59,323) .. (174.5,323) .. controls (123.41,323) and (82,298.15) .. (82,267.5) -- cycle ;
		%Shape: Ellipse [id:dp019477563171045764] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (108,281) .. controls (108,261.12) and (137.77,245) .. (174.5,245) .. controls (211.23,245) and (241,261.12) .. (241,281) .. controls (241,300.88) and (211.23,317) .. (174.5,317) .. controls (137.77,317) and (108,300.88) .. (108,281) -- cycle ;
		%Shape: Ellipse [id:dp6448303857682405] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (127,298) .. controls (127,288.61) and (148.27,281) .. (174.5,281) .. controls (200.73,281) and (222,288.61) .. (222,298) .. controls (222,307.39) and (200.73,315) .. (174.5,315) .. controls (148.27,315) and (127,307.39) .. (127,298) -- cycle ;
		
		% Text Node
		\draw (450,140) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{46.94pt}\setlength\topsep{0pt}
		\begin{center}
		Complex \\numbers $\mathbb{C}$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (515,206) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{46.94pt}\setlength\topsep{0pt}
		\begin{center}
		Imaginary\\numbers
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (191,131) node [anchor=north west][inner sep=0.75pt]   [align=left] {Real numbers $\mathbb{R}$};
		% Text Node
		\draw (385,207) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{55pt}\setlength\topsep{0pt}
		\begin{center}
		Irrational\\numbers $\mathbb{I}$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (121,160) node [anchor=north west][inner sep=0.75pt]   [align=left] {Rational numbers $\mathbb{Q}$};
		% Text Node
		\draw (118,195) node [anchor=north west][inner sep=0.75pt]   [align=left] {Integers $\mathbb{Z}\;\ldots,-2,$\\
		$\qquad\qquad\qquad\quad -1,0,$\\
		$\qquad\qquad\qquad\qquad\;\; 1,2,$\\
		$\qquad\qquad\qquad\qquad\quad\;\; \ldots$};
		% Text Node
		\draw (116,226) node [anchor=north west][inner sep=0.75pt]   [align=left] {Whole numbers $\mathbb{W}$};
		% Text Node
		\draw (121,262) node [anchor=north west][inner sep=0.75pt]   [align=left] {\footnotesize Natural $\mathbb{N}\;1,2,3,\ldots$ };
		% Text Node
		\draw (128,292) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\footnotesize Prime $\mathbb{P}\;2, 3, 5,\ldots$}};
		% Text Node
		\draw (265,60) node [anchor=north west][inner sep=0.75pt]   [align=left] {Quaternions $\mathbb{H}$};
		% Text Node
		\draw (285,16) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textcolor[rgb]{1,1,1}{Octonions $\mathbb{O}$}};
		% Text Node
		\draw (252,80) node [anchor=north west][inner sep=0.75pt]   [align=left] {$a+b\vec{i}+c\vec{j}+d\vec{k}$};
		% Text Node
		\draw (416,314) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\ldots,-3-4\mathrm{i},2+\mathrm{i},\ldots$};
		% Text Node
		\draw (516,260) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\ldots,\mathrm{i},5\mathrm{i},\ldots$};
		% Text Node
		\draw (310,148) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\ldots,-4,-\pi,$\\$-1,4,-1/3,0,$\\$\quad\sqrt{2},\ldots$};
		% Text Node
		\draw (383,240) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\ldots,\sqrt{2},e,$\\$\pi,\varphi,\ldots$};
		% Text Node
		\draw (289,210) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\ldots,-2.5,$\\$-1,-1/3,$\\$0,1/3,$\\$1,2.5$\\$\ldots$};
		% Text Node
		\draw (233,242) node [anchor=north west][inner sep=0.75pt]   [align=left] {$0,1,$\\$2,3,$\\$\;\;\ldots$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[Main number sets summary]{Simple number sets summary\\ (transcendent and algebraic numbers are not represented)}
	\end{figure}
	Obviously we are led to ask ourselves whether $\mathbb{R}$ is countable or not. In other words: can pure science - ie mathematics - quantify the unquantifiable? The answer and the proof is quite simple.
	
	\begin{dem}
	By definition, we have seen above that there must be a bijective correspondence between $\mathbb{Q}$ and $\mathbb{R}$ to that $\mathbb{R}$ is countable.
	
	For simplicity, we will show that the interval $[0,1[$ is then not countable. This will involve of course by extension that $\mathbb{R}$ is not countable!
	
	The elements of this interval are represented by infinite sequences between $0$ and $9$ (in the decimal system):
	\begin{itemize}
		\item Some of these suites are zero from starting from a given rank, some not.
		
		\item So we can identify $[0,1[$ to the set of all sequences (finite or infinite) of integers between $0$ and $9$.
		\begin{table}[H]\centering
		\begin{tabular}{cccccccc}\hline
		$n^\circ 1 $ & $x_{11}$ & $x_{12}$ & $x_{13}$ & $x_{14}$ & $\cdots$ & $x_{1p}$ & $\cdots$ \\
		$n^\circ 2 $ & $x_{21}$ & $x_{22}$ & $x_{23}$ & $x_{24}$ & $\cdots$ & $x_{2p}$ & $\cdots$ \\
		$n^\circ 3 $ & $x_{31}$ & $x_{32}$ & $x_{33}$ & $x_{34}$ & $\cdots$ & $x_{3p}$ & $\cdots$ \\
		$n^\circ 4 $ & $x_{41}$ & $x_{42}$ & $x_{43}$ & $x_{44}$ & $\cdots$ & $x_{4p}$ & $\cdots$ \\
		$n^\circ 5 $ & $x_{51}$ & $x_{52}$ & $x_{53}$ & $x_{54}$ & $\cdots$ & $x_{5p}$ & $\cdots$ \\
		$n^\circ 6 $ & $x_{61}$ & $x_{62}$ & $x_{63}$ & $x_{64}$ & $\cdots$ & $x_{6p}$ & $\cdots$ \\
		 & $\cdots$ & &  &  &  &  &  \\
		 &  & $\cdots$ &  &  &  &  &  \\
		 &  &  & $\cdots$ &  &  &  &  \\
		$n^\circ k $ & $x_{k1}$ & $x_{k2}$ & $x_{k3}$ & $x_{k4}$ & $\cdots$ & $x_{kp}$ & $\cdots$ \\
		 &  &  &  &  &  & $\cdots$ & \\ \hline
		\end{tabular}
		\end{table}
		If this set was countable, we could classify these sequences (with a first, second, etc.). Thus, the sequence $x_{11}x_{12}x_{13}x_{14}\ldots x_{1p}\ldots$ would be classified first and so on ... as proposed in the above table.
		
		We could then edit this infinite matrix as follows: to each element of the diagonal, we add $1$, according to the rule: $0 + 1 = 1, 1 + 1 = 2, 8 + 1 = 9$ and $9 + 1 = 0$:

		\begin{table}[H]\centering
		\begin{tabular}{cccccccc}\hline
		$n^\circ 1 $ & $x_{11}+1$ & $x_{12}$ & $x_{13}$ & $x_{14}$ & $\cdots$ & $x_{1p}$ & $\cdots$ \\
		$n^\circ 2 $ & $x_{21}$ & $x_{22}+1$ & $x_{23}$ & $x_{24}$ & $\cdots$ & $x_{2p}$ & $\cdots$ \\
		$n^\circ 3 $ & $x_{31}$ & $x_{32}$ & $x_{33}+1$ & $x_{34}$ & $\cdots$ & $x_{3p}$ & $\cdots$ \\
		$n^\circ 4 $ & $x_{41}$ & $x_{42}$ & $x_{43}$ & $x_{44}+1$ & $\cdots$ & $x_{4p}$ & $\cdots$ \\
		$n^\circ 5 $ & $x_{51}$ & $x_{52}$ & $x_{53}$ & $x_{54}$ & $\cdots$ & $x_{5p}$ & $\cdots$ \\
		$n^\circ 6 $ & $x_{61}$ & $x_{62}$ & $x_{63}$ & $x_{64}$ & $\cdots$ & $x_{6p}$ & $\cdots$ \\
		 & $\cdots$ & &  &  &  &  &  \\
		 &  & $\cdots$ &  &  &  &  &  \\
		 &  &  & $\cdots$ &  &  &  &  \\
		$n^\circ k $ & $x_{k1}$ & $x_{k2}$ & $x_{k3}$ & $x_{k4}$ & $\cdots$ & $x_{kp}$ & $\cdots$ \\
		 &  &  &  &  &  & $\cdots$ & \\ \hline
		\end{tabular}
		\end{table}
		Then let us consider the sequence on the diagonal:
		\begin{itemize}
			\item It cannot be equal to the first sequence of the first row in the prior-previous table since it is distinguished at least by the first element.
			
			\item It cannot be equal to the second sequence of the second row of the prior-previous table since is distinguished at least by the second element.
			
			\item It cannot be equal to the third sequence of the second row of the prior-previous table since is distinguished at least by the third element.
		\end{itemize}
		and so on ... It the cannot be equal to any of the sequences in this table!
	\end{itemize}
	So whatever the chosen classification of infinite sequences of $0 \ldots 9$, there is always one who escapes this classification! So it is that it is impossible to number them ... simply because they do not form a countable set (i.e. $\mathbb{R} \not\simeq\mathbb{N}$)!
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}	
	\end{dem}
	The technique that has allowed us to achieve this result is known as the "\NewTerm{Cantor diagonal process}\index{Cantor diagonal process}" (because similar to that used for equipotence between the natural and rational set) and the set of real numbers is said to have the "\NewTerm{power of the continuum}\index{power of the continuum}\label{power of the continuum}" by the fact that it is uncountable.
	
	We may assume that it is intuitive for the reader that any real number can be approximated infinitely close by a rational number (for irrational numbers we simply stop at a given number of decimals and find the corresponding rational). Mathematicians say therefore that $\mathbb{Q}$ is "\NewTerm{dense}\index{dense set}" in $\mathbb{R}$ and denote this by:
	
	To see this (in the mathematician way of life...) let us take a real number  $x$. For any $n\in \mathbb{N}$ we denote by:
	
	the integer part of $x$ that was previously multiplied by $10^n$.
	
	By definition of the integer part of a real number, we have:
	
	This inequality is equivalent to:
	
	Let us put for all $n\in \mathbb{N}$:
	
	where $a_n$ and $b_n$ are respectively the approached decimal values to $x\cdot 10^{-n}$ by default or by excess.
	
	Notice that $\forall n\in\mathbb{N}$ we have:
	
	and that $\forall n\in\mathbb{N}$ we have $\{a_n,b_n\}\in \mathbb{Q}$.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\textbf{{\Large \ding{45}}Example:}\\\\
	We want to calculate $\sqrt{2}$. Then:
	\begin{table}[H]
	\centering
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|c|c|c|c|c|}
				\hline
				\multicolumn{1}{c}{\cellcolor[gray]{0.75}\pmb{$n$}} & 
\multicolumn{1}{c}{\cellcolor[gray]{0.75}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pmb{$a_n$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pmb{$b_n$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{error}}\\ \hline
		1 & $1<\sqrt{2}<2$ & $1$ & $2$ & $1$ \\ \hline
		2 & $1.4<\sqrt{2}<1.5$ & $1.4$ & $1.5$ & $0.1$ \\ \hline
		3 & $1.41<\sqrt{2}<1.42$ & $1.41$ & $1.42$ & $0.01$ \\ \hline
		4 & $1.414<\sqrt{2}<1.415$ & $1.414$ & $1.415$  & $0.001$ \\ \hline
	\end{tabular}
	\end{table}	
	\end{tcolorbox}
	\begin{theorem}
	If the sequences $(a_n)$ and $(b_n)$ are adjacents, that is to say that $(a_n)$ is ascending and $(b_n)$ descending then we have:
	
	\end{theorem}
	
	\begin{dem}
	Given $n\in \mathbb{N}$, let us recall that:
	
	where $p_n=[10^nx]$. Let us multiply both sides by $10$ then we get:
	
	Then:
	\begin{itemize}
		\item We have quite obviously:
		
		hence:
		
		and therefore $a_n\leq a_{n+1}$. The sequence $(a_n)$ is therefore ascending.
		
		\item And we also have quite obviously:
		
		hence:
		
		and therefore $b_{n+1}\leq b_n$. The sequence $(b_n)$ is therefore descending.
	\end{itemize}
	As:
	
	We have indeed:
	
	and therefore we have proved that the both sequences $(a_n)$ and $(b_n)$ are adjacents. They are convergent and converge both to a same limit $L\in\mathbb{R}$. 
	
	As $\forall n\in\mathbb{N}$:
	
	We have necessarily $L=x$.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\begin{corollary}
	An real number is a limit of a sequence of rational numbers.
	\end{corollary}
	\begin{dem}
	This is an immediate consequence of the previous theorem. Indeed, the sequences $(a_n)$ and $(b_n)$ are sequences of rationals.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\paragraph{Percentages}\label{percentage}\mbox{}\\\\
	In business\footnote{For interest rates, discounts, bonus, etc.} (but not only obviously!) it is of usage with real numbers to communicate in percentages or per-thousand when we have to deal with ratios.
	
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{itemize}
		\item[D1.] Given a scalar $x \in \mathbb{R}$ then expressed in "\NewTerm{percentage}\index{percentage}" it will be denoted by:
			
		\item[D2.] Given a scalar $x \in \mathbb{R}$ then expressed in "\NewTerm{per-thousand}\index{per-thousand}" it will be denoted by:
			
	\end{itemize}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. The scalar $0.5$ expressed in percentage is given by:
	$$0.5\cdot 100=50\%$$
	E2. The scalar $1$ expressed in percentage is given by:
	$$1\cdot 100=100\%$$
	E3. The percentage $75\%$ expressed as a scalar is given by:
	$$\dfrac{75}{100}=0.75$$
	\end{tcolorbox}
	But we also have another well known percentage...:
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/arithmetics/percentages.jpg}
		\caption{Percentage usage illustration}
	\end{figure}
	You must be especially carefully when manipulating percentages (that are relative values for recall!) and even more when you compare that to absolute values. A famous recent "fake news" (in June 12018 according to holocene calendar) made by a socialist activist (supposing to be graduate in engineering...) in France on Twitter (a social media well-known for its fake news) made a lot of people laugh... but at the same time a huge number of people believed that the "fact" was true\footnote{Serious newspapers, like \textit{Le Monde}, also relayed the information as obviously wrong}. Here is the communication (try to guess where is the error, and notice that the author of the post says that France government communicates "fake news" but in reality... he is the guy who communicated the "fake news"... LoL):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{img/arithmetics/percentage_fake_news.jpg}
	\end{figure}
	Ok let us check if really the government welfare has indeed decrease in France while the population and the Gross Domestic Product (GDP) has at the same time increased as the author of this Tweet argue...
	
	First we have (if we do the calculations correctly):
	
	Secondly:
	
	Finally:
	
	You see the error now...? The author of the tweet calculates for the welfare the increase in what we name in finance "\NewTerm{basis points}\index{basis points}", and not in "percentages" as he does for the two firsts. This a very nice example on how to use statistics to lie... (lies works only with scientific illiterate people obviously!)
	
	You must also be extremely careful relatively to mass-media scientific communications (and even more on social networks...) when you read something like « \textit{... product $X$ increase deadly syndrome $Y$ of $400\%$...} ». Because sometimes, the increase could be (random example) in fact of $2$ individual on $100,000$ to $8$ individuals (still on $100,000$ obviously!). This is a way to bias our brain associating huge percentage to quantities of small magnitudes... So in such situations always look for the original absolute values and to corresponding the risk ratio (\SeeChapter{see section Numerical Methods page \pageref{risk ratio}}).
	
	We don't need to say that this "tweet" was removed a few days after by his author from Twitter...
	
	Another famous academic example on how tricky the manipulation of percentages can be is the so named "potato paradox". The paradox is stated as: «\textit{You have $100$ [kg] of potatoes, which are $99\%$ water by weight. You let them dehydrate until they' re $98\%$ water. How much do they weight now?}». The result using algebra is that after evaporating of the water, the remaining total quantity, $x$, contains $1$ [kg] of pure potatoes and $ (98/100) x$ water. The relation becomes: 
	 
	resulting in $x=50$ [kg].
	
	A last one used by some dishonest politicians, managers and marketing specialists is to say that an economic indicator that should be with a high value had increased of $X\%$ a few years before, and that now thanks to them it has decrease also of the same percentage $X\%$ and that we fall back then on the same value as before and like this nothing has changed! But we let the reader check if $C_0(1+X\%)(1-X\%)=C_0$ or not....
	
	Also keep in mind that in practice it is frequent to normalize a series of values $x_1,\ldots,x_n$ with $x_i\in \mathbb{R}$ (considerated as a vector $\vec{x}$) into percentages, such that $x_{p,i}\in [0,1]$ using the simple obvious following transformation\footnote{The reader must avoid the confusion between "normalizing a variable" and "center reducing a random variable (i.e. standardizing)" to make it a $Z=\mathcal{N}(0,1)$ statistical variable as we will see further below!} named a "\NewTerm{Min-Max scaling}\index{min-max scaling}":
	
	This is especially well known in spreadsheet software like Microsoft Excel when using Normalized bar charts, or normalized key performance indicators (conditional formatting), or also in Data Science for the use of some neural networks algorithms.
		
	
	\subsubsection{Transfinite Numbers}
	We now are with an infinity of real numbers which is different from that of natural numbers. Cantor then dared what no one had dared since Aristotle: the positive integers sequence is also infinite, the set $\mathbb{N}$, is then a set that has a countable infinity of elements, then he said that the cardinal (\SeeChapter{see section Set Theory page \pageref{cardinal}}) of this set was a number that existed as such, without that we need to use the tote symbol $\infty$, he denoted it:
	
	This symbol is as we know (\SeeChapter{see section Set Theory page \pageref{aleph}}) the first letter of the Hebrew alphabet, pronounced "aleph zero". Cantor was going to name this strange number, a "\NewTerm{transfinite number}\index{transfinite number}".
	
	The decisive act is to assert that there is, after the finite, a transfinite, that is to say an unlimited scale of determined modes which by nature are infinite, and yet can be specified, as for the finite, by specific numbers, well defined and distinguishable from each other!! This tool was necessary as a set cardinal can be equal to one of its parts as we will see just below!
	
	After this first stroke going against most ideas for over two thousand years, Cantor would continue its path and build the calculation rules, paradoxical at first glance, of the transfinite numbers. These rules were based, as we said earlier, on the fact that two infinite sets are equivalent if there exists a bijection between the two sets.
	
	Thus, we can easily show that the infinity of even numbers is equivalent to the infinity of integers: for this, it suffices to show that for every integer, we can associate an even number, his double, and vice versa. Therefore the cardinal of integers is equal to those of even numbers (the cardinal of a set can be equal to one of its parts!).
	
	Thus, although if even numbers are included in the set of integers, there is an infinity $\alpha_0$ of them, the two sets are equipotent. By stating that a set can be equal to one of its parts, Cantor goes against what seemed obvious to Aristotle and Euclid: the set of all sets is infinite! This will shake the whole of mathematics and will bring the axiomatic Zermelo-Fraenkel we will see in the section of Set Theory page \pageref{zermelo fraenkel axiomatic}.
	
	From the above, Cantor define the following calculations rules on the cardinals:
	
	At first glance these rules seem non-intuitive, but in fact they are! Indeed, Cantor defined the addition of two transfinite numbers as cardinal of the disjoint union of the corresponding sets.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. By denoting $\aleph_0$ the cardinal of $\mathbb{N}$ we have $\aleph_0+\aleph_0$ which is equivalent to saying that we sum the cardinal of $\mathbb{N}$ disjoint union $\mathbb{N}$. But as $\mathbb{N}$ disjoint union $\mathbb{N}$ is equipotent to $\mathbb{N}$ then $\aleph_0+\aleph_0=\aleph_0$ (it is enough to be convinced to take the set of odd and even integers which are both countable and which disjoint union is also countable).\\
	
	E2. Other trivial example: $\aleph_0+1$ corresponds to the cardinality of $\mathbb{N}$ union a point. This set is still equipotent to $\mathbb{N}$ therefore $\aleph_0+1=\aleph_0$.
	\end{tcolorbox}
	We will also during our study of the section Set Theory (page \pageref{cartesian product}) that the concept of Cartesian product of two countable sets is such that we have:
	
	and therefore:
	
	Similarly (\SeeChapter{see section Set Theory page \pageref{union}}) since $\mathbb{Z}=\mathbb{Z}^{+}\cup\mathbb{Z}^{-}$ we have:
	
	and identifying $\mathbb{Q}$ to $\mathbb{Z}\times\mathbb{Z}$ (ratio of a numerator over denominator) we have immediately:
	
	We can also prove an interesting statement: if we consider the cardinality of the set of all the cardinals, it is necessarily greater than all the cardinals, including itself (it is better to have read previously the section of Set Theory page \pageref{set theory})! In other words: the cardinality of the set of all sets of $A$ is greater than the cardinal of $A$ itself.
	
	This implies that there is no set containing all sets since there is always a bigger one (it is an equivalent form of the famous old Cantor's paradox)!!!
	
	In technical language it means considering a non-empty set $A$ and then to state that:
	
	where $\mathcal{P}(A)$ is the set of subsets of $A$.
	
	That is to say, by definition of the order relation $<$ (strictly less), it suffices to prove that there is no surjective application $f:A\mapsto \mathcal{P}(A)$, in other words that to each element of the set of parts of $A$ it does not match at least one pre-image in $A$.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The set $\mathcal{P}(\mathbb{N})$ for example consists of the set of even numbers, odd numbers, natural numbers, as well as the empty set itself, etc. $\mathcal{P}(\mathbb{N})$ is therefore the set of all "potatoes" (to borrow the vocabulary of high school ...) that make $\mathbb{N}$.
	\end{tcolorbox}
	\begin{dem}
	Suppose that we can number each potato of $\mathcal{P}(A)$ with at least one element of $A$ (imagine that with $\mathbb{N}$). In other words it is equivalent to suppose that $f:A\mapsto \mathcal{P}(A)$ is surjective and let us consider a subset $E$ of $A$ such that:
	
	that is to say the set of elements $x$ of $A$ that do not belong to the set numbered by $x$ (the element $x$ does not belong to the "potato" that it numbers in other terms...).
	
	Or, if $f$ is surjective it must also be a $y \in A$ for this subset $E$ such that:
	
	since $E$ is also a subset of $A$.
	
	Suppose that $y$ belongs to $E$. In this case, by definition of $E$, $y \notin f(y)=E$ (by definition of $E$ that applies for every $x$ and $x$ can also be obviously $y$ or $z$ or doesn't matter what). By consequence, $y\notin E$, but in this second case, always by definition of $E$, $y\in f(y)=E$ (as $y$ is not in $E$). We see therefore that the element $y$ cannot exists and therefore $f$ cannot be surjective. 
	
	We strongly recommend the reader to read the previous sentence more than on time if necessary.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\pagebreak
	\subsubsection{Complex Numbers}\label{complex numbers}
	Invented in the 116th century (holocene calendar) among others by Girolamo Cardano and Rafaello Bombelli, "\NewTerm{complex numbers}\index{complex number}" (also named "\NewTerm{imaginary numbers}\index{imaginary numbers}") are used to solve problems with no solutions in $\mathbb{R}$ and also used to mathematically formalize certain transformations in the plan such as rotation, similarity, translation, etc. and also to generalized some theorem restricted to $\mathbb{R}$ and therefore hiding some interesting results for practical engineering. For physicists, complex numbers above are also a very convenient way to simplify notations. It is thus very difficult to study wave phenomena, General Relativity or quantum mechanics without using complex numbers and expressions.
	
	The importance and utility of complex numbers in biology, cosmology, physics, astrophysics, electronics, electromechanics, astrophysics, microelectronics, electrical engineering, statistics, optics, mechanical engineering, fluid mechanics etc. are almost never questioned by students and professionals as they appear very quickly in the corresponding courses (most of times just after thirty to fifty hours) or textbooks. However a significant percentage of students and professional in Finance and Insurance question the utility of complex numbers as their application cases appear in the fields most of the time only at the postgraduate level (PhD). So here is a small non exhaustive list of topics in Finance and Insurance where complex numbers appear and are really useful (most of the topics below are treated in the present book):
	 \begin{itemize}
	 	\item In Fourier transforms for solving nonlinear partial differential equations (Black-Sholes for example)
	 	
	 	\item In Laplace transforms for solving nonlinear partial differential equations (Heston model)
	 	
	 	\item In Fourier transforms for spectral analysis of time series
	 	
	 	\item In solving polynomials of auto-regressive time series models
	 	
	 	\item In the use of the characteristic function in the CreditRisk First Boston model
	 	
	 	\item In PCA (Principal Component Analysis) analysis when IEEE rounding errors with \texttt{R} and Python produce complex eigenvalues
	 \end{itemize}
	There are several ways to construct complex numbers. The first is typical of the construction way that mathematicians used as part of Set Theory. They define a couple of real numbers and define the operations between these couples to finally arrive at a meaning of the complex number concept. The second one is less rigorous but its approach is simpler and consist to define the pure unit imaginary number $\mathrm{i}$ and then build arithmetic operations from its definition. We will opt for the second method in the texts that will follow!
	
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] We define the "\NewTerm{unit pure imaginary number}" that we denote by $\mathrm{i}$ by the following property\label{unit pure imaginary number}:
		
			
		\item[D2.] A "\NewTerm{complex number}\index{complex number}" is a pair of a real number $a$ and an imaginary number $\mathrm{i}b$ and generally written in the following form:
		
		where $a$ and $b$ are numbers belonging to $\mathbb{R}$.
		
		\item[D3.] We note the set of complex numbers by $\mathbb{C}$ and therefore we have by construction:
		
	\end{enumerate}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The set $\mathbb{C}$ is identified to the oriented Euclidean plane $E$ (\SeeChapter{see section Vector Calculus page \pageref{oriented Euclidean space}}) thanks to the choice of a direct orthonormal basis (we therefore get the "\NewTerm{Argand-Cauchy plane}\index{Argand-Cauchy plane}", also named "\NewTerm{Gauss-Argand plane}\index{Gauss-Argand plane}" or more commonly "\NewTerm{Gauss plane}\index{Gauss plane}\label{gauss plane}" that we will see a little further below and that seems have be defined for the first time in 11806 according to holocene calendar).
	\end{tcolorbox}
	The set of complex numbers that constitutes a field (\SeeChapter{see section Set Theory page \pageref{field (set)}}) and denoted by $\mathbb{C}$, is defined (in a simple way to start) in the notation of set theory by:
	
	In other words we say that the field $\mathbb{C}$ is the field $\mathbb{R}$ to which we have added the imaginary number $\mathrm{i}$. Which is formally denoted by:
	
	The addition and multiplication of complex numbers are internal operations to the set (field) of complex numbers (we will come back much more in detail on certain properties of complex numbers in the section of Set Theory page \pageref{groups}) and defined by:
	
	The "\NewTerm{real part}\index{complex number!real part}" of $z$ is traditionally denoted by:
	
	The "\NewTerm{imaginary part}\index{complex number!imaginary part}" of $z$ is traditionally denoted by:
	
	The "\NewTerm{conjugate}\index{conjugate}\label{complex conjugate}" of $z$ is defined by:
	
	and is sometimes also denoted $z^{*}$ (particularly in quantum physics in some books!).
	
	From a complex and its conjugate, it is possible to find its real and imaginary parts. These are the following obvious relations:
	
	The "\NewTerm{module}\index{module}\label{complex module}" of $z$ (or "\NewTerm{norm}\index{norm}") is the length from the center of the Gaussian plane (see further below a figure of the Gaussian plane) and is simply calculated using the Pythagorean theorem:
		
	and is always a positive number or equal to zero.
	
	We will consider as obvious that it satisfies all the properties of a distance (\SeeChapter{see section of Topology page \pageref{topology} and Vector Calculus page \pageref{vector calculus}}).
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The notation $|z|$ for the module is not innocent since $|z|$ coincides with the absolute value of $z$ when $z$ is real.
	\end{tcolorbox}
	The division between two complex number is calculated as (the denominator is obviously not zero):
	
	The opposite of a complex number is calculated similarly:
	
	We can therefore list $8$ important properties of the module and the complex conjugate:
	\begin{itemize}
		\item[P1.] We affirm that:
		
		\begin{dem}
		By definition of the module $|z|=\sqrt{x^2+y^2}$ so that the sum $x^2+y^2$ is zero, the necessary condition is that as $(x,y)\in \mathbb{R}$:
		
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		
		\item[P2.] We affirm that:
		
		\begin{dem}
		This is immediate by:
		
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		
		\item[P3.] We affirm that:
		
		\begin{dem}
		The two above inequalities can be written:
		
		thus equivalent respectively to:
		
		which are trivial. The rest of the proof is therefore trivial!
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		
		\item[P4.] We have:
		
		and if $z_2\neq 0$:
		
		\begin{dem}
		First:
		
		(we will prove a little further below that generally $\overline{z_1z_2}=\bar{z}_1\bar{z}_2$) and for $z_2\neq 0$:
		
		and taking root square this finish the proof.
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		
		\item[P5.] We affirm that:
		
		\begin{dem}
		This is immediate:
		
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		
		\item[P6.] We affirm that:
		
		\begin{dem}
		The first one is immediate:
		
		and:
		
		and\label{module product complex numbers}:
		
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		\textbf{R1.} In mathematical terms, the first proof helps to show that complex conjugation is what is named an "\NewTerm{involution}\index{involution}" (in the sense that it is changing anything ...).\\
		
		\textbf{R2.} Also in mathematical terms (it is only the vocabulary!), the second proof shows that the combination of the sum of two complex numbers is what we name a "group automorphism $(\mathbb{C},+)$" (\SeeChapter{see section Set Theory page \pageref{automorphism}}).\\
		
		 \textbf{R3.} Again, for vocabulary ... the third proof show that the combination of the product of two complex numbers is what we name a "field automorphism $(\mathbb{C},+,\times)$" (\SeeChapter{see section Set Theory page \pageref{automorphism}}).
		\end{tcolorbox}
		
		\item[P7.] We affirm that for $z$ different from zero:
		
		\begin{dem}
		\label{module ration complex numbers}We will restrict ourselves to the proof of the second relation that is a general case of the first (for $z=1$).
		
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		
		\item[P8.] We have:
		
		for any complex number $z_1,z_2$ (strictly speaking non-zero complex numbers, otherwise the concept of argument of the complex number that we will see further below is undetermined). Furthermore the equality holds if and only if $z_1$ and $z_2$ are collinear (the vectors are "on the same straight line") and of the same direction, in other words.... if it exist $\lambda \in \mathbb{R}$ such as $\lambda z_1=z_2$.
		\begin{dem}
		Directly we have:
		
	This inequality may not be obvious to everyone, therefore let us develop it a bit and let us assume it true:
		
		After simplification:
		
		and again after simplification:
		
		So as the square brackets is necessarily positive or zero it follows:
		
		This last relation thus shows that inequality is true.
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		In fact there is a more general form of this inequality named "\NewTerm{Minkowski inequality}\index{Minkowski inequality}", proved in the section of Vector Calculus page \pageref{triangle inequality} (complex numbers can indeed be written in the form of vectors as we will see later).
		\end{tcolorbox}
	\end{itemize}
	
	
	\paragraph{Geometric Interpretation of Complex Numbers}\mbox{}\\\\
	We can also represent any complex number $a+\mathrm{i}b$ or $a-\mathrm{i}b$ in a plane defined by two axes (two dimensions) of infinite length and orthogonal between them. The vertical axis represents the imaginary part of a complex number and the horizontal axis the real part (see figure below).
	
	So there is correspondence between the set of complex numbers $\mathbb{C}$ and the set of vectors of the Gaussian plane $\mathbb{R}^2$.
	
	We sometimes named this type of representation "\NewTerm{Gauss plane}\index{Gauss plane}" or "\NewTerm{Gauss map}\index{Gauss map}" and we name "\NewTerm{affix}\index{affix}" the point with Cartesian coordinates $(a,b)$ that is identified with the complex number $a+\mathrm{i}b$ itself.
	
	\begin{figure}[H]
		\centering
		\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt
		
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,558); %set diagram left start at 0, and has height of 558
		
		%Straight Lines [id:da6862719257892078] 
		\draw    (176,280.5) -- (504,280.5) ;
		\draw [shift={(507,280.5)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da5768979392466298] 
		\draw    (341.5,408) -- (341.5,156) ;
		\draw [shift={(341.5,153)}, rotate = 90] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Shape: Circle [id:dp5725699200985088] 
		\draw  [fill={rgb, 255:red, 74; green, 74; blue, 74 }  ,fill opacity=1 ] (337.25,280.5) .. controls (337.25,278.15) and (339.15,276.25) .. (341.5,276.25) .. controls (343.85,276.25) and (345.75,278.15) .. (345.75,280.5) .. controls (345.75,282.85) and (343.85,284.75) .. (341.5,284.75) .. controls (339.15,284.75) and (337.25,282.85) .. (337.25,280.5) -- cycle ;
		%Shape: Circle [id:dp48129870576884715] 
		\draw  [fill={rgb, 255:red, 74; green, 74; blue, 74 }  ,fill opacity=1 ] (337.25,230.5) .. controls (337.25,228.15) and (339.15,226.25) .. (341.5,226.25) .. controls (343.85,226.25) and (345.75,228.15) .. (345.75,230.5) .. controls (345.75,232.85) and (343.85,234.75) .. (341.5,234.75) .. controls (339.15,234.75) and (337.25,232.85) .. (337.25,230.5) -- cycle ;
		%Shape: Circle [id:dp6885608592341288] 
		\draw  [fill={rgb, 255:red, 74; green, 74; blue, 74 }  ,fill opacity=1 ] (337.25,331) .. controls (337.25,328.65) and (339.15,326.75) .. (341.5,326.75) .. controls (343.85,326.75) and (345.75,328.65) .. (345.75,331) .. controls (345.75,333.35) and (343.85,335.25) .. (341.5,335.25) .. controls (339.15,335.25) and (337.25,333.35) .. (337.25,331) -- cycle ;
		%Shape: Circle [id:dp6741670160388038] 
		\draw  [fill={rgb, 255:red, 74; green, 74; blue, 74 }  ,fill opacity=1 ] (386.25,280) .. controls (386.25,277.65) and (388.15,275.75) .. (390.5,275.75) .. controls (392.85,275.75) and (394.75,277.65) .. (394.75,280) .. controls (394.75,282.35) and (392.85,284.25) .. (390.5,284.25) .. controls (388.15,284.25) and (386.25,282.35) .. (386.25,280) -- cycle ;
		%Shape: Circle [id:dp2841043284997904] 
		\draw  [fill={rgb, 255:red, 74; green, 74; blue, 74 }  ,fill opacity=1 ] (286.25,280.5) .. controls (286.25,278.15) and (288.15,276.25) .. (290.5,276.25) .. controls (292.85,276.25) and (294.75,278.15) .. (294.75,280.5) .. controls (294.75,282.85) and (292.85,284.75) .. (290.5,284.75) .. controls (288.15,284.75) and (286.25,282.85) .. (286.25,280.5) -- cycle ;
		%Shape: Circle [id:dp35845137565488194] 
		\draw  [fill={rgb, 255:red, 74; green, 74; blue, 74 }  ,fill opacity=1 ] (192.5,280) .. controls (192.5,277.65) and (194.4,275.75) .. (196.75,275.75) .. controls (199.1,275.75) and (201,277.65) .. (201,280) .. controls (201,282.35) and (199.1,284.25) .. (196.75,284.25) .. controls (194.4,284.25) and (192.5,282.35) .. (192.5,280) -- cycle ;
		%Shape: Circle [id:dp20974674240488667] 
		\draw  [fill={rgb, 255:red, 74; green, 74; blue, 74 }  ,fill opacity=1 ] (481.75,280) .. controls (481.75,277.65) and (483.65,275.75) .. (486,275.75) .. controls (488.35,275.75) and (490.25,277.65) .. (490.25,280) .. controls (490.25,282.35) and (488.35,284.25) .. (486,284.25) .. controls (483.65,284.25) and (481.75,282.35) .. (481.75,280) -- cycle ;
		%Shape: Circle [id:dp08254437278271642] 
		\draw  [fill={rgb, 255:red, 74; green, 74; blue, 74 }  ,fill opacity=1 ] (336.75,171.5) .. controls (336.75,169.15) and (338.65,167.25) .. (341,167.25) .. controls (343.35,167.25) and (345.25,169.15) .. (345.25,171.5) .. controls (345.25,173.85) and (343.35,175.75) .. (341,175.75) .. controls (338.65,175.75) and (336.75,173.85) .. (336.75,171.5) -- cycle ;
		%Shape: Circle [id:dp4570052887735252] 
		\draw  [fill={rgb, 255:red, 74; green, 74; blue, 74 }  ,fill opacity=1 ] (337.25,388.5) .. controls (337.25,386.15) and (339.15,384.25) .. (341.5,384.25) .. controls (343.85,384.25) and (345.75,386.15) .. (345.75,388.5) .. controls (345.75,390.85) and (343.85,392.75) .. (341.5,392.75) .. controls (339.15,392.75) and (337.25,390.85) .. (337.25,388.5) -- cycle ;
		%Shape: Circle [id:dp20739972006498264] 
		\draw  [fill={rgb, 255:red, 74; green, 74; blue, 74 }  ,fill opacity=1 ] (481.5,171.5) .. controls (481.5,169.15) and (483.4,167.25) .. (485.75,167.25) .. controls (488.1,167.25) and (490,169.15) .. (490,171.5) .. controls (490,173.85) and (488.1,175.75) .. (485.75,175.75) .. controls (483.4,175.75) and (481.5,173.85) .. (481.5,171.5) -- cycle ;
		%Straight Lines [id:da6061375853108535] 
		\draw    (341.5,280.5) -- (484.15,172.71) ;
		\draw [shift={(485.75,171.5)}, rotate = 142.92] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da6648197401654512] 
		\draw  [dash pattern={on 0.84pt off 2.51pt}]  (341,171.5) -- (485.75,171.5) ;
		%Straight Lines [id:da6125632677020618] 
		\draw  [dash pattern={on 0.84pt off 2.51pt}]  (485.75,171.5) -- (486,280) ;
		%Shape: Circle [id:dp11049011427233912] 
		\draw  [fill={rgb, 255:red, 74; green, 74; blue, 74 }  ,fill opacity=1 ] (482,388.5) .. controls (482,386.15) and (483.9,384.25) .. (486.25,384.25) .. controls (488.6,384.25) and (490.5,386.15) .. (490.5,388.5) .. controls (490.5,390.85) and (488.6,392.75) .. (486.25,392.75) .. controls (483.9,392.75) and (482,390.85) .. (482,388.5) -- cycle ;
		%Straight Lines [id:da3444372420899078] 
		\draw  [dash pattern={on 0.84pt off 2.51pt}]  (486,280) -- (486.25,388.5) ;
		%Straight Lines [id:da6952090878336938] 
		\draw  [dash pattern={on 0.84pt off 2.51pt}]  (341.5,388.5) -- (486.25,388.5) ;
		%Straight Lines [id:da19629761366029164] 
		\draw  [dash pattern={on 0.84pt off 2.51pt}]  (341.5,280.5) -- (484.65,387.3) ;
		\draw [shift={(486.25,388.5)}, rotate = 216.73] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da606232873889516] 
		\draw  [dash pattern={on 0.84pt off 2.51pt}]  (196.75,388.5) -- (341.5,388.5) ;
		%Straight Lines [id:da8633142766808242] 
		\draw  [dash pattern={on 0.84pt off 2.51pt}]  (196.5,280) -- (196.75,388.5) ;
		%Straight Lines [id:da23085168727535588] 
		\draw  [dash pattern={on 0.84pt off 2.51pt}]  (341.5,280.5) -- (198.35,387.3) ;
		\draw [shift={(196.75,388.5)}, rotate = 323.27] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Circle [id:dp6022300872639843] 
		\draw  [fill={rgb, 255:red, 74; green, 74; blue, 74 }  ,fill opacity=1 ] (192.5,388.5) .. controls (192.5,386.15) and (194.4,384.25) .. (196.75,384.25) .. controls (199.1,384.25) and (201,386.15) .. (201,388.5) .. controls (201,390.85) and (199.1,392.75) .. (196.75,392.75) .. controls (194.4,392.75) and (192.5,390.85) .. (192.5,388.5) -- cycle ;
		%Shape: Arc [id:dp596403422535863] 
		\draw  [draw opacity=0] (386.16,247.48) .. controls (390.95,248.22) and (395.62,250.22) .. (399.66,253.5) .. controls (408.18,260.42) and (411.65,271.23) .. (409.54,281.17) -- (382.15,275.06) -- cycle ; \draw   (386.16,247.48) .. controls (390.95,248.22) and (395.62,250.22) .. (399.66,253.5) .. controls (408.18,260.42) and (411.65,271.23) .. (409.54,281.17) ;  
		
		% Text Node
		\draw (420,197) node [anchor=north west][inner sep=0.75pt]   [align=left] {$r$};
		% Text Node
		\draw (490,152) node [anchor=north west][inner sep=0.75pt]   [align=left] {$z$};
		% Text Node
		\draw (318.5,162) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\mathrm{i}b$};
		% Text Node
		\draw (343.5,132.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\Im$};
		% Text Node
		\draw (512.5,270) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\Re$};
		% Text Node
		\draw (468.5,283.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$a$};
		% Text Node
		\draw (384.5,286.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$1$};
		% Text Node
		\draw (408,246.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\varphi$};
		% Text Node
		\draw (493.5,386.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\bar{z}$};
		% Text Node
		\draw (324.5,392) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\mathrm{i}b$};
		% Text Node
		\draw (325.5,219.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\mathrm{i}$};
		% Text Node
		\draw (313,321.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$-\mathrm{i}$};
		% Text Node
		\draw (280.5,286) node [anchor=north west][inner sep=0.75pt]   [align=left] {$-1$};
		% Text Node
		\draw (167,387.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$-z$};
		% Text Node
		\draw (181,283.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$-a$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Complex Gauss plane}
	\end{figure}
	and then we write:
	
	We see on this diagram that a complex number has thus a vector interpretation (\SeeChapter{see section Vector Calculus page \pageref{vector}}) given by:
	
	where the canonical basis is defined such as:
	
	with:
	
	Thus, $\begin{pmatrix}1\\0\end{pmatrix}$ is the unitary basis vector of the carried by the horizontal axis $\mathbb{R}$ and $\begin{pmatrix}0\\1\end{pmatrix}$ is the unitary basis vector carried by the vertical imaginary axis $\mathbb{R}_{\text{i}}$ and $r$ is the module (the norm) that is positive or zero.
	
	This has to be compared with the canonical vectors basis of $\mathbb{R}^2$ (\SeeChapter{see section Vector Calculus page \pageref{canonical basis}}):
	
	with:
	
	so that we can identify the complex plane with the Euclidean plane.
	
	Thanks to the geometric interpretation of the Gaussian plane, the equality below is immediate for example and avoids making some developments:
	
	In addition, the definitions of the cosine and sine (\SeeChapter{see section Trigonometry page \pageref{cyclometrics functions}}) give us:
	
	Finally:
	\begin{equation}
	  \addtolength{\fboxsep}{5pt}
	   \boxed{
	   \begin{gathered}
	   	\begin{aligned}
		&r=\sqrt{a^2+b^2}\\
		&\varphi^{-1}=\cos^{-1}\left(\dfrac{a}{r}\right)=\sin^{-1}\left(\dfrac{b}{r}\right)=\tan^{-1}\left(\dfrac{b}{a}\right)
	 	\end{aligned}
	   \end{gathered}
	   }
	\end{equation}
	Therefore:
	
	complex number which is always equal to itself modulo $2\pi$ by the properties of trigonometric functions:
	
	with $k\in \mathbb{N}$ and where $\varphi$ is named the "\NewTerm{argument of $z$}\index{argument}" and is traditionally denoted by:
	
	The properties of the cosine and sine (\SeeChapter{see section Trigonometry page \pageref{remarkable angles}}) lead us directly to write for the argument:
	
	We also prove among other things with the Taylor series (\SeeChapter{see section Sequences and Series page \pageref{cosine maclaurin dev}}) that:
	
	and:
	
	which sum is similar to\label{taylor expansion complex exponential}:
	
	but instead perfectly identical to the Taylor expansion of $e^{\mathrm{i}x}$:
	
	So finally, we can write:
	
	relation named "\NewTerm{Euler's formula}\index{Euler's formula}\label{euler formula}".
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Notice that:
	
	where $\delta$ is named a "\NewTerm{phase-shift}\index{phase-shift}\label{phase shift}" and $e^{\mathrm{i}\delta}$ is named a "\NewTerm{phase-shift factor}\index{phase-shift factor}":
	\begin{figure}[H]
	\centering
		\begin{tikzpicture}
	  	\begin{axis}[
	    trig format plots=rad,
	    axis lines = middle,
	    enlargelimits,
	    clip=false
	    ]
	    \addplot[domain=-2*pi:2*pi,samples=200,blue] {sin(x)};
	    \addplot[domain=-2*pi:2*pi,samples=200,red] {sin(x-2)};
	    \draw[dotted,blue!40] (axis cs: 0.5*pi,1.1) -- (axis cs: 0.5*pi,0);
	    \draw[dotted,red!40] (axis cs: 0.5*pi+2,1.1) -- (axis cs: 0.5*pi+2,0);
	    \draw[dashed,olive,<->] (axis cs: 0.5*pi,1.05) -- node[above,text=black,font=\footnotesize]{$\delta$}(axis cs: 0.5*pi+2,1.05);
	  	\end{axis}
		\end{tikzpicture}
	\end{figure}
	\end{tcolorbox}
	Using the properties of trigonometric functions:
	
	
	Depending on we sum or subtract this gives us the "\NewTerm{Euler formulas}" or "\NewTerm{de Moivre and Euler formulas}\index{de Moivre and Euler formulas}\label{de Moivre and Euler formulas}":
	\begin{equation}
	  \addtolength{\fboxsep}{5pt}
	   \boxed{
	   \begin{gathered}
	   	\begin{aligned}
		\cos(\varphi)&=\dfrac{e^{\mathrm{i}\varphi}+e^{-\mathrm{i}\varphi}}{2}\\
		\sin(\varphi)&=\dfrac{e^{\mathrm{i}\varphi}-e^{-\mathrm{i}\varphi}}{2\mathrm{i}}\\
	 	\end{aligned}
	   \end{gathered}
	   }
	\end{equation}
	Notice that the angle can be a purely a complex number! This is to say that in all  generality trigonometric functions can be considered as functions that go from $\mathbb{C}$ to $\mathbb{C}$.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Notice that these relations are very useful to linearise expressions such as $\cos^k(\theta)$ or $\sin^k(\theta)$. Indeed:
	
	\end{tcolorbox}
	A well known special case for the formulas above is the case where $\varphi=\pi$ and $r=1$. Then we have:
	
	after rearranging we get the famous geek relation:
	
	Another famous geek case is when we take still $r=1$ but with $\varphi=\pi/2$ and we raise to the power of $\mathrm{i}$. Then we get:
	
	and if we raise that to the power of $\mathrm{i}$:
	
	Some people consider then as a curious fact that $\mathrm{i}$ raised to the $\mathrm{i}$-th power is actually a real number.
	
	Thanks to the exponential form of a complex number, very commonly used in many fields of physics and engineering, we can easily draw relations such that starting from (remember that $\text{cis}$ is an old notation that stands for the $\cos(\varphi)+\mathrm{i}\sin(\varphi)$ being in the parenthesis):
	
	and assuming known the basic trigonometric identities (\SeeChapter{see section Trigonometry page \pageref{remarkable trigonometric identities}}) we have the following relations for the multiplication of two complex numbers:
	
	therefore:
	
	and therefore if $n$ is a positive integer:
	
	For the module (norm) of the multiplication:
	
	Therefore:
	
	For the division of two complex numbers:
	
	The module of their division then comes immediately:
	
	therefore we have for the argument:
	
	and it comes immediately:
	
	For the power of a complex number (or root):
	
	which gives us immediately as already proved previously:
	
	and for the argument:
	
	In case we have a unit module (norm equal to $1$) as $z=\cos(\varphi)+\mathrm{i}\sin(\varphi)$ we then have the relation:
	
	named "\NewTerm{de Moivre formula}\index{de Moivre formula}".

	For the natural logarithm of a complex number, we trivially have the following relation which is discussed in the section of Functional Analysis (page \pageref{logarithms}):
	
	where $\ln(z)$ is often in the complex case written $\mathrm{Log}(z)$ with an uppercase "L".
	 
	 All previous relations could of course be obtained with the trigonometric form of complex numbers but then require some additional lines of mathematical developments.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Notice that the previous developments allow us to calculate things (...) like $\cos(\bar{z})$. Indeed, let $z=x+\mathrm{i}y$ and consider:
	
	Similarly, we have:
	
	Now consider:
	
	\end{tcolorbox}
	
	\subparagraph{Fresnel Vectors (phasors)}\mbox{}\\\\
	A sinusoidal variation $f(t)=r\sin(\omega t)$ can be represented as the projection (\SeeChapter{see section Trigonometry page \pageref{trigonometry}}) on the vertical $y$-axis (imaginary axis the set $\mathbb{C}$) of a rotating vector $\vec{r}$ at angular velocity $\omega$ around the origin in the plane $x\text{O}y$:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,563); %set diagram left start at 0, and has height of 563
		
		%Straight Lines [id:da4802674430553575] 
		\draw    (200,332.5) -- (200,110.5) ;
		\draw [shift={(200,107.5)}, rotate = 90] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da8120091134044312] 
		\draw    (200,317.5) -- (200,221.5) ;
		\draw [shift={(200,218.5)}, rotate = 90] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da6507733777057696] 
		\draw    (161,301.5) -- (457,302.49) ;
		\draw [shift={(460,302.5)}, rotate = 180.19] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da6847070892477265] 
		\draw    (175,301.5) -- (276,301.5) ;
		\draw [shift={(279,301.5)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da7037029008092457] 
		\draw    (201,301.5) -- (298.64,195.97) ;
		\draw [shift={(300,194.5)}, rotate = 132.78] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da9454871565641778] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (300,194.5) -- (199,194.5) ;
		%Straight Lines [id:da5751545473892543] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (300,302.5) -- (300,194.5) ;
		%Shape: Arc [id:dp9967257073781497] 
		\draw  [draw opacity=0] (219.38,282.63) .. controls (224.5,283.01) and (230.9,286.76) .. (236.06,292.91) .. controls (238.48,295.8) and (240.33,298.87) .. (241.56,301.84) -- (227,300.5) -- cycle ; \draw   (219.38,282.63) .. controls (224.5,283.01) and (230.9,286.76) .. (236.06,292.91) .. controls (238.48,295.8) and (240.33,298.87) .. (241.56,301.84) ;  
		
		% Text Node
		\draw (184,214) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\vec{i}$};
		% Text Node
		\draw (268,308) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\vec{1}$};
		% Text Node
		\draw (194,92) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\Im$};
		% Text Node
		\draw (462,295) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\Re$};
		% Text Node
		\draw (305,181) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\vec{r}=a+\mathrm{i}b$};
		% Text Node
		\draw (60,184) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\Im(\vec{r})=b=r\sin(\omega t)$};
		% Text Node
		\draw (296,308) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\Re(\vec{r})=a=r\cos(\omega t)$};
		% Text Node
		\draw (239,274) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\varphi=\omega t$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Fresnel representation}
	\end{figure}
	Such a rotating vector is named "\NewTerm{Fresnel vector}\index{Fresnel vector}" and can be well interpreted as the imaginary part of a complex number given by:
	
	That is to say:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,478); %set diagram left start at 0, and has height of 478
		
		%Shape: Axis 2D [id:dp7815616512473285] 
		\draw  (48,140.7) -- (503.4,140.7)(135.4,28.7) -- (135.4,225) (496.4,135.7) -- (503.4,140.7) -- (496.4,145.7) (130.4,35.7) -- (135.4,28.7) -- (140.4,35.7)  ;
		%Shape: Circle [id:dp10152278291550498] 
		\draw [line width=1.5]  (63.15,143) .. controls (63.15,103.43) and (95.23,71.35) .. (134.8,71.35) .. controls (174.37,71.35) and (206.45,103.43) .. (206.45,143) .. controls (206.45,182.57) and (174.37,214.65) .. (134.8,214.65) .. controls (95.23,214.65) and (63.15,182.57) .. (63.15,143) -- cycle ;
		%Straight Lines [id:da17713205001962828] 
		\draw    (134.8,142) -- (179.11,89.23) ;
		\draw [shift={(180.4,87.7)}, rotate = 130.02] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Arc [id:dp47067421896719286] 
		\draw  [draw opacity=0] (153.06,120.84) .. controls (158.24,126.04) and (161.53,133.14) .. (161.78,141) -- (131.8,142) -- cycle ; \draw   (153.06,120.84) .. controls (158.24,126.04) and (161.53,133.14) .. (161.78,141) ;  
		%Shape: Wave [id:dp45216554602510195] 
		\draw   (297.4,142.2) .. controls (310.53,105.06) and (323.08,69.7) .. (337.65,69.7) .. controls (352.22,69.7) and (364.77,105.06) .. (377.9,142.2) .. controls (391.03,179.34) and (403.58,214.7) .. (418.15,214.7) .. controls (432.72,214.7) and (445.27,179.34) .. (458.4,142.2) .. controls (463.44,127.95) and (468.39,113.97) .. (473.4,102.14) ;
		%Shape: Arc [id:dp9388660428904054] 
		\draw  [draw opacity=0] (62.88,92.39) .. controls (67.75,78.7) and (79.33,68.48) .. (93.4,66.18) -- (99.4,106.85) -- cycle ; \draw    (64,89.57) .. controls (69.38,77.31) and (80.31,68.32) .. (93.4,66.18) ;  \draw [shift={(62.88,92.39)}, rotate = 297.83] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da4327004043436451] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (134.8,214.65) -- (474.4,214.7) -- (504.4,214.7) ;
		%Straight Lines [id:da021933994570277893] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (134.8,71.35) -- (338.4,69.7) ;
		%Straight Lines [id:da769932288124445] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 3.75pt off 3pt on 7.5pt off 1.5pt}]  (180.4,88.7) -- (320.4,87.7) ;
		%Straight Lines [id:da6691774218613131] 
		\draw    (289.4,140.7) -- (471.4,140.7) (332.4,136.7) -- (332.4,144.7)(375.4,136.7) -- (375.4,144.7)(418.4,136.7) -- (418.4,144.7)(461.4,136.7) -- (461.4,144.7) ;
		%Straight Lines [id:da6523707054957257] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (320.4,87.7) -- (321.4,226.7) ;
		%Straight Lines [id:da11679662226866694] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (298.4,133.7) -- (298.4,193.7) -- (298.4,228.7) ;
		%Straight Lines [id:da150694354943953] 
		\draw    (317.33,214.6) -- (302,214.6) ;
		\draw [shift={(299,214.6)}, rotate = 360] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		\draw [shift={(320.33,214.6)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		
		% Text Node
		\draw (159.6,117.85) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\varphi$};
		% Text Node
		\draw (107,42) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\pi/2$};
		% Text Node
		\draw (119,224) node [anchor=north west][inner sep=0.75pt]   [align=left] {$3\pi/2$};
		% Text Node
		\draw (34,132) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\pi$};
		% Text Node
		\draw (64,58) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\omega$};
		% Text Node
		\draw (210,126) node [anchor=north west][inner sep=0.75pt]   [align=left] {$0-2\pi$};
		% Text Node
		\draw (504,140) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\omega t$};
		% Text Node
		\draw (291.4,146.7) node [anchor=north west][inner sep=0.75pt]   [align=left] {$0$};
		% Text Node
		\draw (321.4,144.7) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\pi/2$};
		% Text Node
		\draw (366.4,148.7) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\pi$};
		% Text Node
		\draw (399.4,144.7) node [anchor=north west][inner sep=0.75pt]   [align=left] {$3\pi/2$};
		% Text Node
		\draw (303,227.33) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\varphi$};
		% Text Node
		\draw (456.07,145.7) node [anchor=north west][inner sep=0.75pt]   [align=left] {$2\pi$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Fresnel rotating vector (complex oscillator)}
	\end{figure}
	Or with a supplementary dimension:
	\begin{figure}[H]
		\centering
		\includegraphics[width=1\textwidth]{img/arithmetics/3d_complex_oscillator.pdf}	
		\caption[3D complex oscillator]{3D complex oscillator (author: Izaak Neutelings)}
	\end{figure}
	We will see the phasor again explicitly in our study of wave mechanics and geometrical optics (as part of diffraction) in the sections with the corresponding names page \pageref{wave mechanics} and page \pageref{geometrical optics}.
	
	\paragraph{Transformation in the plane}\mbox{}\\\\	
	It is customary to represent real numbers as points on a graduated line. The algebraic operations have their geometric interpretation on it: the addition is a translation, a multiplication a centered scaling.
	
	In particular we can talk about the "square root of a transformation." A translation of amplitude $T$ may be obtained as the iteration of a translation of amplitude $T / 2$. Similarly, a scaling of amplitude $S$ can be achieved as iterated scaling of faction $\sqrt{S}$. In particular an homothety (scaling) of a factor $9$ can be composed of two homotheties (scaling) of respectively $3$ (or $-3$).
	
	Then we can say that the square root takes on a geometric sense. But what about the square root of negative numbers? In particular of the square root of $-1$???
	
	A scaling of factor $-1$ can be seen as a symmetry with respect to the origin. But if we see this transformation in a continuous manner. Therefore a $-1$ scaling factor also be seen as rotation of $\pi$ rotation around the origin.
	
	So, the problem of negative square root is simplified. Indeed, it is not difficult to break down a rotation of $\pi$ radians into two transformations: we can repeat either a rotation of $\pi/2$ or of $-\pi/2$. The image of $1$ is the square root of $-1$ and $\mathrm{i}$ is situated on a perpendicular to the origin at a distance $1$ either up or down.
	
	Having successfully positioned the number $\mathrm{i}$ it not difficult anymore to put other complex numbers in the Gauss plane. We can therefore associate to $2\mathrm{i}$ the product of the scaling of a factor $2$ (\SeeChapter{see section Euclidean Geometry page \pageref{scaling}}) by the rotation of center O with angle of $\pi/2$, that is to say a similitude centered at the origin. This is what we will endeavour to prove now.
	
	Given:
	
	We have the following geometric transformations properties for complex numbers (see the section Trigonometry at page \pageref{remarkable trigonometric identities} for the properties of sine and cosine) that we can happily combine at our discretion:
	\begin{enumerate}
		\item[P1.] The multiplication of $z_1$ by a real number $\lambda$ in the Gauss plane corresponds trivially to a homothety of center O (the intersection of real and imaginary axis for recall...) and of ratio $\lambda$.
		
		Indeed:
		
		
		\item[P2.] Multiplication of $z_1$ by a complex number of unit module corresponds a rotation of center O and of angle corresponding to the argument of $z_1$. Indeed:
		
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		Then we see immediately, for example, that multiplying a complex number by $\mathrm{i}$ (that is to say a complex number with $ \sin(\omega)=1,\cos(\omega)=0$) corresponds a rotation of $\pi/2$.
		\end{tcolorbox}
		\begin{theorem}
		It is interesting to notice that in vector form the rotation of center O of $z_1$ by $z_0$ can be written using the following matrix:
		
		\end{theorem}
		\begin{dem}
		We have just seen before that $z_0z_1$  is a rotation of center O of and angle $\omega$. We just need to write it first in the old style:
		
		giving in vector form:
		
		thus the linear equivalent application is:
		
		or as well (we fall back on the rotation matrix in the plane that we will see in the section of Euclidean Geometry page \pageref{rotation matrix in the plane} which is a remarkable result!) using:
		
		and in the particular and arbitrary case where $r$ is unitary (in order to have a pure rotation!):
		
		we have immediately (we took again the same notations for the angle as the one we have in the chapter Geometry):
		
		Notice that the 2D rotation matrix can also be written as\label{2d rotation matrix}:
		
		as well:
		
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		Thus we see that the rotation matrices are not only applications but also are complex numbers (well it was obvious from the start but we had to show it in an aesthetic and simple way).
		
		So, we have for usage to put that:
		
		or with another common notation in linear algebra:
		
		The field of complex numbers is isomorphic to the field of real square matrices of dimension $2$ of the type:
		
		It is a result that we use many times in various sections of this book for specific studies in algebra, geometry and relativistic quantum physics.
		
		\item[P3.] The multiplication of two complex corresponds to a homothety added to a rotation. In other words, a "\NewTerm{direct similarity}\index{direct similarity}".
		\begin{dem}
		
		so this is indeed a similarity of ratio $b$ and angle $\beta$.
		
		At the opposite, the following operation:
		
		will be named a "\NewTerm{retrograde linear similarity}\index{retrograde linear similarity}".
		
		Otherwise, it returns trivially an already known following relation:
		
		\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		\textbf{R1.} As the sum of two complex numbers $z_1+z_2$ can not have a special simplified mathematical notation in any form whatsoever, then we say that the resulting quantity is equivalent to an "\NewTerm{amplitude translation}\index{amplitude translation}".\\
		
		\textbf{R2.} The combination of a direct linear similarity (multiplication of two complex numbers) and an amplitude translation (sum by a third complex number) is what we name a "\NewTerm{direct linear similarity}\index{direct linear similarity}".
		\end{tcolorbox}
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		
		\item[P4.] The conjugate of a complex number is geometrically symmetrical with respect to the real axis such that:
		
		without forgetting that (basis of trigonometry):
		
		This gives us a known result:
		
		From which we get the following property:
		
		Hence:
		
		
		\item[P5.] The negation of the conjugate of a complex number is geometrically its symmetrical with respect to the imaginary axis such that:
		
		\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		\textbf{R1.} The combination of the properties P4, P5 is named a "\NewTerm{retrograde similarity}\index{retrograde similarity}".\\
		
		\textbf{R2.} The geometric operation that consist to take the inverse of the conjugate of a complex number (that is to say $\bar{z}^{-1}$) is named a "\NewTerm{pole inversion}\index{pole inversion}".
		\end{tcolorbox}
		
		\item[P6.] The rotation of coordinate center $c$ and angle $\varphi$ is given and denoted by:
		
		Some explanations could be useful for some readers:
		
		The complex $c$ gives a point in the Gaussian plane, which will be the center of rotation. The difference $z_1-c$ gives the chosen radius $r$. The multiplication by $e^{\mathrm{i}\varphi}$ is the counter-clockwise rotation of the radius from the origin of the Gaussian plane. Finally, the addition by $c$ is the necessary translation to take back the rotated radius $r$ at its original place before the rotation (center $c$). Which gives schematically:
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
			%uncomment if require: \path (0,475); %set diagram left start at 0, and has height of 475
			
			%Shape: Axis 2D [id:dp6503119902293624] 
			\draw  (68,102.5) -- (222.4,102.5)(141.4,32) -- (141.4,168.5) (215.4,97.5) -- (222.4,102.5) -- (215.4,107.5) (136.4,39) -- (141.4,32) -- (146.4,39)  ;
			%Flowchart: Connector [id:dp2361160288112869] 
			\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (177,102.25) .. controls (177,100.46) and (178.46,99) .. (180.25,99) .. controls (182.04,99) and (183.5,100.46) .. (183.5,102.25) .. controls (183.5,104.04) and (182.04,105.5) .. (180.25,105.5) .. controls (178.46,105.5) and (177,104.04) .. (177,102.25) -- cycle ;
			%Flowchart: Connector [id:dp843036796517211] 
			\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (138,63.25) .. controls (138,61.46) and (139.46,60) .. (141.25,60) .. controls (143.04,60) and (144.5,61.46) .. (144.5,63.25) .. controls (144.5,65.04) and (143.04,66.5) .. (141.25,66.5) .. controls (139.46,66.5) and (138,65.04) .. (138,63.25) -- cycle ;
			%Flowchart: Connector [id:dp8585498751985121] 
			\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (138,142.25) .. controls (138,140.46) and (139.46,139) .. (141.25,139) .. controls (143.04,139) and (144.5,140.46) .. (144.5,142.25) .. controls (144.5,144.04) and (143.04,145.5) .. (141.25,145.5) .. controls (139.46,145.5) and (138,144.04) .. (138,142.25) -- cycle ;
			%Flowchart: Connector [id:dp4957271666978851] 
			\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (97,102.25) .. controls (97,100.46) and (98.46,99) .. (100.25,99) .. controls (102.04,99) and (103.5,100.46) .. (103.5,102.25) .. controls (103.5,104.04) and (102.04,105.5) .. (100.25,105.5) .. controls (98.46,105.5) and (97,104.04) .. (97,102.25) -- cycle ;
			%Shape: Axis 2D [id:dp9242614420394326] 
			\draw  (278,103.5) -- (432.4,103.5)(351.4,33) -- (351.4,169.5) (425.4,98.5) -- (432.4,103.5) -- (425.4,108.5) (346.4,40) -- (351.4,33) -- (356.4,40)  ;
			%Flowchart: Connector [id:dp009892860884846089] 
			\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (387,103.25) .. controls (387,101.46) and (388.46,100) .. (390.25,100) .. controls (392.04,100) and (393.5,101.46) .. (393.5,103.25) .. controls (393.5,105.04) and (392.04,106.5) .. (390.25,106.5) .. controls (388.46,106.5) and (387,105.04) .. (387,103.25) -- cycle ;
			%Flowchart: Connector [id:dp8488475342061723] 
			\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (348,64.25) .. controls (348,62.46) and (349.46,61) .. (351.25,61) .. controls (353.04,61) and (354.5,62.46) .. (354.5,64.25) .. controls (354.5,66.04) and (353.04,67.5) .. (351.25,67.5) .. controls (349.46,67.5) and (348,66.04) .. (348,64.25) -- cycle ;
			%Flowchart: Connector [id:dp7909786783779764] 
			\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (348,143.25) .. controls (348,141.46) and (349.46,140) .. (351.25,140) .. controls (353.04,140) and (354.5,141.46) .. (354.5,143.25) .. controls (354.5,145.04) and (353.04,146.5) .. (351.25,146.5) .. controls (349.46,146.5) and (348,145.04) .. (348,143.25) -- cycle ;
			%Flowchart: Connector [id:dp8836812903380178] 
			\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (307,103.25) .. controls (307,101.46) and (308.46,100) .. (310.25,100) .. controls (312.04,100) and (313.5,101.46) .. (313.5,103.25) .. controls (313.5,105.04) and (312.04,106.5) .. (310.25,106.5) .. controls (308.46,106.5) and (307,105.04) .. (307,103.25) -- cycle ;
			%Shape: Axis 2D [id:dp2459678077097549] 
			\draw  (67,299.5) -- (221.4,299.5)(140.4,229) -- (140.4,365.5) (214.4,294.5) -- (221.4,299.5) -- (214.4,304.5) (135.4,236) -- (140.4,229) -- (145.4,236)  ;
			%Flowchart: Connector [id:dp0980197138843022] 
			\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (176,299.25) .. controls (176,297.46) and (177.46,296) .. (179.25,296) .. controls (181.04,296) and (182.5,297.46) .. (182.5,299.25) .. controls (182.5,301.04) and (181.04,302.5) .. (179.25,302.5) .. controls (177.46,302.5) and (176,301.04) .. (176,299.25) -- cycle ;
			%Flowchart: Connector [id:dp7373458869457048] 
			\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (137,260.25) .. controls (137,258.46) and (138.46,257) .. (140.25,257) .. controls (142.04,257) and (143.5,258.46) .. (143.5,260.25) .. controls (143.5,262.04) and (142.04,263.5) .. (140.25,263.5) .. controls (138.46,263.5) and (137,262.04) .. (137,260.25) -- cycle ;
			%Flowchart: Connector [id:dp5767959610054052] 
			\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (137,339.25) .. controls (137,337.46) and (138.46,336) .. (140.25,336) .. controls (142.04,336) and (143.5,337.46) .. (143.5,339.25) .. controls (143.5,341.04) and (142.04,342.5) .. (140.25,342.5) .. controls (138.46,342.5) and (137,341.04) .. (137,339.25) -- cycle ;
			%Flowchart: Connector [id:dp41350487186211815] 
			\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (96,299.25) .. controls (96,297.46) and (97.46,296) .. (99.25,296) .. controls (101.04,296) and (102.5,297.46) .. (102.5,299.25) .. controls (102.5,301.04) and (101.04,302.5) .. (99.25,302.5) .. controls (97.46,302.5) and (96,301.04) .. (96,299.25) -- cycle ;
			%Shape: Axis 2D [id:dp28921551119243394] 
			\draw  (278,299.5) -- (432.4,299.5)(351.4,229) -- (351.4,365.5) (425.4,294.5) -- (432.4,299.5) -- (425.4,304.5) (346.4,236) -- (351.4,229) -- (356.4,236)  ;
			%Flowchart: Connector [id:dp010643368019140453] 
			\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (387,299.25) .. controls (387,297.46) and (388.46,296) .. (390.25,296) .. controls (392.04,296) and (393.5,297.46) .. (393.5,299.25) .. controls (393.5,301.04) and (392.04,302.5) .. (390.25,302.5) .. controls (388.46,302.5) and (387,301.04) .. (387,299.25) -- cycle ;
			%Flowchart: Connector [id:dp2880438571130193] 
			\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (348,260.25) .. controls (348,258.46) and (349.46,257) .. (351.25,257) .. controls (353.04,257) and (354.5,258.46) .. (354.5,260.25) .. controls (354.5,262.04) and (353.04,263.5) .. (351.25,263.5) .. controls (349.46,263.5) and (348,262.04) .. (348,260.25) -- cycle ;
			%Flowchart: Connector [id:dp5861682729073594] 
			\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (348,339.25) .. controls (348,337.46) and (349.46,336) .. (351.25,336) .. controls (353.04,336) and (354.5,337.46) .. (354.5,339.25) .. controls (354.5,341.04) and (353.04,342.5) .. (351.25,342.5) .. controls (349.46,342.5) and (348,341.04) .. (348,339.25) -- cycle ;
			%Flowchart: Connector [id:dp15312420972881347] 
			\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (307,299.25) .. controls (307,297.46) and (308.46,296) .. (310.25,296) .. controls (312.04,296) and (313.5,297.46) .. (313.5,299.25) .. controls (313.5,301.04) and (312.04,302.5) .. (310.25,302.5) .. controls (308.46,302.5) and (307,301.04) .. (307,299.25) -- cycle ;
			%Straight Lines [id:da3980272631290729] 
			\draw    (141.4,102.5) -- (202.66,42.11) ;
			\draw [shift={(204.8,40)}, rotate = 135.41] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
			%Straight Lines [id:da2752515975393197] 
			\draw [line width=0.75]    (141.4,102.5) -- (153.14,77.22) ;
			\draw [shift={(154.4,74.5)}, rotate = 114.9] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
			%Straight Lines [id:da016508519287945278] 
			\draw    (351.4,103.5) -- (363.14,78.22) ;
			\draw [shift={(364.4,75.5)}, rotate = 114.9] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
			%Straight Lines [id:da497641302382944] 
			\draw    (364.4,75.5) -- (303.14,135.89) ;
			\draw [shift={(301,138)}, rotate = 315.41] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
			%Straight Lines [id:da9227486698538727] 
			\draw [line width=2.25]    (351.4,103.5) -- (328.8,119) -- (305.13,135.18) ;
			\draw [shift={(301,138)}, rotate = 325.65] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (14.29,-6.86) -- (0,0) -- (14.29,6.86) -- cycle    ;
			%Straight Lines [id:da8270772413404319] 
			\draw    (351.4,299.5) -- (412.66,239.11) ;
			\draw [shift={(414.8,237)}, rotate = 135.41] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
			%Straight Lines [id:da027089139454496758] 
			\draw [line width=2.25]    (414.8,237) -- (392.2,252.5) -- (368.53,268.68) ;
			\draw [shift={(364.4,271.5)}, rotate = 325.65] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (14.29,-6.86) -- (0,0) -- (14.29,6.86) -- cycle    ;
			%Straight Lines [id:da8118574805859802] 
			\draw    (140.4,299.5) -- (201.66,239.11) ;
			\draw [shift={(203.8,237)}, rotate = 135.41] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
			%Straight Lines [id:da5630207552323494] 
			\draw [line width=2.25]    (203.8,237) -- (181.2,252.5) -- (157.53,268.68) ;
			\draw [shift={(153.4,271.5)}, rotate = 325.65] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (14.29,-6.86) -- (0,0) -- (14.29,6.86) -- cycle    ;
			%Shape: Circle [id:dp6779297153514998] 
			\draw   (354.8,238.5) .. controls (354.8,204.53) and (382.33,177) .. (416.3,177) .. controls (450.27,177) and (477.8,204.53) .. (477.8,238.5) .. controls (477.8,272.47) and (450.27,300) .. (416.3,300) .. controls (382.33,300) and (354.8,272.47) .. (354.8,238.5) -- cycle ;
			%Straight Lines [id:da7630414240749388] 
			\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ][line width=2.25]    (413.8,237) -- (429.8,259) -- (448,285.86) ;
			\draw [shift={(450.8,290)}, rotate = 235.89] [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (14.29,-6.86) -- (0,0) -- (14.29,6.86) -- cycle    ;
			%Shape: Arc [id:dp7883693111854888] 
			\draw  [draw opacity=0] (429.23,258.01) .. controls (421.41,262.54) and (411.65,263.5) .. (402.66,259.77) .. controls (398.56,258.06) and (395.04,255.55) .. (392.2,252.5) -- (414.17,232.06) -- cycle ; \draw   (429.23,258.01) .. controls (421.41,262.54) and (411.65,263.5) .. (402.66,259.77) .. controls (398.56,258.06) and (395.04,255.55) .. (392.2,252.5) ;  
			
			% Text Node
			\draw (130,55) node [anchor=north west][inner sep=0.75pt]   [align=left] {i};
			% Text Node
			\draw (118,134) node [anchor=north west][inner sep=0.75pt]   [align=left] {$-\mathrm{i}$};
			% Text Node
			\draw (179,107) node [anchor=north west][inner sep=0.75pt]   [align=left] {$1$};
			% Text Node
			\draw (92,106) node [anchor=north west][inner sep=0.75pt]   [align=left] {$-1$};
			% Text Node
			\draw (340,56) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\mathrm{i}$};
			% Text Node
			\draw (327,135) node [anchor=north west][inner sep=0.75pt]   [align=left] {$-\mathrm{i}$};
			% Text Node
			\draw (389,107) node [anchor=north west][inner sep=0.75pt]   [align=left] {$1$};
			% Text Node
			\draw (300,89) node [anchor=north west][inner sep=0.75pt]   [align=left] {$-1$};
			% Text Node
			\draw (129,252) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\mathrm{i}$};
			% Text Node
			\draw (118,331) node [anchor=north west][inner sep=0.75pt]   [align=left] {$-\mathrm{i}$};
			% Text Node
			\draw (179,304) node [anchor=north west][inner sep=0.75pt]   [align=left] {$1$};
			% Text Node
			\draw (91,303) node [anchor=north west][inner sep=0.75pt]   [align=left] {$-1$};
			% Text Node
			\draw (340,252) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\mathrm{i}$};
			% Text Node
			\draw (327,331) node [anchor=north west][inner sep=0.75pt]   [align=left] {$-\mathrm{i}$};
			% Text Node
			\draw (389,303) node [anchor=north west][inner sep=0.75pt]   [align=left] {$1$};
			% Text Node
			\draw (302,303) node [anchor=north west][inner sep=0.75pt]   [align=left] {$-1$};
			% Text Node
			\draw (270,136) node [anchor=north west][inner sep=0.75pt]   [align=left] {$z_1-c$};
			% Text Node
			\draw (152,58) node [anchor=north west][inner sep=0.75pt]   [align=left] {$z_1$};
			% Text Node
			\draw (206.8,43) node [anchor=north west][inner sep=0.75pt]   [align=left] {$c$};
			% Text Node
			\draw (360,60) node [anchor=north west][inner sep=0.75pt]   [align=left] {$z_1$};
			% Text Node
			\draw (330.17,119) node [anchor=north west][inner sep=0.75pt]   [align=left] {$r$};
			% Text Node
			\draw (416,224) node [anchor=north west][inner sep=0.75pt]   [align=left] {$c$};
			% Text Node
			\draw (380.17,270) node [anchor=north west][inner sep=0.75pt]   [align=left] {$r$};
			% Text Node
			\draw (205,224) node [anchor=north west][inner sep=0.75pt]   [align=left] {$c$};
			% Text Node
			\draw (174.17,243) node [anchor=north west][inner sep=0.75pt]   [align=left] {$r$};
			% Text Node
			\draw (147.25,269.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$c+(z_1-c)$};
			% Text Node
			\draw (310,237.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$c+(z_1-c)$};
			% Text Node
			\draw (445.25,289.5) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ] [align=left] {$c+e^{\mathrm{i}\varphi}(z_1-c)$};
			% Text Node
			\draw (394,264) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\varphi>0$};
			% Text Node
			\draw (180,154) node [anchor=north west][inner sep=0.75pt]   [align=left] {(a)};
			% Text Node
			\draw (398,155) node [anchor=north west][inner sep=0.75pt]   [align=left] {(b)};
			% Text Node
			\draw (179,349) node [anchor=north west][inner sep=0.75pt]   [align=left] {(c)};
			% Text Node
			\draw (398,347) node [anchor=north west][inner sep=0.75pt]   [align=left] {(d)};
			\end{tikzpicture}
			\vspace*{3mm}
		\caption{Representation of the complex rotation}
	\end{figure}
	
		\item[P7.] On the same idea, we get and denote an homothety of center $c$ and ratio $\lambda$ by:
		
		Some explanations could be useful for some readers:
		
		The difference $z_1-c$ always gives the radius $r$ and $c$ a central point in the Gauss plane. The expression $\lambda(z_1-c)$ gives the homothety of the radius from the origin of the Gaussian plane, and finally by adding $c$ gives the necessary translation for the homothety to be see as being made from center $c$.
	\end{enumerate}
	
	\subsubsection{Quaternion Numbers}\label{quaternions}
	Also named "\NewTerm{hypercomplex}\index{hypercomplex}" quaternions numbers were invented in 11843 (holocene calendar) by William Rowan Hamilton to generalize complex numbers.
	
	\textbf{Definition (\#\thesection.\mydef):} A "\NewTerm{quaternion}\index{quaternion}" is an element $(a,b,c,d)\in \mathbb{R}^4$ and for which we denote by $\mathbb{H}$ the set that contains it and what we name the "\NewTerm{set of quaternions}\index{set of quaternions}".
	
	A quaternion can also be represented in a row or column such as:
	
	We define the sum of two quaternions $(a, b, c, d)$ and $(a ', b', c ', d')$ by:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	It is the natural addition in $\mathbb{R}^4$ seen as a $\mathbb{R}$-vector space (\SeeChapter{see section Set Theory page \pageref{vector space}}).
	\end{tcolorbox}
	The associativity is verified by applying the corresponding properties of the operations on $\mathbb{R}$.
	
	We also define the multiplication:
	
	of two quaternions $(a, b, c, d)$ and $(a', b', c', d')$ by the expression:
	
	It may be hard to accept but we will be a little further below that there is a family resemblance with the complex numbers.
	
	We can notice that the law of multiplication is not commutative. Indeed, taking the definition of the multiplication above, we have:
	
	But we can also notice that:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	It is the natural multiplication in $\mathbb{R}^4$ seen as a $\mathbb{R}$-vector space (\SeeChapter{see section Set Theory page \pageref{vector space}}).
	\end{tcolorbox}
	The law of multiplication is distributive with the addition law but it is an excellent example where we must still be careful to prove the left and right distributivity, since the product is not commutative!
	
	The multiplication have for neutral element:
	
	Indeed:
	
	Any element:
	
	is invertible.
	
	Indeed, if $(a,b,c,d)$ is a non-null quaternion, we then have necessarily:
	
	otherwise the four numbers $a, b, c, d$ are of square null, so all zero. Given then the quaternion $(a_1,b_1,c_1,d_1)$ defined by:
	
	then by applying mechanically  the definition of the multiplication of quaternions, we check that:
	
	this latter quaternion is therefore the inverse for the multiplication!
	
	Let us prove (for general knowledge) that the field of complex numbers $(\mathbb{C},+,\times)$ is a subfield of $(\mathbb{H},+,\times)$.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	We could also have put this proof in the section of Set Theory because we will make use of a lot of concepts that are have seen there but it seemed to us a little more relevant to put instead the proof here. We expect the reader to tolerate this choice.
	\end{tcolorbox}
	Given $\mathbb{H}'$ a set of quaternions of the form $(a, b, 0,0)$. If $\mathbb{H}'$ is not empty, and if $(a, b, 0,0)$, $(a ', b', 0.0)$ are elements $\mathbb{H}'$ then $(\mathbb{H}',+\times)$ is a field. Indeed:
	\begin{enumerate}
		\item[P1.] For subtraction (and therefore the addition):
		

		\item[P2.] The multiplication:
			

		\item[P3.] The neutral element:
		

		\item[P4.] And finally the inverse:
		
		of $(a,b,0,0)$ is still in $\mathbb{H}'$.
	\end{enumerate}
	Therefore $(\mathbb{H}',+,\times)$ is a subfield of $\mathbb{H}$. Given then the application:
	
	$f$ is bijective, and we easily check that for any complex $z_1,z_2$, we have:
	
	Therefore $f$ is an isomorphism of $(\mathbb{C},+,\times)$ on $(\mathbb{H}',+,\times)$.
	
	This isomorphism has for interest (caused) to identify $\mathbb{C}$ to $\mathbb{H}'$ and to write $\mathbb{C} \subset\mathbb{H}$, the laws of addition and subtraction on $\mathbb{H}$ extending the already known operations of $\mathbb{C}$.
	
	Thus, by convention, we will write any element of $(a, b, 0,0)$ of $\mathbb{H}'$ in the complex form $a + \mathrm{i}b$. Particularly $0$ is the element $(0,0,0,0)$, $1$ is the element $(1,0,0,0)$ and $\mathrm{i}$ and the element $(0,1,0,0)$.
	
	We denote by analogy and by extension $\mathrm{j}$ the element $(0,0,1,0)$ and $\mathrm{k}$ the element $(0,0,0,1)$. The family $\{1, \mathrm{i}, \mathrm{j}, \mathrm{k}\}$ form a basis of all quaternions seen as a vector space on $\mathbb{R}$, and we will write:
	
	the quaternion $(a, b, c, d)$.

	The notation of quaternions as defined above is perfectly suited to the multiplication operation. For the product of two quaternions we get by developing the expression:
	
	$16$ terms that we have to identify to the original definition of multiplication of quaternions to get the following relations:
	
	Which can be summarized in a table:
	
	Or in a 3D representation of what happens in the corresponding 4D sphere (red, green and blue for $\mathrm{i}$, $\mathrm{j}$ and $\mathrm{k}$ respectively):
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/quaternions_3D_4D_sphere_representation.jpg}
		\caption[]{Starting situation for quaternion rotations}
	\end{figure}
	We can see that the expression of the multiplication of two quaternions looks partly much like a vector product (denoted $\times$ in this book) and dot product (denoted $\circ$ in this book):
	
	If this is not evident (which would be quite understandable), let us make a concrete example:
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Given two quaternions without real part:
	
	and $\vec{u},\vec{v}$ the vectors of $\mathbb{R}^3$ of respective components $(x,y,z)$ and $(x',y',z')$. Then the product:
	
	is equal to:
	
	We can also for curiosity interest us to the general case ... Given for this two quaternions:
	
	Then we have:
	
	\end{tcolorbox}
	\textbf{Definition (\#\thesection.\mydef):} The center of the non-commutative field $(\mathbb{H},+,\times)$ is the set of elements of $\mathbb{H}$ commuting for the law of multiplication with all the elements of $\mathbb{H}$.

	\begin{theorem}
	The center of $(\mathbb{H},+,\times)$ is the set of real numbers!
	\end{theorem}
	\begin{dem}
Give $\mathbb{H}_1$ is the center of $(\mathbb{H},+,\times)$, and $(x, y, z, t)$ a quaternion. We must have the following conditions that are met:

	Given $(x,y,z,t)\in \mathbb{H}_1 $ then for any $(a,b,c,d)\in \mathbb{H}$ we seek:
	
	which give by developing:
	
	after simplification (the first line of the previous system is equal to zero on both sides of equality):
	
	the resolution of this system gives us:
	
	So that the quaternion $(x, y, z, t)$ is the center of $\mathbb{H}$ it must be real (not imaginary parts)!
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Just as for complex numbers, we can define a conjugate of quaternions:
	
	\textbf{Definition (\#\thesection.\mydef):} The conjugate of a quaternion $Z=(a,b,c,d)$ is the quaternion $\bar{Z}=(a,-b,-c,-d)$.

	Just as for the complex number, we notice that:
	\begin{enumerate}
		\item First clearly that if $Z=\bar{Z}$ then it means that $Z\in \mathbb{R}$

		\item That $Z+\bar{Z}\in \mathbb{R}$

		\item That by developing the product $Z\bar{Z}$ we have:
		
		that we will adopt, by analogy with complex numbers, as a definition of the norm (or module) of quaternions such as:
		
		Therefore we also have immediately (relation which will be useful later):
		
	\end{enumerate}
	As for complex numbers (see below), it is easy to show that the conjugation is an automorphism of the group $(\mathbb{H},+)$.
	
	Indeed, given $Z=(a,b,c,d)$ and $Z'=(a',b',c',d')$ then:
	
	It is also easy to prove that it is involutive. Indeed:
	
	But the conjugation is not a multiplicative automorphism of the field $(\mathbb{H},+,\times)$. Indeed, if we consider the multiplication of $Z$, $Z'$ and take the conjugate:
	
	we see immediately (at least for the second row) that we have:
	
	Let us now back to our norm (or module) .... For this, let us calculate the square of the norm $|ZZ'|$:
	
	We know (by definition) that:
	
	Let us denote this product in such a way that:
	
	Then we have:
	
	substituting it comes:
	
	after an elementary algebraic development (frankly boring) we find:
	
	Therefore:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The norm is therefore a homomorphism (\SeeChapter{see section Set Theory page \pageref{homomorphism of group}}) of $(\mathbb{H},\times)$ in $(\mathbb{R},\times)$. Subsequently, we will denote by $\mathbb{G}$ all the quaternions of unit norm.
	\end{tcolorbox}
	
	\paragraph{Matrix Interpretation of Quaternions}\mbox{}\\\\
	Given $q$ and $p$ two quaternions and given the application:
	
	The  (left) multiplication can be made with a linear application (\SeeChapter{see section Linear Algebra page \pageref{linear application}}) on $\mathbb{H}$.
	
	If $q$ is written:
	
	this application has for matrix in the basis $(1,\mathrm{i},\mathrm{j},\mathrm{k})$:
	
	What we check well:
	
	In fact, we can then define the quaternions as the set of matrices with the visible structure above if we wanted to. This will then reduce them to a sub vector space of $M_4(\mathbb{R})$.
	
	Especially, the matrix of $1$ (the real part of the quaternion $q$) is then nothing other than the identity matrix:
	
	as well:
	
	
	\paragraph{Rotations with Quaternions}\mbox{}\\\\
	We will see now that conjugation by an element of the group $\mathbb{G}$ of the quaternions of unit norm can be interpreted as a pure rotation in space!
	
	\textbf{Definition (\#\thesection.\mydef):} The "\NewTerm{conjugation}\index{conjugation}" by a non-null quaternion $q$ of unit norm is the application $S_q$ defined on $\mathbb{H}$ by:
	
	and we affirm that this application is a rotation.
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} As $q$ is of unit norm $1$, we have obviously $|q|=q\bar{q}=1$ therefore $q^{-1}=\bar{q}$. This quaternion can be seen as the proper value (of unit norm) to the application (matrix) $p$ on the vector $\bar{q}$ (we are in a similar situation as the orthogonal rotation matrices seen in the section of Linear Algebra page \pageref{orthogonal matrix}).\\
	
	\textbf{R2.} $S_q$ is a linear application (so if it is rotation, the rotation can be decomposed into several rotations). Indeed, let consider two quaternions $p_1,p_2$ and two real number $\lambda_1,\lambda_2$, then we have:
	
	\end{tcolorbox}
	Let us now check that the application is indeed a pure rotation. As we saw in our study of Linear Algebra and in particular of orthogonal matrices (\SeeChapter{see section Linear Algebra page \pageref{orthogonal matrix}}), a first obvious condition is that the application conserves the norm.
	
	Let us check this:
	
	Moreover, we can check that a rotation of a purely complex quaternion (such that then we restrict ourselves to $\mathbb{R}^3$) and the same summed reverse rotation is zero (the vector sum up to its opposite cancel):
	
	we trivially check that if we have two quaternions $q, p$ then $\overline{p\cdot q}=\bar{q}\bar{p}$ since then:
	
	for this operation to be zero, we immediately see that we need to restrict ourselves to the purely complex quaternions $p$. Since then:
	
	We conclude then that $p$ must be purely complex so the for the application $S_q$ is a rotation and that $S_q(p)$ is a pure quaternion. In other words, this application is stable (in other words: a pure quaternion by this application remains a pure quaternion).
	
	The application $S_q$ restricted to all purely complex quaternions is thus a vectorial isometry, that is to say a symmetry or a rotation.
	
	We have also seen during our study of the rotation matrices in the section of Linear Algebra page \pageref{rotation matrix in linear algebra} and Euclidean Geometry page \pageref{rotation matrix in the plane} that such matrices should have a determinant equal to $+1$ so that we have a rotation. Let's see if this is the case of $S_q$:
	
	For this, we explicitly calculate in function of:
	
	the matrix (in the canonical basis $(\mathrm{i},\mathrm{j},\mathrm{k})$) of $S_q$ and we calculate its determinant. Thus we obtain the coefficients of the columns of this application by remembering that:
	
	and then by calculating:
	
	
	
	
	
	We must then calculate the determinant of the following matrix (pfff ...):
	
	remembering that (which also simplifies the expression of the terms of the diagonal as we can see in some books):
	
	we find that the determinant is indeed equal to $1$. Otherwise, we can check this with Maple 4.00b:
	
	\texttt{>with(linalg):\\
	>A:=linalg[matrix](3,3,[a\string^2+b\string^2-c\string^2-d\string^2,2*(a*d+b*c),\\
	2*(b*d-a*c),2*(b*c-a*d),a\string^2-b\string^2+c\string^2-d\string^2,2*(a*b+c*d),\\
	2*(a*c+b*d),2*(c*d-a*b),a\string^2-b\string^2-c\string^2+d\string^2]);\\
	>factor(det(A));}
	
	Let us now show that this rotation is a half axis turn (the example that may seem particular is in fact general!):
	
	First, if:
	
	we have:
	
	which means that the axis of rotation $(x, y, z)$ is fixed by the application $S_q$ itself!
	
	On the other hand, we have seen that if $q$ is a purely complex quaternion of norm $1$ then:
	
	Which gives us the relation:
	
	This result leads us to calculate the rotation of a rotation:
	
	Conclusion: Since the rotation of a rotation is a full turn, then $S_q$ is necessarily a half-turn:
	
	relatively (!) to the axis $(x, y, z)$.
	
	At this stage, we can say that any rotation of the space can be represented by $S_q$ (the conjugation by a quaternion $q$ of norm $1$). Indeed, the half turns generates the group of rotations, that is to say that any rotation can be expressed as the product of a finite number of half-turns, and therefore by conjugation of a product of quaternions unitary norm (product which is itself a quaternion of unitary norm...).
	
	We will still give an explicit form connecting a rotation and the quaternion that represents it, just as we did for complex numbers.
	\begin{theorem}
	Given $\vec{u}(x,y,z)$ a unit vector and $\theta \in [0,2\pi]$ angle. Then we affirm that the rotation of axis $\vec{u}$ and angle $\theta$ corresponds to the application $S_q$, where $q$ is the quaternion:
	
	For this assertion is verified, we know we need that: 
	\begin{itemize}
		\item The norm of $q$ is equal to $1$

		\item The determinant of the application $S_q$ is equal to $1$

		\item The application $S_q$ conserves the norm
	
		\item The application $S_q$ returns all collinear vector to the axis of rotation on the axis of rotation itself.
	\end{itemize}
	\end{theorem}
	\begin{dem}
	Ok let us check every point:
	\begin{enumerate}
		\item The norm of the quaternion previously proposed is indeed equal to $1$:
		
		and as $\vec{u}(x,y,z)$ is of unit norm, we have:
		
		Therefore:
		
		
		\item The fact that $q$ is a quaternion of unit norm immediately leads to the fact that the determinant of the application $S_q$ is also equal to $1$. We have already proved it above in the general case of any quaternion of norm $1$ (necessary and sufficient condition).
		
		\item It is the same for the conservation of the norm. We have already proved earlier above that this was the case anyway when the quaternion $q$ of norm $1$ (necessary and sufficient condition).

		\item Let us now prove that all collinear vector to the axis of rotation is projected onto the axis of rotation itself. Let us denote by $q'$ the purely imaginary unitary quaternion $xi+yj+zk$. Then we have:
		
		Then:
		
		but as $q'$ is the restriction of $q$ to the pure elements that constitute it, this is equivalent as to write:
		
		Let us now show why we choose the writing $\theta/2$. If $\vec{v}=(x_1,y_1,z_1)$ denotes a unit vector orthogonal to $\vec{u}$ (therefore perpendicular to the axis of rotation), and $p$ the quaternion $x\mathrm{i}+y\mathrm{j}+z\mathrm{k}$ then we have:	
		
		We have shown during the definition of multiplication of two quaternions that:
		
		therefore we get:
		
		We have also prove earlier above that:
		
		Therefore:
		
		(the half turn of axis $(x, y, z)$). So:
		
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		We are beginning to see here already the usefulness of having chose from the beginning $\theta/2$ for the angle!
		\end{tcolorbox}
	\end{enumerate}
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	We know that $p$ is the pure quaternion likened to a unit vector $\vec{v}$ orthogonal to the axis of rotation $\vec{u}$ itself equated with the purely imaginary part of $q'$. We notice then immediately that the imaginary part of the product (defined!) of the quaternion $q'p$ is equal to the cross product $\vec{u}\times\vec{v}=\vec{w}$. This vector product therefore generates a vector perpendicular to $\vec{u},\vec{v}$.
	
	The pair $(\vec{v},\vec{w})$ thus form a plane perpendicular to the axis of rotation $\vec{u}$ (that's as for the simple complex numbers $\mathbb{C}$ in which we have the Gaussian plane and perpendicular to it the axis of rotation!).
	
	Then finally:
	
	We fall back with on rotation based on a plane (but therefore be in space!) identical to that shown earlier above with the standard complex numbers $\mathbb{C}$ in the Gaussian plane. For more details the reader can refer the section of Spinor Calculus page \pageref{spinors}.
	
	So we know how to do any kind of rotation in space in a single mathematical operation and with a bonus: with the free choice of the axis!

	We can now better understand why the algebra of quaternions is not commutative. Indeed, the vector rotations of the plan are commutative but those of space are not like show us the example below:
	
	Given the initial configuration:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1,scale=0.8]
		%uncomment if require: \path (0,468); %set diagram left start at 0, and has height of 468
		
		%Shape: Cube [id:dp854266058210877] 
		\draw   (223,147.1) -- (273.1,97) -- (406.3,97) -- (406.3,213.9) -- (356.2,264) -- (223,264) -- cycle ; \draw   (406.3,97) -- (356.2,147.1) -- (223,147.1) ; \draw   (356.2,147.1) -- (356.2,264) ;
		%Shape: Rectangle [id:dp6002775516083763] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (223,147.1) -- (356.2,147.1) -- (356.2,264) -- (223,264) -- cycle ;
		%Straight Lines [id:da6279662835778315] 
		\draw    (295.3,204) -- (210.64,297.52) ;
		\draw [shift={(209.3,299)}, rotate = 312.15] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Parallelogram [id:dp3287628989076503] 
		\draw  [fill={rgb, 255:red, 248; green, 231; blue, 28 }  ,fill opacity=1 ] (273.2,96.95) -- (405.95,96.95) -- (355.75,147.1) -- (223,147.1) -- cycle ;
		%Straight Lines [id:da295158764101221] 
		\draw    (314.47,122.03) -- (314.47,18.03) ;
		\draw [shift={(314.47,16.03)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Polygon [id:ds5494881254406647] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (356.2,147.1) -- (356.2,264) -- (406.3,213.9) -- (405.95,96.95) -- cycle ;
		%Straight Lines [id:da06487802044464974] 
		\draw    (382.3,183) -- (494.3,183) ;
		\draw [shift={(496.3,183)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		
		% Text Node
		\draw (185,300.4) node [anchor=north west][inner sep=0.75pt]    {$Z$};
		% Text Node
		\draw (501,175.4) node [anchor=north west][inner sep=0.75pt]    {$X$};
		% Text Node
		\draw (293,16.4) node [anchor=north west][inner sep=0.75pt]    {$Y$};	
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[]{Starting situation for quaternion rotations}
	\end{figure}
	Then a rotation about the $X$-axis followed by a rotation around the $Y$ axis:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1,scale=0.8]
		%uncomment if require: \path (0,468); %set diagram left start at 0, and has height of 468
		
		%Shape: Cube [id:dp854266058210877] 
		\draw   (42,142.1) -- (92.1,92) -- (225.3,92) -- (225.3,208.9) -- (175.2,259) -- (42,259) -- cycle ; \draw   (225.3,92) -- (175.2,142.1) -- (42,142.1) ; \draw   (175.2,142.1) -- (175.2,259) ;
		%Shape: Rectangle [id:dp6002775516083763] 
		\draw  [fill={rgb, 255:red, 248; green, 231; blue, 28 }  ,fill opacity=1 ] (42,142.1) -- (175.2,142.1) -- (175.2,259) -- (42,259) -- cycle ;
		%Straight Lines [id:da6279662835778315] 
		\draw    (114.3,199) -- (29.64,292.52) ;
		\draw [shift={(28.3,294)}, rotate = 312.15] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Parallelogram [id:dp3287628989076503] 
		\draw  [fill={rgb, 255:red, 189; green, 16; blue, 224 }  ,fill opacity=1 ] (92.2,91.95) -- (224.95,91.95) -- (174.75,142.1) -- (42,142.1) -- cycle ;
		%Straight Lines [id:da295158764101221] 
		\draw    (133.47,117.03) -- (133.47,13.03) ;
		\draw [shift={(133.47,11.03)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Polygon [id:ds5494881254406647] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (175.2,142.1) -- (175.2,259) -- (225.3,208.9) -- (224.95,91.95) -- cycle ;
		%Straight Lines [id:da06487802044464974] 
		\draw    (201.3,178) -- (313.3,178) ;
		\draw [shift={(315.3,178)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Cube [id:dp7430057818161375] 
		\draw   (360,141.1) -- (410.1,91) -- (543.3,91) -- (543.3,207.9) -- (493.2,258) -- (360,258) -- cycle ; \draw   (543.3,91) -- (493.2,141.1) -- (360,141.1) ; \draw   (493.2,141.1) -- (493.2,258) ;
		%Shape: Rectangle [id:dp9889252961840849] 
		\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (360,141.1) -- (493.2,141.1) -- (493.2,258) -- (360,258) -- cycle ;
		%Straight Lines [id:da1545312872929705] 
		\draw    (432.3,198) -- (347.64,291.52) ;
		\draw [shift={(346.3,293)}, rotate = 312.15] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Parallelogram [id:dp7743823661456366] 
		\draw  [fill={rgb, 255:red, 189; green, 16; blue, 224 }  ,fill opacity=1 ] (410.2,90.95) -- (542.95,90.95) -- (492.75,141.1) -- (360,141.1) -- cycle ;
		%Straight Lines [id:da39845627905011627] 
		\draw    (451.47,116.03) -- (451.47,12.03) ;
		\draw [shift={(451.47,10.03)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Polygon [id:ds259471879486632] 
		\draw  [fill={rgb, 255:red, 248; green, 231; blue, 28 }  ,fill opacity=1 ] (493.2,141.1) -- (493.2,258) -- (543.3,207.9) -- (542.95,90.95) -- cycle ;
		%Straight Lines [id:da926776502828371] 
		\draw    (519.3,177) -- (631.3,177) ;
		\draw [shift={(633.3,177)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Arc [id:dp825778999071634] 
		\draw  [draw opacity=0] (274.56,206.2) .. controls (271.36,207.36) and (267.9,208) .. (264.3,208) .. controls (247.73,208) and (234.3,194.57) .. (234.3,178) .. controls (234.3,161.43) and (247.73,148) .. (264.3,148) .. controls (267.9,148) and (271.36,148.64) .. (274.56,149.8) -- (264.3,178) -- cycle ; \draw   (274.56,206.2) .. controls (271.36,207.36) and (267.9,208) .. (264.3,208) .. controls (247.73,208) and (234.3,194.57) .. (234.3,178) .. controls (234.3,161.43) and (247.73,148) .. (264.3,148) .. controls (267.9,148) and (271.36,148.64) .. (274.56,149.8) ;  
		%Straight Lines [id:da7703776746716067] 
		\draw    (263.2,207.8) -- (271.59,206.62) ;
		\draw [shift={(274.56,206.2)}, rotate = 171.98] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Shape: Arc [id:dp2827356048387415] 
		\draw  [draw opacity=0] (479.49,44.22) .. controls (480.66,47.42) and (481.31,50.87) .. (481.3,54.47) .. controls (481.28,70.91) and (467.83,84.22) .. (451.26,84.2) .. controls (434.69,84.18) and (421.28,70.83) .. (421.3,54.39) .. controls (421.31,50.79) and (421.96,47.33) .. (423.14,44.14) -- (451.3,54.43) -- cycle ; \draw   (479.49,44.22) .. controls (480.66,47.42) and (481.31,50.87) .. (481.3,54.47) .. controls (481.28,70.91) and (467.83,84.22) .. (451.26,84.2) .. controls (434.69,84.18) and (421.28,70.83) .. (421.3,54.39) .. controls (421.31,50.79) and (421.96,47.33) .. (423.14,44.14) ;  
		%Straight Lines [id:da9861470217269179] 
		\draw    (480.87,55.8) -- (479.87,47.42) ;
		\draw [shift={(479.51,44.44)}, rotate = 83.2] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		
		% Text Node
		\draw (4,295.4) node [anchor=north west][inner sep=0.75pt]    {$Z$};
		% Text Node
		\draw (320,170.4) node [anchor=north west][inner sep=0.75pt]    {$X$};
		% Text Node
		\draw (112,11.4) node [anchor=north west][inner sep=0.75pt]    {$Y$};
		% Text Node
		\draw (322,294.4) node [anchor=north west][inner sep=0.75pt]    {$Z$};
		% Text Node
		\draw (638,169.4) node [anchor=north west][inner sep=0.75pt]    {$X$};
		% Text Node
		\draw (430,10.4) node [anchor=north west][inner sep=0.75pt]    {$Y$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[]{Example quaternion $X-Y$ rotations}
	\end{figure}
	is not equal to a rotation around the $Y$-axis followed by a rotation about the axis $X$:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1,scale=0.8]
		%uncomment if require: \path (0,701); %set diagram left start at 0, and has height of 701
		
		%Shape: Cube [id:dp854266058210877] 
		\draw   (363,146.1) -- (413.1,96) -- (546.3,96) -- (546.3,212.9) -- (496.2,263) -- (363,263) -- cycle ; \draw   (546.3,96) -- (496.2,146.1) -- (363,146.1) ; \draw   (496.2,146.1) -- (496.2,263) ;
		%Shape: Rectangle [id:dp6002775516083763] 
		\draw  [fill={rgb, 255:red, 248; green, 231; blue, 28 }  ,fill opacity=1 ] (363,146.1) -- (496.2,146.1) -- (496.2,263) -- (363,263) -- cycle ;
		%Straight Lines [id:da6279662835778315] 
		\draw    (435.3,203) -- (350.64,296.52) ;
		\draw [shift={(349.3,298)}, rotate = 312.15] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Parallelogram [id:dp3287628989076503] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (413.2,95.95) -- (545.95,95.95) -- (495.75,146.1) -- (363,146.1) -- cycle ;
		%Straight Lines [id:da295158764101221] 
		\draw    (454.47,121.03) -- (454.47,17.03) ;
		\draw [shift={(454.47,15.03)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Polygon [id:ds5494881254406647] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (496.2,146.1) -- (496.2,263) -- (546.3,212.9) -- (545.95,95.95) -- cycle ;
		%Straight Lines [id:da06487802044464974] 
		\draw    (522.3,182) -- (634.3,182) ;
		\draw [shift={(636.3,182)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Cube [id:dp7430057818161375] 
		\draw   (34,147.1) -- (84.1,97) -- (217.3,97) -- (217.3,213.9) -- (167.2,264) -- (34,264) -- cycle ; \draw   (217.3,97) -- (167.2,147.1) -- (34,147.1) ; \draw   (167.2,147.1) -- (167.2,264) ;
		%Shape: Rectangle [id:dp9889252961840849] 
		\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (34,147.1) -- (167.2,147.1) -- (167.2,264) -- (34,264) -- cycle ;
		%Straight Lines [id:da1545312872929705] 
		\draw    (106.3,204) -- (21.64,297.52) ;
		\draw [shift={(20.3,299)}, rotate = 312.15] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Parallelogram [id:dp7743823661456366] 
		\draw  [fill={rgb, 255:red, 248; green, 231; blue, 28 }  ,fill opacity=1 ] (84.2,96.95) -- (216.95,96.95) -- (166.75,147.1) -- (34,147.1) -- cycle ;
		%Straight Lines [id:da39845627905011627] 
		\draw    (125.47,122.02) -- (125.47,18.02) ;
		\draw [shift={(125.47,16.02)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Polygon [id:ds259471879486632] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (167.2,147.1) -- (167.2,264) -- (217.3,213.9) -- (216.95,96.95) -- cycle ;
		%Straight Lines [id:da926776502828371] 
		\draw    (193.3,183) -- (305.3,183) ;
		\draw [shift={(307.3,183)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Arc [id:dp825778999071634] 
		\draw  [draw opacity=0] (595.56,210.2) .. controls (592.36,211.36) and (588.9,212) .. (585.3,212) .. controls (568.73,212) and (555.3,198.57) .. (555.3,182) .. controls (555.3,165.43) and (568.73,152) .. (585.3,152) .. controls (588.9,152) and (592.36,152.64) .. (595.56,153.8) -- (585.3,182) -- cycle ; \draw   (595.56,210.2) .. controls (592.36,211.36) and (588.9,212) .. (585.3,212) .. controls (568.73,212) and (555.3,198.57) .. (555.3,182) .. controls (555.3,165.43) and (568.73,152) .. (585.3,152) .. controls (588.9,152) and (592.36,152.64) .. (595.56,153.8) ;  
		%Straight Lines [id:da7703776746716067] 
		\draw    (584.2,211.8) -- (592.59,210.62) ;
		\draw [shift={(595.56,210.2)}, rotate = 171.98] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Shape: Arc [id:dp2827356048387415] 
		\draw  [draw opacity=0] (153.49,50.22) .. controls (154.66,53.42) and (155.31,56.87) .. (155.3,60.47) .. controls (155.28,76.91) and (141.83,90.22) .. (125.26,90.2) .. controls (108.69,90.18) and (95.28,76.83) .. (95.3,60.39) .. controls (95.31,56.79) and (95.96,53.33) .. (97.14,50.14) -- (125.3,60.43) -- cycle ; \draw   (153.49,50.22) .. controls (154.66,53.42) and (155.31,56.87) .. (155.3,60.47) .. controls (155.28,76.91) and (141.83,90.22) .. (125.26,90.2) .. controls (108.69,90.18) and (95.28,76.83) .. (95.3,60.39) .. controls (95.31,56.79) and (95.96,53.33) .. (97.14,50.14) ;  
		%Straight Lines [id:da9861470217269179] 
		\draw    (154.87,61.8) -- (153.87,53.42) ;
		\draw [shift={(153.51,50.44)}, rotate = 83.2] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		
		% Text Node
		\draw (325,299.4) node [anchor=north west][inner sep=0.75pt]    {$Z$};
		% Text Node
		\draw (641,174.4) node [anchor=north west][inner sep=0.75pt]    {$X$};
		% Text Node
		\draw (433,15.4) node [anchor=north west][inner sep=0.75pt]    {$Y$};
		% Text Node
		\draw (0,300.4) node [anchor=north west][inner sep=0.75pt]    {$Z$};
		% Text Node
		\draw (312,175.4) node [anchor=north west][inner sep=0.75pt]    {$X$};
		% Text Node
		\draw (104,16.4) node [anchor=north west][inner sep=0.75pt]    {$Y$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[]{Example of non-equivalence for quaternion rotation}
	\end{figure}
	The results will be fundamental for our understanding of spinors (\SeeChapter{see section Spinor Calculus page \pageref{spinors}})!
	 \begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Notice that we have (where $\mathbb{K}$ is a type of numbers named "Cayley numbers" that we have not planned to present in this book):
	
	and that at each stage some element of abstract structure is lost. Ordering is lost on the passage from $\mathbb{R}$ to $\mathbb{C}$. Commutativity disappears on going from $\mathbb{C}$ to $\mathbb{H}$. The Cayley numbers $\mathbb{K}$ lose associativity.
	\end{tcolorbox}
	
	\subsubsection{Algebraic and Transcendental Numbers}
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] We name "\NewTerm{algebraic integer of degree $n$}\index{algebraic integer of degree $n$}", any complex number that is a solution of an univariate algebraic equation of degree $n$, ie a polynomial of degree $n$ (concept that we will discuss in the chapter of Algebra) whose coefficients are integers and whose dominant coefficient is equal to $1$.

		\item[D2.] We name "\NewTerm{algebraic number of degree $n$}\index{algebraic number of degree $n$}", any complex number that is a solution of an univariate algebraic equation of degree $n$, ie a polynomial of degree $n$ whose coefficients are rational.
	\end{enumerate}
	
	The set of algebraic number is sometimes denoted by $\overline{\mathbb{Q}}$ or $\mathbb{A}$.
	
	\begin{theorem}
	A first interesting result and particularly in this area of study (mathematical curiosity...) is that a rational number is an "algebraic integer of degree $n$" if and only if it's an integer (read several times need...). In scientific terms, we the say that the ring $\mathbb{Z}$ is "\NewTerm{fully closed}\index{fully closed ring}".
	\end{theorem}
	\begin{dem}
	We will assume that the number $p/q$ , where $p$ and $q$ are two prime integers (that is to say that their ratio does not give an integer or more rigorously ... that the greatest common divisor of $p,q$ is equal to $1$! , is a root of the following polynomial (\SeeChapter{see section Calculus page \pageref{polynomial}}) with relative integer coefficients ($\in\mathbb{Z}$) and whose dominant coefficient is equal to $1$:
	
	where the equality with zero of the polynomial is implicit.
	
	In this case:
	
	Since the coefficients are by definition all integers and their multiple in the parenthesis also, then the parenthesis has necessarily a value in $\mathbb{Z}$.
	
	Therefore, $q$ (at the right of the parenthesis) divides a power of $p$ (at the left of the equality), which is possible, in the set $\mathbb{Z}$ (because our bracket has a value in this same set for recall...), only if $q$ is equal to $\pm 1$ (as they were prime together).
	
	So among all rational numbers the only that are solutions of polynomial equations with relative integer coefficients $(\in \mathbb{Z}$) for which the dominant coefficient is equal to $1$ are relative integers!
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	To take another interesting and particular case, it is easy to show that any rational number is an algebraic number. Indeed, if we take the simplest following univariate polynomial:
	
	where $p$ and $q$ are relatively prime and where $q$ is different from $1$. So as this is a simple polynomial with rational coefficients ($\in\mathbb{Q}$), after remaniment we have:
	
	So since $p$ and $q$ are relatively prime and $q$ is different from $1$, we have indeed that every rational number is an "algebraic number of degree $1$".
	
	We also have the real (and irrational) number $\sqrt{2}$ which is an "algebraic integer of degree $2$" because it is the root of:
	
	and the complex number $\mathrm{i}$ is also an "algebraic integer of degree $2$" because it is the root of the equation:
	
	etc.
	
	\textbf{Definition (\#\thesection.\mydef):} A "\NewTerm{transcendental number}\index{transcendent number}" is a real or complex number that is not algebraic. That is, it is not a root of a non-zero polynomial equation with rational coefficients.
	
	\begin{theorem}
	The set of all transcendental numbers is uncountable. The proof is simple and requires no difficult mathematical development.
	\end{theorem}
	\begin{dem}
	Indeed, since the polynomial with integer coefficients are countable, and since each of these polynomials has a finite number of roots (see the Factorization Theorem in the section Calculus page \pageref{factorization theorem}), the set of algebraic numbers is countable! But the argument of Cantor's diagonal (\SeeChapter{see section Set Theory page \pageref{Cantor's diagonal}}) states that real numbers (and therefore also the complex numbers) are uncountable, so the set of all transcendental numbers must be uncountable.
	
	In other words, there is much more transcendental numbers than algebraic numbers...
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem} 
	The best known transcendent numbers are $\pi$ and $e$. We are still looking to provide you a proof more nice and intuitive than that of Hilbert or Lindemann–Weierstrass.
	
	\pagebreak
	\subsubsection{Universe Numbers (normal numbers)}
	\textbf{Definition (\#\thesection.\mydef):} A "\NewTerm{Universe number}\index{Universe number}" also named "\NewTerm{normal number}\index{normal number}" is a real number whose infinite sequence of digits in every base $b$ is distributed uniformly in the sense that each of the $b$ digit values has the same natural density $1/b$. Intuitively this means that no digit, or (finite) combination of digits, occurs more frequently than any other. The set of Universe numbers is sometimes denoted $\mathbb{U}$.

	While a general proof can be given that almost all purely real numbers are Universe numbers \cite{filip2010elementary} this proof is not constructive and only very few specific numbers have been shown to be Universe numbers. It is widely believed that the (computable) numbers $\sqrt{2}$, $\pi$, and $e$ are Universe numbers, but a proof remains elusive still in this year 12016 (holocene calendar). All of them however are strongly conjectured to be because of some empirical evidence. It is not even known whether all digits occur infinitely often in the decimal expansions of those constants. In particular, the popular claim "every string of numbers eventually occurs in $\pi$" or "the whole Holy book is contained in $\pi$ is not known to be true. It has been conjectured that every irrational algebraic number is a Universe number, while no counterexamples are known, there also exists no algebraic number that has been proven to be a Universe number in any base.
	
	More formally, let $\sum$ be a finite alphabet of $b$ digits, and $\sum^\infty$ the set of all sequences that may be drawn from that alphabet. Let $S\in\sum^\infty$ be such a sequence. For each $a$ in $\sum$ let $N_S(a, n)$ denote the number of times the letter $a$ appears in the first $n$ digits of the sequence $S$. We say that $S$ is a "\NewTerm{simple Universe number}\index{simple Universe number}" if the limit:
	
	for each $a$ is satisfied. 

	Now let $w$ be any finite string in $\sum^{*}$ and let $N_S(w, n)$ to be the number of times the string $w$ appears as a substring in the first $n$ digits of the sequence $S$ (for instance, if $S = 01010101\ldots$, then $N_S(010, 8) = 3$). Then $S$ is a "\NewTerm{Universe number}\index{Universe number}" if, for all finite strings $w\in \sum^{*}$:
	
	$S$ is therefore a Universe number if all strings of equal length occur with equal asymptotic frequency. A given infinite sequence is either a Universe number or not, whereas a pure real number, having a different base-$b$ expansion for each integer $b\geq 2$, may be a Universe number in one base but not in another. A "\NewTerm{disjunctive sequence}\index{disjunctive sequence}" is a sequence in which every finite string appears. A Universe number sequence is a "\NewTerm{disjunctive sequence}\index{disjunctive sequence}" but a disjunctive sequence need obviously not be a Universe number.
	
	It is possible to prove (yet we don't wish not present this proof in a book on applied mathematics) with the "Universe number theorem" that almost all pure real numbers are Universe number. The set of non-Universe numbers, though "small" in the sense of being a null set, is "large" in the sense of being uncountable (for example no rational number is normal to any base, since the digit sequences of rational numbers are eventually periodic!). For instance, there are uncountable many numbers whose decimal expansion does not contain the digit $5$, and none of these are Universe number.
	
	\pagebreak
	\subsubsection{Abstract Numbers (variables)}
	\textbf{Definition (\#\thesection.\mydef):} A number may be considered as doing abstraction from the nature of the objects that constitute the group that it characterizes as well as how to codify it (Indian notation, Roman notation, etc.). We then say that the number is an "\NewTerm{abstract number}\index{abstract number}". In other words, an abstract number, is a number that does not designate the quantity of any particular kind of thing.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Arbitrarily, the human being has adopted a numerical system mainly used in the World and represented by the symbols $0, 1, 2, 3, 4, 5, 7, 8, 9$ of the decimal system that will be supposedly known both in writing than orally by the reader (language learning).
	\end{tcolorbox}
	For mathematicians, it is not advantageous to work with these symbols because they represent only specific cases. What seek theoretical physicists and mathematicians are "\NewTerm{literal relations}\index{literal relations}" applicable in a general case and that engineers can according to their needs change these abstract numbers by numeric values that correspond to the problem they need resolve.

These abstract numbers today commonly named "\NewTerm{variable}\index{variable}" or "\NewTerm{unknown}", used in the context of "\NewTerm{literal calculation}\index{literal calculation}" are very often represented since the 116th century (holocene calendar) by:
	\begin{enumerate}
		\item The Latin alphabet:
		\begin{gather*}
			a,b,c,d,e,\ldots,x,y,z;A,B,C,D,E,\ldots, X,Y,Z
		\end{gather*}
		where the first lower case letters of the latin alphabet ($a, b, c, d, e \ldots$) are often used to represent an abstract constant, while the lowercase letters of the end of the latin alphabet ($\ldots,x, y, z$) are used to represent entities (variables or unknowns) we seek the value. Commonly, uppercase latin letters are reserved to represent matrices or random variables.
		
		\item The Greek alphabet:
		\begin{table}[H]\centering\small
			\begin{tabular}{clcl}\hline
			A$\alpha$ & Alpha & $\Lambda \lambda$ & Lambda \\
			B$\beta$  & Beta  & M$\mu$ & Mu \\
			$\Gamma\gamma$ & Gamma & N$\nu$ & Nu \\
			$\Delta\delta$ & Delta & $\Xi\xi$ & Xi\\
			E$\epsilon\varepsilon$ & Epsilon & O$o$ & Omicron\\
			Z$\zeta$ & Zeta & $\Pi\pi$ & Pi\\
			H$\eta$ & Eta & P$\rho$ & Rho \\
			$\Theta\theta\vartheta$ & Theta & $\Sigma\sigma$ & Sigma\\
			I$\iota$ & Iota & T$\tau$ & Tau \\
			K$\kappa$ & Kappa & $\Upsilon\upsilon$ & Upsilon \\
			$\Phi\phi\varphi$ & Phi & X$\chi$ & chi \\
			$\Psi\psi$ & Psi & $\Omega\omega$ & Omega \\ \hline
			\end{tabular}
			\caption{Greek Alphabet}
		\end{table}
		which is particularly used to represent more or less complex mathematical operators (such as the index sum $\Sigma$, the indexed product $\Pi$, the variational $\delta$, the infinitesimal element $\varepsilon$, partial differential $\partial$, etc.) or variables in the field of physics (as $\omega$ for the pulsation, $\nu$ for the frequency, $\rho$ for the density, etc.).
		
		\item The modernized Hebrew alphabet (with less intensity...)
		
		As we have seen, a transfinite cardinal for example is denoted by the letter "aleph": $\mathcal{N}_0$.
	\end{enumerate}
	Although these symbols can represent any number there are some who can represent physical constants also named "\NewTerm{Universal constant}\index{universal physical constant}" as the speed of light $c$, the gravitational constant $G$, the Planck constant $h$, the number $\pi$, etc.
	
	We use very often still other symbols that we will introduce and define when reading this book.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The letters to represent numbers had been used for the first time by Vieta in the $116$th century (holocene calendar).
	\end{tcolorbox}
	
	\paragraph{Domain of a Variable}\mbox{}\\\\
	A variable is therefore likely to take different numerical values. All these values can vary according to the character of the problem considered. 
	
	Given two numbers $a$ and $b$ such that $a<b$, then:
	
	\textbf{Definitions (\#\thesection.\mydef):}\label{domain of definition}
	\begin{enumerate}
		\item[D1.] We name "\NewTerm{domain of definition}\index{domain of definition}" of a variable, all numerical values it is likely to take between two specified limits (endpoints) or on a set (like $\mathbb{N}, \mathbb{R},\mathbb{R}^+,$ etc.).
		
		\item[D2.] We name "\NewTerm{closed interval with endpoints $a$ and $b$}\index{closed interval}", the set of all numbers $x$ between these two values and we denote as example as follows:
		
		The left notation is named obviously "\NewTerm{interval notation}\index{interval notation}", the right one is named "\NewTerm{setbuilder notation}\index{setbuilder notation}".
		
		\item[D3.] We name "\NewTerm{open interval with endpoints $a$ and $b$}\index{open interval}", the set of all numbers $x$ between these two values not included and we denote it as example as follows:
		
		
		\item[D4.] We name "\NewTerm{interval closed, left open right}\index{semi-interval}" or "\NewTerm{semi-closed left}" the following relation as example:
		
		
		\item[D5.] We name "\NewTerm{interval open left, closed right}\index{semi-interval}" or "\NewTerm{semi-closed right}" the following relation as example:
		
	\end{enumerate}
	Or in a summary and imaged form and as often denoted in Switzerland:
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|c|c|c|p{6cm}|}
					\hline
					\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Type}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Visual}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Math notation}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Explicitly}} \\ \hline
					$[a,b]$ & \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,651); %set diagram left start at 0, and has height of 651

%Straight Lines [id:da40894461362176937] 
\draw    (118.5,123) -- (257.5,123) ;
%Straight Lines [id:da5897997862947812] 
\draw    (144,117.5) -- (144,128.5) ;
%Straight Lines [id:da16842115799856439] 
\draw    (144,117.5) -- (146.75,117.5) ;
%Straight Lines [id:da9870986050565889] 
\draw    (144,128.5) -- (146.75,128.5) ;

%Straight Lines [id:da3903246853203841] 
\draw    (234.25,118) -- (234.25,129) ;
%Straight Lines [id:da4023176496179317] 
\draw    (231.5,118) -- (234.25,118) ;
%Straight Lines [id:da5714610327729315] 
\draw    (231.5,129) -- (234.25,129) ;

%Straight Lines [id:da3592002024441787] 
\draw [line width=2.25]    (144.5,123) -- (233.5,123) ;

% Text Node
\draw (140.5,104.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle a$};
% Text Node
\draw (228.5,101.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle b$};


\end{tikzpicture}
 & $a\leq x \leq b$ & Closed bounded interval\label{closed bounded interval} \\ \hline
					$[a,b[$ & \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,651); %set diagram left start at 0, and has height of 651

%Straight Lines [id:da40894461362176937] 
\draw    (118.5,123) -- (257.5,123) ;
%Straight Lines [id:da5897997862947812] 
\draw    (144,117.5) -- (144,128.5) ;
%Straight Lines [id:da16842115799856439] 
\draw    (144,117.5) -- (146.75,117.5) ;
%Straight Lines [id:da9870986050565889] 
\draw    (144,128.5) -- (146.75,128.5) ;

%Straight Lines [id:da3592002024441787] 
\draw [line width=2.25]    (144.5,123) -- (233.5,123) ;
%Straight Lines [id:da34480177631265363] 
\draw    (234,117.5) -- (234,128.5) ;
%Straight Lines [id:da07984713609902672] 
\draw    (234,117.5) -- (236.75,117.5) ;
%Straight Lines [id:da870291659141859] 
\draw    (234,128.5) -- (236.75,128.5) ;


% Text Node
\draw (140.5,104.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle a$};
% Text Node
\draw (228.5,101.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle b$};


\end{tikzpicture}
 & $a\leq x < b$ & Semi-closed and bounded interval on $a$ and semi-open on $b$ (or left semi-closed and right semi-open)\\ \hline
					$]a,b]$ & \tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        

\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,651); %set diagram left start at 0, and has height of 651

%Straight Lines [id:da40894461362176937] 
\draw    (118.5,123) -- (257.5,123) ;
%Straight Lines [id:da3903246853203841] 
\draw    (234.25,118) -- (234.25,129) ;
%Straight Lines [id:da4023176496179317] 
\draw    (231.5,118) -- (234.25,118) ;
%Straight Lines [id:da5714610327729315] 
\draw    (231.5,129) -- (234.25,129) ;

%Straight Lines [id:da3592002024441787] 
\draw [line width=2.25]    (144.5,123) -- (233.5,123) ;
%Straight Lines [id:da5191621459117126] 
\draw    (145.25,118) -- (145.25,129) ;
%Straight Lines [id:da4674547744703945] 
\draw    (142.5,118) -- (145.25,118) ;
%Straight Lines [id:da5138254690804982] 
\draw    (142.5,129) -- (145.25,129) ;


% Text Node
\draw (140.5,104.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle a$};
% Text Node
\draw (228.5,101.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle b$};


\end{tikzpicture}
 & $a< x \leq b$ & Semi-open bounded interval on $a$ and semi-closed on $b$ (or left semi-open and right semi-closed)\\ \hline
					$]a,b[$ & \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,651); %set diagram left start at 0, and has height of 651

%Straight Lines [id:da40894461362176937] 
\draw    (118.5,123) -- (257.5,123) ;
%Straight Lines [id:da5897997862947812] 
\draw    (233,117.5) -- (233,128.5) ;
%Straight Lines [id:da16842115799856439] 
\draw    (233,117.5) -- (235.75,117.5) ;
%Straight Lines [id:da9870986050565889] 
\draw    (233,128.5) -- (235.75,128.5) ;

%Straight Lines [id:da3903246853203841] 
\draw    (145.25,118) -- (145.25,129) ;
%Straight Lines [id:da4023176496179317] 
\draw    (142.5,118) -- (145.25,118) ;
%Straight Lines [id:da5714610327729315] 
\draw    (142.5,129) -- (145.25,129) ;

%Straight Lines [id:da3592002024441787] 
\draw [line width=2.25]    (144.5,123) -- (233.5,123) ;

% Text Node
\draw (140.5,104.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle a$};
% Text Node
\draw (228.5,101.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle b$};


\end{tikzpicture}
 & $a< x < b$ & Bounded open interval\\ \hline
					$]-\infty,b]$ & \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,651); %set diagram left start at 0, and has height of 651

%Straight Lines [id:da40894461362176937] 
\draw    (118.5,123) -- (257.5,123) ;
%Straight Lines [id:da3903246853203841] 
\draw    (234.25,118) -- (234.25,129) ;
%Straight Lines [id:da4023176496179317] 
\draw    (231.5,118) -- (234.25,118) ;
%Straight Lines [id:da5714610327729315] 
\draw    (231.5,129) -- (234.25,129) ;

%Straight Lines [id:da3592002024441787] 
\draw [line width=2.25]    (118.5,123) -- (234.25,123) ;

% Text Node
\draw (228.5,101.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle b$};


\end{tikzpicture}
 & $ x \leq b$ & Unbounded interval closed on $b$ (or closed right) \\ \hline
					$]-\infty,b[$ & \tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        

\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,651); %set diagram left start at 0, and has height of 651

%Straight Lines [id:da40894461362176937] 
\draw    (118.5,123) -- (257.5,123) ;
%Straight Lines [id:da5897997862947812] 
\draw    (233,117.5) -- (233,128.5) ;
%Straight Lines [id:da16842115799856439] 
\draw    (233,117.5) -- (235.75,117.5) ;
%Straight Lines [id:da9870986050565889] 
\draw    (233,128.5) -- (235.75,128.5) ;

%Straight Lines [id:da3592002024441787] 
\draw [line width=2.25]    (118.5,123) -- (232.5,123) ;

% Text Node
\draw (228.5,101.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle b$};


\end{tikzpicture}
 & $ x \leq b$ & Unbounded interval open on $b$ (or open right) \\ \hline
					$[a,+\infty[$ & \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,651); %set diagram left start at 0, and has height of 651

%Straight Lines [id:da40894461362176937] 
\draw    (118.5,123) -- (257.5,123) ;
%Straight Lines [id:da5897997862947812] 
\draw    (144,118.5) -- (144,129.5) ;
%Straight Lines [id:da16842115799856439] 
\draw    (144,118.5) -- (146.75,118.5) ;
%Straight Lines [id:da9870986050565889] 
\draw    (144,129.5) -- (146.75,129.5) ;

%Straight Lines [id:da3592002024441787] 
\draw [line width=2.25]    (143.5,123) -- (257.5,123) ;

% Text Node
\draw (140.5,104.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle a$};


\end{tikzpicture}
 & $a\leq x$ & Unbounded interval closed on $a$ (or closed left) \\ \hline
					$]a,+\infty[$ & \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,651); %set diagram left start at 0, and has height of 651

%Straight Lines [id:da40894461362176937] 
\draw    (118.5,123) -- (257.5,123) ;
%Straight Lines [id:da3903246853203841] 
\draw    (145.25,118) -- (145.25,129) ;
%Straight Lines [id:da4023176496179317] 
\draw    (142.5,118) -- (145.25,118) ;
%Straight Lines [id:da5714610327729315] 
\draw    (142.5,129) -- (145.25,129) ;

%Straight Lines [id:da3592002024441787] 
\draw [line width=2.25]    (145.5,123) -- (257.5,123) ;

% Text Node
\draw (140.5,104.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle a$};


\end{tikzpicture}
 & $a< x $ & Unbounded interval open on $a$ (or open left) \\ \hline
			\end{tabular}
		\end{center}
		\caption{Summary of typical domain of definitions in Switzerland}
	\end{table}
	and according to the international norm ISO 80000-2:2009 \textit{Mathematical signs and symbols to be used in the natural sciences and technology} (since Switzerland is quite good in the no respect of international norms and standards):
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|c|c|c|p{6cm}|}
					\hline
					\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Type}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Visual}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Math notation}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Explicitly}} \\ \hline
					$[a,b]$ & \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,651); %set diagram left start at 0, and has height of 651

%Straight Lines [id:da40894461362176937] 
\draw    (118.5,123) -- (257.5,123) ;
%Straight Lines [id:da5897997862947812] 
\draw    (144,117.5) -- (144,128.5) ;
%Straight Lines [id:da16842115799856439] 
\draw    (144,117.5) -- (146.75,117.5) ;
%Straight Lines [id:da9870986050565889] 
\draw    (144,128.5) -- (146.75,128.5) ;

%Straight Lines [id:da3903246853203841] 
\draw    (234.25,118) -- (234.25,129) ;
%Straight Lines [id:da4023176496179317] 
\draw    (231.5,118) -- (234.25,118) ;
%Straight Lines [id:da5714610327729315] 
\draw    (231.5,129) -- (234.25,129) ;

%Straight Lines [id:da3592002024441787] 
\draw [line width=2.25]    (144.5,123) -- (233.5,123) ;

% Text Node
\draw (140.5,104.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle a$};
% Text Node
\draw (228.5,101.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle b$};


\end{tikzpicture} & $a\leq x \leq b$ & Closed bounded interval \\ \hline
					$[a,b)$ & \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,651); %set diagram left start at 0, and has height of 651

%Straight Lines [id:da40894461362176937] 
\draw    (118.5,123) -- (257.5,123) ;
%Straight Lines [id:da5897997862947812] 
\draw    (144,117.5) -- (144,128.5) ;
%Straight Lines [id:da16842115799856439] 
\draw    (144,117.5) -- (146.75,117.5) ;
%Straight Lines [id:da9870986050565889] 
\draw    (144,128.5) -- (146.75,128.5) ;

%Straight Lines [id:da3592002024441787] 
\draw [line width=2.25]    (144.5,123) -- (233.5,123) ;
%Straight Lines [id:da34480177631265363] 
\draw    (234,117.5) -- (234,128.5) ;
%Straight Lines [id:da07984713609902672] 
\draw    (234,117.5) -- (236.75,117.5) ;
%Straight Lines [id:da870291659141859] 
\draw    (234,128.5) -- (236.75,128.5) ;


% Text Node
\draw (140.5,104.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle a$};
% Text Node
\draw (228.5,101.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle b$};


\end{tikzpicture} & $a\leq x < b$ & Semi-closed and bounded interval on $a$ and semi-open on $b$ (or left semi-closed and right semi-open)\\ \hline
					$(a,b]$ & \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,651); %set diagram left start at 0, and has height of 651

%Straight Lines [id:da40894461362176937] 
\draw    (118.5,123) -- (257.5,123) ;
%Straight Lines [id:da3903246853203841] 
\draw    (234.25,118) -- (234.25,129) ;
%Straight Lines [id:da4023176496179317] 
\draw    (231.5,118) -- (234.25,118) ;
%Straight Lines [id:da5714610327729315] 
\draw    (231.5,129) -- (234.25,129) ;

%Straight Lines [id:da3592002024441787] 
\draw [line width=2.25]    (144.5,123) -- (233.5,123) ;
%Straight Lines [id:da5191621459117126] 
\draw    (145.25,118) -- (145.25,129) ;
%Straight Lines [id:da4674547744703945] 
\draw    (142.5,118) -- (145.25,118) ;
%Straight Lines [id:da5138254690804982] 
\draw    (142.5,129) -- (145.25,129) ;


% Text Node
\draw (140.5,104.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle a$};
% Text Node
\draw (228.5,101.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle b$};


\end{tikzpicture} & $a< x \leq b$ & Semi-open bounded interval on $a$ and semi-closed on $b$ (or left semi-open and right semi-closed)\\ \hline
					$(a,b)$ & \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,651); %set diagram left start at 0, and has height of 651

%Straight Lines [id:da40894461362176937] 
\draw    (118.5,123) -- (257.5,123) ;
%Straight Lines [id:da5897997862947812] 
\draw    (233,117.5) -- (233,128.5) ;
%Straight Lines [id:da16842115799856439] 
\draw    (233,117.5) -- (235.75,117.5) ;
%Straight Lines [id:da9870986050565889] 
\draw    (233,128.5) -- (235.75,128.5) ;

%Straight Lines [id:da3903246853203841] 
\draw    (145.25,118) -- (145.25,129) ;
%Straight Lines [id:da4023176496179317] 
\draw    (142.5,118) -- (145.25,118) ;
%Straight Lines [id:da5714610327729315] 
\draw    (142.5,129) -- (145.25,129) ;

%Straight Lines [id:da3592002024441787] 
\draw [line width=2.25]    (144.5,123) -- (233.5,123) ;

% Text Node
\draw (140.5,104.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle a$};
% Text Node
\draw (228.5,101.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle b$};


\end{tikzpicture} & $a< x < b$ & Bounded open interval\\ \hline
					$(-\infty,b]$ & \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,651); %set diagram left start at 0, and has height of 651

%Straight Lines [id:da40894461362176937] 
\draw    (118.5,123) -- (257.5,123) ;
%Straight Lines [id:da3903246853203841] 
\draw    (234.25,118) -- (234.25,129) ;
%Straight Lines [id:da4023176496179317] 
\draw    (231.5,118) -- (234.25,118) ;
%Straight Lines [id:da5714610327729315] 
\draw    (231.5,129) -- (234.25,129) ;

%Straight Lines [id:da3592002024441787] 
\draw [line width=2.25]    (118.5,123) -- (234.25,123) ;

% Text Node
\draw (228.5,101.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle b$};


\end{tikzpicture} & $ x \leq b$ & Unbounded interval closed on $b$ (or closed right) \\ \hline
					$(-\infty,b[$ & \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,651); %set diagram left start at 0, and has height of 651

%Straight Lines [id:da40894461362176937] 
\draw    (118.5,123) -- (257.5,123) ;
%Straight Lines [id:da5897997862947812] 
\draw    (233,117.5) -- (233,128.5) ;
%Straight Lines [id:da16842115799856439] 
\draw    (233,117.5) -- (235.75,117.5) ;
%Straight Lines [id:da9870986050565889] 
\draw    (233,128.5) -- (235.75,128.5) ;

%Straight Lines [id:da3592002024441787] 
\draw [line width=2.25]    (118.5,123) -- (232.5,123) ;

% Text Node
\draw (228.5,101.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle b$};


\end{tikzpicture} & $ x \leq b$ & Unbounded interval open on $b$ (or open right) \\ \hline
					$[a,+\infty)$ & \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,651); %set diagram left start at 0, and has height of 651

%Straight Lines [id:da40894461362176937] 
\draw    (118.5,123) -- (257.5,123) ;
%Straight Lines [id:da5897997862947812] 
\draw    (144,118.5) -- (144,129.5) ;
%Straight Lines [id:da16842115799856439] 
\draw    (144,118.5) -- (146.75,118.5) ;
%Straight Lines [id:da9870986050565889] 
\draw    (144,129.5) -- (146.75,129.5) ;

%Straight Lines [id:da3592002024441787] 
\draw [line width=2.25]    (143.5,123) -- (257.5,123) ;

% Text Node
\draw (140.5,104.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle a$};


\end{tikzpicture} & $a\leq x$ & Unbounded interval closed on $a$ (or closed left) \\ \hline
					$(a,+\infty)$ & \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,651); %set diagram left start at 0, and has height of 651

%Straight Lines [id:da40894461362176937] 
\draw    (118.5,123) -- (257.5,123) ;
%Straight Lines [id:da3903246853203841] 
\draw    (145.25,118) -- (145.25,129) ;
%Straight Lines [id:da4023176496179317] 
\draw    (142.5,118) -- (145.25,118) ;
%Straight Lines [id:da5714610327729315] 
\draw    (142.5,129) -- (145.25,129) ;

%Straight Lines [id:da3592002024441787] 
\draw [line width=2.25]    (145.5,123) -- (257.5,123) ;

% Text Node
\draw (140.5,104.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle a$};


\end{tikzpicture} & $a< x $ & Unbounded interval open on $a$ (or open left) \\ \hline
			\end{tabular}
		\end{center}
		\caption{Summary of typical domain of definitions according to ISO norms}
	\end{table}

	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} The notation $\{x\text{ such that } a<x<b\}$ denotes the set of real numbers $x$ strictly greater than $x$ and strictly less than $b$.\\
	
	\textbf{R2.} To fact that an interval is for example opened on $b$ means that the real number $b$ is not part thereof. By cons, if it had been closed then $b$ would be part of it.\\
	
	\textbf{R3.} If the variable $x$ can take all possible negative and positive values we write therefore: $\left] -\infty,+\infty \right[$ where the symbol "$\infty$" means "infinite". Obviously there can be combinations of open infinite right intervals with left endpoint and vice versa.\\
	
	\textbf{R4.} We will recall some of these concepts with a different approach when studying Algebra (literal calculation).
	\end{tcolorbox}	

	We say that the variable $x$ is an "\NewTerm{ordered variable}\index{ordered variable}" if by representing its domain of definition by a horizontal axis where each point on the axis represents a value of $x$, then for each pair of values, we can say that there is an "\NewTerm{antecedent}\index{antecedent}" and one that is a "\NewTerm{subsequent}\index{subsequent}". Here the notion of antecedent and subsequent is not related to the concept of time it expresses just how the values of the variable are ordered.
	
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] A variable is said to be "\NewTerm{increasing}\index{increasing variable}" if each subsequent value is greater than each antecedent value.

		\item[D2.] A variable is said to be "\NewTerm{decreasing}\index{decreasing variable}" if each subsequent value is smaller than each antecedent value.

		\item[D3.] The increasing and decreasing variables are named "\NewTerm{variables with monotonic variations}\index{monotonic variable}" or simply "\NewTerm{monotonic variables}".
	\end{enumerate}

	
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{90} & \pbox{20cm}{\score{4}{5} \\ {\tiny 31 votes, 69.68\%}} 
	\end{tabular} 
	\end{flushright}
	
	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Arithmetic Operators}
	\lettrine[lines=4]{\color{BrickRed}T}{alking} about numbers like we did in the previous section naturally leads us to consider the operations of calculus. It is therefore logic that we make a non-exhaustive description of the operations that may exist between the numbers. This will be the goal of this section.
	
	We will consider in this book that there are two types of key tools in Arithmetics (we do not speak of Algebra but Arithmetic!):
	
	\begin{itemize}
		\item Arithmetic operators:
		
		There are two basic operators (addition "$+$" and subtraction "$-$") from which we can build other operators: the "multiplication" (whose contemporary symbol $\times$ was introduced in 11574 - according to holocene calendar - by William Oughtred) and the "division" (whose old symbol was "$\div$" but since the end of the 120th century (holocene calendar) we use simple the slash $/$ symbol).
		
		These four operators ($+$, $-$, $\times$, $/$) are commonly named "\NewTerm{rational operators}\index{rational operators}". We will see them more in details after setting the binary relations.
		
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		Rigorously addition could be enough if we consider the common set of real number $\mathbb{R}$ because therefore the subtraction is only the addition of a negative number.
		\end{tcolorbox}
	
		\item Binary operators (relations):
		
		There are six basic binary relations (equal $=$, different $\neq$, greater than $>$, less than $<$, greater or equal $\geq$, less than or equal $\leq$) that compare the order of amplitude of elements that are on the left and on the right of these relations (thus at the number of two, hence the name "binary") in order to draw some conclusions. The majority of binary relations symbols were introduced by Vieta and Harriot in the 116th century (holocene calendar).
	\end{itemize}

	It is obviously essential to know as best a possible these tools and their properties before going through into more strenuous calculations.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.4]{img/arithmetics/operators.jpg}
	\end{figure}
	
	\subsection{Binary Relations}
	The concept of "\NewTerm{relation}\index{relations}" is the basis of all mathematics whose purpose is to study - by observation and deduction (reasoning), calculation and comparison - configurations or abstract/concrete relations of its objects (numbers, forms, structures) by seeking to establish the logical, numerical or conceptual links between these objects.
	
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] Consider two non-empty sets $E$ and $F$ (\SeeChapter{see section Set Theory page \pageref{empty set}}) not necessarily identical. If to some given elements $x$ of $A$ we can associate with a precise mathematical rule $\mathcal{R}$ (unambiguous) one element $y$ of $F$, we define therefore a "\NewTerm{functional relation}\index{functional relation}" that maps $E$ to $F$ and that we write:
		
		Thus, more generally, a functional relation $\mathcal{R}$ can be defined as a mathematical rule that associates to given components $x$ of $E$, some given elements $y$ of $F$.
		
		So, in this more general context, if $x\mathcal{R}y$, we say that there $y$ is an "\NewTerm{image}\index{image}" of $x$ through $\mathcal{R}$ and that $x$ is a "\NewTerm{precedent}" or "\NewTerm{preimage}\index{preimage}" of $y$.
		
		The set of pairs $(x, y)$ such that $x\mathcal{R}y$ is a true statement generates a "graph" or "representation" of the relation $\mathcal{R}$. We can represent these couples in a proper chosen way to make a graphical representation of the relation $\mathcal{R}$.
		
		This is a type of relation on which we will come back in the section Functional Analysis (page \pageref{composite function}) under the form: $\mathcal{R}:f(x)=y\circ f$ and that does not interest us directly in this section.
		
		\item[D2.] Consider a non-empty set $E$, if we associate with this set (and only to this one!) tools to compare its items between them then we talk about a "\NewTerm{binary relation}\index{binary relation}" or "\NewTerm{comparison relation}\index{comparison relation}" and that we write for any element $x$ and $y$ of $E$:
		
		These relations can also most of time be presented graphically. In the case of conventional binary operators comparison where $E$ is the set of natural numbers $\mathbb{N}$, relative $\mathbb{Z}$, rationals $\mathbb{Q}$ or real $\mathbb{R}$, that is graphically represented by a horizontal line (typically...); in the case of congruence (\SeeChapter{see section Number Theory page \pageref{congruence}}) it is represented by lines in the plane whose points are given by the constraint of congruence.
	\end{enumerate}
	As we already mentioned, there are $6$ fundamental binary relationships (equal $=$, different from $\neq$, greater than $>$, smaller than $<$, bigger or equal $\geq$ , smaller or equal $\leq $). But we will see a little further that the rigorous definition of binary relations allows us to build more abstract tools (such as congruence well known by students of small classes and that we will study in the section of Number Theory).
	
	\subsubsection{Equalities}
	It is difficult to define the term "equality" in a general case applicable to any situation. For our part, we will allow ourselves for this definition to take the inspiration of the extensionality axiom of Set Theory (discussed later at page \pageref{extensionality axiom}).
	
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] Two elements are "\NewTerm{equal}\index{equal}" if and only if they have the same values. The strict equality is denoted by the symbol $=$ \label{equality} that therefore means "equal to" (this symbol was introduced in 11557 - according to holocene calendar - by Robert Rocorde).
		
		If we have $a=b$ and $c$ is any given number (or vector/matrix) and $\star$ any operation (such as addition, subtraction, multiplication or division) then:
		
		This property is used to solve or simplify any type of equations. In practice, the abbreviation "LHS" is informal shorthand for the left-hand side of an equality. Similarly, "RHS" is the right-hand side abbreviation of that latter. 
		
		Obviously we have (property of reflexivity):
		
		And also (property of transitivity):
		\begin{gather*}
			\begin{rcases*}
			a=c \\
			b=c
			\end{rcases*} a=b
		\end{gather*}
		
		
		We will not enumerate the other properties of the equality in this section (for more details see the section Set Theory page \pageref{equality}).
		
		\item[D2.] If two elements are not strictly equal, that is to say "\NewTerm{inequal}\index{inequal}"..., we are connecting them by the symbol $\neq$ and we say they are "not equal".
		
		If we have $a>b$ or $a<b$ then:
		
	\end{enumerate}
	There are still other equality symbols, which are an extension of two we have defined previously. Unfortunately, they are often misused (we could say rather that they are used in the wrong places) in most of the books available on the market (and this book is not an exception):
	\begin{enumerate}
		\item $\cong$: Should be used for congruence but in fact is mostly used to indicate an approximation.
		
		\item $\approx$: Should be used for approximations but in fact $\cong$ is used instead.
		
		\item $\equiv$: Should be used to say that two elements are equivalent but in practice most people use $=$.
		
		\item $:=$: Is used to say that one element is by definition equal to another one.
		
		\item $\doteq$: Should be used to say "equal by definition to" but in fact most people use instead $:=$.
		
		\item $\sim$: Is used most of time in Statistics to say "follows the law..." but some practitioners use instead $=$ or to say "asymptotically equal".
	\end{enumerate}
	
	\subsubsection{Comparators}\label{comparators}
	The comparators are tools that allow us to compare and order any pair of numbers (and also Sets!).
	
	The possibility of ordering numbers is fundamental in mathematics. Otherwise (if it was not possible to order them), there would be a lot of things that would shock our habits, for example (some of the concepts presented in the following sentence have not yet been presented but we would still make reference to them): no more monotonic functions (especially sequences) and linked to it the derivation would therefore indicate nothing more about the "variation direction", no more approach of roots of polynomial by dichotomy (classical research algorithm in an ordered set that split in two at each iteration), no more segments in geometry, no more than half space, no more convexity, we can not oriented space anymore, etc. It is therefore important to be able to order things as you can see...!
	
	Thus, for any $a,b,c\in \mathbb{R}$ we write when $a$ is greater than or equal to $b$:
	
	and when $a$ is less than or equal to $b$:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	It is useful to recall that the set of real numbers $\mathbb{R}$ is a totally ordered group (\SeeChapter{see section Set Theory page \pageref{groups}}), otherwise we could not establish order relations among its elements (which is not the case for complex numbers $\mathbb{C}$ that we can not order!).
	\end{tcolorbox}
	
	\textbf{Definition (\#\thesection.\mydef):} The symbol $\leq$ is an "\NewTerm{relation order}\index{relation order}" (see the rigorous definition further below!) which means "\NewTerm{less than or equal to}" and conversely the symbol $\geq$ is also an order relation that means "\NewTerm{greater than or equal to}\index{greater than or equal to}".
	
	We also have relatively to the strict comparison the following properties that are relatively intuitive:
	
	and:
	
	if:
	
	if:
	
	and vice versa:
	
	We also have:
	
	and vice versa:
	
	We can obviously multiply, divide, add or subtract a term from each side of the relation as it is always true. Notice, however, that if you multiply both sides by a negative number it will obviously change as the comparator such that:
	
	and vice versa:
	
	We also have:
	
	Consider now that $b<a<0$ and $p\in \mathbb{N}^{*}$. Then if $p$ is an even integer:
	
	else if $p$ is odd:
	
	This result simply comes from the multiplication of signs rule since the power when not fractional is only a multiplication.

	Finally:
	
	The relations:
	
	thus correspond respectively to: (strictly) greater than, (strictly) smaller than, smaller or equal, greater or equal, much bigger than, much smaller than.
	
	These relations can be defined in a little more subtle and rigorous way and apply not only to comparators (see for example the congruence relation in the section of Set Theory page \pageref{congruence})!
	
	Let us see this (the vocabulary that follows is also defined in the section of Set Theory page \pageref{surjective application}):
	
	\textbf{Definition (\#\thesection.\mydef):} Given a binary relation $\mathcal{R}$ of a set $A$ to itself, a relation $R$ on $A$ is a subset of the Cartesian product $\mathcal{R}\subseteq A\times A$ (that is to say, the binary relation generates a subset by the constraints it imposes on the elements of $A$ satisfying the relation) with the property of being:
	\begin{enumerate}\label{strict order}
		\item[P1.] A "\NewTerm{reflexive relation}\index{reflexive relation}\label{reflexive}" if $\forall x \in A$:
		
		
		\item[P2.] A "\NewTerm{symmetrical relation}\index{symmetrical relation}" if $\forall x,y \in A$:
		
		
		\item[P3.] An "\NewTerm{anti-symmetrical relation}\index{anti-symmetrical relation}" if $\forall x,y \in A$:
		
		
		\item[P4.] A "\NewTerm{transitive relation}\index{transitive relation}" if $\forall x,y,z \in A$:
		
		
		\item[P5.] An "\NewTerm{connex relation}\index{connex relation}" if $\forall x,y \in A$:
		
	\end{enumerate}
	Mathematicians have given special names to the families of relations satisfying some of these properties.
	
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] A relation is named "\NewTerm{strict order relation}\index{strict order relation}" if and only if it is only transitive (some specify then that it is necessarily antireflexive but this last fact is then obvious...).
		
		\item[D2.] A relation is named a "\NewTerm{preorder}\index{preorder}" if and only if it is reflexive and transitive.
		
		\item[D3.] A relation is named an "\NewTerm{equivalence relation}\index{equivalence relation}\label{equivalence relation}" if and only if it is reflexive, symmetric, and transitive.
		
		\item[D4.] A relation is named "\NewTerm{order relation}\index{order relation}\label{order relation}" if and only if it is reflexive, transitive and antisymmetric (thus the relations $>, <$ are not order relations because obviously not reflexive relations).
		
		\item[D5.] A relation is named "\NewTerm{total order relation}\index{total order relation}\label{total order relation}" if and only if it is reflexive, transitive, connex and antisymmetric.
	\end{enumerate}
	For the other combinations it seems (as far as we know) that there are no special name among the mathematicians...

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The binary relations have all similar properties in natural sets $\mathbb{N}$, relative $\mathbb{Z}$, rational $\mathbb{Q}$ and real $\mathbb{R}$ (there is no natural order relation on the set of complex numbers $\mathbb{C}$).
	\end{tcolorbox}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Of the below set of eight copies of books, the relation "... has the same ISBN as ..." is an equivalence relation:
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{img/arithmetics/equivalence_relation.jpg}
		\caption[]{Illustration of an equivalence relation (source: Wikipedia, author: Stephan Kulla)}
	\end{figure}
	\end{tcolorbox}
	If we summarize:
	\begin{table}[H]\centering\small
		\renewcommand{\arraystretch}{1.2}
		\begin{tabular}{lcccccc}\hline
		\rowcolor[gray]{0.75}\textsc{Binary relation} & = & $\neq$ & $>$ & $<$ & $\leqslant$ & $\geqslant$ \\ \hline
		reflexive & yes & no & no & no & yes &  yes \\
		symmetric 	& yes  & yes & no & no & no & no \\
		transitive 	& yes & no & yes & yes & yes & yes \\
		connex & no & no & no & no & yes & yes \\
		antisymmetric 	& yes & no & no & no & yes & yes \\ \hline
		\end{tabular}
		\caption{Binary Relations}
	\end{table}
	Thus we see that the binary relations $\leq, \geq$ form with the previously mentioned sets, total order relations and it is very easy to see which binary relations are partial, total or equivalence order relations.
	
	\textbf{Definition (\#\thesection.\mydef):} If $\mathcal{R}$ is an equivalence relation on $A$. For $\forall x\in A$, the "\NewTerm{equivalence class}\index{equivalence class}\label{equivalence class}" of $x$ is by definition the set:
	
	$[x]$ is therefore a subset of $A$ ($x \subseteq A$) which we denote also thereafter ... $R$.
	
	We thus have a new set that is named the "\NewTerm{set of equivalence classes}\index{set of equivalence classes}" or "\NewTerm{quotient set}\index{quotient set}\label{quotient set}" denoted in this book by $A / R$. So:
	
	You should know that in $A/\mathcal{R}$ we do not look anymore at $[x]$ as a subset of $A$, but as an element!
	
	A relation of equivalence, presented in a popularized manner... thus serves to stick one unique label to items that satisfy the same property, and to confuse them with the said label (knowing what we do with this label).
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. Set of the classes of equivalence of the previously illustration about the ISBN:
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{img/arithmetics/class_of_equivalence.jpg}
		\caption[]{Illustration of set of equivalence classes (source: Wikipedia, author: Stephan Kulla)}
	\end{figure}
	
	E2. In the set of integers $\mathbb{Z}$, if we study the remains of the division of number by $2$, we have that the result is always $0$ or $1$.\\
	
	The zero equivalence class is then named the "set of even integers numbers", the one equivalence class is therefore named the "set of odd integers". So we have two classes of equivalence for two partitions of $\mathbb{Z}$ (always keep in mind this simple example for theoretical elements that follow it helps a lot!).\\
	
	If we name the first $0$ and the second $1$, we fall back on the operation rules between odd and even numbers:
	\begin{gather*}
		0+0=0\qquad 0+ 1=0\qquad 1+1=0
	\end{gather*}
	which respectively means that the sum of two even integers is even, that the sum of an even and an odd integer is odd and that the sum of two odd integer is even.\\
	
	And for the multiplication:
	\begin{gather*}
		0\times 0=0\qquad 0\times 1=0\qquad 1\times 1=1
	\end{gather*}
	which respectively means that the two product of two even integer is even, the product of an even and an odd integer is even and that the product of two odd integer is odd.\\
	
	Now, to verify that we are dealing with an equivalence relation, we should still check that it is reflexive ($x\mathcal{R}x$), symmetrical (if $x\mathcal{R}y$ then $y\mathcal{R}x$) and transitive (if $x\mathcal{R}y$ and $y\mathcal{R}z$ then $x\mathcal{R}z$). We will see how to check it a few paragraphs further below because this example is a very special case of congruence relation.
	\end{tcolorbox}
	
	\textbf{Definition (\#\thesection.\mydef):} The application $f:A\mapsto A/\mathcal{R}$ defined by $x\mapsto [x]$ is named "\NewTerm{canonical projection}\index{canonical projection}". Any element $z\in [x]$ is therefore named "\NewTerm{class representative}\index{class representative}" of $[x]$.
	\begin{theorem}
	Now consider a set $E$. Then we propose to proved that there is correspondence between the set of equivalence relations on $E$ and all partitions of $E$. In other words, this theorem says that an equivalence relation on $E$ is nothing more but a partition on $E$ (this is intuitive).
	\end{theorem}
	\begin{dem}
	Let $\mathcal{R}$ be an equivalence relation on $E$. We choose $I=E/\mathcal{R}$ as set partition indexing and all we ask for any $[x]\in E/\mathcal{R}$, $E_{[x]}=[x]$.
	
	We just have to check the following two properties of the definition of partitions to show that the family $\left(E_{[x]}\right)_I$ is a partition of $E$:
	
	\begin{enumerate}
		\item[P1.] Given $[x],[y]\in E/\mathcal{R}$ such that $[x]\neq [y]$ then (obvious) $E_{[x]}\cap E_{[y]}=\varnothing$.
		
		\item[P2.] $E=\displaystyle\bigcup_{[x]\in E/\mathcal{R}}$ is obvious because if $x\in E$ then $x\in [x]=E_{[x]}$.
	\end{enumerate}
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Again, it should by easy to check with the practical example of the division by $2$ given previously that the partition of even and odd numbers satisfies these two properties (if not reader can contact us we will add this as an example).
	
	We have therefore associated to the equivalence relation $\mathcal{R}$ a partition $E$. Conversely, if $(E_i)_I$ is a partition of $E$ then we almost easily verify that the relation $\mathcal{R}$ is defined by $x\mathcal{R}y$ if and only if there exists $i \in I$ such as $x,y \in E_i$ is an equivalence relation! Both applications are thus bijective and the inverses of each other.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	We will now apply an example a little less trivial than the last we have seen to the construction of rings $\mathbb{Z}/d\mathbb{Z}$ after a few reminders equation (for the concept of ring see the section Set Theory page \pageref{ring}).\\
	
	Reminders:
	\begin{enumerate}
		\item Given two numbers $n,m\in \mathbb{Z}$. We say that "\NewTerm{$n$ divides $m$}\index{divide}" and we write $n|m$ if and only if there exists an integer $k\in \mathbb{Z}$ such as $m=kn$ (\SeeChapter{see section Numbers Theory page \pageref{division}}).
		
		\item Given $d\geq 1$ is an integer. We define the relation $\mathcal{R}$ by $n\mathcal{R}m$ if and only if $d|(n-m)$ or in other words $n\mathcal{R}m$ if and only if there exists $d\in\mathbb{Z}$ such that $n=m+kd$. Usually we write this $n\equiv m\; (\text{modulo } d)$ instead of $n\mathcal{R}m$  and we say that "\NewTerm{$n$ is congruent to $m$ modulo $d$}\index{congruent}". Remember also that $n\equiv 0\; (\text{modulo } d)$ if and only if $d$ divides $n$ (\SeeChapter{see section Numbers Theory page \pageref{congruence}}).
	\end{enumerate}
	We will now introduce an equivalence relation on $\mathbb{Z}$. Let us prove that for any integer $d\geq 1$, the congruence modulo $d$ is an equivalence relation on $\mathbb{Z}$ (we have already proved this in the section of Number Theory in our study of congruence on page \pageref{congruence}, but let us redo this work for the fun...).\\
	
	To prove this we simply have to control the three properties of the equivalence relation:
	\begin{enumerate}
		\item[P1.] Reflexivity: $n\equiv n$ since $n=n+0d$.
		
		\item[P2.] Symmetry: If $n \equiv m$ then $n=m+kd$ and therefore $m=n+(-k)d$ that is to say $m\equiv n$.
		
		\item[P3.] Transitivity: If $n\equiv m$ and then $m\equiv j$ then $n=m+kd$ and $m=j+k'd$ therefore $n=j+(k+k')d$ that is to say $n\equiv j$.
	\end{enumerate}
	In the above situation, we denote by $\mathbb{Z}/d\mathbb{Z}$ the set of equivalence classes and we will denote by $[n]_d$ the equivalence class of congruence of a given integer $n$ given by:
	
	(each difference of two values in the braces is divisible by $d$ and this is therefore an equivalence class), thus:
	
	In particular (trivial since we obtain thus the all $\mathbb{Z}$):
	
	\end{tcolorbox}
	Thus, we see that the first example we gave with the even and odd numbers is a particularly simple case of congruence equivalence classes modulo $2$ because they all reduce to only two classes.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The operations of addition and multiplication on $\mathbb{Z}$ define also the operation of addition and multiplication on $\mathbb{Z}/d\mathbb{Z}$. Then we say that these operations are compatible with the equivalence relation and then form a ring (\SeeChapter{see section Set Theory page \pageref{ring}}).
	\end{tcolorbox}
	
	\pagebreak
	\subsection{Fundamental Arithmetic Laws}
	As we have said before, there is a fundamental operator (addition) from which we can define multiplication, subtraction (provided that the chosen Numbers Set is adapted to it....) and division (provided that the chosen Numbers Set is also adapted to it....) and around which we can build the entire Analytical Mathematics.
	
	Obviously there are some subtleties to be considered when the level of rigour increase. The reader can then refer to the whole section of Set Theory (page \pageref{set theory}) where fundamental laws are redefined more accurately than what will follow.
	
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/fundamentals_delucq.jpg}
	\end{figure}
	
	\subsubsection{Addition}
	\textbf{Definition (\#\thesection.\mydef):} The "\NewTerm{addition}\index{addition}\label{addition}" of integers is an operation denoted "$+$" which has for only purpose to bring together in one number all the units contained in several others. The result of the operation is named the "\NewTerm{sum}\index{sum}", the "\NewTerm{total}" or "\NewTerm{cumul}". The numbers to be added are named therefore "\NewTerm{terms of the addition}\index{terms of the addition}".
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The signs of addition "$+$" and subtraction "$-$" are due to the German mathematician Johannes Widmann (11489 according to holocene calendar).
	\end{tcolorbox}
	Thus, $A + B + C \ldots$ are the terms of the addition and the result is the sum of the terms of the addition.
	
	Or in schematic form of a special case:
	\begin{gather*}
		0+4+3=4+3=7
	\end{gather*}
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,651); %set diagram left start at 0, and has height of 651
		
		%Straight Lines [id:da4397877495434799] 
		\draw    (126,144) -- (498.5,144) (153,140) -- (153,148)(180,140) -- (180,148)(207,140) -- (207,148)(234,140) -- (234,148)(261,140) -- (261,148)(288,140) -- (288,148)(315,140) -- (315,148)(342,140) -- (342,148)(369,140) -- (369,148)(396,140) -- (396,148)(423,140) -- (423,148)(450,140) -- (450,148)(477,140) -- (477,148) ;
		\draw [shift={(501.5,144)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (10.72,-5.15) -- (0,0) -- (10.72,5.15) -- (7.12,0) -- cycle    ;
		\draw [shift={(123,144)}, rotate = 0] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (10.72,-5.15) -- (0,0) -- (10.72,5.15) -- (7.12,0) -- cycle    ;
		%Shape: Arc [id:dp9685248366339188] 
		\draw  [draw opacity=0] (153.2,131.81) .. controls (154.18,109.33) and (178.04,91.33) .. (207.33,91.33) .. controls (237.25,91.33) and (261.5,110.1) .. (261.5,133.25) .. controls (261.5,133.6) and (261.49,133.96) .. (261.48,134.31) -- (207.33,133.25) -- cycle ; \draw    (153.2,131.81) .. controls (154.18,109.33) and (178.04,91.33) .. (207.33,91.33) .. controls (237.25,91.33) and (261.5,110.1) .. (261.5,133.25) ; \draw [shift={(261.48,134.31)}, rotate = 261.36] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (10.72,-5.15) -- (0,0) -- (10.72,5.15) -- (7.12,0) -- cycle    ; 
		%Shape: Arc [id:dp2727865493648489] 
		\draw  [draw opacity=0] (261.48,134.31) .. controls (262,110.58) and (279.71,91.52) .. (301.49,91.52) .. controls (323.59,91.52) and (341.5,111.15) .. (341.5,135.37) .. controls (341.5,135.63) and (341.5,135.89) .. (341.49,136.15) -- (301.49,135.37) -- cycle ; \draw    (261.48,134.31) .. controls (262,110.58) and (279.71,91.52) .. (301.49,91.52) .. controls (323.59,91.52) and (341.5,111.15) .. (341.5,135.37) ; \draw [shift={(341.49,136.15)}, rotate = 263.81] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (10.72,-5.15) -- (0,0) -- (10.72,5.15) -- (7.12,0) -- cycle    ; 
		
		% Text Node
		\draw (148,151.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 0$};
		% Text Node
		\draw (174.33,151.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 1$};
		% Text Node
		\draw (201.66,151.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 2$};
		% Text Node
		\draw (228.99,151.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 3$};
		% Text Node
		\draw (255.32,151.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 4$};
		% Text Node
		\draw (282.65,151.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 5$};
		% Text Node
		\draw (308.98,151.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 6$};
		% Text Node
		\draw (337.31,151.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 7$};
		% Text Node
		\draw (364.64,151.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 8$};
		% Text Node
		\draw (390.97,151.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 9$};
		% Text Node
		\draw (413.3,151.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 10$};
		% Text Node
		\draw (440.63,151.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 11$};
		% Text Node
		\draw (469,151.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 12$};
		% Text Node
		\draw (202.32,70.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 4$};
		% Text Node
		\draw (297.32,69.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 3$};
		% Text Node
		\draw (250.66,71.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle +$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{One possible schema for addition}
	\end{figure}
	Here is a list of some intuitive properties that we assume without proofs (as in fact they are axioms) of the operation of addition:
	\begin{enumerate}
		\item[P1.] The sum of several numbers do not depend on the order of terms. Then we say that the addition is a "\NewTerm{commutative operation}\index{commutative operation}". This means concretely for any two numbers:
		
		
		\item[P2.] The sum of several numbers does not change if we replace two or more of them by their intermediate result. Then we say that the addition is an "\NewTerm{associative operation}\index{associative operation}":
		
		
		\item[P3.] The Zero is the neutral element of addition because any number added to zero gives that number:
		
		
		\item[P4.] Depending on the set in which we work ($\mathbb{Z},\mathbb{Q},\mathbb{R},\ldots$), the addition may include a term in such a way that a sum is zero. Then we say that there exists an "\NewTerm{opposite}\index{opposite}" to the sum such as:
		
	\end{enumerate}
	We have define more rigorously the addition using the Peano axioms in the particular case of all natural numbers $\mathbb{N}$ as we have already see in the section Numbers (page \pageref{natural numbers}). So, with these axioms it is possible to prove that there exists one and only one application (uniqueness), denoted "$+$" of $\mathbb{N}\times \mathbb{N}$ in $\mathbb{N}$ satisfying:
	
	where $s$ means "successor".
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	As this book has not be written for mathematicians, we will pass the proof (relatively long and of little interest in the case of business) and we will assume that the application "$+$" exists and is unique ... and that it follows from the above properties.
	\end{tcolorbox}
	Let $x_1,x_2,\ldots,x_n$ be any numbers then we can write the sum as following:
	
	by defining upper and lower bound to the indexed sum (below and above the uppercase greek symbol Sigma).
	
	Here are some properties relatively to this condenses notation that should be obvious (if not the reader can send us a request we will add the details):
	
	where $k$ is a constant.
	
	Such a notation is especially useful when we have to sum for example double entry tables like the special case illustrated below (symmetric square matrix):
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/matrix_sum.pdf}
	\end{figure}
	
	Let us see now some concrete examples of additions of various simple number in the purpose to practice the basis:
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	The addition of two numbers relatively small is quite easy since we have learn by heart to count to a number resulting of the operation. Therefore (examples taken on decimal basis):
	
	and:
	
	and:
	
	For more bigger number we can adopt another method that human must also learn by heart. For example:
	
	The algorithm (process) is therefore the following: We add the columns ($4$ columns in this example) from right to left. For the first column we have therefore $4+5=9$ this gives:
	
	and we continue like this for the second column where we have $4+7=11$ at the difference that now we have a number $>10$, then we report the first left digit on the next (left) column for the addition. Therefore:
	
	The third column we be calculated therefore as $1+2+4=7$ which give us:
	
	For the last column we have $9+3=12$ and once again we report the first digit from the left on the next column of the addition. Therefore:
	
	Finally:
	
	\end{tcolorbox}
	This example shows how we can proceed for the addition of any real numbers: we do an addition column by column from the right to the left and if the result of one addition is greater than $10$, we report the left digit on the next (left) column.
	
	Following a reader request, here is also an illustrated example on how we teach to add simple fractions together at the middle school level, first in the case where the denominators are the same:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1467); %set diagram left start at 0, and has height of 1467
		
		%Shape: Rectangle [id:dp15058673133726264] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (50,54) -- (97.5,54) -- (97.5,225) -- (50,225) -- cycle ;
		%Shape: Rectangle [id:dp879865345102756] 
		\draw  [fill={rgb, 255:red, 216; green, 216; blue, 216 }  ,fill opacity=1 ] (97.5,54) -- (145,54) -- (145,225) -- (97.5,225) -- cycle ;
		%Shape: Rectangle [id:dp8546106944927188] 
		\draw  [fill={rgb, 255:red, 216; green, 216; blue, 216 }  ,fill opacity=1 ] (145,54) -- (192.5,54) -- (192.5,225) -- (145,225) -- cycle ;
		%Shape: Rectangle [id:dp3379471409722581] 
		\draw  [fill={rgb, 255:red, 216; green, 216; blue, 216 }  ,fill opacity=1 ] (192.5,54) -- (240,54) -- (240,225) -- (192.5,225) -- cycle ;
		%Shape: Rectangle [id:dp7784363512191921] 
		\draw   (240,54) -- (287.5,54) -- (287.5,225) -- (240,225) -- cycle ;
		%Shape: Rectangle [id:dp7607364347634353] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (29,164) -- (76.5,164) -- (76.5,335) -- (29,335) -- cycle ;
		%Shape: Rectangle [id:dp8487664269502082] 
		\draw  [fill={rgb, 255:red, 216; green, 216; blue, 216 }  ,fill opacity=1 ] (114.5,164) -- (257.5,164) -- (257.5,335) -- (114.5,335) -- cycle ;
		%Shape: Brace [id:dp29628925778273896] 
		\draw   (286.5,49) .. controls (286.5,44.33) and (284.17,42) .. (279.5,42) -- (178.75,42) .. controls (172.08,42) and (168.75,39.67) .. (168.75,35) .. controls (168.75,39.67) and (165.42,42) .. (158.75,42)(161.75,42) -- (58,42) .. controls (53.33,42) and (51,44.33) .. (51,49) ;
		%Shape: Rectangle [id:dp037870380325618846] 
		\draw  [fill={rgb, 255:red, 128; green, 128; blue, 128 }  ,fill opacity=1 ] (353,54) -- (400.5,54) -- (400.5,225) -- (353,225) -- cycle ;
		%Shape: Rectangle [id:dp4092949920839781] 
		\draw  [fill={rgb, 255:red, 128; green, 128; blue, 128 }  ,fill opacity=1 ] (400.5,54) -- (448,54) -- (448,225) -- (400.5,225) -- cycle ;
		%Shape: Rectangle [id:dp0912974430996607] 
		\draw  [fill={rgb, 255:red, 128; green, 128; blue, 128 }  ,fill opacity=1 ] (448,54) -- (495.5,54) -- (495.5,225) -- (448,225) -- cycle ;
		%Shape: Rectangle [id:dp1688770449899546] 
		\draw  [fill={rgb, 255:red, 128; green, 128; blue, 128 }  ,fill opacity=1 ] (495.5,54) -- (543,54) -- (543,225) -- (495.5,225) -- cycle ;
		%Shape: Rectangle [id:dp9694770296545836] 
		\draw   (543,54) -- (590.5,54) -- (590.5,225) -- (543,225) -- cycle ;
		%Shape: Rectangle [id:dp24081059584890796] 
		\draw  [fill={rgb, 255:red, 216; green, 216; blue, 216 }  ,fill opacity=1 ] (338.5,164) -- (529.5,164) -- (529.5,335) -- (338.5,335) -- cycle ;
		
		% Text Node
		\draw (45,225.9) node [anchor=north west][inner sep=0.75pt]  [font=\large]  {$\dfrac{1}{5}$};
		% Text Node
		\draw (182,225.9) node [anchor=north west][inner sep=0.75pt]  [font=\large]  {$\dfrac{3}{5}$};
		% Text Node
		\draw (82,234.9) node [anchor=north west][inner sep=0.75pt]  [font=\huge]  {$+$};
		% Text Node
		\draw (279,240.9) node [anchor=north west][inner sep=0.75pt]  [font=\huge]  {$=$};
		% Text Node
		\draw (139,13) node [anchor=north west][inner sep=0.75pt]   [align=left] {$1$ unit ($5/5$)};
		% Text Node
		\draw (427,225.9) node [anchor=north west][inner sep=0.75pt]  [font=\large]  {$\dfrac{4}{5}$};
		\end{tikzpicture}
		\vspace*{2mm}
		\caption{Addition of fractions with common denominator}
	\end{figure}
	Or if the denominators are not common, we seek common fractions equivalent to each term of the addition, in such a way that all the fractions have a common denominator and afterwards we add as we did before:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1467); %set diagram left start at 0, and has height of 1467
		
		%Shape: Rectangle [id:dp0361199669384944] 
		\draw  [fill={rgb, 255:red, 216; green, 216; blue, 216 }  ,fill opacity=1 ] (181.5,54) -- (200.5,54) -- (200.5,225) -- (181.5,225) -- cycle ;
		%Shape: Rectangle [id:dp5589332134481337] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (123.5,54) -- (181.5,54) -- (181.5,225) -- (123.5,225) -- cycle ;
		%Shape: Rectangle [id:dp7784363512191921] 
		\draw   (181.5,54) -- (238.5,54) -- (238.5,225) -- (181.5,225) -- cycle ;
		%Shape: Brace [id:dp29628925778273896] 
		\draw   (238.5,51) .. controls (238.5,46.33) and (236.17,44) .. (231.5,44) -- (191.25,44) .. controls (184.58,44) and (181.25,41.67) .. (181.25,37) .. controls (181.25,41.67) and (177.92,44) .. (171.25,44)(174.25,44) -- (131,44) .. controls (126.33,44) and (124,46.33) .. (124,51) ;
		%Shape: Rectangle [id:dp6772297828585432] 
		\draw  [fill={rgb, 255:red, 216; green, 216; blue, 216 }  ,fill opacity=1 ] (188.75,164.88) -- (236.25,164.88) -- (236.25,335.88) -- (188.75,335.88) -- cycle ;
		%Shape: Rectangle [id:dp6033296048500232] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (103.5,164.88) -- (161.5,164.88) -- (161.5,335.88) -- (103.5,335.88) -- cycle ;
		%Shape: Rectangle [id:dp7439695955841104] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (277.5,54) -- (287.5,54) -- (287.5,225) -- (277.5,225) -- cycle ;
		%Shape: Brace [id:dp25134071643680067] 
		\draw   (389.5,51) .. controls (389.5,46.33) and (387.17,44) .. (382.5,44) -- (342.25,44) .. controls (335.58,44) and (332.25,41.67) .. (332.25,37) .. controls (332.25,41.67) and (328.92,44) .. (322.25,44)(325.25,44) -- (282,44) .. controls (277.33,44) and (275,46.33) .. (275,51) ;
		%Shape: Rectangle [id:dp7502166699454875] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (287.5,54) -- (297.5,54) -- (297.5,225) -- (287.5,225) -- cycle ;
		%Shape: Rectangle [id:dp8007953447519565] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (297.5,54) -- (307.5,54) -- (307.5,225) -- (297.5,225) -- cycle ;
		%Shape: Rectangle [id:dp08498779265648793] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (307.5,54) -- (317.5,54) -- (317.5,225) -- (307.5,225) -- cycle ;
		%Shape: Rectangle [id:dp039525029428370884] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (317.5,54) -- (327.5,54) -- (327.5,225) -- (317.5,225) -- cycle ;
		%Shape: Rectangle [id:dp909126796479599] 
		\draw  [fill={rgb, 255:red, 216; green, 216; blue, 216 }  ,fill opacity=1 ] (327.5,54) -- (337.5,54) -- (337.5,225) -- (327.5,225) -- cycle ;
		%Shape: Rectangle [id:dp4821185452215482] 
		\draw  [fill={rgb, 255:red, 216; green, 216; blue, 216 }  ,fill opacity=1 ] (337.5,54) -- (347.5,54) -- (347.5,225) -- (337.5,225) -- cycle ;
		%Shape: Rectangle [id:dp5210603595241672] 
		\draw   (347.5,54) -- (357.5,54) -- (357.5,225) -- (347.5,225) -- cycle ;
		%Shape: Rectangle [id:dp27588102697771943] 
		\draw   (357.5,54) -- (367.5,54) -- (367.5,225) -- (357.5,225) -- cycle ;
		%Shape: Rectangle [id:dp8562918566682411] 
		\draw   (367.5,54) -- (377.5,54) -- (377.5,225) -- (367.5,225) -- cycle ;
		%Shape: Rectangle [id:dp8563278338831437] 
		\draw   (377.5,54) -- (387.5,54) -- (387.5,225) -- (377.5,225) -- cycle ;
		%Shape: Rectangle [id:dp24081059584890796] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (265.5,164.88) -- (316.5,164.88) -- (316.5,335.88) -- (265.5,335.88) -- cycle ;
		%Shape: Rectangle [id:dp4103848433135051] 
		\draw  [fill={rgb, 255:red, 216; green, 216; blue, 216 }  ,fill opacity=1 ] (342.13,164.88) -- (362.88,164.88) -- (362.88,335.88) -- (342.13,335.88) -- cycle ;
		%Shape: Rectangle [id:dp8725868255401792] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (415.5,54) -- (425.5,54) -- (425.5,225) -- (415.5,225) -- cycle ;
		%Shape: Brace [id:dp5322288595077529] 
		\draw   (527.5,51) .. controls (527.5,46.33) and (525.17,44) .. (520.5,44) -- (480.25,44) .. controls (473.58,44) and (470.25,41.67) .. (470.25,37) .. controls (470.25,41.67) and (466.92,44) .. (460.25,44)(463.25,44) -- (420,44) .. controls (415.33,44) and (413,46.33) .. (413,51) ;
		%Shape: Rectangle [id:dp9467908913524625] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (425.5,54) -- (435.5,54) -- (435.5,225) -- (425.5,225) -- cycle ;
		%Shape: Rectangle [id:dp8689111044223923] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (435.5,54) -- (445.5,54) -- (445.5,225) -- (435.5,225) -- cycle ;
		%Shape: Rectangle [id:dp9125697340608361] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (445.5,54) -- (455.5,54) -- (455.5,225) -- (445.5,225) -- cycle ;
		%Shape: Rectangle [id:dp8430312724051474] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (455.5,54) -- (465.5,54) -- (465.5,225) -- (455.5,225) -- cycle ;
		%Shape: Rectangle [id:dp3010950625653217] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (465.5,54) -- (475.5,54) -- (475.5,225) -- (465.5,225) -- cycle ;
		%Shape: Rectangle [id:dp9593600553283077] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (475.5,54) -- (485.5,54) -- (485.5,225) -- (475.5,225) -- cycle ;
		%Shape: Rectangle [id:dp5300287302300448] 
		\draw   (485.5,54) -- (495.5,54) -- (495.5,225) -- (485.5,225) -- cycle ;
		%Shape: Rectangle [id:dp8329703393589711] 
		\draw   (495.5,54) -- (505.5,54) -- (505.5,225) -- (495.5,225) -- cycle ;
		%Shape: Rectangle [id:dp19643072823803132] 
		\draw   (505.5,54) -- (515.5,54) -- (515.5,225) -- (505.5,225) -- cycle ;
		%Shape: Rectangle [id:dp4627657611915843] 
		\draw   (515.5,54) -- (525.5,54) -- (525.5,225) -- (515.5,225) -- cycle ;
		%Shape: Rectangle [id:dp7589492057429126] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (403.5,164.88) -- (474.5,164.88) -- (474.5,335.88) -- (403.5,335.88) -- cycle ;
		
		% Text Node
		\draw (124.5,226.78) node [anchor=north west][inner sep=0.75pt]  [font=\large]  {$\dfrac{1}{2}$};
		% Text Node
		\draw (161,236.77) node [anchor=north west][inner sep=0.75pt]  [font=\huge]  {$+$};
		% Text Node
		\draw (236,243.77) node [anchor=north west][inner sep=0.75pt]  [font=\huge]  {$=$};
		% Text Node
		\draw (133,10) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 1$ whole ($\displaystyle 5/5$)};
		% Text Node
		\draw (278,226.78) node [anchor=north west][inner sep=0.75pt]  [font=\large]  {$\dfrac{5}{10}$};
		% Text Node
		\draw (204.5,226.78) node [anchor=north west][inner sep=0.75pt]  [font=\large]  {$\dfrac{1}{5}$};
		% Text Node
		\draw (284,12) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 1$ whole ($\displaystyle 10/10$)};
		% Text Node
		\draw (316,236.77) node [anchor=north west][inner sep=0.75pt]  [font=\huge]  {$+$};
		% Text Node
		\draw (370,243.77) node [anchor=north west][inner sep=0.75pt]  [font=\huge]  {$=$};
		% Text Node
		\draw (339.5,226.4) node [anchor=north west][inner sep=0.75pt]  [font=\large]  {$\dfrac{2}{10}$};
		% Text Node
		\draw (422,12) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 1$ whole ($\displaystyle 10/10$)};
		% Text Node
		\draw (426,226.78) node [anchor=north west][inner sep=0.75pt]  [font=\large]  {$\dfrac{7}{10}$};
		\end{tikzpicture}
		\vspace*{2mm}
		\caption{Addition of fractions without common denominators}
	\end{figure}
	We then see above that to add a fraction from another we first put the fraction at the same denominator, we then add the numerators and we keep the denominator if there is no possible simplification:
	

	Obviously when we add fractions together we could also get a number that represents another number that is greater than $1$ as for example:
	
	This can be illustrated as following:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1467); %set diagram left start at 0, and has height of 1467
		
		%Shape: Pie [id:dp03534249548355661] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (82,109.86) .. controls (81.87,137.52) and (59.65,159.89) .. (32.25,159.89) .. controls (32.13,159.89) and (32,159.89) .. (31.88,159.89) -- (32.25,109.63) -- cycle ;
		%Shape: Pie [id:dp40224005131640395] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (136.75,109.86) .. controls (136.62,137.52) and (114.4,159.89) .. (87,159.89) .. controls (86.88,159.89) and (86.75,159.89) .. (86.63,159.89) -- (87,109.63) -- cycle ;
		%Shape: Pie [id:dp8889585242982567] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (211,109.86) .. controls (210.87,137.52) and (188.65,159.89) .. (161.25,159.89) .. controls (161.13,159.89) and (161,159.89) .. (160.88,159.89) -- (161.25,109.63) -- cycle ;
		%Shape: Pie [id:dp5785461820295488] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (266.87,109.86) .. controls (266.74,137.52) and (244.52,159.89) .. (217.12,159.89) .. controls (217,159.89) and (216.87,159.89) .. (216.75,159.89) -- (217.12,109.63) -- cycle ;
		%Shape: Pie [id:dp408982970956576] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (322.75,109.86) .. controls (322.62,137.52) and (300.4,159.89) .. (273,159.89) .. controls (272.88,159.89) and (272.75,159.89) .. (272.63,159.89) -- (273,109.63) -- cycle ;
		%Shape: Pie [id:dp4731960173379932] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (401,109.86) .. controls (400.87,137.52) and (378.65,159.89) .. (351.25,159.89) .. controls (351.13,159.89) and (351,159.89) .. (350.88,159.89) -- (351.25,109.63) -- cycle ;
		%Shape: Pie [id:dp8280920425318896] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (457.31,109.86) .. controls (457.18,137.52) and (434.96,159.89) .. (407.56,159.89) .. controls (407.44,159.89) and (407.31,159.89) .. (407.19,159.89) -- (407.56,109.63) -- cycle ;
		%Shape: Pie [id:dp4145923559488267] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (513.62,109.86) .. controls (513.49,137.52) and (491.27,159.89) .. (463.87,159.89) .. controls (463.75,159.89) and (463.62,159.89) .. (463.5,159.89) -- (463.87,109.63) -- cycle ;
		%Shape: Pie [id:dp23163035423558487] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (569.93,109.86) .. controls (569.8,137.52) and (547.58,159.89) .. (520.18,159.89) .. controls (520.06,159.89) and (519.93,159.89) .. (519.81,159.89) -- (520.18,109.63) -- cycle ;
		%Shape: Pie [id:dp2609449016599821] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (626.25,109.86) .. controls (626.12,137.52) and (603.9,159.89) .. (576.5,159.89) .. controls (576.38,159.89) and (576.25,159.89) .. (576.13,159.89) -- (576.5,109.63) -- cycle ;
		
		% Text Node
		\draw (137,115.53) node [anchor=north west][inner sep=0.75pt]  [font=\LARGE]  {$+$};
		% Text Node
		\draw (324,121) node [anchor=north west][inner sep=0.75pt]  [font=\LARGE]  {$=$};
		\end{tikzpicture}
	\end{figure}
	If you want to assemble these quarter discs, here is what we get:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1467); %set diagram left start at 0, and has height of 1467
		
		%Shape: Pie [id:dp5346911074682219] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (425,27.64) .. controls (424.83,27.64) and (424.67,27.64) .. (424.5,27.64) .. controls (382.77,27.64) and (348.94,61.28) .. (348.94,102.78) -- (424.5,102.78) -- cycle ;
		%Shape: Pie [id:dp03534249548355661] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (313.62,107.48) .. controls (313.44,148.61) and (280.15,181.89) .. (239.13,181.89) -- (239.13,107.13) -- cycle ;
		%Shape: Pie [id:dp894017693019493] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (239.25,29.13) .. controls (280.38,29.32) and (313.66,62.38) .. (313.66,103.13) -- (238.9,103.13) -- cycle ;
		%Shape: Pie [id:dp18174465997432065] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (235.5,181.13) .. controls (235.21,181.13) and (234.91,181.13) .. (234.62,181.13) .. controls (193.33,180.84) and (160.09,147.48) .. (160.38,106.61) -- (235.14,107.13) -- cycle ;
		%Shape: Pie [id:dp5610284498240614] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (235.63,29.13) .. controls (235.47,29.13) and (235.3,29.13) .. (235.14,29.13) .. controls (193.85,29.13) and (160.38,62.26) .. (160.38,103.13) -- (235.14,103.13) -- cycle ;
		%Shape: Circle [id:dp487119526160791] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}] (348.38,104.27) .. controls (348.38,61.95) and (382.68,27.64) .. (425,27.64) .. controls (467.32,27.64) and (501.62,61.95) .. (501.62,104.27) .. controls (501.62,146.59) and (467.32,180.89) .. (425,180.89) .. controls (382.68,180.89) and (348.38,146.59) .. (348.38,104.27) -- cycle ;
		\end{tikzpicture}
	\end{figure}
	You understand then why we can say that we have $5/4$ of a disc!

	This algorithm (process or methodology) of addition is quite simple to understand and to execute. We will not go further on this subject at this day!
	
	\pagebreak
	\subsubsection{Subtraction}
	\textbf{Definition (\#\thesection.\mydef):} The "\NewTerm{subtraction}\index{subtraction}" is a mathematical operation that represents the operation of removing objects from a collection. More formally the subtraction of the number $A$ by the number $B$ denoted by the symbol "$-$" consist in founding the number $C$ such that added to $B$ gives $A$.

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	As we saw it in the section of Set Theory (page \pageref{internal composition law}) the subtraction in the set $\mathbb{N}$ could be possible only if $A>B$.
	\end{tcolorbox}
	
	Formally we write an inline literal subtraction in the form:
	
	That must satisfies:
	
	Or in schematic form of a special case:
	\begin{gather*}
		10-3-4=7-4=3
	\end{gather*}
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,651); %set diagram left start at 0, and has height of 651
		
		%Straight Lines [id:da4397877495434799] 
		\draw    (126,143.94) -- (170.5,143) -- (498.5,143.99) (152.91,139.37) -- (153.08,147.37)(180,139.03) -- (179.98,147.03)(207,139.11) -- (206.98,147.11)(234,139.19) -- (233.98,147.19)(261,139.27) -- (260.98,147.27)(288,139.35) -- (287.98,147.35)(315,139.44) -- (314.98,147.44)(342,139.52) -- (341.98,147.52)(369,139.6) -- (368.98,147.6)(396,139.68) -- (395.98,147.68)(423,139.76) -- (422.98,147.76)(450,139.84) -- (449.98,147.84)(477,139.93) -- (476.98,147.93) ;
		\draw [shift={(501.5,144)}, rotate = 180.17] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (10.72,-5.15) -- (0,0) -- (10.72,5.15) -- (7.12,0) -- cycle    ;
		\draw [shift={(123,144)}, rotate = 358.79] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (10.72,-5.15) -- (0,0) -- (10.72,5.15) -- (7.12,0) -- cycle    ;
		%Shape: Arc [id:dp9685248366339188] 
		\draw  [draw opacity=0] (235.2,131.81) .. controls (236.18,109.33) and (260.04,91.33) .. (289.33,91.33) .. controls (319.25,91.33) and (343.5,110.1) .. (343.5,133.25) .. controls (343.5,133.6) and (343.49,133.96) .. (343.48,134.31) -- (289.33,133.25) -- cycle ; \draw    (235.47,128.81) .. controls (238.34,107.75) and (261.36,91.33) .. (289.33,91.33) .. controls (319.25,91.33) and (343.5,110.1) .. (343.5,133.25) .. controls (343.5,133.6) and (343.49,133.96) .. (343.48,134.31) ;  \draw [shift={(235.2,131.81)}, rotate = 282.94] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (10.72,-5.15) -- (0,0) -- (10.72,5.15) -- (7.12,0) -- cycle    ;
		%Shape: Arc [id:dp2727865493648489] 
		\draw  [draw opacity=0] (343.48,134.31) .. controls (344,110.58) and (361.71,91.52) .. (383.49,91.52) .. controls (405.59,91.52) and (423.5,111.15) .. (423.5,135.37) .. controls (423.5,135.63) and (423.5,135.89) .. (423.49,136.15) -- (383.49,135.37) -- cycle ; \draw    (343.66,131.14) .. controls (345.6,108.9) and (362.69,91.52) .. (383.49,91.52) .. controls (405.59,91.52) and (423.5,111.15) .. (423.5,135.37) .. controls (423.5,135.63) and (423.5,135.89) .. (423.49,136.15) ;  \draw [shift={(343.48,134.31)}, rotate = 278.75] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (10.72,-5.15) -- (0,0) -- (10.72,5.15) -- (7.12,0) -- cycle    ;
		
		% Text Node
		\draw (148,151.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 0$};
		% Text Node
		\draw (174.33,151.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 1$};
		% Text Node
		\draw (201.66,151.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 2$};
		% Text Node
		\draw (228.99,151.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 3$};
		% Text Node
		\draw (255.32,151.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 4$};
		% Text Node
		\draw (282.65,151.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 5$};
		% Text Node
		\draw (308.98,151.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 6$};
		% Text Node
		\draw (337.31,151.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 7$};
		% Text Node
		\draw (364.64,151.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 8$};
		% Text Node
		\draw (390.97,151.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 9$};
		% Text Node
		\draw (413.3,151.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 10$};
		% Text Node
		\draw (440.63,151.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 11$};
		% Text Node
		\draw (469,151.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 12$};
		% Text Node
		\draw (281.32,70.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle -4$};
		% Text Node
		\draw (375.32,69.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle -3$};
		% Text Node
		\draw (335.66,71.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle +$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{One possible schema for subtraction}
	\end{figure}
	Here are some intuitive properties that we assume without proof for the subtraction operation (as it can be deduce from the addition...):
	\begin{enumerate}
		\item[P1.] The subtraction of several numbers depends on the order of the terms. We say when than subtraction is a "\NewTerm{non-commutative operation}\index{non-commutative operation}". Indeed:
		
		
		\item[P2.] The subtraction of several numbers change if we replace two or more of them by their intermediate result. We say when the subtraction is a "\NewTerm{non-associative operation}\index{non-associative operation}". Indeed:
		
		
		\item[P3.]The zero is not the neutral element of subtraction. Indeed, any number to which we subtract zero gives the same number, so zero is neutral on the right ... but not left because any number we subtract to zero does not give zero! We then say that the zero is only "\NewTerm{neutral on the right}\index{neutral on the right}" in the case of subtraction. Indeed:
		
		
		\item[P4.] Depending on the set in which we work, the subtraction can include a term in such a way that the total is zero. We then say that there is a "\NewTerm{opposite}\index{opposite}" for the subtraction.
	\end{enumerate}
	
	In most complicated cases we have a special vocabulary:
	
	The "\NewTerm{minuend}\index{minuend}" is $704$, the "\NewTerm{subtrahend}\index{subtrahend}" is $512$. The minuend digits are $m_3= 7$, $m_2 = 0$ and $m_1 = 4$. The subtrahend digits are $s_3 = 5$, $s_2 = 1$ and $s_1 = 2$. Beginning at the one's place, $4$ is not less than $2$ so the difference $2$ is written down in the result's one place. In the ten's place, $0$ is less than $1$, so the $0$ is increased by $10$, and the difference with $1$, which is $9$, is written down in the ten's place. The American method corrects for the increase of ten by reducing the digit in the minuend's hundreds place by one. That is, the $7$ is struck through and replaced by a $6$. The subtraction then proceeds in the hundreds place, where $6$ is not less than $5$, so the difference is written down in the result's hundred's place. We are now done, the result is $192$.
	
	Let us see now some concrete examples of subtractions of various simple number in the purpose to practice the basis:
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	The subtraction of two relatively small numbers is pretty easy once we memorized to count to at least the number resulting from this operation. So:
	
	and:
	
	and:
	
	For larger numbers another possible method must be learned by heart (as well as for the addition). For example:
	
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	we subtract the columns ($4$ columns in this example) from right to left. In the first column we have $4-5=-1<0$ so we report $-1$ to the next column (second one) and we write $10-1=9$ below the horizontal line of the first column:
	
	and we continue as well for the second column $7-8=-1<0$ so that we report $-1$ on the next column (third one) and as $-1-1=-2$ we report $10-2=8$ below the horizontal bar of the second column:
	
	The third column is calculated as $5-7=-2<0$ and we report $-1$ on the next column (fourth one) and as $-1-2=-3$ we report $10-3=7$ below the line of the third column bar:
	
	In the last column we have $4-3=1>0$ therefore we report the nothing on the next column and as $1-1=0$ we report $0$ below the line of the fourth column bar:
	
	\end{tcolorbox}
	That's how we therefore we proceed to subtracting any numbers. We make a subtraction by column from the right to the left and if the result is a subtraction is less than zero we report $-1$ to the next column and the addition of the latest report on the subtraction obtained below the line.
	
	Following a reader request, here is also an illustrated example on how we teach to subtract simple fractions together at the middle school level:
	 \begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1467); %set diagram left start at 0, and has height of 1467
		
		%Shape: Rectangle [id:dp9603791211131505] 
		\draw  [fill={rgb, 255:red, 128; green, 128; blue, 128 }  ,fill opacity=1 ] (283.5,69) -- (302.5,69) -- (302.5,240) -- (283.5,240) -- cycle ;
		%Shape: Rectangle [id:dp32576458528181185] 
		\draw  [fill={rgb, 255:red, 128; green, 128; blue, 128 }  ,fill opacity=1 ] (302.5,69) -- (321.5,69) -- (321.5,240) -- (302.5,240) -- cycle ;
		%Shape: Rectangle [id:dp11175024419101609] 
		\draw  [fill={rgb, 255:red, 128; green, 128; blue, 128 }  ,fill opacity=1 ] (321.5,69) -- (340.5,69) -- (340.5,240) -- (321.5,240) -- cycle ;
		%Shape: Rectangle [id:dp1568517666696665] 
		\draw  [fill={rgb, 255:red, 216; green, 216; blue, 216 }  ,fill opacity=1 ] (340.5,69) -- (359.5,69) -- (359.5,240) -- (340.5,240) -- cycle ;
		%Shape: Rectangle [id:dp40124397454928995] 
		\draw  [fill={rgb, 255:red, 216; green, 216; blue, 216 }  ,fill opacity=1 ] (359.5,69) -- (378.5,69) -- (378.5,240) -- (359.5,240) -- cycle ;
		%Shape: Rectangle [id:dp5991002450937528] 
		\draw   (378.5,69) -- (397.5,69) -- (397.5,240) -- (378.5,240) -- cycle ;
		%Shape: Rectangle [id:dp5994511597673278] 
		\draw  [fill={rgb, 255:red, 128; green, 128; blue, 128 }  ,fill opacity=1 ] (264.5,69) -- (283.5,69) -- (283.5,240) -- (264.5,240) -- cycle ;
		%Shape: Rectangle [id:dp24993392853963248] 
		\draw  [fill={rgb, 255:red, 128; green, 128; blue, 128 }  ,fill opacity=1 ] (245.5,69) -- (264.5,69) -- (264.5,240) -- (245.5,240) -- cycle ;
		%Shape: Rectangle [id:dp2227043587632629] 
		\draw  [fill={rgb, 255:red, 128; green, 128; blue, 128 }  ,fill opacity=1 ] (137.5,69) -- (156.5,69) -- (156.5,240) -- (137.5,240) -- cycle ;
		%Shape: Rectangle [id:dp36878785137272874] 
		\draw  [fill={rgb, 255:red, 128; green, 128; blue, 128 }  ,fill opacity=1 ] (118.5,69) -- (137.5,69) -- (137.5,240) -- (118.5,240) -- cycle ;
		%Shape: Rectangle [id:dp16604312962966983] 
		\draw  [fill={rgb, 255:red, 128; green, 128; blue, 128 }  ,fill opacity=1 ] (99.5,69) -- (118.5,69) -- (118.5,240) -- (99.5,240) -- cycle ;
		%Shape: Rectangle [id:dp6563509215738539] 
		\draw  [fill={rgb, 255:red, 128; green, 128; blue, 128 }  ,fill opacity=1 ] (80.5,69) -- (99.5,69) -- (99.5,240) -- (80.5,240) -- cycle ;
		%Shape: Rectangle [id:dp7146153115576943] 
		\draw  [fill={rgb, 255:red, 128; green, 128; blue, 128 }  ,fill opacity=1 ] (61.5,69) -- (80.5,69) -- (80.5,240) -- (61.5,240) -- cycle ;
		%Shape: Brace [id:dp29628925778273896] 
		\draw   (211.5,63) .. controls (211.5,58.33) and (209.17,56) .. (204.5,56) -- (146.75,56) .. controls (140.08,56) and (136.75,53.67) .. (136.75,49) .. controls (136.75,53.67) and (133.42,56) .. (126.75,56)(129.75,56) -- (69,56) .. controls (64.33,56) and (62,58.33) .. (62,63) ;
		%Shape: Rectangle [id:dp7950984026622849] 
		\draw  [fill={rgb, 255:red, 128; green, 128; blue, 128 }  ,fill opacity=1 ] (453.5,69) -- (472.5,69) -- (472.5,240) -- (453.5,240) -- cycle ;
		%Shape: Rectangle [id:dp7284265213336241] 
		\draw  [fill={rgb, 255:red, 128; green, 128; blue, 128 }  ,fill opacity=1 ] (472.5,69) -- (491.5,69) -- (491.5,240) -- (472.5,240) -- cycle ;
		%Shape: Rectangle [id:dp26492145011505386] 
		\draw  [fill={rgb, 255:red, 128; green, 128; blue, 128 }  ,fill opacity=1 ] (491.5,69) -- (510.5,69) -- (510.5,240) -- (491.5,240) -- cycle ;
		%Shape: Rectangle [id:dp1150414817255585] 
		\draw   (510.5,69) -- (529.5,69) -- (529.5,240) -- (510.5,240) -- cycle ;
		%Shape: Rectangle [id:dp8607298745671128] 
		\draw   (529.5,69) -- (548.5,69) -- (548.5,240) -- (529.5,240) -- cycle ;
		%Shape: Rectangle [id:dp32615298886448785] 
		\draw   (548.5,69) -- (567.5,69) -- (567.5,240) -- (548.5,240) -- cycle ;
		%Shape: Rectangle [id:dp8784460349845167] 
		\draw   (567.5,69) -- (586.5,69) -- (586.5,240) -- (567.5,240) -- cycle ;
		%Shape: Rectangle [id:dp339029299717053] 
		\draw   (586.5,69) -- (605.5,69) -- (605.5,240) -- (586.5,240) -- cycle ;
		%Shape: Rectangle [id:dp06077932211597559] 
		\draw  [fill={rgb, 255:red, 216; green, 216; blue, 216 }  ,fill opacity=1 ] (156.5,69) -- (194.5,69) -- (194.5,240) -- (156.5,240) -- cycle ;
		%Shape: Rectangle [id:dp8242036059184403] 
		\draw   (194.5,69) -- (213.5,69) -- (213.5,240) -- (194.5,240) -- cycle ;
		%Shape: Rectangle [id:dp7800156234658595] 
		\draw  [fill={rgb, 255:red, 128; green, 128; blue, 128 }  ,fill opacity=1 ] (44.5,175.5) -- (138.5,175.5) -- (138.5,346.5) -- (44.5,346.5) -- cycle ;
		%Shape: Rectangle [id:dp1989886679410673] 
		\draw  [fill={rgb, 255:red, 216; green, 216; blue, 216 }  ,fill opacity=1 ] (167.5,175.5) -- (206.5,175.5) -- (206.5,346.5) -- (167.5,346.5) -- cycle ;
		%Shape: Rectangle [id:dp9596300543770471] 
		\draw  [fill={rgb, 255:red, 128; green, 128; blue, 128 }  ,fill opacity=1 ] (256.5,175.5) -- (350.5,175.5) -- (350.5,346.5) -- (256.5,346.5) -- cycle ;
		%Shape: Rectangle [id:dp9677568127564606] 
		\draw  [fill={rgb, 255:red, 216; green, 216; blue, 216 }  ,fill opacity=1 ] (379.5,175.5) -- (418.5,175.5) -- (418.5,346.5) -- (379.5,346.5) -- cycle ;
		%Shape: Rectangle [id:dp35983750906414635] 
		\draw  [fill={rgb, 255:red, 128; green, 128; blue, 128 }  ,fill opacity=1 ] (474.5,175.5) -- (531.5,175.5) -- (531.5,346.5) -- (474.5,346.5) -- cycle ;
		%Shape: Brace [id:dp5607434369611208] 
		\draw   (395.5,63) .. controls (395.5,58.33) and (393.17,56) .. (388.5,56) -- (330.75,56) .. controls (324.08,56) and (320.75,53.67) .. (320.75,49) .. controls (320.75,53.67) and (317.42,56) .. (310.75,56)(313.75,56) -- (253,56) .. controls (248.33,56) and (246,58.33) .. (246,63) ;
		%Shape: Brace [id:dp5670434991040649] 
		\draw   (604.5,63) .. controls (604.5,58.33) and (602.17,56) .. (597.5,56) -- (539.75,56) .. controls (533.08,56) and (529.75,53.67) .. (529.75,49) .. controls (529.75,53.67) and (526.42,56) .. (519.75,56)(522.75,56) -- (462,56) .. controls (457.33,56) and (455,58.33) .. (455,63) ;
		
		% Text Node
		\draw (88,25) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 1$ whole ($\displaystyle 8/8$)};
		% Text Node
		\draw (83.5,237.4) node [anchor=north west][inner sep=0.75pt]  [font=\large]  {$\dfrac{5}{8}$};
		% Text Node
		\draw (138,246.4) node [anchor=north west][inner sep=0.75pt]  [font=\huge]  {$-$};
		% Text Node
		\draw (179,237.4) node [anchor=north west][inner sep=0.75pt]  [font=\large]  {$\dfrac{1}{4}$};
		% Text Node
		\draw (218,255) node [anchor=north west][inner sep=0.75pt]  [font=\huge]  {$=$};
		% Text Node
		\draw (295.5,237.4) node [anchor=north west][inner sep=0.75pt]  [font=\large]  {$\dfrac{5}{8}$};
		% Text Node
		\draw (351,246.4) node [anchor=north west][inner sep=0.75pt]  [font=\huge]  {$-$};
		% Text Node
		\draw (391,237.4) node [anchor=north west][inner sep=0.75pt]  [font=\large]  {$\dfrac{2}{8}$};
		% Text Node
		\draw (427.5,255) node [anchor=north west][inner sep=0.75pt]  [font=\huge]  {$=$};
		% Text Node
		\draw (497,237.4) node [anchor=north west][inner sep=0.75pt]  [font=\large]  {$\dfrac{3}{8}$};
		% Text Node
		\draw (272,25) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 1$ whole ($\displaystyle 8/8$)};
		% Text Node
		\draw (481,25) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 1$ whole ($\displaystyle 8/8$)};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Subtraction of fractions without common denominators}
	\end{figure}
	We then see above that to subtract a fraction from another we first put the fraction at the same denominator, we then subtract the numerators and we keep the denominator if there is no possible simplification:
	
	We have when we mix the addition and subtraction the following resulting relation that should be obvious for most readers:
	
	The methodology used for subtraction being based on exactly the same rules that for addition we will expand the subject more as this seems actually useless in our point of view. This method is very simple and of course requires some habits to work with  numbers to be fully understood and mastered.
	
	\subsubsection{Multiplication}
	\textbf{Definition (\#\thesection.\mydef):} The "\NewTerm{multiplication}\index{multiplication}\label{multiplication}" of numbers is an operation that has for purpose, given two numbers, one named "\NewTerm{multiplier}\index{multiplier}" $m$, and the other "\NewTerm{multiplicand}\index{multiplicand}" $M$, to find a third number named "\NewTerm{product}\index{product}" $P$ that is the sum (multiplication is only a successive number of sums!) as many equal numbers to the multiplicand as there are units multiplier:
	
	The multiplicand and multiplier are named "\NewTerm{product factors}\index{product factors}".

	The multiplication is indicated in kindergarten by the symbol "$\times $"of the elevated dot symbol in higher classes "$\cdot $" or even when there is no possible confusion... without anything:
	
	We can define the multiplication using the Peano axioms in the special case of natural numbers $\mathbb{N}$ as already mentioned in the section Numbers (page \pageref{peano axioms}). Thus, with these axioms it is possible to prove that there is (exists) one and only one (unique) application, denoted "$\times$" or more often "$\cdot$" of $\mathbb{N}^2$ to $\mathbb{N}$ satisfying:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	As this book has not be written for mathematicians, we will pass the proof (relatively long and of little interest in the case of business) and we will assume that the application "$\times $" exists and is unique ... and that it follows from the above properties.
	\end{tcolorbox}
	The "\NewTerm{power}"\index{power} is a specific notation of a special case of the multiplication. When to multiplicand(s) and the multiplier(s) are typically identical in numerical values, we denote therefore the multiplication by (for example):
	
	This is what we name the "\NewTerm{power notation}\index{power notation}" or "\NewTerm{exponentations}\index{exponentation}". The number in superscript is what we the name the "\NewTerm{power}\index{power}" or the "\NewTerm{exponant}\index{exponant}" of the number. The notation with exponants is said to be see for the first time in a book of Chuquet in 1484.
	
	In general:
	
	You can check by yourself that its properties are the following (for example):
	
	and also:
	
	Here are some obvious properties about the multiplication that we will admit without proof (this is a Set properties point of view listing):
	\begin{enumerate}
		\item[P1.] The multiplication of several numbers does not depend on the order of terms. Then we say that multiplication is a "\NewTerm{commutative operation}\index{commutative operation}".
		
		\item[P2.] The multiplication of several numbers does not change if we replace two or more of them by their intermediate result. We then say that the multiplication is an "\NewTerm{associative operation}\index{associative operation}".
		
		\item[P3.] The unit is the neutral element of the multiplication as any multiplicand multiplied by the multiplier $1$ is equal to the multiplicand itself.
		
		\item[P4.] The multiplication may have a term such that the product is equal to unity (the neutral element). Then we say that there exists a "\NewTerm{multiplicative inverse}\index{multiplicative inverse}" (but this depends strictly speaking in what set of numbers we work as in some the concept of decimal number does not exist!).
		
		\item[P5.] Multiplication is a "\NewTerm{distributive operation}\index{distributive operation}", that is to say:	
		
		the reverse being named a "\NewTerm{factorization operation}\index{factorization operation}".
		
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		Probably the best-known case in the world of factorization (by users of spreadsheets softwares like Microsoft Excel) is the calculation of the addition or removal of a tax on a given amount! For example, consider adding $8\%$ VAT to an amount of $100.-$. This will typically be written in one of the following ways:
		$$100+100\cdot8\%=100\cdot 1+100\cdot 8\%=100\cdot(1+8\%)=100\cdot(1+8/100)=100\cdot 1.08$$
		\end{tcolorbox}
		
		\item[P6.] Any number at the power of zero is equal to $1$ as:
		
		From this result some deduce too quickly (and even some professors teach it!) that $0^0=1$. But in reality $0^0$ is just undefined as it the ratio of two zeros.
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		There are two main reasons why some professors teach $0^0=1$. The first one is an application of the binomial theorem (\SeeChapter{see section Calculus page \pageref{binomial theorem}}):
		$$
		(1+x)^{n}=\sum_{k=0}^{n}\left(\begin{array}{l}
		n \\
		k
		\end{array}\right) 1^{n-k} x^{k}
		$$
		When $x=0$, the expression becomes:
		$$
		\begin{aligned}
		(1+0)^{n} &=\sum_{k=0}^{n}\left(\begin{array}{l}
		n \\
		k
		\end{array}\right) 1^{n-k} 0^{k} \\
		1^{n} &=1 \cdot 0^{0} \\
		1 &=1 \cdot 0^{0}
		\end{aligned}
		$$
		In this context, $0^{0}$ has to be equal to $1$.\\
		
		The second go with the reasoning that $1^1$ is $1$, $0.5^{0.5}$ is $0.70$ etc. $x^x$ goes down from one until it hits $0.4$ and then it goes back up and approaches the limit of one but never hits. Ie $0.000000001^{0.000000001}$ is almost $1$, so some people round $0^0$ to $1$.\\
		
		To summarize it seems that there are no international convention as far as we know.
		\end{tcolorbox}
	\end{enumerate}

	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,646); %set diagram left start at 0, and has height of 646
		
		%Curve Lines [id:da04583121758625386] 
		\draw    (153.86,199.24) .. controls (153.89,149.97) and (155.04,118.28) .. (212.86,118.28) ;
		\draw [shift={(153.86,202.28)}, rotate = 270] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da6768928442160407] 
		\draw    (153.86,257.28) -- (153.86,337.28) ;
		\draw [shift={(153.86,340.28)}, rotate = 270] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Curve Lines [id:da8677283564086484] 
		\draw    (354.86,202.24) .. controls (354.8,152.81) and (352.68,117.28) .. (294.86,117.28) ;
		\draw [shift={(354.86,205.28)}, rotate = 270] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Curve Lines [id:da64400896235815] 
		\draw    (154.03,337.22) .. controls (158.39,296.82) and (246.04,230.2) .. (291.86,227.28) ;
		\draw [shift={(153.86,340.28)}, rotate = 270] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da942634649316979] 
		\draw    (413.86,227.28) -- (460.86,227.28) ;
		\draw [shift={(463.86,227.28)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da8640810565798616] 
		\draw    (353,254.28) -- (353,303.28) ;
		\draw [shift={(353,306.28)}, rotate = 270] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da7449810574062539] 
		\draw    (353,365.28) -- (353,414.28) ;
		\draw [shift={(353,417.28)}, rotate = 270] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da022380621415581947] 
		\draw    (513.5,254.28) -- (513.5,298.28) ;
		\draw [shift={(513.5,301.28)}, rotate = 270] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da13649973872440602] 
		\draw    (513.5,374.28) -- (513.5,418.28) ;
		\draw [shift={(513.5,421.28)}, rotate = 270] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		
		% Text Node
		\draw    (213,93) -- (295,93) -- (295,141) -- (213,141) -- cycle  ;
		\draw (216,94) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{53.13pt}\setlength\topsep{0pt}
		\begin{center}
		\textbf{Definition:}\\$\displaystyle a^{m}$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw    (97,203) -- (213,203) -- (213,257) -- (97,257) -- cycle  ;
		\draw (100,203) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{85pt}\setlength\topsep{0pt}
		\begin{center}
		\textbf{Distributes}\\\textbf{Over Products:}\\$\displaystyle ( ab)^{m} =a^{m} b^{m}$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw    (94,342) -- (215,342) -- (215,424) -- (94,424) -- cycle  ;
		\draw (95,348) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{90pt}\setlength\topsep{0pt}
		\begin{center}
		\textbf{Distributes}\\\textbf{Over Quotients:}\\$\displaystyle \left(\dfrac{a}{b}\right)^{m} =\dfrac{a^{m}}{b^{m}}$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw    (292,206) -- (414,206) -- (414,254) -- (292,254) -- cycle  ;
		\draw (297,210) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{80.12pt}\setlength\topsep{0pt}
		\begin{center}
		\textbf{Products:}\\$\displaystyle a^{m} \cdot a^{n} =a^{m+n} $
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw    (464,206) -- (563,206) -- (563,254) -- (464,254) -- cycle  ;
		\draw (467,210) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{70pt}\setlength\topsep{0pt}
		\begin{center}
		\textbf{Zero:}\\$\displaystyle a^{0} =1,a\neq 0$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw    (279,307) -- (427,307) -- (427,365) -- (279,365) -- cycle  ;
		\draw (286,315) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{98.05pt}\setlength\topsep{0pt}
		\begin{center}
		\textbf{Powers:}\\$\displaystyle \left( a^{m}\right)^{n} \cdot \left( a^{n}\right)^{m} =a^{mn}$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw    (270,418) -- (436,418) -- (436,479) -- (270,479) -- cycle  ;
		\draw (273,425) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{120pt}\setlength\topsep{0pt}
		\begin{center}
		\textbf{Rational:}\\$\displaystyle a^{m/n} =\sqrt[n]{a^{m}} =\left(\sqrt[n]{a}\right)^{m}$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw    (471.5,302) -- (555.5,302) -- (555.5,375) -- (471.5,375) -- cycle  ;
		\draw (474.5,306) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{54.29pt}\setlength\topsep{0pt}
		\begin{center}
		\textbf{Negatives:}\\$\displaystyle a^{-m} =\dfrac{1}{a^{m}}$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw    (472,422) -- (556,422) -- (556,496) -- (472,496) -- cycle  ;
		\draw (470,430) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{65pt}\setlength\topsep{0pt}
		\begin{center}
		\textbf{Quotients:}\\$\displaystyle \dfrac{a^{m}}{a^{n}} =a^{m-n}$
		\end{center}
		
		\end{minipage}};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Summary of exponents rules}
	\end{figure}	
	
	Let us also introduce some special notations for the multiplication:
	\begin{enumerate}
		\item Given any numbers $x_1,x_2,\ldots,x_n$ (not necessarily equal) then we can write the product as following:
		
		by defining upper and lower bounds to the indexed product (above and below the uppercase Greek letter "Pi").
		
		We trivially have respectively to the latter notation (on request we can detail more...):
		
		for any number $k$ such that:
		
		We also have for example:
		
		
		\item We define the "\NewTerm{factorial}\index{factorial}" simply ("simply"... because it exists also a more complex way of defining it through the Euler Gamma function as it is done in the section of Integral and Differential Calculus page \pageref{gamma euler function}) by:
		
		with the special fact that (only the complex definition mentioned before can make this fact obvious... so for the proof see again page \pageref{factorial of zero}):
		
	\end{enumerate}
	Let us see some simple examples of basic multiplications:
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. The multiplication of two relatively small numbers is fairly easy once we have memorized count to at least the number resulting from this operation. So:
	
	E2. For much larger numbers we must adopt another method that has to be memorized. \\
	
	For example:
	
	This methodology is very logical if you understand how we build a number in base ten. Thus we have (we'll assume that the distributive property is mastered):
	
	To avoid overloading the notations in the multiplication by the "vertical" method, we do not represent the zeros that would overload unnecessarily the calculations (and even more if the multiplier and / or the multiplicand are very large numbers).
	\end{tcolorbox}
	Following a reader request, here is also an illustrated example on how we teach (on possible way among others) to multiply simple fractions together at the middle school level:
	 \begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1467); %set diagram left start at 0, and has height of 1467
		
		%Shape: Rectangle [id:dp8385596048419905] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (414.5,117) -- (540,117) -- (540,291) -- (414.5,291) -- cycle ;
		%Straight Lines [id:da8278444832950962] 
		\draw [line width=1.5]    (477,72) -- (477,290) ;
		%Shape: Rectangle [id:dp2183522221162273] 
		\draw  [line width=1.5]  (414,72) -- (604.5,72) -- (604.5,290) -- (414,290) -- cycle ;
		%Straight Lines [id:da24867202195798033] 
		\draw [line width=1.5]    (540,73) -- (540,291) ;
		%Straight Lines [id:da14259068738252134] 
		\draw [line width=1.5]    (414.5,117) -- (605.5,117) ;
		%Straight Lines [id:da17279568674918555] 
		\draw [line width=1.5]    (413.5,160) -- (604.5,160) ;
		%Straight Lines [id:da03807231310058046] 
		\draw [line width=1.5]    (413.5,203) -- (604.5,203) ;
		%Straight Lines [id:da19130392511547556] 
		\draw [line width=1.5]    (414.5,246) -- (603.5,246) ;
		%Straight Lines [id:da11846338712312954] 
		\draw    (145,82) -- (315.5,154) (181.17,94.02) -- (178.84,99.55)(216.18,108.8) -- (213.85,114.33)(251.19,123.58) -- (248.85,129.11)(286.19,138.37) -- (283.86,143.9) ;
		\draw [shift={(315.5,154)}, rotate = 202.89] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (0,5.59) -- (0,-5.59)   ;
		\draw [shift={(145,82)}, rotate = 202.89] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (0,5.59) -- (0,-5.59)   ;
		%Straight Lines [id:da04538370060707542] 
		\draw    (69,104) -- (204.5,248) (117.1,150.74) -- (112.73,154.85)(163.01,199.53) -- (158.64,203.64) ;
		\draw [shift={(204.5,248)}, rotate = 226.74] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (0,5.59) -- (0,-5.59)   ;
		\draw [shift={(69,104)}, rotate = 226.74] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (0,5.59) -- (0,-5.59)   ;
		%Straight Lines [id:da24665092312924797] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=2.25]    (145,82) -- (286.5,142) ;
		%Straight Lines [id:da39691819424634023] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][line width=2.25]    (69,104) -- (160.5,202) ;
		
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (454,176) -- (497,176) -- (497,223) -- (454,223) -- cycle  ;
		\draw (457,180.4) node [anchor=north west][inner sep=0.75pt]    {$\dfrac{4}{5} \cdot \dfrac{2}{3}$};
		% Text Node
		\draw (390,185.4) node [anchor=north west][inner sep=0.75pt]    {$\dfrac{4}{5}$};
		% Text Node
		\draw (472,297.4) node [anchor=north west][inner sep=0.75pt]    {$\dfrac{2}{3}$};
		% Text Node
		\draw (212,122.4) node [anchor=north west][inner sep=0.75pt]    {$\dfrac{4}{5}$};
		% Text Node
		\draw (91,153.4) node [anchor=north west][inner sep=0.75pt]    {$\dfrac{2}{3}$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Multiplication of fractions without common denominators}
	\end{figure}
	We have therefore:
	
	This example illustrates that to multiply two fractions we simply multiply the numerators together and the denominators together.
	
	\subsubsection{Division}\label{division}
	\textbf{Definition (\#\thesection.\mydef):} The "\NewTerm{division}\index{division}\label{division}" of integers (to start with the simplest case ...) is an operation, which aims, given two integers, one named "\NewTerm{dividend}\index{dividend}" $D$, the other named "\NewTerm{divider}\index{divider}" $d$ , to find a third number named "\NewTerm{quotient}\index{quotient}" $Q$ which is the largest number whose product by the divisor can be subtracted (so the division result of the subtraction!) the dividend (the difference being named the "\NewTerm{rest}\index{rest}" $R$ or sometimes the "\NewTerm{congruence}\index{congruence}").
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In the case of real numbers there are never any rest at the end of the division operation (because the quotient multiplied by the divisor gives always exactly the dividend)!
	\end{tcolorbox}
	Generally in the context of integers (or algebraic equation division), if we denote by $D$ the dividend and by $d$ the divisor, the quotient $Q$ and the remainder $R$ we have the relation:
	
	knowing that the division was initially written as follows:
	
	We indicate the operation of division by placing between the two numbers, the dividend and the divider, a symbol "$:$" or a slash "$/$" or even in kindergarten with the symbol $\div$.
	
	We refer also often by the term "\NewTerm{fraction}\index{fraction}" (instead of "quotient"), the ratio of two numbers or in other words, the division of the first by the second.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The sign of division "$:$" is said to be due to Gottfried Wilhelm Leibniz. The slash symbol could have been see for the first time in the works of Leonardo Fibonacci (11202) and is probably due to the Hindus.
	\end{tcolorbox}
	If we divide two numbers and we want an integer as quotient and as remainder (if there is one...), then we speak of "\NewTerm{Euclidiean division}\index{Euclidean division}".
	
	For example, dividing a cake, is not a Euclidean division because the quotient is not an integer, except if one takes the four quarters...:
	\begin{figure}[H]
		\centering
		\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt
		
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,437); %set diagram left start at 0, and has height of 437
		
		%Shape: Pie [id:dp6544669529477478] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (485.38,110.45) .. controls (485.49,110.45) and (485.59,110.45) .. (485.7,110.45) .. controls (517.57,110.45) and (543.4,135.95) .. (543.4,167.4) .. controls (543.4,198.85) and (517.57,224.35) .. (485.7,224.35) .. controls (453.83,224.35) and (428,198.85) .. (428,167.4) .. controls (428,167.23) and (428,167.06) .. (428,166.89) -- (485.7,167.4) -- cycle ;
		%Shape: Circle [id:dp7983481014960396] 
		\draw   (102,167.4) .. controls (102,135.7) and (127.7,110) .. (159.4,110) .. controls (191.1,110) and (216.8,135.7) .. (216.8,167.4) .. controls (216.8,199.1) and (191.1,224.8) .. (159.4,224.8) .. controls (127.7,224.8) and (102,199.1) .. (102,167.4) -- cycle ;
		%Shape: Circle [id:dp31810694748535373] 
		\draw   (269,167.4) .. controls (269,135.7) and (294.7,110) .. (326.4,110) .. controls (358.1,110) and (383.8,135.7) .. (383.8,167.4) .. controls (383.8,199.1) and (358.1,224.8) .. (326.4,224.8) .. controls (294.7,224.8) and (269,199.1) .. (269,167.4) -- cycle ;
		%Straight Lines [id:da9375924831860807] 
		\draw    (326.4,110) -- (326.4,224.8) ;
		%Straight Lines [id:da3997916944300044] 
		\draw    (269,167.4) -- (383.8,167.4) ;
		%Shape: Circle [id:dp3114265774787126] 
		\draw   (428,167.4) .. controls (428,135.7) and (453.7,110) .. (485.4,110) .. controls (517.1,110) and (542.8,135.7) .. (542.8,167.4) .. controls (542.8,199.1) and (517.1,224.8) .. (485.4,224.8) .. controls (453.7,224.8) and (428,199.1) .. (428,167.4) -- cycle ;
		%Straight Lines [id:da10901230430385156] 
		\draw    (485.4,110) -- (485.4,224.8) ;
		%Straight Lines [id:da1260690037592609] 
		\draw    (428,167.4) -- (542.8,167.4) ;
		
		% Text Node
		\draw (111,233) node [anchor=north west][inner sep=0.75pt]   [align=left] {an entire cake};
		% Text Node
		\draw (250,228.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{105.55pt}\setlength\topsep{0pt}
		\begin{center}
		a cake divided into... \\four quarters
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (417,228.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{97.41pt}\setlength\topsep{0pt}
		\begin{center}
		three quarters of the \\cake in gray
		\end{center}
		\end{minipage}};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Schematic example of a division (fractions)}
	\end{figure}
	If we have:
	
	we denote by $i_D$ the inverse of the dividend. At any number is associated an inverse that satisfies this condition.
	From this definition it comes the notation (with $x$ being any number other than zero)
	
	In the case of two fractional numbers, we say they are "\NewTerm{inverse}\index{inverse}" or "\NewTerm{reciprocal}\index{reciprocal}", when their product is equal to unity (as the previous relations).
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} A division by zero is what we name a "\NewTerm{singularity}\index{singularity}". That is to say the result of the division is: undetermined!!\\
	
	\textbf{R2.} When we multiply the dividend and the divisor of a division (fraction) by a same number, the quotient does not change (this is an: "\NewTerm{equivalent fraction}\index{equivalent fraction}"), but the remainder is multiplied by that number.\\
	
	\textbf{R3.} Divide a number by a product made of several factors is equivalent to divide this number successively by each of the factors of the product and vice versa.\\
	
	\textbf{R4.} Fractions that are greater than $0$ but less than $1$ are named "\NewTerm{proper fractions}\index{proper fractions}". In proper fractions, the numerator is less than the denominator. When a fraction has a numerator that is greater than or equal to the denominator, the fraction is an "\NewTerm{improper fraction}\index{improper fraction}". An improper fraction is always $1$ or greater than $1$. And, finally, a mixed number is a combination of a whole number and a proper fraction.
	\end{tcolorbox}
	The properties of the divisions with the condensed power notations (exponentiation) are typically as example (we will leave to the reader the fact to check this up to with numerical values):
	
	or obviously another example:
	
	We therefore deduce that:
	
	Let us recall that a prime number (relative integer $\mathbb{Z}$) is a number greater than $1$ that has for divisors only itself and unity (remember that $2$ is prime for example). Therefore any number that is not prime has at least one prime number as a divisor (except $1$ by definition!). The smallest divisors of an integer is a prime number (we will detail the properties of prime numbers relatively to the operation of division in the section Numbers Theory page \pageref{prime number}).
	
	Let us see some properties of the division (some of us are already known because they arise from logical reasoning of the multiplication properties):
	
	where: 
	
	is what we name a "\NewTerm{terms amplification}\index{terms amplification}" and: 
	
	is an operation consisting by putting everything with a "\NewTerm{common denominator}\index{common denominator}".
	
	We also have the following properties:
	\begin{enumerate}
		\item[P1.] The division of several numbers depend on the order of terms. We then say that the division is a "\NewTerm{non-commutative operation}". This means we have when $a$ that is different from $b$ and that both are different from zero:
		
		
		\item[P2.] The result of the division of several numbers change if we replace two or more of them by their intermediate result. We then say that the division is a "\NewTerm{non-associative operation}":
		
		
		\item[P3.] The unit is the neutral element has that we multiply the dividend or the divider by $1$ the result of the division remains the same:
		
		
		\item[P4.] The division may include a divider in such a way that the division is equal to unity (neutral element $1$). We then say that there exist a "\NewTerm{symmetrical to the division}\index{division symmetrical}" that is obviously equal to the numerator (dividend) itself.
		
		\item[P5.] The incrementation of numerator and denominator by a constant value is not equal to the initial ratio in the general case where $a\neq b$:
		
	\end{enumerate}
	Now that we know the multiplication (and therefore power notation) and division, if we consider $a$ and $b$ are two positive real numbers, different from zero we have:
	
	and (named sometimes the "\NewTerm{zero exponent rule of exponents}\index{zero exponent rule of exponents}"):
	
	and:
	
	We have also obviously:
	
	Therefore (not directly related to the subject of division... but shouldn't we introduce this now...?):
	
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	When dealing with division, be extremely careful abound dividing stuff by zero. This may lead to some absurd results, like the non-less famous one shared a lot on social networks:
	\begin{equation*}
		\begin{aligned}
		a &=b \\
		a^{2} &=a b \\
		a^{2}-b^{2} &=a b-b^{2} \\
		(a+b)(a-b) &=b(a-b) \\
		a+b &=b \\
		2 b &=b \\
		2 &=1
		\end{aligned}
	\end{equation*}
	\end{tcolorbox}
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/arithmetics/division_joke.jpg}
	\end{figure}
	
	\pagebreak
	\subsubsection{$n$-root}
	Now that we have introduce in a simple and not too much formal way the operations of multiplication (and power notation) and division we can introduction the concept of $n$-root.
	
	As we know for example that:
	\begin{gather*}
		2^32^2=2^{3+2}
	\end{gather*}
	we can by reverse inference for example also write:
	
	and therefore it means that fractional power exist! This is what we name $n$-root (in the above example we speak of $2$-root).
	
	We can now define the principal $n$-root of any number!
	
	\textbf{Definition (\#\thesection.\mydef):} In mathematics, the $n$-th root of a number $a$, where $n$ is a positive integer, is a number $r$ which, when raised to the power $n$ yields $x$. That is to say such that: $r^n=x$, where $n$ is the degree of the root. By convention we write:
	
	Roots are usually written using the "\NewTerm{radical}\index{radical}" symbol $\sqrt[n]{\ldots}$ or also named the "\NewTerm{radix}\index{radix}". The number $n\in\mathbb{N}^*$ is named the "\NewTerm{radicand}\index{radicand}" and sometimes the "\NewTerm{index}\index{index}".
	
	From what has been said for the powers, we can easily conclude that the $n$-th root of a product of several factors is the product of $n$-th roots of each factor:
	
	as (seen previously):
	
	And therefore:
	
	Obviously it comes:
	
	We also have if $a<0$:
	
	if $n\in \mathbb{N}^{*}$ is odd and:
	
	if $n\in \mathbb{N}^{*}$ is even.

	If $x<0$ and $n\in \mathbb{N}^{*}$ is odd then:
	
	is the number $y$ such that:
	
	If $n\in \mathbb{N}^{*}$ is even then obviously, as we already have seen it earlier, the root belong to $\mathbb{C}$ (\SeeChapter{see section Numbers page \pageref{unit pure imaginary number}}).
	
	If the denominator of a fraction contains a factor of the form $\sqrt[n]{a^k}$ with $a\neq 0$, by multiplying the numerator and denominator by $\sqrt[n]{a^{n-}}$, we will remove the root of the denominator, since:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us see a world famous example of the application of the root about the origin of the ISO paper formats: A6, A5, A4, A3, A2, A1, A0, etc.\\
	
	These format of papers have in fact the property (there is a goal at the origin!) to keep the proportions when we bend or cut the sheet in half in its largest dimension. Thus, if we denote by $L$ the length and $W$ the width of the sheet, we have:
	
	Hence we have:
	
	As the A0 format by definition has an area of $1\;[\text{m}^2]$. For this format we have then:
	
	Therefore we deduce that:
	
	and therefore:
	
	from whence we derive:
	
	\end{tcolorbox}
	
	Obviously the reader at this point may ask himself: \textit{What types of algorithms do computers/calculators typically use to compute fractional exponents?}
	
	The basic idea is to use logarithms (\SeeChapter{see section Analysis page \pageref{logarithms}}). If $y=x^p$, then:
	
	so:
	
	The actual value of $\log (x)$ and $e^{p\log (x)}$ are then calculated using like a Taylor polynomial (\SeeChapter{see section Sequences and Series page \pageref{Taylor polynomial}}).
	
	\pagebreak
	\subsection{Arithmetic Polynomials}
	\textbf{Definition (\#\thesection.\mydef):} An "\NewTerm{arithmetic polynomial}\index{arithmetic polynomial}" (not to be confused with "algebraic polynomial" that will be studied later in the section Calculus page \pageref{polynomial}) is a set of numbers separated from each other by the operators of addition or subtraction ($+$ or $-$) including therefore the multiplication...
	
	The components enclosed in the polynomial are known as "\NewTerm{terms}" of the polynomial. When the polynomial contains a single term, then we speak of "\NewTerm{monomial}\index{monomial}", if there are two terms we speak of "\NewTerm{binomial}\index{binomial}", and so on...
	
	\begin{theorem}
	The value of an arithmetic polynomial is equal to the excess of the sum of the terms preceded by the $+$ sign on the sum of the terms preceded by the sign $-$.
	\end{theorem}
	\begin{dem}
	
	whatever the values of the terms.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Highlight the negative unit $-1$ is what we name, as we already know, a "\NewTerm{factorization}". The reverse operation is named as we also already know a "\NewTerm{distribution}" or "\NewTerm{development}".
	
	The product of several polynomials can always be replaced by a single polynomial that we name the... "\NewTerm{resulting product}\index{resulting product}". We usually operate as follows: we multiply successively all the terms of the first polynomial, starting from the left, with the first, the second, ..., the last by the second polynomial. We obtain a first partial product. We do, if necessary, a reduction (simplification) of similar terms. We then multiply each of the terms of the partial product successively by the first, the second, ..., the last term of the third polynomial starting from the left and so on.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	
	\end{tcolorbox}
	The product of the polynomials $P_1$, $P_2$, $P_3, \ldots, P_k, \ldots$ is the sum of all products of $n_i$ factors formed with a term of $P_i$, of a term of $P_2$, ..., and a term of $P_k$ and so. if there is no reduction, the number of terms is equal to the product of the numbers of terms of each polynomial such that the final number of therms is equals to:
	

	\subsection{Absolute Value}\label{absolute value}
	\textbf{Definition (\#\thesection.\mydef):} In mathematics, the "\NewTerm{absolute value}\index{absolute value}" $|x|$ of a real number $x$ is the non-negative value of $x$ without regard to its sign. Namely, $|x| = x$ for a positive $x$, $|x| = -x$ for a negative $x$ (in which case $-x$ is positive), and $|0| = 0$. For example, the absolute value of $3$ is $3$, and the absolute value of $-3$ is also $3$. The absolute value of a number may be thought of as its distance from zero.
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} The term absolute value has been used in this sense from at least 11806 (holocene calendar). The notation $|x|$, with a vertical bar on each side, was introduced by Karl Weierstrass in 11841 (holocene calendar).\\
	
	\textbf{R2.} For plots about the absolute value, the reader is referred to the Functional Analysis section of this book (page \pageref{absolute value plot}).
	\end{tcolorbox}
	For any real number $x$, the "absolute value" $x$, is formally given by:	
	
	At the origin the absolute value was defined as:
	
	We notice that also the following possible notation:
	
	And the equivalent expressions:
	
	and also:
	
	the latter being often used in the context of solving inequalities.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Solving an inequality such that: 
	
	is then solved simply by using the intuitive concept of distance. The solution is the set of real numbers whose distance from the real number $3$ is less than or equal to $9$. This is the range of center $3$ and radius $9$ or formally:
	
	\end{tcolorbox}
	
	Let us indicate that it is also useful to interpret the term: 
	
	as the (euclidean!) distance between the two numbers $x$ and $y$ on the real line. Thus, by providing the set of real numbers of the absolute value distance , it becomes a metric space (see the section of Topology page \pageref{distance} to have a robust introduction to what is a distance)!!!
	
	The absolute value has some trivial properties that we will give without proof (excepted on reader request) as they seem to us quite intuitive:
	\begin{enumerate}
		\item[P1.] Non-negativity:
			
		
		\item[P2.] Positive-definiteness:
			
			
		\item[P3.] Multiplicativeness:
			
			The proof is straightforward:
			

		\item[P4.] Subadditivity ("first" triangle inequality):
			
		This can be proven in one shot\label{absolute value triangle inequality}:
		
	\end{enumerate}
	
	Other important properties of the absolute value include:
	\begin{enumerate}
		\item[P5.] 	Idempotence (the absolute value of the absolute value is the absolute value):
			
		
		\item[P6.] Evenness (reflection symmetry of the graph):
			
			The proof is straightforward:
			
			
		\item[P7.] Preservation of division (equivalent to multiplicativeness) if $y\neq 0$:
			
			The proof is straightforward:
			

		\item[P8.] Reverse ("second") triangle inequality\label{reverse triangle inequality} (equivalent to subadditivity):
			
			For all $x, y \in \mathbb{R}$, the triangle inequality gives:
			
			Interchaning $x \leftrightarrow y$ gives:
			
			which when rearranged gives:
			
			Now combining:
			
			gives:
			
			Hence the desired result:
			
	\end{enumerate}
	The absolute function is strongly related to some other functions that we will use quite some times in different section of this book. A well known an important example is the "\NewTerm{signum function}\index{signum function}\label{signum function}" or "\NewTerm{sign function}\index{sign function}" that is an odd mathematical function that extracts the sign of a real number and defined by:
	
	Alternatively (the signum function is the derivative of the absolute value function, up to the indeterminacy at zero):
	
	Any real number can be expressed as the product of its absolute value and its sign function:
	

	\pagebreak
	\subsection{Calculation Rules (operators priorities)}
	Frequently in computing (in development in particular), we speak of "\NewTerm{operators precedence}\index{operators precedence}". In mathematics we speak of "\NewTerm{priority of the sets of operations and rules of signs}\index{rules of signs}". What is this exactly?
	
	We have already seen what are the properties of addition, subtraction, multiplication, division and power. We therefore insist that the reader distinguishes the concept of "property" of this of "priority" (that we will immediately see) which are (obviously) two completely different things!
	
	In mathematics, in particular, we first define the priorities of the symbols $\left\lbrace\left[\left(\right)\right]\right\rbrace$:
	\begin{enumerate}
		\item Operations that are in parenthesis $()$ should be performed first in the polynomial.
	
		\item Operations that are in brackets $[\,]$ should be made afterwards from the results of operations that were in parenthesis $()$.
	
		\item Finally, from the intermediate results of operations that were in parenthesis $()$ and brackets $[\,]$, we calculate the operations that are between the braces $\left\lbrace \right\rbrace$.
	\end{enumerate}	

	Let us do an example, this will be more telling.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Consider the calculation of the following arithmetic polynomial:
	
	According to the rules we defined earlier, we first calculate all the elements that are in parenthesis $()$, that is to say:
	
	Which give us:
	
	Always according to the rules we defined earlier, now we calculate all the elements between brackets by always starting to calculate the terms that are in brackets $[\,]$ at the lowest level of the other brackets $[\,]$. Thus, we first calculate the expression $[4+14\cdot 2]$  that is in the top-level bracket: $[5\cdot 10+3\cdot \ldots]$.\\
	
	This gives us $[4+14\cdot 2]=32$ and therefore:
	
	It remains to us to calculate now $[5\cdot 10+3\cdot 32]=146$ and therefore:
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	We now calculate the single term in braces, which gives us:
	
	Finally it remains:
	
	Obviously this is a special case ... But the idea remains the same in general.
	\end{tcolorbox}
	The priority of arithmetic operators is a problem mainly related to computer languages (as we have already mentioned) because we can only write mathematical relation on a single line and this is many times as source of confusion for people not having technical skill.
	
	Therefore in computing science, the following expression:
	
	will be written (pretty much on most computer languages):
	
	A non initiated could read this in many ways:
	
	what you will agree, is very dangerous because we will arrive at different results each time (special cases aside...)!
	
	Thus it has logically be defined an order of prioritization of operators such that the operations are carried out in the following order:
	\begin{enumerate}
		\item $-$ Negation
		
		\item $\string^$ Power (exponentiation)

		\item $*$ Multiplication and $/$ division\footnote{The \textit{Physical Review} journals, for example, state that in the submission instruction that multiplication is of higher precedence than division!}

		\item $\backslash$ Integer division (specific to computer science)
		
		\item $\mathrm{mod}$ Modulus (\SeeChapter{see section Number Theory page \pageref{congruence}})
		
		\item $+,-$ Addition and subtraction
	\end{enumerate}
	Obviously the rules of parentheses $()$, brackets $[\,]$, and braces $\left\lbrace \right\rbrace$ that were defined in mathematics apply also to computing.
	
	Thus we get in the order (we replace every transaction made with a symbol):
	
	First the terms in parentheses:
	
	Afterwards we can apply the priority rules of the operators in the order defined previously:
	\begin{enumerate}
		\item First the negation (rule 1):
		
		
		\item The power (rule 2):
		
		
		\item We apply the multiplication (rule 3):
		
		
		\item And we apply division (rule 3 again):
		
		The rules (4) and (5) does not apply to this particular example.
		\item And Finally (rule 6):
		
	\end{enumerate}
	Thus, following these rules, neither a computer nor a human can (should) be wrong in interpreting an equation written on a single line to avoid such issues (hence the important to have ISO standards in all field of the industry and administration):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/arithmetics/operators_priorities_calculators.jpg}
	\end{figure}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Mnemonics are often used to help students in highs-school to remember the most basic rules, but the rules taught by the use of acronyms can be misleading. In the United States, the acronym "\NewTerm{PEMDAS}\index{PEMDAS}" is common. It stands for Parentheses, Exponents, Multiplication, Division, Addition, Subtraction. PEMDAS is often expanded to the mnemonic "Please Excuse My Dear Aunt Sally". Canada and New Zealand use BEDMAS, standing for Brackets, Exponents, Division, Multiplication, Addition, Subtraction. Most common in the UK, India and Australia are BODMAS meaning "B"rackets, "O"f or "O"rder, "D"ivision, "M"ultiplication, "A"ddition and "S"ubtraction. Nigeria and some other West African countries use BIDMAS.
	\end{tcolorbox}
	
	
	In computer code, however, there are several operators that we do not always find in pure mathematics and which order property frequently change depending from a computer language to another. We will not dwell too much on that stuff as it is almost without end, however, we have below a small description:
	\begin{itemize}
		\item The concatenation operator "\&" is evaluated before comparisons operators.
		
		\item Comparison operators ($=, <,>, \ldots$) all have equal priority.
	\end{itemize}
	However, the leftmost operator in an expression, hold a higher priority.
	
	The logical operators are evaluated in the following order of priority in most computing languages:
	\begin{multicols}{2}
		\begin{enumerate}
			\item Not ($\neg$)
			\item And ($\wedge$)
			\item Or ($\vee$)
			\item Xor ($\oplus$)
			\item Eqv ($\Leftrightarrow$)
			\item Imp ($\Rightarrow$)
		\end{enumerate}
	\end{multicols}
	Now that we have seen the operator priorities, what are the rules about signs applicable in mathematics and computing science?

	First, you must know that these latter rules only apply in the case of multiplication and division. Given two positive numbers $(+x),(+y)$. We have:
	
	In other words, the multiplication of two positive numbers is a positive number and this can be generalized to the multiplication of $n$ positive numbers.
	
	We have:
	
	In other words, the multiplication of a positive number to a negative number is negative. Which can be generalized: to a positive result of a multiplication if there is an even number of negative numbers, and a negative result if there is an odd number of negative numbers on all $n$ numbers included in the multiplication.

	We have:
	
	In other words, multiplying two negative numbers is positive. What can be generalized: to a positive result of the multiplication if there is an even number of negative numbers and a negative result if there is an odd number of negative numbers.

	About divisions, the reasoning is the same:
	
	In other words, if the numerator and denominator are positive, then the result of the division will be positive.

	We have:
	
	In other words, if either the numerator or denominator is negative, then the result of the division will be necessarily negative.
	
	We have:
	
	In other words, if the numerator and denominator are positive, then the result of the division, will necessarily be positive.
	
	Obviously if we have a subtraction of terms, it is possible to rewrite it in the form:
	 
	
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{80} & \pbox{20cm}{\score{4}{5} \\ {\tiny 11 votes, 76.36\%}} 
	\end{tabular} 
	\end{flushright}
	
	
	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Number Theory}
	\lettrine[lines=4]{\color{BrickRed}T}{raditionally}, number theory is a branch of mathematics that deals with properties of integers, whether natural or whole integers. More generally, the field of study of this theory concerns a broad class of problems that naturally come from the study of integers. Number theory can be divided into several branches of study (algebraic number theory, computational number theory, etc.) depending on the methods used and the issues addressed.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The term "arithmetic" was also used to refer to number theory but it is a fairly old term, which is not as popular as in the past.
	\end{tcolorbox}
	We chose to introduce in this section only the subjects that are essential to the study of mathematics and theoretical physics of this book as well as those to be absolutely part of the general culture of the engineer (some results have application in Biostatistics!).
		
	\subsection{Principle of good order}
	We will take for granted the principle that says that every non-empty set $S \subset \mathbb{N}$ contains a smaller element.
	
	We can use this theorem to prove an important property of numbers named " \NewTerm{Archimedean property}\index{Archimedean property}" or "\NewTerm{Archimedes' axiom}\index{Archimedes' axiom}" which states:
	
	For $\forall a,b \in \mathbb{N}$ where $a$ is non-zero, there is at least one positive integer $n$ such that:
	
	In other words, for two unequal values, there is always an integer multiple of the smallest, bigger than the larger one. We name "\NewTerm{Archimedean}\index{Archimedean integer}", structures whose elements satisfy a comparison property (\SeeChapter{see section Set Theory page \pageref{comparison relations}}).
	
	While this is trivial to understand in the case of integers let us prove it because it allows us to see the type of approaches used by mathematicians when they must prove trivial items like this...
	
	\begin{dem}
	Let us suppose the opposite by saying that for $\forall n \in \mathbb{N}$ we have:
	
	If we can prove that it is absurd for any $n$ then we will have prove the Archimedean property (and also if $a, b$ are real).
	
	Let us consider then the set:
	
	Using the principle of good order, we deduce that there exist $s_0 \in S$ such as $s_0 \leq s$ for all $s \in S$. Let us write that this smaller element is:
	
	and therefore we also have:
	
	As by hypothesis $na\leq b$ then we must have:
	
	and if we reorganize and simplify:
	
	and that we simplify the negative sign we had to get...:
	
	 an obvious contradiction!
	
	This contradiction leads that the initial assumption as $na < b$ for all $n$ then is false and therefore the Archimedean property is proved by the absurd.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\subsection{Induction Principle}
	Let $S$ be a set of natural numbers that has the following two properties:
	\begin{enumerate}
		\item[P1.] $1\in S$

		\item[P2.] If $k\in S$, then $k+1\in S$
	\end{enumerate}
	then:
	
	We are build like this the set of natural numbers (refer to the section Set Theory page \pageref{zermelo fraenkel axiomatic} to see the rigorous construction of the set of natural number $\mathbb{N}$ with the Zermelo-Fraenkel axioms).
	
	\begin{theorem}
	Given now:
	
	the symbol "$\setminus$" meaning for recall "excluding". We want to prove that:
	
	\end{theorem}
	Again, even if it is trivial to understand, let us do the proof because it allows us to see the type of approaches used by mathematicians when they must prove trivial stuff like this...
	\begin{dem}
	Let us suppose the opposite, that is to say:
	
	By the principle of good order, since $B\subset \mathbb{N}$, $B$ must have a smallest element which we will denote by $b_0$.

	But since $1\in S$ by the property (P1), we have that $b_0>1$ and of course also that $1\in B$, that is to say also $b_0-1\in S$. By using the property (P2), we finally have that $b_0\in S$, that is to say that $b_0\not\in B$, therefore we get a contradiction.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	We want to show thanks to the induction principle, that the sum of the first $n$ square equals $n(n+1)(2n+1)/6$, that is to say for $n\geq 1$, we would have to (\SeeChapter{see section Sequences and Series page \pageref{sum of squares integers}}):
	
	First the above relation is easily verified for $n=1$ we will show that $n=k+1$ also verifies that relation. Under the induction hypothesis:
	
	although we fall back on the assumption of the validity of the first relation but with $n=k+1$, hence the result.
	\end{tcolorbox}
	This proved process is therefore of great importance in the study of arithmetic. Often observation and induction have led to a suspicion of laws it would have been more difficult to find by a priori. We realize the accuracy of formulas by the previous method that gave birth to modern algebra by Fermat and Pascal studies on the Pascal's triangle (\SeeChapter{see section Calculus page \pageref{Pascal's triangle}}).
	
	\pagebreak
	\subsection{Divisibility}
	\textbf{Definition (\#\thesection.\mydef):} Given $A,B\in \mathbb{Z}$ with $A\neq 0$. We say that "\NewTerm{$A$ divides $B$ (without rest)}" if there is an integer $q$ (the quotient) such that:
	
	in which case we write to differentiate of the classic division:
	
	Otherwise, we write (if $A$ doesn't divide $B$ without rest):
	
	and we say that "\NewTerm{$A$ does not divide $B$}".
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} Remember that $|$ is a relation when the symbol $/$ is an operation!\\
	
	\textbf{R2.} Do not confuse the expression "$A$ divides $B$" which means that rest is necessarily zero and the expression "$A$ is the divisor of the division by $B$" indicating that the rest is not necessarily zero!
	\end{tcolorbox}
	Moreover, if $A|B$, we also say that "\NewTerm{$B$ can be divided by $A$}" or "\NewTerm{$B$ is a multiple of $A$}".
	
	In case where $A|B$ and that $1\geq A <B$, we will say that $A$ is a "\NewTerm{proper divisor}\index{proper divisor}" of $B$.
	
	Moreover, it is clear that $A|0$ regardless of $A\in \mathbb{Z}\setminus \{0\}$ otherwise what we have a singularity.

	Here are some basic theorems relating to the division:
	\begin{theorem}
	If $A|B$, then $A|BC$ whatever $C\in \mathbb{Z}$. Or more formally:
	
	\end{theorem}
	\begin{dem}
	If $A|B$, the it exists an integer $q$ such that:
	
	Then:
	
	and therefore:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{theorem}
	If $A|B$ and $B|C$, then $A|C$ or more formally:
	
	\end{theorem}
	\begin{dem}
	If $A|B$ and $B|C$ then, there exists two integers $q$ and $r$ such that $B=Aq$ and $C=Br$. More formally:
	
	Therefore:
	
	and hence:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{theorem}
	If $A|B$ and $A|C$ then:
	
	\end{theorem}
	\begin{dem}
	If $A|B$ and $A|C$ then, there exists two integers $q$ and $r$ such that $B=Aq$ and $C=Ar$. It follows:
	
	and therefore:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{theorem}
	If $A|B$ and $B|A$ then:
	
	\end{theorem}
	\begin{dem}
	If $A|B$ and $B|A$ then, there exists two integers $q$ and $r$ such that $B=Aq$ and $A=Br$.

	We then have:
	 
	and thus $qr=1$. This is why we can have $q=\pm 1$ if $r=\pm 1$ and thus:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{theorem}
	If $A|B$ and $B\neq 0$ then:
	
	\end{theorem}
	\begin{dem}
	If $A|B$ then there exist an integer $q\neq 0$ such that $B=Aq$. But then:
	
	as $|q|\geq 1$.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\subsubsection{Euclidean Division}\label{euclidean division}
	The Euclidean division is an operation that, to two integers named respectively the "\NewTerm{dividend}\index{dividend}" and "\NewTerm{divisor}\index{divisor}" combines two other integers named the "\NewTerm{quotient}\index{quotient}" and "\NewTerm{remainder}\index{remainder}". Initially define only for non-zero integers, it can be generalized to relative integers and polynomials, for example.
	
	\textbf{Definition (\#\thesection.\mydef):} We name "\NewTerm{euclidean division}\index{Euclidean division}" or "\NewTerm{integer division}\index{integer division}" of two numbers $A$ and $B$ the operation of dividing $B$ by $A$, stopping when the rest is strictly less than $A$.
	
	Let us recall (\SeeChapter{see section Numbers page \pageref{prime number}}) that any number which admits exactly two euclidean divisors (such that division gives no remainder) that are the $1$ and itself is named a "\NewTerm{prime number}\index{prime number}" (which excludes the number $1$ of the list of primes) and that any pair of numbers which have only $1$ as common Euclidean divider are say to be "\NewTerm{relatively prime}\index{relatively prime numbers}", "\NewTerm{mutually prime}\index{mutually prime numbers}", or "\NewTerm{coprime}\index{coprime numbers}".
	\begin{theorem}
	Given $A,B\in \mathbb{Z}$ with $A>0$. The "\NewTerm{theorem of the Euclidean division}\index{theorem of the Euclidean division}" state that there are unique integers $q$ (quotient) and $r$ (remainder) such as:
	
	where $0\leq r <A$. Furthermore, if $A\nmid B$, then $0<r<A$.
	\end{theorem}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	We have one cake with $9$ parts ($B$), we then have to divide it between $4$ people ($A$) with one part remaining ($r=1$) such that $q=2$:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/euclidean_division.jpg}
		\caption[]{The pie has $9$ slices, so each of the $4$ people receive $2$ slices and $1$ is left over}
	\end{figure}
	and therefore:
	
	\end{tcolorbox}
		\begin{dem}
	Let us consider the set:
	
	It is relatively easy to see that $S\subset \mathbb{N}^{*}\cup \{0\}$ and that $S\neq \varnothing$, hence, according to the principle of good order, we conclude that $S$ contains a smaller element $r\geq 0$.
	
	Given $q$ the integer satisfying thus:
	
	We want to first show that $r<A$ assuming the opposite (proof ad absurdum), that is to say that $r\geq A$. So, in this case, we have:
	
	which is equivalent to:
	
	but $B-(q+1)A\in S$ and:
	
	This contradicts the fact that:
	
	is the smallest element of $S$. So $r<A$. Finally, it is clear that if $r=0$, we have $A|B$, hence the second statement of the theorem.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In the statement of the Euclidean division, we assumed that $A>0$. What do we get when $A<0$? In this situation, $-A$ is obviously positive, and then we can apply the Euclidean division to $B$ and $-A$. Therefore, there are integers $q$ and $r$ integers such that:
	
	where $0\geq r <|A|$. But this relation can be written:
	
	where obviously, $-q$ is an integer. The conclusion is that the Euclidean division can be stated in a more general form.\\

	Given $A,B\in \mathbb{Z}$, there exist two integers $q$ and $r$ such that:
	
	where $0\geq r <|A|$. Furthermore, if $A\nmid B$, then $0<r<|A|$
	\end{tcolorbox}
	The integers $q$ and $r$ are unique in the Euclidean division. Indeed, if there are two other integers $q'$ and $r'$ such as:
	
	always with $0\leq r'<A$, then:
	
	and therefore:
	
	Following theorem 4.17 we have if $r-r'\neq 0$ that $|r-r'|\geq A$.

	But, this last inequality is impossible as by construction $-A<r-r'$. Therefore $r=r'$ and, as $A\neq 0$, then $q'=q$ hence the unicity.
	
	\paragraph{Greatest common divisor}\label{greatest common divisor}\mbox{}\\\\
	The "\NewTerm{greatest common divisor}" (gcd) (also known as  "\NewTerm{greatest common factor}" (gcf), "\NewTerm{highest common factor}" (hcf), "\NewTerm{greatest common measure}" (gcm), or "\NewTerm{highest common divisor}") of two or more integers, when at least one of them is not zero, is the largest positive integer that divides the numbers without a remainder. 
	
	\textbf{Definition (\#\thesection.\mydef):} Given $a,b\in\mathbb{Z}$ such as $ab\neq 0$. The "\NewTerm{greatest common divisor}\index{greatest common divisor}" (gc) of $a$ and $b$, denoted:
	
	is the positive integer $n$ that satisfies the following two properties:
	\begin{enumerate}
		\item[P1.] $d|a$ and $d|b$ (so without remainder $r$ in the division!)

		\item[P2.] If $c|a$ and $c|b$ then $c\leq d$ and $c|d$ (by definition!)
	\end{enumerate}
	Note that $1$ is always a common divisor of two arbitrary integers.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us consider the positive integers $36$ and $54$. A common divisor\index{common divisor} of $36$ and $54$ is a positive integer that divides $36$, and also $54$. For example, $1$ and $2$ are common divisors $36$ and $54$.

	
	We have the intersection represented by the following Venn diagram (\SeeChapter{see section Set Theory page \pageref{Venn diagrams}}):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,437); %set diagram left start at 0, and has height of 437
		
		%Shape: Ellipse [id:dp12089358265758166] 
		\draw   (171.48,241.5) .. controls (151.73,213.22) and (177.46,161.15) .. (228.94,125.2) .. controls (280.43,89.24) and (338.17,83.02) .. (357.92,111.3) .. controls (377.67,139.57) and (351.94,191.64) .. (300.46,227.6) .. controls (248.97,263.55) and (191.23,269.78) .. (171.48,241.5) -- cycle ;
		%Shape: Ellipse [id:dp8983569570660415] 
		\draw   (286.8,91.99) .. controls (310.82,63.02) and (369.47,72.02) .. (417.82,112.1) .. controls (466.16,152.18) and (485.88,208.15) .. (461.86,237.12) .. controls (437.84,266.09) and (379.18,257.09) .. (330.84,217.01) .. controls (282.5,176.93) and (262.78,120.96) .. (286.8,91.99) -- cycle ;
		%Shape: Circle [id:dp3558645938922629] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (239,149.45) .. controls (239,147.49) and (240.59,145.9) .. (242.55,145.9) .. controls (244.51,145.9) and (246.1,147.49) .. (246.1,149.45) .. controls (246.1,151.41) and (244.51,153) .. (242.55,153) .. controls (240.59,153) and (239,151.41) .. (239,149.45) -- cycle ;
		%Shape: Circle [id:dp07743283673117096] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (216,201.45) .. controls (216,199.49) and (217.59,197.9) .. (219.55,197.9) .. controls (221.51,197.9) and (223.1,199.49) .. (223.1,201.45) .. controls (223.1,203.41) and (221.51,205) .. (219.55,205) .. controls (217.59,205) and (216,203.41) .. (216,201.45) -- cycle ;
		%Shape: Circle [id:dp32720280859909656] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (254,221.45) .. controls (254,219.49) and (255.59,217.9) .. (257.55,217.9) .. controls (259.51,217.9) and (261.1,219.49) .. (261.1,221.45) .. controls (261.1,223.41) and (259.51,225) .. (257.55,225) .. controls (255.59,225) and (254,223.41) .. (254,221.45) -- cycle ;
		%Shape: Circle [id:dp37000338624029916] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (413,171.45) .. controls (413,169.49) and (414.59,167.9) .. (416.55,167.9) .. controls (418.51,167.9) and (420.1,169.49) .. (420.1,171.45) .. controls (420.1,173.41) and (418.51,175) .. (416.55,175) .. controls (414.59,175) and (413,173.41) .. (413,171.45) -- cycle ;
		%Shape: Circle [id:dp8742099228208493] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (379,216.45) .. controls (379,214.49) and (380.59,212.9) .. (382.55,212.9) .. controls (384.51,212.9) and (386.1,214.49) .. (386.1,216.45) .. controls (386.1,218.41) and (384.51,220) .. (382.55,220) .. controls (380.59,220) and (379,218.41) .. (379,216.45) -- cycle ;
		%Shape: Circle [id:dp6162906369236292] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (300,112.45) .. controls (300,110.49) and (301.59,108.9) .. (303.55,108.9) .. controls (305.51,108.9) and (307.1,110.49) .. (307.1,112.45) .. controls (307.1,114.41) and (305.51,116) .. (303.55,116) .. controls (301.59,116) and (300,114.41) .. (300,112.45) -- cycle ;
		%Shape: Circle [id:dp32877291240141493] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (328,119.45) .. controls (328,117.49) and (329.59,115.9) .. (331.55,115.9) .. controls (333.51,115.9) and (335.1,117.49) .. (335.1,119.45) .. controls (335.1,121.41) and (333.51,123) .. (331.55,123) .. controls (329.59,123) and (328,121.41) .. (328,119.45) -- cycle ;
		%Shape: Circle [id:dp08530837796705604] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (307,137.45) .. controls (307,135.49) and (308.59,133.9) .. (310.55,133.9) .. controls (312.51,133.9) and (314.1,135.49) .. (314.1,137.45) .. controls (314.1,139.41) and (312.51,141) .. (310.55,141) .. controls (308.59,141) and (307,139.41) .. (307,137.45) -- cycle ;
		%Shape: Circle [id:dp6384072496486974] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (330,151.45) .. controls (330,149.49) and (331.59,147.9) .. (333.55,147.9) .. controls (335.51,147.9) and (337.1,149.49) .. (337.1,151.45) .. controls (337.1,153.41) and (335.51,155) .. (333.55,155) .. controls (331.59,155) and (330,153.41) .. (330,151.45) -- cycle ;
		%Shape: Circle [id:dp0517087554507385] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (299,160.45) .. controls (299,158.49) and (300.59,156.9) .. (302.55,156.9) .. controls (304.51,156.9) and (306.1,158.49) .. (306.1,160.45) .. controls (306.1,162.41) and (304.51,164) .. (302.55,164) .. controls (300.59,164) and (299,162.41) .. (299,160.45) -- cycle ;
		%Shape: Circle [id:dp18634079695566785] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (321,176.45) .. controls (321,174.49) and (322.59,172.9) .. (324.55,172.9) .. controls (326.51,172.9) and (328.1,174.49) .. (328.1,176.45) .. controls (328.1,178.41) and (326.51,180) .. (324.55,180) .. controls (322.59,180) and (321,178.41) .. (321,176.45) -- cycle ;
		
		% Text Node
		\draw (179,114) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\text{Div}_{36}$};
		% Text Node
		\draw (459,245.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\text{Div}_{54}$};
		% Text Node
		\draw (243,134) node [anchor=north west][inner sep=0.75pt]   [align=left] {$12$};
		% Text Node
		\draw (225,189) node [anchor=north west][inner sep=0.75pt]   [align=left] {$4$};
		% Text Node
		\draw (263,209) node [anchor=north west][inner sep=0.75pt]   [align=left] {$36$};
		% Text Node
		\draw (422,159) node [anchor=north west][inner sep=0.75pt]   [align=left] {$27$};
		% Text Node
		\draw (388,204) node [anchor=north west][inner sep=0.75pt]   [align=left] {$54$};
		% Text Node
		\draw (309,100) node [anchor=north west][inner sep=0.75pt]   [align=left] {$1$};
		% Text Node
		\draw (337,107) node [anchor=north west][inner sep=0.75pt]   [align=left] {$2$};
		% Text Node
		\draw (316,125) node [anchor=north west][inner sep=0.75pt]   [align=left] {$3$};
		% Text Node
		\draw (339,142.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$9$};
		% Text Node
		\draw (308,152.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$6$};
		% Text Node
		\draw (328,166.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$18$};	
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Venn diagram of common divisors}
	\end{figure}
	with the following set of common divisors:
	
	and hence, the highest common factor is:
	
	and we find that the set of common divisors of $36$ and $54$ is also the set of
dividers of $18$.
	\end{tcolorbox}
	
	However it is not necessarily obvious that the greatest common divisor other than $1$ of two integers $a$ and $b$ that are not relatively prime always exists. This is proved by the following theorem (however, if the gcd exists, it is by definition unique!) named "\NewTerm{Bézout's theorem}\index{Bézout's theorem}\label{bezout theorem}" that can also gives the opportunity to prove other interesting properties of two numbers as we shall see later.
	\begin{theorem}
	Given $a,b\in \mathbb{Z}$ such that $ab\neq 0$. If $d$ divides $a$ and $d$ divides $b$ (for both without remainder $r$ !) then there must two integers $x$ and $y$ such that:
	
	This relation is named the "\NewTerm{Bézout's identity}\index{Bézout's identity}" and it is a linear Diophantine equation (\SeeChapter{see section Calculus page \pageref{diophantine equation}}).
	\end{theorem}
	\begin{dem}
	Obviously, if $a$ and $b$ are relatively prime we know that $d$ is then $1$.
	
	To prove the Bézout identity let first consider the set:
	
	As $S\subset \mathbb{N}$ and $S\neq \varnothing$, we can use the principle of good order and conclude that $S$ has a smaller element $d$. We can then write:
	
	for some given choice $x_0,y_0\in\mathbb{Z}$. So it is sufficient to prove that $d=(a,b)$ to prove the Bézout identity!
	
	Let us proceed with a proof by contradiction by assuming $d\nmid a$. Then if this is the case, following the Euclidean division, there exist $q,r\in\mathbb{Z}$ such as $a=qd+r$, where $0<r<d$. But then:
	
	Thus we have that $r\in S$ and $r<d$, which contradicts the fact that $d$ is the smallest possible element of $S$. Thus we have proven not only that $d | a$, but also that $d$ always exists and, in the same way we prove that $d | b$.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{corollary}
	As important corollary let us now prove that if $a,b\in\mathbb{Z}$ such that $ab\neq 0$, then:
	
	is the set of all multiples of $d(a,b)$.
	\end{corollary}
	\begin{dem}
	As $d | a$ and $d | b$, then we have necessarily $dax+by|$ for any $x,y\in \mathbb{Z}$. Either $M=\{nd|n\in\mathbb{Z}\}$. Our problem is then reduced to prove the fact that $S=M$.
	
	Given first $s\in S$ which means that $d|s$ and involves $s\in M$.

	Given a $m\in M$, this would mean that $m=nd$ for a certain $n\in\mathbb{Z}$.
	
	As $d=ax_0+by_0$ for any choice of integers $x_0,y_0\in\mathbb{Z}$, then:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	The assumptions may seem complicated but put your attention a given time on the last equality. You will quickly understand!
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	If instead of defining the greatest common divisor of two non-zero integers, we allow one of them to be equal to $0$, say: $a\neq b$, $b=0$. In this case, we have $a|b$ and, according to our definition of the GCD, it is clear that $(a,0)=|a|$.
	\end{tcolorbox}
	Given $d=(a,b)$ and $m\in\mathbb{Z}$, then we have the following properties of the GCD (without proof but if a reader request them we will give the details):
	\begin{enumerate}
		\item[P1.] $(a,b+ma)=(a,b)=(a,-b)$
		\item[P2.] $(am,bm)=|m|(a,b)$ where $m\neq 0$
		\item[P3.] $\left(\dfrac{a}{d},\dfrac{b}{d}\right)=1$
		\item[P4.] If $g\in\mathbb{Z}\setminus \{0\}$ such that $g|a$ and $g|b$ then $\left(\dfrac{a}{g},\dfrac{b}{g}\right)=\dfrac{1}{|g|}(a,b)$
	\end{enumerate}
	In some books, these four properties are proved using intrinsically the property itself. Personally we abstain make usage of this approach because doing this is more ridiculous than anything else as the statement of the property is a proof in itself.
	
	Let us now develop a method (algorithm) that will be very useful to us to calculate (determine) the greatest common divisor of two integers (sometimes useful computing science).
	
	\subsubsection{Euclidean Algorithm}
	The Euclidean algorithm is an algorithm for determining the greatest common divisor of two integers (we have hesitate to put this subject in the section of Theoretical Computing...).
	
	To address this method intuitively, you must know that you need to see that an integer as a length, a pair of integers as a rectangle (sides) and their GCD is the size of the largest square for tile (paving) their rectangle by definition (yes if you think for a moment it's quite logical!).

	The algorithm decomposes the original rectangle into squares, always smaller and smaller, by successive Euclidean division of the length by the width, then the width by the remainder until a zero remainder. We must understand this geometric approach to then understand the algorithm.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us consider that we seek the GCD of $(a, b)$ where $b$ is equal $21$ and $a$ is equal $15$ and keep in mind that the GCD, besides the fact that it divides $a$ and $b$, must leave a zero remainder! In other words it must divide the remainder of the division of $b$ by $a$ also!\\
	
	So we have the following rectangle of $21$ by $15$:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/arithmetics/euclids_algorithm_step1.jpg}
		\caption[]{First step of the GCD algorithm}
	\end{figure}
	First we see if $15$ is the GCD (it always starts with the smallest). We then divide $21$ by $15$, which is equivalent geometrically to:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/euclids_algorithm_step2.jpg}
		\caption[]{Second step of the GCD algorithm}
	\end{figure}
	$15$ is therefore not the GCD (we suspected it...). We immediately see that we can not pave the rectangle with a square of $15$ by $15$.\\
	
	So we have a remainder of $6$ (left rectangle). The GCD as we know must, if it exists, by definition divide that remains and leave a zero remainder.\\

	So we have a rectangle of $15$ by $6$. So we are looking now to pave this new rectangle because we know that the greatest common divisor is by construction less than or equal to $6$. Then we have:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/euclids_algorithm_step3.jpg}
		\caption[]{Third step of the GCD algorithm}
	\end{figure}
	So we divide $15$ by remainder $6$ (this result will be less than $6$ and immediately permits to tests whether the remainder will be the GCD). We are getting:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/euclids_algorithm_step4.jpg}
		\caption[]{Fourth step of the GCD algorithm}
	\end{figure}
	Again, we can not pave the rectangle only with squares. In other words, we have a non-zero remainder which is $3$. Given now a rectangle of $6$ by $3$. So we are looking now to pave the new rectangle because we know that the greatest common divisor is by construction less than or equal to $3$ and that it will leave a remainder equal to zero, if it exists. We then have geometrically:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/euclids_algorithm_step5.jpg}
		\caption[]{Fifth step of the GCD algorithm}
	\end{figure}
	We divide $6$ by $3$ (which will be less than $3$ and permits us to test immediately whether the rest will be the GCD):
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/euclids_algorithm_step6.jpg}
		\caption[]{Sixth step of the GCD algorithm}
	\end{figure}
	and it's all good! We then have $3$ that leaves us with a remainder equal to zero and divides the remainder $6$ so this is the GCD. So we have in the end:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/euclids_algorithm_step7.jpg}
		\caption{Summary of the GCD algorithm}
	\end{figure}
	\end{tcolorbox}
	Now let us see the equivalent formal approach.
	
	Given $a,b\in\mathbb{Z}$, where $a>0$. Applying successively the Euclidean division (with $b> a$), we get the following sequence of equations:
	
	if $d=(a,b)$, then $d=r_j$. With the corresponding pseudocode algorithm:
	
	\begin{algorithm}[H]
	 \KwData{$a$,$ b$}
	 \KwResult{$b$}
	 initialization\;
	$r=a\mod b$\;
	 \While{$r\neq 0$}{
	  $a:=b$\;
	  $b:=r$\;
	   $f(b):=f(a)$\;
	   $a:=x_1$\;
	   $f(a):=f(x_1)$\;
	   }
	  Display $x_1$\;
	 \caption{GCD pseudocode algorithm}
	\end{algorithm}
	Otherwise even more formally:
	\begin{dem}
	We want first prove that $r_j=(a,b)$. But, following the property P1:
	
	we have:
	
	To prove the second property of the Euclide's algorithm, we write the prior-previous equation of the system under the form:
	
	Now, using the previous equation this prior-previous equation of the system, we have:
	
	Continuing this process, we can express $r_j$ as a linear combination of $a$ and $b$.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us calculate the greatest common divisor of $(429,966)$ and express this number as a linear combination of $429$ and $966$.\\
	
	We apply obviously Euclide's algorithm:
	
	We therefore conclude that:
	
	and, in addition, that:
	
	Thus the GCD is indeed expressed as a linear combination of $a$ and $b$ and constitutes as such the GCD.
	\end{tcolorbox}
	\textbf{Definition (\#\thesection.\mydef):} We say that the integers $a_1,a_2,\ldots,a_n$ are for recall "\NewTerm{relatively prime}" if:
	
	
	\subsubsection{Least Common Multiple}
	The least common multiple (also named the "\NewTerm{lowest common multiple}\index{lowest common multiple}" or "\NewTerm{smallest common multiple}\index{smallest common multiple}") of two integers $a$ and $b$, usually denoted by $\text{LCM}(a, b)$, is the smallest positive integer that is divisible by both $a$ and $b$. Since division of integers by zero is undefined, this definition has meaning only if $a$ and $b$ are both different from zero.
	
	The LCM is familiar from grade-school arithmetic as the "\NewTerm{lowest common denominator (LCD)} \index{lowest common denominator}" (also named "\NewTerm{smallest common denominator}\index{smallest common denominator}") that must be determined before fractions can be added, subtracted or compared. The LCM of more than two integers is also well-defined: it is the smallest positive integer that is divisible by each of them.
	
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] Given $a_1,a_2,\ldots,a_n\in \mathbb{Z}\setminus \{0\}$, we say that $m$ is a "\NewTerm{common multiple}\index{common multiple}" of $a_1,a_2,\ldots,a_n$ if $a_i|m$ for $i=1,2,\ldots,n$

		\item[D2.] Given $a_1,a_2,\ldots,a_n\in \mathbb{Z}\setminus \{0\}$, we name "\NewTerm{lowest common multiple}\index{lowest common multiple}" (LCM) of $a_1,a_2,\ldots,a_n$ if $a_i|m$ for $i=1,2,\ldots,n$ denoted traditionally by:
		
		the lowest integer positive common multiple to all common multiples of $a_1,a_2,\ldots,a_n$.
	\end{enumerate}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. Let us consider the positive integers $3$ and $5$. A common multiple of $3$ and $5$ is a positive integer which is both a multiple of $3$, and a multiple of $5$. In other words, which is divisible by $3$ and $5$. We have therefore:
	
	We then have the intersection represented by the following Venn diagram (\SeeChapter{see section Set Theory \pageref{Venn diagrams}}):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,651); %set diagram left start at 0, and has height of 651
		
		%Shape: Ellipse [id:dp05980843935508706] 
		\draw   (135.27,235.35) .. controls (110.36,201.29) and (133.76,141.8) .. (187.53,102.47) .. controls (241.31,63.14) and (305.1,58.87) .. (330.01,92.93) .. controls (354.92,126.99) and (331.52,186.48) .. (277.74,225.81) .. controls (223.96,265.14) and (160.17,269.41) .. (135.27,235.35) -- cycle ;
		%Shape: Ellipse [id:dp13338854097187336] 
		\draw   (248.9,89.06) .. controls (274.11,55.23) and (337.86,60.07) .. (391.29,99.87) .. controls (444.71,139.68) and (467.58,199.38) .. (442.37,233.22) .. controls (417.16,267.05) and (353.41,262.21) .. (299.99,222.41) .. controls (246.56,182.6) and (223.69,122.9) .. (248.9,89.06) -- cycle ;
		%Shape: Circle [id:dp8269745384252254] 
		\draw  [fill={rgb, 255:red, 74; green, 74; blue, 74 }  ,fill opacity=1 ] (205,101) .. controls (205,99.9) and (205.9,99) .. (207,99) .. controls (208.1,99) and (209,99.9) .. (209,101) .. controls (209,102.1) and (208.1,103) .. (207,103) .. controls (205.9,103) and (205,102.1) .. (205,101) -- cycle ;
		%Shape: Circle [id:dp027010843844663013] 
		\draw  [fill={rgb, 255:red, 74; green, 74; blue, 74 }  ,fill opacity=1 ] (183,123) .. controls (183,121.9) and (183.9,121) .. (185,121) .. controls (186.1,121) and (187,121.9) .. (187,123) .. controls (187,124.1) and (186.1,125) .. (185,125) .. controls (183.9,125) and (183,124.1) .. (183,123) -- cycle ;
		%Shape: Circle [id:dp957501483560155] 
		\draw  [fill={rgb, 255:red, 74; green, 74; blue, 74 }  ,fill opacity=1 ] (169,149) .. controls (169,147.9) and (169.9,147) .. (171,147) .. controls (172.1,147) and (173,147.9) .. (173,149) .. controls (173,150.1) and (172.1,151) .. (171,151) .. controls (169.9,151) and (169,150.1) .. (169,149) -- cycle ;
		%Shape: Circle [id:dp007773715708945028] 
		\draw  [fill={rgb, 255:red, 74; green, 74; blue, 74 }  ,fill opacity=1 ] (150,176) .. controls (150,174.9) and (150.9,174) .. (152,174) .. controls (153.1,174) and (154,174.9) .. (154,176) .. controls (154,177.1) and (153.1,178) .. (152,178) .. controls (150.9,178) and (150,177.1) .. (150,176) -- cycle ;
		%Shape: Circle [id:dp8451578572490672] 
		\draw  [fill={rgb, 255:red, 74; green, 74; blue, 74 }  ,fill opacity=1 ] (211,149) .. controls (211,147.9) and (211.9,147) .. (213,147) .. controls (214.1,147) and (215,147.9) .. (215,149) .. controls (215,150.1) and (214.1,151) .. (213,151) .. controls (211.9,151) and (211,150.1) .. (211,149) -- cycle ;
		%Shape: Circle [id:dp8632231680939308] 
		\draw  [fill={rgb, 255:red, 74; green, 74; blue, 74 }  ,fill opacity=1 ] (218,186) .. controls (218,184.9) and (218.9,184) .. (220,184) .. controls (221.1,184) and (222,184.9) .. (222,186) .. controls (222,187.1) and (221.1,188) .. (220,188) .. controls (218.9,188) and (218,187.1) .. (218,186) -- cycle ;
		%Shape: Circle [id:dp6072012944536633] 
		\draw  [fill={rgb, 255:red, 74; green, 74; blue, 74 }  ,fill opacity=1 ] (163,213) .. controls (163,211.9) and (163.9,211) .. (165,211) .. controls (166.1,211) and (167,211.9) .. (167,213) .. controls (167,214.1) and (166.1,215) .. (165,215) .. controls (163.9,215) and (163,214.1) .. (163,213) -- cycle ;
		%Shape: Circle [id:dp843305867351881] 
		\draw  [fill={rgb, 255:red, 74; green, 74; blue, 74 }  ,fill opacity=1 ] (197,212) .. controls (197,210.9) and (197.9,210) .. (199,210) .. controls (200.1,210) and (201,210.9) .. (201,212) .. controls (201,213.1) and (200.1,214) .. (199,214) .. controls (197.9,214) and (197,213.1) .. (197,212) -- cycle ;
		%Shape: Circle [id:dp11136876657020411] 
		\draw  [fill={rgb, 255:red, 74; green, 74; blue, 74 }  ,fill opacity=1 ] (239,222) .. controls (239,220.9) and (239.9,220) .. (241,220) .. controls (242.1,220) and (243,220.9) .. (243,222) .. controls (243,223.1) and (242.1,224) .. (241,224) .. controls (239.9,224) and (239,223.1) .. (239,222) -- cycle ;
		%Shape: Circle [id:dp3694007210229193] 
		\draw  [fill={rgb, 255:red, 74; green, 74; blue, 74 }  ,fill opacity=1 ] (266,100) .. controls (266,98.9) and (266.9,98) .. (268,98) .. controls (269.1,98) and (270,98.9) .. (270,100) .. controls (270,101.1) and (269.1,102) .. (268,102) .. controls (266.9,102) and (266,101.1) .. (266,100) -- cycle ;
		%Shape: Circle [id:dp6656321863788908] 
		\draw  [fill={rgb, 255:red, 74; green, 74; blue, 74 }  ,fill opacity=1 ] (303,115) .. controls (303,113.9) and (303.9,113) .. (305,113) .. controls (306.1,113) and (307,113.9) .. (307,115) .. controls (307,116.1) and (306.1,117) .. (305,117) .. controls (303.9,117) and (303,116.1) .. (303,115) -- cycle ;
		%Shape: Circle [id:dp4231908292936364] 
		\draw  [fill={rgb, 255:red, 74; green, 74; blue, 74 }  ,fill opacity=1 ] (265,142) .. controls (265,140.9) and (265.9,140) .. (267,140) .. controls (268.1,140) and (269,140.9) .. (269,142) .. controls (269,143.1) and (268.1,144) .. (267,144) .. controls (265.9,144) and (265,143.1) .. (265,142) -- cycle ;
		%Shape: Circle [id:dp20952065397993413] 
		\draw  [fill={rgb, 255:red, 74; green, 74; blue, 74 }  ,fill opacity=1 ] (285,162) .. controls (285,160.9) and (285.9,160) .. (287,160) .. controls (288.1,160) and (289,160.9) .. (289,162) .. controls (289,163.1) and (288.1,164) .. (287,164) .. controls (285.9,164) and (285,163.1) .. (285,162) -- cycle ;
		%Shape: Circle [id:dp24478838529876312] 
		\draw  [fill={rgb, 255:red, 74; green, 74; blue, 74 }  ,fill opacity=1 ] (352,107) .. controls (352,105.9) and (352.9,105) .. (354,105) .. controls (355.1,105) and (356,105.9) .. (356,107) .. controls (356,108.1) and (355.1,109) .. (354,109) .. controls (352.9,109) and (352,108.1) .. (352,107) -- cycle ;
		%Shape: Circle [id:dp21545414105671368] 
		\draw  [fill={rgb, 255:red, 74; green, 74; blue, 74 }  ,fill opacity=1 ] (385,139) .. controls (385,137.9) and (385.9,137) .. (387,137) .. controls (388.1,137) and (389,137.9) .. (389,139) .. controls (389,140.1) and (388.1,141) .. (387,141) .. controls (385.9,141) and (385,140.1) .. (385,139) -- cycle ;
		%Shape: Circle [id:dp18572286364196056] 
		\draw  [fill={rgb, 255:red, 74; green, 74; blue, 74 }  ,fill opacity=1 ] (349,157) .. controls (349,155.9) and (349.9,155) .. (351,155) .. controls (352.1,155) and (353,155.9) .. (353,157) .. controls (353,158.1) and (352.1,159) .. (351,159) .. controls (349.9,159) and (349,158.1) .. (349,157) -- cycle ;
		%Shape: Circle [id:dp9466895261682506] 
		\draw  [fill={rgb, 255:red, 74; green, 74; blue, 74 }  ,fill opacity=1 ] (414,189) .. controls (414,187.9) and (414.9,187) .. (416,187) .. controls (417.1,187) and (418,187.9) .. (418,189) .. controls (418,190.1) and (417.1,191) .. (416,191) .. controls (414.9,191) and (414,190.1) .. (414,189) -- cycle ;
		%Shape: Circle [id:dp8732922332978812] 
		\draw  [fill={rgb, 255:red, 74; green, 74; blue, 74 }  ,fill opacity=1 ] (342,189) .. controls (342,187.9) and (342.9,187) .. (344,187) .. controls (345.1,187) and (346,187.9) .. (346,189) .. controls (346,190.1) and (345.1,191) .. (344,191) .. controls (342.9,191) and (342,190.1) .. (342,189) -- cycle ;
		%Shape: Circle [id:dp5569881865252084] 
		\draw  [fill={rgb, 255:red, 74; green, 74; blue, 74 }  ,fill opacity=1 ] (376,209) .. controls (376,207.9) and (376.9,207) .. (378,207) .. controls (379.1,207) and (380,207.9) .. (380,209) .. controls (380,210.1) and (379.1,211) .. (378,211) .. controls (376.9,211) and (376,210.1) .. (376,209) -- cycle ;
		
		% Text Node
		\draw (133,94) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle M_{3}$};
		% Text Node
		\draw (408,85) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle M_{5}$};
		% Text Node
		\draw (183,235) node [anchor=north west][inner sep=0.75pt]   [align=left] {...};
		% Text Node
		\draw (281,181) node [anchor=north west][inner sep=0.75pt]   [align=left] {...};
		% Text Node
		\draw (369,230) node [anchor=north west][inner sep=0.75pt]   [align=left] {...};
		% Text Node
		\draw (211,93) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 3$};
		% Text Node
		\draw (192,115) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 6$};
		% Text Node
		\draw (216,140) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 12$};
		% Text Node
		\draw (175,140) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 9$};
		% Text Node
		\draw (155,166) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 18$};
		% Text Node
		\draw (224,177) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 21$};
		% Text Node
		\draw (168,205) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 24$};
		% Text Node
		\draw (204,204) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 27$};
		% Text Node
		\draw (245,212) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 33$};
		% Text Node
		\draw (271,92) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 15$};
		% Text Node
		\draw (307.38,107) node [anchor=north west][inner sep=0.75pt]  [xslant=0.04] [align=left] {$\displaystyle 60$};
		% Text Node
		\draw (271,133) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 30$};
		% Text Node
		\draw (291,154) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 45$};
		% Text Node
		\draw (358,99) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 5$};
		% Text Node
		\draw (352,149) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 10$};
		% Text Node
		\draw (390,132) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 20$};
		% Text Node
		\draw (346,181) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 25$};
		% Text Node
		\draw (418,180) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 40$};
		% Text Node
		\draw (380,201) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 35$};
		\end{tikzpicture}
	\end{figure}
	we then have the following set of common multiples:
	
	and therefore the LCM is given by:
	
	Or if it can help here is another possible visualization of the concept:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,651); %set diagram left start at 0, and has height of 651
		
		%Straight Lines [id:da4397877495434799] 
		\draw    (62,153.94) -- (106.5,153) -- (541.5,153) (88.91,149.37) -- (89.08,157.37)(115.99,149) -- (115.99,157)(142.99,149) -- (142.99,157)(169.99,149) -- (169.99,157)(196.99,149) -- (196.99,157)(223.99,149) -- (223.99,157)(250.99,149) -- (250.99,157)(277.99,149) -- (277.99,157)(304.99,149) -- (304.99,157)(331.99,149) -- (331.99,157)(358.99,149) -- (358.99,157)(385.99,149) -- (385.99,157)(412.99,149) -- (412.99,157)(439.99,149) -- (439.99,157)(466.99,149) -- (466.99,157)(493.99,149) -- (493.99,157)(520.99,149) -- (520.99,157) ;
		\draw [shift={(544.5,153)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (10.72,-5.15) -- (0,0) -- (10.72,5.15) -- (7.12,0) -- cycle    ;
		\draw [shift={(59,154)}, rotate = 358.79] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (10.72,-5.15) -- (0,0) -- (10.72,5.15) -- (7.12,0) -- cycle    ;
		%Shape: Arc [id:dp2727865493648489] 
		\draw  [draw opacity=0] (412.51,143.17) .. controls (413.07,120.07) and (431.21,101.52) .. (453.5,101.52) .. controls (476.14,101.52) and (494.5,120.65) .. (494.5,144.26) .. controls (494.5,144.53) and (494.5,144.79) .. (494.49,145.06) -- (453.5,144.26) -- cycle ; \draw    (412.51,143.17) .. controls (413.07,120.07) and (431.21,101.52) .. (453.5,101.52) .. controls (476.14,101.52) and (494.5,120.65) .. (494.5,144.26) ; \draw [shift={(494.49,145.06)}, rotate = 263.35] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (10.72,-5.15) -- (0,0) -- (10.72,5.15) -- (7.12,0) -- cycle    ; 
		%Shape: Arc [id:dp5936943672610289] 
		\draw  [draw opacity=0] (330.53,141.28) .. controls (331.09,118.18) and (349.22,99.63) .. (371.52,99.63) .. controls (394.16,99.63) and (412.52,118.77) .. (412.52,142.37) .. controls (412.52,142.64) and (412.52,142.91) .. (412.51,143.17) -- (371.52,142.37) -- cycle ; \draw    (330.53,141.28) .. controls (331.09,118.18) and (349.22,99.63) .. (371.52,99.63) .. controls (394.16,99.63) and (412.52,118.77) .. (412.52,142.37) ; \draw [shift={(412.51,143.17)}, rotate = 263.35] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (10.72,-5.15) -- (0,0) -- (10.72,5.15) -- (7.12,0) -- cycle    ; 
		%Shape: Arc [id:dp47666429293916357] 
		\draw  [draw opacity=0] (248.55,139.39) .. controls (249.11,116.29) and (267.24,97.74) .. (289.54,97.74) .. controls (312.18,97.74) and (330.54,116.88) .. (330.54,140.48) .. controls (330.54,140.75) and (330.54,141.02) .. (330.53,141.28) -- (289.54,140.48) -- cycle ; \draw    (248.55,139.39) .. controls (249.11,116.29) and (267.24,97.74) .. (289.54,97.74) .. controls (312.18,97.74) and (330.54,116.88) .. (330.54,140.48) ; \draw [shift={(330.53,141.28)}, rotate = 263.35] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (10.72,-5.15) -- (0,0) -- (10.72,5.15) -- (7.12,0) -- cycle    ; 
		%Shape: Arc [id:dp038940846420294495] 
		\draw  [draw opacity=0] (166.57,137.51) .. controls (167.12,114.4) and (185.26,95.85) .. (207.56,95.85) .. controls (230.2,95.85) and (248.56,114.99) .. (248.56,138.59) .. controls (248.56,138.86) and (248.56,139.13) .. (248.55,139.39) -- (207.56,138.59) -- cycle ; \draw    (166.57,137.51) .. controls (167.12,114.4) and (185.26,95.85) .. (207.56,95.85) .. controls (230.2,95.85) and (248.56,114.99) .. (248.56,138.59) ; \draw [shift={(248.55,139.39)}, rotate = 263.35] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (10.72,-5.15) -- (0,0) -- (10.72,5.15) -- (7.12,0) -- cycle    ; 
		%Shape: Arc [id:dp5377923054681832] 
		\draw  [draw opacity=0] (222.62,187.24) .. controls (224.11,210.46) and (254.17,229) .. (291.03,229) .. controls (328.84,229) and (359.5,209.49) .. (359.5,185.43) .. controls (359.5,184.98) and (359.49,184.53) .. (359.47,184.09) -- (291.03,185.43) -- cycle ; \draw    (222.62,187.24) .. controls (224.11,210.46) and (254.17,229) .. (291.03,229) .. controls (328.84,229) and (359.5,209.49) .. (359.5,185.43) ; \draw [shift={(359.47,184.09)}, rotate = 99.48] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (10.72,-5.15) -- (0,0) -- (10.72,5.15) -- (7.12,0) -- cycle    ; 
		%Shape: Arc [id:dp9397670937255853] 
		\draw  [draw opacity=0] (359.47,184.09) .. controls (360.96,207.31) and (391.02,225.85) .. (427.88,225.85) .. controls (465.7,225.85) and (496.35,206.34) .. (496.35,182.27) .. controls (496.35,181.83) and (496.34,181.38) .. (496.32,180.93) -- (427.88,182.27) -- cycle ; \draw    (359.47,184.09) .. controls (360.96,207.31) and (391.02,225.85) .. (427.88,225.85) .. controls (465.7,225.85) and (496.35,206.34) .. (496.35,182.27) ; \draw [shift={(496.32,180.93)}, rotate = 99.48] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (10.72,-5.15) -- (0,0) -- (10.72,5.15) -- (7.12,0) -- cycle    ; 
		%Straight Lines [id:da586075839598071] 
		\draw    (494.49,145.06) -- (516.5,62) ;
		%Straight Lines [id:da14968367822254347] 
		\draw    (516.5,62) -- (536.5,62) ;
		
		% Text Node
		\draw (84,161.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 0$};
		% Text Node
		\draw (110.33,161.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 1$};
		% Text Node
		\draw (137.66,161.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 2$};
		% Text Node
		\draw (164.99,161.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 3$};
		% Text Node
		\draw (191.32,161.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 4$};
		% Text Node
		\draw (218.65,161.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 5$};
		% Text Node
		\draw (244.98,161.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 6$};
		% Text Node
		\draw (273.31,161.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 7$};
		% Text Node
		\draw (300.64,161.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 8$};
		% Text Node
		\draw (326.97,161.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 9$};
		% Text Node
		\draw (349.3,161.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 10$};
		% Text Node
		\draw (376.63,161.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 11$};
		% Text Node
		\draw (405,161.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 12$};
		% Text Node
		\draw (65.32,106.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {multiples of $\displaystyle 3$};
		% Text Node
		\draw (431,161.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 13$};
		% Text Node
		\draw (457,161.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 14$};
		% Text Node
		\draw (483,161.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 15$};
		% Text Node
		\draw (510,161.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 16$};
		% Text Node
		\draw (65.32,201.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {multiples of $\displaystyle 5$};
		% Text Node
		\draw (519,44) node [anchor=north west][inner sep=0.75pt]   [align=left] {Least Common\\Multiple: LCM};
		\end{tikzpicture}
	\end{figure}
	We see obviously that all the common multiples of $3$ and $5$ is the set of multiples of $15$.\\
	
	E2. Wikipedia gives also a nice example of a Venn Diagram (\SeeChapter{see section Set Theory page \pageref{Venn diagrams}}) when we seek for a LCM of $5$ multiples using a visual tool:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.4]{img/arithmetics/lcm_5ary_venn_diagramm.jpg}
	\end{figure}
	\end{tcolorbox}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Given $a_1,a_2,\ldots,a_n\in\mathbb{Z}\setminus \{0\}$. Then the least common multiple exists. Indeed, consider the set $E$ of natural integers $m$ that for all $i$ divide $a_i$. What we will write:
	
	Since we have necessarily $|a_1a_2\ldots a_n|\in E$, then the set is not empty and, according to the axiom of good order, the set $E$ contains a smaller positive element.
	\end{tcolorbox}
	Let us now see some theorems related to the LCM:
	\begin{theorem}
	If $m$ is any common multiple of $a_1,a_2,\ldots,a_n$ then $[a_1,a_2,\ldots,a_n]|m$ that is to say that $m$ divides each of the $a_i$.
	\end{theorem}
	\begin{dem}
	Given $M=[a_1,a_2,\ldots,a_n]$. Then, by the Euclidean division, there are integers $q$ and $r$ such that:
	
	It suffices to show that $r=0$. Let us suppose that $r\neq 0$ (reductio ad absurdum). Since $a_i|m$ and $a_i|M$, then we have $a_i|r$ and this for $i=1,2,\ldots,n$. So $r$ is common multiple of $a_1,a_2,\ldots,a_n$ of the smallest than the LCM. We just obtained a contradiction, which proves the theorem.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{theorem}
	If $k>0$, then $[ka_1,ka_2,\ldots,ka_n]=k[a_1,a_2,\ldots,a_n]$

	The proof will be assumed obvious (if not as always contact us and will add the details!)
	\end{theorem}
	\begin{theorem}
	$[a,b]\cdot(a,b)=|ab|$
	\end{theorem}
	\begin{dem}
	\begin{lemma}
	For this proof, we will use the "\NewTerm{Euclid's lemma}\index{Euclid's lemma}\index{Euclid's theorem!Number theory}\label{euclid lemma}" that says that if $a|bc$ and $(a,b)=1$ then $a|c$.
	
	In other words Euclid's lemma captures a fundamental property of prime numbers, namely: If a prime divides the product of two numbers, it must divide at least one of those numbers. It is also named "\NewTerm{Euclid's first theorem}\index{Euclid's first theorem}". This lemma is the key of the proof of the fundamental theorem of arithmetic that we will see just further below!
	
	Indeed, this can be easily verified because we have seen that there exists $x,y\in\mathbb{Z}$ such as $1=ax+by$ and then $c=acx+bcy$. But $a|ac$ and $a|bc$ imply that $a|(acx+bcy)$, that is to say also that $a|c$.
	\end{lemma}
	
	Ok let us now return to our theorem:
	
	Since $(a,b)=(a,-b)$ and $[a,b]=[a,-b]$, it suffices to prove the result for positive integers $a$ and $b$. 

First of all, let consider the case where $(a,b)=1$. The integer $[a, b]$ being a multiple of $a$, we can write $[a,b]=ma$. Thus, we have $b|ma$ and since $(a,b)=01$, it follows, by Euclid's lemma, that $b | m$. Therefore, $b\leq m$ and then $ab\leq am$. But $ab$ is a common multiple of $a$ and $b$ that can not be smaller than the LCM. therefore $ab=ma=[a,b]$.

	For the general case, that is to say $(a,b)=d>1$, we have, according to the property:
	
	and with the result obtained previously that:
	
	When we multiply both sides of the equation by $d^2$, the result follows and the proof is done.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\pagebreak
	\subsubsection{Fundamental Theorem of Arithmetic}\label{fundamental theorem of arithmetic}
	The fundamental theorem of arithmetic says that every natural number $n>1$ can be written as a product of primes, and this representation is unique, except for the order in which the prime factors are arranged.
	
	The theorem establishes the importance of prime numbers. Essentially, they are the building blocks of building positive integers, each positive integer containing primes in a unique way.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	This theorem is sometimes named "factorization theorem" (wrongly ... because some other theorems have the same name ...).
	\end{tcolorbox}
	So let's go:
	\begin{theorem}
	Every integer greater than $1$ either is prime itself or is the product of prime numbers, and that this product is unique, up to the order of the factors.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	This theorem is one of the main reasons why $1$ is not considered a prime number: if $1$ were prime, the factorization would not be unique.
	\end{tcolorbox}
	\end{theorem}
	\begin{dem}
	The proof uses Euclid's lemma: if a prime $p$ divides the product of two natural numbers $a$ and $b$, then either $p$ divides $a$ or $p$ divides $b$ (or both).
	
	If $n$ is prime, and therefore product of a unique prime integer, namely itself, the result is true and the proof is complete (say that a prime number is product of itself is obviously a misnomer!). Suppose that $n$ is not prime and therefore strictly greater than $1$ and consider the set:
	
	So, $D\subset \mathbb{N}$ and since $n$ is composite, we have that $D\neq \varnothing$. According to the principle of good order, $D$ has a smaller element $p_1$ that is prime, otherwise the minimum choice of $p_1$ is contradicted. We can the write $n=p_1n_1$. If $n_1$ is prime, then the proof is complete. If $n_1$ is also composite, then we repeat the same argument as before and we deduce the existence of a prime number $p_2$ and of an integer $n_2<n_1$, such as $n=p_1p_2n_2$. By continuing we come inevitably to the conclusion that $n_k$ will be prime.
	
	So finally we have well shown that any number can be decomposed into prime numbers factors with the principle of good order.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	We do not know to this day a simple law that allows to calculate the $n$-th prime factor $p_n$. Thus, to know if an integer $m$ is a prime, it is almost easier at this date to verify its presence in a table of prime numbers.

	In fact, we use nowadays the following method:

	Given an integer $m$, if we want to determine whether it is prime or not, we calculate if it is divisible by the primes number $p_n$ belonging to the set:
	

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	The integer $223$ is neither divisible by $2$ or by $3$ or by $5$ or by $7$, or by $11$, or by $13$. It is useless to continue with the next prime number, because $17^2=289>223$. We conclude therefore that the number $223$ is prime.
	\end{tcolorbox}
	
	\subsubsection{Congruences (modular arithmetic)}\label{congruence}
	Modular arithmetic is a system of arithmetic for integers, where numbers "wrap around" upon reaching a certain value, named the "\NewTerm{modulus}" (plural moduli).
	
	A familiar use of modular arithmetic is in the $12$-hour clock (and also the calendar), in which the day is divided into two $12$-hour periods. If the time is 7:00 now, then $8$ hours later it will be 3:00. Usual addition would suggest that the later time should be $7 + 8 = 15$, but this is not the answer because clock time "wraps around" every $12$ hours; in $12$-hour time, there is no "$15$ o'clock". Likewise, if the clock starts at 12:00 (noon) and $21$ hours elapse, then the time will be $9:00$ the next day, rather than 33:00. Because the hour number starts over after it reaches $12$, this is arithmetic modulo $12$. According to the definition below, $12$ is congruent not only to $12$ itself, but also to $0$, so the time "12:00" could also be written "0:00", since $12$ is congruent to $0$ modulo $12$.
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/modular_arithmetics_clock.jpg}
		\caption[Time-keeping on a clock using modulo $12$ arithmetics]{Time-keeping on a clock using modulo $12$ arithmetics (source: Wikipedia)}
	\end{figure}
	\textbf{Definition (\#\thesection.\mydef):} Let $m\in\mathbb{Z}\setminus 0$. If $a$ and $b$ have the same remainder when divided by $m$ in the Euclidean division then we say "$a$ is congruent to $b$ modulo $m$", and we write:
	
	or equivalently there is (at least) one relative integer $k$ such that:
	
	Often denoted:
	
	where $q$ is the "quotient" and $r$ the "remainder" of the euclidean division.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	We have:
	$$51\equiv 1 \mod(10)$$
	as (we apply $a=b+km$):
	$$51=1+5\cdot 10$$
	And we have:
	$$-51\equiv 9 \mod(10)$$
	as (we apply again $a=b+km$):
	$$-51=9+(-6)\cdot 10$$
	\end{tcolorbox}
	We also name the number $b$ "\NewTerm{residue}\index{residue}". Thus, a residue is an integer congruent to another, modulo a given integer $m$. The reader can verify that this requires that:
	
	The case with negative number is many times difficult to grasp. So let us do a step by step in-deep companion example!
	
	Suppose we try to calculate $-3 \div 12$. A possible answer is:
	$$
	-3=-1 \cdot 12+9
	$$
	In other words, that $-3 \mod 12=9$.  But can't we represent this as:
	$$
	-3=-2 \cdot 12+21
	$$
	It's true that $-3 \equiv 21 \mod 12$. And also that $-3 \equiv 117 \mod 12$, and of course $-3 \equiv 9 \mod 12$ (or alternatively $-3 \mod 12=9$). All of these numbers are in the same "congruence or residue class". Typically the smallest non-negative value is used to represent the class by tradition such that $0\geq b < m$. 
	
	So we could write this congruence class as $\{\ldots,-15,-3,9,21,33, \ldots\}$ but that would become tedious so by convention we represent the congruence class by just one of its members, often the smallest non-negative member. So we represent $\{\ldots,-15,-3,9,21,33, \ldots\}$ by $9 \mod 12$.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	All spreadsheets and business intelligence softwares have a modulo function often denoted \texttt{MOD(value,congruence)} (that returns the $b$ of $a=b+km$) and used for various reasons. But the most used and well known practical case is the calculation of the fiscal month, given the civil month (for example the civil month starts obviously in January ($1$) but the fiscal month may start for a corporation in April ($4$). So how can we implement a formula that give the right column in the table below (in the practice you can't create the column by writing the fiscal months manually because most corporations work on a daily basis and have thousands of days for which they want to know what is the fiscal month!):
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|}
		\hline & \textbf{Civil Month} (CM) & \textbf{Fiscal Month} (FM) \\
		\hline JAN & $1$ & $(-3 \mod 12) + 1 = 9 +1 = 10$ \\
		\hline FEB & $2$ & $(-2 \mod 12) + 1 = 10 +1 =11$ \\
		\hline MAR & $3$ & $(-1 \mod 12) + 1 = 11 +1 =12$ \\
		\hline APR & $4$ & $(0 \mod 12) + 1 = 0 +1 =1$ \\
		\hline MAY & $5$ & $(1 \mod 12) + 1 = 1 +1 =2$ \\
		\hline JUN & $6$ & $(2 \mod 12) + 1 = 2 +1 =3$ \\
		\hline JUL & $7$ & $(3 \mod 12) + 1 = 3 +1 =4$ \\
		\hline AUG & $8$ & $(4 \mod 12) + 1 = 4 +1 =5$ \\
		\hline SEP & $9$ & $(5 \mod 12) + 1 = 5 +1 =6$ \\
		\hline OCT & $10$ & $(6 \mod 12) + 1 = 6 +1 =7$ \\
		\hline NOV & $11$ & $(7 \mod 12) + 1 = 7 +1 =8$ \\
		\hline DEC & $12$ & $(8 \mod 12) + 1 = 8 +1 =9$ \\
		\hline
		\end{tabular}
	\end{table}
	In the special case above where the fiscal month is April ($4$), in all spreadsheet softwares, business intelligence softwares, engineering softwares and programming languages, the formula would be:
	\begin{center}
		\texttt{Fiscal Month = MOD(Civil Month-4,12)+1}
	\end{center}
	Or written in more general way:
	\begin{center}
		\texttt{MOD(Civil Month-Fiscal Month Start,12)+1}
	\end{center}
	This is equivalent to:
	$$a=b+km \Leftrightarrow \texttt{b = MOD(a,m)} \Rightarrow \texttt{FM=MOD(CM-FMS,12)+1} $$
	\end{tcolorbox}
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} The reader must well understand that congruence implies a null remainder for the division!\\
	
	\textbf{R2.} We exclude in addition to the $0$ also the $1$ and $-1$for the possible values of $m$ in the definition of congruence in some books.\\
	
	\textbf{R3.} Behind the term congruence are hidden similar concepts of different levels of abstraction:
	\begin{itemize}
		\item In modular arithmetic, so we say that "two integers $a$ and $b$ are congruent modulo $m$ if they have the same remaining in the Euclidean division by $m$". We can also say that they are congruent modulo $m$ if their difference is a multiple of $m$.

		\item In the study of oriented angles, we say that "two measurements are congruent modulo $2\pi$ [rad] if and only if their difference is a multiple of $2\pi$ [rad]". This characterize two measures of the same angle (\SeeChapter{see section Trigonometry page \pageref{periodicity trigonometric functions}}).

		\item In algebra, we speak of congruence modulo $I$ in a commutative ring (\SeeChapter{see section Set Theory page \pageref{ring}}) for which $I$ is an ideal: "$x$ is congruent to $y$ modulo $I$ if and only if their difference belongs to $I$". This congruence is an equivalence relation compatible with the operations of addition and multiplication and gives the possibility to define a quotient ring of the parent set with its ideal $I$.

		\item We sometimes see in the study of geometry (\SeeChapter{see section Euclidean Geometry page \pageref{congruence axioms}}) the term "congruence" used in place of "similar". It is then a simple equivalence relation on the set of plane figures.
	\end{itemize}
	\end{tcolorbox}
	The relation of congruence $\equiv$ is an equivalence relation (\SeeChapter{see section Operators page \pageref{equivalence relation}}), in other words, given $a,b,c,m\in\mathbb{Z},m>1$ then the congruence relation is:
	\begin{enumerate}
		\item[P1.] Reflexive:
		
		\item[P2.] Symmetric:
		
		\item[P3.] Transitive:
		
	\end{enumerate}
	The properties P1 and P2 are obvious (if this is not the case please let us know we will develop!). We will prove only P3.
	\begin{dem}
	The assumptions imply that:
	
	 But then:
	
	This proves that $a$ and $c$ are congruent modulo $m$.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	The relation of congruence $\equiv$ is compatible with the sum and the product (remember that power is ultimately an extension of the product!).
	
	Indeed, given $(a,b,a',b',m)\in\mathbb{Z},m>1$ such that $a\equiv \mod(m)$ and $a'\equiv b'$ then:
	\begin{enumerate}
		\item[P1.] $a+a'\equiv b+b'\mod(m)$

		\item[P2.] $aa'\equiv bb'\mod(m)$
	\end{enumerate}
	\begin{dem}
	We have:
	
	by hypothesis. But then:
	
	which proves P1. We also have:
	
	which proves P2.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The congruence relation behaves in many point like the relation of equality. However a property of the relation of equality $=$ is not true for that of congruence  $\equiv$, namely the simplification: If $ab\equiv \mod(m)$, we do not have  necessarily $b\equiv c \mod(m)$.
	\end{tcolorbox}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\
	
	\end{tcolorbox}
	So far we have seen the properties of congruences involving a single modulus. We will now study the behaviour of the congruence relation on a change of modulus.
	\begin{enumerate}
		\item[P1.] If $a\equiv b \mod(m)$ and $d|m$, then $a\equiv b \mod(d)$
		\item[P2.] If $a\equiv b$ and $a\equiv b \mod(s)$ then $a$ and $b$ are congruate modulus $[r,s]$
	\end{enumerate}
	We think this two properties are obvious. We do not need to go into details for P1. For P2, since $b-a$ is a multiple of $r$ and $s$ since by hypothesis:
	
	$b-a$ is then a multiple of the LCM of $r$ and $s$, which proves P2.
	
	From these properties it follows that if we denote by $f(x)$ a polynomial with integer coefficients (positive or negative):
	
	The congruence $a \equiv b \mod(m)$ will also give $f(a)\equiv f(b) \mod(m)$.

	If we replace $x$ successively by all integers in a polynomial $f(x)$ with integer coefficients, and if we take the remaining modulus $m$, these remaining are reproduced from $m$ to $m$ (in the sense where the congruence is satisfied), since we have, regardless of the number $m$ and $x$:
	
	We then deduce then the impossibility of solving the following congruence:
	
	with integer numbers, if $r$ is anyone of the "non-remaining" (a residue that does not satisfy the congruence).
	
	\paragraph{Congruence Class}\mbox{}\\\\
	\textbf{Definition (\#\thesection.\mydef):} We name "\NewTerm{modulo $m$ congruence class}\index{module congruence class}", the subset of the set $\mathbb{Z}$ defined by the property that two elements $a$ and $b$ of $\mathbb{Z}$ are in the same class if and only if $a\equiv b \mod(m)$ or that a set of elements are congruent by this same modulo.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	We saw in the section Operators that this is in fact an equivalence class (page \pageref{equivalence class}) as the congruence modulo $m$ is, as we have proved above, a relation of equivalence!!!
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Given $m=3$. We divide the set of integers into congruence classes of modulo $3$. Here are for example three sets whose elements are congruent with one another without rest (see well what gives the set of all these classes together!):
	
	Thus we see that for each pair of elements of a congruence class, the congruence modulo $3$ exists. However, we see that we can not take that $-9\equiv -8 \mod(3)$ where $-9$ is in the first class and $-8$ in the second.\\
	
	The smallest non-negative number of the first class is $0$, this of the second is $1$ and the last is $2$. Thus, we will denote these three classes respectively $[0]_3,[1]_3,[2]_3$, the number $3$ in the index indicating the modulus.\\
	
	It is interesting to notice that if we take any number of the first class and any number of the second class, then their sum is always in the second class. This can be generalized and allows to define a sum of classes modulo $3$ by writing:
	
	and also:
	
	\end{tcolorbox}
	Thus, for any $m>1$, the congruence class:
	
	is the set of integers congruent to a modulo $m$ (and congruent modulo $m$ between them)!!! This class is denoted by:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Having bracketed the expression "and congruent modulo $m$ between them" is due to the fact that the congruence, being an equivalence relation we have as we have proved above that $b\equiv a \mod(m)$, $c\equiv \mod(m)$, then $b\equiv a\mod(m)$.
	\end{tcolorbox}
	
	\textbf{Definition (\#\thesection.\mydef):} The set of congruence classes $[a]_m$ (that forms by the fact that congruence is an equivalence relation: "equivalence classes"), for a fixed $m$ gives what we name a "\NewTerm{quotient set}\index{quotient set}" (\SeeChapter{see section Operators page \pageref{quotient set}}). More rigorously, we speak of the "quotient set of $\mathbb{Z}$ by the congruence relation" whose elements are the congruence classes (or: equivalence classes) and then form the ring $\mathbb{Z}/m\mathbb{Z}$.
	
	
	We deduce from the definition the following two trivial properties:
	\begin{enumerate}
		\item The number $b$ is in the class $[a]_m$ if and only if $a\equiv b\mod(m)$
		\item The classes $[a]_m$ and $[b]_m$ are equal if and only if $a\equiv b\mod(m)$
	\end{enumerate}
	\begin{theorem}
	There are exactly $m$ different congruence classes of modulo $m$, ie $[0]_m,[1]_m,\ldots,[m-1]_m$.
	\end{theorem}
	\begin{dem}
	Given $m>1$, than any integer $a$ is congruent modulo $m$ to one and only one integer $r$ of the set $\{0,1,2,\ldots,m-1\}$ (notice well, it is important, that we restrict ourselves to the positive integers without taking into account the negative one!). In addition, this integer $r$ is exactly the remaining of the division of $a$ by $m$. In other words, if $0\leq r<m$, then:
	
	if and only if $a=qm+r$ where $q$ is the quotient of $a$ by $m$ and $r$ is the remainder. The proof is an immediate consequence of the definition of the congruence and of the Euclidean division.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\textbf{Definition (\#\thesection.\mydef):} An integer $b$ in a congruence class modulo $m$ is named a "\NewTerm{representative of this class}\index{representative of a class}" (it is clear that by the equivalence relation that two representative of the same class are congruent modulo $m$ to each other).
	
	We can now be able to build an addition and a multiplication on the congruence classes. To define the sum of two classes $[a]_m,[b]_m$, it suffices to take one representative from each class, to their sum and take the congruence class of the result. Thus (see examples above):
	
	And same for the multiplication:
	
	By construction of the addition and multiplication, we see that $0$ (zero) is the neutral element for addition:
	
	and the class of the integer $1$ is the neutral element for multiplication:
	
	\textbf{Definition (\#\thesection.\mydef):} An element $[a]_m$ of $\mathbb{Z}/m\mathbb{Z}$ is "\NewTerm{one unit}" if there is an element $[b]_m\in \mathbb{Z}/m\mathbb{Z}$ such that $[a]_m\cdot[b]_m$.
	The following theorem helps to characterize classes modulo $m$ which are units in $\mathbb{Z}/m/\mathbb{Z}$:
	\begin{theorem}
	Given $[a]$ an element of $\mathbb{Z}/m/\mathbb{Z}$. Then $[a]$ is a unit if and only if $(a,m)=1$.
	\end{theorem}
	\begin{dem}
	Suppose first that $(a,m)=1$. Then by Bézout's theorem, we have its identity:
	
	In other words, $as$ is congruent to $1$ modulo $m$. But this is equivalent to write by definition that $[a][s]=1$ showing that $[a]$ is a unit. Conversely, if $[a]$ is a unit, this implies that there exists a class $[s]$ such that $[a][s]=1$.
	
	Thus, we have just proved that $\mathbb{Z}/\mathbb{Z}$ is indeed a ring since it has an addition, a multiplication, a neutral element and an inverse!!
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\paragraph{Complete set of residues}\mbox{}\\\\
	\textbf{Definition (\#\thesection.\mydef):} A set of numbers $a_0, a_1, \ldots,a_{(m-1)} \mod (m)$ form a "\NewTerm{complete set of residues}\index{complete set of residue}", also named a "\NewTerm{covering system}\index{covering system}", if they satisfy $a_i=i \mod (m) $ for $i=0, 1, \ldots, m-1$. 

	This type of systems will help us to introduce in the section of Cryptography (page \pageref{cryptography}) an important function used to secured communication devices at the end of the 120th century (holocene calendar) and at beginning of the 121st century.

	To introduce this concept, consider the following finite system of congruences modulo $6$:
	
	where as the reader will have probably noticed it: no residue is repeated in the list and no residue taken in pairs are congruent between them modulo $m$ (is this last point that oblige to stop at $5$ in our example). We then say that the residues are "\NewTerm{mutually incongruent}\index{mutually incongruent}".

	If these conditions are met, then we say that the ordered set $\{6, 13, 2, -3, 22, 11\}$ is a "complete system of residues modulo $m$" as already defined. Such a set is not unique for a given module. Thus, the set $\{0, 1, 2, 3, 4, 5\}$ is also a complete  (trivial) system of residues modulo $6$.
	
	If we eliminate from this entire system all numbers that are not prime to $m$, then we have a "\NewTerm{system of reduced residue modulo $m$}\index{system of reduced residue}\label{system of reduced residue}". So in the above example, the reduced residue system modulo $6$ will be $\{13, 11\}$.
	
	Reduces systems will be useful to us in the section Cryptography (page \pageref{cryptography}) to prove an important result in the asymmetric public key systems.

	We will see also in the section Cryptography (page \pageref{euler indicator function cryptography}), the "\NewTerm{Euler indicator function}\index{Euler indicator function}\label{euler indicator function}" when $m$ is prime (which is not the case in the previous example) gives the cardinal of the reduced system modulus $m$ as being equal to:
	
	So under the assumption condition that $m$ is prime, the reduced system of residue is obviously written:
	
	
	\paragraph{Chinese remainder theorem}\mbox{}\\\\
	In its basic form, the Chinese remainder theorem will determine a number $n$ that, when divided by some given divisors, leaves given remainders. In Sun Tzu's example (stated in modern terminology), what is the smallest number $n$ that when divided by $3$ leaves a remainder of $2$, when divided by $5$ leaves a remainder of $3$, and when divided by $7$ leaves a remainder of $2$?
	
	The Chinese remainder theorem can therefore be seen as solving a linear system but in a modular system. For many students and future engineers, this theorem will never be used in practice, but some will see it again it in the field of cryptography (in the context of decryption especially).
	
	There are several possible proofs as always but we opted for the one that, as always for us, seemed the most educational.

	Given $m$ and $n$ both prime integers between them. Then special case of a system of two congruences (see further below for an example of resolution of a system of three congruences):
	
	has a unique solution.
	\begin{dem}
	As $m$ and $n$ are assumed as prime between them, there exists then $u$ and $v$ two integers such as (application of the Bézout identity proved earlier above):
	
	Therefore we have:
	
	That is to say:
	
	Then we have also by extension:
	
	That is to say:
	
	So to be clear, we have so far:
	
	We then have for recall:
	
	But we can also write with $k\in\mathbb{Z}$:
	
	Therefore:
	
	Then we also have:
	
	But we can also write with $k\in\mathbb{Z}$:
	
	Therefore:
	
	So to be always clear, we have so far:
	
	So finally we get that:
	
	is a particular solution of the system. But we also have $\forall i,j\in\mathbb{Z}$ by the definition of the congruence:
	
	So that $x$ is always solution of the system, we must have:
	
	and therefore:
	
	Therefore a little bit more general solution is:
	
	But by extension, we have the general solution:
	
	with $z\in\mathbb{Z}$. We then say sometimes that the solution is "$x$ modulo $nm$".
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	As an example, consider the problem of finding an integer $x$ such that:
	
	A brute-force approach converts these congruences into sets and writes the elements out to the product of $3\cdot 4\cdot 5 = 60$ (the solutions modulo $60$ for each congruence):
	
	To find an $x$ that satisfies all three congruences, intersect the three sets to get:
	
	This solution is modulo $60$, hence all solutions are expressed as:
	
	Another way to find a solution is with basic algebra, modular arithmetic, and stepwise substitution.\\
	
	We start by translating these congruences into equations for some $t$, $s$, and $u$:
	
	After we substitute the $x$ from the first equation into the second congruence:
	
	That is to say:
	
	Hence:
	
	meaning that $t = 3 + 4s$ for some integer $s$. Substitute now $t$ into the first equation:
	
	Substitute this $x$ into the third congruence:
	
	That is to say:
	
	Hence:
	
	meaning that $s = 0 + 5u$ for some integer $u$. Finally:
	
	So, we have solution $\{11,71,131,191,\ldots\}$.
	\end{tcolorbox}
	
	\subsubsection{Continued fraction}\label{continued fraction}
	
	\textbf{Definition (\#\thesection.\mydef):} A "\NewTerm{continued fraction}\index{continued fraction}" is an expression obtained through an iterative process of representing a number as the sum of its integer part and the reciprocal of another number, then writing this other number as the sum of its integer part and another reciprocal, and so on. In a finite continued fraction (or terminated continued fraction), the iteration/recursion is terminated after finitely many steps by using an integer in lieu of another continued fraction. In contrast, an infinite continued fraction is an infinite expression. In either case, all integers in the sequence, other than the first, must be positive. The integers $a_i$ are named the "coefficients" or "terms" of the continued fraction.
	
	The notion of continued fraction come back from the time of Fermat and culminated with the work of Lagrange and Legendre in the late 118th century (holocene calendar). These fractions are important in physics because we find them back in our study of acoustic and also in the thought process that led Galois to create his group theory and also in the studies of gear ratios (for watch complications as discussed in the section of Mechanical Engineering page \pageref{gears association}).

	To understand the motivation of continued fraction let us introduce a basic example.
	
	Consider a typical rational number:
	
	which is around $4.4624$.

	As first approximation, start with $4$, which is the integer part:
	
	Note that the fractional part is the reciprocal of $93/43$ which is about $2.1628$. Use the integer part, $2$, as an approximation for the reciprocal, to get a second approximation of:
	
	So we have so far:
	
	Note that the fractional part is the reciprocal of $43/7$ which is about $6.1429$. Use the integer part, $6$, as an approximation for the reciprocal, to get a second approximation of:
	
	Therefore:
	
	Note that the fractional part $1/7$ is the reciprocal of $7$ which is about... $7$ Use the integer part, $7$, as an approximation for the reciprocal, to get a second approximation of:
	
	Therefore we get:
	
	This expression is named as we know the "continued fraction representation of the number".
	
	Dropping some of the less essential parts of the expression:
	
	gives the abbreviated notation:
	 
	Note that it is customary to replace only the first comma by a semicolon.

	 As generalization of the previous example let us consider in a first time the rational number $a / b$ with $(a,b)=1$ with $b>0$ and $a>b$. We know that all the quotients $q_i$ and the remaining $r_i$ are within the scope of the Euclidean division positive integers.

	Let us recall that the Euclidean algorithm already seen earlier (but written in a slightly different way):
	
	By successive substitutions, we get:
	
	What is also sometimes written:
	
	So any positive rational number can be expressed as a finite continued fraction where $q_n\in\mathbb{N}$.
	
	Taking our introducing example:
	
	we notice indeed that $q_n\in\mathbb{N}$ and that we have by construction:
	
	where the brackets represent the integer part and that we also have:
	 
	The development of the number $a / b$ is named the "\NewTerm{development of the number $a / b$ in finite continued fraction}\index{infinite fraction}" and is condensed in the following notation:
	
	Let us see now another example:
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us see how the extract the square root of a number $A$ (for example $A=2$ such that we want to extract $\sqrt{2}$) by the continued fraction method.\\

	Given $a$ the largest integer whose square $a^2$ is smaller than $A$. We subtract it to $A$. So there is a remaining of (for $A=2$, we have $a=1$):
	
	where we have used a remarkable identities that we will prove in the section Calculus later (page \pageref{calculus remarkable identities}). Hence dividing both members by the second parenthesis, we have:
	
	Therefore:
	
	In the denominator, we replace $\sqrt{A}$ by:
	
	That gives:
	
	etc... we thus see that the system is simple to determine the expression of a root square in terms of continued fraction.
	\end{tcolorbox}
	We consider now as intuitive that every rational number can be expressed as finite continued fraction and conversely that any finite continued fraction represents a rational number. By extension, an irrational number is represented by an infinite continued fraction!

	Now consider $[q_1;q_2,q_3,\ldots,q_n]$ a finite continued fraction. The continued fraction:
	
	where $k=1,2,\ldots,n$ is named the "\NewTerm{$k$-th reduced}\index{$k$-reduced}" or "\NewTerm{$k$-th convergent}\index{$k$-convergent}" or the "\NewTerm{$k$-th partial quotient}\index{$k$-th partial quotient}".

	With this notation, we have:
	
	To simplify the expressions above, we introduce the sequences $\{n_i\},\{d_i\}$ ($n$ is for \textbf{n}umerator and $d$ for \textbf{d}enominator) defined by:
	
	Thanks to this construction, we have an interesting immediate little inequality that will be useful to us further below:
	
	With the above definition, we find that:
	
	Either by generalizing:
	
	Now let us show for later use that for $i\geq 1$, we have:
	
	The result is immediate for $i=1$. Assuming that the result is true for $i$ let us show that it is true for $i+1$. Since:
	
	then using the induction hypothesis, we get the result!
	
	We can now establish a vital relation for what will follow.
	\begin{theorem}
	Let us prove that if $C_k$ is the $k$-th reduced to of the simple finite continued fraction $[q_1;q_2,\ldots,q_n]$ then:
	
	\end{theorem}
	\begin{dem}
	
	as:
	
	therefore:
	
	indicating to us that the sign of $C_{k+1}-C_k$ is the same as $(-1)^{k+1}$.

	It follows that $C_{k+2}>C_k$ for an odd $k$, and $C_{k+2}<C_k$ for $k$ even. Then:
	
	and after as:
	
	So for $k$ even, we have $C_k>C_{k-1}$, we therefore deduce that:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Let us show now that every infinite continued fraction can be any irrational number.
	
	In formal terms, if $\{q_n\}$ is a sequence of positive integers and that we consider $C_n=[q_1;q_2,\ldots,q_n]$ then it necessarily converges to a real number if $n\rightarrow +\infty$.

	Actually it is not difficult to observe (it's quite intuitive) with a practical example that we have:
	
	when $k\rightarrow +\infty$.
	
	Now, let us denote by $x$ any real number and $q_1=[x]$ the integer part of this real number. Then we saw at the beginning of our study of continued fractions that:
	
	Therefore it comes:
	
	
	Let's look for the needs of the section on Acoustics (page \pageref{acoustic}) on the calculation of a continued fraction of a logarithm using the previous relation!

	First let us recall that:
	
	That is (relation proved in the section of Functional Analysis page \pageref{logarithms}):
	
	with $1<a<u$ and $(a,u)=1$.
	
	Given $y_n$ defined by:
	
	Therefore let us prove that:
	
	Indeed for $n=1$ we have:
	
	for $n=2$ we use first the fact that:
	
	
	Therefore:
	
	and as we had proved that:
	
	etc... by induction demonstrating our right to use this notation changes.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us look for the expression of the continuous fraction of:
	
	We know by playing with the definition of the logarithm that:
	
	therefore:
	
	therefore $q_1=1$. Then we have:
	
	and as:
	
	it comes:
	
	So we have the first partial quotient:
	
	Verbatim we have already:
	
	Let us simplify:
	
	So the first partial quotient can be written:
	
	and let us go to the second partial quotient. We already know for this that:
	
	So it is immediate that $q_2=1$ and then as:
	
	We have:
	
	Finally we get:
	
	etc... etc.
	\end{tcolorbox}
	
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{90} & \pbox{20cm}{\score{4}{5} \\ {\tiny 21 votes, 68.57\%}} 
	\end{tabular} 
	\end{flushright}
	
	
	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Set Theory}\label{set theory}
	\lettrine[lines=4]{\color{BrickRed}D}{uring} our study of numbers, operators, and number theory (in the chapters of the same name), we often used the terms "groups", "rings", "fields", "homomorphism", etc. and thereafter we will continue to do it again many times. Besides the fact that these concepts are of utmost importance, to give demonstrations or build mathematical concepts essential to the study of contemporary theoretical physics (quantum field physics, string theory, standard model, ... ), they allow us to understand the components and the basic properties of mathematics and its operators by storing them in separate categories. So, choose to present the Set Theory as the 5th chapter of this book is a very questionable choice as rigorously that it is where almost everything begins... However, we still needed to expose the Proof Theory for the notations and methods that will be used here.
	
	Moreover, when teaching modern mathematics in the secondary or primary (in the years 11970 according to holocene calendar), the language of sets and the preliminary study of binary relations to a more rigorous approach to the notion of functions and applications of mathematics in general was introduced (see definition below).

	\textbf{Definition (\#\thesection.\mydef):} We talk about "\NewTerm{arrow diagram}\index{arrow diagram}" (or "\NewTerm{sagittal diagram}\index{sagittal diagram}" from latin "sagitta" = arrow) to all diagram showing a correspondence between the two sets of components connected wholly or partially by a set of arrows. When there is no arrows, we then just speak of "\NewTerm{Venn diagrams}\index{Venn diagrams}\label{Venn diagrams}".

	For example, the graphical representation of a defined function of the set $E=\left\lbrace -3,-2,-1,0,1,2,3\right\rbrace $ to the set $F=\left\lbrace 0,1,2,4,9\right\rbrace $ (usually denoted $E\mapsto F$) leads to the sagittal diagram below:

	\begin{figure}[H]
	\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,651); %set diagram left start at 0, and has height of 651
		
		%Shape: Ellipse [id:dp3766795265336349] 
		\draw  [fill={rgb, 255:red, 248; green, 231; blue, 28 }  ,fill opacity=1 ] (71,159.5) .. controls (71,90.74) and (111.18,35) .. (160.75,35) .. controls (210.32,35) and (250.5,90.74) .. (250.5,159.5) .. controls (250.5,228.26) and (210.32,284) .. (160.75,284) .. controls (111.18,284) and (71,228.26) .. (71,159.5) -- cycle ;
		%Shape: Ellipse [id:dp2753017835292213] 
		\draw  [fill={rgb, 255:red, 248; green, 231; blue, 28 }  ,fill opacity=1 ] (395,158.5) .. controls (395,89.74) and (435.18,34) .. (484.75,34) .. controls (534.32,34) and (574.5,89.74) .. (574.5,158.5) .. controls (574.5,227.26) and (534.32,283) .. (484.75,283) .. controls (435.18,283) and (395,227.26) .. (395,158.5) -- cycle ;
		%Shape: Circle [id:dp17205307135357462] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (160,68) .. controls (160,66.9) and (160.9,66) .. (162,66) .. controls (163.1,66) and (164,66.9) .. (164,68) .. controls (164,69.1) and (163.1,70) .. (162,70) .. controls (160.9,70) and (160,69.1) .. (160,68) -- cycle ;
		%Shape: Circle [id:dp6441097412108119] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (133,110) .. controls (133,108.9) and (133.9,108) .. (135,108) .. controls (136.1,108) and (137,108.9) .. (137,110) .. controls (137,111.1) and (136.1,112) .. (135,112) .. controls (133.9,112) and (133,111.1) .. (133,110) -- cycle ;
		%Shape: Circle [id:dp28121802677024865] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (204,136) .. controls (204,134.9) and (204.9,134) .. (206,134) .. controls (207.1,134) and (208,134.9) .. (208,136) .. controls (208,137.1) and (207.1,138) .. (206,138) .. controls (204.9,138) and (204,137.1) .. (204,136) -- cycle ;
		%Shape: Circle [id:dp4026432821686903] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (129,164) .. controls (129,162.9) and (129.9,162) .. (131,162) .. controls (132.1,162) and (133,162.9) .. (133,164) .. controls (133,165.1) and (132.1,166) .. (131,166) .. controls (129.9,166) and (129,165.1) .. (129,164) -- cycle ;
		%Shape: Circle [id:dp37421880409266595] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (201,196) .. controls (201,194.9) and (201.9,194) .. (203,194) .. controls (204.1,194) and (205,194.9) .. (205,196) .. controls (205,197.1) and (204.1,198) .. (203,198) .. controls (201.9,198) and (201,197.1) .. (201,196) -- cycle ;
		%Shape: Circle [id:dp28183498588700284] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (136,226) .. controls (136,224.9) and (136.9,224) .. (138,224) .. controls (139.1,224) and (140,224.9) .. (140,226) .. controls (140,227.1) and (139.1,228) .. (138,228) .. controls (136.9,228) and (136,227.1) .. (136,226) -- cycle ;
		%Shape: Circle [id:dp548195184521429] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (176,255) .. controls (176,253.9) and (176.9,253) .. (178,253) .. controls (179.1,253) and (180,253.9) .. (180,255) .. controls (180,256.1) and (179.1,257) .. (178,257) .. controls (176.9,257) and (176,256.1) .. (176,255) -- cycle ;
		%Shape: Circle [id:dp36554991081637245] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (473,61) .. controls (473,59.9) and (473.9,59) .. (475,59) .. controls (476.1,59) and (477,59.9) .. (477,61) .. controls (477,62.1) and (476.1,63) .. (475,63) .. controls (473.9,63) and (473,62.1) .. (473,61) -- cycle ;
		%Shape: Circle [id:dp09737534260332659] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (444,100) .. controls (444,98.9) and (444.9,98) .. (446,98) .. controls (447.1,98) and (448,98.9) .. (448,100) .. controls (448,101.1) and (447.1,102) .. (446,102) .. controls (444.9,102) and (444,101.1) .. (444,100) -- cycle ;
		%Shape: Circle [id:dp18460713099888748] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (513,98) .. controls (513,96.9) and (513.9,96) .. (515,96) .. controls (516.1,96) and (517,96.9) .. (517,98) .. controls (517,99.1) and (516.1,100) .. (515,100) .. controls (513.9,100) and (513,99.1) .. (513,98) -- cycle ;
		%Shape: Circle [id:dp056823657267702554] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (489,125) .. controls (489,123.9) and (489.9,123) .. (491,123) .. controls (492.1,123) and (493,123.9) .. (493,125) .. controls (493,126.1) and (492.1,127) .. (491,127) .. controls (489.9,127) and (489,126.1) .. (489,125) -- cycle ;
		%Shape: Circle [id:dp31128804498242935] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (484.75,158.5) .. controls (484.75,157.4) and (485.65,156.5) .. (486.75,156.5) .. controls (487.85,156.5) and (488.75,157.4) .. (488.75,158.5) .. controls (488.75,159.6) and (487.85,160.5) .. (486.75,160.5) .. controls (485.65,160.5) and (484.75,159.6) .. (484.75,158.5) -- cycle ;
		%Shape: Circle [id:dp058955545233141615] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (448.75,189.5) .. controls (448.75,188.4) and (449.65,187.5) .. (450.75,187.5) .. controls (451.85,187.5) and (452.75,188.4) .. (452.75,189.5) .. controls (452.75,190.6) and (451.85,191.5) .. (450.75,191.5) .. controls (449.65,191.5) and (448.75,190.6) .. (448.75,189.5) -- cycle ;
		%Shape: Circle [id:dp08222257577505254] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (503.75,199.5) .. controls (503.75,198.4) and (504.65,197.5) .. (505.75,197.5) .. controls (506.85,197.5) and (507.75,198.4) .. (507.75,199.5) .. controls (507.75,200.6) and (506.85,201.5) .. (505.75,201.5) .. controls (504.65,201.5) and (503.75,200.6) .. (503.75,199.5) -- cycle ;
		%Shape: Circle [id:dp556758747969998] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (456.75,227.5) .. controls (456.75,226.4) and (457.65,225.5) .. (458.75,225.5) .. controls (459.85,225.5) and (460.75,226.4) .. (460.75,227.5) .. controls (460.75,228.6) and (459.85,229.5) .. (458.75,229.5) .. controls (457.65,229.5) and (456.75,228.6) .. (456.75,227.5) -- cycle ;
		%Shape: Circle [id:dp9783386805236325] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (501.75,226.5) .. controls (501.75,225.4) and (502.65,224.5) .. (503.75,224.5) .. controls (504.85,224.5) and (505.75,225.4) .. (505.75,226.5) .. controls (505.75,227.6) and (504.85,228.5) .. (503.75,228.5) .. controls (502.65,228.5) and (501.75,227.6) .. (501.75,226.5) -- cycle ;
		%Shape: Circle [id:dp6953594723627954] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (463.75,257.5) .. controls (463.75,256.4) and (464.65,255.5) .. (465.75,255.5) .. controls (466.85,255.5) and (467.75,256.4) .. (467.75,257.5) .. controls (467.75,258.6) and (466.85,259.5) .. (465.75,259.5) .. controls (464.65,259.5) and (463.75,258.6) .. (463.75,257.5) -- cycle ;
		%Straight Lines [id:da7891149365327295] 
		\draw [color={rgb, 255:red, 184; green, 233; blue, 134 }  ,draw opacity=1 ]   (169,70) -- (452.81,251.26) ;
		\draw [shift={(454.5,252.33)}, rotate = 212.56] [color={rgb, 255:red, 184; green, 233; blue, 134 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da26614827317521716] 
		\draw [color={rgb, 255:red, 184; green, 233; blue, 134 }  ,draw opacity=1 ]   (189.5,255.33) -- (455.5,258.31) ;
		\draw [shift={(457.5,258.33)}, rotate = 180.64] [color={rgb, 255:red, 184; green, 233; blue, 134 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da46705817167746577] 
		\draw [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ]   (149.5,225.33) -- (475.54,160.72) ;
		\draw [shift={(477.5,160.33)}, rotate = 168.79] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da6544853379059046] 
		\draw [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ]   (147.5,110.33) -- (476.52,154.07) ;
		\draw [shift={(478.5,154.33)}, rotate = 187.57] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da2557235857908313] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (215.5,195.33) -- (438.64,107.07) ;
		\draw [shift={(440.5,106.33)}, rotate = 158.42] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da9105057381040726] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (217.5,136.33) -- (435.53,98.67) ;
		\draw [shift={(437.5,98.33)}, rotate = 170.2] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da012546201371525845] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ]   (141.5,163.33) -- (462.59,62.93) ;
		\draw [shift={(464.5,62.33)}, rotate = 162.64] [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		
		% Text Node
		\draw (135,59) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle -3$};
		% Text Node
		\draw (110,101) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle -2$};
		% Text Node
		\draw (181,126) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle -1$};
		% Text Node
		\draw (115,156) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 0$};
		% Text Node
		\draw (178,187) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle +1$};
		% Text Node
		\draw (112,217) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle +2$};
		% Text Node
		\draw (151,246) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle +3$};
		% Text Node
		\draw (481,50) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 0$};
		% Text Node
		\draw (451,91) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 1$};
		% Text Node
		\draw (519,89) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 2$};
		% Text Node
		\draw (497,117) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 3$};
		% Text Node
		\draw (492,150) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 4$};
		% Text Node
		\draw (457,181) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 5$};
		% Text Node
		\draw (512,191) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 6$};
		% Text Node
		\draw (464,219) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 7$};
		% Text Node
		\draw (509,221) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 8$};
		% Text Node
		\draw (472,250) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 9$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Sagittal diagram example from a definition set to a destination set}
	\end{figure}

	A relation from $E$ to $E$ (usually denoted $E\mapsto E$) provides an arrow diagram of the type:

	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,651); %set diagram left start at 0, and has height of 651
		
		%Shape: Ellipse [id:dp3766795265336349] 
		\draw  [fill={rgb, 255:red, 248; green, 231; blue, 28 }  ,fill opacity=1 ] (325.75,42.75) .. controls (394.51,42.75) and (450.25,82.93) .. (450.25,132.5) .. controls (450.25,182.07) and (394.51,222.25) .. (325.75,222.25) .. controls (256.99,222.25) and (201.25,182.07) .. (201.25,132.5) .. controls (201.25,82.93) and (256.99,42.75) .. (325.75,42.75) -- cycle ;
		%Shape: Circle [id:dp28121802677024865] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (310,75) .. controls (310,73.9) and (310.9,73) .. (312,73) .. controls (313.1,73) and (314,73.9) .. (314,75) .. controls (314,76.1) and (313.1,77) .. (312,77) .. controls (310.9,77) and (310,76.1) .. (310,75) -- cycle ;
		%Shape: Circle [id:dp4026432821686903] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (243,130) .. controls (243,128.9) and (243.9,128) .. (245,128) .. controls (246.1,128) and (247,128.9) .. (247,130) .. controls (247,131.1) and (246.1,132) .. (245,132) .. controls (243.9,132) and (243,131.1) .. (243,130) -- cycle ;
		%Shape: Circle [id:dp37421880409266595] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (295,178) .. controls (295,176.9) and (295.9,176) .. (297,176) .. controls (298.1,176) and (299,176.9) .. (299,178) .. controls (299,179.1) and (298.1,180) .. (297,180) .. controls (295.9,180) and (295,179.1) .. (295,178) -- cycle ;
		%Shape: Circle [id:dp28183498588700284] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (379,169) .. controls (379,167.9) and (379.9,167) .. (381,167) .. controls (382.1,167) and (383,167.9) .. (383,169) .. controls (383,170.1) and (382.1,171) .. (381,171) .. controls (379.9,171) and (379,170.1) .. (379,169) -- cycle ;
		%Shape: Circle [id:dp548195184521429] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (409,110) .. controls (409,108.9) and (409.9,108) .. (411,108) .. controls (412.1,108) and (413,108.9) .. (413,110) .. controls (413,111.1) and (412.1,112) .. (411,112) .. controls (409.9,112) and (409,111.1) .. (409,110) -- cycle ;
		%Curve Lines [id:da3553130518849785] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (253.5,128.33) .. controls (275.81,130.27) and (281.18,141.62) .. (290.61,171.51) ;
		\draw [shift={(291.5,174.33)}, rotate = 252.65] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Shape: Arc [id:dp7613051990474817] 
		\draw  [draw opacity=0][line width=1.5]  (246.49,134.77) .. controls (247.39,137.06) and (247.61,139.59) .. (246.98,142.08) .. controls (245.27,148.87) and (237.91,152.86) .. (230.55,151) .. controls (223.19,149.14) and (218.61,142.14) .. (220.32,135.35) .. controls (222.03,128.57) and (229.39,124.58) .. (236.75,126.44) .. controls (238.23,126.81) and (239.6,127.39) .. (240.83,128.14) -- (233.65,138.72) -- cycle ; \draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (246.49,134.77) .. controls (247.39,137.06) and (247.61,139.59) .. (246.98,142.08) .. controls (245.27,148.87) and (237.91,152.86) .. (230.55,151) .. controls (223.19,149.14) and (218.61,142.14) .. (220.32,135.35) .. controls (222.03,128.57) and (229.39,124.58) .. (236.75,126.44) .. controls (236.9,126.47) and (237.05,126.51) .. (237.19,126.56) ; \draw [shift={(240.83,128.14)}, rotate = 187.8] [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (13.4,-6.43) -- (0,0) -- (13.4,6.44) -- (8.9,0) -- cycle    ; 
		%Shape: Arc [id:dp013488124452606831] 
		\draw  [draw opacity=0][line width=1.5]  (315.23,71.74) .. controls (317.02,70.05) and (319.28,68.9) .. (321.83,68.55) .. controls (328.76,67.6) and (335.21,72.93) .. (336.24,80.46) .. controls (337.27,87.98) and (332.48,94.85) .. (325.55,95.79) .. controls (318.62,96.74) and (312.17,91.41) .. (311.14,83.88) .. controls (310.93,82.37) and (310.96,80.88) .. (311.2,79.46) -- (323.69,82.17) -- cycle ; \draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (318.55,69.5) .. controls (319.57,69.03) and (320.67,68.71) .. (321.83,68.55) .. controls (328.76,67.6) and (335.21,72.93) .. (336.24,80.46) .. controls (337.27,87.98) and (332.48,94.85) .. (325.55,95.79) .. controls (318.62,96.74) and (312.17,91.41) .. (311.14,83.88) .. controls (310.93,82.37) and (310.96,80.88) .. (311.2,79.46) ;  \draw [shift={(315.23,71.74)}, rotate = 345.19] [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (13.4,-6.43) -- (0,0) -- (13.4,6.44) -- (8.9,0) -- cycle    ;
		%Shape: Arc [id:dp6195471932893442] 
		\draw  [draw opacity=0][line width=1.5]  (405.31,110.69) .. controls (402.93,110.07) and (400.75,108.77) .. (399.1,106.79) .. controls (394.62,101.42) and (395.71,93.12) .. (401.54,88.26) .. controls (407.36,83.39) and (415.72,83.79) .. (420.21,89.16) .. controls (424.69,94.53) and (423.6,102.83) .. (417.77,107.7) .. controls (416.6,108.68) and (415.33,109.44) .. (414,110) -- (409.65,97.98) -- cycle ; \draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (405.31,110.69) .. controls (402.93,110.07) and (400.75,108.77) .. (399.1,106.79) .. controls (394.62,101.42) and (395.71,93.12) .. (401.54,88.26) .. controls (407.36,83.39) and (415.72,83.79) .. (420.21,89.16) .. controls (424.69,94.53) and (423.6,102.83) .. (417.77,107.7) .. controls (417.66,107.8) and (417.54,107.89) .. (417.42,107.99) ; \draw [shift={(414,110)}, rotate = 313.76] [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (13.4,-6.43) -- (0,0) -- (13.4,6.44) -- (8.9,0) -- cycle    ; 
		%Shape: Arc [id:dp8472061340307231] 
		\draw  [draw opacity=0][line width=1.5]  (384.86,165.55) .. controls (387.02,164.37) and (389.5,163.83) .. (392.06,164.15) .. controls (399,164.99) and (403.89,171.79) .. (402.97,179.33) .. controls (402.05,186.87) and (395.67,192.29) .. (388.73,191.44) .. controls (381.78,190.6) and (376.9,183.8) .. (377.82,176.26) .. controls (378.01,174.74) and (378.41,173.31) .. (379,172) -- (390.39,177.8) -- cycle ; \draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (384.86,165.55) .. controls (387.02,164.37) and (389.5,163.83) .. (392.06,164.15) .. controls (399,164.99) and (403.89,171.79) .. (402.97,179.33) .. controls (402.05,186.87) and (395.67,192.29) .. (388.73,191.44) .. controls (381.78,190.6) and (376.9,183.8) .. (377.82,176.26) .. controls (377.84,176.11) and (377.86,175.96) .. (377.88,175.81) ; \draw [shift={(379,172)}, rotate = 90.59] [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (13.4,-6.43) -- (0,0) -- (13.4,6.44) -- (8.9,0) -- cycle    ; 
		%Shape: Arc [id:dp8932401569587372] 
		\draw  [draw opacity=0][line width=1.5]  (295.95,171.88) .. controls (295.5,169.46) and (295.77,166.93) .. (296.86,164.6) .. controls (299.83,158.27) and (307.81,155.75) .. (314.69,158.97) .. controls (321.56,162.2) and (324.73,169.94) .. (321.76,176.28) .. controls (318.79,182.61) and (310.8,185.13) .. (303.93,181.91) .. controls (302.55,181.26) and (301.31,180.43) .. (300.25,179.46) -- (309.31,170.44) -- cycle ; \draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (295.95,171.88) .. controls (295.5,169.46) and (295.77,166.93) .. (296.86,164.6) .. controls (299.83,158.27) and (307.81,155.75) .. (314.69,158.97) .. controls (321.56,162.2) and (324.73,169.94) .. (321.76,176.28) .. controls (318.79,182.61) and (310.8,185.13) .. (303.93,181.91) .. controls (303.79,181.84) and (303.65,181.78) .. (303.52,181.71) ; \draw [shift={(300.25,179.46)}, rotate = 18.76] [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (13.4,-6.43) -- (0,0) -- (13.4,6.44) -- (8.9,0) -- cycle    ; 
		%Curve Lines [id:da35203054181260907] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (289.5,180.33) .. controls (269.13,190.03) and (263.81,164.92) .. (252.56,138.76) ;
		\draw [shift={(251.5,136.33)}, rotate = 66.04] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Curve Lines [id:da21789353265193556] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (401.5,115.33) .. controls (362.5,114.36) and (357.72,132.4) .. (375.12,162.03) ;
		\draw [shift={(376.5,164.33)}, rotate = 238.5] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		%Curve Lines [id:da2339860861726053] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (381.5,160.33) .. controls (408.18,147.45) and (406.08,136.27) .. (404.8,125.21) ;
		\draw [shift={(404.5,122.33)}, rotate = 85.24] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (14.21,-4.28) .. controls (9.04,-1.82) and (4.3,-0.39) .. (0,0) .. controls (4.3,0.39) and (9.04,1.82) .. (14.21,4.28)   ;
		
		% Text Node
		\draw (363,167) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 5$};
		% Text Node
		\draw (238,110) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 0$};
		% Text Node
		\draw (304,53) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 1$};
		% Text Node
		\draw (407.54,112.31) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 2$};
		% Text Node
		\draw (291,184) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 3$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Function returning in its own set of definitions}
	\end{figure}

	The closure of each element showing a "\NewTerm{reflexive relation}\index{reflexive relation}" and the systematic presence of a back arrow indicating a "\NewTerm{symmetrical relation}\index{symmetrical relation}".

	\textbf{Definition (\#\thesection.\mydef):} If the target set is identical to the original set, we say that we have a "\NewTerm{binary relation}\index{binary relation}".

	However choosing to introduce the Set Theory in school classrooms has also some other reason. In fact, for the sake of internal rigour (i.e. not related to reality), a very large part of mathematics was rebuilt within a single axiomatic framework, so named "\NewTerm{Set Theory}\index{set theory}", in the sense that each mathematical concept (previously independent of the other) is returned to a definition where all the logical components come from this same framework: it is regarded as fundamental! Thus, the rigour of reasoning carried out within Set Theory is guaranteed by the fact that the frame is "non-contradictory" or "consistent". Let us see now the definitions that build this framework.

\textbf{Definitions (\#\thesection.\mydef):}

\begin{itemize}
	\item[D1.] We name "\NewTerm{set}\index{set}" any list, collection or gathering of well-defined objects, explicitly or implicitly.
	
	\item[D2.] A "\NewTerm{Universe}\index{Universe (mathematics)}" $U$ is an object whose constituents are sets .\\\\
	Note that what mathematicians name "Universe" is not a set! In fact it is a model that satisfies to the axioms of sets.\\\\
	Indeed, we will see that we can not talk about the set of all sets (because this is not a set) to designate the object that consists of all the sets and that's why we talk about "Universe".

	\item[D3.] We name "\NewTerm{elements}\index{elements (mathematics)}" or "\NewTerm{members of the set}\index{members of a set}" objects belonging to the set and we write:
	
	if $p$ is an element of the set $A$ and in the contrary case:
	
	If $B$ is a "\NewTerm{part}\index{part of a set}" of $A$, or "\NewTerm{subset}\index{subset}\label{subset}" of $A$, we write this:
	
	Thus:
	
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. $A=\lbrace 1,2,3 \rbrace$\\\\
	E2. $X=\lbrace X \mid x\:\text{is a positive integer} \rbrace$
	\end{tcolorbox}
	
		\item[D4.] We can provide sets with a number of relations that compare (useful sometimes...) their elements or to compare some of their properties. These relations are named "\NewTerm{comparison relations}\index{comparison relations}\label{comparison relations}" or "\NewTerm{order relations}\index{order relations}" (\SeeChapter{see section Operators page \pageref{order relation}}).

\end{itemize}

	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} The structure of ordered set has original been set up in the framework of the Numbers Theory by Cantor and Dedekind.\\
	
	\textbf{R2.} As we have proved in the section on Operators, $\mathbb{N},\mathbb{Z},\mathbb{Q},\mathbb{R}$ are totally ordered by the usual relations $\leq,\geq$. The relation $<$, often named "\NewTerm{strict order}\index{strict order}" is not an order relation because not reflexive and not antisymmetric (\SeeChapter{see section Operators page \pageref{strict order}}). For example, in $\mathbb{N}$ the relation "$a$ divides $b$" , often denoted by the symbol "|" is a partial order.\\
	
	\textbf{R3.} If $\mathcal{R}$ is an ordering on $E$ and $F$ is a subset of $E$, the restriction to $F$ of the relation $\mathcal{R}$ is an order on $F$, named "\NewTerm{order induced by $\mathcal{R}$ in $F$}".\\
	
	\textbf{R4.} If $\mathcal{R}$ is an order on $E$, the relation $\mathcal{R}'$ defined by:
	\begin{gather*}
		x\mathcal{R}'y \Leftrightarrow y\mathcal{R}x
	\end{gather*}
	is an order on $E$, named "\NewTerm{reciprocal order}\index{reciprocal order}" of $\mathcal{R}$. The reciprocal order $\leq$ of the usual order is the order noted $\geq$ and reciprocal order to the order "$a$ divides $b$" in $\mathbb{N}$ is the order "$b$ is a multiple of $a$".
	\end{tcolorbox}

	The set is the basic mathematical entity whose existence is defined: it is not defined as itself but by its properties, given by the axioms. It uses a human process: a kind of categorization feature, which allows thought to distinguish several independent qualified elements.
	
	\begin{theorem}
	We can demonstrate from these concepts, that the number of subsets of a set of cardinal $n$ is $2^n$.
	\end{theorem}

	\begin{dem}
	First there is the empty set $\varnothing$, that is $0$ items chosen from $n$, i.e. $C_{0}^{n}$ (notation of binomial coefficient non-conform with ISO 31-11!) as we have seen in the section of Probabilities:
	
	and so on...

	The number of subsets (cardinal) of $E$ corresponds to the summation of all binomial coefficients:
	

	But, we have (\SeeChapter{see section Algebraic Calculation page \pageref{binomial theorem}}):
	

	therefore:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Consider the set $S=\left\lbrace x_1,x_2,x_3\right\rbrace $, we have the set of all parts of $P(S)$ consisting of:
	
	\begin{itemize}
		\item[$-$] The empty set: $\left\lbrace \right\rbrace =\varnothing$
		\item[$-$] The singletons: ${x_1},{x_2},{x_3}$
		\item[$-$] The duets: ${x_1,x_2},{x_1,x_3},{x_2,x_3}$	
		\item[$-$] Itself: $\left\lbrace x_1,x_2,x_3\right\rbrace $
	\end{itemize}
	Such that:
	
	What makes indeed $8$ elements!
	\end{tcolorbox}

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
The order in which the elements are differentiated does not come into account when counting parts of the original set.
	\end{tcolorbox}

In Applied Mathematics, we work almost exclusively with sets of numbers. Therefore, we will limit our study of definitions and properties of these.

Now let us formalize the basic concepts for working with the most common sets we encounter in the basic school curriculum.

	\subsection{Zermelo-Fraenkel Axiomatic}	\label{zermelo fraenkel axiomatic}

	The Zermelo-Fraenkel axiomatic, abbreviated sometimes "\NewTerm{ZF-C axioms}\index{Zermelo-Fraenkel axiomatic}" shown below was formulated by Ernst Zermelo and Abraham Adolf Fraenkel specified by the early 120th century (holocene calendar) and completed by the axiom of choice (hence the capital C in ZF-C). It is considered as the most natural axiomatic structure in the context of set theory.

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	There are many other axiomatic structures, based on the more general concept of "class", as developed by von Neumann, Bernays and Gödel (for the notations, see section Proof Theory page \pageref{proof theory}).
	\end{tcolorbox}
	
	Strictly technically speaking..., the ZF axioms are statements of calculation for first order predicate (\SeeChapter{see section Proof Theory page \pageref{first order predicate}}) egalitarian in a language with only one primitive symbol for membership (binary relation). The following should therefore only be seen as an attempt (...) to express in English the expected significance of these axioms.

	\begin{itemize}
		\item[A1.] Axiom of extensionality:\label{extensionality axiom}
		
		Two sets are equal if and only if they have the same elements. This is what we denote:
		
		So $A$ and $B$ are equal if every element $x$ of $A$ is also in $B$ and every element $x$ of $B$ also belongs to $A$.
		
		\item[A2.] Axiom of empty set\label{empty set}:
		
		The empty set exists, we denote it:
		
		and it has no element, its cardinality is therefore $0$.
		
		In fact this axiom can be deduced from another axiom that we will see a little further but it is convenient to introduce it by convenience for teaching in high-school classes.
		
		\item[A3.] Axiom of pairing:
		
		If $A$ and $B$ are two sets, then, there exist a set $C$ containing $A$ and $B$ alone and as components. This set $C$ is then denoted $\{A,B\}$.
		
		From the perspective of the sets considered elements that gives: 
		
		This axiom also shows the existence of the "\NewTerm{singleton}\index{singleton}" a set noted:
		
		which is a set whose only element is $X$ (and therefore with unitary cardinal). We simply need to apply the axiom asking equality between $A$ and $B$.
		
		\item[A4.] Axiom of the sum (also named "axiom of union"):
		
		This axiom allows us to build the union (merge)\label{union} of sets. Said in a most common way: the union (merge) of any family of a set, is... a set.
		
		The union of any family of sets is often denoted:
		
		or if we take some of its elements:
		
		
		\item[A5.] Axiom of subsets:
		
		This axiom defines that for any set $A$, the set of all its parts $P(A)$ exists (do not confuse with the "$P$" of probability!).
		
		So for any set $A$, we can associate a set $B$ which contains exactly the parts $C$ (verbatim the subsets) of the first:
			
		
		\item[A6.] Axiom of infinity:
		
		This axiom express the fact that there exist an infinite set. To formalize it, we say that there exist a set, named "\NewTerm{autosuccessor set}\index{autosuccessor set}" $A$ containing $\varnothing$ (the empty set) such that if $x$ belongs to $A$, then also $x \cup \lbrace x\rbrace $ belongs to $A$:
		
		This axiom expresses for example that the set of integers exists. Indeed, $\mathbb{N}$ is so the smallest autosuccessor set in the sense of inclusion $\mathbb{N}=\lbrace \varnothing ,\lbrace \varnothing , \lbrace \varnothing , \ldots \rbrace \rbrace \rbrace$ and by convention we note (where we build the Natural Set):
		
			
		\item[A7.] Axiom of regularity (also named "foundation axiom"):
		
		The main purpose of this axiom is just to eliminate the possibility of having $A$ as part of itself.
		
		Thus, for any non-empty set $A$, there exists a set $B$ which is an element of $A$ such that no element of $A$ is an element of $B$ (you must distinguish the level of the language used, a set and its elements have not the same status!) that we note:
		
		and thus result we expected to have:
		
		\begin{dem}
		Indeed, let $A$ be a set such that $A \in A$. Consider the singleton $\lbrace A \rbrace$, set whose only element is $A$. According to the axiom of foundation, we must have an element of this singleton that has no element in common with him. But the only possible element is $A$ itself , that is to say that we must have:
		
		But by hypothesis $A \in A$ and by construction $A \in \lbrace A \rbrace$. So:
		
		which contradicts the previous assertion. Therefore:
		
		\end{dem}
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright} 
		
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
 	 skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
 	 skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
 	 skin last is subskin of={enhancedlast}{drop lifted shadow}]
		Every set contains as a subset the empty set $\emptyset=\{\}$, and it also equally trivially contains itself as a subset. For the empty set these two cases are in fact the same, and indeed the empty set is the unique subset of the empty set. So we can form the set $\{\emptyset\}$ whose only member is $\emptyset$ but note that $\{\emptyset\} \neq \emptyset$, because $\emptyset \in\{\emptyset\}$ however $\emptyset \notin \emptyset$. In other words: nothing can be part of something but cannot contain itself!
		\end{tcolorbox}
		
		\item[A8.] Axiom of replacement (also named "Axiom schema of replacement"):
		
		This axiom expresses the fact that if a formula $f$ is a functional then for any set $A$, there is a set $B$ consisting precisely of the images of $A$ by this function.
	
		So, in a little more formally way, the set $A$ of elements $a$ and a binary relation $f$ (which is quite generally a functional), there exist a set $B$ consisting of elements $b$ such that $f(a, b)$ is true. If $f$ is a function where $b$ is not free then it means that:
		
		In a technical way we write this axiom as following:
		
		So for every set $A$ and any item it contains, there is one and only one $b$ defined by the functional $f$ such that there exists a set $B$ for which any element $a$ belonging to the set $A$ there is a $b$ belonging to set $B$ defined by the functional $f$.
	
		Let's see an example with the following binary predicate that for the value of any $a$ from $A$ determines the value of any $b$ of $B$:
		
		Therefore from the knowledge that $a$ is equal $1$ we derive that $b$ is equal $2$ and similarly (i.e. by replacement) when $a$ is equal $3$, we derive that $b$ is equal $4$.
	
		We see well through this small example the strong relation that exists considering the predicate $P$ as a naive function! Moreover, as there an infinity of possible functions $f$, the replacement scheme is considered as an infinite number of axioms.
	
		\item[A9.] Axiom of selection (also named "Axiom comprehension schema"):
		
		This axiom simply expresses that for any set $A$ and any property $P$ expressible in the language of set theory, the set of all elements of $A$ satisfying the property $P$ exist.
	
		So more formally, to any set $A$ and any condition or proposition $P(x)$, there is a set $B$ whose elements are exactly the elements $x$ of $A$ for which $P(x)$ is true. This is what we write:
		
		In a more comprehensive and rigorous way we have in fat for any functional $f$ that does not include $a$ as free variable:
		
		It is typically the axiom that we use to construct the set of even numbers:
		
		or to prove the existence of the empty set (which invalidates the axiom of the empty set) because you just have to ask that there exist a set that satisfies the property:
		
		and regardless of the set $A$. And only the empty set satisfies this property by the selection axiom.
		
		The compliance with the strict conditions of this axiom eliminates the paradoxes of the "\NewTerm{naive set theory}\index{naive set theory}", as Russell's paradox or Cantor's paradox who invalidated the naive set theory.
		
		For example, consider the Russell set $R$ of all sets that do not contain themselves (note that we give a property of $R$ without specifying what is this set):
		
		The problem is to know whether or not $R$ contains itself or not. If $R \in R$, then, $R$ is self-contained, and, by definition $R \not\in R$, and vice versa. Each possibility is contradictory.
	
		If we now denote by $C$ the set of all sets (Cantor Universe) we have in particular:
		
	
		which is impossible (i.e. with the power of the continuum of real numbers), according to Cantor's theorem (\SeeChapter{see section Numbers page \pageref{Cantor's diagonal}}).
	
		These "paradoxes" (or "syntactic antinomies") come from a non-compliance with the conditions of application of the selection axiom: to define $E$ (in the example of Russell), there must be a proposition $P$ which bears on the set $R$, which should be explicated. The proposal defining the set of Russell or that of Cantor does not indicate what is the set $E$. It is therefore invalid!
	
		A very nice and well known example (this is why we present it) helps to better understand (this is the "Russell paradox" which we have already spoken about in length in the section on Proof Theory page \pageref{russell paradox}):
	
		A young student went one day to his barber. He entered into conversation and asked him if he had many competitors in his pretty city. Seemingly innocent way, the barber replied, « \textit{I have no concurrence. Because of all the men of the city, I obviously do not shave those who shave themselves, but I am fortunate to shave. all those who do not shave themselves}».
	
		What then in such a so simple statement could take to the fault the logic of our young smart student?
	
		The answer is in fact innocent, until we decide to apply to the case of the barber: Does he shaves himself, Yes or No?
	
		Suppose he shaves himself: he then belongs to the category of those who shave themselves, those who the barber said he did of course not shave.... So he does not shave himself..........
	
		Finally, this unfortunate barber is in a strange position: if he shaves himself, he does not shave himself, and if he does not shave himself, he shaves himself. This logic is self-destructive, contradictory stupidly, rationally irrational.
	
		Then comes the selection axiom: We exclude the barber of all persons to which the declaration applies. Because in reality, the problem is that the barber is a member of the set of all the men of the city. So what applies to all men does not apply to the individual case of the barber.
	
		\item[A10.] Axiom of choice:
		
		Given a set $A$ of non-empty mutually disjoint sets, there exist a set $B$ (the set of choices for $A$) containing exactly one element for each member $A$.
		
		However let us indicate that the issue of the axiomatization and therefore of the foundations found himself still shaken by two questions at the time of their construction: what valid axioms must be chosen and in a system of axioms are the mathematics coherent (do we not have a risk of seeing a contradiction)?
		
		The first issue was first raised by the continuum hypothesis: if we can put two sets of numbers in correspondence term to term, they have the same number of elements (cardinal). We can thus map all integer numbers $\mathbb{N}$ with rational numbers $\mathbb{Q}$ as we have shown in the section on Numbers (page \pageref{natural and rational numbers equipotence}), so they have the same cardinality, be we can not map integer numbers with all the real numbers. The question then is whether there is a set whose number of elements would be located between the two or not? This question is important to build the classical theory of analysis and mathematicians usually choose to say there is none, but we can also say the opposite.
	
		In fact the continuum hypothesis is linked in a more profound way we could thing to the axiom of choice which can also be formulated as follows: if $C$ is a collection of non-empty sets then we can select any element of each set of the collection. If $C$ has a finite number of elements or a countable number of elements, the axiom seems pretty trivial: we can sort and number the sets of $C$ and the selection of an element in each set is simple. Where it begin to get complicated is when the set $C$ has the power of the continuum: how to choose the elements if it is not possible to number them?
	
		Finally in 11938 (holocene calendar) Kurt Gödel shows that set theory is consistent without the axiom of choice and without the continuum hypothesis as well as with! And to end it all Paul Cohen in 11963 (holocene calendar) shows that the axiom of choice and the continuum hypothesis are not related.
	\end{itemize}
	
	Ok to make a pedagogical summary of all this stuff consider the following figure (excluding the axiom of choice):
	\begin{figure}[H]
		\label{continuous distributions}
		\centering
		\includegraphics{img/arithmetics/zf_axioms.jpg}	
		\caption[Zermelo-Frankel axioms visual summary]{Zermelo-Frankel axioms visual summary (source:?)}
	\end{figure}

\subsubsection{Cardinals}\label{cardinal}

\textbf{Definition (\#\thesection.\mydef):} Sets are said to be "\NewTerm{equipotent}\index{equipotent}" if there exists a bijection (one-one correspondence) between these sets. We thus say they have same "\NewTerm{cardinal}\index{cardinal}" that the norm ISO 3111 advocated to write $card(S)$ but in this book we will also use the notation $\mathrm{Card}(S)$ (many U.S. books use non-official notation that looks exactly like the absolute value $\mid S \mid$ or $\# S$).

Thus, more rigorously, a cardinal (which quantifies the number of items in the set) is an equivalence class (\SeeChapter{see section Operators page \pageref{equivalence class}}) for the relation of equipotence.

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
Cantor is the main creator of set theory, in a form that we name today "naive set theory". But, apart to elementary considerations, his theory was also consisting of higher abstraction levels. The real novelty of the Cantor theory is that it lets talk about infinity. For example, an important idea Cantor was precisely to define the "equipotence".
	\end{tcolorbox}
	
If we write $c_1=c_2$ as equality of cardinals, we mean by that there are two equipotent sets $A$ and $B$ such that:
	
	
Cardinals can be compared. The order thus defined is a total ordering (\SeeChapter{see section Operators page \pageref{total order relation}}) between the Cardinals (the proof that the order relation is complete uses the axiom of choice and the proof that it is antisymmetric is known under the name of Cantor-Bernstein's theorem that we will demonstrate further below).

	Say that $c_1<c_2$ means in simple language that $A$ is equipotent to a proper part of $B$, but $B$ is not equipotent to any own part of $A$. Mathematicians would say that $\mathrm{Card}(A)$ is smaller or equal to the $\mathrm{Card}(B)$ if there is an injection of $A$ into $B$.
	
	We saw during our study of numbers (\SeeChapter{see section Numbers page \pageref{countable set}}), especially of transfinite numbers, that an equipotent set (or bijection) to $\mathbb{N}$ was told to "\NewTerm{countable set}\index{countable set}".
	
	Let us now see this notion a little more in detail:
	
	Let $A$ be a set, if there is an integer $n$ such that there is at least for each element of $A$ a corresponding item int the set $\left\lbrace 1,2, \ldots,n\right\rbrace $ (in fact this is rigorously a bijection... concept that we will define later) then we say that the cardinal of $A$, denoted $\mathrm{Card}(A)$ or $\mathrm{Card}(A)$ is a "\NewTerm{finite cardinal}\index{finite cardinal}" and its value is $n$.
	
	Otherwise, we say that the set $A$ has an "\NewTerm{infinite cardinal}\index{infinite cardinal}" and we write:
	

A set $A$ is "\NewTerm{countable}\index{countable set}\label{countable set}" if there is a bijection between $A$ and $\mathbb(N)$. A set of numbers $A$ is "\NewTerm{countable}" if there is a bijection between $A$ and part of $\mathbb(N)$. A set at maximum countable is thus of finite cardinal, or countable.

We can therefore check the following proposals: 
\begin{itemize}
	\item[P1.] A part of a countable set is at most countable.

	\item[P2.] A set containing a non-countable set is also not countable.

	\item[P3.] The product of two countable sets is countable.
\end{itemize}

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
We can restrict a set of numbers relatively with the null element and the negative or positive elements in it and therefore we write (example for the real set):

These concepts are similar for $\mathbb{N},\mathbb{Z},\mathbb{Q}$ (the set of complex numbers $\mathbb{C}$ being not ordered, the second and third line does not apply to).
	\end{tcolorbox}
So any infinite subset of $\mathbb{N}$ is equipotent to $\mathbb{N}$ itself, what may seem counter-intuitive at first...!

In particular, there are as many even integers as any natural integer numbers (use the bijection $f(n)=2n$) from $\mathbb{N}$ to $P$, where $P$ is the set of even natural numbers. As many relative numbers as integers, as many integers as rational numbers (see the section Numbers page \pageref{natural and rational numbers equipotence} for the proofs).

Thus we can write\label{aleph}:
	
and more generally, any infinite part of $\mathbb{Q}$ is countable.

Thus we have an important result: any infinite set therefore has an infinite countable part.

Since we have proved in the section on Numbers (page \pageref{power of the continuum}) that the set of real numbers $\mathbb{R}$ has the "\NewTerm{power of the continuum}\index{power of the continuum}" and that the set of natural numbers $\mathbb{N}$ has transfinite cardinal $\aleph_0$, Cantor raised the question whether there was a cardinal between the transfinite cardinal $\aleph_0$ and the cardinal of $\mathbb{R}$? In other words, we have an infinite amount of integers, and an even greater amount of real numbers. So does it exist an infinite greater than the infinite of integers and smaller than that of the real numbers?

The problem arose by writing $\aleph_0$ the cardinal of $\mathbb{N}$ and $\aleph_1$ (new) the cardinal of $\mathbb{R}$ and offering to demonstrate or contradict that:
	
according to the combinatorial law that gives the number of elements that we can get from all subsets of a set (as we have proved it before).

The rest of his life, Cantor tried, in vain, to prove this result that we name the "\NewTerm{continuum hypothesis}\index{continuum hypothesis}". He did not succeed and descended into madness. In 19000 (holocene calendar), the International Congress of Mathematicians, Hilbert considered that this was one of the 23 major issues that should be resolved in the 120th century (holocene calendar).

This problem is solved in a rather surprising way. First, in 11938, one of the greatest logicians of the 120th century (holocene calendar), Kurt Gödel showed that the hypothesis of Cantor was not refutable, that is to say, we could never prove that it was false. Then in 11963 (holocene calendar), the mathematician Paul Cohen closed the debate. He demonstrated that we could never prove that it was true!!! We can conclude rightly that Cantor had become mad to try to demonstrate a problem that could not be proved.

\subsubsection{Cartesian Product}\label{cartesian product}

\textbf{Definition (\#\thesection.\mydef):} If $E$ and $F$ are two sets, we name "\NewTerm{Cartesian product of $E$ by $F$}\index{Cartesian product}" the set noted $E \times F$ (not to be confused with the vector product notation) consisting of all possible pairs $(e,f)$ where $e$ is an element of $E$ and $f$ an element of $F$.

More formally:
	
We note the Cartesian product of $E$ by itself:
	
and then we say that $E^2$ is the "\NewTerm{set of pairs of elements of $E$}".

We can perform the Cartesian product of a sequence $E_1 \times E_2 \times \ldots \times E_n$ of sets and get all $n$-tuples $(e_1,e_2,\ldots,e_n)$ where $e_1 \in E_1,e_2 \in E_2,\ldots,e_n \in E_n$.

In the case where all sets $E_i$ are identical to $E$, the Cartesian product $E_1 \times E_2 \times \ldots \times E_n$ is obviously noted $E^n$. We then say that $e^n$ is the "\NewTerm{set of all $n$-tuples of elements of $E$}".

If $E$ and $F$ are finite then the Cartesian product $E \times F$ is finished. Moreover:
	
From here we see that if the sets $E_1, E_2, \ldots, E_n$ are finished then the Cartesian product $E_1 \times E_2 \times \ldots \times E_n$ is finished and we have:
	
In particular:
	
if $E$ is a finite set.

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. If $\mathbb{R}$ is the set of real numbers, then $\mathbb{R}^2$ is the set of all couples of real numbers. In the plane reported to a referential, any point $M$ has the coordinates that are an element of $\mathbb{R}^2$.\\
	
	E2. When we run two dice whose faces are numbered $1$ through $6$, each die can be symbolized by the set $E=\left\lbrace 1,2,3,4,5,6\right\rbrace $. The outcome of a roll of dies is then an element of $E^2=E \times E$. The Cardinal of $E \times E$ is then $36$. There are therefore $36$ possible results when we launch two dices whose faces are numbered $1$ to $6$.
	\end{tcolorbox}

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
Set theory and the concept of cardinal is the theoretical basis of relational database softwares.
	\end{tcolorbox}
	
\subsubsection{Intervals}

	Let $M$ be a set of any numbers so that $M \subset \mathbb{R}$ (particular but frequent example). We have for definitions:
	\textbf{Definitions (\#\thesection.\mydef):} 
	\begin{itemize}
		\item[D1.] $x \in \mathbb{R}$ is named "\NewTerm{upper bound}\index{upper bound}" of the set $M$, if $x \geq m$ for $\forall m \in M$. Conversely, we speak about "\NewTerm{lower bound}\index{lower bound}" (so do not confuse the concept of terminal with the concept of interval!).
		\item[D2.] Either $M \subset \mathbb{R},M \neq \varnothing$. $x \in \mathbb{R}$ is named the "\NewTerm{smallest upper bound}\index{smallest bound}" noted:
		
	of $M$ if $x$ is an upper bound of $M$ and if for any upper bound $y \in \mathbb{R}$ we have $x \leq y$. Conversely, we speak about the "\NewTerm{smaller lower bound}\index{smaller lower bound}" that we note:
				
	\end{itemize}
	The definitions are equivalent in the context of functional analysis (see section of the same name page \pageref{functional analysis}) as the functions are defined on sets.

	Indeed, let $f$ be a function whose domain of definition $I$ swept all $\mathbb{R}$. We note that:
		
and let $x_0 \in \mathbb{R}$.

\textbf{Definitions (\#\thesection.\mydef):}
	\begin{itemize}
		\item[D1.] We say that $f$ has a "\NewTerm{global maximum}\index{global maximum}" on $x_0$ if:
		
		
		\item[D2.] We say that $f$ has a "\NewTerm{global minimum}\index{global minimum}" on $x_0$ if:
		
		In each of these cases, we say that $f$ has an "\NewTerm{global extremum}\index{global extremum}" on $x_0$ (it is a concept that we often use in the sections of Analytical Mechanics page \pageref{lagrangian mechanics} and Numerical Methods page \pageref{numerical methods}!).
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
			%uncomment if require: \path (0,1467); %set diagram left start at 0, and has height of 1467
			
			%Shape: Axis 2D [id:dp053554925069400294] 
			\draw  (90,346) -- (535.5,346)(134.55,58) -- (134.55,378) (528.5,341) -- (535.5,346) -- (528.5,351) (129.55,65) -- (134.55,58) -- (139.55,65) (178.55,341) -- (178.55,351)(222.55,341) -- (222.55,351)(266.55,341) -- (266.55,351)(310.55,341) -- (310.55,351)(354.55,341) -- (354.55,351)(398.55,341) -- (398.55,351)(442.55,341) -- (442.55,351)(486.55,341) -- (486.55,351)(129.55,302) -- (139.55,302)(129.55,258) -- (139.55,258)(129.55,214) -- (139.55,214)(129.55,170) -- (139.55,170)(129.55,126) -- (139.55,126)(129.55,82) -- (139.55,82) ;
			\draw   (185.55,358) node[anchor=east, scale=0.75]{1} (229.55,358) node[anchor=east, scale=0.75]{2} (273.55,358) node[anchor=east, scale=0.75]{3} (317.55,358) node[anchor=east, scale=0.75]{4} (361.55,358) node[anchor=east, scale=0.75]{5} (405.55,358) node[anchor=east, scale=0.75]{6} (449.55,358) node[anchor=east, scale=0.75]{7} (493.55,358) node[anchor=east, scale=0.75]{8} (131.55,302) node[anchor=east, scale=0.75]{1} (131.55,258) node[anchor=east, scale=0.75]{2} (131.55,214) node[anchor=east, scale=0.75]{3} (131.55,170) node[anchor=east, scale=0.75]{4} (131.55,126) node[anchor=east, scale=0.75]{5} (131.55,82) node[anchor=east, scale=0.75]{6} ;
			%Curve Lines [id:da2755867557042644] 
			\draw [line width=1.5]    (185.5,92) .. controls (192.23,138.05) and (228.5,295) .. (263.5,295) .. controls (298.5,295) and (350.5,151) .. (383.5,151) .. controls (416.5,151) and (428.78,210.56) .. (462.5,232) .. controls (496.22,253.44) and (516.86,230.48) .. (521.5,227) ;
			%Straight Lines [id:da8522458817068415] 
			\draw    (231.5,105) -- (195.45,96.46) ;
			\draw [shift={(193.5,96)}, rotate = 13.32] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
			%Straight Lines [id:da09874834525257326] 
			\draw    (311.5,315) -- (279.25,296.98) ;
			\draw [shift={(277.5,296)}, rotate = 29.2] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
			%Straight Lines [id:da835770490512689] 
			\draw    (401.5,110) -- (389.2,143.13) ;
			\draw [shift={(388.5,145)}, rotate = 290.38] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
			%Straight Lines [id:da2299247987209121] 
			\draw    (470.5,271) -- (480.69,247.83) ;
			\draw [shift={(481.5,246)}, rotate = 113.75] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
			
			% Text Node
			\draw (239,95) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 189; green, 16; blue, 224 }  ,opacity=1 ] [align=left] {global maximum};
			% Text Node
			\draw (318,307) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 189; green, 16; blue, 224 }  ,opacity=1 ] [align=left] {global minimum};
			% Text Node
			\draw (383,90) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,opacity=1 ] [align=left] {local maximum};
			% Text Node
			\draw (432,277) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,opacity=1 ] [align=left] {local minimum};
			% Text Node
			\draw (536,349.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
			% Text Node
			\draw (120,39.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
			\end{tikzpicture}	
			\caption[Global/Local maximum and ninimum example]{Global/Local naximum and minimum example (source: Wikipedia)}
		\end{figure}
		
		\item[D3.] $f$ is "\NewTerm{upper bounded}" if there is a real number $M$ such as $\forall x \in I, f(x) \leq M$. In this case, the function has an upper bound of $f$ on its domain of definition $I$ traditionally denoted:
		
		
		\item[D4.] $f$ is "\NewTerm{lower bounded}" if there is a real $M$ such that $\forall x \in I, f(x) \geq M$. In this case, the function has a lower bound of $f$ on its domain of definition $I$ traditionally denoted:
		
		
		\item[D5.] We say that $f$ is "\NewTerm{bounded}\index{bounded function}" if it is both lower bounded and upper bounded (typically the case of trigonometric functions).
	\end{itemize}

	\subsection{Set Operations}\label{set operations}
	
	We can build from at least three sets $A, B, C$ all sets operations (which notations are due to Richard Dedekind) existing in set theory (very useful in the study of probability and statistics).

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
Some of the notations below will be frequently use later in relatively complex theorems, so it is necessary to understand them deeply!
	\end{tcolorbox}	

Thus, we can construct the following set operations:

\subsubsection{Inclusion}
In the simplest case, we define the "\NewTerm{inclusion}\index{inclusion}" as:
	
	In a non-specialized language here's C: $A$ is "included" (is a "part", or is a "subset") in $B$ then for all $x$ belonging to each of these $x$ also belongs to $B$:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,651); %set diagram left start at 0, and has height of 651
		
		%Shape: Rectangle [id:dp3263952603705411] 
		\draw   (178,46) -- (456.5,46) -- (456.5,244.33) -- (178,244.33) -- cycle ;
		%Shape: Circle [id:dp896956414833294] 
		\draw   (230.33,145.17) .. controls (230.33,97.16) and (269.25,58.25) .. (317.25,58.25) .. controls (365.25,58.25) and (404.17,97.16) .. (404.17,145.17) .. controls (404.17,193.17) and (365.25,232.08) .. (317.25,232.08) .. controls (269.25,232.08) and (230.33,193.17) .. (230.33,145.17) -- cycle ;
		%Shape: Circle [id:dp2449688554056697] 
		\draw   (257.4,145.17) .. controls (257.4,112.11) and (284.19,85.31) .. (317.25,85.31) .. controls (350.31,85.31) and (377.1,112.11) .. (377.1,145.17) .. controls (377.1,178.22) and (350.31,205.02) .. (317.25,205.02) .. controls (284.19,205.02) and (257.4,178.22) .. (257.4,145.17) -- cycle ;
		
		% Text Node
		\draw (178,26) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle A\subset B$};
		% Text Node
		\draw (309,134) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle A$};
		% Text Node
		\draw (274,201) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle B$};
		% Text Node
		\draw (429,217) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle U$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Visual example (Euler diagram) of the inclusion}
	\end{figure}
where the $U$ in the lower right corner of the figure represents the Cantor Universe.

From this it follows the following properties:
\begin{itemize}
	\item[P1.] If $A \in B$ and $B \in A$ then it implies  $A=B$ and vice versa.
	\item[P2.] If $A \in B$ and $B \in C$ then implies  $A \in C$.
\end{itemize}

\subsubsection{Intersection}

In the simplest case, we define the "\NewTerm{intersection}\index{intersection}\label{intersection}" as:

In a non-specialized language here's what you have to read: the "intersection" of sets $A$ and $B$ consists of all the elements that are both in $A$ and in $B$:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,651); %set diagram left start at 0, and has height of 651
		
		%Shape: Rectangle [id:dp3263952603705411] 
		\draw   (178,46) -- (456.5,46) -- (456.5,244.33) -- (178,244.33) -- cycle ;
		%Shape: Path Data [id:dp1761244213329074] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (326.1,140.17) .. controls (326.1,155.84) and (320.08,170.1) .. (310.22,180.77) .. controls (299.81,170) and (293.4,155.33) .. (293.4,139.17) .. controls (293.4,123.49) and (299.42,109.23) .. (309.28,98.56) .. controls (319.69,109.33) and (326.1,124) .. (326.1,140.17) -- cycle ;
		%Shape: Circle [id:dp8465673712307287] 
		\draw   (206.4,140.17) .. controls (206.4,107.11) and (233.19,80.31) .. (266.25,80.31) .. controls (299.31,80.31) and (326.1,107.11) .. (326.1,140.17) .. controls (326.1,173.22) and (299.31,200.02) .. (266.25,200.02) .. controls (233.19,200.02) and (206.4,173.22) .. (206.4,140.17) -- cycle ;
		%Shape: Circle [id:dp6969565727442095] 
		\draw   (293.4,139.17) .. controls (293.4,106.11) and (320.19,79.31) .. (353.25,79.31) .. controls (386.31,79.31) and (413.1,106.11) .. (413.1,139.17) .. controls (413.1,172.22) and (386.31,199.02) .. (353.25,199.02) .. controls (320.19,199.02) and (293.4,172.22) .. (293.4,139.17) -- cycle ;
		
		% Text Node
		\draw (178,26) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle A\cap B$};
		% Text Node
		\draw (367,126) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle A$};
		% Text Node
		\draw (241,124) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle B$};
		% Text Node
		\draw (429,217) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle U$};
		\end{tikzpicture}
		\vspace*{3mm}	
		\caption{Visual example (Euler diagram) of the intersection}
	\end{figure}
More generally, if $(A_i)$ is a family of sets indexed by $i \in I$, the intersection of the $(A_i),i \in I$ is denoted:

This intersection is explicitly defined by:

That is to say the intersection of the family of indexed sets includes all $x$ that are located in each set of all sets of the family.

Given two sets $A$ and $B$, we say they are "\NewTerm{disjoint}\index{disjoint}" if and only if:


Furthermore, if:

Mathematicians note that:

and name it "\NewTerm{disjoint union}\index{disjoint union}".

We sometimes joke that knowledge is built on the disjunction... (those who understand will appreciate...).

\textbf{Definition (\#\thesection.\mydef):} A collection $S=\{S_i\}$ of non-empty sets form a "\NewTerm{partition}\index{partition}" of a set $A$ if the following properties hold:
\begin{itemize}
	\item[P1.] $\forall S_i,S_j \in S$ and $i \neq j \Rightarrow S_i \cap S_j = \varnothing$
	\item[P2.] $A= \displaystyle\bigcup_{S_i \in S} S_i$
\end{itemize}

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\textbf{{\Large \ding{45}}Example:}\\\\
The set of even numbers and the set of odd numbers are a partitions of $\mathbb{Z}$.
	\end{tcolorbox}

The intersection law is trivially a commutative law (see further below the definition of the concept of "law") as:


\subsubsection{Union}

In the simplest case, we define the "\NewTerm{union}\index{union}" (also sometimes named "merge") as:

In a non-specialized language here's what you have to read: the "union" (or "merge") of the sets $A$ and $B$ is the set of elements that are in $A$ plus those that are in $B$.
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,651); %set diagram left start at 0, and has height of 651
		
		%Shape: Circle [id:dp8465673712307287] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (206.4,140.17) .. controls (206.4,107.11) and (233.19,80.31) .. (266.25,80.31) .. controls (299.31,80.31) and (326.1,107.11) .. (326.1,140.17) .. controls (326.1,173.22) and (299.31,200.02) .. (266.25,200.02) .. controls (233.19,200.02) and (206.4,173.22) .. (206.4,140.17) -- cycle ;
		%Shape: Circle [id:dp6969565727442095] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (293.4,139.17) .. controls (293.4,106.11) and (320.19,79.31) .. (353.25,79.31) .. controls (386.31,79.31) and (413.1,106.11) .. (413.1,139.17) .. controls (413.1,172.22) and (386.31,199.02) .. (353.25,199.02) .. controls (320.19,199.02) and (293.4,172.22) .. (293.4,139.17) -- cycle ;
		%Shape: Rectangle [id:dp3263952603705411] 
		\draw   (178,46) -- (456.5,46) -- (456.5,244.33) -- (178,244.33) -- cycle ;
		%Shape: Path Data [id:dp1761244213329074] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (326.1,140.17) .. controls (326.1,155.84) and (320.08,170.1) .. (310.22,180.77) .. controls (299.81,170) and (293.4,155.33) .. (293.4,139.17) .. controls (293.4,123.49) and (299.42,109.23) .. (309.28,98.56) .. controls (319.69,109.33) and (326.1,124) .. (326.1,140.17) -- cycle ;
		
		% Text Node
		\draw (178,26) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle A\cup B$};
		% Text Node
		\draw (367,126) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle A$};
		% Text Node
		\draw (241,124) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle B$};
		% Text Node
		\draw (429,217) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle U$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Visual example (Euler diagram) of the union}
	\end{figure}
More generally, if $(A_i)$ is a family of sets indexed by $i \in I$, the union of the $(A_i),i \in I$ is denoted:

This union is explicitly defined by:

That is to say that the union of the family of indexed sets includes all $x$ for which there is a set indexed by $i$ such that $x$ is included in one of the set $A_i$.

We have the following distributive properties:

The law of union $\cup$  is a commutative law (see further below the definition of the concept of "law") as:

We also name "\NewTerm{idempotences laws}\index{idempotence laws}" the relations (note that for the general culture):

and "\NewTerm{absorptions laws}\index{absorptions laws}" the relations:

The laws of intersection and union are associative, such that:

and distributive such that:

If we recall the concept of "cardinal" (see above) we have with the previously defined operations, the following relation:

Hence if $A\cap B=\varnothing$:
	

\subsubsection{Difference}

In the simplest case, we define the "\NewTerm{difference}\index{difference}" as:

In a non-specialized language here's what you have to read: The "difference" of the sets $A$ and $B$ consists of all the elements found only in $A$ (and thus excluding those of $B$):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,651); %set diagram left start at 0, and has height of 651
		
		%Shape: Circle [id:dp8465673712307287] 
		\draw   (206.4,140.17) .. controls (206.4,107.11) and (233.19,80.31) .. (266.25,80.31) .. controls (299.31,80.31) and (326.1,107.11) .. (326.1,140.17) .. controls (326.1,173.22) and (299.31,200.02) .. (266.25,200.02) .. controls (233.19,200.02) and (206.4,173.22) .. (206.4,140.17) -- cycle ;
		%Shape: Circle [id:dp6969565727442095] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (293.4,139.17) .. controls (293.4,106.11) and (320.19,79.31) .. (353.25,79.31) .. controls (386.31,79.31) and (413.1,106.11) .. (413.1,139.17) .. controls (413.1,172.22) and (386.31,199.02) .. (353.25,199.02) .. controls (320.19,199.02) and (293.4,172.22) .. (293.4,139.17) -- cycle ;
		%Shape: Rectangle [id:dp3263952603705411] 
		\draw   (178,46) -- (456.5,46) -- (456.5,244.33) -- (178,244.33) -- cycle ;
		%Shape: Path Data [id:dp1761244213329074] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (326.1,140.17) .. controls (326.1,155.84) and (320.08,170.1) .. (310.22,180.77) .. controls (299.81,170) and (293.4,155.33) .. (293.4,139.17) .. controls (293.4,123.49) and (299.42,109.23) .. (309.28,98.56) .. controls (319.69,109.33) and (326.1,124) .. (326.1,140.17) -- cycle ;
		
		% Text Node
		\draw (178,26) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle A/B$};
		% Text Node
		\draw (367,126) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle A$};
		% Text Node
		\draw (241,124) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle B$};
		% Text Node
		\draw (429,217) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle U$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Visual example (Euler diagram) of the difference}
	\end{figure}
	
\subsubsection{Symmetric Difference}

Let $U$ be a set. For any $A,B\subseteq U$ we define the "\NewTerm{symmetric difference}\index{symmetric difference}\label{symmetric difference}" $A\delta B$ between $A$ and $B$ by:

Or more explicitly using the "exclusive or" in (boolean) logic (see page \pageref{exclusive or}):
	
	In a non-specialized language here's what you have to read: The "symmetric difference" of the sets $A$ and $B$ consists of all items that are only in $A$ and those found only in $B$ (we pass aside elements that are common):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,651); %set diagram left start at 0, and has height of 651
		
		%Shape: Circle [id:dp8465673712307287] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (206.4,140.17) .. controls (206.4,107.11) and (233.19,80.31) .. (266.25,80.31) .. controls (299.31,80.31) and (326.1,107.11) .. (326.1,140.17) .. controls (326.1,173.22) and (299.31,200.02) .. (266.25,200.02) .. controls (233.19,200.02) and (206.4,173.22) .. (206.4,140.17) -- cycle ;
		%Shape: Circle [id:dp6969565727442095] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (293.4,139.17) .. controls (293.4,106.11) and (320.19,79.31) .. (353.25,79.31) .. controls (386.31,79.31) and (413.1,106.11) .. (413.1,139.17) .. controls (413.1,172.22) and (386.31,199.02) .. (353.25,199.02) .. controls (320.19,199.02) and (293.4,172.22) .. (293.4,139.17) -- cycle ;
		%Shape: Rectangle [id:dp3263952603705411] 
		\draw   (178,46) -- (456.5,46) -- (456.5,244.33) -- (178,244.33) -- cycle ;
		%Shape: Path Data [id:dp1761244213329074] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (326.1,140.17) .. controls (326.1,155.84) and (320.08,170.1) .. (310.22,180.77) .. controls (299.81,170) and (293.4,155.33) .. (293.4,139.17) .. controls (293.4,123.49) and (299.42,109.23) .. (309.28,98.56) .. controls (319.69,109.33) and (326.1,124) .. (326.1,140.17) -- cycle ;
		
		% Text Node
		\draw (178,26) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle A\Delta B$};
		% Text Node
		\draw (367,126) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle A$};
		% Text Node
		\draw (241,124) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle B$};
		% Text Node
		\draw (429,217) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle U$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Visual example (Euler diagram) of the symmetric difference}
	\end{figure}
So as we can see we have:

Some trivial properties are given below:
\begin{itemize}
	\item[P1.] Commutativity: $A \bigtriangleup B= B \bigtriangleup A$
	\item[P2.] Complementarity (see definition below): $A^c \bigtriangleup B^c = A \bigtriangleup B$
	\item[P3.] $A \bigtriangleup B=(A\cup B)\setminus(A\cap B)$
\end{itemize}

\subsubsection{Product}

In the simplest case, we define the "\NewTerm{set product}\index{set product}" or "\NewTerm{cartesian product}\index{Cartesian product}" as:


In a non-specialized language here's what to you have to read: "product" (not to be confused with the multiplication or cross product of vectors) of two sets $A$ and $B$ is the set of pairs such as each element of each set is combined with all elements of the other set.

The product set of real numbers for example generates the plane where each element is defined by $X$ and $Y$ axis. 

We often find products sets in mathematics and physics when we work with functions. For example, a function of two real variables which gives real output will be written:
	
or more simply:
	
	
\subsubsection{Complementarity}

In the simplest case, we define the "\NewTerm{complementarity}\index{complementary}" as:

In a non-specialized language here's what you have to read: The "complementary" is defined as taking a set $U$ and a subset $A$ of $U$ then the complement of $A$ in $U$ is the set of elements that are in $U$ but not in $A$:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,651); %set diagram left start at 0, and has height of 651
		
		%Shape: Rectangle [id:dp3263952603705411] 
		\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (178,46) -- (456.5,46) -- (456.5,244.33) -- (178,244.33) -- cycle ;
		%Shape: Circle [id:dp6969565727442095] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (257.4,145.17) .. controls (257.4,112.11) and (284.19,85.31) .. (317.25,85.31) .. controls (350.31,85.31) and (377.1,112.11) .. (377.1,145.17) .. controls (377.1,178.22) and (350.31,205.02) .. (317.25,205.02) .. controls (284.19,205.02) and (257.4,178.22) .. (257.4,145.17) -- cycle ;
		
		% Text Node
		\draw (178,26) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle A^{U}$};
		% Text Node
		\draw (307,135) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle A$};
		% Text Node
		\draw (429,217) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle U$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Visual example (Euler diagram) of the complementarity}
	\end{figure}
Other notations of complementarity that is sometimes found in the literature and the following book are (depending on the context to avoid confusion with other stuff):

or in the particular example above, we could also just write $U \setminus A$.

We have for properties for all $A_i$ included in any $B$:

Here are some trivial properties regarding to complementarity:

There are other very important relations that also applied to Boolean logic (\SeeChapter{see section Logic Systems page \pageref{logical systems}}). If we consider three sets $A, B, C$ as shown below:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}
		  \tikzset{venn circle/.style={draw,circle,minimum width=6cm,fill=#1,opacity=0.4}}
		
		  \node [venn circle = red] (A) at (0,0) {$A$};
		  \node [venn circle = blue] (B) at (60:4cm) {$B$};
		  \node [venn circle = green] (C) at (0:4cm) {$C$};
		  \node[left] at (barycentric cs:A=1/2,B=1/2 ) {$A \cap B$}; 
		  \node[below] at (barycentric cs:A=1/2,C=1/2 ) {$A \cap C$};   
		  \node[right] at (barycentric cs:B=1/2,C=1/2 ) {$B \cap C$};   
		  \node[below] at (barycentric cs:A=1/3,B=1/3,C=1/3 ){$A \cap B \cap C$};
		\end{tikzpicture} 
	\end{figure}
	then we have:


and the famous "\NewTerm{De Morgan's laws}\index{De Morgan's laws}" in set form (\SeeChapter{see section Logic Systems page \pageref{de morgan theorem}}), which are given by the relations:
\begin{equation}
  \addtolength{\fboxsep}{5pt}
   \boxed{
   \begin{gathered}
     \overline{A \cap B} = \bar{A} \cup \bar{B} \\
	\overline{A \cup B} = \bar{A} \cap \bar{B}
   \end{gathered}
   }
\end{equation}

We would like indicate before moving on to another topic, that a significant number of adults in employment (mostly managers) having forgotten the previous defined concepts after leaving high school must study them again when they learn the SQL language (Structured Query Language) which is the most common worldwide language to query corporate databases servers in the 120th and 121st century (holocene calendar). Most of them learn in training centers the following scheme to build queries with joins:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.33]{img/arithmetics/sql_joins.pdf}	
		\caption{Common SQL query expressions with joins}
	\end{figure}

	\pagebreak
	\subsection{Functions and Applications}	\label{functions and applications}

\textbf{Definition (\#\thesection.\mydef):} In mathematics, an "\NewTerm{application}\index{application}" (or "\NewTerm{function}\index{function}") denoted typically $f$ - in analysis - or $A$ - in linear algebra - is the information of two sets, the departure set $E$ and arrival set $F$ (or "image of $E$"), and a relation associating each element $x$ of the departure set one and only one element of the arrival set, which we name "\NewTerm{image of $x$ by $f$}" in the analysis field we note that $f(x)$ or $f(E)$ to explicit the departure set. We name "\NewTerm{images}\index{images}" the elements of $f(E)$ and the elements of $E$ are named the "\NewTerm{antecedents}\index{antecedent}".

Then we say that $f$ is an application from $E$ to $F$ denoted:

(remember the first arrow/sagittal diagram presented at the beginning of this section), or we also say that this is an application of arguments in $E$ and values in $F$.

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The term "function" is often used for applications with scalar numeric values, real or complex, that is to say when the arrival set is $\mathbb{R}$ or $\mathbb{C}$. We speak then of "real function" or "complex function". In the case of vector we prefer to use the word "application" as we already mention it in the definition.
	\end{tcolorbox}	
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Consider the following arbitrary function:
	
	with $\vec{x}\in\mathbb{R}^n$, $\vec{p}\in[0,1]^n$ and $d\in\mathbb{R}_+^n$ then the application will be written:
	
	\end{tcolorbox}
	
\textbf{Definitions (\#\thesection.\mydef):}

\begin{enumerate}
	\item[D1.] The "\NewTerm{graph}\index{graph}" or "\NewTerm{plot}\index{plot}" (or also named "graphic" or "representative") of an application or function $f:E \mapsto F$ is the subset of the cartesian product $E \times F$ consisting of pairs $(x, f (x))$ for $x$ varying in $E$. The data of the graph $f$ determines its starting set (by projection on the first argument often denoted $x$) and image (projection on the second argument often denoted $y$).
	
	\item[D2.] If the triplet $f(E,F,\Gamma)$ is a function where $E$ and $F$ are two sets and $\Gamma \subset (E \times F)$ is a graph, $E$ and $F$ are the source and purpose of $f$ respectively. The "\NewTerm{definition domain}\index{definition domain}" or "\NewTerm{departure set}\index{departure set}" of $f$ is:
	
	
	\item[D3.] Given three non empty sets $E, F, G$, any function of $E \times F$ to $G$ is named a "\NewTerm{composition law}\index{composition law}" of $E \times F$ with values in $G$.
	
	\item[D4.] An "\NewTerm{internal composition law}\index{internal composition law}\label{internal composition law}" (or simply "\NewTerm{internal law}\index{internal law}") in $E$ is a composition law of $E \times E$ with values in $E$ (that is to say this is the case $E = F = G$).
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The subtraction in $\mathbb{N}$ is not an internal composition law although it is part of the four basic high-school arithmetic operators. But the addition in $\mathbb{N}$ is such an internal law.
	\end{tcolorbox}
		
	\item[D5.] An "\NewTerm{external composition law}\index{external composition law}" (or simply "\NewTerm{external law}"\index{external law}) in $E$ is a composition law of $F \times E$ with values in $E$, where $F$ is a separate set of $E$. In general, $F$ is a set, named "\NewTerm{scalar set}\index{scalar set}".

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\textbf{{\Large \ding{45}}Example:}\\\\
In the case of a vector space (see definition much lower) the multiplication of a vector (whose components are based on a given set) by a real scalar is an example of external composition law.
	\end{tcolorbox}

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
An external composition law with values in $E$ is also named "\NewTerm{action of $F$ on $E$}". The set $F$ is then the field operators. They also say that $F$ operates on $E$ (keep in mind the example of the vectors mentioned above).
	\end{tcolorbox}	
	
	\item[D5.] We name "\NewTerm{image of $f$}", and note $\Ima (f)$, the subset defined by:
	
	Thus, "the image" of a function $f:E \mapsto F$ is the collection of $f(x)$ for $x$ browsing $E$. It is a subset of $F$.
	
	And we name "\NewTerm{ker of $f$}\index{kerr}\label{kerr}", and we note $\ker (f)$, the very important subset in mathematics defined by:
	
	According to the figure (you must deeply understand this concept because we will reuse the ker many times to prove theorems that have important practical applications later in various chapters):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,300); %set diagram left start at 0, and has height of 300
		
		%Shape: Polygon Curved [id:ds737335882382459] 
		\draw   (121.5,62) .. controls (141.5,52) and (202.5,65) .. (233,92) .. controls (263.5,119) and (194.5,137) .. (239.5,175) .. controls (284.5,213) and (148.5,224) .. (128.5,194) .. controls (108.5,164) and (101.5,72) .. (121.5,62) -- cycle ;
		%Shape: Polygon Curved [id:ds9916326055423017] 
		\draw   (151,113) .. controls (161.5,97) and (187.5,115) .. (194.5,122) .. controls (201.5,129) and (204.5,135) .. (188.5,153) .. controls (172.5,171) and (145.5,170) .. (141.5,149) .. controls (137.5,128) and (140.5,129) .. (151,113) -- cycle ;
		%Shape: Polygon Curved [id:ds6423914040942698] 
		\draw   (351.5,92) .. controls (379.5,64) and (390.5,62) .. (449,83) .. controls (507.5,104) and (595.5,239) .. (499.5,189) .. controls (403.5,139) and (370.5,187) .. (341.5,200) .. controls (312.5,213) and (323.5,120) .. (351.5,92) -- cycle ;
		%Shape: Polygon Curved [id:ds32125248273168383] 
		\draw   (393,104) .. controls (421.5,108) and (463,106) .. (472.5,113) .. controls (482,120) and (472,130) .. (462.5,139) .. controls (453,148) and (387.5,172) .. (367.5,142) .. controls (347.5,112) and (364.5,100) .. (393,104) -- cycle ;
		%Curve Lines [id:da6246281206581012] 
		\draw    (179.5,121) .. controls (219.3,91.15) and (320.48,61.3) .. (397.34,122.08) ;
		\draw [shift={(398.5,123)}, rotate = 218.84] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Curve Lines [id:da7093016502816365] 
		\draw    (186.5,177) .. controls (216.2,233.43) and (313.53,199.69) .. (354.28,169.9) ;
		\draw [shift={(355.5,169)}, rotate = 143.13] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da06528018729237695] 
		\draw    (425.5,31) -- (457,97) ;
		%Straight Lines [id:da8523740000228754] 
		\draw    (425.5,31) -- (409.5,31) ;
		
		% Text Node
		\draw (168,213) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle E$};
		% Text Node
		\draw (148,125) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \ker( f)$};
		% Text Node
		\draw (405,120) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 0_{F}$};
		% Text Node
		\draw (500,159) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle F$};
		% Text Node
		\draw (265,212) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle f$};
		% Text Node
		\draw (282,62) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle f$};
		% Text Node
		\draw (332,12) node [anchor=north west][inner sep=0.75pt]   [align=left] {Im$\displaystyle ( f)$ image of $\displaystyle f$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{ker concept of a function}
	\end{figure}
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} $\ker (f)$ is derived from the German "Kern", simply meaning "kernel".\\
	
	\textbf{R2.} Note that $\ker(f) \subset \ker\left(f^{2}\right)$ because $f(x)=0$ implies $f(f(x))=0$ (and same for any power $n$ of $f^n$). It follows that $\ker(f)=\ker\left(f^{n}\right)$ (i.e. $\ker(f^{m}) = \ker(f^{m + 1})$).\\
	
	\textbf{R3.} Normally the notations $\Ima$ and $\ker$ are reserved for group homomorphisms, rings, fields and to linear applications between vector spaces and modules, etc. (see further below). We do not usually use them for any applications between any sets. But ... it does not really matter for the moment at this level of the book.
	\end{tcolorbox}	
\end{enumerate}	

\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\textbf{{\Large \ding{45}}Example:}\\\\
The sine function has from its argument a kernel that is $2\pi \mathbb{Z}$.
\end{tcolorbox}

Applications and functions can have a phenomenal amount of properties. Below you can find some easy one that are part of the general knowledge of the physicist (for more information about what a function is, see the section on Functional Analysis page \pageref{functional analysis}).

Let $f$ be an application or function of a set $E$ to a set $F$ then we have the following properties:

\begin{enumerate}
	\item[P1.] An application or function is said to be "\NewTerm{surjective}\index{surjective application}\label{surjective application}" if:\\\\
	Any element $y$ of $F$ is the image by $f$ of at least (we emphasize on the "at least") an element of $E$. We thus say that it is a "surjection" from $E$ to $F$. It follows from this definition, that an application or function $f:E\mapsto F$ is surjective and denoted:
	
	if and only if $F= \Ima f$. In other words, we also write this definition as following:
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,300); %set diagram left start at 0, and has height of 300
		
		%Shape: Rectangle [id:dp23967321180430523] 
		\draw   (193,46) -- (249.5,46) -- (249.5,204) -- (193,204) -- cycle ;
		%Shape: Rectangle [id:dp2678587754367949] 
		\draw   (360,64) -- (407.5,64) -- (407.5,191) -- (360,191) -- cycle ;
		%Shape: Ellipse [id:dp7298966931921733] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (217.88,66.5) .. controls (217.88,64.57) and (219.13,63) .. (220.69,63) .. controls (222.24,63) and (223.5,64.57) .. (223.5,66.5) .. controls (223.5,68.43) and (222.24,70) .. (220.69,70) .. controls (219.13,70) and (217.88,68.43) .. (217.88,66.5) -- cycle ;
		%Shape: Ellipse [id:dp8760804594918974] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (217.88,95.5) .. controls (217.88,93.57) and (219.13,92) .. (220.69,92) .. controls (222.24,92) and (223.5,93.57) .. (223.5,95.5) .. controls (223.5,97.43) and (222.24,99) .. (220.69,99) .. controls (219.13,99) and (217.88,97.43) .. (217.88,95.5) -- cycle ;
		%Shape: Ellipse [id:dp8200796878011007] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (217.88,124.5) .. controls (217.88,122.57) and (219.13,121) .. (220.69,121) .. controls (222.24,121) and (223.5,122.57) .. (223.5,124.5) .. controls (223.5,126.43) and (222.24,128) .. (220.69,128) .. controls (219.13,128) and (217.88,126.43) .. (217.88,124.5) -- cycle ;
		%Shape: Ellipse [id:dp7229011341233307] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (217.88,153.5) .. controls (217.88,151.57) and (219.13,150) .. (220.69,150) .. controls (222.24,150) and (223.5,151.57) .. (223.5,153.5) .. controls (223.5,155.43) and (222.24,157) .. (220.69,157) .. controls (219.13,157) and (217.88,155.43) .. (217.88,153.5) -- cycle ;
		%Shape: Ellipse [id:dp7661231271454696] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (217.88,182.5) .. controls (217.88,180.57) and (219.13,179) .. (220.69,179) .. controls (222.24,179) and (223.5,180.57) .. (223.5,182.5) .. controls (223.5,184.43) and (222.24,186) .. (220.69,186) .. controls (219.13,186) and (217.88,184.43) .. (217.88,182.5) -- cycle ;
		%Shape: Ellipse [id:dp6290514854392619] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (381.88,91.5) .. controls (381.88,89.57) and (383.13,88) .. (384.69,88) .. controls (386.24,88) and (387.5,89.57) .. (387.5,91.5) .. controls (387.5,93.43) and (386.24,95) .. (384.69,95) .. controls (383.13,95) and (381.88,93.43) .. (381.88,91.5) -- cycle ;
		%Shape: Ellipse [id:dp45277112771508676] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (381.88,120.5) .. controls (381.88,118.57) and (383.13,117) .. (384.69,117) .. controls (386.24,117) and (387.5,118.57) .. (387.5,120.5) .. controls (387.5,122.43) and (386.24,124) .. (384.69,124) .. controls (383.13,124) and (381.88,122.43) .. (381.88,120.5) -- cycle ;
		%Shape: Ellipse [id:dp695852920766413] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (381.88,149.5) .. controls (381.88,147.57) and (383.13,146) .. (384.69,146) .. controls (386.24,146) and (387.5,147.57) .. (387.5,149.5) .. controls (387.5,151.43) and (386.24,153) .. (384.69,153) .. controls (383.13,153) and (381.88,151.43) .. (381.88,149.5) -- cycle ;
		%Straight Lines [id:da0999711204221787] 
		\draw    (220.69,66.5) -- (379.9,91.19) ;
		\draw [shift={(381.88,91.5)}, rotate = 188.82] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da9081550983960311] 
		\draw    (223.5,95.5) -- (379.88,91.55) ;
		\draw [shift={(381.88,91.5)}, rotate = 178.55] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da6527053580015407] 
		\draw    (220.69,124.5) -- (379.92,91.9) ;
		\draw [shift={(381.88,91.5)}, rotate = 168.43] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da4137678012161392] 
		\draw    (220.69,153.5) -- (379.92,120.9) ;
		\draw [shift={(381.88,120.5)}, rotate = 168.43] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da09735626548133824] 
		\draw    (220.69,182.5) -- (379.92,149.9) ;
		\draw [shift={(381.88,149.5)}, rotate = 168.43] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		
		% Text Node
		\draw (215,26) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle E$};
		% Text Node
		\draw (377,43) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle F$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Schematic representation of a surjective application or function}
	\end{figure}
	
	\item[P2.] An application or function is said to be "\NewTerm{injective}\index{injective application}\label{injective}" if:\\
	
	Any element $y$ of $F$ is the image by $f$ of at most (we emphasize the "at most") a single element of $E$. We thus say that $f$ is an injection of $E$ to $F$. It follows from this definition, that an application or function $f:E\mapsto F$ is injective and denoted:
		
	if and only if the relations $x_1,x_2 \in E$ and $f(x_1)=f(x_2)$ involve. In other words: an application or function for which two separate elements have distinct images is say to be "injective". Or an application or function is injective at least if one of the following equivalent properties holds: 
	\begin{enumerate}
		\item[P2.1] $\forall x,y\in E^2:\;f(x)=f(y)\Rightarrow x=y$
		\item[P2.2] $\forall x,y:\;  x\neq y \Rightarrow f(x) \neq f(y)$
		\item[P2.3] $\forall y \in F$ the equation in $x$, $y=f(x)$ has at least one solution in $E$
	\end{enumerate}
	All this can be resumed by:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,300); %set diagram left start at 0, and has height of 300
		
		%Shape: Rectangle [id:dp23967321180430523] 
		\draw   (346,48) -- (402.5,48) -- (402.5,206) -- (346,206) -- cycle ;
		%Shape: Rectangle [id:dp2678587754367949] 
		\draw   (174,66) -- (221.5,66) -- (221.5,193) -- (174,193) -- cycle ;
		%Shape: Ellipse [id:dp7298966931921733] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (370.88,68.5) .. controls (370.88,66.57) and (372.13,65) .. (373.69,65) .. controls (375.24,65) and (376.5,66.57) .. (376.5,68.5) .. controls (376.5,70.43) and (375.24,72) .. (373.69,72) .. controls (372.13,72) and (370.88,70.43) .. (370.88,68.5) -- cycle ;
		%Shape: Ellipse [id:dp8760804594918974] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (370.88,97.5) .. controls (370.88,95.57) and (372.13,94) .. (373.69,94) .. controls (375.24,94) and (376.5,95.57) .. (376.5,97.5) .. controls (376.5,99.43) and (375.24,101) .. (373.69,101) .. controls (372.13,101) and (370.88,99.43) .. (370.88,97.5) -- cycle ;
		%Shape: Ellipse [id:dp8200796878011007] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (370.88,126.5) .. controls (370.88,124.57) and (372.13,123) .. (373.69,123) .. controls (375.24,123) and (376.5,124.57) .. (376.5,126.5) .. controls (376.5,128.43) and (375.24,130) .. (373.69,130) .. controls (372.13,130) and (370.88,128.43) .. (370.88,126.5) -- cycle ;
		%Shape: Ellipse [id:dp7229011341233307] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (370.88,155.5) .. controls (370.88,153.57) and (372.13,152) .. (373.69,152) .. controls (375.24,152) and (376.5,153.57) .. (376.5,155.5) .. controls (376.5,157.43) and (375.24,159) .. (373.69,159) .. controls (372.13,159) and (370.88,157.43) .. (370.88,155.5) -- cycle ;
		%Shape: Ellipse [id:dp7661231271454696] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (370.88,184.5) .. controls (370.88,182.57) and (372.13,181) .. (373.69,181) .. controls (375.24,181) and (376.5,182.57) .. (376.5,184.5) .. controls (376.5,186.43) and (375.24,188) .. (373.69,188) .. controls (372.13,188) and (370.88,186.43) .. (370.88,184.5) -- cycle ;
		%Shape: Ellipse [id:dp6290514854392619] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (194.88,89.1) .. controls (194.88,87.2) and (196.13,85.67) .. (197.69,85.67) .. controls (199.24,85.67) and (200.5,87.2) .. (200.5,89.1) .. controls (200.5,91) and (199.24,92.53) .. (197.69,92.53) .. controls (196.13,92.53) and (194.88,91) .. (194.88,89.1) -- cycle ;
		%Shape: Ellipse [id:dp45277112771508676] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (194.88,115.59) .. controls (194.88,113.69) and (196.13,112.15) .. (197.69,112.15) .. controls (199.24,112.15) and (200.5,113.69) .. (200.5,115.59) .. controls (200.5,117.48) and (199.24,119.02) .. (197.69,119.02) .. controls (196.13,119.02) and (194.88,117.48) .. (194.88,115.59) -- cycle ;
		%Shape: Ellipse [id:dp695852920766413] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (194.88,142.08) .. controls (194.88,140.18) and (196.13,138.64) .. (197.69,138.64) .. controls (199.24,138.64) and (200.5,140.18) .. (200.5,142.08) .. controls (200.5,143.97) and (199.24,145.51) .. (197.69,145.51) .. controls (196.13,145.51) and (194.88,143.97) .. (194.88,142.08) -- cycle ;
		%Straight Lines [id:da0999711204221787] 
		\draw    (200.5,89.1) -- (371.7,68.74) ;
		\draw [shift={(373.69,68.5)}, rotate = 173.22] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da9081550983960311] 
		\draw    (200.5,115.59) -- (371.7,97.71) ;
		\draw [shift={(373.69,97.5)}, rotate = 174.04] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da6527053580015407] 
		\draw    (197.69,142.08) -- (368.88,126.68) ;
		\draw [shift={(370.88,126.5)}, rotate = 174.86] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da4137678012161392] 
		\draw    (200.5,168.57) -- (371.69,155.65) ;
		\draw [shift={(373.69,155.5)}, rotate = 175.69] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Ellipse [id:dp6687338820163167] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (194.88,168.57) .. controls (194.88,166.67) and (196.13,165.13) .. (197.69,165.13) .. controls (199.24,165.13) and (200.5,166.67) .. (200.5,168.57) .. controls (200.5,170.46) and (199.24,172) .. (197.69,172) .. controls (196.13,172) and (194.88,170.46) .. (194.88,168.57) -- cycle ;
		
		% Text Node
		\draw (192,46) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle E$};
		% Text Node
		\draw (369,28) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle F$};
		\end{tikzpicture}
		\vspace*{3mm}	
		\caption{Schematic representation of an injective application or function}
	\end{figure}
	\item[P3.] An application or function is said to be "\NewTerm{bijective}\index{bijective application}" or "\NewTerm{total application/function}\label{bijection}" if:\\
	
	An application or function $f$ from $E$ to $F$ is both injective and surjective. In this case, we have that for any element $y$ of $F$, the equation $y=f(x)$ admits in $E$ a single (not "at least" or not "at most") pre-image $x$ and denoted:
	 
	In other words, we also write this definition as following:
	
	This is illustrated by:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,300); %set diagram left start at 0, and has height of 300
		
		%Shape: Rectangle [id:dp2678587754367949] 
		\draw   (174,66) -- (221.5,66) -- (221.5,193) -- (174,193) -- cycle ;
		%Shape: Ellipse [id:dp6290514854392619] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (194.88,89.1) .. controls (194.88,87.2) and (196.13,85.67) .. (197.69,85.67) .. controls (199.24,85.67) and (200.5,87.2) .. (200.5,89.1) .. controls (200.5,91) and (199.24,92.53) .. (197.69,92.53) .. controls (196.13,92.53) and (194.88,91) .. (194.88,89.1) -- cycle ;
		%Shape: Ellipse [id:dp45277112771508676] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (194.88,115.59) .. controls (194.88,113.69) and (196.13,112.15) .. (197.69,112.15) .. controls (199.24,112.15) and (200.5,113.69) .. (200.5,115.59) .. controls (200.5,117.48) and (199.24,119.02) .. (197.69,119.02) .. controls (196.13,119.02) and (194.88,117.48) .. (194.88,115.59) -- cycle ;
		%Shape: Ellipse [id:dp695852920766413] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (194.88,142.08) .. controls (194.88,140.18) and (196.13,138.64) .. (197.69,138.64) .. controls (199.24,138.64) and (200.5,140.18) .. (200.5,142.08) .. controls (200.5,143.97) and (199.24,145.51) .. (197.69,145.51) .. controls (196.13,145.51) and (194.88,143.97) .. (194.88,142.08) -- cycle ;
		%Straight Lines [id:da0999711204221787] 
		\draw    (200.5,89.1) -- (373.69,89.1) ;
		\draw [shift={(375.69,89.1)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da9081550983960311] 
		\draw    (200.5,115.59) -- (373.69,115.59) ;
		\draw [shift={(375.69,115.59)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da6527053580015407] 
		\draw    (197.69,142.08) -- (370.88,142.08) ;
		\draw [shift={(372.88,142.08)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da4137678012161392] 
		\draw    (200.5,168.57) -- (373.69,168.57) ;
		\draw [shift={(375.69,168.57)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Ellipse [id:dp6687338820163167] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (194.88,168.57) .. controls (194.88,166.67) and (196.13,165.13) .. (197.69,165.13) .. controls (199.24,165.13) and (200.5,166.67) .. (200.5,168.57) .. controls (200.5,170.46) and (199.24,172) .. (197.69,172) .. controls (196.13,172) and (194.88,170.46) .. (194.88,168.57) -- cycle ;
		%Shape: Rectangle [id:dp23347500883427985] 
		\draw   (352,66) -- (399.5,66) -- (399.5,193) -- (352,193) -- cycle ;
		%Shape: Ellipse [id:dp14377255785146725] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (372.88,89.1) .. controls (372.88,87.2) and (374.13,85.67) .. (375.69,85.67) .. controls (377.24,85.67) and (378.5,87.2) .. (378.5,89.1) .. controls (378.5,91) and (377.24,92.53) .. (375.69,92.53) .. controls (374.13,92.53) and (372.88,91) .. (372.88,89.1) -- cycle ;
		%Shape: Ellipse [id:dp3912778474068872] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (372.88,115.59) .. controls (372.88,113.69) and (374.13,112.15) .. (375.69,112.15) .. controls (377.24,112.15) and (378.5,113.69) .. (378.5,115.59) .. controls (378.5,117.48) and (377.24,119.02) .. (375.69,119.02) .. controls (374.13,119.02) and (372.88,117.48) .. (372.88,115.59) -- cycle ;
		%Shape: Ellipse [id:dp7536984086451557] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (372.88,142.08) .. controls (372.88,140.18) and (374.13,138.64) .. (375.69,138.64) .. controls (377.24,138.64) and (378.5,140.18) .. (378.5,142.08) .. controls (378.5,143.97) and (377.24,145.51) .. (375.69,145.51) .. controls (374.13,145.51) and (372.88,143.97) .. (372.88,142.08) -- cycle ;
		%Shape: Ellipse [id:dp017400220350601403] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (372.88,168.57) .. controls (372.88,166.67) and (374.13,165.13) .. (375.69,165.13) .. controls (377.24,165.13) and (378.5,166.67) .. (378.5,168.57) .. controls (378.5,170.46) and (377.24,172) .. (375.69,172) .. controls (374.13,172) and (372.88,170.46) .. (372.88,168.57) -- cycle ;
		
		% Text Node
		\draw (192,46) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle E$};
		% Text Node
		\draw (373,46) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle F$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Schematic representation of a bijective application or function}
	\end{figure}
We are thus naturally led to define a new application from $F$ to $E$, named "\NewTerm{inverse function}\index{inverse application}" or "\NewTerm{reciprocal function}\index{reciprocal application}" of $f$ and denoted $f^{-1}$ (some authors and teachers use the notation $^rf$ instead...) that to every element of $F$ matches the unique pre-image element $x$ of $E$ (also named sometimes "solution") of the equation $y=f(x)$. In other words:
	
	The existence of an inverse (reciprocal) function or application implies that the graph of a bijective function or application (in the set of real numbers...) and that of its inverse (reciprocal) are symmetric with respect to the right of equation $y=x$.

	Indeed, we notice that if $y=f(x)$ is equivalent to $x=f^{-1}(y)$, then these equations imply that the point $(x, y)$ is on the graph of $f$ if and only if the point $(y, x)$ is the graph of equation $f^{-1}$.

	As we can see for example in the figure below with the sinus function (\SeeChapter{see section Trigonometry page \pageref{trigonometry}}):

	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1467); %set diagram left start at 0, and has height of 1467
		
		%Shape: Rectangle [id:dp297159319602309] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (251.5,178.05) -- (352.5,178.05) -- (352.5,279.05) -- (251.5,279.05) -- cycle ;
		%Straight Lines [id:da6770495163433752] 
		\draw [line width=1.5]    (139.5,388.1) -- (464.5,69) ;
		%Shape: Wave [id:dp6427376127928364] 
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]  (94.5,236.96) .. controls (97.16,234.37) and (99.82,231.7) .. (102.5,229) .. controls (118.81,212.61) and (134.4,197) .. (152.5,197) .. controls (170.6,197) and (186.19,212.61) .. (202.5,229) .. controls (218.81,245.39) and (234.4,261) .. (252.5,261) .. controls (270.6,261) and (286.19,245.39) .. (302.5,229) .. controls (318.81,212.61) and (334.4,197) .. (352.5,197) .. controls (370.6,197) and (386.19,212.61) .. (402.5,229) .. controls (418.81,245.39) and (434.4,261) .. (452.5,261) .. controls (464.63,261) and (475.65,253.98) .. (486.5,244.42) ;
		%Shape: Wave [id:dp35908448167375995] 
		\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]  (333.14,390.74) .. controls (334.54,386.83) and (335.31,382.84) .. (335.26,378.72) .. controls (335.07,360.63) and (319.3,345.2) .. (302.75,329.07) .. controls (286.19,312.94) and (270.43,297.51) .. (270.23,279.42) .. controls (270.03,261.32) and (285.46,245.56) .. (301.66,229.08) .. controls (317.87,212.59) and (333.29,196.83) .. (333.1,178.73) .. controls (332.9,160.64) and (317.14,145.21) .. (300.58,129.08) .. controls (284.02,112.95) and (268.26,97.52) .. (268.07,79.43) .. controls (267.99,72.78) and (270.03,66.44) .. (273.45,60.26) ;
		%Shape: Axis 2D [id:dp9721041919525066] 
		\draw  (108,228.6) -- (495.5,228.6)(301.5,47) -- (301.5,409) (488.5,223.6) -- (495.5,228.6) -- (488.5,233.6) (296.5,54) -- (301.5,47) -- (306.5,54) (333.5,223.6) -- (333.5,233.6)(365.5,223.6) -- (365.5,233.6)(397.5,223.6) -- (397.5,233.6)(429.5,223.6) -- (429.5,233.6)(461.5,223.6) -- (461.5,233.6)(269.5,223.6) -- (269.5,233.6)(237.5,223.6) -- (237.5,233.6)(205.5,223.6) -- (205.5,233.6)(173.5,223.6) -- (173.5,233.6)(141.5,223.6) -- (141.5,233.6)(296.5,196.6) -- (306.5,196.6)(296.5,164.6) -- (306.5,164.6)(296.5,132.6) -- (306.5,132.6)(296.5,100.6) -- (306.5,100.6)(296.5,68.6) -- (306.5,68.6)(296.5,260.6) -- (306.5,260.6)(296.5,292.6) -- (306.5,292.6)(296.5,324.6) -- (306.5,324.6)(296.5,356.6) -- (306.5,356.6)(296.5,388.6) -- (306.5,388.6) ;
		\draw   (340.5,240.6) node[anchor=east, scale=0.75]{1} (372.5,240.6) node[anchor=east, scale=0.75]{2} (404.5,240.6) node[anchor=east, scale=0.75]{3} (436.5,240.6) node[anchor=east, scale=0.75]{4} (468.5,240.6) node[anchor=east, scale=0.75]{5} (276.5,240.6) node[anchor=east, scale=0.75]{-1} (244.5,240.6) node[anchor=east, scale=0.75]{-2} (212.5,240.6) node[anchor=east, scale=0.75]{-3} (180.5,240.6) node[anchor=east, scale=0.75]{-4} (148.5,240.6) node[anchor=east, scale=0.75]{-5} (298.5,196.6) node[anchor=east, scale=0.75]{1} (298.5,164.6) node[anchor=east, scale=0.75]{2} (298.5,132.6) node[anchor=east, scale=0.75]{3} (298.5,100.6) node[anchor=east, scale=0.75]{4} (298.5,68.6) node[anchor=east, scale=0.75]{5} (298.5,260.6) node[anchor=east, scale=0.75]{-1} (298.5,292.6) node[anchor=east, scale=0.75]{-2} (298.5,324.6) node[anchor=east, scale=0.75]{-3} (298.5,356.6) node[anchor=east, scale=0.75]{-4} (298.5,388.6) node[anchor=east, scale=0.75]{-5} ;
		
		% Text Node
		\draw (497,227.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (275,26.4) node [anchor=north west][inner sep=0.75pt]    {$y=f( x)$};
		% Text Node
		\draw (413.43,87.68) node [anchor=north west][inner sep=0.75pt]  [rotate=-316.28]  {$y=x$};
		% Text Node
		\draw (406,266.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,opacity=1 ]  {$f( x) =\sin( x)$};
		% Text Node
		\draw (146,68.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ]  {$ \begin{array}{l}
		f( x) =\sin^{-1}( x)\\
		f( x) =\arcsin( x)
		\end{array}$};
		
		\end{tikzpicture}
		\vspace*{3mm}	
		\caption{Bijective function example}
	\end{figure}

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\textbf{{\Large \ding{45}}Example:}\\\\
Take the case of a holiday station where a group of tourists must be housed in a hotel. Each way to allocate these tourists in hotel rooms may be represented by an application of all tourists to all the rooms (to each tourist is assigned a room).
	\begin{itemize}
		\item Tourists want the application to be injective, that is to say, each of them has a single room. This is only possible if the number of tourists does not exceed the number of rooms.
		
		\item The hotel manager hopes that the application is surjective, that is to say, each room is occupied. This is only possible if there are at least as many tourists than rooms.
		
		\item If it is possible to spread the tourists so that there is only one per room, and all the rooms are occupied: the application will then be both injective and surjective that is to say bijective.
	\end{itemize}
	\end{tcolorbox}
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} It comes from the definitions above that $f$ is bijective in the set of real numbers if and only if any horizontal line intersects the graph of the function at a single point. This leads us to the second following remark:\\

	\textbf{R2.} An application that satisfies the test of the horizontal line is continuously increasing or decreasing at any point in its domain.
	\end{tcolorbox}
	
	\item[P4.] An application or function is named "\NewTerm{composite application}\index{composite application}" or "\NewTerm{composite function}" if:
	
	Let $\varphi$ be an application or function from $E$ to $F$ and $\psi$ an application or function of $F$ in $G$. The application or function that associates to each element $x$ of the set $E$ an element $\psi(\varphi(x))$ of $G$ is named "\NewTerm{composed application}\index{composed application}" of $\varphi$ and $\psi$ and is denoted by:
	
	where the symbol "$\circ$" is named "\NewTerm{round}\index{round}" (do not confused with the scalar product we will see later in the section of Vector Calculus at page \pageref{dot product}!). Thus, the above relation is written "psi round phi" but has to be read "phi round psi" (...). So:
	
	Let, moreover, $\chi$ be an application (not a function!) of $G$ in $H$. We check immediately that the composition operation is associative for applications (for more details see the section of Linear Algebra page \pageref{non-commutativity matrices}):
	
	This allows us to omit parentheses and write more simply:
	
	In the particular case where $\varphi$ would be an application or function from $E$ to $E$, we note $\varphi^k$ the composed application $\varphi \circ \varphi \circ \ldots \circ \varphi$ ($k$ times).
\end{enumerate}

	What's important in what we have seen until now in this section is that all defined properties listed above are applicable to numbers' sets.
	
	Let us see a concrete and very powerful example:

	\subsubsection{Cantor-Bernstein Theorem}
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Caution!! This theorem, for which the result may seem trivial, is not necessarily easy to approach (its mathematical formalism is not very aesthetic...). We advise you to read the proof slowly and imagine the sagittal diagrams in your head during the developments.
	\end{tcolorbox}
	
	Here is the hypothesis to prove: 
	\begin{theorem}
	Let $X$ and $Y$ be two sets. If there is an injection (remember the definition of an injective function or application above) from $X$ to $Y$ and another from $Y$ to $X$, then both sets are in bijection (remember the definition of a bijective function or application above). It is therefore an antisymmetric relation.

	This is illustrated by:
	\begin{figure}[H]
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,387); %set diagram left start at 0, and has height of 387
		
		%Shape: Ellipse [id:dp9734592465323353] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (14,107.5) .. controls (14,63.04) and (41.42,27) .. (75.25,27) .. controls (109.08,27) and (136.5,63.04) .. (136.5,107.5) .. controls (136.5,151.96) and (109.08,188) .. (75.25,188) .. controls (41.42,188) and (14,151.96) .. (14,107.5) -- cycle ;
		%Shape: Ellipse [id:dp11312635854883002] 
		\draw   (181.5,107.5) .. controls (181.5,63.04) and (208.92,27) .. (242.75,27) .. controls (276.58,27) and (304,63.04) .. (304,107.5) .. controls (304,151.96) and (276.58,188) .. (242.75,188) .. controls (208.92,188) and (181.5,151.96) .. (181.5,107.5) -- cycle ;
		%Shape: Ellipse [id:dp18596374017710948] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (196,105.5) .. controls (196,78.16) and (211.78,56) .. (231.25,56) .. controls (250.72,56) and (266.5,78.16) .. (266.5,105.5) .. controls (266.5,132.84) and (250.72,155) .. (231.25,155) .. controls (211.78,155) and (196,132.84) .. (196,105.5) -- cycle ;
		%Straight Lines [id:da42029364730082985] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (75.25,27) -- (231.25,56) ;
		%Straight Lines [id:da3045352497019469] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (75.25,188) -- (231.25,155) ;
		%Shape: Ellipse [id:dp547714519681255] 
		\draw   (6.5,295.5) .. controls (6.5,251.04) and (33.92,215) .. (67.75,215) .. controls (101.58,215) and (129,251.04) .. (129,295.5) .. controls (129,339.96) and (101.58,376) .. (67.75,376) .. controls (33.92,376) and (6.5,339.96) .. (6.5,295.5) -- cycle ;
		%Shape: Ellipse [id:dp49916481336381047] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (36,272.5) .. controls (36,245.16) and (51.78,223) .. (71.25,223) .. controls (90.72,223) and (106.5,245.16) .. (106.5,272.5) .. controls (106.5,299.84) and (90.72,322) .. (71.25,322) .. controls (51.78,322) and (36,299.84) .. (36,272.5) -- cycle ;
		%Shape: Ellipse [id:dp6780409446075686] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (181.5,293.5) .. controls (181.5,249.04) and (208.92,213) .. (242.75,213) .. controls (276.58,213) and (304,249.04) .. (304,293.5) .. controls (304,337.96) and (276.58,374) .. (242.75,374) .. controls (208.92,374) and (181.5,337.96) .. (181.5,293.5) -- cycle ;
		%Straight Lines [id:da392205678165759] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (71.25,223) -- (236.5,213) ;
		%Straight Lines [id:da8268172962681364] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (71.25,322) -- (235.5,373) ;
		%Right Arrow [id:dp3454364565615464] 
		\draw   (305,176.5) -- (365.9,176.5) -- (365.9,155) -- (406.5,198) -- (365.9,241) -- (365.9,219.5) -- (305,219.5) -- cycle ;
		%Shape: Ellipse [id:dp8136409171305066] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (414,199.5) .. controls (414,155.04) and (433.59,119) .. (457.75,119) .. controls (481.91,119) and (501.5,155.04) .. (501.5,199.5) .. controls (501.5,243.96) and (481.91,280) .. (457.75,280) .. controls (433.59,280) and (414,243.96) .. (414,199.5) -- cycle ;
		%Shape: Ellipse [id:dp6315318625894815] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (547.5,199.5) .. controls (547.5,155.04) and (567.09,119) .. (591.25,119) .. controls (615.41,119) and (635,155.04) .. (635,199.5) .. controls (635,243.96) and (615.41,280) .. (591.25,280) .. controls (567.09,280) and (547.5,243.96) .. (547.5,199.5) -- cycle ;
		%Straight Lines [id:da23817558016880924] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (457.75,119) -- (591.25,119) ;
		%Straight Lines [id:da4893651218794717] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (457.75,280) -- (591.25,280) ;
		
		% Text Node
		\draw (68,7) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle X$};
		% Text Node
		\draw (235,7) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle Y$};
		% Text Node
		\draw (63,198) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle Y$};
		% Text Node
		\draw (236,196) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle X$};
		% Text Node
		\draw (452,97) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle X$};
		% Text Node
		\draw (585,99) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle Y$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Representation of an antisymmetric relation}
	\end{figure}
	\end{theorem}
	Formally this theorem is sometimes written:
	
	or more technically:
	
	For the proof we need rigorously to demonstrate beforehand a lemma (intuitively obvious... but not formally) who's statement is as follows:

	\begin{lemma}
	Let $X, Y, Z$ three sets such that $X \subseteq Z \subseteq Y$. If $X$ and $Y$ are in bijection through a function $f$, then $X$ and $Z$ are in bijection through a function $g$.

	Technically, we write this:
	
	
	An example of application of this lemma is the set of natural numbers $\mathbb{N}$ and rational numbers $\mathbb{Q}$ which are in bijection (see the section of Number Theory page \pageref{Cantor's diagonal} for the proof). Therefore, all the rational numbers are in bijection with the set of natural numbers since $\mathbb{N} \subseteq \mathbb{Z} \subseteq \mathbb{Q}$.
\end{lemma}

	\begin{dem}
	First, formally, we create a function $f$ from $Y$ to $X$ such that it is bijective:
	
	To continue we need a set $A$ that will be defined by the union of the images of the functions of the functions $f$ (of the kind $f(f(f \ldots)))$... and build such a tool is the trick of the proof!) of the pre-images of the set $Z$ (remember that $Z \subseteq Y$) from which we exclude the elements of $X$ (that we will be noted for this proof: $Z-X$):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
				% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_c77937knm}{
		\pgfdeclarepatternformonly[\mcThickness,\mcSize]{_c77937knm}
		{\pgfqpoint{0pt}{-\mcThickness}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathmoveto{\pgfqpoint{0pt}{\mcSize}}
		\pgfpathlineto{\pgfpoint{\mcSize+\mcThickness}{-\mcThickness}}
		\pgfusepath{stroke}
		}}
		\makeatother
		
		% Pattern Info
		 
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_y38wxxv95}{
		\pgfdeclarepatternformonly[\mcThickness,\mcSize]{_y38wxxv95}
		{\pgfqpoint{0pt}{-\mcThickness}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathmoveto{\pgfqpoint{0pt}{\mcSize}}
		\pgfpathlineto{\pgfpoint{\mcSize+\mcThickness}{-\mcThickness}}
		\pgfusepath{stroke}
		}}
		\makeatother
		\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt  

		%uncomment if require: \path (0,387); %set diagram left start at 0, and has height of 387
		
		%Shape: Rectangle [id:dp5361124037847305] 
		\draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ][fill={rgb, 255:red, 129; green, 184; blue, 245 }  ,fill opacity=1 ] (91,151) -- (122.5,151) -- (122.5,176) -- (91,176) -- cycle ;
		%Shape: Ellipse [id:dp6097296773063512] 
		\draw  [fill={rgb, 255:red, 239; green, 163; blue, 222 }  ,fill opacity=1 ] (181,181.5) .. controls (181,108.32) and (241.55,49) .. (316.25,49) .. controls (390.95,49) and (451.5,108.32) .. (451.5,181.5) .. controls (451.5,254.68) and (390.95,314) .. (316.25,314) .. controls (241.55,314) and (181,254.68) .. (181,181.5) -- cycle ;
		%Shape: Ellipse [id:dp4550541758243909] 
		\draw  [fill={rgb, 255:red, 129; green, 184; blue, 245 }  ,fill opacity=0.62 ] (215,171.5) .. controls (215,118.76) and (256.97,76) .. (308.75,76) .. controls (360.53,76) and (402.5,118.76) .. (402.5,171.5) .. controls (402.5,224.24) and (360.53,267) .. (308.75,267) .. controls (256.97,267) and (215,224.24) .. (215,171.5) -- cycle ;
		%Shape: Ellipse [id:dp60312843968095] 
		\draw  [pattern=_c77937knm,pattern size=6pt,pattern thickness=0.75pt,pattern radius=0pt, pattern color={rgb, 255:red, 155; green, 155; blue, 155}] (215,171.5) .. controls (215,118.76) and (256.97,76) .. (308.75,76) .. controls (360.53,76) and (402.5,118.76) .. (402.5,171.5) .. controls (402.5,224.24) and (360.53,267) .. (308.75,267) .. controls (256.97,267) and (215,224.24) .. (215,171.5) -- cycle ;
		%Shape: Ellipse [id:dp36865869567837994] 
		\draw  [fill={rgb, 255:red, 184; green, 233; blue, 134 }  ,fill opacity=1 ] (259,184) .. controls (259,148.1) and (286.65,119) .. (320.75,119) .. controls (354.85,119) and (382.5,148.1) .. (382.5,184) .. controls (382.5,219.9) and (354.85,249) .. (320.75,249) .. controls (286.65,249) and (259,219.9) .. (259,184) -- cycle ;
		%Shape: Ellipse [id:dp8079426905533325] 
		\draw  [fill={rgb, 255:red, 245; green, 166; blue, 35 }  ,fill opacity=1 ] (287.5,196) .. controls (287.5,172.25) and (301.6,153) .. (319,153) .. controls (336.4,153) and (350.5,172.25) .. (350.5,196) .. controls (350.5,219.75) and (336.4,239) .. (319,239) .. controls (301.6,239) and (287.5,219.75) .. (287.5,196) -- cycle ;
		%Shape: Circle [id:dp3963026501594642] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (252,120.5) .. controls (252,117.46) and (254.46,115) .. (257.5,115) .. controls (260.54,115) and (263,117.46) .. (263,120.5) .. controls (263,123.54) and (260.54,126) .. (257.5,126) .. controls (254.46,126) and (252,123.54) .. (252,120.5) -- cycle ;
		%Shape: Circle [id:dp168090898838114] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (310.75,176) .. controls (310.75,172.96) and (313.21,170.5) .. (316.25,170.5) .. controls (319.29,170.5) and (321.75,172.96) .. (321.75,176) .. controls (321.75,179.04) and (319.29,181.5) .. (316.25,181.5) .. controls (313.21,181.5) and (310.75,179.04) .. (310.75,176) -- cycle ;
		%Shape: Circle [id:dp42625479842381875] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (307.75,215) .. controls (307.75,211.96) and (310.21,209.5) .. (313.25,209.5) .. controls (316.29,209.5) and (318.75,211.96) .. (318.75,215) .. controls (318.75,218.04) and (316.29,220.5) .. (313.25,220.5) .. controls (310.21,220.5) and (307.75,218.04) .. (307.75,215) -- cycle ;
		%Shape: Circle [id:dp39012864263450897] 
		\draw  [fill={rgb, 255:red, 65; green, 117; blue, 5 }  ,fill opacity=1 ] (311,99.5) .. controls (311,96.46) and (313.46,94) .. (316.5,94) .. controls (319.54,94) and (322,96.46) .. (322,99.5) .. controls (322,102.54) and (319.54,105) .. (316.5,105) .. controls (313.46,105) and (311,102.54) .. (311,99.5) -- cycle ;
		%Curve Lines [id:da9848696312567122] 
		\draw    (257.5,126) .. controls (250.61,140.78) and (269.18,173.01) .. (308.92,175.89) ;
		\draw [shift={(310.75,176)}, rotate = 182.79] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Curve Lines [id:da28545989038100417] 
		\draw    (321.75,176) .. controls (359.92,157.29) and (357.58,112.37) .. (323.58,100.04) ;
		\draw [shift={(322,99.5)}, rotate = 17.95] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Rectangle [id:dp8074981647422514] 
		\draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ][fill={rgb, 255:red, 239; green, 163; blue, 222 }  ,fill opacity=1 ] (91,55) -- (122.5,55) -- (122.5,80) -- (91,80) -- cycle ;
		%Shape: Rectangle [id:dp8222919734625609] 
		\draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ][fill={rgb, 255:red, 129; green, 184; blue, 245 }  ,fill opacity=1 ] (91,86) -- (122.5,86) -- (122.5,111) -- (91,111) -- cycle ;
		%Shape: Rectangle [id:dp17112590109100734] 
		\draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ][fill={rgb, 255:red, 184; green, 233; blue, 134 }  ,fill opacity=1 ] (91,118) -- (122.5,118) -- (122.5,143) -- (91,143) -- cycle ;
		%Curve Lines [id:da12046122371941692] 
		\draw    (311,99.5) .. controls (297.14,107.42) and (256.33,178.56) .. (306.21,213.94) ;
		\draw [shift={(307.75,215)}, rotate = 213.82] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Rectangle [id:dp1992934862305189] 
		\draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ][pattern=_y38wxxv95,pattern size=6pt,pattern thickness=0.75pt,pattern radius=0pt, pattern color={rgb, 255:red, 155; green, 155; blue, 155}] (91,151) -- (122.5,151) -- (122.5,176) -- (91,176) -- cycle ;
		%Shape: Rectangle [id:dp9576289478470585] 
		\draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ][fill={rgb, 255:red, 245; green, 166; blue, 35 }  ,fill opacity=1 ] (91,183) -- (122.5,183) -- (122.5,208) -- (91,208) -- cycle ;
		
		% Text Node
		\draw (319,81) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {pre-image};
		% Text Node
		\draw (303,185) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {image};
		% Text Node
		\draw (302,221) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {image};
		% Text Node
		\draw (244,144) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle f$};
		% Text Node
		\draw (73,62) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle Y$};
		% Text Node
		\draw (73,94) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle Z$};
		% Text Node
		\draw (73,125) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle X$};
		% Text Node
		\draw (346,143) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle f^{-1}$};
		% Text Node
		\draw (40,159) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle Z-X$};
		% Text Node
		\draw (73,190) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle A$};
		\end{tikzpicture}
	\end{figure}
	In other words (if the first form is not clear...) we define the set $A$ as the union of images of $(Z-X)$ by the applications $f \circ f \circ \ldots \circ f$. What we write in a condensed a pretty way as following:
	
Because $f:Y \mapsto X$ and that $(Z-X)\subseteq Y$ we have by construction $A \subseteq X$ and thus:
	
 	Notice that we also have:
	
and by reindexing:
	
	We then have (make a pattern in your head of the arrow diagrams can help at that level of the proof...) whatever $A$:
	
	We can elegantly prove this last equality as this is one of the main result (!):
	
	Since $Z$ can be partitioned (nothing prevents us to do this!) in two disjoint subsets (we can draw a figure on request):
	
	 and without forgetting that $X \subseteq Z \subseteq Y$ and $A \subseteq X$, now we introduce as definition the function $g$ (we don't give more information about it yet) such that:
	
and for every pre-image $a$ of $g$ of the partition $((Z-X)\cup A) \subseteq Z$ we have:
	
	This means that because $((Z-X)\cup A) \subseteq Z$ and $Z \subseteq Y$ we can thus apply (associate) the bijective function $f$ (remember that $f:Y \rightarrow X$) as equivalent of the function $g$ to any element of $((Z-X)\cup A)$.

	We also have also for every pre-image $a$ of $g$ of the partition $(X-A)$ (remember that $A \subseteq X$):
	
	this means that we just apply the identity function we could also associate it to $g$.
	
	So to sum up, we have build a function $g$ that is then bijective because its restrictions to the $((Z-X)\cup A)$ is $f$ and that to $(X-A)$ is the identity which are both bijective, the first by definition, the second by construction!

	Finally there exists, by construction, a bijection between $X$ and $Z$ and we have proved indeed, the lemma that:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
\end{dem}

	Now that we have proved the Lemma let us recall the assumptions of the Cantor-Bernstein theorem using the result of the Lemma:
	
	Consider $\varphi$ an injection from $X$ to $Y$  and $\psi$ an injection for $Y$ to $X$ with $X \subseteq Y$. 
	
	We thus have:
	
	Hence:
	
	Then so far we have:
	\begin{itemize}
		\item As $\varphi$ is injective, then $X$ and $\varphi(X)$ are by definition bijective (yes! indeed, an injective function is by definition bijective when we reduce its image set to the pre-image set\footnote{but it must be notice that this work only for infinite sets!})
		
		\item As $\psi$ is injective, $\psi(\varphi(X))$ and $\varphi(X)$ are bijective.
	\end{itemize}

	Therefore from the previous proved lemma, $X$ and $\psi(\varphi(X))$ are also in bijection!
	
	By using also the lemma on $\psi(\varphi(X))$, $\psi(Y)$ and $X$, it follows that $\psi(\varphi(X))$ is the also in bijection with $\psi(Y)$ which gives us with what have just seen, that since $\psi(\varphi(X))$ and $\varphi(X)$ are in bijection, that $\psi(Y)$ is in bijection with $\varphi(X)$, then $X$ and $Y$ are indeed related by a bijective application (phew!!! it is a beautiful reasoning but it is as vicious as simple...) and we have:
	
	This theorem can obviously be interpreted in the following way: If we can count a part of a set with all the elements of another set, and vice versa, then they have the same number of elements. This is quite obvious for finished sets. The theorem above then generalizes this notion for infinite sets and this is its strength!

	From there, this theorem represents one of the basic bricks to generalize the notion of set sizes to infinite sets.
	
	\pagebreak
	\subsection{Structures}\label{structures}
	
	The so-named "\NewTerm{modern algebra}\index{modern algebra}" or "\NewTerm{abstract algebra}" begins with the theory of algebraic structures due in part to Carl F. Gauss and especially to Évariste Galois. These structures exist in a very large number but only the fundamentals will interest us here (this book being mainly dedicated, for recall, to engineers). Before describing them, here is a synoptic diagram of these main structures and their hierarchy:
	\begin{figure}[H]
		\centering
		\includegraphics[width=\textwidth]{img/arithmetics/abstract_algebra.pdf}
		\caption[Synoptic diagram of common algebraic structures]{Synoptic diagram of common algebraic structures (author: ?)}
	\end{figure}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	At the top of the diagram, the structure has the minimum number of constraints (exactly one), at the bottom, a maximum (to seven). The more we descend, the more specialized the structure is.
	\end{tcolorbox}	
	To simplify the notations, let us assume that $\star$ and $\circ$ represents a composition\footnote{This generalized notation is sometimes named "\NewTerm{stellar notation}\index{stellar notation}".} like (like the addition, subtraction, multiplication or division, ...), then:
	
	\textbf{Definitions (\#\thesection.\mydef):} Given $\star$ and $\circ $ the symbols of internal laws (this could be addition and multiplication to take the best known case) to a given set $E$ then:
	\begin{enumerate}
		\item[D1.] $\star$ is a "\NewTerm{commutative law}\index{commutative law}" if: 
		
		
		\item[D2.] $\star$ is an "\NewTerm{associative law}\index{associative law}" if:
		
		
		\item[D3.] $n$ is the "\NewTerm{neutral element}\index{neutral element}\label{neutral element}" for $\star$ if:
		
		We will also admit without proof (it is quite intuitive) that if there is a neutral element, then it is unique.
		
		\item[D4.] $a'$ is the "\NewTerm{symmetrical element}\index{symmetrical element}\label{symmetrical element}" (in the general sense of the "\NewTerm{opposite}\index{opposite}", for example for addition and "\NewTerm{inverse}\index{inverse}" for multiplication) of $a$ for $\star$ if:
		
		We shall also admit, and without proof, that the symmetric of any element is unique.		
		
		\item[D5.] $\circ$ is a "\NewTerm{distributive law}\index{distributive law}" with respect to $\star$ if:
		
	
		\item[D6.] $b$ is the "\NewTerm{absorbing element}\index{absorbing element}" if for all $a$ and a law $\star$ we have:
		
	\end{enumerate}
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} If $a$ is its own symmetric with respect to the law $\star$, mathematicians say that $a$ is "\NewTerm{involutive}\index{involutive}".\\

	\textbf{R2.} If an element $b$ of $E$ satisfies:
	
	then $b$ is say to be an "\NewTerm{absorbing element}\index{absorbing element}" for the law $\star$.\\

	\textbf{R3.} It must always be checked that the neutral and the symmetrical elements are such on the "left" \underline{and} on the "right". Thus, for example, in $(\mathbb{Z},-)$, the element $0$ is neutral to the right because $x-0=x$ but $0-x=-x$.
	\end{tcolorbox}	
	
	\subsubsection{Magma}
	\textbf{Definition (\#\thesection.\mydef):} We denote a set by the term "\NewTerm{magma $M$}\index{magma}", if the components constituting it are operable with respect to an internal law $\star$:
	
	It is therefore important to remember that if we designate an algebraic structure by the term "magma", it does not mean in any case that the internal law is commutative, associative or even that it possesses a neutral element!
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} If, moreover, the internal law $\star$ is commutative, we speak of "\NewTerm{commutative magma}\index{commutative magma}".\\

	\textbf{R2.} If, moreover, the internal law $\star$ is commutative, we speak of "\NewTerm{associative magma}\index{associative magma}".\\

	\textbf{R3.} If, moreover, the internal law $\star$ possesses a neutral element $n$, we speak sometimes of "unitary associative magma" or respectively "unitary commutative magma" but we will see just below that the both have in practice other official names.
	\end{tcolorbox}	
	
	\textbf{Definition (\#\thesection.\mydef):} In a magma $(M,\star)$, an element $x$ is named "\NewTerm{regular element}\index{regular element}" (or "\NewTerm{simplifiable element}\index{simplifiable element}") to the left if for any pair $(a,b)\in M$ we have:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	We also define following the same logic a regular element to the right.
	\end{tcolorbox}	
	Thus, an element is say to be "\NewTerm{regular}" if it is regular to the right and to the left. If $\star$ is commutative (which is the case for a commutative magma), the notions of regular element to the left or to the right coincide.
	
	A magma $(M,\star)$ is therefore an elementary algebraic structure. There are more subtle structures (monoids, groups, rings, fields, vector space, etc.) in which a set is provided with several laws and different properties. We will see them right now and use them throughout this book explicitly or implicitly.
	
	\pagebreak
	\subsubsection{Monoid}\label{monoid}
	\textbf{Definition (\#\thesection.\mydef):} If the law $\star$ is associative and has a neutral element $n$, then we say that the "unitary associative magma" is a "\NewTerm{monoid}\index{monoid}":
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} If the internal law $\star$ is commutative then we say that the structure forms an "\NewTerm{abelian monoid}\index{abelian monoid}" (or simply "\NewTerm{commutative monoid}\index{commutative monoid}").\\

	\textbf{R2.} In some textbooks we also find that the monoid is a "\NewTerm{semi-group}" (with an associative $\star$ law!) that has a neutral element $n$.
	\end{tcolorbox}
	Let us show immediately that the set of natural integers $\mathbb{N}$ is a totally ordered abelian monoid (as we have partially seen it in the section Operators page \pageref{comparators}) with respect to the laws of addition and multiplication:
	
	The addition law "+" is an internal operation such that $\forall a,b \in\mathbb{N}$ we have:
	
	We can prove that this is indeed the case by knowing that $1$ belongs to $\mathbb{N}$ such that:
	
	Therefore $c\in\mathbb{N}$ and the addition is indeed an internal law (we also say that the set $\mathbb{N}$ is "\NewTerm{stable}\index{stable}" with respect to addition) and at the same time associative since $1$ can be added to itself by definition in any order without the result being altered. If you remember that multiplication is a law that is built on addition (\SeeChapter{see section Operators page \pageref{multiplication}}), then the multiplication law $\times$ is also an internal and associative law!
	
	We will assume starting from here that it is trivial that the addition law $+$ is also commutative and that the zero "$0$" is the neutral element $n$. Thus, the multiplication law $\times$ is also commutative and it is trivial that "$1$" is the neutral element $n$.

	On the other hand, to speak about something that is not directly related to the monoids... but that will be useful to us a little further below, does it exist in line with the previous example for the addition law $+$ a symmetric $\exists c$ such that $\forall a,b\in\mathbb{N}$ we have:
	
	with $c\in\mathbb{N}$?
	
	It is quite trivial that for this equality to be satisfied we must have:
	
	thus:
	
	but negative numbers do not exist in $\mathbb{N}$. This also leads us to the conclusion that the addition law $+$ has no symmetric element in $\mathbb{N}$ and that the subtraction law $-$ does not exist in $\mathbb{N}$ (the subtraction being rigorously the addition of a number negative!).
	
	Similarly, since it will also be useful to us a little further below, is there a symmetric $a'$ for the multiplication law $\times$ such that $\forall a\in\mathbb{N}$ we have:
	
	with $a'\in\mathbb{N}$?
	
	First it is obvious that:
	
	But excepted for $a=1$, the quotient $1/a$ does not exist in $\mathbb{N}$. Therefore we must conclude that there do not exist for any element of $\mathbb{N}$ for the multiplication law $\times$ and therefore that the division law $/$ does not exist in $\mathbb{N}$ and that the multiplication law does not form a monoid in this set .

	Synthesis:
	\begin{table}[H]
		\centering
		\begin{tabular}{|
		>{\columncolor[gray]{0.75}}l |c|c|c|c|}
		\hline
		\multicolumn{1}{|c|}{\cellcolor[gray]{0.75}${\mathbb{N}}$} & \cellcolor[gray]{0.75}${(+)}$ & \cellcolor[gray]{0.75}${(-)}$ & \cellcolor[gray]{0.75}${\times}$ & \cellcolor[gray]{0.75}${/}$ \\ \hline
		Internal operation (closure) & yes &  & yes &  \\ \cline{1-2} \cline{4-4}
		Commutative & yes &  & yes &  \\ \cline{1-2} \cline{4-4}
		Neutral Element & yes ($0$) &  & yes ($1$) &  \\ \cline{1-2} \cline{4-4}
		Absorbing element & no &  & yes ($0$) &  \\ \cline{1-2} \cline{4-4}
		Symmetrical & no & \multirow{-5}{*}{no} & no & \multirow{-5}{*}{no} \\ \hline
		\end{tabular}
		\caption{Laws and their properties in $\mathbb{N}$}
	\end{table}
	We have, for example, the following properties with respect to the set of natural integers $\mathbb{N}$ and the concept of monoid:
	\begin{enumerate}
		\item[P1.] $(\mathbb{N},\le,\ge)$ is completely ordered (noticed that this notation is somewhat abusive, it suffices that there is just one of the two order relations $\mathcal{R}$ so that the set is totally ordered).
		
		\item[P2.] $(\mathbb{N},+)$ and $(\mathbb{N},\times)$ are abelian monoids.
		
		\item[P3.] The element zero "$0$" is the absorbing element for the monoid $(\mathbb{N},\times)$.
		
		\item[P4.] The laws of subtraction $-$ and division $/$ do not exist in the set $\mathbb{N}$.
		
		\item[P5.] $\mathbb{N}$ is an abelian monoid totally ordered with respect to the laws of addition $+$ and multiplication $\times$ (caution! the following notation is abusive because the monoid is composed of only one internal law and a unique order relation $\mathcal{R}$ which would give in total $4$ unique monoids):
		
	\end{enumerate}
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} It is rare to use monoids in daily work of the engineer or physicist. Indeed, this is because often when we find ourselves faced with a structure too poor to really discuss about something or develop a physical model. Then we extend it to something richer, like a group, or a field (see below) such as the set of relative integers $\mathbb{Q}$ or of real number $\mathbb{R}$ (at least...).\\

	\textbf{R2.} Say that an algebraic structure is totally ordered with respect to some laws means that given a law $\star$, and $\mathcal{R}$ an order relation and $a$, $b$, $c$, $d$ four elements of the structure concerned, then if $a\;\mathcal{R}\;b$ and $c\;\mathcal{R}\;d$ imply $(a\star c)\mathcal{R}(b\star d)$, we denote this structure $(S,\star, \mathcal{R})$ or simply $(S, \mathcal{R})$ and indicating the concerned law later in the text.
	\end{tcolorbox}	
	
	\subsubsection{Groups}\label{groups}
	\textbf{Definition (\#\thesection.\mydef):} We designate a set by the term "\NewTerm{group}\index{group}", if the constituent components satisfy the three conditions of what we name the "\NewTerm{internal group law}\label{group law}", defined below:
	
	In this case, the law of internal composition $\star$ will often (but not exclusively!) be denoted "$+$" and named "\NewTerm{addition}\index{addition}", the neutral element is denoted $e$ and its value is equal to "$0$" and the symmetric of $x$ is denoted "$-x$".

	Let us emphasize that group structure is probably one of the most important in the practice of modern engineering and physics in general. This is why it is necessary to pay special attention to it (\SeeChapter{see section Set Algebra page \pageref{groups}})!

	If, moreover, the internal law $\star$ is also commutative, then we say that the group is an "\NewTerm{abelian group}\index{abelian group}\label{abelian group}" or simply a "\NewTerm{commutative group}\index{commutative group}".

	If there exists in $G$ at least one element $a$ such that every element of $G$ is a power of$a$ or of the symmetric $a'$ of $a$, we say that $(G,\star)$ is a "\NewTerm{cyclic group of generator $a$}\index{cyclic group}" if it is finite, otherwise say that it is "\NewTerm{monogenic}" (we will come back in details on cyclic groups in the section of Set Algebra page \pageref{cyclic groups}).

	More generally, a group $(G,\star)$ of neutral element $e$, not reduced only to $\{e\}$ will be monogeneous, if there exists an element $a$ of $G$ distinct from $e$ such that $G=\left\lbrace e,a^1,a^2,a^3,\ldots,a^n,\ldots\right\rbrace$. Such a group will be cyclic, if there exists a non-zero integer $n$ for which $a^n=e$. The smallest non-zero integer satisfying this equality is then the "\NewTerm{order of the group}\index{order of a group}". 
	
	Let us show immediately that the set of relative integers $\mathbb{Z}$ is a totally ordered abelian group (as we have seen partially in the section Operators page \pageref{comparators}) with respect to the laws of addition $+$ and multiplication $\times$.

	First, to shorten the developments, it is useful to recall that the set $\mathbb{Z}$ is an "extension" of $\mathbb{N}$ by the fact that we have added to it all the symmetric negative sign numbers ($\mathbb{N}\subset \mathbb{Z}$).
	
	Thus, always with the abuse of notations (because normally a group has only one law $\star$ and one order relation $\mathcal{R}$ that is sufficient to order it):
	
	forms a completely ordered abelian group ($4$ groups by the way!) and:
	
	a completely ordered abelian monoid (two monoids in facts!).

	Let us also notice that the law of division does not exist for any element of the set $\mathbb{Z}$! So in general we say that the division does not exist in it!

	Synthesis:
	\begin{table}[H]
		\centering
		\begin{tabular}{|
		>{\columncolor[gray]{0.75}}l |c|c|c|c|}
		\hline
		${\mathbb{Z}}$ & \multicolumn{1}{c|}{\cellcolor[gray]{0.75}${(+)}$} & \multicolumn{1}{c|}{\cellcolor[gray]{0.75}$\pmb{(-)}$} & \multicolumn{1}{c|}{\cellcolor[gray]{0.75}${(\times)}$} & \multicolumn{1}{c|}{\cellcolor[gray]{0.75}${(/)}$} \\ \hline
		Internal operation & yes & yes & yes &  \\ \cline{1-4}
		Associativity & yes & no & yes &  \\ \cline{1-4}
		Commutativity & yes & no & yes &  \\ \cline{1-4}
		Neutral element & yes ($0$) & \begin{tabular}[c]{@{}c@{}}no\\ {\footnotesize ($0$ not neutral to the left)}\end{tabular} & yes ($1$) &  \\ \cline{1-4}
		Absorbing element & no & no & yes ($0$) &  \\ \cline{1-4}
		Symmetrical & \begin{tabular}[c]{@{}c@{}}yes\\ {\footnotesize (opposed sign)}\end{tabular} & yes & no & \multirow{-6}{*}{no} \\ \hline
		\end{tabular}
		\caption{Laws and their properties in $\mathbb{Z}$}
	\end{table}
	We therefore have the following properties:
	\begin{enumerate}
		\item[P1.] $(\mathbb{Z},\le,\ge)$ is completely ordered (caution! again this notation is a little abusive! It is enough that there is just one of the two order relation $\mathcal{R}$ so that the set is totally ordered).

		\item[P2.] $(\mathbb{Z},+)$ is a commutative group whose zero "$0$" is the neutral element.

		\item[P3.] The division law does not exist in the set $\mathbb{Z}$.

		\item[P4.] The set $\mathbb{Z}$ is an abelian group totally ordered with respect to the law of addition $+$ (caution! the following notation is again abusive because the group is composed only of one order relation $\mathcal{R}$ which would give a total of two groups):
		
		The set $\mathbb{Z}$ set is not a commutative group totally ordered with respect to the law of multiplication:
		
	\end{enumerate}
	We see then immediately that $\mathbb{Z}$ has too restricted properties, that is why it is interesting to extend it by the set of rational $\mathbb{Q}$ defined in a very simplistic way ... by (\SeeChapter{see section Numbers page \pageref{rational numbers}}):
	
	This means for recall that the set of rationals $\mathbb{Q}$ is defined by the set of quotients $p$ and $q$ belonging each to $\mathbb{Z}$ of which we exclude to $q$ to take the value zero (the notation $/q$ signifying for recall the "exclusion").
	
	And we obviously have:
	
	It is therefore obvious (without proof and always using the abusive notation already commented many times earlier above...) that $(\mathbb{Q},\le,\ge)$ is also totally ordered and also that $\mathbb{Q}$ is an abelian group totally ordered with respect to the law of addition $+$ only:
	
	What becomes interesting with $\mathbb{Q}$ is that the law of multiplication becomes an internal law and forms a commutative abelian group named "multiplicative group" with respect to $\mathbb{Q}^{*}$.
	\begin{dem}
	Let us prove then that the symmetric exists for the law of multiplication $\times$ such that:
	
	Since in $\mathbb{Q}^{*}$ any number can be put in the form:
	
	with $p\in\mathbb{Z}$, $q\in\mathbb{Z}^{*}$.
	
	So since:
	
	There is therefore a symmetric to every rational in $\mathbb{Q}^{*}$ for the law of multiplication.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	By definition, or by construction, the division exists in $\mathbb{Q}^{*}$ and is an internal operation. But is it associative such that $\forall (p,q,r)\in\mathbb{Q}^{*}$ we have:
	
	In fact, the verification is fairly trivial if we remember that division is defined from the law of multiplication of the inverse and that the latter law is... associative! So it comes:
	
	We can also ask ourselves the division law ($/$) is commutative such that the relation:
	
	with $\forall (a,b)\in\mathbb{Q}^{*}$.
	
	We see very well that this is not the case since we can write this last relation in the form:
	
	To sum up:
	\begin{table}[H]
		\centering
		\resizebox{\textwidth}{!}{\begin{tabular}{|
		>{\columncolor[gray]{0.75}}l |c|c|c|c|}
		\hline
		${\mathbb{Q}}$ & \multicolumn{1}{c|}{\cellcolor[gray]{0.75}${(+)}$} & \multicolumn{1}{c|}{\cellcolor[gray]{0.75}${(-)}$} & \multicolumn{1}{c|}{\cellcolor[gray]{0.75}${(\times)}$} & \multicolumn{1}{c|}{\cellcolor[gray]{0.75}${(/)}$} \\ \hline
		Internal law & yes & yes & yes & yes \\ \hline
		Associativity & yes & no & yes & no \\ \hline
		Commutativity & yes & no & yes & no \\ \hline
		Neutral element & yes ($0$) & \begin{tabular}[c]{@{}c@{}}no\\ {\footnotesize ($0$ not neutral to the left)}\end{tabular} & yes ($1$) & \begin{tabular}[c]{@{}c@{}}yes\\ {\footnotesize ($1$ neutral to the right)}\end{tabular} \\ \hline
		Absorbing element & no & no & yes ($0$) & \begin{tabular}[c]{@{}c@{}}yes\\ {\footnotesize ($0$ at the numerator)}\end{tabular} \\ \hline
		Symmetrical & \begin{tabular}[c]{@{}c@{}}yes\\ {\footnotesize (opposed sign)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}yes\\ {\footnotesize (opposed sign)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}no\\ {\footnotesize (excepted in $\mathbb{Q}^{*}$)}\end{tabular} & no \\ \hline
		\end{tabular}}
		\caption{Laws and their properties in $\mathbb{Q}$}
	\end{table}
	We therefore have the following properties:
	\begin{enumerate}
		\item[P1.] $(\mathbb{Q},\le,\ge)$ is totally ordered
	
		\item[P2.] $(\mathbb{Q},+),(\mathbb{Q}^{*},\times)$ are independently totally ordered abelian groups
	
		\item[P3.] Zero "$0$" is the absorbent element with respect to the group $(\mathbb{Q}^{*},\times)$
	
		\item[P4.] The set $\mathbb{Q}$ is an abelian group totally ordered with respect to the laws of addition and multiplication that we denote:
		
	\end{enumerate}
	The same properties are applicable to $\mathbb{R}$ and $\mathbb{C}$ but with the difference that the latter are not orderable.

	However, it may be understandable that for $\mathbb{C}$ the reader is sceptical. Let us develop all this:
	
	We must make sure that the sum "$+$", the difference "$-$", the product "$\times$" and the quotient "$/$" of two numbers of the type $x+\mathrm{i}y$ gives something of the same type again (that is to say an application $\mathbb{C}\mapsto \mathbb{C}$).

	Let us add the numbers $a+\mathrm{i}b$ and $c+\mathrm{i}d$ where $a$, $b$, $c$ and $d$ are real numbers:
	
	Therefore the addition is indeed a commutative and associative internal law for which there exists a neutral and symmetric element in the set of complex numbers $\mathbb{C}$.
	
	Let us subtracts the numbers $a+\mathrm{i}b$ and $c+\mathrm{i}d$ where $a$, $b$, $c$ and $d$ are here again, real numbers:
	
	Therefore the subtraction is an internal law, but it is not commutative, neither associative it has no neutral element to the left and neither symmetric elements.
	
	Let us now multiply the numbers $a+\mathrm{i}b$ and $c+\mathrm{i}b$ where $a$, $b$, $c$ and $d$ are still real numbers. To achieve our work, we use the distributivity of multiplication with respect to addition:
	
	Thus the law of multiplication is indeed a commutative, associative and distributive (!) internal operation for which there exists a neutral and symmetric element in $\mathbb{C}^{*}$ (see below).
	
	A division is before all a multiplication by the inverse. Prove that there exists an inverse proves that there exists a symmetric for multiplication. Let us therefore inverse the number $x+\mathrm{i}y$ where $x$ and $y$ are still real numbers (different from zero!):
	
	So the inverse of a complex number is indeed an internal non-associative and non-commutative operation for which there exists a neutral element, and it is symmetric. The same is true for division, which corresponds to the product by the inverse of a complex number.
	
	Let us now consider quickly an example of a cyclic group (we will in-deep the subject in the section of Set Algebra page \pageref{cyclic groups}): In $\mathbb{C}$, let us consider $G = \{1, \mathrm{i}, -1, -\mathrm{i}\}$ equipped with the usual multiplication of complex numbers. Then $(G,\times)$ is obviously an abelian group. Such a group is also monogenic because it is generated by the powers of one of its elements: $\mathrm{i}$ (or $-\mathrm{i}$). This monogenic group being finite, it is then a named a "cyclic group\index{cyclic group}".
	
	\subsubsection{Ring}\label{ring}
	The ring is the heart of the commutative algebra which is the algebraic structure corresponding to the high-school concepts of addition, subtraction, and multiplication.
	
	\textbf{Definition (\#\thesection.\mydef):} A commutative group (or "abelian group") $A$ is a "\NewTerm{ring}\index{ring}" if it is provided with a second internal law of composition satisfying the following properties:
	
	As we already know, the neutral element of the first internal composition law "$+$" is denoted "$0$" and named "zero" of the ring. The second internal law $\times$ is often denoted by a mid-height point "$\cdot$" (or by a cross $\times$ in high-school) and named "multiplication".
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} If, moreover, the second internal law of composition "$\times$" is also commutative, the ring is say to be a "\NewTerm{commutative ring}\index{commutative ring}\label{communtative ring}". We also encounter non-commutative rings in which the commutativity relation is not imposed or just not necessary and then we sometimes have to impose it, then we must reinforce the property of the neutral element of this second law by imposing "$1$" to be a neutral element both to right and to left such that: $1a=a1=a$ (an example of a non-commutative ring is provided by the set of square $n\times n$ matrices with coefficients in a ring $A$, for example $M_n(\mathbb{R})$ as seen in the section of Linear Algebra page \pageref{non-commutativity matrices}).\\

	\textbf{R2.} If, in addition, there exists in $A$ a neutral element for the second internal composition law "$\times$", and that this neutral element is the unit "$1$" then we say that the ring is a "\NewTerm{unitary ring}\index{unitary ring}" and "$1$" is named "\NewTerm{unit of the ring}". If the ring is commutative and has a neutral element for the second internal composition law then we speak of "\NewTerm{commutative unitary ring}\index{commutative unitary ring}".\\
	
	\textbf{R3.} If $a\times b=0\Rightarrow (a=0\text{ or } b=0)$, regardless of the elements $a$, $b$ of $A$, the ring is named an "\NewTerm{integral ring}\index{integral ring}" or "\NewTerm{ring without zero divisors}" (if not, it is obviously named "\NewTerm{non-integral ring}").\\
	
	\textbf{R4.} A "\NewTerm{factorial ring}\index{factorial ring}" is a unitary and integral commutative ring in which the fundamental theorem of arithmetic (\SeeChapter{see section Number Theory page \pageref{fundamental theorem of arithmetic}}) is verified.
	\end{tcolorbox}	
	
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] An element $a$ of a ring $A$ is a "unitary element" if there is a $b\in A$ such that $ab=ba=1$. If such a $b$ exists it is unique (we saw such an example in our study of congruence classes in the section of Number Theory page \pageref{congruence}).

		\item[D2.] An element $a$ of a ring $A$ is named a "\NewTerm{left zero divisor}\index{left zero divisor}" if there exists $x \neq 0$ such that $ax = 0$. Similarly, an element $a$ of a ring is named a "\NewTerm{right zero divisor}\index{right zero divisor}" if there exists $y\neq 0$ such that $ya = 0$. An element $a$ that is both a left AND a right zero divisor is named a "\NewTerm{two-sided zero divisor}\footnote{If the ring is commutative, then the left and right zero divisors are obviously the same.}\index{two-sided zero divisor}".
	\end{enumerate}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. The only zero divisor of the ring $\mathbb {Z}$  of integers is $0$.\\
	
	E2. In the ring of $2\times 2$ matrices (over any non-zero ring) we can find and $a$ and a non zero $x$ as following:
	
	In the latter example it is difficult to choose which of the matrix is the "zero divisor" this is why we can find in some textbooks that $a$ AND $x$ are considered as "zero divisors".
	\end{tcolorbox} 
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} It should be obvious from the preceding relations that clear that a ring is integral if and only if it has no zero divisors (excepted the $0$ as already mentioned earlier!).\\

	\textbf{R2.} The concepts of unit and zero divisor are incompatible but one element of a ring can be neither. This is the case, for example, of all integers different from $\{0,-1,1\}$ in $\mathbb{Z}$. These are neither units nor divisors from zero! We name them "\NewTerm{regular elements}\index{regular element}".
	\end{tcolorbox}	
	We will see an important example of ring in the context of our study of polynomials (\SeeChapter{see section Calculus page \pageref{polynomial ring}}), but we have already seen some very important ones in our study of congruence classes in the section of Number Theory page \pageref{congruence}.
	
	Let's see some examples of rings! During our study of groups earlier above, we have seen that the structures:
	
	are all four abelian groups and the first three are in addition totally ordered.
	
	Since the law of division is in no way associative, we can restrict ourselves to studying for each of the above groups the pair of laws: "$+$" and "$\times$".

	So it comes very quickly that:
	
	constitute unitary and integral commutative rings.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	We will consider as obvious, at this level of our discourse, that the reader will have noticed that $\mathbb{Z}$ is a "sub-ring" of $\mathbb{Q}$ in the sense that the operations defined are internal to each set and that the neutral and identity elements are identical and that there exists for each element of these sets an opposite which is in the same set. We will deepen the concept of "sub-ring" a little further.
	\end{tcolorbox}	
	Let $A$ be a ring. We have the following properties:
	\begin{enumerate}
		\item[P1.] $a+b=a+c\Rightarrow (b=c)\qquad \forall a,b,c\in A$
		\begin{dem}
		This derives from the definition D4 seen at the beginning of the part concerning the algebraic structures (every element has an opposite / symmetric). Indeed, we can add to:
		
		the element $-a$. We then get:
		
		 by the existence of the opposite this gives:
		
		hence:
		
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}

		\item[P2.] $0\cdot a=0\qquad \forall a\in A$
		\begin{dem}
		This property derives from the definitions D3 (existence of the neutral element), and D4 (existence of the opposite / symmetric), and D5 (distributivity with respect to the other law) and finally of property P1 above. Indeed, we have:
		
		We therefore have:
		
	 	The property P1 above allows us to conclude that:
		
		(we could discuss the relevance of this kind of proof...).
		\begin{flushright}
		$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}

		\item[P3.] $(-1)\cdot a=-a\qquad \forall a\in A$
		\begin{dem}
		This property is proved  using P2. We have:
		
		by adding $-a$ to this last equality, we get:
		
			\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
	\end{enumerate}
	
	\paragraph{Sub-ring}\mbox{}\\\\
	\textbf{Definitions (\#\thesection.\mydef):} Given $A$ a ring and $S\subset A$ a subset of $A$. We say that $S$ is a "\NewTerm{sub-ring}\index{sub-ring}" of $A$ if:
	\begin{enumerate}
		\item[P1.] $n\in S$ (the neutral element of $A$ is also that of $S$)

		\item[P2.] $a\in S \Rightarrow -a\in S$

		\item[P3.] $(a,b)\in S\Rightarrow a+b\in S$

		\item[P4.] $(a,b)\in S\Rightarrow a\cdot b\in S$
	\end{enumerate}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	The ring $\mathbb{Z}$ is a sub-ring of $\mathbb{Q}$ and $\mathbb{R}$.
	\end{tcolorbox}
	
	\subsubsection{Field}
	\textbf{Definition (\#\thesection.\mydef):} We denote a set of numbers by the term "\NewTerm{field}\index{field (set)}\label{field (set)}" if:
	
	Hence a field is a non-zero ring in which any non-zero element is invertible or in other words: a ring of which all non-zero elements are unit elements is a field.
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} If the internal law $\times$ is also commutative, the field is obviously named a "\NewTerm{commutative field}\index{commutative field}".\\

	\textbf{R2.} The quaternions (\SeeChapter{see section Numbers page \pageref{quaternions}}) form a "\NewTerm{non-commutative field}\index{non-commutative field}" for addition and multiplication.
	\end{tcolorbox}
	Let us see examples of field among the following unitary rings:
	
	We must first determine which ones do not constitute groups with respect to the internal law of multiplication "$\times$".

	As we have already seen in our study of the groups earlier above, it is evident that we must eliminate $(\mathbb{Z},+,\times)$ because of the existence of inverses which is not assured in this set.

	Thus, the fundamental fields of arithmetic are:
	
	and since the law of multiplication "$\times$" is commutative in these sets, we can affirm that these fields are also commutative fields.

	We often have in the small classes the following diagram for the most important field used by students, engineers and managers:
	
	Thus, we named "Field" a set $F$ of real or complex numbers $a$ such that the sum, difference, product and quotient of any two of these numbers $a$ belong to the same system $F$.

	We also state this property in the following way: the numbers of a field reproduce by the rational operations (addition, subtraction, multiplication, division). Thus it is evident that the number zero can never form the denominator of a quotient and the set of integers can not form a field because the division in the set of integers does not necessarily give a numerical value that exist in this same set.
	
	\pagebreak
	\subsubsection{Vector Spaces}\label{vector space}
	When we define a "\NewTerm{vector}" (\SeeChapter{see section Vector Calculus page \pageref{vector}}), we usually refer in high-school to an "Euclidean space" (see also the section Vector Calculus) of $n$ dimensions denoted $\mathbb{R}^n$. However, the notion of vector space is much more larger as we will see by reading the other sections and chapters of this book than the latter which represents only one particular case.
	
	\textbf{Definition (\#\thesection.\mydef):} A "\NewTerm{vector space (EV)}\index{vector space}" or "\NewTerm{$K$-vector space}" (abbreviated sometimes: $K$-ev) on the field $K$ (we will frequently take and use for this field $\mathbb{R}$ or $\mathbb{C}$) is a set $(E,+,\cdot)$ have the following  properties:
	
	We thus have two composition laws  (taking the traditional notations of the vectors which will perhaps be more intuitive and useful for the reader and for what will also follow...):
	\begin{enumerate}
		\item An internal law of composition: the addition denoted "$+$" which satisfies:
		\begin{enumerate}[label*=\arabic*.]
			\item Associativity:
			
			
			\item Commutativity: 
			
			
			\item Neutral element:
			
			
			\item Opposite element: 
			
		\end{enumerate}
		\item An external composition law: the multiplication by a scalar, denoted "$\cdot$" (to avoid the confusion with the cross product "$\times$" that we will introduce in the section of Vector Calculus page \pageref{cross product}), which satisfies:
		\begin{enumerate}[label*=\arabic*.]
			\item Associativity:
			
			
			\item Distributivity on the right with respect to the field $K$: 
			
			
			\item Left distributivity with respect to $E$:
				
	
			\item Neutral element (from $K$ to $E$):
			
		\end{enumerate}
	\end{enumerate}
	Hence $10$ properties at the total (yes indeed... the fact of having an internal or external law is a property on itself!).
	
	We then say that the vector space has a "\NewTerm{vectorial algebraic structure}" and that these elements are "\NewTerm{vectors}" (at least in the most common case...), the elements of $K$ are the "scalars" (\SeeChapter{see section Numbers page \pageref{scalar}}).
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} The respective internal laws that are frequently used as addition and multiplication that we already know very well on $\mathbb{R}$, which is very convenient for our habits...\\
	
	\textbf{R2.} From now on, to distinguish the elements of the field $K$ from the set $E$, we shall denote those of $K$ by Greek letters and those of $E$ by Latin capital letters.
	\end{tcolorbox}
	It is should not be necessary to prove that these properties are satisfied for $\mathbb{R}^n$ and consequently for $\mathbb{R}^2$. We can, however, ask ourselves some subsets of $\mathbb{R}^n$?
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. Let us consider the rectangular region of $\mathbb{R}^3$ shown in figure ($a$) and in perspective in the ($c$) below:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.9]{img/arithmetics/vector_space_concept.jpg}	
		\caption{Example of a concept of vector space}
	\end{figure}
	This subset (the rectangle) of $\mathbb{R}^2$ is not a vector space because, among other things, the internal operation property (closure) of the abelian group is not satisfied. Indeed, if we take two vectors $\vec{v}$,$\vec{w}$ inside the rectangle and add them, the result $\vec{v}+\vec{w}$ may come out of the rectangle. On the other hand, it is easy to see that the (infinite) line $\mathbb{R}^1$ illustrated in figure ($b$) follows all the properties enumerated above and, consequently, defines a vector space. Let us notice, however, that this line must pass through the origin, otherwise the property of the neutral element of the abelian group would not be respected (the neutral element then would no longer exists).\\
	
	E2. Another example of a vector space is the "\NewTerm{polynomial vector space}\index{polynomial vector space}" with real coefficients of degree two or less denoted $\mathcal{P}^2$ (\SeeChapter{see section Calculus page \pageref{polynomial vector}}). For example, two randomly chosen elements of this space are:
	
	This polynomial set respects the properties of a vector space. Indeed, if we add two polynomials of degree two or less, we get another polynomial of degree two or less. We can also multiply a polynomial by a scalar without changing the order (or degree) of that latter, and so on.... We can therefore represent a polynomial by vectors whose terms are the coefficients of the polynomial.
	\end{tcolorbox}
	
	Let us notice that we can also form vector spaces with sets of functions more general than polynomials. It is only important to respect the $10$ fundamental properties of a vector space!

	So defined, a vector space $E$ on $K$ is an action of $(K,\times)$ on $(E,+)$ which is compatible with the group law (by extension an "automorphism" - see definition below - on $(E,+)$).
	
	\textbf{Definition (\#\thesection.\mydef):} Let $E$ be a vector space, we name "\NewTerm{vectorial subspace}\index{vectorial subspace}" $F$ of $E$ a subset of $E$ if and only if (as written by nice mathematicians...):
	
	or by using another notation (the one used more by physicists and engineers):
	
	
	\subsubsection{$C$-algebra $A$}
	\textbf{Definition (\#\thesection.\mydef):} A "\NewTerm{$C$-algebra $A$}\index{$C$-algebra $A$}" where $C$ is a commutative field (also named sometimes "\NewTerm{$K$-algebra $A$}" where the $K$ state for "Körper" in German) is a set $A$ with two internal composition laws: the addition "$+$" and the cross product "$\times$" and one external law (multiplication) "$\cdot$" with operator domain $C$ (multiplication by a scalar) if and only if:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. To take an example in the line of the one on vector space examples, the Euclidean space $\mathbb{R}^3$ with the addition "$+$", the multiplication "$\cdot$" and the cross product "$\times$" is a non-associative and non-commutative $\mathbb{R}$-algebra denoted $(\mathbb{R}^3,\mathbb{R},+,\cdot,\times)$.\\
	
	E2. The set $\mathbb{C}$ is an $\mathbb{R}$-algebra (as complex numbers can be seen as a vector with two components according to what we saw in the section Numbers page \pageref{gauss plane}).
	\end{tcolorbox}
	
	
	\pagebreak
	\subsubsection{Summary}
	Ok... so far a lot of definitions and concepts. To sum up the most important algebraic structures seen so far we get the authorization of Kevin Binz to reproduce very nice visual figures that could help the reader brain to have an overview of most important concepts seen so far.
	
	So first let us introduce the different set structures in the point of view where we build them by adding each time a new property:
	\begin{table}[H]
		\centering
		\begin{tabular}{llll|l|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}\textbf{Magma}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Semigroup}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{Monoid}} & \textbf{Group} & \textbf{Abelian Group} \\ \hline
		\multicolumn{1}{|l|}{Closure} & \multicolumn{1}{l|}{Closure} & \multicolumn{1}{l|}{Closure} & Closure & Closure \\ \hline
		\multicolumn{1}{l|}{} & \multicolumn{1}{l|}{Associativity} & \multicolumn{1}{l|}{Associativity} & Associativity & Associativity \\ \cline{2-5} 
		 & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{Identity element} & Identity element & Identity element \\ \cline{3-5} 
		 &  & \multicolumn{1}{l|}{} & Inverse element & Inverse element \\ \cline{4-5} 
		 &  &  &  & Commutativity \\ \cline{5-5} 
		\end{tabular}
		\caption[Summary of most common algebraic structures]{Summary of most common algebraic structures\\(source: \url{https://kevinbinz.com/tag/identity-element/}, author: Kevin Binz)}
	\end{table}
	Of course, there is no particularly strong reason to "knife" axioms in this order. More esoteric options are available (in the below, red axioms are removed, green are re-introduced) introducing at the same time some set structure that we did not introduce before:
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.84]{img/arithmetics/abelian_other_group_types.eps}
		\end{center}	
		\caption[Summary of most common algebraic structures]{Summary of most common algebraic structures\\(source: \url{https://kevinbinz.com/tag/identity-element/}, author: Kevin Binz)}
	\end{figure}
	
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.9]{img/arithmetics/group_addition_summary.eps}
		\end{center}	
		\caption[Group Theory addition summary ]{Group Theory addition summary\\(source: \url{https://kevinbinz.com/tag/identity-element/}, author: Kevin Binz)}
	\end{figure}
	If we think of the above as a function, three inputs are relevant to us: the target set, the operator, and the identity element. Thus, we can condense the above into $(S, +, 0)$.
	
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.9]{img/arithmetics/group_multiplication_summary.eps}
		\end{center}	
		\caption[Group Theory multiplication summary]{Group Theory multiplication summary\\(source: \url{https://kevinbinz.com/tag/identity-element/}, author: Kevin Binz)}
	\end{figure}
	If we think of the above as a function, three inputs are relevant to us: the target set, the operator, and the identity element. Thus, we can condense the above into $(S, \times, 0)$\label{multiplication binary operator}.
	
	Did the above two sections feel painfully similar? Yes it seems!

	One lesson they teach you in computer science is: if you notice yourself copy-pasting code, you should try to consolidate your software into one function.

	Analogously, we can generalize addition and multiplication, like this:
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.4]{img/arithmetics/group_addition_multiplication_merge.eps}
		\end{center}	
		\caption[Merging of addition and multiplication]{Merging of addition and multiplication\\(source: \url{https://kevinbinz.com/tag/identity-element/}, author: Kevin Binz)}
	\end{figure}
	Here, we generalize our three inputs defined above:
	\begin{itemize}
		\item $S$ becomes a specific instance of an input set
		\item $+$ and $\times$ become specific instances of a general class of operator.
		\item $0$ and $1$ become specific instances of a general class of identity elements.
	\end{itemize}
	We have seen earlier that this particular set of five axioms is an "Abelian group" (also known as a "commutative group") that can be summarized as:
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=1]{img/arithmetics/abelian_group_summary_structure.eps}
		\end{center}	
		\caption[Abelian group structure]{Abelian group structure (source: \url{https://kevinbinz.com/tag/identity-element/}, author: Kevin Binz)}
	\end{figure}
	Let us marry addition-groups and multiplication-groups together, into a field:
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.85]{img/arithmetics/field.eps}
		\end{center}	
		\caption[Field set structure]{Field set structure (source: \url{https://kevinbinz.com/tag/identity-element/}, author: Kevin Binz)}
	\end{figure}
	
	\pagebreak
	\subsection{Morphisms}
	In many fields of mathematics, morphism refers to a structure-preserving map from one mathematical structure to another. The notion of morphism recurs in much of contemporary mathematics. In set theory, morphisms are functions; in linear algebra, linear transformations; in group theory, group homomorphisms; in topology, continuous functions, and so on.
	
	The concept of homomorphisms (from the Greek homoios = similar and morphê = form) was defined by mathematicians because it makes it possible to highlight remarkable properties of functions in particular with their structures, their nucleus, and what we name the "ideals" (see further below). They will allow us to identify one algebraic structure of another.
	
	\textbf{Definitions (\#\thesection.\mydef):} 
	\begin{enumerate}
		\item[D1.] If $(A,\star)$ and $(B,\circ)$ are two magmas (regardless of the notation used for internal laws), an application $f$ of $A$ into $B$ is a named "\NewTerm{homomorphism of magma}\index{homomorphism of magma}" or "\NewTerm{morphism of magma}\index{morphism of magma}" (by abuse of language we sometimes write just "homomorphism") if:
		
		In other words: if the image of a composition in $A$ is the composition of the images in $B$.
		
		\item[D2.] If $(A,\star)$ and $(B,\circ)$ are two monoids, an application $f$ of $A$ into $B$ is a "\NewTerm{homomorphism of monoid}\index{homomorphism of monoid}\label{homomorphism of monoid}" if:
		
		where $1_A$, $1_B$ are the respective neutral elements of the monoids $A$, $B$.
		
		\item[D3.] If $A$, $B$ are two rings, a "\NewTerm{homomorphism of rings}\index{homomorphism of rings}" (very important for the Cryptography section of this book page \pageref{cryptography}!) of $A$ in $B$ is a function $f:A\mapsto B$ such that for any $a,a'\in A$ we have:
		
		where $1_A$, $1_B$ are the neutral elements of the rings $A$, $B$ with respect to the multiplication "$\cdot$".
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[bullet/.style={circle,fill,inner sep=1.4pt,outer sep=0pt},>={Stealth[bend,length=4.5pt,inset=1pt]}]
			   \begin{scope}[local bounding box=R,name prefix=R-]
			   \path node[bullet,label=above:$a$](a){}
			    (1,0.6) node[bullet,label=above:$a'$](b){}
			    (-0.2,-1) node[bullet,label=below:$a+a'$](aplusb){}
			    (1.3,-0.8) node[bullet,label=below:$a\cdot a'$](adotb){};
			  \end{scope}
			  \pgfmathsetseed{42}
			  \draw plot[smooth cycle,domain=0:330,samples=11,variable=\t,tension=0.7] 
			   ([xshift={0.25*(1+rnd)*cos(\t)*1cm+0.75*cos(\t)*1cm},
			    yshift={0.5*(1+rnd)*sin(\t)*1cm}]R.\t);
			  %
			  \begin{scope}[local bounding box=S,xshift=6cm,name prefix=S-]
			   \path node[bullet,label=above:$\phi(a)$](a){}
			    (1.4,0.4) node[bullet,label=above:$\phi(a')$](b){}
			    (0.2,-1.3) node[bullet,label=below:$\phi(a+a')$](aplusb){}
			    (1.6,-0.6) node[bullet,label=below:$\phi(a\cdot a')$](adotb){};
			  \end{scope}
			  \draw plot[smooth cycle,domain=0:330,samples=11,variable=\t,tension=0.7] 
			   ([xshift={0.25*(1+rnd)*cos(\t)*1cm+0.75*cos(\t)*1cm},
			    yshift={0.5*(1+rnd)*sin(\t)*1cm}]S.\t);
			  %
			  \path (R.120) node[above=2.5em]{$A$} (S.60) node[above=2.5em]{$B$};
			  \begin{scope}[->]
			   \begin{scope}[dashed]
			     \foreach \X in {R,S}
			     {\draw (\X-a) -- (\X-aplusb);
			     \draw (\X-b) -- (\X-adotb);
			     \draw (\X-a) -- (\X-adotb);
			     \draw (\X-b) -- (\X-aplusb);
			     }
			    \end{scope} 
			    \foreach \X [count=\Y] in {a,b,adotb,aplusb}
			    {\draw (R-\X) to[bend right={sign(\Y-2.5)*20}] 
			        node[above,pos={0.5+0.1*pow(-1,\Y)*sign(\Y-2.5)}]{$\phi$} (S-\X);}   
			  \end{scope}
			\end{tikzpicture}
			\vspace*{3mm}
			\caption{Ring homomorphism}
		\end{figure}
		
		Given $f:A\mapsto B$ a homomorphism of rings. Then:
		\begin{enumerate}
			\item[P1.] $f(0)=0$
			\begin{dem}
			By:
			
			we have
			
			By adding $-f(a)$ on both sides of the equality, we get:
			
			\begin{flushright}
				$\blacksquare$  Q.E.D.
			\end{flushright}
			\end{dem}
	
			\item[P2.] $f(-a)=-f(a)$
			\begin{dem}
			This property also follows from:
			
			and of the property P1. Indeed, we have:
			
			 By adding $-f(a)$ to the two sides of the last equality, we get:
			
			\begin{flushright}
				$\blacksquare$  Q.E.D.
			\end{flushright}
			\end{dem}
			
			\item[P3.] If $a$ is a unit of $A$, then $f(a)$ is a unit of $B$ and $f(a)^{-1}=f(a^{-1})$.
			\begin{dem}
			Given $a,b\in A$ such that:
			 
			Then by $f(a\cdot b)=f(a)\cdot f(b)$ and $f(1_A)=1_B$, we have:
			
			and also:
			
			which shows that $f(b)$ is the inverse of $f(a)$ if $b$ is the inverse of $a$.
			\begin{flushright}
				$\blacksquare$  Q.E.D.
			\end{flushright}
			\end{dem}
		\end{enumerate}
		Let us now introduce a quite powerful theorem:
		\begin{theorem}
		Let us now show that a homomorphism of rings $f:A\mapsto B$ is injective if and only if the element $0$ is the only pre-image of $0$ (and therefore reciprocally), which is technically written:
		
		That is to say: the kernel is "trivial".
		\end{theorem}
		\begin{dem}
		The condition is clearly necessary. Let us show that it is sufficient:

		We therefore assume that $\ker(f)=0$. Given $a,a'\in A$ such that $f(a)=f(a')$. Then as we have a homomorphism of ring we can write:
		
		Which implies that $a-a'=0$ and therefore that $a=a'$.
	
		This shows that $f$ is injective if it is a homomorphism and that $\ker (f)=0$ is indeed a sufficient condition.
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
	
		\item[D4.]  Given $(A,+)$ and $(B,\star)$, two groups and $f$ an application $f:A\mapsto B$. We say that $f$ is a "\NewTerm{homomorphism of group}\index{homomorphism of group}\label{homomorphism of group}" if (we could just as well put the "$\star$" instead of the "$+$" in the first group and the "$+$" instead of the "$\star$" in the second group, the definition would remain the same by simply replacing the respective operators !):
		
		where $1_A$, $1_B$ are the respective neutral elements of the groups $A$, $B$. We notice that the only difference between a homomorphism of rings and a homomorphism of groups is that it has two laws instead of one and that we add the concept of inverse.

		That said, the third proposition above is in fact a consequence of the definition composed only of the first two lines. Indeed, consider a homomorphism $f$ between the groups $(A,+)$ and $(B,\star)$ with $1_A$ and $1_B$ respectively the neutral elements of $A$ and $B$, then we have:
		
		Hence:
		
		Therefore:
		
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		The exponential function $e$ and the logarithm function $\ln$ are a group homorphism of the group $(\mathbb{R}^+, +)$ on the group $(\mathbb{R}^+, \cdot)$.
		\end{tcolorbox}
	
		\item[D5.] Let $f$ be an application $f:A\mapsto B$ from one field to another. We say that $f$ is a "\NewTerm{homomorphism of field}\index{homomorphism of field}" if $f$ is a homomorphism of ring...
		
		Indeed, the fact that the homomorphism of field is the same as that of a ring is due to the fact that the difference between the two structures is that the elements of the field are all invertible (no law or law property differs between the both according to their definition of the homomorphism).
		
		Let us show now that every homomorphism of field is injective ("injective homomorphism") by remembering that earlier above we have proved that every homomorphism of rings was injective!
		\begin{dem}
		If $a$ is different from $0$ and $b=a^{-1}$ (we use here the property that the elements of a field are invertible!) then:
		
		So when $a$ is different from zero, $f(a)$ is also different from $0$ which proves that $\ker (f)=\{0\}$ and therefore that $f$ is injective.
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
	
		\item[D6.] Given $A$ and $B$ be two $K$-vector spaces and $f:A\mapsto B$ an application of $A$ into $B$. We say that $f$ is a "\NewTerm{linear mapping}\index{linear mapping}" or "\NewTerm{linear application}\index{linear application}"  or "\NewTerm{homomorphism of vector spaces}\index{homomorphism of vector spaces}" (it is implicit that this is relative to the indicated laws and for the chosen application) if:
		
		and we denote by $L(A, B)$ the set of linear applications.
		\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		\textbf{R1.} We have already defined the concept of linear application but did not specify that the two sets $A$ and $B$ were $K$-vector spaces.\\
	
		\textbf{R2.} A linear application is named a "\NewTerm{linear form}\index{linear form}" if and only if $B=K$.
		\end{tcolorbox}
	
		\item[D7.] If the homomorphism is bijective we will say that $f$ is an "\NewTerm{isomorphism}\index{isomorphism}\label{isomorphism}". If there is an isomorphism between $A$ and $B$, we say that $A$ and $B$ are "\NewTerm{isomorphic}" and we will denote this $A\simeq B$.
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		The isomorphism allows the identification of two sets with an identical algebraic structure (group, ring, etc.) but whose elements are named in a different way.
		\end{tcolorbox}
		
		\item[D8.] If the homomorphism $f$ is a purely internal mapping, then we will say that $f$ is an "\NewTerm{endomorphism}\index{endomorphism}\label{endomorphism}" (in other words, we have an endomorphism if in the definition of the homomorphism we have $A = B$).
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		If we have an endomorphism $f$ of $E$, then $f$ is then restricted to $\Im(f)$. So the term "endomorphism" just means that the application $f$ arrives in $E$ and not that it touches all the elements of $E$ itself. We have then $f(E)\subset E$ and not necessarily $f(E)=E$ because in the latter case we say that $f$ is surjective as we have already seen it.
		\end{tcolorbox}
		
		\item[D9.] If the endomorphism $f$ is furthermore bijective (hence in other words if the homomorphism is an endomorphism \underline{and} an isomorphism), then we will say that $f$ is an "\NewTerm{automorphism}\index{automorphism}\label{automorphism}".
	\end{enumerate}
	\begin{figure}[H]
		\centering
	    \begin{tikzpicture}
		\def\homomorphism{(0:0cm) circle (5.0cm)}
	  	\def\isomorphism{(180:2.5cm) ellipse (2.0cm and 3.0cm)}
	  	\def\endomorphism{(90:2.0cm) ellipse (3.0cm and 1.5cm)}
	      \begin{scope}[fill opacity=0.1]
	        \fill[magenta] \homomorphism;
	      \end{scope}
	
	      \begin{scope}[fill opacity=0.5]
	        \fill[cyan] \endomorphism;
	      \end{scope}
	
	      \begin{scope}[fill opacity=0.5]
	        \fill[orange] \isomorphism;
	      \end{scope}
	
	      \draw \homomorphism;
	      \draw \isomorphism;
	      \draw \endomorphism;
	
	      {
	        \scalefont{2.0}
	        \node[text=black] at ( 1.8,-2) {Homomorphism};
	      }
	
	      {
	        \scalefont{1.6}
	        \node[text=black] at (-2.5, 0) {Isomorphism};
	      }
	
	      {
	        \scalefont{1.4}
	        \node[text=black] at (   1, 2) {Endomorphism};
	      }
	      \node[align=left] at (  -2, 2) {Auto-\\morphism};
	    \end{tikzpicture}
	    \vspace*{3mm}
		\caption[Venn diagram of different types of homomorphisms]{Venn diagram of different types of homomorphisms\\(source: Wikipedia, author: Martin Thoma)}
	\end{figure}
	
	
	\subsubsection{Ideal}
	\textbf{Definition (\#\thesection.\mydef):} 
	Let $A$ be a commutative ring (like $(\mathbb{R},+,\cdot)$ for example). A subset $S\subset A$ is an "\NewTerm{ideal}\index{ideal}" if:
	\begin{enumerate}
		\item[P1.] For all $a,a'\in S$:
		
	
		\item[P2.] For all $a\in S$ and all $r\in A$:
		
	\end{enumerate}
	In other words, an ideal is a closed subset for addition and stable for the multiplication by any element of $A$.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	The set of even numbers $\mathbb{Z}_{2k}$ is an example of ideal of the set of relative integers $\mathbb{Z}$.
	\end{tcolorbox}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The ideals $S=\{0\}$ and $S=A$ are named the "\NewTerm{trivial ideals}".
	\end{tcolorbox}
	To know if an ideal is equal to the whole ring, it is useful to use the following property which specifies that if $A$ is a ring and $I$ an ideal of $A$, then if $1\in A$ we have $I=A$.
	\begin{dem}
	This results from the property P2 of the definition of an ideal:
	
	For any $r\in A$, we have $r=r\cdot 1\in 1$ because $1\in I$.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Another quite general example of ideal is given by the kernel of a homomorphism of rings. Indeed, let us prove that the kernel of a homomorphism $F:R\mapsto S$ is an ideal of $R$.
	\begin{dem}
	Given $a,a'\in \ker (f)$. Then:
	
	which shows that $a+a'\in \ker(f)$. Given $r\in R$, then:
	
	Which shows that $r\cdot a\in \ker(f)$.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Proposition: Let $A$ be a ring and let $a\in A$. The subset:
	
	Denoted $(a)$ or $aA$, is an ideal (we will see a concrete example after the next small set of definitions).
	
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] An ideal $I\neq A$ of a ring $A$ is named a "\NewTerm{principal ideal}" if there exists $a\in A$ such as $I=(a)$.
	
		\item[D2.] A ring of which all the ideals are principal is named "\NewTerm{principal ring}".
	\end{enumerate}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us now show that the ring $\mathbb{Z}$ is principal (because all its ideals are principal).\\
	
	Let $I$ be an ideal of $\mathbb{Z}$ (it is easy to choose one: for example, all multiples of $2$ or $3$, etc.). Let $r\in I$ be the smallest non-zero positive integer of $I$. We will show that $I=r\mathbb{Z}=(r)$.\\
	
	Let $a$ be any element of $I$. The Euclidean division allows us to write:
	
	with $0\le r' <r$ (as we have already proveed it).\\
	
	But as $r'=a-qr$ and that $a,r\in I$, by the definition of an ideal, we have $r'\in I$ (the sum or difference of the elements of an ideal belonging to the ideal). By the choice of $r$ ($r'$ being less than $r$) this implies that $r'=0$ and therefore that $a=rq$.\\
	
	Thus every element of $I$ is a multiple $r$ of $q$:
	
	So for the set of even number (denoted as we know $2\mathbb{Z}$ or $\mathbb{Z}_{2k}$) we have:
	
	\end{tcolorbox}
	The example above uses only the Euclidean division on $\mathbb{Z}$. We can then generalize this result to the rings which possess a Euclidean division. Thus, for example, the ring $K[X]$ of the polynomials (\SeeChapter{see section Calculus page \pageref{polynomial ring}}) with coefficients in a field $K$ is a principal ring because it has an Euclidean division.

	\begin{theorem}
	The ring $K[X]$ of the polynomials with coefficients in a field $K$ is a principal ring (all its ideal are principal).
	\end{theorem}
	\begin{dem}
	Let $I$ be an ideal of $K[X]$. Let us denote by $d$ the smallest degree that can have a non-zero polynomial of $I$. If $d=0$ then $1\in I$ and therefore $I=1\cdot K[X]=K[X]$. Otherwise, given $a(X)$ a polynomial of degree $d$. If $u(X)\in I$ then one can divide $u(X)$ by $a(X)$. There exists therefore $q(X),r(X)\in K[X]$  such as $\deg(r)<\deg(a)=d$ and:
	
	Therefore $r(X)\in I$ which lead to $r=0$ (otherwise contradiction with the minimality of $d$). Therefore:
	
	 We have just shown that:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	To come back on $\mathbb{Z}$... we have then proved that the only ideals are those of the form $r\mathbb{Z}$. Moreover if we have $d$ and $r$ which are integers $>1$. Then $r\mathbb{Z}\subset d\mathbb{Z}$ if and only if $d | r$.
	\begin{dem}
	If $d | r$ then there exists $n$ with $r=d\cdot n$. Let $m\cdot a$ be an element of $r\mathbb{Z}$. Therefore:
	
	which shows that $r\mathbb{Z}\subset d\mathbb{Z}$.

	Conversely, if $r\in d\mathbb{Z}$ this implies that $r$ is of the form $d\cdot n$ and this proves that $d$ divides $r$.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{theorem}
	Let us also prove that a ring $R$ is a field if and only if it possesses only the trivial ideals:
	
	\end{theorem}
	\begin{dem}
	Let us show that the condition is necessary: Let $I$ be a non-zero ideal of $R$ (that is so say different of $\{0\}$) and $r\in I$ a non-zero element. By hypothesis (that it is a field), it is invertible, that is to say that there exists $t\in R$ such that:
	
	This implies that $1\in I$ and therefore, by a result obtained earlier above $I=R$.
	
	Conversely, suppose that every ideal $I\neq R$ is the null ideal (that is to say $\{0\}$). Then if $r\in R$ is a non-zero element of $R$, the principal ideal $(r)$ must be equal to $R$. But this implies that $1\in (r)$ and therefore that there exists $x\in R$ with $r\cdot x=1$ which shows that $r$ is invertible. The ring $R$ is therefore a field.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	This characterization will enable us to quite easily prove that:
	\begin{theorem}
	Any homomorphism starting from a field is injective. That is, if $f:R\mapsto S$ is a homomorphism where $R$ is a field, then $f$ is injective.
	\end{theorem}
	\begin{dem}
	We put together what has been seen so far:
	\begin{itemize}
		\item We have proved earlier above that the kernel $\ker(f)$ of a homomorphism of ring is an ideal. 
	
		\item We have also proved just earlier above that set was a field if $\{0\}$ and $R$ were the only trivial ideals.
	\end{itemize}
	Therefore for the both point above we have either $\ker(f)=0$ or $\ker(f)=R$ for the field (as it encompass the concept of ring!).
	
	But since $f(1)=1\neq 0$ (by definition of a homomorphism!) it follows that it remains to us only the choice $\ker(f)=\{0\}$. This implies by a previous theorem (where we have proved long time before that if $\ker(f)=\{0\}$ the homomorphism is injective) that ... $f$ is injective.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Let us now study the homomorphisms whose starting ring is $\mathbb{Z}$. Let $A$ be a ring and $f:\mathbb{Z}\mapsto A$ a homomorphism. By definition of a homomorphism and by its properties, it is necessary that as we have defined long time before that $f(0)=0$ and $f(1)=1$ (among others). But we also still need that:
	
	for any $k\in\mathbb{Z}$. Thus $f$ is completely determined by the information of $f(1)$ and is therefore unique.
	
	Conversely, we show that the application $f:\mathbb{Z}\mapsto A$ defined by:
	
	is a homomorphism of rings. In summary, there exists one and only one homomorphism of $\mathbb{Z}$ in any ring $A$.
	
	\textbf{Definition (\#\thesection.\mydef):} Given $R$ a ring and $f:\mathbb{Z}\mapsto R$ the unique homomorphism defined just previously. If $f$ is injective, we say that $R$ is of "\NewTerm{zero characteristic}\index{zero characteristic}" and we denote it:
	
	Otherwise, $\ker(f)$ is a non trivial ideal of $\mathbb{Z}$ and as $\mathbb{Z}$ is therefore principal (as we have demonstrated above) it is of the form $k\mathbb{Z}$ with $k>0$. The integer $k$ is named the "\NewTerm{characteristic of $R$}\index{characteristic}" and we have therefore:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Less formally, the characteristic of a $A$ ring is the smallest positive integer $k$ such that $k \cdot 1_A = 0$. If there is none, then the characteristic is zero.
	\end{tcolorbox}	
	The definition above may not be very clear (at least for me it is not!). So let us see another approach to introduce the characteristic much more detailed...:
	
	Given $R$ a ring and any of its elements $r$, let us denote integers with boldface type. So $\pmb{1}$ is the integer number one, for instance, $\pmb{0}$ is the integer zero, whereas $0_R$ and $1_R$ are the neutral elements in $R$.

	Since $R$ under addition is an abelian group, we can as usual define:
	
	By definition of a homomorphism, we have if $f:\mathbb{Z}\mapsto R$ that:
	
	then for recall:
	\begin{itemize}
		\item $f(\pmb{m}+\pmb{n})=f(\pmb{m})+f(\pmb{n})$
		\item $f(\pmb{mn})=f(\pmb{m})f(\pmb{n})$
		\item $f(\pmb{1})=1_R$
	\end{itemize}
	so $f$ is a homomorphism of rings. Like all ring homomorphisms, $f:\mathbb{Z}\mapsto R$ has a kernel which is an ideal as we have proved just earlier, so we have $\ker(f)=\pmb{k}\mathbb{Z}$ for a unique $\pmb{k}\ge \pmb{0}$.
	
	If $k=0$, then as we have proved much earlier above, then $f$ is injective. Otherwise, we have:
	
	by definition of the kernel, which means that:
	
	for every $r\in R$. This integer $\pmb{k}$ is the "characteristic" of $S$.
	
	That is, $\text{char}(R)$ is the smallest positive number $\pmb{k}$ such that:
	
	if such a number $\pmb{k}\in\mathbb{N}$ exists, and $0$ otherwise.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. The only ring that has $\text{char}(R)=1$ such that:
	
	is the trivial ring $R=\{0_R\}$.\\
	
	E2. The ring $\mathbb{Z}$ is of zero characteristic because the unique homomorphism $f:\mathbb{Z}\mapsto \mathbb{Z}$ is the identity. It is therefore injective.\\
	
	E3. The injections $\mathbb{Z}\mapsto \mathbb{Q}$ and $\mathbb{Z}\mapsto \mathbb{R}$ show that $\mathbb{Q}$ and $\mathbb{R}$ (and also $\mathbb{C}$) are fields of zero characteristic.
	\end{tcolorbox}
	\begin{theorem}
	We now propose to prove that the characteristic of an integral ring (and in particular of a field) is equal to $0$ or to a prime number $p$.
	\end{theorem}
	\begin{dem}
	We show the contraposition. Let $R$ be a ring of characteristic $m\neq 0$ with $m$ not prime.
	
	There are then natural $n,r\in\mathbb{N}$ constraint by $n,r<m$ such that $m=n\cdot r$.

	Given $f:\mathbb{Z}\mapsto R$ the unique homomorphism (defined earlier above). By definition of $m$ we have then $f(m)=0_R$ but... (!) $f(r)\neq 0 \neq f(n)$. But then (and this it the trick of the proof!):
	
	which shows that $R$ is not integral.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The reciprocal of the theorem is not true as shown in the example of the ring $\mathbb{R}\times\mathbb{R}$ where addition and multiplication are made component by component. It is a ring of zero characteristic but with divisors of zero:
	
	\end{tcolorbox}	
	
	
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{95} & \pbox{20cm}{\score{4}{5} \\ {\tiny 16 votes, 62.5\%}} 
	\end{tabular} 
	\end{flushright}

	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Probabilities}\label{probabilities}
	\lettrine[lines=4]{\color{BrickRed}P}{robability} is the measure of the likelihood that an event will occur and therefore the calculation of probabilities handles random phenomena (known more aesthetically as "\NewTerm{stochastic processes}\index{stochastic process}" when their are time-dependent), that is to say, phenomena that do not always lead to the same outcome and that can be studied using numbers their implications and occurrences. However, even if these phenomena have variable outcomes, depending on chance, we observe a certain statistical regularity.

	Probability is quantified as a number between $0$ and $1$ (where $0$ indicates impossibility and 1$ $indicates certainty). The higher the probability of an event, the more certain we are that the event will occur.
	
	Probabilities are very important in practice, especially at very high management or political level. Indeed, in many cases, it is more practical to use a simple but uncertain rule rather than a complex but certain one, even if the true rule is deterministic and our modelling system has the fidelity to accommodate a complex rule. For example, the simple rule "the sales will increase" is cheap to develop and is broadly useful, while a rule of the form, "the sales will increase if the inflation index also increase, and there is no epidemia, no earthquakes, no conflict, no asteroids, no major flood, no political change, no strikes..." is expensive to develop, maintain and communicate, and after all of this effort is still very brittle and prone to failure.

	The concepts related to probabilities have been given an axiomatic mathematical formalization in probability theory (see further below), which is used widely in such areas of study as mathematics, statistics, finance, gambling, science (in particular physics), artificial intelligence/Machine Learning, computer science, game theory, and philosophy to, for example, draw inferences about the expected frequency of events. Probability theory is also used to describe the underlying mechanics and regularities of complex systems.
	
	\begin{fquote}[Emmanuel Kant]We measure the degree of intelligence of an individual by the quantity of incertitudes he can handle.
 	\end{fquote}

	\textbf{Definitions (\#\thesection.\mydef):} There are several ways to define a probability. Mainly we are talking about:
	\begin{itemize}
		\item[D1.] "\NewTerm{Experimental or inductive probability}\index{inductive probability}" which is the probability derived from the whole population.

		\item[D2.] "\NewTerm{Theoretical or deductive probability}\index{deductive probability}" which is the known probability through the study of the underlying phenomenon without experimentation. It is therefore an "a priori" knowledge as opposed to the previous definition that was rather referring to a notion of "a posteriori" probability.
	\end{itemize}
	As it is not always possible to determine a priori probabilities, we are often asked to perform experiments. We must then be able to pass from the first to the second solution. This passage is supposed to be possible in terms of limit (with a population sample whose size approaches the size of the whole population).

	The formal modelling of the probability calculus was invented by A.N Kolmogorov in a book published in 11933 (holocene calendar). This model is based on the probability space ($U$, $A$, $P$) that we will define a little further and that we can relate to the theory of measurement (\SeeChapter{see section Measure Theory page \pageref{measure theory}}). However, the probabilities were studied in the scientific point of view by Pierre de Fermat and Blaise Pascal in the mid 117th century (holocene calendar).

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	If you have a teacher or trainer who dare to teach statistics and probabilities only with examples based on gambling (cards, dice, match, toss, etc.) dispose it to whom it may concern because it would mean that he has no experience in the field and he will teach you anything and no matter how (examples could normally be based on industry, pharmaceutics, economy or R\&D, in short: areas daily used in companies but especially not on gambling...!).
	\end{tcolorbox}	
	
	\subsection{Event Universe}
	\begin{fquote}In physics anything that can happen (meaning has a non-zero probability of occurring) eventually will happen!
 	\end{fquote}
 	
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{itemize}
		\item[D1.] The "\NewTerm{universe of events}\index{universe of events}", or "\NewTerm{universe of observables}\index{universe of observables}", $U$ is the set of all possible outcomes (results), named "elementary events" that occur during a random determined test. The universe can be finite (countable) if the elementary events are finite or continuous (uncountable) if they are infinite.
	
		\item[D2.] Any "\NewTerm{event}\index{event}" $A$ is a set of elementary events and is part of the universe of possible $U$. It is possible that an event is composed of only a single elementary event.
	\end{itemize}
	
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Examples:}\\\\
		E1. Consider the universe of all possible blood groups, then event $A$ "the individual is Rh positive" is represented by:
		
		while the event $B$ "the individual is a universal donor" is represented by:
		
		which is therefore an elementary event.\\
		
		E2. In $(\mathbb{N},+)$ every element is regular and $(\mathbb{N},\times)$ any non-zero element is regular.
		\end{tcolorbox}
		
	\begin{itemize}
		\item[D3.] Let $U$ be a universe and $A$ an event, we say that the event $A$ "occurs" (or "is realized") if during the run of the trial the issue $i\:\left( i \in U \right)$ occurs and that $i \in A$. Otherwise, we say that $A$ "was not realized".
	
		\item[D4.] The empty subset $\varnothing$ of $U$ is named "\NewTerm{impossible event}\index{impossible event}". Indeed, if during a trial where the event $i$ occurs, we always have $i \in \varnothing$ and the event $\varnothing$ then never occurred.\\
		
		If $U$ is finite, or countably infinite, any subset of $U$ is an event, that is no longer true if $U$ is uncountable (we will see in the chapter Statistics why).
	
		\item[D5.] The set $U$ is also named "\NewTerm{certain event}\index{certain event}". Indeed, if at the end of the trial the event $i$ occurs, we have always $i\in U$ (since $U$ is the universe of events). The event $U$ then always occurred!
	
		\item[D6.] Let $A$ and $B$ be two subsets of $U$. We know that the events $A \cup B$ and $A \cap B$ are both subsets of $U$ then, events that are respectively "\NewTerm{joint events}\index{joint events}" and "\NewTerm{disjoint events}\index{disjoint events}\label{disjoint events}".
		
		\item[D7.] If two events $A$ and $B$ are such that:
		
		the two events may not be feasible during the same trial, then we say that they are "\NewTerm{mutually exclusive events}\index{mutually exclusive events}".
	
		\item[D8.] If two events $A$ and $B$ are such that:
		
		the two events may be feasible during the same trial (the possibility to see a black cat when we pass under a ladder, for example...), we say conversely that they are "\NewTerm{independent events}\index{independent events}".
		
		\item[D9.] "\NewTerm{Randomness}\index{random}" is the lack of pattern or predictability in events. A random sequence of events, symbols or steps has no order and does not follow an intelligible pattern or combination. Individual random events are by definition unpredictable, but in many cases the frequency of different outcomes over a large number of events (or "trials") is predictable.
		
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		If we randomly (i.e. uniformly) choose a real number in the interval $[0,1]$ then for every number there is a zero probability that we will pick this number. This does not mean that we did not pick any number at all!\\
	
		Similarly with the rationals, while infinite, and dense and all that, they are very very sparse in the aspect of measure and probability. It is perfectly possible that if we throw countably many darts at the real line we will hit exactly all the rationals and every rational exactly once. This scenario is highly unlikely, because the rational numbers is a measure zero set.\\
	
		Probability deals with \textit{what are the odds of that happening}? \underline{a priori}, not a posteriori!! So we are interested in measuring a certain structure a set has, in modern aspects of probability and measure, the rationals have size zero and this means zero probability. More formally, as we will see in the section Statistics page \pageref{probability density function}, the probabilities are obtained by integrating a probability density function $f(x)$ over an interval. The function is non negative and it has the property:
		
		The probability of selecting a real in an interval $[a,b]\subset \mathbb{R}$ is then given by:
		
		The reader can see now that, given a real number $x_0\in\mathbb{R}$, we have:
			
		\end{tcolorbox}	
	\end{itemize}
	
	\begin{fquote}[P. S. de Laplace]Probability theory is nothing but common sense reduced to calculation.
 	\end{fquote}

	\pagebreak
	\subsubsection{Infinite monkey theorem (Borel's law)}\label{Borel law}
	The "\NewTerm{infinite monkey theorem }\index{infinite monkey theorem}", also named sometimes "\NewTerm{Borel's law}\index{Borel's law}", states that a monkey hitting keys at random on a typewriter keyboard for an infinite amount of time will almost surely type a given text, such as the complete works of William Shakespeare. In fact the monkey would almost surely type every possible finite text an infinite number of times. However, the probability of a universe full of monkeys typing a complete work such as Shakespeare's Hamlet is so tiny that the chance of it occurring during a period of time hundreds of thousands of orders of magnitude longer than the age of the universe is extremely low (but technically not zero).
	
	In this context, "almost surely" is a mathematical term with a precise meaning, and the "monkey" is not an actual monkey, but a metaphor for an abstract device that produces an endless random sequence of letters and symbols.
	\begin{theorem}
	If we have an infinite number of monkeys each hitting keys at random on typewriter keyboards then, with probability $1$, one of them will type the complete works of William Shakespeare (something like a randomness machine, which is typing keys randomly at a typewriter, would eventually write any book that's ever existed, given enough time).
	\end{theorem}

	\begin{dem}
	Let $A_n$ be the event that the $n$th monkey types the complete works of Shakespeare. Then if there are $m$ characters on the keyboard and $N$ characters in the complete works of Shakespeare, then the probability to get the complete works of Shakespeare should be (see further below page \pageref{simple arrangements with repetitions}):
	
	for each $n$. Furthermore the $A_n$ are mutually independent (disjoint event). Hence:
	
	Infinitely many of the events $A_n$ occur i.e. infinitely many monkeys will type the complete works of Shakespeare.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem} 
	 Therefore if we accept infinity in an argument, then we can end up also accepting that "given infinity, anything can happen" (the next example about complex structures will also lead us to the same conclusion!).
	 
	 \begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	This result is a special case of a more general one named "\NewTerm{second Borel–Cantelli lemma}\index{ second Borel–Cantelli lemma}". This lemma states that if events $E_n$ are independent and the sum of the probabilities of the $E_n$ diverges to infinity, then the probability that infinitely many of them occur is $1$. Ie, if $\sum_{n=1}^{+\infty} P\left(E_{n}\right)=+\infty$ and the events $\left(E_{n}\right)_{n=1}^{+\infty}$ are independent, then $P\left(\limsup _{n \rightarrow +\infty} E_{n}\right)=1$. Formerly:
	
	\end{tcolorbox}
	\begin{fquote}[James Clerk Maxwell]The true logic of this world is in the calculus of probabilities.
 	\end{fquote}
	 
	 \pagebreak
	 \subsubsection{Do probabilities refute complex structures?}
	
	\begin{fquote}[P. S. de Laplace]The most important questions in life are indeed, for the most part, really only problems of probability.
 	\end{fquote}
 		
	Both traditional creationists and intelligent design writers have invoked probability arguments in criticisms of biological evolution. They argue that some features of biology (for an introduction to biology see \cite{miller2012miller} and for an excellent introduction to origin of life see \cite{rauchfuss2008chemical}) are so fantastically improbable that they could never have been produced by a purely natural, "random" process, even assuming the billions of years of history asserted by geologists and astronomers. They often equate the hypothesis of evolution to the absurd suggestion that monkeys randomly typing at a typewriter could compose a selection from the works of Shakespeare, or that an explosion in an aerospace equipment yard could produce a working 747 airliner (we won't give the name of the people that wrote that because the quality of their book and their religious agenda don't deserve to be quoted here!). More recent studies of this type, in an attempt to promote an "intelligent design" world-view, argue that functional biology operates on an exceedingly small subset of the space of all possible DNA sequences, and that any changes to the "computer program" of biology are, like changes to human computer programs, almost certain to make the organism non-functional (again we won't give the name of people who claim that for the same reasons as before!).
	
	\textbf{Definition (\#\thesection.\mydef):} "\NewTerm{Biological evolution}\index{biological evolution}\label{biological evolution}" is the cumulative change in the heritable characteristics properties of groups of organisms over the course of many generations through days to millions years depending on the type of organisms. It embraces everything from slight changes in the proportions of different forms of a gene due to natural environment constraints, random sequential genetic drift and genetic variations within a population to the alterations that led from the earliest self-replicating molecules structures to organisms, dinosaurs, bees, oaks, and humans.
	
	While not generally appreciated by the public at large, it is a well-known fact in the world of scientific research that arguments based on probability and statistics are fraught with potential fallacies and errors, and even "expert" researchers can fool themselves with invalid reasoning. For these reasons, rigorous courses in probability and statistics are now required of students in virtually all fields of science (keep in mind that engineering is not science, this is why actually the huge majority of engineers have a very low understanding of probabilities and statistics), and in numerous other disciplines as well. Attorneys need to be at least moderately well-versed in probability and statistics arguments and how they can go awry in the courtroom arguments. In the finance world, statistical over-fitting and other errors of probability and statistics are now thought to be a leading reason behind the fact that many strategies and investment funds which look great on paper often fail miserably in real-world usage.
	
	To illustrate the difficulties with probability arguments, mathematics teachers often ask their class (let's say it has 23 students) if they think it is likely that two or more persons in the class have exactly the same birthday. Most students say that it is highly unlikely, thinking that the chances that two people have the same particular birthday is $1/365$, and so $23$ times this amount is only $23/365$. But this argument is fallacious, indeed, if one numbers the $23$ people from $1$ to $23$, the event that all $23$ people have different birthdays is the same as the event that person $2$ does not have the same birthday as person $1$, and that person $3$ does not have the same birthday as either person $1$ or person $2$, and so on, and finally that person $23$ does not have the same birthday as any of persons $1$ through $22$.
	
	Let these events respectively be named "Event $2$", "Event $3$", and so on. One may also add an "Event 1", corresponding to the event of person $1$ having a birthday, which occurs with probability $1$. This conjunction of events may be computed using conditional probability: the probability of Event 2 is $364/365$, as person $2$ may have any birthday other than the birthday of person 1. Similarly, the probability of Event $3$ given that Event $2$ occurred is $363/365$, as person $3$ may have any of the birthdays not already taken by persons $1$ and $2$. This continues until finally the probability of Event $23$ given that all preceding events occurred is $343/365$. Finally, the principle of conditional probability implies that the probability $P(A')$ that no two people in the room have the same birthday is equal to the product of these individual probabilities:
	
	Therefore, the probability that at least two people in the class have the same birthday is given by (as both events are mutually exclusive):
	
	
	One creationist-intelligent design argument (high-school level argument...) goes like this: the human alpha-globin molecule (abbreviated HBA1), a component of haemoglobin that performs a key oxygen transfer function, is a protein chain based on a sequence of $141$ amino acids\footnote{Haemoglobin molecule is made up of four polypeptide chains: two alpha chains of 141 amino acids residues each and two beta chains of 146 amino acids residues each} (average mass of $10^{-25}$ [kg]). There are $20$ different amino acids common in living systems:
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.75\textwidth]{img/arithmetics/amino_acids.jpg}
		\caption[Common amino-acids]{Common amino-acids (author: ?)}
	\end{figure} 
	With the sequencing of the alpha-globin that is:
	
	\texttt{Val-Leu-Ser-Pro-Ala-Asp-Lys-Thr-Asn-Val-Lys-Ala-Ala-Trp-Gly-Lys-Val-Gly\\
	-Ala-His-Ala-Gly-Glu-Tyr-Gly-Ala-Glu-Ala-Leu-Glu-Arg-Met-Phe-Ser-Phe-Pro\\
	-Thr-Thr-Lys-Thr-Tyr-Phe-Pro-His-Phe-Leu-Ser-His-Gly-Ser-Ala-GIn-Val-Lys\\
	-Gly-His-Gly-Lys-Lys-Lys-Val-Ala-Asp-Ala-Leu-Thr-Ala-Val-His-Val-Hal-Hal\\
	-Hsp-Met-Pro-Asn-Ala-Leu-Ser-Ala-Leu-Ser-Asp-Leu-His-Ala-Heu-Arg-Val-Asp\\
	-Pro-Val-Asn-Phe-Lys-Leu-Leu-Ser-His-Cys-Leu-Leu-Leu-Ala-His-Leu-Pro-Ala\\
	-Glu-Phe-Thr-Pro-Ala-Val-His-Ala-Ser-Leu-Asp-Lys-Phe-Leu-Ala-Ser-Val-Ser\\
	-Thr-Val-Leu-Thr-Ser-Lys-Tyr-Arg}
	
	Or for an abbreviated version of the four polypeptide chains:
	\begin{itemize}
		\item Chain A:\\
		\texttt{VLSPADKTNV KAAUGKVGAH AGEYGAEALE RMFLSFPTTK TYFPHFDLSH GSAOVKGHGK KVADALTHAV AHVDDMPMAL SALSDLHAHK LRVDPVNFKL. LSHCLLVTLA AHLPAEFTPA VHASLDKFLA SVSTVLTSKY R}
		
		\item Chain B:\\
		\texttt{VHLTPEEKSA VTALUGKVNV DEVGGEALGR LLVVYPUTOR FFESFGDLST pDAVHGWRV KAHGKKWLGA FSDGLAHLDN LKGTFATLSE LHCDKLHWDP ENFRLLGNVL VCVLAHHFGK EFTPPVOAAY OKVVAGVANA LAHKYH}
		
		\item Chain C:\\
		\texttt{WLSPADKTNV KAAUGKVGAH AGEYGAEALE RMFLSFPTTK TYFPHFDLSH GSAOVKGHGK KVADALTNAV AHVDDHPNAL SALSDLHAHK LRVDPVNFKL LSHCLLVTLA AHLPAEFTPA VHASLDKFLA SVSTVLTSKY R}
		
		\item Chain D:\\
		\texttt{WHLTPEEKSA VTALUGKVNV DEVGEEALGR LLWYPUTOR FFESFGDLST PDAVMGNPKV KAHGKKVLGA FSDGLAHLDN LKGTFATLSE LHCDKLHVDP ENFRLLGNVL VCVLAHHFGK EFTPPVQ}
	\end{itemize}
	
	So the number of potential chains of length $141$ is\footnote{This calculation is anyway quite wrong as symmetrical permutations (mirror results) are counted - but it won't however change the order of magnitude of the result - and experimental evidence have shown that there are forbidden/unstable sequences of amino acids!} (see further below page \pageref{simple arrangements with repetitions} for the proof):
	
	The creationists and also many theists argue then that this figure is so enormous that even after billions of years of random molecular trials, no human alpha-globin protein molecule would ever appear "at random," and thus the hypothesis that human alpha-globin arose by an evolutionary process is decisively refuted.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In 12023 (holocene calendar), Ryo Mizuuchi and Norikazu Ichihashi (see \cite{mizuuchi2023minimal}), identified a 20-nucleotide RNA variant that self-reproduces! As a nucleotide is made of $5$ carbon atoms, $8$ hydrogen atoms, $1$ nitrogen atom, $1$ phosphorus atom, $4$ oxygen atoms therefore a single nucleotide contains $19$ atoms. That means the smallest known RNA that can self-reproduce has only $19\cdot 20= 380$ atoms!
	\end{tcolorbox}
	
	One major fallacy in the alpha-globin argument mentioned above, common to many others of this type, is that it ignores the fact that a large class of alpha-globin molecules can perform the essential oxygen transfer function, so that the computation of the probability of a single instance is misleadingly remote. Indeed, most of the $141$ amino acids in alpha-globin can be changed without altering the key oxygen transfer function, as can be seen by noting the great variety in alpha-globin molecules across the animal kingdom. When one revises the calculation above, based on only $25$ locations essential for the oxygen transport function (which is a generous over-estimate), one obtains $\color{red} 10^{33}$ fundamentally different chains, a enormous figure but incomparably smaller than $10^{183}$. And keep in mind that it's the number if we assume a total independence! The slightest constraint can greatly reduce this value. Keep in mind, for example, the problem of the 100 prisoners whose probability goes from $8\cdot 10^{-31}$ to about $\sim 31\%$ just by adding a stupid constraint (see Wikipedia for the explanation of this problem and the detailed proof)...
	
	A calculation such as this can be refined further, taking into account other features of alpha-globin and its related biochemistry. Some of these calculations produce probability values even more extreme than the above. But do any of these calculations really matter\footnote{For example a physicist applying the scientific method with the frequentist approach will calculate the odds of the appearance of life with the available experimental evidence! Thus the current data available to us gives us $1$ observable universe and $1$ life, meaning $100\%$ probability (i.e. the probability of observing life statistically according to the available sample).}? The main problem is that all such calculations, whether done accurately or not, suffer from the fatal fallacy of presuming that a structure such as human alpha-globin arose by a single all-at-once random trial event. But generating a molecule "at random" in a single shot is decidedly not the scientific hypothesis in question -- this is a creationist theory, not a scientific theory. Instead, available evidence from hundreds of published studies on the topic has shown above any reasonable doubt that alpha-globin arose as the end product of a long sequence of intermediate steps, each of which was biologically useful in an earlier context.
	
	In short, the creationist-intelligent design argument claiming that scientists assert an all-at-once "at random" creation of various biomolecules, and then asserting that this is probabilistically impossible, is a classic "straw man" fallacy. Scientists do not believe this, so this line of argumentation is completely invalid. In other words, it does not matter how good or how bad the mathematics used in the analysis is, if the underlying model is a fundamentally invalid description of the phenomenon in question. Any simplistic probability calculation of evolution that does not take into account the step-by-step process by which the structure came to be is almost certainly fallacious and can easily mislead!
	
	Some of the difficulties with creationist probability arguments can be illustrated by considering snowflakes (even if we must keep in mind that \textit{comparison is not reason}!). Bentley and Humphrey's book \textit{Snow Crystals} includes over $2,000$ high-resolution black-and-white photos of real snowflakes, each with intricate yet highly regular patterns that are almost perfectly six-way symmetric (see \cite{libbrecht2007formation} for more details).
	
	By employing a reasoning based on six-way symmetry, one can calculate the chances that one of these structures can form "at random" as roughly one part in $10^{2500}$. This probability figure is even more extreme than some that have appeared in the creationist-intelligent design literature. So is this proof that each individual snowflake has been designed by a supernatural intelligent entity? Obviously not.
	
	One of the fallacy here, once again, is presuming an all-at-once random assembly of molecules. Instead, snowflakes, like biological organisms, are formed as the product of a long series of steps acting under well-known physical laws, and the outcomes of such processes very sensitively depend on the starting conditions and numerous environmental parameters. It is thus folly to presume that one can correctly reckon the chances of a given outcome by means of superficial probability calculations that ignore the processes by which they formed.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Notice also the art of cherry picking from theists and creationists. A deity created something "perfect" as DNA (or trees whatsoever!). However he did it with almost $4$ or $6$ thousands genetic disease (coding errors), with a light source (Sun) that gives cancer, on a planet (Earth) where the majority of water is undrinkable\footnote{Water quantities types varies hugely across time on Earth (for example new research indicates that our planet was an ocean world some 3 billion years ago). But actually in the 21st century water from oceans and marginal seas, saline groundwater and water from saline closed lakes amount to over $97\%$ of the water on Earth (corresponding to approximately $70.9\pm 3.5\%$ of Earth's area, seas represents approximetely $11.3\%$). Only $2.5\%$ is made of freshwater (itself made of $1.2\%$ of surface water, $30.1\%$ of ground-water and $88.7\%$ of glaciers and ice caps).}, near a star (Sun) that will explode, in a orbit that will make the planet unlivable in a few billion years, in a finite resource ecosystem with deadly rampant diseases, in a universe in majority uninhabitable... Yes logic makes sense!
	\end{tcolorbox}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{img/arithmetics/snowflakes.jpg}
	\end{figure} 
	
	It's time to put some numbers on the formulas of statistical mechanics to bring home just how impressive the things going on inside our cells actually are.

	To start, we live (actually...) at temperatures of approximately $300$ [K]. If you have studied statistical mechanics you know that the kinetic energy of a molecule is given by (\SeeChapter{see section Continuum Mechanics page \pageref{virial theorem}}):
	
	Therefore:
	
	Or in terms of molar mass:
	
	Therefore for water with $18.01528\cdot 10^{-3}\;[\text{kg}\cdot\text{mol}^{-1}]$ this gives at room temperature:
	
	This is not a too bad result as many sources give $590\;[\text{m}\cdot\text{s}^{-1}]$. We will take that latter value.
	
	The thermal velocity relation works even for something as large as RNA polymerase II ($\sim 830\cdot 10^{-24}$ [kg]). 
	
	To make things really easy lets work with a molecular complex of mass even bigger (i.e. more complex). Something as: $1\cdot 10^{-21}$ [kg]. Such a mass would have an average thermal velocity of:
	
	Let us take a lower bound of $2\;[\text{m}\cdot\text{s}^{-1}]$ according to some non peer-reviewed sources.

	Cells are small. The 3 polymerases transcribed DNA into RNA have masses in the order of $1\cdot 10^{-21}$ [kg] range. So how long should it take them to traverse a quite big of nucleus 10 microns ($10^{-5}$ [m]) in diameter? If it's going at $2\;[\text{m}\cdot\text{s}^{-1}]$ so it will traverse it statistically in average $250,000$ times in a second or one time every $4\;[\mu\text{s}]$.

	We clearly left something out — nothing in the cell moves in a straight line. It is very crowded, so that even though things are moving very quickly their trajectory isn't straight obviously!
	
	We have proved in the section of Continuum Mechanics (page \pageref{mean free path}) that the average distance travelled by a molecule (assumed as spherical) between two collisions is given by:
	
	
	And we proved at the same place that the number of collision per second were given by:
	

	To calculate how much water can fit into a cell nucleus, we need to know how big is that latter. A source says that water can be considered a squashed sphere of maximum radius of $1.41$ Angstroms ($10^{-10}$ [m]).

	So what is the volume of a water molecule? It's:
	
	So the density of molecules is given by:
		
	
	Therefore:
	
	and (still for water only and in one cube meter!):
	
	so it's like to meet another molecule of water every (roughly) $10^{-12}$ seconds ($1$ pico-second).
	
	Notice a billion years has:
	
	So we have roughly by billion years a total of collision for one cubic meter equal to:
	
	And as the amount of water on Earth is estimated roughly at (assuming that all this water was involved equally on the whole planet to the same chemical processes and conditions...):
	
	Therefore on the whole Earth we have:
	
	So per billion years we have $1,000$ times more collisions than the number $10^{33}$ of combinations of different chains of the alpha-globin. Even if we consider a factor $100$ of errors in the number of collisions per second (it's not impossible that it is our magnitude of error in our estimations!), we would need only $1$ billion years to try all $10^{33}$ combination just by random collisions.
	
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Keep in mind that this is the result for considering only one Earth planet like (ours). Whatever the result, we should have multiplied it by the lower bound approximation of Earth like planets in our Galaxy (estimated to at least $1$ billion. ie $10^9$) and the number of galaxies in the observable Universe (estimated at least $1$ trillion, ie $10^{12}$). And this is only for the  \underline{observable} Universe... (keeping in mind that the Universe is to our actual knowledge very likely flat and therefore infinite with $\Omega_0=1$ or $k=0$ as seen in the section of Cosmogony page \pageref{critical density}).
	\end{tcolorbox}
	\label{andromeda m31 hd}
	\begin{figure}[H]
		\centering
		\includegraphics[width=\textwidth]{img/cosmology/andromeda_hd_nasa.jpg}
		\caption[NASA/ESA Hubble space telescope sharpest and largest M31 (Andromeda galaxy) image ever]{Hubble space telescope sharpest and largest M31 (Andromeda galaxy) image ever with 1.5 billion pixels with approximately 100 million stars across over 40,000 light-years released the in 12015, 5 January (credit: NASA, ESA, J. Dalcanton (University of Washington, USA), B. F. Williams (University of Washington, USA), L. C. Johnson (University of Washington, USA), the PHAT team, and R. Gendler)}
	\end{figure}
	
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	So far we see that the main factor here for the number of collisions is not the radius of the molecules involved, nor their speed or their mass but their quantity (i.e. density). Obviously the time is important but there is now too much incertitude about the range of that latter.
	\end{tcolorbox}
	
	But let us be more realistic! We should take into account that:
	\begin{enumerate}
		\item Water is not amino acids (the biggest amino acid has a weight of $204.2\;[\text{g}\cdot\text{mole}^{-1}]$) and there is elastic collisions between water molecules and such organic molecules.
		
		\item The water molecules density is not equal to that of amino acids density per litre of water. Sadly this is a value it seems we don't have access to and is highly speculative\footnote{It is related to the problem of proportion compounds of the Miller-Urey experiment.}.
	\end{enumerate}
	Let us consider the relation for an elastic collision in one dimension (\SeeChapter{see section Classical Mechanics page \pageref{elastic collision one dimensions}}):
	
	And let us consider the worst case where $v_{2i}=0$. Then:
	
	So the speed of amino acids would only decrease our number of collisions by maximum a factor $10$. Not enough to make "life" something rare on the scale of a billion years (to make it rare we should have at least a decrease in an order of $10,000$).
	
	Let us do now some reverse engineering. To make emerging life rare on a billion years scale, as already mentioned and thanks to previous calculations, we should have a least a decrease of $10,000$. Thanks the to the elastic collision, we can take down this value to $1,000$. 
	
	So we should found somewhere, something in our calculations, to make the collisions $1,000$ times more rare. The idea would take this into the parameter $n$ (molecules density) in the above relations. If we assume that the ratio of amino acid to water molecules is at $1$ for $1,000$ then it starts to make things difficult on the scale of a billion years! That means in 1 moles of water (i.e. $18$ [g] or $18$ [ml]), we would have $0.018$ [g] of amino acid.
	
	So as other extra-solar planets were something unlikely a century ago, it is very likely that the future provides evidence that life is something very common on planets that have water or atmospheres with the right components and temperature on a time range of the billion years.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Some scientifically illiterate people may argue that Roger Penrose calculated that the probability for our Universe (on the assumption that the latter is in a Black Hole...) to be in the low entropy he is now is about $10^{123}$ to $1$. Hence it is impossible for our Universe to not have been created by a deity (passing under silence the question of who created that complex deity...). But it is irrelevant! Even if what Penrose said is $100\%$ accurate or off by a factor of a googol, it's irrelevant!! Indeed, if we run a computer program that emulates a coin flipped one billion times and that we write the sequence of heads and tails to a file, then we just produced a sequence more than a trillion trillion trillion times rarer!!  In fact, we could run that same computer program until the end of human existence and it would likely never produce that same string of outcomes. And yet it occurred nonetheless... 
	\end{tcolorbox}
	If the reader has time, he can compute the probability of his own existence. This, may be computed naively as the probability that a particular sperm and egg united multiplied by the probability that his mother and father met, that his grandparents met, and so on back through the generations. We bet the number obtained is quite small. The point is that low probability events happen every day in this universe. Once they do happen, their probabilities are $100\%$.
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The reader will also note the art of cherry picking among believers and creationists. A deity would have created something "perfect" like DNA (or trees, whatever!). However it would have done it with nearly $4$ to $6$ thousand genetic diseases (coding errors), with a source of light (Sun) which gives cancer, on a planet (Earth) where the majority of water is undrinkable near a star (Sun) which will explode and which is in an orbit which will make it unlivable in a few billion years, with an ecosystem of limited resources with endemic deadly diseases, in a universe mostly uninhabitable... Yes that makes sense hmmm...
	\end{tcolorbox}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.86\textwidth]{img/arithmetics/nature_timespiral.jpg}
		\caption[Nature Timespiral]{Nature Timespiral (author: Pablo Carlos Budassi)}
	\end{figure} 
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Another fallacy of bigots is that the probability of cellular life's arising from non-living matter (abiogenesis) as calculated by Fred Hoyle is about one-in-$10^{40,000}$. But there is no sources from Fred Hoyle with the detailed mathematical derivation that lead to this value. Another lie from religious people...! There is also the Junkyard tornado analogy used by bigots but as we have already proved it: all analogies are wrong equivalences!
	\end{tcolorbox}
	
	\begin{fquote}[Richard Leakey]I have your letter and the best thing I can do is refer to my published works, both scientific and popular. The creationist movement is lead by a dishonest bunch of operators and misquotation is the hall mark of their work. Responding to them is time wasting and a letter would not be adequate to put your questions to rest. There are some things best ignored and the stupidity of these so called religious fanatics continues to astonish me.
 	\end{fquote}
 	
 	\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|}
			\rowcolor[HTML]{C0C0C0}
			\hline \textbf{Description} & \begin{tabular}[c]{@{}l@{}}\textbf{Probability}\\ (or estimated)\end{tabular} \\
			\hline Probability of dying one day or another & $100 \%$ \\
			\hline \begin{tabular}[c]{@{}l@{}}Probability of >7 Richter scale earthquake somewhere\\ on Earth during your lifetime\end{tabular} & $100 \%$ \\
			\hline Probability of erupting volcano somewhere on Earth during your lifetime  & $100 \%$ \\
			\hline Six Sigma management probability to produce a defect-free product  & $99.99966\%$ \\
			\hline  \begin{tabular}[c]{@{}l@{}}Banking regulatory sector (Basel Committee 12022)\\ Value at Risk (VaR) threshold\end{tabular}   & $99\%$ \\
			\hline  \begin{tabular}[c]{@{}l@{}}Banking regulatory sector (Basel Committee 12022)\\ Expected Shortfall (VaR) threshold\end{tabular}   & $97.5\%$ \\
			\hline Prior statistical test typical power (type II error) & $80 \%$ \\
			\hline Probability of flipping a coin and it landing on heads & $50 \%$ \\
			\hline Probability of experiencing a pandemic in one's lifetime & $\sim 38 \%$ \\
			\hline The 100 prisoners problem optimal solution & $\sim 31.18 \%$ \\
			\hline Probability of massive star forming a black hole (unovae) & $\lesssim 10-30 \%$ \\
			\hline Probability of identical birthday in a group of $23$ people & $50 \%$ \\
			\hline Risk of dying from any given cancer during lifetime & $\sim 18 \%$ \\
			\hline Probability of rolling a six on a six-sided dice & $16.\bar{6} \%$ \\
			\hline Probability of miscarriage & $\sim 15.3\%$ \\
			\hline Typical $p$-value threshold in social and medical science (type I error) & $ 5 \%$ \\
			\hline Probability of baby being born with a birth defect & $\sim 2.63 \%$ \\		
			\hline Probability of winning roulette game in a casino & $\sim 2.6 \%$ \\
			\hline Probability of drawing an Ace of Spades from a deck of cards & $\sim 1.92 \%$ \\
			\hline Risk of having a stroke for people aged between 35-40 & $1.4-2.3\%$ \\
			\hline Typical $p$-value threshold in engineering science (type I error) & $ 1 \%$ \\
			\hline Risk of dying from a heart attack during lifetime & $\sim 0.99 \%$ \\
			\hline Probability of a newborn to have a genetic anomaly  & $\sim 0.2 \%$ \\
			\hline Probability $10$ km size asteroid colliding Earth during your lifetime & $0.0000830\%$ \\
			\hline Typical $p$-value threshold in particle physics (type I error) & $0.000001\%$ \\
			\hline Probability of getting a royal flush in poker & $0.00000154\%$ \\
			\hline Probability of winning the Powerball jackpot & $0.00000021\%$ \\
			\hline Fatality risk for an average person using an average commercial flight & $\sim 3\cdot 10^{-8}$ \\
			\hline Fatality risk for an average person crossing an average road & $\sim 4\cdot 10^{-9}$ \\
			\hline Fatality risk for an average person using an average lift & $\sim 1\cdot 10^{-10}$ \\
			\hline Probability of tunnel effect in polonium decay process & $10^{-15}$ \\
			\hline \begin{tabular}[c]{@{}l@{}}Probability of alpha-globin created completely by chance based on\\  amino-acides on Earth like planets in the observable Universe\end{tabular}  & $\sim 10^{-4}-10^{-10}$ \\
			\hline \begin{tabular}[c]{@{}l@{}}Probability of alpha-globin created completely by\\ chance based on amino-acides only on Earth\end{tabular}  & $10^{-25}-10^{-31}$ \\
			\hline \begin{tabular}[c]{@{}l@{}}Probability of the actual entropy\\of our Universe according to Roger Penrose calculation\end{tabular}  & $\sim 10^{-123} \%$ \\
			\hline Probability of rolling fifty six-sided dices and get a given result & $\sim 10^{-300} \%$ \\
			\hline Probability a monkey rewrites \textit{Hamlet} by chance & $\sim 10^{-182,856}$ \\			
			\hline Probability of being born & $\sim 10^{-2,685,000}$ \\
			\hline Probability of human to cross a wall by tunnelling effect & $\sim 10^{-4.10^{30}}$ \\
			\hline Probability of our universe to exist as it is & $\sim 10^{10^{500}}$ \\
			\hline
		\end{tabular}
		\caption[Some famous probabilities (and estimations) values]{Some famous probabilities (and estimations) values as known in the early 121st century}
	\end{table}
	
	\begin{fquote}[David Frum]Nature wants $5$ of your $7$ children dead. It wants you dead by $50$. Everything better than that is brought to you by science and technology.
 	\end{fquote}

	\pagebreak
	\subsection{Kolmogorov's Axioms}\label{kolmogorov axioms}
	In Chapter 2 of \textit{A First Course in Probability}, Sheldon Ross cautions us against "defining" probability as the limiting proportion of events over repeated samples/ trials/experiments: «\textit{...it does not at all seem to be a priori evident that this [limiting frequency] need be the case. Also,..., how do we know that if the experiment is repeated..., we shall again obtain the same limiting proportion...? ...would it not be more reasonable to assume a set of simpler... axioms... and then attempt to prove [mathematically] that such a constant limiting frequency does in some sense exist... [using] the law of large numbers... This latter approach is the modern axiomatic approach to probability theory...}»

	On the surface, this seems entirely appropriate; however, under this formalism, it appears we must treat probability, $P$, as an ethereal and ephemeral characteristic of an event $A$ pertaining to any particular trial - i.e., an unfalsifiable causative propensity. Then, the strong law of large numbers (SLLN) is a theorem to be rigorously proven\footnote{The detailed proof of the strong law of large numbers is actually, due of its difficulty, out of the scope of this textbook. That's also the reason why the huge majority of engineers and postgraduate physicists, astrophysicts, cosmologists, biologists, chemists, etc. don't study it during their curriculum.} mathematically stating that: the limiting proportion of events over repeated trials converges to this very same propensity. Ie:
		
	 From this perspective, the existence of a limiting proportion is a consequence resting on unfalsifiable axioms!
	 
	 \begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
  	The weak law of large numbers states that for a specified large $n$, the average $\bar{X}_n$ is likely to be near $\mu$ (see page \pageref{weak law of large numbers} for the detailed proof). Thus, it leaves open the possibility that $\left|\bar{X}_n-\mu\right|>\varepsilon$ happens an infinite number of times, although at infrequent intervals.\\

	The strong law of large numbers shows that this almost surely will not occur. Note that it does not imply that with probability $1$, we have that for any $\varepsilon>0$ the inequality $\left|\bar{X}_n-\mu\right|<\varepsilon$ holds for all large enough $n$, since the convergence is not necessarily uniform on the set where it holds.
	\end{tcolorbox}	

	The probability of an event is somehow responding to the notion of frequency of a random phenomena, in other words, at each event we will attach a real number in the interval $[0,1]$, which measure the probability (chance) of realization. The properties of frequencies we can highlight during various trials allow us to determine the properties of probabilities.
	
	Let $U$ be a universe. We say that we define a probability on the events of $U$ if to any event $A$ of $U$ we associate a number or measure $P(A)$, named "\NewTerm{a priori probability of event $A$}\index{a priori probability}" or "\NewTerm{marginal probability of $A$}\index{marginal probability}". 
	
	Here are the "\NewTerm{Kolmogorov's axioms}\index{Kolmogorov's axioms}":
	\begin{enumerate}
		\item[A1.] For any event $A$:
		
		Thus, the probability of any event is a real number between $0$ and $1$ inclusive (this is common human sense...).

		\item[A2.] The probability of the certain event or of the set (sum) of possible events is equal to $1$:
		

		\item[A3.] If $A \cap B = \varnothing$, ie the two events are incompatible (disjoint), then:
		
		the probability of the merge ("or") of two mutually incompatibles events (or mutually exclusive) is therefore equal to the sum of their probabilities (law of addition). We then speak of "\NewTerm{disjoint probability}\index{disjoint probability}\label{disjoint probability}".
	
		We understand better that the third axiom requires  $A \cap B = \varnothing$ otherwise the  sum of all probabilities could be greater than 1 (imagine again the set diagram of the two events in your head!).
	\end{enumerate}
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} Probabilities are often communicated as percentages. So be very careful about typical biases of mass media (and not only!) when dealing with them. We have already warn you, with example, about this trap on page \pageref{percentage}.\\
	
	\textbf{R2.} You must also be very careful with random match probabilities! The most well known case is named the "\NewTerm{prosecutor's fallacy}\index{prosecutor's fallacy}\label{prosecutor fallacy}". The underlying trap can be understand by considering a crime-scene DNA sample compared against a database of $20,000$ men. A match is found, an individual is then accused and at his trial, it is testified that the probability that two DNA profiles match by chance is only $1$ in $10,000$ (prosecutor's argument!). This does not mean the probability that the suspect is innocent is $1$ in $10,000$ (what jurors may wrongly understand...)! Since $20,000$ individuals were tested, there were $20,000$ opportunities to find a match by chance (to see the detailed calculation on how to get the probability of getting at least one match based on frequentist approach, see page \pageref{prosecutor fallacy frequentist example} and page \pageref{prosecutor fallacy bayesian example} for the bayesian approach).
	\end{tcolorbox}	
	
	Let $A$ be an event, $P$ be the probability measure. $A$ has zero probability if $P(A)=0$. $A$ is impossible if $A=\varnothing$.
	
	Impossibility implies zero probability, but the reverse is false! Consider the real line $\mathbb{R}$; if you randomly select a number $x$, the probability that $x=0$ is $0$, but this is not impossible. In fact, the probability that $x$ belongs to some countable set, e.g $\mathbb{Q}$, is also $0$.
	
	What we mean is that $P(A)=0$ does not imply $A=\varnothing$, i.e. knowing probability measure $=0$ does not help you figure out if a set is empty or not!

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Suppose the typewriter has $50$ keys, and the word to be typed is \textit{banana}. If the keys are pressed randomly and independently, it means that each key has an equal chance of being pressed. Then, the chance that the first letter typed is \textit{b} is $1/50$, and the chance that the second letter typed is \textit{a} is also $1/50$, and so on. Therefore, the chance of the first six letters spelling \textit{banana} is:
	
	less than one in $15$ billion, but not zero, hence a possible outcome.\\
	
	From the above, the chance of not typing banana in a given block of $6$ letters is $1-(1/50)^6$. Because each block is typed independently, the chance $X_n$ of not typing banana in any of the first $n$ blocks of $6$ letters is:
	
	As $n$ grows, $X_n$ gets smaller. For an $n$ of a million, $X_n$ is roughly $0.9999$, but for an $n$ of $10$ billion $X_n$ is roughly $0.53$ and for an $n$ of $100$ billion it is roughly $0.0017$. As $n$ approaches infinity, the probability $X_n$ approaches zero; that is, by making $n$ large enough, $X_n$ can be made as small as is desired, and the chance of typing banana approaches $100\%$.
	\end{tcolorbox}

	We will find an example of this kind of disjoint probability in the chapter of Industrial Engineering when studying F.M.E.A. (Failure Modes and Effects Analysis) for fault analysis systems with a complex structure.

	In other words in a more general form if $\left( A_{i} \right)_{i \in \mathbb{N}}$ is a sequence of pairwise disjoint events ($A_{i}$ and $A_{j}$ can not occur at the same time though $i \neq j$) then:	
	

	We then speak of "\NewTerm{$\sigma$-additivity}\index{$\sigma$-additivity}" because if we look more closely at the three axioms above the measure $P$ forms a $\sigma$-algebra (\SeeChapter{see section Measure Theory page \pageref{sigma algebra}}).

	At the opposite, if the events are not incompatibles (they can overlap or in other words: they have a joint probability), we then have for probability that at most one of the two takes place:
	
	This means that the probability that at most one of the events $A$ or $B$ occurs is equal to the sum of the probabilities for the realization of $A$ or $B$ occurred, minus the probability that $A$ and $B$ occurred simultaneously (we will show later that this is simply equal to the probability that the two do not occur at the same time!).


	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. Consider that in a given area, over $50$ years, the probability of a major earthquake is $5\%$ and on the same period the probability a major flood is $10\%$... We would like to know what is the probability that a nuclear plant meets at most one of two events during the same period if the are not incompatibles. We then calculate the probability from the above relation that gives $14.5\%$...\\
	
	E2. If we throw a handful of two dices, the probability of getting a double is:
	
	Why ? Because all the possible outcomes at launch give $6^2 = 36$ issues and the number of favourable cases of double $1$, double $2$, ... double $6$ is $6$ and so $P=\frac{6}{6^2}=\frac{6}{36}= \frac{1}{6}$.
	\end{tcolorbox}
	
	And thus if they were incompatibles we would have then $A \cap B = \varnothing $ we find again the disjoint probability:
	
	
	If we generalize to $n=3$ events we get:
	
	The general relation for $n$ events, named the "\NewTerm{inclusion-exclusion formula}\index{inclusion-exclusion formula}" is:
	
	In words, we take all possible intersections of one, two, ... $n$ sets and the signs of the sums alternate.
	\begin{dem}
	A point that is in $k$ sets is counted $k$ times by the first sum, $C^k_2$ by the second, $C^k_3$ by the third and so on until it is counted $1$ time by the $k$th term.The net result is:
	
	To show that this adds up to $1$, we recall the Binomial theorem (\SeeChapter{see section Algebraic Calculation page \pageref{binomial theorem}}):
	
	Setting $a=1$ and $b=-1$ we have:
	
	which proves the desired result.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	This lead us also naturally to "\NewTerm{Boole's inequality}\index{Boole's inequality}\label{Boole's inequality}",  that the probability of one or more of events occurring is no greater than the sum of the probabilities of the individual events:
	
	\end{tcolorbox}

	An immediate consequence of the axioms (A2) and (A3) is the relations between the probability of an event $A$ and its complement, noted $\bar{A}$ (or more rarely in accordance with the notation used in the chapter of Proof Theory the complementary may be noted $\neg A$):


Let $U$ be a universe with a finite number of $n$ possible outcomes:


where the events:

are named "\NewTerm{elementary events}\index{elementary events}". When these events have the same probability, we say they are "\NewTerm{equiprobables\index{equiprobables}}". In this case, it is very easy to calculate the probability. Indeed, these events being by definition incompatible with each other at this level of our discussion, we have under the third axiom (A3) of probabilities:


but since:


and that the probability of the right hand are by hypothesis equiprobable, we have:


	\textbf{Definition (\#\thesection.\mydef):} If $A$ and $B$ are not mutually exclusive but independent, we know that by their (in)compatibility $A \cap B=\varnothing$, that (very important in statistics!):
	
	the probability of the intersection ("and" operator) of two independent events is equal to the product of their probabilities (law of multiplication). We name it "\NewTerm{joint probability}\index{joint probability}\label{joint probability}" (this is the most common case).
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Consider that in a given area, over $50$ years, the probability of a major earthquake is $5\%$ and on the same period the probability a major flood is $10\%$... Assume that these two events are not mutually exclusive. In other words that they are compatible. We will be interested to their independence. Thus, we would like to know what is the probability that a nuclear power plants meets the two events at the same time, at any time, during this same period. We then calculate the probability from the above equation that gives $0.05\%$...
	\end{tcolorbox}
	
Under a more general form, the events $A_1,A_2,\ldots ,A_n$ are independent if the probability of the intersection is the product of the probabilities:


	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Be careful to not confuse "independent" and "incompatible"!
	\end{tcolorbox}

	So far to summarize a bit we have:

	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|p{7.5cm}|p{7.5cm}|}
			\hline
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Type}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Expression}}\\ \hline
			2 incompatibles events (disjoints) & $P(A \cup B)=P(A)+P(B)$ \\ \hline
			2 not incompatibles events (joints) & $P(A \cup B)=P(A)+P(B)-P(A \cap B)$ \\ \hline
			2 not incompatibles but independents events & $P(A \cap B)=P(A) \cdot P(B)$\\ \hline
		\end{tabular}
		\caption{Classical cases of probabilities}
	\end{table}	

	Thanks the above definition, we can show that the probability that either $A$ or $B$ is to take place (e.g. at least one of the two but not both at the same time), is simply equal to... the probability that the two do not does not occur at the same time:
	
	
	We can also use this definition to determine the probability that only one of two events occurs:
	
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Consider that in a given area, over $50$ years, the probability of a major earthquake is $5\%$ and on the same period the probability a major flood is $10\%$.... We would like to know what is the probability that a nuclear power plant exactly meets one of the both events during the same period, assuming they can not occur at the same time. We then calculate the probability from the above equation and that gives $14\%$...
	\end{tcolorbox}

There is a common (among others) and important area in the industry where the four following relations are frequently used:


	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Data scientists also use the some of the results above especially when working with proportions in surveys or classification problems (\SeeChapter{see section Numerical Methods page \pageref{lift association rule}}).
	\end{tcolorbox}
	
	This is the "\NewTerm{tree analysis error}\index{tree analysis error}" or "\NewTerm{probabilistic tree analysis}\index{probabilistic tree analysis}" which is used to analyze the possible reasons for failure of a system of any kind (industrial, administrative or other).

	To close this part of the chapter consider the following figure displaying Venn diagrams (\SeeChapter{see section Set Theory page \pageref{Venn diagrams}}) for all $16$ events (including the impossible event) that can be described in terms of two given events $A$ and $B$. In each case, the event is represented by the red area:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,771); %set diagram left start at 0, and has height of 771
		
		%Shape: Rectangle [id:dp16707597303121213] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (57,33) -- (187.3,33) -- (187.3,141) -- (57,141) -- cycle ;
		%Shape: Rectangle [id:dp08084570597141028] 
		\draw   (193.33,33) -- (323.63,33) -- (323.63,141) -- (193.33,141) -- cycle ;
		%Shape: Rectangle [id:dp011409549327390467] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (329.66,33) -- (459.96,33) -- (459.96,141) -- (329.66,141) -- cycle ;
		%Shape: Rectangle [id:dp6047070681087203] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (466,33) -- (596.3,33) -- (596.3,141) -- (466,141) -- cycle ;
		%Shape: Rectangle [id:dp003590863566973912] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (57,150) -- (187.3,150) -- (187.3,258) -- (57,258) -- cycle ;
		%Shape: Rectangle [id:dp12071819917419613] 
		\draw   (193.33,150) -- (323.63,150) -- (323.63,258) -- (193.33,258) -- cycle ;
		%Shape: Rectangle [id:dp9839689610114077] 
		\draw   (329.66,150) -- (459.96,150) -- (459.96,258) -- (329.66,258) -- cycle ;
		%Shape: Rectangle [id:dp24242888160668152] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (466,150) -- (596.3,150) -- (596.3,258) -- (466,258) -- cycle ;
		%Shape: Rectangle [id:dp32886798980184917] 
		\draw   (57,267) -- (187.3,267) -- (187.3,375) -- (57,375) -- cycle ;
		%Shape: Rectangle [id:dp45347557892019075] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (193.33,267) -- (323.63,267) -- (323.63,375) -- (193.33,375) -- cycle ;
		%Shape: Rectangle [id:dp058827899005341244] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (329.66,267) -- (459.96,267) -- (459.96,375) -- (329.66,375) -- cycle ;
		%Shape: Rectangle [id:dp8254082986757767] 
		\draw   (466,267) -- (596.3,267) -- (596.3,375) -- (466,375) -- cycle ;
		%Shape: Rectangle [id:dp10355076486316817] 
		\draw   (58,384) -- (188.3,384) -- (188.3,492) -- (58,492) -- cycle ;
		%Shape: Rectangle [id:dp8915060825581052] 
		\draw   (194.33,384) -- (324.63,384) -- (324.63,492) -- (194.33,492) -- cycle ;
		%Shape: Rectangle [id:dp9825363183555067] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (330.66,384) -- (460.96,384) -- (460.96,492) -- (330.66,492) -- cycle ;
		%Shape: Rectangle [id:dp7776092107802999] 
		\draw   (467,384) -- (597.3,384) -- (597.3,492) -- (467,492) -- cycle ;
		%Shape: Circle [id:dp3126329648291537] 
		\draw   (60.4,86.8) .. controls (60.4,67.58) and (75.98,52) .. (95.2,52) .. controls (114.42,52) and (130,67.58) .. (130,86.8) .. controls (130,106.02) and (114.42,121.6) .. (95.2,121.6) .. controls (75.98,121.6) and (60.4,106.02) .. (60.4,86.8) -- cycle ;
		%Shape: Circle [id:dp7789614236861069] 
		\draw   (112.4,86.8) .. controls (112.4,67.58) and (127.98,52) .. (147.2,52) .. controls (166.42,52) and (182,67.58) .. (182,86.8) .. controls (182,106.02) and (166.42,121.6) .. (147.2,121.6) .. controls (127.98,121.6) and (112.4,106.02) .. (112.4,86.8) -- cycle ;
		%Shape: Circle [id:dp12367603379324565] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (197.4,87) .. controls (197.4,67.78) and (212.98,52.2) .. (232.2,52.2) .. controls (251.42,52.2) and (267,67.78) .. (267,87) .. controls (267,106.22) and (251.42,121.8) .. (232.2,121.8) .. controls (212.98,121.8) and (197.4,106.22) .. (197.4,87) -- cycle ;
		%Shape: Circle [id:dp5249677148569305] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (249.4,87) .. controls (249.4,67.78) and (264.98,52.2) .. (284.2,52.2) .. controls (303.42,52.2) and (319,67.78) .. (319,87) .. controls (319,106.22) and (303.42,121.8) .. (284.2,121.8) .. controls (264.98,121.8) and (249.4,106.22) .. (249.4,87) -- cycle ;
		%Shape: Circle [id:dp8082556557113818] 
		\draw   (333.4,85.8) .. controls (333.4,66.58) and (348.98,51) .. (368.2,51) .. controls (387.42,51) and (403,66.58) .. (403,85.8) .. controls (403,105.02) and (387.42,120.6) .. (368.2,120.6) .. controls (348.98,120.6) and (333.4,105.02) .. (333.4,85.8) -- cycle ;
		%Shape: Circle [id:dp7861663713440694] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (385.4,85.8) .. controls (385.4,66.58) and (400.98,51) .. (420.2,51) .. controls (439.42,51) and (455,66.58) .. (455,85.8) .. controls (455,105.02) and (439.42,120.6) .. (420.2,120.6) .. controls (400.98,120.6) and (385.4,105.02) .. (385.4,85.8) -- cycle ;
		%Shape: Path Data [id:dp8627837726171699] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (403,85.8) .. controls (403,94.68) and (399.67,102.78) .. (394.2,108.93) .. controls (388.73,102.78) and (385.4,94.68) .. (385.4,85.8) .. controls (385.4,76.92) and (388.73,68.82) .. (394.2,62.67) .. controls (399.67,68.82) and (403,76.92) .. (403,85.8) -- cycle ;
		%Shape: Circle [id:dp15078222450482492] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (470.4,84.8) .. controls (470.4,65.58) and (485.98,50) .. (505.2,50) .. controls (524.42,50) and (540,65.58) .. (540,84.8) .. controls (540,104.02) and (524.42,119.6) .. (505.2,119.6) .. controls (485.98,119.6) and (470.4,104.02) .. (470.4,84.8) -- cycle ;
		%Shape: Circle [id:dp8210247966729192] 
		\draw   (522.4,84.8) .. controls (522.4,65.58) and (537.98,50) .. (557.2,50) .. controls (576.42,50) and (592,65.58) .. (592,84.8) .. controls (592,104.02) and (576.42,119.6) .. (557.2,119.6) .. controls (537.98,119.6) and (522.4,104.02) .. (522.4,84.8) -- cycle ;
		%Shape: Path Data [id:dp2851944853136723] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (540,84.8) .. controls (540,93.68) and (536.67,101.78) .. (531.2,107.93) .. controls (525.73,101.78) and (522.4,93.68) .. (522.4,84.8) .. controls (522.4,75.92) and (525.73,67.82) .. (531.2,61.67) .. controls (536.67,67.82) and (540,75.92) .. (540,84.8) -- cycle ;
		%Shape: Circle [id:dp062389219837829746] 
		\draw   (62.4,203.8) .. controls (62.4,184.58) and (77.98,169) .. (97.2,169) .. controls (116.42,169) and (132,184.58) .. (132,203.8) .. controls (132,223.02) and (116.42,238.6) .. (97.2,238.6) .. controls (77.98,238.6) and (62.4,223.02) .. (62.4,203.8) -- cycle ;
		%Shape: Circle [id:dp9367298957106263] 
		\draw   (114.4,203.8) .. controls (114.4,184.58) and (129.98,169) .. (149.2,169) .. controls (168.42,169) and (184,184.58) .. (184,203.8) .. controls (184,223.02) and (168.42,238.6) .. (149.2,238.6) .. controls (129.98,238.6) and (114.4,223.02) .. (114.4,203.8) -- cycle ;
		%Shape: Path Data [id:dp4945497059662054] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (132,203.8) .. controls (132,212.68) and (128.67,220.78) .. (123.2,226.93) .. controls (117.73,220.78) and (114.4,212.68) .. (114.4,203.8) .. controls (114.4,194.92) and (117.73,186.82) .. (123.2,180.67) .. controls (128.67,186.82) and (132,194.92) .. (132,203.8) -- cycle ;
		%Shape: Circle [id:dp8805780250369639] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (197.4,203.8) .. controls (197.4,184.58) and (212.98,169) .. (232.2,169) .. controls (251.42,169) and (267,184.58) .. (267,203.8) .. controls (267,223.02) and (251.42,238.6) .. (232.2,238.6) .. controls (212.98,238.6) and (197.4,223.02) .. (197.4,203.8) -- cycle ;
		%Shape: Circle [id:dp5616041722584406] 
		\draw   (249.4,203.8) .. controls (249.4,184.58) and (264.98,169) .. (284.2,169) .. controls (303.42,169) and (319,184.58) .. (319,203.8) .. controls (319,223.02) and (303.42,238.6) .. (284.2,238.6) .. controls (264.98,238.6) and (249.4,223.02) .. (249.4,203.8) -- cycle ;
		%Shape: Circle [id:dp5869793882668504] 
		\draw   (334.4,203.8) .. controls (334.4,184.58) and (349.98,169) .. (369.2,169) .. controls (388.42,169) and (404,184.58) .. (404,203.8) .. controls (404,223.02) and (388.42,238.6) .. (369.2,238.6) .. controls (349.98,238.6) and (334.4,223.02) .. (334.4,203.8) -- cycle ;
		%Shape: Circle [id:dp5602234722002819] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (386.4,203.8) .. controls (386.4,184.58) and (401.98,169) .. (421.2,169) .. controls (440.42,169) and (456,184.58) .. (456,203.8) .. controls (456,223.02) and (440.42,238.6) .. (421.2,238.6) .. controls (401.98,238.6) and (386.4,223.02) .. (386.4,203.8) -- cycle ;
		%Shape: Circle [id:dp20970627325188773] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (470.4,203.8) .. controls (470.4,184.58) and (485.98,169) .. (505.2,169) .. controls (524.42,169) and (540,184.58) .. (540,203.8) .. controls (540,223.02) and (524.42,238.6) .. (505.2,238.6) .. controls (485.98,238.6) and (470.4,223.02) .. (470.4,203.8) -- cycle ;
		%Shape: Circle [id:dp7609920734190174] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (522.4,203.8) .. controls (522.4,184.58) and (537.98,169) .. (557.2,169) .. controls (576.42,169) and (592,184.58) .. (592,203.8) .. controls (592,223.02) and (576.42,238.6) .. (557.2,238.6) .. controls (537.98,238.6) and (522.4,223.02) .. (522.4,203.8) -- cycle ;
		%Shape: Path Data [id:dp6158555051513324] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (540,203.8) .. controls (540,212.68) and (536.67,220.78) .. (531.2,226.93) .. controls (525.73,220.78) and (522.4,212.68) .. (522.4,203.8) .. controls (522.4,194.92) and (525.73,186.82) .. (531.2,180.67) .. controls (536.67,186.82) and (540,194.92) .. (540,203.8) -- cycle ;
		%Shape: Circle [id:dp08315146715689292] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (61.4,320.8) .. controls (61.4,301.58) and (76.98,286) .. (96.2,286) .. controls (115.42,286) and (131,301.58) .. (131,320.8) .. controls (131,340.02) and (115.42,355.6) .. (96.2,355.6) .. controls (76.98,355.6) and (61.4,340.02) .. (61.4,320.8) -- cycle ;
		%Shape: Circle [id:dp14561957199498665] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (113.4,320.8) .. controls (113.4,301.58) and (128.98,286) .. (148.2,286) .. controls (167.42,286) and (183,301.58) .. (183,320.8) .. controls (183,340.02) and (167.42,355.6) .. (148.2,355.6) .. controls (128.98,355.6) and (113.4,340.02) .. (113.4,320.8) -- cycle ;
		%Shape: Path Data [id:dp7558089426939942] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (131,320.8) .. controls (131,329.68) and (127.67,337.78) .. (122.2,343.93) .. controls (116.73,337.78) and (113.4,329.68) .. (113.4,320.8) .. controls (113.4,311.92) and (116.73,303.82) .. (122.2,297.67) .. controls (127.67,303.82) and (131,311.92) .. (131,320.8) -- cycle ;
		%Shape: Circle [id:dp9126268923984504] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (248.4,321) .. controls (248.4,301.78) and (263.98,286.2) .. (283.2,286.2) .. controls (302.42,286.2) and (318,301.78) .. (318,321) .. controls (318,340.22) and (302.42,355.8) .. (283.2,355.8) .. controls (263.98,355.8) and (248.4,340.22) .. (248.4,321) -- cycle ;
		%Shape: Circle [id:dp8982839953457324] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (334.4,320.8) .. controls (334.4,301.58) and (349.98,286) .. (369.2,286) .. controls (388.42,286) and (404,301.58) .. (404,320.8) .. controls (404,340.02) and (388.42,355.6) .. (369.2,355.6) .. controls (349.98,355.6) and (334.4,340.02) .. (334.4,320.8) -- cycle ;
		%Shape: Circle [id:dp2715162382308445] 
		\draw   (386.4,320.8) .. controls (386.4,301.58) and (401.98,286) .. (421.2,286) .. controls (440.42,286) and (456,301.58) .. (456,320.8) .. controls (456,340.02) and (440.42,355.6) .. (421.2,355.6) .. controls (401.98,355.6) and (386.4,340.02) .. (386.4,320.8) -- cycle ;
		%Shape: Path Data [id:dp0722046727527701] 
		\draw   (404,320.8) .. controls (404,329.68) and (400.67,337.78) .. (395.2,343.93) .. controls (389.73,337.78) and (386.4,329.68) .. (386.4,320.8) .. controls (386.4,311.92) and (389.73,303.82) .. (395.2,297.67) .. controls (400.67,303.82) and (404,311.92) .. (404,320.8) -- cycle ;
		%Shape: Circle [id:dp7490176047798185] 
		\draw   (471.4,320.8) .. controls (471.4,301.58) and (486.98,286) .. (506.2,286) .. controls (525.42,286) and (541,301.58) .. (541,320.8) .. controls (541,340.02) and (525.42,355.6) .. (506.2,355.6) .. controls (486.98,355.6) and (471.4,340.02) .. (471.4,320.8) -- cycle ;
		%Shape: Circle [id:dp5535571004311564] 
		\draw   (523.4,320.8) .. controls (523.4,301.58) and (538.98,286) .. (558.2,286) .. controls (577.42,286) and (593,301.58) .. (593,320.8) .. controls (593,340.02) and (577.42,355.6) .. (558.2,355.6) .. controls (538.98,355.6) and (523.4,340.02) .. (523.4,320.8) -- cycle ;
		%Shape: Path Data [id:dp6899945152273903] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (541,320.8) .. controls (541,329.68) and (537.67,337.78) .. (532.2,343.93) .. controls (526.73,337.78) and (523.4,329.68) .. (523.4,320.8) .. controls (523.4,311.92) and (526.73,303.82) .. (532.2,297.67) .. controls (537.67,303.82) and (541,311.92) .. (541,320.8) -- cycle ;
		%Shape: Circle [id:dp5675431489409879] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (63.4,437.8) .. controls (63.4,418.58) and (78.98,403) .. (98.2,403) .. controls (117.42,403) and (133,418.58) .. (133,437.8) .. controls (133,457.02) and (117.42,472.6) .. (98.2,472.6) .. controls (78.98,472.6) and (63.4,457.02) .. (63.4,437.8) -- cycle ;
		%Shape: Circle [id:dp19834774875336825] 
		\draw   (115.4,437.8) .. controls (115.4,418.58) and (130.98,403) .. (150.2,403) .. controls (169.42,403) and (185,418.58) .. (185,437.8) .. controls (185,457.02) and (169.42,472.6) .. (150.2,472.6) .. controls (130.98,472.6) and (115.4,457.02) .. (115.4,437.8) -- cycle ;
		%Shape: Path Data [id:dp5303147008208915] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (133,437.8) .. controls (133,446.68) and (129.67,454.78) .. (124.2,460.93) .. controls (118.73,454.78) and (115.4,446.68) .. (115.4,437.8) .. controls (115.4,428.92) and (118.73,420.82) .. (124.2,414.67) .. controls (129.67,420.82) and (133,428.92) .. (133,437.8) -- cycle ;
		%Shape: Circle [id:dp0031470716047183878] 
		\draw   (199.4,437.8) .. controls (199.4,418.58) and (214.98,403) .. (234.2,403) .. controls (253.42,403) and (269,418.58) .. (269,437.8) .. controls (269,457.02) and (253.42,472.6) .. (234.2,472.6) .. controls (214.98,472.6) and (199.4,457.02) .. (199.4,437.8) -- cycle ;
		%Shape: Circle [id:dp8917830713420924] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (251.4,437.8) .. controls (251.4,418.58) and (266.98,403) .. (286.2,403) .. controls (305.42,403) and (321,418.58) .. (321,437.8) .. controls (321,457.02) and (305.42,472.6) .. (286.2,472.6) .. controls (266.98,472.6) and (251.4,457.02) .. (251.4,437.8) -- cycle ;
		%Shape: Path Data [id:dp059213509632552785] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (269,437.8) .. controls (269,446.68) and (265.67,454.78) .. (260.2,460.93) .. controls (254.73,454.78) and (251.4,446.68) .. (251.4,437.8) .. controls (251.4,428.92) and (254.73,420.82) .. (260.2,414.67) .. controls (265.67,420.82) and (269,428.92) .. (269,437.8) -- cycle ;
		%Shape: Circle [id:dp5373440573471773] 
		\draw   (472.4,437.8) .. controls (472.4,418.58) and (487.98,403) .. (507.2,403) .. controls (526.42,403) and (542,418.58) .. (542,437.8) .. controls (542,457.02) and (526.42,472.6) .. (507.2,472.6) .. controls (487.98,472.6) and (472.4,457.02) .. (472.4,437.8) -- cycle ;
		%Shape: Circle [id:dp08634317674899727] 
		\draw   (524.4,437.8) .. controls (524.4,418.58) and (539.98,403) .. (559.2,403) .. controls (578.42,403) and (594,418.58) .. (594,437.8) .. controls (594,457.02) and (578.42,472.6) .. (559.2,472.6) .. controls (539.98,472.6) and (524.4,457.02) .. (524.4,437.8) -- cycle ;
		%Shape: Path Data [id:dp9258242187050338] 
		\draw   (542,437.8) .. controls (542,446.68) and (538.67,454.78) .. (533.2,460.93) .. controls (527.73,454.78) and (524.4,446.68) .. (524.4,437.8) .. controls (524.4,428.92) and (527.73,420.82) .. (533.2,414.67) .. controls (538.67,420.82) and (542,428.92) .. (542,437.8) -- cycle ;
		%Shape: Circle [id:dp9532553387614995] 
		\draw   (196.4,321) .. controls (196.4,301.78) and (211.98,286.2) .. (231.2,286.2) .. controls (250.42,286.2) and (266,301.78) .. (266,321) .. controls (266,340.22) and (250.42,355.8) .. (231.2,355.8) .. controls (211.98,355.8) and (196.4,340.22) .. (196.4,321) -- cycle ;
		%Shape: Circle [id:dp045136037611473556] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (334.4,438) .. controls (334.4,418.78) and (349.98,403.2) .. (369.2,403.2) .. controls (388.42,403.2) and (404,418.78) .. (404,438) .. controls (404,457.22) and (388.42,472.8) .. (369.2,472.8) .. controls (349.98,472.8) and (334.4,457.22) .. (334.4,438) -- cycle ;
		%Shape: Circle [id:dp9925450237606521] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (386.4,438) .. controls (386.4,418.78) and (401.98,403.2) .. (421.2,403.2) .. controls (440.42,403.2) and (456,418.78) .. (456,438) .. controls (456,457.22) and (440.42,472.8) .. (421.2,472.8) .. controls (401.98,472.8) and (386.4,457.22) .. (386.4,438) -- cycle ;
		%Shape: Path Data [id:dp6106790611976933] 
		\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (404,438) .. controls (404,446.88) and (400.67,454.98) .. (395.2,461.13) .. controls (389.73,454.98) and (386.4,446.88) .. (386.4,438) .. controls (386.4,429.12) and (389.73,421.02) .. (395.2,414.87) .. controls (400.67,421.02) and (404,429.12) .. (404,438) -- cycle ;
		%Shape: Path Data [id:dp9858493793696685] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (267,87) .. controls (267,95.88) and (263.67,103.98) .. (258.2,110.13) .. controls (252.73,103.98) and (249.4,95.88) .. (249.4,87) .. controls (249.4,78.12) and (252.73,70.02) .. (258.2,63.87) .. controls (263.67,70.02) and (267,78.12) .. (267,87) -- cycle ;
		%Shape: Path Data [id:dp2260232217825766] 
		\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (404,204) .. controls (404,212.88) and (400.67,220.98) .. (395.2,227.13) .. controls (389.73,220.98) and (386.4,212.88) .. (386.4,204) .. controls (386.4,195.12) and (389.73,187.02) .. (395.2,180.87) .. controls (400.67,187.02) and (404,195.12) .. (404,204) -- cycle ;
		
		% Text Node
		\draw (174,122.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (311,122.4) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (447,122.4) node [anchor=north west][inner sep=0.75pt]    {$3$};
		% Text Node
		\draw (583,122.4) node [anchor=north west][inner sep=0.75pt]    {$4$};
		% Text Node
		\draw (174,240.4) node [anchor=north west][inner sep=0.75pt]    {$5$};
		% Text Node
		\draw (311,240.4) node [anchor=north west][inner sep=0.75pt]    {$6$};
		% Text Node
		\draw (447,240.4) node [anchor=north west][inner sep=0.75pt]    {$7$};
		% Text Node
		\draw (583,240.4) node [anchor=north west][inner sep=0.75pt]    {$8$};
		% Text Node
		\draw (174,356.4) node [anchor=north west][inner sep=0.75pt]    {$9$};
		% Text Node
		\draw (303,356.4) node [anchor=north west][inner sep=0.75pt]    {$10$};
		% Text Node
		\draw (440,357.4) node [anchor=north west][inner sep=0.75pt]    {$11$};
		% Text Node
		\draw (576,357.4) node [anchor=north west][inner sep=0.75pt]    {$12$};
		% Text Node
		\draw (168,473.4) node [anchor=north west][inner sep=0.75pt]    {$13$};
		% Text Node
		\draw (305,473.4) node [anchor=north west][inner sep=0.75pt]    {$14$};
		% Text Node
		\draw (441,473.4) node [anchor=north west][inner sep=0.75pt]    {$15$};
		% Text Node
		\draw (577,473.4) node [anchor=north west][inner sep=0.75pt]    {$16$};
		% Text Node
		\draw (72,79.4) node [anchor=north west][inner sep=0.75pt]    {$A$};
		% Text Node
		\draw (159,79.4) node [anchor=north west][inner sep=0.75pt]    {$B$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Possible Venn diagrams for two events}
	\end{figure}
	Consider the situation where $A$ represents and earthquake and $B$ represents a major flood and $U$ the universe of all dramatics events for a nuclear power plant. We consider that the two events are independents. Then each of the 16 events can be described as follows, either mathematically or verbally:
	\begin{enumerate}
		\item An earthquake can occur or a flood or nothing or the both together or any other event (to summarize: any event can occur).
		
		\item $A \cup B$: Any event with an earthquake a flood or the both event together can occur.
		
		\item $A \cup B^c$: Any event with earthquake can occur with or without a flood excepted events with a flood not associated to an earthquake.
		
		\item $A^c \cup B$: Any event with earthquake can occur with or without a flood excepted events with a flood not associated to an earthquake.
		
		\item $A^c \cup B^c$: Any event can occur excepted those associated with an earthquake together with a flood.
		
		\item $A$: Any event with an earthquake can occur (this include the events associating an earthquake and a flood).
		
		\item $B$: Any event with a flood can occur (this include the events associating a flood and an earthquake).
		
		\item $(A \cap B) \cup (A^c \cap B^c)$: Any event can occur excepted those including an earthquake without a flood or those including a flood without an earthquake.
		
		\item $(A \cap B^c) \cup (A^c \cup B)$: Any event including an earthquake without a flood or a flood without an earthquake can occur.
		
		\item $B^c$: Any event excepted those including a flood can occur.
		
		\item $A^c$: Any event excepted those including an earthquake can occur.
				
		\item $A \cap B$: Any events associating an earthquake and a flood together can occur.
		
		\item $A \cap B^c$: Any event with an earthquake and without a flood can occur.
		
		\item $A^c \cap B$: Any event with a flood and without an earthquake can occur.
		
		\item $A^c \cap B^c$: Any event can occur excepted those including an earthquake and/or a flood.
		
		\item $A \cap A^c$ or $B \cap B^c$: Impossible Event.
				
	\end{enumerate}

	\subsection{Conditional Probabilities}\label{bayesian inference}
	
	What can we infer about the probability of an event $B$ knowing that an event $A$ has occurred, aware that there is a link between $A$ and $B$? In other words, if there is a link between $A$ and $B$, the completion of $A$ has to change our understanding of $B$ and we want to know if it is possible to define the conditional probability of an event (relatively) to another event.
	
	This type of probability is named "\NewTerm{conditional probability}\index{conditional probability}" or "\NewTerm{a posterior probability}\index{a posteriori probability}" of $B$ knowing $A$, and is denoted in the context of the study of conditional probabilities:
	
	and often in practice to avoid confusion with a possible division:
	
	and we sometimes find in U.S. books the notation:
	
	or also:
	
	We also have the case:
	
	which is named "\NewTerm{likelihood function of $A$}\index{likelihood function}" or "\NewTerm{a priori probability of $A$ given $B$}\index{a priori probability}".

	Historically, the first mathematician to have used the correct notion of conditional probability was Thomas Bayes (1702-1761). This is why we often say "\NewTerm{Bayes}\index{Bayes' probabilities}" or "\NewTerm{Bayesian}\index{Bayesian probabilities}" probabilities as soon as conditional probabilities are involved: "\NewTerm{Bayes formula}\index{Bayes' formula}", "\NewTerm{Bayesian statistics}\index{Bayesian statistics}", etc.
	
	The notion of conditional probability that we will introduce is much less simple than it first appears and the conditionals problems are an inexhaustible source of errors of any kind (there are famous paradoxes on the subject and even expert requires peer review to minimize mistakes).
	
	Let's start with a simple example: Suppose we have two dice. Now imagine that we only launched the first die. We want to know what is the probability that by throwing the second dice, the sum of the two numbers is equal to a given minimum value. Thus, the probability of obtaining the minimum value given the value of the first die is totally different from the probability of obtaining the same minimum value in throwing two dice at the same time. How to calculate this new probability?
	
	Let us now formalize the process! After the launch of the first dice, we have:
	
	Under the hypothesis that $B \subset A$ , we feel that $P(B/A)$ must be proportional to $P(B)$, the proportionality constant being determined by the normalization:
	
Now let $B \subset A^c$ ($B$ is included in the complement of $A$ so that the events are mutually exclusive). It is then relatively intuitive .... that under the previous hypothesis of incompatibility we have the conditional probability:
	
This leads us to the following definitions of respectively a posteriori and a priori probabilities:
	
Thus, the fact to know that $A$ has occurred reduces all possible outcomes of the universe $U$ of $B$. From there, only the events of type $A \cap B$ are important. The probability of $A$ given $B$ or vice versa (by symmetry) must be proportional to $P(A \cap B)$!

The coefficient of proportionality is the denominator and it ensures the certain event. Indeed, if two events $A$ and $B$ are independent (think the black cat and the scale for example), then we have:
	
and then we see that $P(B/A)$ is equal to $P(B)$ and therefore the event $A$ adds no information to $B$ and vice versa! So in other words, if $A$ and $B$ are independent, we have:
	
Another fairly intuitive way to see things is to represent the probability measure $P$ as a measure of subsets areas (surface) of $\mathbb{R}^2$.

Indeed, if $A$ and $B$ are two subsets of respective areas $P(A)$ and $P(B)$ then the question of what is the probability that a point in the plane belongs to $B$ knowing that it belongs to $A$ it is quite obvious that the probability is given by answer:
	
We would like to indicate that the definition of conditional probability is often used in the following way:
	
named "\NewTerm{formula of compound probabilities}\index{formula of compound probabilities}\label{compound probabilities}" or "\NewTerm{product rule}\index{product rule}". It is written in many different ways (just a matter of notation convention):
	
	Thus, the a posteriori probability of $B$ knowing $A$ can also be written as:
	
	The way that this formula gives an update of the probability hypothesis, $B$, in light of some body of data, $A$, is named the "\NewTerm{diachronic interpretation}\index{diachronic interpretation}". "Diachronic" meaning that something is happening over time, in this case the probability of the hypothesis changes, over time, as we see new data.

	In this interpretation the different terms have a name:
	\begin{itemize}
		\item $P(B)$ is the probability of the hypothesis before we see the data, name as we already know, the "\NewTerm{prior probability}\index{prior probability}\footnote{Where does the prior come from is the million dollar question... In principle the bayesian is supposed to choose a prior that represents their prior information. This will be challenging in high dimensional cases to say the least. Also, critics will say that someone's prior opinions should not be included in a data analysis because this is not scientific.}", or just the "\NewTerm{prior}\index{prior}".

		\item $P(B/A)$ is what we want to compute, the probability of the hypothesis after we see the data, named as we already know, the "posterior".

		\item $P(A/B)$ is the probability of the data under the hypothesis, named the "\NewTerm{likelihood}\index{likelihood}".

		\item $P(A)$ is the probability of the data under the hypothesis, named the "\NewTerm{normalizing constant}" or "\NewTerm{expectedness}".
	\end{itemize}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. Let us consider a disease like meningitis. The probability of having the meningitis will be denoted by $P(M)=0.001$ (arbitrary value for the example) and a sign of this disease like headache will be noted $P(S)=0.1$. We assume known that the a posteriori probability of having a headache if we have meningitis is:
	
	The Bayes' theorem then gives the a priori probability of having meningitis if we have a headache!:
	
	E2. In a quiz-show (this example is named the Monty Hall paradox), the candidate has to select one of three doors. Behind two doors there is a goat (an undesirable prize). Behind the one door there is a car. At the beginning the candidate tries to guess the door with the car. Afterwards the moderator Monty Hall opens one of the doors with a goat which is not the candidate's door. So two doors are left where one of them is the one, the candidate has chosen. Monty asks the candidate whether he would like to change his mind; he is free to select the other door.\\

	In general, is it better to change the mind, stick to the first selection or doesn't it matter?\\
	
	Assumption (one of three cases): The candidate selects door \#1.
	\begin{center}
	  \begin{tabular}{cc}
	    $D_1$ & Car behind door \#1 \\
	    $D_2$ & Car behind door \#2 \\
	    $D_3$ & Car behind door \#3
	  \end{tabular}
	\end{center}
	We define the corresponding probabilities:
	
	We somehow have to model the behaviour of the moderator. Therefore we introduce a hypothesis $H$ that he opens door \#3. The probability that he selects door \#3 in favour of door \#2 is $\frac{1}{2}$, because \#1 is already taken by the candidate:
		
	Furthermore:
	
	So the probability that he opens door \#3 under the condition that door \#1 contains the car is $\frac{1}{2}$. The probability is $1$ if the car is in door \#2. And if the car is in door \#3, the moderator will never show door \#3.\\
	
	The above scenario can be build easily with a software like @Risk of Palisade (the PrecisionTree module):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/arithmetics/montey_hall_non_bayesian_decision_tree.jpg}
	\end{figure}
	In the above figure we assume that the contester has already chooses door \#1. If the car is behind door \#1 Monty well flip a coin to open door \#2 or \#3 (hence the $50\%$ and $50\%$). However if the car is behind door \#2 Monty will surely open door \#3 (hence the $100\%$). And finally if the car is behind door \#3 Monty will surely open door \#2 (hence the second $100\%$).\\
		
	We can spot some asymmetry here. This point is evidently responsible why
	an intuition like \textit{it does not matter} might be wrong.\\
	
	So we will use the Bayes' Theorem:
	
	The important question is: Under the assumption that we have selected door \#1 (our first assumption) and Monty opens door \#3 (our hypothesis or expectedness), what is the probability that the car is behind door \#1 compared to door \#2? 
	
	So apparently, the probability that door \#2 contains the car is higher than for door \#1. Or in other words: in this case the candidate should prefer to change his mind.\\
	
	Now we have to evaluate this for other hypotheses (moderator opens door \#2) and other candidate selections (door \#2 and door \#3). The approach is the same and not carried out here. But the conclusion keeps the same in all cases: Being open to change your mind will make you probabilistically win the car.\\
	
	So here Bayes rule is necessary because the contester needs to flip the tree above. To get the nodes in the opposite order. The reason is that the contester will first see the door that Monty opens. At that point the contester need to know the posterior probability. In @Risk Palisade has a Bayes revision option that flips the tree and gives:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/arithmetics/montey_hall_non_bayesian_decision_tree_bayes_revision.jpg}
	\end{figure}
	Now Monty action is on the left and the location is on the right. Can you see that the contester should switch no matter what Monty does?  Monty won't certainly open door 1 because the contester has already chosen it. You can see on the left that the chance are 50/50 that Monty opens door \#2 or door \#3. If he opens door \#2 the posterior probability that the car is behind door \#3 is $2/3$ that is twice the probability which is twice the posterior probability that it is behind door \#1. In the same way if he opens door \#3,  the posterior probability that the car is behind door \#2 is $2/3$ that is twice the probability which is twice the posterior probability that it is behind door \#1.\\
	
	Some people think that this analysis is all smoke and mirrors. In fact some people continue to believe that Bayes rule is quite controversial. However everything here is based on mathematical ground and there is nothing controversial about it. The key about understanding the apparent paradox, that the contester can do better by switching, is that Monty has supplied some useful information by showing a door where the car is not located. This information together with Bayes rule allow the contester to improve his chances of winning the car by switching doors.
	\end{tcolorbox}
	
	We also notice that:
	
	So we can know the probability of the event $A$ knowing the elementary probabilities $P(B_i)$ of its causes and the conditional probabilities of $A$ for each $B_i$:
	
	which is named the "\NewTerm{formula of total probabilities}\index{formula of total probabilities}" or "\NewTerm{total probabilities theorem}\index{total probabilities theorem}" (relation that is also very useful when we don't know the value of $P(A)$ or can't estimate it directly easily). But also, for any $j$, we have the following corollary using the previous results that gives us following an event $A$, the probability that it is the cause $B_i$ that produced it:
	
	which is the general form of the "\NewTerm{Bayes formula}\index{Bayes formula}\label{bayes formula}" or "\NewTerm{Bayes' theorem}\index{Bayes' theorem}" that we will use a little in the Statistical Mechanics chapter and through the study of the queueing theory (\SeeChapter{see section Quantitative Management page \pageref{queueing theory}}). You should know that the implications of this theorem are, however, considerable in daily life, in medicine, in industry and in the field of Data Mining.

We often find in the literature many examples of applications of the previous relation with only two possible outcomes $B$ with respect to the event $A$. Therefore we find the Bayes formula written in the following form for each issue:
	
and note that in this particular case (binary outcomes):
	
is an intuitive result.

For binary events, we also have (returning to the theorem of total probabilities seen above):
	
	Or in the case of odds ratio:
	

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. A disease affects $10$ people on $10,000$ ($0.1\% = 0.001$). A test has been developed which has a $5\%$ false positives (people not having the disease but for which the test says they are affected) but still always detects the disease if a person has it. What is the probability that a random person for which the test gives a positive result really has this disease?\\\\There is therefore $10,000$ people, $500$ of which are false positives, and we know a posteriori that $10$ people have really the disease. Then the probability that somebody who has a positive test result is really sick is:
	
	This is often a shocking and counter-intuitive result. It also highlights why diagnostic tests must be extremely reliable!\\

	E2. Two machines $M_1$ and $M_2$ produce respectively 100 and 200 pieces. $M_1$ produces $5\%$ defective pieces and $M_2$ produces $6\%$ (posterior probabilities). What is the a priori probability that a defective piece was manufactured by the machine $M_1$?\\

	We then have:
	
	E3. From a batch of $10$ pieces with $30\%$ defective, we take a sample of size $3$ without replacement. What is the probability that the second piece is correct (whatever the first is)?\\

	We have:
	
	E4. Consider that corporations assess employees knowledge with exams or assessments in the form of multiple choice questions (MCQs). If an employee responds to a question there are two issues: either he knows the answer or he try to guess it. Let $p$ be the probability that the employee know the answer and therefore $1-p$ that he guess it. We admit that the employee who guesses will correctly answer with a probability of $1/m$ where $m$ is the number of proposed answers. What is the a priori probability that an employee (really) knows the answer to a question with $5$ choices if he answered correctly?\\

	Let $B$ and $A$ be respectively the events "the employee knows the answer" and "the employee correctly answers the question". Then the a priori probability that an employee knows (really) the answer to a question that he answered correctly is:
	
	E5. Suppose a doctor thinks the odds are $2/1$ that a patient has a certain disease. She orders two independents tests. Let $H$ be the event the patient has the disease and $E_1$ and $E_2$ be the event the tests are positive. Suppose the first test has probability $0.1$ of a false positive and probability $0.05$ of a false negative. The second test has probabilities $0.05$ and $0.08$ of a false positive and false negative, respectively. If both tests are positive, what is the posterior probability the patient has the disease?\\
	
	Assuming that $E_1$ and $E_2$ are conditionally independent of $H$ and also of $\bar{H}$, we work first in terms of odds, then convert to probability:
	
	The data are:
	
	Substituting value, we get:
	
	Similarly we get:
	
	E6.\label{prosecutor fallacy bayesian example} Let us come back again on prosecutor's fallacy with the DNA case (see earlier above page \pageref{prosecutor fallacy}). Let us consider that DNA profiling of a suspect gives rise to a rare trait, $X$, such that:
	
	which is equivalent to:
	
	This is of no interest! We want ($G$ is for "potential guilty"):
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Caution! Be careful when reading books containing probabilities calculations especially when the authors are theists and even more when the probabilities involve Bayes' theorem. The believing physicist Stephen Unwin, as presented in his 12003 (holocene calendar) book, calculates the probability of his christian god to be $67\%$. He begins his calculation by assuming the prior probability of christian god's existence, before any evidence is considered, to be $50\%$... How does he know that when we know that there are thousands of very different gods existing on Earth? If we compare Unwin's calculation with the unpublished one of the non-believing physicist Larry Ford. Ford used all the same equations as Unwin, differing only in the numbers inserted in the calculation. He argued the prior probability of Unwin's christian god must be compared with other unobserved entities such as Big Foot or the Loch Ness Monster, which he took to be one in a million ($10^{-6}$). With his set of numbers, Ford's conclusion was that the probability christian god exists is one in a hundred thousand trillion ($10^{-17}$).
	\end{tcolorbox}
	
	Bayesian analysis provides also a powerful tool to formalize reasoning under uncertainty and the examples we have shown above illustrate how this tool can be difficult to use.
	
	Finally, for a future use in the section of Theoretical Computing, for the Data Mining technique named "binomial naive Bayes classification" we will need a last result!
	
	Let us recall the product rule seen just earlier above:
	
	We can extend this for three variables. Indeed, let us consider:
	
	and let us put $D=B\cap C$ then:
	
	But as the product rule gives us also:
	
	Then:
	
	also sometimes denoted:
	
	and in general for $n$ variables:
	
	In general we refer to this as the "\NewTerm{chain rule}\index{chain rule}\label{chain rule}" or to avoid any confusion with the same term in the field of Differential and Integral Calculus we refer to this as the "\NewTerm{Bayes' chain rule}\index{Bayes' chain rule}\label{Bayes chain rule}".

	This formula is especially significant for Bayesian Belief Nets. It provides a means of calculating the full joint probability distribution ; in BBNs many of the variables $A_i$ will be conditionally independent.
	
	Now the reader must keep something in mind: Any probability result that is true for unconditional probability remains true if everything is conditioned on some event!
	
	We know that by definition:
	
	and so if we condition everything on $C$ having occurred, we get that:
	
	Hence:
	
	also sometimes (badly) denoted:
	
	or\label{bayes relation for complete data likelihood}:
	
	Intuitively to get this result it can help to draw the Venn diagram (almost as always):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}
	    \begin{scope}[blend group = soft light]
	    \fill[red!30!white]   ( 90:1.2) circle (2);
	    \fill[green!30!white] (210:1.2) circle (2);
	    \fill[blue!30!white]  (330:1.2) circle (2);
	    \end{scope}
	    \node at (90:2)    {$B$};
	    \node at (210:2)   {$A$};
	    \node at (330:2)   {$C$};
	    \node  {$1$};
	    % Text Node
		\draw (1,1) node [anchor=north west][inner sep=0.75pt]  {$2$};
		\end{tikzpicture}
	\end{figure}
	We then have:
	
	and then just do the multiplications to see that everything is fine.

	\subsubsection{Conditional Expectation and Variance}\label{conditional expectation}
	Now, we will see the continuous version of the conditional probability by introducing the subject directly with a particular example (the general theory being indigestible) infinitely important in the field of social statistics and quantitative finance. However, this choice (the study of a particular case) implies that the reader has read the first chapter of Statistics to study the functions of continuous distributions and especially that of the Pareto law.
	
	So here's the scenario: Often, in social sciences or economics, we find in the literature dealing with the laws of Pareto statements like the following (but almost never with a detailed proof): whatever your income, the average income of those who have an income above yours is in a constant ratio, greater than $1$, to your income if it follows a Pareto random variable. Then we say that the law is isomorphic to any truncated part itself.
	
	Let's see what it is exactly!
	
	Let $X$ be a random variable equal to the income and following a Pareto with the density (\SeeChapter{see section Statistics page \pageref{pareto distribution}}):
	
	with $k>1,x_m>0,x \geq x_m$ and that has for distribution function (see also the Statistics chapter for the detailed proof):
	
	The sentence begins with «\textit{whatever your income ...}», then select any income $x_0 \geq x_m$.
	
	Now we need to compute «\textit{the average income of those with income higher than} $x_0$». It is therefore asked to calculate the expected (average income) of a new random variable $Y$ that is equal to $X$, but restricted to the population of people with an income above $x_0$:
	
	The distribution function of $Y$ is given by:
	
	This expression is of course equal to zero if $x \leq x_0$.
	Well, so far we have only do vocabulary. First recall the following conditional probability relation already seen before:
	
	for $x \geq x_0 $ we have for the conditional law:
	
	Before going further, you should be aware that the numerator and denominator are independent but that the whole must be considered, however, as the realization of a single random variable which we denote $Y$. Furthermore, only the numerator is a dependent variable. The denominator can it be considered as a normalization constant.

	So we see that the density of $Y$ is given by the function:
	\begin{gather*}
	f_Y(y) = \begin{cases}
	\qquad 0 & y<x_0 \\
	\dfrac{f(y)}{P(X \geq x_0)} & y \geq x_0 \\
	\end{cases}
	\end{gather*}
	Now we can calculate the expected mean (\SeeChapter{see section Statistics page \pageref{expected mean continuous variable}}) of $Y$ (assuming $X\geq x_0$):
	
	Knowing that (\SeeChapter{see section Statistics page \pageref{pareto tail distribution}}):
	
	We finally have:
	
	$\text{E}(Y)$ represents also the average income of those with a income above $x_0$ and as can be seen from the above equality it is in a constant ratio, greater than $1$, to your income $x_0$.

	We can check this result by doing a Monte Carlo simulation in a spreadsheet software (it is interesting to mention it to generalize to situations not computable by hand). You just need to simulate the inverse of the distribution function:
	
	in Microsoft Excel 11.8346:

	\begin{center}
 	\texttt{=(\$B\$7\^{}\$B\$6}/(1-RANDBETWEEN(1,10000)/10000))\^{}(1/\$B\$6)
	\end{center}

	and then take the average of the values obtained above or equal to a given $X$ (which corresponds to $x_0$) and ensure that we get the good results as proved above!

	Therefore, generally for a discrete variable, the "\NewTerm{conditional mean}\index{conditional mean}" or "\NewTerm{conditional expectation}" is commonly defined by:
	
	But this is only the case when we take an equality with the prior variable! If we generalize it to any other order relation, then a better definition would be:
	
	Obviously, we could also calculate the conditional variance (verbatim the conditional standard deviation) as we will see it further below!
	
	\begin{theorem}
	Let us prove now a property that will be useful to us especially during our study of financial time series. It is based on the fact that sometimes we have to calculate such relations:
	
	that are named "\NewTerm{iterated conditional mean}\index{iterated conditional mean}\label{iterated conditional mean}" or "\NewTerm{law of total expectation}\index{law of total expectation}" (and even sometimes named the "tower rule"...) also sometimes denoted $\text{E}\left(\text{E}(X|Y=y)\right)$ or $\text{E}\left(\text{E}(X|\pi)\right)$.
	\end{theorem}
	\begin{dem}
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	Let us do the same for the variance (\SeeChapter{see section Statistics page \pageref{variance continuous variable}})! First let us recall the definition of the variance (or "unconditional variance"). If $Y$ is a random variable, and we denote:
	
	then the unconditional variance of $Y$ is obviously defined as:
	
	Now let us now consider two random variables $X,Y$ and we want to calculate the conditional variance of $Y$ given $X$ that we will denote $\text{V}(Y|X)$. Similarly as the unconditional variance we can put:
	
	The conditional variance will be defined as:
	
	Keep in mind that the unconditional variance is a scalar, when the conditional variance remains a random variable itself! So to get a scalar for the conditional variance, we have to estimate that latter for a given value of $x$, what we will denote by $\text{V}(Y|X=x)$.
	
	Thus:
	
	As the terms $\text{E}(Y|X=x)$ are scalars, we can take them out:
	
	The expected conditional value of a constant is always equal to that constant, ie:
	
	Therefore:
	
	We recognize in the last equality above, the Huygens relation (see page \pageref{huygens relation}) of the unconditional mean. Therefore the conditional variance is defined as:
	
	Explicitly this will be written for continuous variables:
	
	\begin{theorem}
	Now let us prove the law of "\NewTerm{iterated conditional variance}\index{iterated conditional variance}\label{iterated condititional variance}", also named the "\NewTerm{law of total variance}\index{law of total variance}":
	
	\end{theorem}
	\begin{dem}
	First we start form Huygens relation (see page \pageref{huygens relation}):
	
	The law of iterated means give us:
	
	Injecting it in the previous relation, we get:
	
	Using the result we have get earlier above:
	
	and injecting that in the previous relation, we get:
	
	Now we use again the law of iterated means that we square:
	
	And we inject it in the previous relation to get:
	
	We recognize in that latter equality the Huygens relation (see page \pageref{huygens relation})! Indeed:
	
	So finally:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}

	\pagebreak
	\subsubsection{Bayesian Networks}
	"\NewTerm{Bayesian networks}\index{Bayesian networks}", also named "\NewTerm{Bayesian Belief Nets}\index{Bayesian Belief Nets}" abbreviated BNNs, or "\NewTerm{Causal Bayesian networks}, are simply a graphical representation of a problem of conditional probabilities to better visualize the interaction between the different variables when they begin to be in large numbers.

	This is a technique increasingly used in decision aided software (Data Mining), artificial intelligence (A.I.) and also in the analysis and risk management (ISO 31010 norm).

	Bayesian networks are by definition directed acyclic graphs (\SeeChapter{see section  Graphs Theory page \pageref{acyclic graph}}), so that an event can not (even indirectly) influence its own probability, with quantitative description of dependencies between events.

	These graphs are used for both knowledge representation models and calculating conditional probabilities machines. They are mainly used for diagnosis (medical and industrial), risk analysis (diagnostics failures, faults or accidents), spam detection (Bayesian filter), voice text and image opinions analysis, fraud detection or bad payers as well as data mining (M.K.M.: Mining and Knowledge Management) in general.

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Many systems and software based on drawings or on information in existing databases exists to build and analyse Bayesian networks. Paid solutions: SQL Server, Oracle, Hugin. Free solutions (at this date): Tanagra, Microsoft Belief Network MSBNX 1.4.2, RapidMiner, \texttt{R} (\texttt{bnlearn} package). Personally for pedagogical reasons, I prefer the simplicity of the small software MSBNX from Microsoft. For information, in 15 years of professional experience as a consultant I have met so far only one company on more than 800 multinationals which used Bayesian networks... (in the field of transportation).
	\end{tcolorbox}

	Use a Bayesian network is assimilated to do "\NewTerm{Bayesian inference}\index{Bayesian inference}" or "\NewTerm{Causal inference}\index{causal inference}". Based on observed information (causal probabilities), we calculate the probability of possible known data but not observed.

	For a given domain (e.g. medical), we describe the causal relations between variables of interest by a graph (we do not need again to specify that it is acyclic). In this graph, the causal relations between variables are not deterministic, but probabilistic. Thus, the observation of a cause or multiple causes does not always implies the effect or effects that depend on it, but only changes the probability of observing them.

	The particular interest of Bayesian networks is to consider simultaneously a priori knowledge of experts (in the graph) and experience contained in the data.

	Example of five variables with their relations (directed acyclic graph) and numbering of states/variables:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,622); %set diagram left start at 0, and has height of 622
		
		%Shape: Ellipse [id:dp12467334724920098] 
		\draw   (46,64) .. controls (46,42.78) and (76.11,25.57) .. (113.25,25.57) .. controls (150.39,25.57) and (180.5,42.78) .. (180.5,64) .. controls (180.5,85.22) and (150.39,102.43) .. (113.25,102.43) .. controls (76.11,102.43) and (46,85.22) .. (46,64) -- cycle ;
		%Shape: Ellipse [id:dp49657665179078525] 
		\draw   (223,64) .. controls (223,42.78) and (253.11,25.57) .. (290.25,25.57) .. controls (327.39,25.57) and (357.5,42.78) .. (357.5,64) .. controls (357.5,85.22) and (327.39,102.43) .. (290.25,102.43) .. controls (253.11,102.43) and (223,85.22) .. (223,64) -- cycle ;
		%Shape: Ellipse [id:dp809184291752139] 
		\draw   (142,228) .. controls (142,206.78) and (172.11,189.57) .. (209.25,189.57) .. controls (246.39,189.57) and (276.5,206.78) .. (276.5,228) .. controls (276.5,249.22) and (246.39,266.43) .. (209.25,266.43) .. controls (172.11,266.43) and (142,249.22) .. (142,228) -- cycle ;
		%Straight Lines [id:da468659977485846] 
		\draw    (137.25,100.43) -- (160.11,146.19) -- (182.61,191.21) ;
		\draw [shift={(183.5,193)}, rotate = 243.45] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da12694664273113965] 
		\draw    (262.5,99) -- (239.98,191.06) ;
		\draw [shift={(239.5,193)}, rotate = 283.75] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Ellipse [id:dp923721477205723] 
		\draw   (336,225) .. controls (336,203.78) and (366.11,186.57) .. (403.25,186.57) .. controls (440.39,186.57) and (470.5,203.78) .. (470.5,225) .. controls (470.5,246.22) and (440.39,263.43) .. (403.25,263.43) .. controls (366.11,263.43) and (336,246.22) .. (336,225) -- cycle ;
		%Straight Lines [id:da12709420914335312] 
		\draw    (319.5,99) -- (356.78,196.13) ;
		\draw [shift={(357.5,198)}, rotate = 249] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da010908811603276503] 
		\draw    (430.5,260) -- (467.78,357.13) ;
		\draw [shift={(468.5,359)}, rotate = 249] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Ellipse [id:dp9799222255620781] 
		\draw   (441,389) .. controls (441,367.78) and (471.11,350.57) .. (508.25,350.57) .. controls (545.39,350.57) and (575.5,367.78) .. (575.5,389) .. controls (575.5,410.22) and (545.39,427.43) .. (508.25,427.43) .. controls (471.11,427.43) and (441,410.22) .. (441,389) -- cycle ;
		
		% Text Node
		\draw (64,48.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{75pt}\setlength\topsep{0pt}
		\begin{center}
		Work Accident\\$S1$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (236,48.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{80pt}\setlength\topsep{0pt}
		\begin{center}
		Machine Failure\\$S2$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (186,200) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{29.36pt}\setlength\topsep{0pt}
		\begin{center}
		Alarm\\$S3$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (351,208) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{80pt}\setlength\topsep{0pt}
		\begin{center}
		Production Stop\\$\displaystyle S4$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (469,365) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{53.19pt}\setlength\topsep{0pt}
		\begin{center}
		Evacuation\\$S5$
		\end{center}
		
		\end{minipage}};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Example of Bayesian network (acyclic oriented) with $5$ states}
	\end{figure}
	Obviously, the construction of the causal graph is based primarily on return of experience (REX) and sometimes results on standards or reports of expert committees. In computing science, the causal graph automatically change depending on the content of databases (think at the Amazon book store in real time target advertisements based on your past purchases or at the Genius Apple service). But we can rarely think to all possibilities and there will sometimes hidden states between two known states that have been forgotten and that would have allowed to better modelize the situations.
	
	Suppose in the example above that with the help of a corporate database, we know that in about $100,000$ man-days, we had in the company $1,000$ accidents (i.e.. $1\%$ of total) and $100$ machines failures (i.e.. $0.01\% $of total). Then we represent it in the traditional form as follows:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,622); %set diagram left start at 0, and has height of 622
		
		%Shape: Ellipse [id:dp12467334724920098] 
		\draw   (46,64) .. controls (46,42.78) and (76.11,25.57) .. (113.25,25.57) .. controls (150.39,25.57) and (180.5,42.78) .. (180.5,64) .. controls (180.5,85.22) and (150.39,102.43) .. (113.25,102.43) .. controls (76.11,102.43) and (46,85.22) .. (46,64) -- cycle ;
		%Shape: Ellipse [id:dp49657665179078525] 
		\draw   (223,64) .. controls (223,42.78) and (253.11,25.57) .. (290.25,25.57) .. controls (327.39,25.57) and (357.5,42.78) .. (357.5,64) .. controls (357.5,85.22) and (327.39,102.43) .. (290.25,102.43) .. controls (253.11,102.43) and (223,85.22) .. (223,64) -- cycle ;
		%Shape: Ellipse [id:dp809184291752139] 
		\draw   (142,228) .. controls (142,206.78) and (172.11,189.57) .. (209.25,189.57) .. controls (246.39,189.57) and (276.5,206.78) .. (276.5,228) .. controls (276.5,249.22) and (246.39,266.43) .. (209.25,266.43) .. controls (172.11,266.43) and (142,249.22) .. (142,228) -- cycle ;
		%Straight Lines [id:da468659977485846] 
		\draw    (137.25,100.43) -- (160.11,146.19) -- (182.61,191.21) ;
		\draw [shift={(183.5,193)}, rotate = 243.45] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da12694664273113965] 
		\draw    (262.5,99) -- (239.98,191.06) ;
		\draw [shift={(239.5,193)}, rotate = 283.75] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Ellipse [id:dp923721477205723] 
		\draw   (336,225) .. controls (336,203.78) and (366.11,186.57) .. (403.25,186.57) .. controls (440.39,186.57) and (470.5,203.78) .. (470.5,225) .. controls (470.5,246.22) and (440.39,263.43) .. (403.25,263.43) .. controls (366.11,263.43) and (336,246.22) .. (336,225) -- cycle ;
		%Straight Lines [id:da12709420914335312] 
		\draw    (319.5,99) -- (356.78,196.13) ;
		\draw [shift={(357.5,198)}, rotate = 249] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da010908811603276503] 
		\draw    (430.5,260) -- (467.78,357.13) ;
		\draw [shift={(468.5,359)}, rotate = 249] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Ellipse [id:dp9799222255620781] 
		\draw   (441,389) .. controls (441,367.78) and (471.11,350.57) .. (508.25,350.57) .. controls (545.39,350.57) and (575.5,367.78) .. (575.5,389) .. controls (575.5,410.22) and (545.39,427.43) .. (508.25,427.43) .. controls (471.11,427.43) and (441,410.22) .. (441,389) -- cycle ;
		
		% Text Node
		\draw (64,48.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{75pt}\setlength\topsep{0pt}
		\begin{center}
		Work Accident\\$S1$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (236,48.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{80pt}\setlength\topsep{0pt}
		\begin{center}
		Machine Failure\\$S2$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (186,200) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{29.36pt}\setlength\topsep{0pt}
		\begin{center}
		Alarm\\$S3$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (351,208) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{80pt}\setlength\topsep{0pt}
		\begin{center}
		Production Stop\\$\displaystyle S4$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (469,365) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{53.19pt}\setlength\topsep{0pt}
		\begin{center}
		Evacuation\\$S5$
		\end{center}
		
		\end{minipage}};
		
		% Text Node
		\draw (46,4.4) node [anchor=north west][inner sep=0.75pt]    {$P( S1) =[ 1\%,99\%]$};
		% Text Node
		\draw (212,4.4) node [anchor=north west][inner sep=0.75pt]    {$P( S2) =[ 0.1\%,99.9\%]$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Directed acyclic Bayesian network with probability of departure}
	\end{figure}	
	where we have the subset $S2, S4, S5$ which is what experts name a "\NewTerm{serial or linear connection}\index{serial or linear connection}", the triplet $S3, S2, S4$ is a named a "\NewTerm{divergent relation}\index{divergent relation}" (if the arrows were reversed for the triplet, we would have a "\NewTerm{converging relation}\index{converging relation}").
	
	Before going further with our example we will make some observations in relation to these three types of relations:
	
	For clarity, we distinguish first "\NewTerm{conditional independence}\index{conditional independence}" and "\NewTerm{conditional dependence}\index{condition dependence}".
	
	We say that events $A$ and $C$ are "\NewTerm{conditionally independent}\index{conditionally independent}" if given an event $B$ the following equality holds:
	
So the term "conditional" implies the presence of $B$ and the fact that $C$ does not influence the probability of the event $A$.

	About "\NewTerm{conditional dependence}", this time we can distinguish three types of relations.
	\begin{enumerate}
		\item The conditional dependence of the following type is named a "\NewTerm{serial or linear connection}\index{serial or linear connection}" (already mentioned above):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,387); %set diagram left start at 0, and has height of 387
		
		%Shape: Ellipse [id:dp767195317495946] 
		\draw  [line width=1.5]  (111,65.83) .. controls (111,42.73) and (139.54,24) .. (174.75,24) .. controls (209.96,24) and (238.5,42.73) .. (238.5,65.83) .. controls (238.5,88.94) and (209.96,107.67) .. (174.75,107.67) .. controls (139.54,107.67) and (111,88.94) .. (111,65.83) -- cycle ;
		%Shape: Ellipse [id:dp19993954535292335] 
		\draw  [line width=1.5]  (263,65.83) .. controls (263,42.73) and (291.54,24) .. (326.75,24) .. controls (361.96,24) and (390.5,42.73) .. (390.5,65.83) .. controls (390.5,88.94) and (361.96,107.67) .. (326.75,107.67) .. controls (291.54,107.67) and (263,88.94) .. (263,65.83) -- cycle ;
		%Shape: Ellipse [id:dp23863514407536024] 
		\draw  [line width=1.5]  (415,65.83) .. controls (415,42.73) and (443.54,24) .. (478.75,24) .. controls (513.96,24) and (542.5,42.73) .. (542.5,65.83) .. controls (542.5,88.94) and (513.96,107.67) .. (478.75,107.67) .. controls (443.54,107.67) and (415,88.94) .. (415,65.83) -- cycle ;
		%Straight Lines [id:da9873075564161087] 
		\draw    (238.5,65.83) -- (261,65.83) ;
		\draw [shift={(263,65.83)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da315975467596066] 
		\draw    (390.5,65.83) -- (413,65.83) ;
		\draw [shift={(415,65.83)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		
		% Text Node
		\draw (166,54) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle A$};
		% Text Node
		\draw (321,54) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle B$};
		% Text Node
		\draw (470,55) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle C$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Serial/linear Bayesian network}
	\end{figure}
	where $A$, $B$ and $C$ are dependent (in this particular example there are 3 dependent nodes $A$, $B$ and $C$, but in general this dependence relates to all nodes if there were more than 3)
	
	In addition, $A$ and $C$ are conditionally dependent to $B$. But if the variable $B$ is known, $A$ no longer provides any useful information about $C$ (the path of uncertainty is somehow broken) and therefore $A$ and $C$ become conditionally independent. We then have the conditional probability that simplify as follows:
	
	
	\item The conditional dependence of the following type is named a "\NewTerm{divergent connection}\index{divergent connection}" (as already mentioned above):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,387); %set diagram left start at 0, and has height of 387
		
		%Shape: Ellipse [id:dp767195317495946] 
		\draw  [line width=1.5]  (243,58.83) .. controls (243,35.73) and (271.54,17) .. (306.75,17) .. controls (341.96,17) and (370.5,35.73) .. (370.5,58.83) .. controls (370.5,81.94) and (341.96,100.67) .. (306.75,100.67) .. controls (271.54,100.67) and (243,81.94) .. (243,58.83) -- cycle ;
		%Shape: Ellipse [id:dp19993954535292335] 
		\draw  [line width=1.5]  (151,196.83) .. controls (151,173.73) and (179.54,155) .. (214.75,155) .. controls (249.96,155) and (278.5,173.73) .. (278.5,196.83) .. controls (278.5,219.94) and (249.96,238.67) .. (214.75,238.67) .. controls (179.54,238.67) and (151,219.94) .. (151,196.83) -- cycle ;
		%Shape: Ellipse [id:dp23863514407536024] 
		\draw  [line width=1.5]  (335,199.83) .. controls (335,176.73) and (363.54,158) .. (398.75,158) .. controls (433.96,158) and (462.5,176.73) .. (462.5,199.83) .. controls (462.5,222.94) and (433.96,241.67) .. (398.75,241.67) .. controls (363.54,241.67) and (335,222.94) .. (335,199.83) -- cycle ;
		%Straight Lines [id:da9873075564161087] 
		\draw    (255.5,83.67) -- (215.74,153.26) ;
		\draw [shift={(214.75,155)}, rotate = 299.74] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da315975467596066] 
		\draw    (355.5,85.67) -- (397.72,156.28) ;
		\draw [shift={(398.75,158)}, rotate = 239.12] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		
		% Text Node
		\draw (298,47) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle A$};
		% Text Node
		\draw (209,185) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle B$};
		% Text Node
		\draw (390,189) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle C$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Divergent Bayesian network}
	\end{figure}
	In addition, $B$ and $C$ are conditionally dependent on $A$. But if $A$ is known, $B$ does not provide any more information on $C$ (again the path of uncertainty is somehow broken) and therefore $B$ and $C$ become independent. We then have for example if $A$ is known:
	
	
	\item The conditional dependence of the following type is named a "\NewTerm{convergence connection}\index{convergence connection}" or "\NewTerm{$V$-Structure}\index{$V$-structure}" (as already mentioned above):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,387); %set diagram left start at 0, and has height of 387
		
		%Shape: Ellipse [id:dp767195317495946] 
		\draw  [line width=1.5]  (246,209.83) .. controls (246,186.73) and (274.54,168) .. (309.75,168) .. controls (344.96,168) and (373.5,186.73) .. (373.5,209.83) .. controls (373.5,232.94) and (344.96,251.67) .. (309.75,251.67) .. controls (274.54,251.67) and (246,232.94) .. (246,209.83) -- cycle ;
		%Shape: Ellipse [id:dp19993954535292335] 
		\draw  [line width=1.5]  (148,62.83) .. controls (148,39.73) and (176.54,21) .. (211.75,21) .. controls (246.96,21) and (275.5,39.73) .. (275.5,62.83) .. controls (275.5,85.94) and (246.96,104.67) .. (211.75,104.67) .. controls (176.54,104.67) and (148,85.94) .. (148,62.83) -- cycle ;
		%Shape: Ellipse [id:dp23863514407536024] 
		\draw  [line width=1.5]  (332,62.83) .. controls (332,39.73) and (360.54,21) .. (395.75,21) .. controls (430.96,21) and (459.5,39.73) .. (459.5,62.83) .. controls (459.5,85.94) and (430.96,104.67) .. (395.75,104.67) .. controls (360.54,104.67) and (332,85.94) .. (332,62.83) -- cycle ;
		%Straight Lines [id:da9873075564161087] 
		\draw    (362.5,97.67) -- (322.74,167.26) ;
		\draw [shift={(321.75,169)}, rotate = 299.74] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da315975467596066] 
		\draw    (253.5,95.67) -- (295.72,166.28) ;
		\draw [shift={(296.75,168)}, rotate = 239.12] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		
		% Text Node
		\draw (301,198) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle A$};
		% Text Node
		\draw (206,51) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle B$};
		% Text Node
		\draw (387,52) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle C$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Convergent Bayesian network}
	\end{figure}
	where this time the parents are independent.
	
	So $B$ and $C$ are independent but become conditionally dependent on $A$. If $A$ is known, then we have:
	
	The dependence between parents therefore requires the observation of their common child.
	\end{enumerate}
	
	Now to make a concrete example, let us first recall the Bayes' theorem:
	
	and now suppose that we have a knowledge database that gives us (thanks to quality managers who always inputs the quality issues) that when a machine failure occurred, $99$ times out of $100$ ($99\%$) there has been a total production stop (i.e. $1\%$ of time there was no production stop) and on all stop production $1\%$ was not due to a machine failure. What we traditionally represent as follows:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,622); %set diagram left start at 0, and has height of 622
		
		%Shape: Ellipse [id:dp12467334724920098] 
		\draw   (46,64) .. controls (46,42.78) and (76.11,25.57) .. (113.25,25.57) .. controls (150.39,25.57) and (180.5,42.78) .. (180.5,64) .. controls (180.5,85.22) and (150.39,102.43) .. (113.25,102.43) .. controls (76.11,102.43) and (46,85.22) .. (46,64) -- cycle ;
		%Shape: Ellipse [id:dp49657665179078525] 
		\draw   (223,64) .. controls (223,42.78) and (253.11,25.57) .. (290.25,25.57) .. controls (327.39,25.57) and (357.5,42.78) .. (357.5,64) .. controls (357.5,85.22) and (327.39,102.43) .. (290.25,102.43) .. controls (253.11,102.43) and (223,85.22) .. (223,64) -- cycle ;
		%Shape: Ellipse [id:dp809184291752139] 
		\draw   (142,228) .. controls (142,206.78) and (172.11,189.57) .. (209.25,189.57) .. controls (246.39,189.57) and (276.5,206.78) .. (276.5,228) .. controls (276.5,249.22) and (246.39,266.43) .. (209.25,266.43) .. controls (172.11,266.43) and (142,249.22) .. (142,228) -- cycle ;
		%Straight Lines [id:da468659977485846] 
		\draw    (137.25,100.43) -- (160.11,146.19) -- (182.61,191.21) ;
		\draw [shift={(183.5,193)}, rotate = 243.45] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da12694664273113965] 
		\draw    (262.5,99) -- (239.98,191.06) ;
		\draw [shift={(239.5,193)}, rotate = 283.75] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Ellipse [id:dp923721477205723] 
		\draw   (336,225) .. controls (336,203.78) and (366.11,186.57) .. (403.25,186.57) .. controls (440.39,186.57) and (470.5,203.78) .. (470.5,225) .. controls (470.5,246.22) and (440.39,263.43) .. (403.25,263.43) .. controls (366.11,263.43) and (336,246.22) .. (336,225) -- cycle ;
		%Straight Lines [id:da12709420914335312] 
		\draw    (319.5,99) -- (356.78,196.13) ;
		\draw [shift={(357.5,198)}, rotate = 249] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da010908811603276503] 
		\draw    (430.5,260) -- (467.78,357.13) ;
		\draw [shift={(468.5,359)}, rotate = 249] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Ellipse [id:dp9799222255620781] 
		\draw   (441,389) .. controls (441,367.78) and (471.11,350.57) .. (508.25,350.57) .. controls (545.39,350.57) and (575.5,367.78) .. (575.5,389) .. controls (575.5,410.22) and (545.39,427.43) .. (508.25,427.43) .. controls (471.11,427.43) and (441,410.22) .. (441,389) -- cycle ;
		
		%Shape: Rectangle [id:dp717086071799653] 
		\draw   (421,64) -- (607.5,64) -- (607.5,174) -- (421,174) -- cycle ;
		%Straight Lines [id:da9043646976206514] 
		\draw    (444.67,138) -- (607.33,138) ;
		%Straight Lines [id:da9780338009570977] 
		\draw    (466,86) -- (466,174.33) ;
		%Straight Lines [id:da6258827855395317] 
		\draw    (530,86) -- (530,174.33) ;
		%Straight Lines [id:da5251415486124953] 
		\draw    (444,104) -- (606.67,104) ;
		%Straight Lines [id:da2995892779113214] 
		\draw    (445,86) -- (607.67,86) ;
		%Straight Lines [id:da24286384796129057] 
		\draw    (445,64) -- (445,174.33) ;
		
		% Text Node
		\draw (64,48.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{75pt}\setlength\topsep{0pt}
		\begin{center}
		Work Accident\\$S1$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (236,48.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{80pt}\setlength\topsep{0pt}
		\begin{center}
		Machine Failure\\$S2$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (186,200) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{29.36pt}\setlength\topsep{0pt}
		\begin{center}
		Alarm\\$S3$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (351,208) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{80pt}\setlength\topsep{0pt}
		\begin{center}
		Production Stop\\$\displaystyle S4$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (469,365) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{53.19pt}\setlength\topsep{0pt}
		\begin{center}
		Evacuation\\$S5$
		\end{center}
		
		\end{minipage}};
		
		% Text Node
		\draw (46,4.4) node [anchor=north west][inner sep=0.75pt]    {$P( S1) =[ 1\%,99\%]$};
		% Text Node
		\draw (212,4.4) node [anchor=north west][inner sep=0.75pt]    {$P( S2) =[ 0.1\%,99.9\%]$};
		% Text Node
		\draw (487.33,44.4) node [anchor=north west][inner sep=0.75pt]    {$P( S4/S2)$};
		\draw (424,172) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Production stop};
		% Text Node
		\draw (469,67) node [anchor=north west][inner sep=0.75pt]   [align=left] {Machine Failure};
		% Text Node
		\draw (484.17,88) node [anchor=north west][inner sep=0.75pt]   [align=left] {Yes};
		% Text Node
		\draw (554,88) node [anchor=north west][inner sep=0.75pt]   [align=left] {No};
		% Text Node
		\draw (448.33,133.17) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Yes};
		% Text Node
		\draw (448.33,166.33) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {No};
		% Text Node
		\draw (480.67,112.57) node [anchor=north west][inner sep=0.75pt]    {$99\%$};
		% Text Node
		\draw (552,112.57) node [anchor=north west][inner sep=0.75pt]    {$1\%$};
		% Text Node
		\draw (484.67,148.23) node [anchor=north west][inner sep=0.75pt]    {$1\%$};
		% Text Node
		\draw (548,148.23) node [anchor=north west][inner sep=0.75pt]    {$99\%$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[]{Directed acyclic Bayesian network with probability of departure}
	\end{figure}
	So the "\NewTerm{implicit probability}\index{implicit probability}" (expectedness) that there is a production stop is given by:
	
	This value represents the implicit proportion of productions stop from the $100,000$ man-days (so we can give a proportion of rows in the database that represents a production stop regardless of the cause and even without knowing the details of the database).
		
	It then follows immediately that the implicit probability (expectedness) that there is no production stop is given by:
	
	This is consistent with what gives us the freeware MSBNX 1.4.2:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/arithmetics/msbnx_beginning.eps}
		\caption[]{Beginning of the Bayesian network with MSBNX 1.4.2}
	\end{figure}
	Now suppose we observed a production stop. What is the a posteriori probability that it is due to a machine failure? We then have:
	
	We can also check this with the software MSBNX 1.4.2:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/arithmetics/msbnx_machine_failure_aposteriori_probability.eps}
		\caption{A Posteriori probability of a production stop due to a machine failure in MSBNX 1.4.2}
	\end{figure}
	Now, imagine that our database gives us (always thanks to quality managers who ensured to input quality issues) that $99$ times out of $100$ ($99\%$) when there was a production stop, there was an evacuation. However $5\%$ of evacuations were identified as having nothing to do with a production stop (i.e. $95\%$ of evacuations are due to fire exercises OR other events):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,622); %set diagram left start at 0, and has height of 622
		
		%Shape: Ellipse [id:dp12467334724920098] 
		\draw   (46,64) .. controls (46,42.78) and (76.11,25.57) .. (113.25,25.57) .. controls (150.39,25.57) and (180.5,42.78) .. (180.5,64) .. controls (180.5,85.22) and (150.39,102.43) .. (113.25,102.43) .. controls (76.11,102.43) and (46,85.22) .. (46,64) -- cycle ;
		%Shape: Ellipse [id:dp49657665179078525] 
		\draw   (223,64) .. controls (223,42.78) and (253.11,25.57) .. (290.25,25.57) .. controls (327.39,25.57) and (357.5,42.78) .. (357.5,64) .. controls (357.5,85.22) and (327.39,102.43) .. (290.25,102.43) .. controls (253.11,102.43) and (223,85.22) .. (223,64) -- cycle ;
		%Shape: Ellipse [id:dp809184291752139] 
		\draw   (142,228) .. controls (142,206.78) and (172.11,189.57) .. (209.25,189.57) .. controls (246.39,189.57) and (276.5,206.78) .. (276.5,228) .. controls (276.5,249.22) and (246.39,266.43) .. (209.25,266.43) .. controls (172.11,266.43) and (142,249.22) .. (142,228) -- cycle ;
		%Straight Lines [id:da468659977485846] 
		\draw    (137.25,100.43) -- (160.11,146.19) -- (182.61,191.21) ;
		\draw [shift={(183.5,193)}, rotate = 243.45] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da12694664273113965] 
		\draw    (262.5,99) -- (239.98,191.06) ;
		\draw [shift={(239.5,193)}, rotate = 283.75] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Ellipse [id:dp923721477205723] 
		\draw   (336,225) .. controls (336,203.78) and (366.11,186.57) .. (403.25,186.57) .. controls (440.39,186.57) and (470.5,203.78) .. (470.5,225) .. controls (470.5,246.22) and (440.39,263.43) .. (403.25,263.43) .. controls (366.11,263.43) and (336,246.22) .. (336,225) -- cycle ;
		%Straight Lines [id:da12709420914335312] 
		\draw    (319.5,99) -- (356.78,196.13) ;
		\draw [shift={(357.5,198)}, rotate = 249] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da010908811603276503] 
		\draw    (430.5,260) -- (467.78,357.13) ;
		\draw [shift={(468.5,359)}, rotate = 249] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Ellipse [id:dp9799222255620781] 
		\draw   (441,389) .. controls (441,367.78) and (471.11,350.57) .. (508.25,350.57) .. controls (545.39,350.57) and (575.5,367.78) .. (575.5,389) .. controls (575.5,410.22) and (545.39,427.43) .. (508.25,427.43) .. controls (471.11,427.43) and (441,410.22) .. (441,389) -- cycle ;
		
		%Shape: Rectangle [id:dp717086071799653] 
		\draw   (421,64) -- (607.5,64) -- (607.5,174) -- (421,174) -- cycle ;
		%Straight Lines [id:da9043646976206514] 
		\draw    (444.67,138) -- (607.33,138) ;
		%Straight Lines [id:da9780338009570977] 
		\draw    (466,86) -- (466,174.33) ;
		%Straight Lines [id:da6258827855395317] 
		\draw    (530,86) -- (530,174.33) ;
		%Straight Lines [id:da5251415486124953] 
		\draw    (444,104) -- (606.67,104) ;
		%Straight Lines [id:da2995892779113214] 
		\draw    (445,86) -- (607.67,86) ;
		%Straight Lines [id:da24286384796129057] 
		\draw    (445,64) -- (445,174.33) ;
		%Shape: Rectangle [id:dp36239181446824764] 
		\draw   (243,334) -- (429.5,334) -- (429.5,444) -- (243,444) -- cycle ;
		%Straight Lines [id:da740059675612978] 
		\draw    (266.67,408) -- (429.33,408) ;
		%Straight Lines [id:da7983637069425531] 
		\draw    (288,356) -- (288,444.33) ;
		%Straight Lines [id:da6001737009244024] 
		\draw    (352,356) -- (352,444.33) ;
		%Straight Lines [id:da6194045298398894] 
		\draw    (266,374) -- (428.67,374) ;
		%Straight Lines [id:da39009385039482813] 
		\draw    (267,356) -- (429.67,356) ;
		%Straight Lines [id:da7710086368840021] 
		\draw    (267,334) -- (267,444.33) ;
		
		% Text Node
		\draw (64,48.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{75pt}\setlength\topsep{0pt}
		\begin{center}
		Work Accident\\$S1$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (236,48.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{80pt}\setlength\topsep{0pt}
		\begin{center}
		Machine Failure\\$S2$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (186,200) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{29.36pt}\setlength\topsep{0pt}
		\begin{center}
		Alarm\\$S3$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (351,208) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{80pt}\setlength\topsep{0pt}
		\begin{center}
		Production Stop\\$\displaystyle S4$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (469,365) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{53.19pt}\setlength\topsep{0pt}
		\begin{center}
		Evacuation\\$S5$
		\end{center}
		
		\end{minipage}};
		
		% Text Node
		\draw (46,4.4) node [anchor=north west][inner sep=0.75pt]    {$P( S1) =[ 1\%,99\%]$};
		% Text Node
		\draw (212,4.4) node [anchor=north west][inner sep=0.75pt]    {$P( S2) =[ 0.1\%,99.9\%]$};
		% Text Node
		\draw (487.33,44.4) node [anchor=north west][inner sep=0.75pt]    {$P( S4/S2)$};
		\draw (424,172) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Production stop};
		% Text Node
		\draw (469,67) node [anchor=north west][inner sep=0.75pt]   [align=left] {Machine Failure};
		% Text Node
		\draw (484.17,88) node [anchor=north west][inner sep=0.75pt]   [align=left] {Yes};
		% Text Node
		\draw (554,88) node [anchor=north west][inner sep=0.75pt]   [align=left] {No};
		% Text Node
		\draw (448.33,133.17) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Yes};
		% Text Node
		\draw (448.33,166.33) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {No};
		% Text Node
		\draw (480.67,112.57) node [anchor=north west][inner sep=0.75pt]    {$99\%$};
		% Text Node
		\draw (552,112.57) node [anchor=north west][inner sep=0.75pt]    {$1\%$};
		% Text Node
		\draw (484.67,148.23) node [anchor=north west][inner sep=0.75pt]    {$1\%$};
		% Text Node
		\draw (548,148.23) node [anchor=north west][inner sep=0.75pt]    {$99\%$};
		% Text Node
		\draw (246,426.67) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Evacuation};
		% Text Node
		\draw (291,337) node [anchor=north west][inner sep=0.75pt]   [align=left] {Production Stop};
		% Text Node
		\draw (306.17,358) node [anchor=north west][inner sep=0.75pt]   [align=left] {Yes};
		% Text Node
		\draw (376,358) node [anchor=north west][inner sep=0.75pt]   [align=left] {No};
		% Text Node
		\draw (268.83,403.17) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Yes};
		% Text Node
		\draw (269.33,436.33) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {No};
		% Text Node
		\draw (302.67,382.57) node [anchor=north west][inner sep=0.75pt]    {$99\%$};
		% Text Node
		\draw (374,382.57) node [anchor=north west][inner sep=0.75pt]    {$5\%$};
		% Text Node
		\draw (306.67,418.23) node [anchor=north west][inner sep=0.75pt]    {$1\%$};
		% Text Node
		\draw (370,418.23) node [anchor=north west][inner sep=0.75pt]    {$95\%$};
		% Text Node
		\draw (309.33,314.4) node [anchor=north west][inner sep=0.75pt]    {$P( S5/S4)$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[]{2nd level Bayesian network}
	\end{figure}
	Now to calculate the implicit probability (expectedness) of evacuations compared to machines failures, we saw that when we had a conditional dependence series, the conditional probability depends only on the direct parent. Thus, we get the following expectedness:
		
	We can also check that this result is compliant with the on given by the software MSBNX 1.4.2:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/arithmetics/msbnx_implicit_probability_evacuation.eps}
		\caption[]{Implicit probability of an evacuation in MSBNX 1.4.2}
	\end{figure}
	So the implicit probability (expectedness) of evacuation does not actually depend on machine failures.

	Now suppose we have observed an evacuation. We want to know what is the a posteriori probability that it is due to a machine failure! Then we have:
	
	We can also check this with the software MSBNX 1.4.2:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/arithmetics/msbnx_a_posteriori_probability_evacuation.eps}
		\caption{A Posteriori probability of an evacuation due to a machine failure in MSBNX 1.4.2}
	\end{figure}
	
	Now we study the case with the alarm and again a database allows us to build a table with different probabilities:

	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,622); %set diagram left start at 0, and has height of 622
		
		%Shape: Ellipse [id:dp12467334724920098] 
		\draw   (46,64) .. controls (46,42.78) and (76.11,25.57) .. (113.25,25.57) .. controls (150.39,25.57) and (180.5,42.78) .. (180.5,64) .. controls (180.5,85.22) and (150.39,102.43) .. (113.25,102.43) .. controls (76.11,102.43) and (46,85.22) .. (46,64) -- cycle ;
		%Shape: Ellipse [id:dp49657665179078525] 
		\draw   (223,64) .. controls (223,42.78) and (253.11,25.57) .. (290.25,25.57) .. controls (327.39,25.57) and (357.5,42.78) .. (357.5,64) .. controls (357.5,85.22) and (327.39,102.43) .. (290.25,102.43) .. controls (253.11,102.43) and (223,85.22) .. (223,64) -- cycle ;
		%Shape: Ellipse [id:dp809184291752139] 
		\draw   (142,228) .. controls (142,206.78) and (172.11,189.57) .. (209.25,189.57) .. controls (246.39,189.57) and (276.5,206.78) .. (276.5,228) .. controls (276.5,249.22) and (246.39,266.43) .. (209.25,266.43) .. controls (172.11,266.43) and (142,249.22) .. (142,228) -- cycle ;
		%Straight Lines [id:da468659977485846] 
		\draw    (137.25,100.43) -- (160.11,146.19) -- (182.61,191.21) ;
		\draw [shift={(183.5,193)}, rotate = 243.45] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da12694664273113965] 
		\draw    (262.5,99) -- (239.98,191.06) ;
		\draw [shift={(239.5,193)}, rotate = 283.75] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Ellipse [id:dp923721477205723] 
		\draw   (336,225) .. controls (336,203.78) and (366.11,186.57) .. (403.25,186.57) .. controls (440.39,186.57) and (470.5,203.78) .. (470.5,225) .. controls (470.5,246.22) and (440.39,263.43) .. (403.25,263.43) .. controls (366.11,263.43) and (336,246.22) .. (336,225) -- cycle ;
		%Straight Lines [id:da12709420914335312] 
		\draw    (319.5,99) -- (356.78,196.13) ;
		\draw [shift={(357.5,198)}, rotate = 249] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da010908811603276503] 
		\draw    (430.5,260) -- (467.78,357.13) ;
		\draw [shift={(468.5,359)}, rotate = 249] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Ellipse [id:dp9799222255620781] 
		\draw   (441,389) .. controls (441,367.78) and (471.11,350.57) .. (508.25,350.57) .. controls (545.39,350.57) and (575.5,367.78) .. (575.5,389) .. controls (575.5,410.22) and (545.39,427.43) .. (508.25,427.43) .. controls (471.11,427.43) and (441,410.22) .. (441,389) -- cycle ;
		
		%Shape: Rectangle [id:dp717086071799653] 
		\draw   (421,64) -- (607.5,64) -- (607.5,174) -- (421,174) -- cycle ;
		%Straight Lines [id:da9043646976206514] 
		\draw    (444.67,138) -- (607.33,138) ;
		%Straight Lines [id:da9780338009570977] 
		\draw    (466,86) -- (466,174.33) ;
		%Straight Lines [id:da6258827855395317] 
		\draw    (530,86) -- (530,174.33) ;
		%Straight Lines [id:da5251415486124953] 
		\draw    (444,104) -- (606.67,104) ;
		%Straight Lines [id:da2995892779113214] 
		\draw    (445,86) -- (607.67,86) ;
		%Straight Lines [id:da24286384796129057] 
		\draw    (445,64) -- (445,174.33) ;
		%Shape: Rectangle [id:dp36239181446824764] 
		\draw   (243,334) -- (429.5,334) -- (429.5,444) -- (243,444) -- cycle ;
		%Straight Lines [id:da740059675612978] 
		\draw    (266.67,408) -- (429.33,408) ;
		%Straight Lines [id:da7983637069425531] 
		\draw    (288,356) -- (288,444.33) ;
		%Straight Lines [id:da6001737009244024] 
		\draw    (352,356) -- (352,444.33) ;
		%Straight Lines [id:da6194045298398894] 
		\draw    (266,374) -- (428.67,374) ;
		%Straight Lines [id:da39009385039482813] 
		\draw    (267,356) -- (429.67,356) ;
		%Straight Lines [id:da7710086368840021] 
		\draw    (267,334) -- (267,444.33) ;
		%Shape: Rectangle [id:dp404912101869251] 
		\draw   (2.33,273.33) -- (238.67,273.33) -- (238.67,387.67) -- (2.33,387.67) -- cycle ;
		%Straight Lines [id:da4538963681157826] 
		\draw    (19.33,273.33) -- (19.33,388.33) ;
		%Straight Lines [id:da7337347967469341] 
		\draw    (19,293.33) -- (239.33,293.33) ;
		%Straight Lines [id:da6500189527665825] 
		\draw    (19.33,313.33) -- (207.33,313.33) -- (238.67,313.33) ;
		%Straight Lines [id:da7469305914760125] 
		\draw    (18.67,351.33) -- (206.67,351.33) -- (238,351.33) ;
		%Straight Lines [id:da1447557601088325] 
		\draw    (37.33,294) -- (37.33,387) ;
		%Straight Lines [id:da5953106903295038] 
		\draw    (92.67,294) -- (92.67,387) ;
		%Straight Lines [id:da5921673653282611] 
		\draw    (143,294.33) -- (143,387.33) ;
		%Straight Lines [id:da8533495589691469] 
		\draw    (192.67,294) -- (192.67,387) ;
		
		% Text Node
		\draw (64,48.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{75pt}\setlength\topsep{0pt}
		\begin{center}
		Work Accident\\$S1$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (236,48.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{80pt}\setlength\topsep{0pt}
		\begin{center}
		Machine Failure\\$S2$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (186,200) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{29.36pt}\setlength\topsep{0pt}
		\begin{center}
		Alarm\\$S3$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (351,208) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{80pt}\setlength\topsep{0pt}
		\begin{center}
		Production Stop\\$\displaystyle S4$
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (469,365) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{53.19pt}\setlength\topsep{0pt}
		\begin{center}
		Evacuation\\$S5$
		\end{center}
		
		\end{minipage}};
		
		% Text Node
		\draw (46,4.4) node [anchor=north west][inner sep=0.75pt]    {$P( S1) =[ 1\%,99\%]$};
		% Text Node
		\draw (212,4.4) node [anchor=north west][inner sep=0.75pt]    {$P( S2) =[ 0.1\%,99.9\%]$};
		% Text Node
		\draw (487.33,44.4) node [anchor=north west][inner sep=0.75pt]    {$P( S4/S2)$};
		\draw (424,172) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Production stop};
		% Text Node
		\draw (469,67) node [anchor=north west][inner sep=0.75pt]   [align=left] {Machine Failure};
		% Text Node
		\draw (484.17,88) node [anchor=north west][inner sep=0.75pt]   [align=left] {Yes};
		% Text Node
		\draw (554,88) node [anchor=north west][inner sep=0.75pt]   [align=left] {No};
		% Text Node
		\draw (448.33,133.17) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Yes};
		% Text Node
		\draw (448.33,166.33) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {No};
		% Text Node
		\draw (480.67,112.57) node [anchor=north west][inner sep=0.75pt]    {$99\%$};
		% Text Node
		\draw (552,112.57) node [anchor=north west][inner sep=0.75pt]    {$1\%$};
		% Text Node
		\draw (484.67,148.23) node [anchor=north west][inner sep=0.75pt]    {$1\%$};
		% Text Node
		\draw (548,148.23) node [anchor=north west][inner sep=0.75pt]    {$99\%$};
		% Text Node
		\draw (246,426.67) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Evacuation};
		% Text Node
		\draw (291,337) node [anchor=north west][inner sep=0.75pt]   [align=left] {Production Stop};
		% Text Node
		\draw (306.17,358) node [anchor=north west][inner sep=0.75pt]   [align=left] {Yes};
		% Text Node
		\draw (376,358) node [anchor=north west][inner sep=0.75pt]   [align=left] {No};
		% Text Node
		\draw (268.83,403.17) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Yes};
		% Text Node
		\draw (269.33,436.33) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {No};
		% Text Node
		\draw (302.67,382.57) node [anchor=north west][inner sep=0.75pt]    {$99\%$};
		% Text Node
		\draw (374,382.57) node [anchor=north west][inner sep=0.75pt]    {$5\%$};
		% Text Node
		\draw (306.67,418.23) node [anchor=north west][inner sep=0.75pt]    {$1\%$};
		% Text Node
		\draw (370,418.23) node [anchor=north west][inner sep=0.75pt]    {$95\%$};
		% Text Node
		\draw (309.33,314.4) node [anchor=north west][inner sep=0.75pt]    {$P(S5/S4)$};
		% Text Node
		\draw (3,353.33) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Alarm};
		% Text Node
		\draw (60.67,252.07) node [anchor=north west][inner sep=0.75pt]    {$P( S3/S1,S2)$};
		% Text Node
		\draw (22.33,276.33) node [anchor=north west][inner sep=0.75pt]   [align=left] {Work Accident, Machine Failure};
		% Text Node
		\draw (21,345.83) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Yes};
		% Text Node
		\draw (21,379) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {No};
		% Text Node
		\draw (37.33,295.33) node [anchor=north west][inner sep=0.75pt]   [align=left] {Yes,Yes};
		% Text Node
		\draw (142.67,295.33) node [anchor=north west][inner sep=0.75pt]   [align=left] {No,Yes};
		% Text Node
		\draw (93,295.33) node [anchor=north west][inner sep=0.75pt]   [align=left] {Yes,No};
		% Text Node
		\draw (192.33,295.33) node [anchor=north west][inner sep=0.75pt]   [align=left] {No,No};
		% Text Node
		\draw (49.67,324.23) node [anchor=north west][inner sep=0.75pt]    {$75\%$};
		% Text Node
		\draw (100.39,324.23) node [anchor=north west][inner sep=0.75pt]    {$10\%$};
		% Text Node
		\draw (150.21,324.23) node [anchor=north west][inner sep=0.75pt]    {$99\%$};
		% Text Node
		\draw (199.86,324.23) node [anchor=north west][inner sep=0.75pt]    {$10\%$};
		% Text Node
		\draw (49.67,361.9) node [anchor=north west][inner sep=0.75pt]    {$25\%$};
		% Text Node
		\draw (100.39,361.9) node [anchor=north west][inner sep=0.75pt]    {$90\%$};
		% Text Node
		\draw (154.21,361.9) node [anchor=north west][inner sep=0.75pt]    {$1\%$};
		% Text Node
		\draw (199.86,361.9) node [anchor=north west][inner sep=0.75pt]    {$90\%$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[Full Bayesian network analysis]{2nd level Bayesian network with second branch}
	\end{figure}

	Now to calculate the implicit probability (expectedness) that there is an alarm, we will have to consider the four possible situations. We then use again the theorem of total probability:
	
What a little more rigorously should be written:
	
The numerical application therefore provides for the implied probability (expectedness) of an alarm:
	
	What can be built and check as follows with MSBNX 1.4.2:
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.75]{img/arithmetics/msbnx_implicit_probability.eps}
	\caption[]{Implicit probability in MSBNX 1.4.2}
\end{figure}

	It may be useful to the reader to know that he can sometimes found in the literature the following notations:
	
	or:
	
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In the particular example studied above all event have only two states. But in practice they can have 3, 4 and more states. Therefore probabilities cross-tables quickly become enormous.
	\end{tcolorbox}

	As in the previous case, suppose we know that there was a working accident. We wish then calculate the a posteriori probability of an alarm. We then have (observe that the probability depends actually only to the state $S2$ state since the state $S1$ is completely known!):
	
	We can also check this with the software MSBNX 1.4.2:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/arithmetics/msbnx_implicit_probability_alarm.eps}
		\caption{Implicit probability of an alarm in MSBNX 1.4.2}
	\end{figure}

	To complete this example, we would calculate the a posteriori probabilities $P(S2/S3)$ and $P(S1/S3)$. To do this, we must first calculate the a priori probabilities  $P(S3/S2)$ and $P(S3/S1)$ (this last one has been calculated just before).

	We have for the missing value (which can be easily checked as before with MSBNX 1.4.2 software):
	
	We then have:
	
	We now have everything we need to calculate the a posteriori probability of $P(S2/S3)$ and $P(S1/S3)$:
	
	So the a posteriori probability that there is a machine breakdown when we know that there is an alarm is $0.979\%$ (i.e. $0.021\%$ that the trigger of the alarm is not a priori due to a machine failure). Respectively there is, a posteriori, $0.998\%$ probability that there is a work accident when we know there is an alarm (and then $0.002\%$ that it is not a priori due to a work accident).

	From the critical point of view, when there is finally an alarm we can not say a lot of things.... This is because, in this case, to the fact that the events of significant interest both have low probability to occur (work accident and machine failure) and that the employees respond quite well at the start of the alarm (otherwise if the a priori probabilities were high it would mean that the behaviour of the employees is not good because we can guess - with exasperation - in advance which problem occurs with a good confidence).

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	We did not find how to check these last calculations with MSBNX 1.4.2. If someone finds how to do, it would be great to give us the details of the process.
	\end{tcolorbox}	
	
	To conclude, the reader will have noticed that the calculations can quickly become annoying when the graph becomes complex and this explains the use of computer software. Furthermore, in the banking sector that uses for example Bayesian networks for credit risk, the a priori probability can be more complex. For example we might want to know the a priori probability that there is a machine failure knowing that we have an alarm and an accident:
	
	For people wishing to go deeper into the subject, with practical cases on \texttt{R} software and the use of Bayesian Gaussian networks, we strongly recommend reading \cite{scutari2014bayesian}!
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The Bayesian causal networks we have looked at so far do not show the passage of time. Yet time is essential to causality, insofar as effects must not precede their causes. Fortunately, it is easy to incorporate time into BNs by using successive time periods, named "time slices", to distinguish between the values of the same variables at different times or in different time periods. This creates a "\NewTerm{dynamic Bayesian network}" (DBN) that shows dependencies among the values of different random variables over time.
	\end{tcolorbox}	
	
	\subsection{Geometric Probabilities}
	In mathematics, Buffon's needle problem is a question first posed in the 118th century (holocene calendar) by Georges-Louis Leclerc, Comte de Buffon. Buffon's needle is very interesting because it is known as the earliest problem in "\NewTerm{geometric probability}\index{geometric probability}" to be solved using integral geometry.  Furthermore there are many similar cases of calculations in particle physics and astrophysics.

	The problem is stated as following: Suppose that you drop a short needle on ruled paper what is then the probability that the needle comes to lie in a position where it crosses one of the lines?

	The probability depends on the distance $d$ between the lines of the ruled paper, and it depends on the length $L$ of the needle that we drop - or rather it depends only on the ratio $L/d$. A short needle for our purpose is one of length $L\leq d$. In other words, a short needle is one that cannot cross two lines at the same time (and will come to touch two lines only with probability zero).
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,387); %set diagram left start at 0, and has height of 387
		
		%Straight Lines [id:da702494892222246] 
		\draw [line width=1.5]    (197,42) -- (427.5,42) ;
		%Straight Lines [id:da9326619902503188] 
		\draw [line width=1.5]    (197,72) -- (427.5,72) ;
		%Straight Lines [id:da57581950933806] 
		\draw [line width=1.5]    (197,105) -- (427.5,105) ;
		%Straight Lines [id:da5027703766739089] 
		\draw [line width=1.5]    (197,139) -- (427.5,139) ;
		%Straight Lines [id:da0238893534875384] 
		\draw [line width=1.5]    (197,171) -- (427.5,171) ;
		%Straight Lines [id:da17839436719011004] 
		\draw [line width=1.5]    (197,203) -- (427.5,203) ;
		%Straight Lines [id:da1349797118832008] 
		\draw [line width=1.5]    (197,236) -- (427.5,236) ;
		%Straight Lines [id:da8051394467609985] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (399,209.67) -- (399,230.67) ;
		%Straight Lines [id:da8571204096746852] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (266.5,199.67) -- (279.5,223.67) ;
		%Straight Lines [id:da036218482140149] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (227.5,187.67) -- (237.5,211.67) ;
		%Straight Lines [id:da13843804105209845] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (251.5,177.67) -- (280.5,179.67) ;
		%Straight Lines [id:da6428084390786433] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (304.5,168.67) -- (292.5,191.67) ;
		%Straight Lines [id:da15374095795823384] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (319.5,183.67) -- (346.5,187.67) ;
		%Straight Lines [id:da563638144486482] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (359.5,175.67) -- (354.5,195.67) ;
		%Straight Lines [id:da43039261518702543] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (357.5,165.67) -- (329.5,177.67) ;
		%Straight Lines [id:da7904096402204281] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (389.5,131.67) -- (365.5,147.67) ;
		%Straight Lines [id:da5616869687465209] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (247.5,149.67) -- (228.5,135.67) ;
		%Straight Lines [id:da8630910874710953] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (279.5,143.67) -- (253.5,146.67) ;
		%Straight Lines [id:da10206026514149413] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (289.5,122.67) -- (293.5,97.67) ;
		%Straight Lines [id:da23491765044759094] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (333.5,109.67) -- (305.5,99.67) ;
		%Straight Lines [id:da4262912759108146] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (357.5,127.67) -- (331.5,134.67) ;
		%Straight Lines [id:da7269570669067358] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (365.5,104.67) -- (349.5,121.67) ;
		%Straight Lines [id:da3567390233391008] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (374.5,87.67) -- (393.5,111.67) ;
		%Straight Lines [id:da343604667406215] 
		\draw    (405.5,76.67) -- (405.5,100.67) ;
		\draw [shift={(405.5,103.67)}, rotate = 270] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		\draw [shift={(405.5,73.67)}, rotate = 90] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da9429635562901326] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (268.5,89.67) -- (263.5,65.67) ;
		%Straight Lines [id:da8476798237584204] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (336.5,84.67) -- (322.5,64.67) ;
		%Straight Lines [id:da4376776868452559] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (385.5,65.67) -- (359.5,53.67) ;
		%Straight Lines [id:da4713605663644398] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (285.5,66.67) -- (296.5,45.67) ;
		%Straight Lines [id:da5277459841340251] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (223.5,55.67) -- (252.5,53.67) ;
		%Straight Lines [id:da7557667795669358] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (282.5,160.67) -- (359.5,159.67) ;
		%Straight Lines [id:da7822999940994502] 
		\draw    (282.5,160.67) -- (357.5,144.67) ;
		%Shape: Arc [id:dp8740083114051005] 
		\draw  [draw opacity=0] (327.11,150.5) .. controls (328.19,153.38) and (328.83,156.47) .. (328.97,159.69) -- (299,161) -- cycle ; \draw   (327.11,150.5) .. controls (328.19,153.38) and (328.83,156.47) .. (328.97,159.69) ;  
		
		% Text Node
		\draw (401,212.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle L$};
		% Text Node
		\draw (412,80.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle d$};
		% Text Node
		\draw (333,150) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \alpha $};
		\end{tikzpicture}
	\end{figure}
	The trick to get a solution is to consider the slope of the needle; let's say it drops to lie with an angle of $\alpha$ away from horizontal, where $\alpha$ will be in the range $0 \leq \alpha \leq \frac{\pi}{2}$ (we will ignore the case where the needle comes to lie with negative slope, since that case is symmetric to the case of positive slope, and produces the same probability). A needle that lies with angle $\alpha$ has height $L\sin (\alpha)$, and the probability that such a needle crosses one of the horizontal lines of distance $d$ is:
	
	Thus we get the probability by averaging over the possible angles $\alpha,$ as:
	
	For a long needle, we get the same probability $\frac{L \sin (\alpha)}{d}$ as long as $L \sin( \alpha) \leq d$ that is, in the range $0\leq \alpha \leq \arcsin (d/L)$. However, for larger angles $c$ the needle must cross a line, so the probability is 1. Hence we compute:
	
	for $L\leq d$.
	
	This last relation yields obviously $2/\pi$ for $L=d$. And it tends to $1$ for $L\rightarrow +\infty$.

	\subsection{Martingales}
	A martingale in probabilities (there is another one in stochastic processes) is a technique to increase the chances of winning in gambling while respecting the rules of the game. The principle is completely dependent on the type of game we are focusing, but this term is accompanied by an aura of mystery that some players would know efficient secret techniques to cheat with chance. For example, many players (or candidates to play) search THE martingale that will beat the bank in the most common games in casinos (institutions whose profitability relies almost entirely on the difference - however small - between the chances of winning and losing as we will see in details further below).
	
	Many martingales are the dream of their author, some are actually inapplicable, some could actually give the possibility to cheat a little. Gambling in general are unfair: whatever the shot played, the probability of winning of the casino (or of the State in the case of a lottery) is greater than this of the player. In this type of game, it is not possible to inverse the chances, just to minimize the probability of gambler's ruin.
	
	The most common example is the roulette wheels martingale. It consists to play a single chance to the roulette wheels (red or black, odd or even) to win, for example, a unit in a series of moves by doubling his bet if we lose, and that until we earn. Example: the player bets 1 unit on red, if red comes out, it stops playing and won $1$ unit ($2$ units less gain setting unit), if black comes out, he doubles his betting by $2$ units on red and so on until he wins.
	
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/casino_roulette.eps}
		\caption[]{Casino roulette wheel}
	\end{figure}
	
	Having a chance on two to win, he may think he will eventually win, and when he wins, he is necessarily paid for everything he has played more one unit of his initial bet.
	
	This martingale appears to be safe in practice. Note that in theory, to be sure of winning, we should have the opportunity to play an unlimited number of times.... This has major drawbacks:
	
	This martingale is in fact limited by the bets that the player can do because you have to double the bet every time you lose: $2$ times the initial bet, then $4$, $8$, $16$ .... if he loses $10$ times, he must be able to bet $1024$ times its initial investment for the 11th party! Therefore a lot of money for little gain!
	
	The roulette wheels also have a "$0$" which is neither red nor black. The risk of losing at every shot is then larger than $1/2$...
	
	In addition, to paralyse this strategy, casinos offer table games per set: from $1$ to $100.-$, from $2$ to $200.-$, from $5$ to $500.-$, ... Therefore it is impossible to use this method on a large number of shots, which increases the risk of losing it all.
	
	Blackjack is a game that has winning strategies: several playing techniques, which usually require to memorize the cards, can overturn the chances in favour of the player. The mathematician Edward Thorp has published in 11962 (holocene calendar) a book that was at the time a real best-seller. But all these methods require long training weeks and are easily detectable by the croupier (sudden changes in the amounts of bets are typical). The casino has then the opportunity to banish from its establishment the players using this playing martingale.
	
	It should be noted that there are enough advanced methods. One of them is based on the less played combinations. In games where the gain depends on the number of winning players (Lotto...), playing the least played combinations maximize gains. This is how some people sell combinations that would be statistically very rarely used by other players.
	
	Based on this reasoning, we can still conclude that a player who would have been able to determine statistically the least played combinations, to maximize its expected pay-off, will in fact certainly not be the only player to have achieved this by the analysis of these famous combinations! This means that in theory the numbers the least played are actually overplayed combinations, the best might be to achieve a mix of played numbers and overplayed numbers to play for the ideal combinations. Another conclusion to all this is maybe that the best is to play random combinations which ultimately are less likely to be chosen by the players who incorporate a human and harmonious factor in the choice of their numbers.
	
	\subsubsection{Gambling}
	At its core the business of casino gaming is pretty simple. Casinos make money on their games because of the mathematics behind the games. As Nico Zographos, dealer-extraordinaire for the 'Greek Syndicate' in Deauville, Cannes, and Monte Carlo in the 11920s (holocene calendar) observed about casino gaming: «\textit{There is no such thing as luck. It is all mathematics.}»
	
	With a few notable exceptions, the house always wins \underline{in the long run}\footnote{Without this underlined term, the sentence makes no sense. The devil is in the details!} because of the mathematical advantage the casino enjoys over the player. 
	
	A common question about gampling is: How does house always win in gambling, even if the chances are $50/50$?
	
	The answer relies in the fact that when a player lose the hand, they take his bet. If you win the hand, they only give you back your bet plus $95\%$ of his bet. In order to calculate winnings we need to understand the concept of expected value.
	
	In short, expected value is the expected payout from a particular probabilistic event. In simple case, we can calculate it by summing all the possible probabilities $p_i$ multiplied by their payouts $\text{Pay-out}_i$. In this example if the player bets \$$1$ every game, he will gain \$$0.95$ when he wins and lost \$$1$ when he lose. Ignoring the possibility of a tie this comes out to!
	
	The expected payout value is therefore:
	
	This means that on any given hand, the player will expect to lose about $2$ or $3$ cents. Really, he is actually gaining \$$0.95$ or losing \$1 but on average if he plays forever, it will come out to \$$-0.025$ per hand and he will slowly lose money. Even though the chances of  winning any given hand are $50/50$.
	
	In other words, by the law of large numbers (see further below page \pageref{weak law of large numbers}) says that as the number of bets placed goes to infinity, the average profit or loss realized by a player per bet will converge to the expected value.  Since the expected value is negative, that means that the \underline{infinitely busy} gambler will lose infinitely much money (or more realistically, go bankrupt). Or from the another point of view, the house always wins because the casino makes a LARGE number of bets.
	
	The house advantage - the all-important percentage that explains how casinos make money - is also named the "house edge", the "theoretical win percentage", and "expected win percentage". In double-zero roulette, this figure is $5.3\%$. In the long run the house will retain $5.3\%$ of the money wagered. In the short term, of course, the actual win percentage will differ from the theoretical win percentage (the magnitude of this deviation can be predicted from statistical theory). The actual win percentage is just the (actual) win divided by the handle. Because of the law of large numbers - or as some prefer to name it, the "law of averages" - as the number of trials gets larger, the actual win percentage should get closer to the theoretical win percentage.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In an effort to entice players and increase business, casinos occasionally offer novel wagers, side bets, increased pay-offs, or rule variations. These promotions have the effect of lowering the house advantage and the effective price of the game for the player. This is sound reasoning from a marketing standpoint, but can be disastrous for the casino if care is not taken to ensure the math behind the promotion is sound (the are some famous examples of such disasters with a Illinois riverboat Casino, also an Indian Casino in California and some Casinos in Las Vegas, etc.). 
	\end{tcolorbox}
	
	The two tables below show the house advantages for many of the popular casino games:
	\begin{table}[H]
	\centering
	\begin{tabular}{|l|c|}
	\hline
	\rowcolor[gray]{0.75} 
	\textbf{\begin{tabular}[c]{@{}l@{}}Popular Casino Games \\ (under optimal strategies)\end{tabular}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{House Advantage}} \\ \hline
	Roulette (double-zero) & $5.3\%$ \\ \hline
	Craps (pass/come) & $1.4\%$ \\ \hline
	Craps (pass/come with double odds) & $0.6\%$ \\ \hline
	Blackjack - average player & $2.0\%$ \\ \hline
	Blackjack - 6 decks, basic strategy & $0.5\%$ \\ \hline
	Blackjack - single deck, basic strategy & $0.0\%$ \\ \hline
	Baccarat (no tie bets) & $1.2\%$ \\ \hline
	Caribbean Stud & $5.2\%$ \\ \hline
	Let It Ride & $3.5\%$ \\ \hline
	Three Card Poker & $3.4\%$ \\ \hline
	Pai Gow Poker (ante/play) & $2.5\%$ \\ \hline
	Slots & $5-10\%$ \\ \hline
	Video Poker & $0.5-3\%$ \\ \hline
	Keno (average) & $27\%$ \\ \hline
	\end{tabular}
	\end{table}
	Let us prove for example where the value $5.3\%$ for the Roulette above comes from!
	
	The house edge is the percentage of each bet that the casino expects to win over the long run. The easiest way to calculate this number is to assume that someone is playing for \$100 a bet, and determining how much he'll lose on average over the long run, then divide that into the number of bets.

	I'll use roulette as an example again, because it's illustrative.
	
	You place 38 bets on a single number at the roulette table. If you had a mathematically perfect set of spins (which is what you want to assume when you're calculating the house edge), you'll win once and lose 37 times.
	
	The winning bet pays off at $35$ to $1$, so you'll win \$$3500$ on it. But you'll lose \$$3700$ on the $37$ losing spins. Your net loss is \$$3700 -$ \$$3500$, or \$$200$.
	
	If you divide \$$200$ by $38$, you get \$$5.26$ per spin, which is your average loss per spin.
	
	That is the house edge for the game $-5.26\%$.

	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Caution!!!!!!!!! It's important to remember that the house edge is a long term mathematical expectation. In the short term, anything can (and often will) happen.
	\end{tcolorbox}
	We will stop here about the subject of gambling because it's not the most ethical business and there are already plenty of graduate level textbooks dedicated to that topic.

	\subsection{Combinatorial Analysis}\label{combinatorial analysis}
	
	"\NewTerm{Combinatorial analysis}\index{combinatorial analysis}" (counting techniques) is the field of mathematics that deals with the study of all the issues, events or facts (distinguishable or indistinguishable) with their arrangements (combinations) ordered or not according to some given constraints.
	
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] A sequence of objects (events, issues, objects, ...) is said "\NewTerm{ordered}\index{ordered sequence}" if each suite with a particular order of objects is recognized as a particular configuration.
		
		\item[D2.] A sequel is "\NewTerm{unordered}\index{unordered sequence}" if and only if we are interested in the frequency of appearance of objects regardless of their order.
		
		\item[D3.] The objects (of a sequence) are said "\NewTerm{distincts}\index{distincts objects}" if their characteristics can not to be confused with the other objects.
	\end{enumerate}

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	We chose to put combinatorial analysis in this chapter because when we calculate probabilities, we also often need to know what is the probability of finding a combination or arrangement of given events under given constraints. Also we have decided (as always!) to focus on combinatorial mathematics that have important practical applications in others sections of this book!
	\end{tcolorbox}

Students often have difficulty remembering the difference between a permutation, an arrangement and a combination. Here is a little summary of what we'll see:
	\begin{itemize}
	 	\item \NewTerm{Permutation}\index{permutation}: We take all the objects.

	 	\item \NewTerm{Arrangement}\index{arrangement}: We choose objects from the original set and the order intervenes.

	 	\item \NewTerm{Combination}\index{combinatorial}: Same as for the arrangement, but the order does not intervenes.	
	\end{itemize}

You must not forget that for each result, the reverse will give the probability of falling respectively on a given permutation/arrangement/combination!

We will present and demonstrate below the 6 most common cases from which we can find (usually) all others:

\subsubsection{Simple Arrangements with Repetitions}\label{simple arrangements with repetitions}

\textbf{Definition (\#\thesection.\mydef):} A "\NewTerm{simple arrangement with repetition}\index{simple arrangement with repetition}" or equivalently a "\NewTerm{random sampling with replacement}"\index{random sampling with replacement}" (RSWR) is an ordered sequence of length $m$ of $n$ distinct objects not necessarily all different in the sequence (either: with possible repetitions!).

Let $A$ and $B$ be two finite sets of respective cardinal $m, n$ such that there is trivially $m$ ways to choose an object in $A$ (of type $a$) and $n$ ways to choose an object in $B$ (of type $b$).

We saw in the section Set Theory (page \pageref{cardinal}) that if $A$ and $B$ are disjoint, that:
	

We therefore deduce the following properties:
	\begin{enumerate}
		\item[P1.] If an object can not be at the same time of type $a$ and type $b$ and if there is $m$ ways to select an object of type $a$ and $n$ ways to choose an object of type $b$, then the union of objects gives $m+n$ selections (this is typically the result of the SQL UNION queries without filters in corporate Relational Databases Management System).
		
		\item[P2.] If we can choose an object type of type $a$ in $m$ ways then an object of type $b$ in $n$ ways, then there is according to the Cartesian product of two sets (\SeeChapter{see section Set Theory page \pageref{cartesian product}}):
		
ways to choose a single object of type $a$ then an object of type $b$.
	\end{enumerate}
With the same notation for $m$ and $n$, we can choose for each element of $A$, its single image among the $n$ elements of $B$. So there are $n$ ways to choose the image of the first element of $A$, then also $n$ ways to choose the image of the second element of $A$, ..., and $n$ ways to choose the image of the $m$-th element of $A$. The total number of consecutive possible applications from $A$ to $B$ is thus equal to the $m$ product of $n$ (thus $m$ times the cartesian product of the cardinality of the set $B$ with itself!). It is usual to write it under the following way (we have indicated the different ways to write his result as it can be found in various textbooks):
	
where $B^A$ is the set of applications from $A$ to $B$. The increase in the number of possibilities is geometric (not "exponential" as it is often wrongly said!).

This result is mathematically similar to the ordered result (an arrangement where the order of elements in the sequence is taken into account) of $m$ trials in a bag containing $n$ different balls with replacement after each trial. In France this result is traditionally named a "\NewTerm{$p$-list}\index{$p$-list}".

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. How many (ordered) "words" of 7 letters can we form from a separate alphabet of 24 letters (very useful to know the number of trials to find a password for example)? The solution is:
	
	Such a huge number is not very convenient to indicate the strength of a password. That's why IT professional prefer to use a measure based on entropy as we will see in the section of Statistical Mechanics (see page \pageref{information entropy}).\\
	
	E2. How many groups of people will we have in a referendum on 5 subjects and where each can be either accepted or rejected? The solution is (widely used in some Swiss companies):
	
	\end{tcolorbox}

	
A simple generalization of this result can consist of the following problem statement:

If we have $m$ such objects $k_1,k_2,\ldots,k_m$ as $k_i$ may take $n_i$ different values then the number of possible combinations is:
	
	And if $n_1=n_2=\ldots=n_m$ then we fall back on:
	
	
	\subsubsection{Simple Permutations without Repetitions}
	\textbf{Definition (\#\thesection.\mydef):} A "\NewTerm{simple permutation without repetitions}\index{simple permutation without repetition}\label{simple permutation without repetitions}" (formerly named "substitution") of $n$ distinct objects is an ordered (different) sequence of these $n$ objects all different by definition in the sequence (without repetition).

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Be careful not to confuse the concept of permutation ($n$ elements between them) and this of arrangement (of $n$ elements among $m$)!
	\end{tcolorbox}	

	The number of permutations of $n$ items can be calculated by induction: there are $n$ places for a first element, $n-1$ for a second element, ..., and there will be only one place for the last remaining element.

	It is therefore trivial that we the number of permutations is given by:
	
	Recall that the product:
	
is named the "\NewTerm{factorial of $n$}\index{factorial}" and we note it $n!$ for $n \in \mathbb{N}$.

There is therefore for $n$ distinguishable elements\label{distinguishable elements}:
	
as possible permutations. This type of calculation can be useful for example in project management (calculation of the number of different ways to get in a production line $n$ different parts ordered from external suppliers).	

\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\textbf{{\Large \ding{45}}Example:}\\\\
How many (ordered) "words" of 7 different letters without repetition can we create?
	
This result leads us to assimilate it to the ordered results (an arrangement $A_n$ in which the order of elements in the sequence is taken into account) of the trial of balls that are all different from a bag containing $n$ distinguishable balls without replacement.
	\end{tcolorbox}
	The huge values that can bring the factorial may be quite surprising or even sometimes counter-intuitive (consider for example the number of ways you can create groups of just say...7 friends!). By the way it also question if the Universe is computable or not as such calculations leads to huge numbers!

\subsubsection{Simple Permutations with Repetitions}

	\textbf{Definition (\#\thesection.\mydef):} A "\NewTerm{simple permutation with repetition}\index{simple permutation with repetition}" is when we consider the number of ordered permutations (different) of a sequence of $n$ distinct objects not necessarily all different in a given quantity.

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Do not confuse this definition with the "simple arrangement with repetition" seen before!
	\end{tcolorbox}	
	
	When some elements are not all distinguishable in a sequence of objects (they are repeated in the sequence), then the number of permutations that we can be do are then trivially reduced to a smaller number then if all the elements were all distinguishable.

	Consider $n_i$ as the number of objects of the type $i$, with:	
	
	then we write:
	
	the number of possible permutations (yet unknown) with repetition (one or more elements in a sequence of repetitive elements are not distinguishable by permutation).

	If each of the $n_i$ positions occupied by identical elements were occupied by different elements, the number of permutations could then have to be multiplied by each of the $n_i!$ (previous case).

	It comes that we fall back on the factorial such as:
	
	from what we deduce immediately:
	
	If the $n$ objects are all different in the sequence, we then have:
	
and we fall back again on a simple permutation (without repetition) as:
	
	It should therefore be remembered that the permutations with repetition are in smaller quantity than those without repetition (obvious since we do not take into account the permutations of identical elements between them!).

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\textbf{{\Large \ding{45}}Example:}\\\\
How many (ordered) "words" can we create with the letters of the word "Mississippi":
	
	\end{tcolorbox}
This result leads us to assimilate it to an ordered result (a permutation $\bar{A}_n$  where the order of elements in the sequence is not taken into account) of the trial of $n$ balls that are not all different from a bag containing $k \geq n$ balls with limited replacement for each ball.

\subsubsection{Simple Arrangements without Repetitions}\label{simple arrangements without repetitions}

\textbf{Definition (\#\thesection.\mydef):} A "\NewTerm{simple arrangement without repetition}\index{simple arrangement without repetition}" or equivalently a "\NewTerm{random sampling without replacement}\index{random sampling without replacement}" (RSWOR) is an ordered sequence of $p$ objects all distinct taken from $n$ distinct objects with $n \geq p$.

We now propose to enumerate the possible arrangements of $n$ objects among $p$ without repetition. We denote $A_n^p$ the number of these arrangements.

It is easy to calculate that $A_n^1=n$ and to check that $A_n^2=n(n-1)$. Indeed, there are $n$ ways to choose the first object and $(n-1)$ ways to choose the second when we already have the first.

To determine a nice expression for $A_n^p$ , we reason by induction. We assume $A_n^{p-1}$ known and we deduce that:
	
It comes: 
	
then:
	 
whence:
	
This result leads us to assimilate it to the ordered results (an arrangement $A_n^p$ in which the order of elements in the sequence is taken into account) of the trial of $p$ distinct balls from a bag containing $n$ different balls without replacement.


	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Consider the $24$ letters of the alphabet, how many (ordered) "words" of $7$ distinct letters can we create?
	
	\end{tcolorbox}
The reader may have noticed that if $p=n$ we end up with:
	
So we can say that a simple permutation of $n$ elements without repetition is like a simple arrangement without repetition when $n=p$.

\subsubsection{Simple Combinations without Repetitions}\label{simple combinations without repetitions}

\textbf{Definition (\#\thesection.\mydef):} A "\NewTerm{simple combination without repetitions}\index{simple combination without repetitions}" or "\NewTerm{choice function}\index{choice function}\label{choice function}" is a non-ordered sequence (where the order doesn't interest us!) of $p$ elements all different (not necessarily in the visual sense of the word!) selected from $n$ distinct objects and is by definition denoted $C_p^n$ in this book and named the "\NewTerm{binomial}\index{binomial}" or "\NewTerm{binomial coefficient}\index{binomial coefficient}\label{binomial coefficient}".

If we permute the elements of each simple arrangement of elements $p$ of $n$, we get all simple permutations and we know that there are in a number of $p!$, using the notation convention of this book we then have (contrary to that recommended one by ISO 31-11!):
	
	It is a relation often used in gambling but also in the industry trough the hypergeometric distribution (\SeeChapter{see section Industrial Engineering page \pageref{quality control hypergeometric}}).

	A simple way to remember this function is the following trick: Consider we must select $p$ ($3$ individuals) among $n$ ($6$ individuals) independently of the order what are the number of possibilities? 
	
	We know that we have $6\cdot 5\cdot 4=120$ possibilities to select them all taking into account the order! The calculation we just made is obviously equal to $n!/p!=6!/3!= 6\cdot 5\cdot 4$. But as the order must not be taken into account we must divide the $120$ by the number of ways we can arrange the $3$ people in the group. So we divide $120$ by $3!$ or more generally and logically by $(n-p)!$. Hence the relation above!
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} We have necessarily by construction $C_p^n \leq A_n^p$.\\

	\textbf{R2.} Depending on the authors we inverse the index and suffix of $C$ then you must be careful!
	\end{tcolorbox}
This result leads us to assimilate it to the unordered result (an arrangement $C_n^p$ in which the order of elements of the sequence is not taken into account) of the trial of $p$ balls of a bag containing $n$ different balls without replacement.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. Consider the $24$ letters of the alphabet, how many choices do we have to take $7$ letters in the $24$ without taking into account the trial order?
		
	The same value can be obtained with the function $\texttt{COMBIN( )}$ of Microsoft Excel 11.8346 (English version).\\
	
	E2. In a Design of Experiment (\SeeChapter{see section Industrial Engineering page \pageref{doe}}) we have $2$ factors of $L=3$ levels each and therefore we need $N=9$ runs to completely determine all the interactions. If we consider that we can take a subset of $S=3$ runs, how many combinations of $3$ among the $9$ can we choose if repetitions are forbidden?
	
	We understand therefore why in Design of Experiments it is important to find a trick to choose the best subset (D-optimum designs).
	\end{tcolorbox}
	There is, in relation to the binomial coefficient, another relation very often used in many case studies and also more globally in physics or functional analysis. This is the "\NewTerm{Pascal's Formula}\index{Pascal's Formula}\label{pascal formula}":
	\begin{dem}
		
We also have $p!=p(p-1)!$, then:
	
and because $(n-p)(n-p-1)!=(n-p)!$:
	
Then:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[my rule/.style={line width=\myrulewidth},
                    my outline color/.initial=blue!75!black,
                    my text/.style={
                      color/.expanded={\pgfkeysvalueof{/tikz/my outline color}}},
                    my arrow/.style={
                      ->, line width=0.6pt,
                      draw/.expanded={\pgfkeysvalueof{/tikz/my outline color}}},
                    my highlight/.style={
                      draw/.expanded={\pgfkeysvalueof{/tikz/my outline color}},
                      fill=blue!15, line width=0.6pt, rounded corners=2pt}]

		\newlength{\myrulewidth}
		\setlength{\myrulewidth}{0.4pt} % c'est la valeur par défaut de \pgflinewidth
		\newlength{\myCellSize}
		\setlength{\myCellSize}{0.7cm}
		
		\matrix (mat) [
		  my rule, draw, inner sep=0, matrix of math nodes,
		  nodes={minimum width=\myCellSize,
		         text height=0.6\myCellSize, text depth=0.4\myCellSize}]
		  {
		    \raisebox{-0.8ex}{$n$}\kern 0.3em\raisebox{0.7ex}{$p$}
		           & 0      & 1 & 2 & 3 & 4 & 5   &[-2pt] \cdots \\
		    0      & 1                                           \\
		    1      & 1      & 1                                  \\
		    2      & 1      & 2 & 1                              \\
		    3      & 1      & 3 & 3  & 1                         \\
		    4      & 1      & 4 & 6  & 4  & 1                    \\
		    5      & 1      & 5 & 10 & 10 & 5 & 1                \\[-2pt]
		    \vdots & \vdots &   &    &    &   &   &       \ddots \\
		  };
		
		\begin{scope}[my rule]
		\draw (mat-1-1.north east) -- (mat-8-1.south east);
		\draw (mat-1-1.south west) -- (mat-1-8.south east);
		\draw ([shift={(0.5\myrulewidth, -0.5\myrulewidth)}]mat-1-1.north west) --
		      (mat-1-1.south east);
		\end{scope}
		
		\path[my text] node [right=1cm of mat, label=above:{Pascal's formula}]
		  (formula)
		  {%
		    $\begin{aligned}
		      \binom{p}{n} &\mathrel{+} \binom{p}{n+1} \\
		                   &=           \binom{p+1}{n+1}
		    \end{aligned}$%
		  };
		
		\begin{scope}[on background layer]
		  \path[name path=p, my highlight]
		    (mat-5-3.north west) -| coordinate (A)
		    (mat-6-4.south east) -|
		    (mat-5-3.south east) --
		    (mat-5-3.south west)
		    -- cycle;
		
		  \pgfmathsetlengthmacro{\myWidth}{width("$\displaystyle \binom{p}{n}$")}
		  \pgfmathsetlengthmacro{\myTotalHeight}{
		    height("$\displaystyle \binom{p}{n}$") +
		    depth("$\displaystyle \binom{p}{n}$")}
		  \pgfmathsetlengthmacro{\myXshift}{\pgfkeysvalueof{/pgf/inner xsep} + \myWidth}
		  \pgfmathsetlengthmacro{\myYshift}{\myTotalHeight +
		                                    2*\pgfkeysvalueof{/pgf/inner ysep}}
		
		  \path[my highlight]
		    (formula.north west) -|
		    (formula.south east) -|
		    ([shift={(\myXshift,-\myYshift)}]formula.north west) coordinate (B) --
		    (formula.north west |- B) --
		    cycle;
		  % Pour que la flèche parte vraiment de la bordure arrondie de p, il faut
		  % ruser un peu. D'abord, il nous faut le point de départ :
		  \path[name path=s] ([shift={(-10pt,-10pt)}]A) -- (A);
		  \path[name intersections={of=p and s}];
		\end{scope}
		
		% Ensuite, on peut tracer la flèche.
		\path[my arrow] (intersection-1) to[bend left=10]
		                ($(formula.north west)!0.5!(formula.west)$);
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Pascal's formula illustrated}
	\end{figure}
	
	\subsubsection{Simple Combinations with Repetitions}
	
	\textbf{Definition (\#\thesection.\mydef):} A "\NewTerm{simple combination with repetition}\index{simple combination with repetition}" of $p$ elements of $n$ is a collection of $p$ non-ordered elements, not necessarily distinct.

	Simple combinations with repetition are very important for the Wald-Wolfowitz statistical test used in economics and biology and that we will study in the Statistics section.

	We will introduce this kind of combination directly with an example an ingenious approach that we have thanks to the physicist and 11938 (holocene calendar) Nobel Prize in Physics: Enrico Fermi.

	Consider $\left\lbrace a, b, c, d, e, f\right\rbrace $ a set having a number $n$ of elements equal to 6 and where we draw a number of elements $p$ equal to 8. We would like to calculate the number of combinations with repetition of elements in a starting set of cardinal 6 in a destination set of cardinal 8.

	Consider, for example, the following three combinations:
	
	where as the order of elements does not occur, we have grouped the elements to facilitate the reading. Now represent all the above elements by the same symbol "0" and separate the groups consisting of a single element by bars (this is the trick Enrico Fermi). Thus, when one or more elements are not included in a combination, we still denote the separation bars (corresponding to the number of missing elements + the separation of group). Thus, the three combinations above can be written as:
	
	We see above that in each case, there are eight "0" (logic. ..) but also that there are also always five "$\mid$". The number of combinations with repetitions of six elements of a starting set to the final one of 8 elements is equal to the number of permutations with repetitions of $8+5=13$ elements, so:
	
	We also see that in the general case the number of combinations without consideration of repetitions order can also be written:
	
It is traditional to write this:
	
We also see that:
	
Then:
	
That we also sometimes write:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	How many different combinations of outcomes can we make by rolling three standard ($6$-sided) dice if the order of the dice does not matter?\\
	
	The answer is:
	
	But if the order matter, then it was simply $6\cdot 6\cdot 6=216$.
	\end{tcolorbox}
	To summarize:
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|p{8cm}|p{6cm}|}
			\hline
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Type}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Expression}} \\ \hline
			Simple arrangement with repetitions (denoted $^RV_n^m$ according to ISO 80000-2:2009) & \centering\arraybackslash\ $\bar{A}_n^m=n^m$ \\ \hline
			Simple arrangement without repetitions (denoted $V_n^m$ according to ISO 80000-2:2009) & \centering\arraybackslash\ $A_m^n=\dfrac{n!}{(n-m)!}$  \\ \hline
			Simple permutation without repetitions & \centering\arraybackslash\ $A_n=n!$  \\ \hline
			Simple permutation with repetitions & \centering\arraybackslash\ $\bar{A}_n(n_1,n_2,\ldots ,n_k)=\dfrac{n!}{n_1!n_2!\ldots n_k!}$  \\ \hline
			Simple combination without repetitions: case of the simple arrangement without repetitions where the order is not taken into account (denoted $^RC_n^p$ according to ISO 80000-2:2009) & \centering\arraybackslash\ $C_m^n=\begin{pmatrix}n\\p\end{pmatrix}=\dfrac{A_n^m}{m!}=\dfrac{n!}{m!(n-m)!}$  \\ \hline
			Simple combination with repetitions
(case of the simple permutation with repetition where the order is not taken into account) & \centering\arraybackslash\ $\Gamma_p^n=C_p^{n+p-1}=\dfrac{(n+p-1)!}{(n-1)!p!}$  \\ \hline
		\end{tabular}
		\caption{Resume of main Combinatorial Analysis cases}
	\end{table}
	Four of the relations above can also be presented as following:
	\begin{table}[H]
		\centering
		\begin{tabular}{|
		>{\columncolor[gray]{0.75}}l |c|c|}
		\hline
		 & \cellcolor[gray]{0.75}\textbf{Order Matters} & \cellcolor[gray]{0.75}\textbf{Order doesn't Matters} \\ \hline
		\textbf{With Replacement} & $\bar{A}_n^m=n^m$ & $\Gamma_p^n=C_p^{n+p-1}=\dfrac{(n+p-1)!}{(n-1)!p!}$  \\ \hline
		\textbf{Without Replacement} &  $A_m^n=\dfrac{n!}{(n-m)!}$ & $C_m^n=\begin{pmatrix}n\\p\end{pmatrix}=\dfrac{A_n^m}{m!}=\dfrac{n!}{m!(n-m)!}$ \\ \hline
		\end{tabular}
	\end{table}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	If we consider three elements $A$, $B$ and $C$ the total cumulated number of combinations where the order matter is quite obviously (this is straightforward from the Binomial theorem proved at page \pageref{binomial theorem}, where we put $a=1$ and $b=1$):
	
	But if the order doesn't matter, we have:
	
	\end{tcolorbox}	
	We also have the number of ways $n$ different items can be arranged into $n$ bags with exactly $n$ objects each (we can reuse any objective, however no repetition is allowed inside the bags), that is given by:
	
	and the number of ways in which $m\cdot n$ different items can be divided equally into $m$ groups each containing $n$ objects and where order of the groups matters that is given by:
	
	and the number of ways in which $m\cdot n$ different items can be divided equally into $m$ groups, each containing $n$ objects and where the order of the groups is doesn't matter is:
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/arithmetics/gaming.eps}
	\end{figure}
	To close this study on the basics of combinatorial analysis, there would be another important case but that seems unsolved still in this beginning of the 121st century (holocene calendar) and related to chemistry (some experts speak about "\NewTerm{combinatorial chemistry}"). Indeed, given a number of pure chemical elements with their respective valence in some thermodynamics conditions, how many molecules involving one of all of them can be created?
	
	For example, with the Adenine (one of the four fundamentals components of DNA) given by $\mathrm{C}_5\mathrm{N}_5\mathrm{H}_5$ and visible in the figure below:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/arithmetics/acgt_dna.jpg}
	\end{figure}
	We then could ask ourselves what is total number of combinations involving $5\mathrm{C}$, $5\mathrm{N}$ and $5\mathrm{H}$?

	An upper bound to this question would be given (based on their respective valence by):
	
	But if we draw all physical possible combination in standard temperature and pressure conditions, it seems that an upper limit is rather between $200$ and $2,000$ different molecules.
	
	So this is still and important "open problem" in chemistry with huge application possibilities (some industries actually use computers to find feasible combinations of pure chemical elements that may have applications in medicine!).

	\pagebreak
	\subsection{Markov Chains}\label{markov chains}

Markov chains are simple but powerful probabilistic and statistical tools but for which the choice of the mathematical presentation can sometimes be a nightmare... We will try here to simplify a maximum the writings to introduce this great tool widely used in businesses to manage supply chain, in queuing theory for call centers or cash desk, in failure theory for preventive maintenance, statistical physics and biological engineering and also in time series analysis and forecasting (and the list goes on and for more details the reader should refer to the relevant chapters available in this book...).

\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] We note by $\left\lbrace X(t) \right\rbrace_{t \in T} $  a probabilistic process function of time whose value at any time depends on the outcome of a random experiment. Thus, at each time $t, X(t)$ is a random variable that we name "\NewTerm{stochastic process}\index{stochastic process}" (for more details on financial applications see the chapter Economy).

		\item[D2.] If we consider a discrete time, we then note  "\NewTerm{discrete time stochastic process}\index{discrete time stochastic process}" as $\left\lbrace X_n \right\rbrace_{n \in \mathbb{N}} $.

		\item[D3.] If we further assume that the random variables $X_n$ can take only a discrete set of values we then speak of "\NewTerm{process in discrete time and discrete space}\index{process in discrete time and discrete space}".
	\end{enumerate}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	It is quite possible as in the queuing theory (\SeeChapter{see section Management Techniques page \pageref{queueing theory}}) to have a continuous time process with discrete state space.
	\end{tcolorbox}
	\textbf{Definition (\#\thesection.\mydef):} $\left\lbrace X_n \right\rbrace_{n \in \mathbb{N}} $ is a "\NewTerm{Markov chain}\index{Markov chain}" if and only if:
	
	
in other words (it is very easy!) the probability that the chain is in a certain state on the $n$-th step of the process depends only on the state of the process at step $n-1$ and not on any previous steps!

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
Also in probabilities a stochastic process verifies the Markov property if and only if the conditional probability distribution of future states, given the present moment, depends only on the present state and not even past states as the relation above. A process with this property is also named a "\NewTerm{Markov process}\index{Markov process}".
	\end{tcolorbox}
\textbf{Definition (\#\thesection.\mydef):} A "\NewTerm{homogeneous Markov chain}\index{homogenous Markov chain}" is a chain such that the probability that it has to go in a certain state at the $n$-th stage is independent of time. In other words, the probability distribution characterizing the next step does not depend on time (of the previous step), at all times the probability distribution of the chain is always the same for characterizing the transition to the current step.

We can then define (reduce) the law of "\NewTerm{probability transition}\index{probability transition}" of a state $i$ to state $j$ by:
	
It is then natural to define the "\NewTerm{transition matrix}\index{transition matrix}\label{transition matrix}" or "\NewTerm{stochastic matrix}\index{stochastic matrix}":
	
as the matrix that contains all possible transition probabilities between states in an oriented graph.

Markov chains can be represented graphically as an oriented graph $G$ (\SeeChapter{see section Graph Theory page \pageref{oriented graph}}) sometimes named "\NewTerm{automata}\index{automata}" having for the top points (states) $i$ and for the edges the oriented couples $(i, j)$. We then associate to each component an oriented arc and a transition probability.

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,387); %set diagram left start at 0, and has height of 387
		
		%Shape: Ellipse [id:dp8274922044746049] 
		\draw   (100,156.83) .. controls (100,139.8) and (122.5,126) .. (150.25,126) .. controls (178,126) and (200.5,139.8) .. (200.5,156.83) .. controls (200.5,173.86) and (178,187.67) .. (150.25,187.67) .. controls (122.5,187.67) and (100,173.86) .. (100,156.83) -- cycle ;
		%Shape: Ellipse [id:dp6913540283102424] 
		\draw   (235,61.83) .. controls (235,44.8) and (257.5,31) .. (285.25,31) .. controls (313,31) and (335.5,44.8) .. (335.5,61.83) .. controls (335.5,78.86) and (313,92.67) .. (285.25,92.67) .. controls (257.5,92.67) and (235,78.86) .. (235,61.83) -- cycle ;
		%Shape: Ellipse [id:dp7011720404807722] 
		\draw   (309,179.83) .. controls (309,162.8) and (331.5,149) .. (359.25,149) .. controls (387,149) and (409.5,162.8) .. (409.5,179.83) .. controls (409.5,196.86) and (387,210.67) .. (359.25,210.67) .. controls (331.5,210.67) and (309,196.86) .. (309,179.83) -- cycle ;
		%Shape: Ellipse [id:dp6076040890154173] 
		\draw   (195,278.83) .. controls (195,261.8) and (217.5,248) .. (245.25,248) .. controls (273,248) and (295.5,261.8) .. (295.5,278.83) .. controls (295.5,295.86) and (273,309.67) .. (245.25,309.67) .. controls (217.5,309.67) and (195,295.86) .. (195,278.83) -- cycle ;
		%Curve Lines [id:da38166658522637653] 
		\draw    (150.25,126) .. controls (125.62,48.06) and (165.1,41.73) .. (233.96,61.53) ;
		\draw [shift={(235,61.83)}, rotate = 196.18] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Curve Lines [id:da3572639187769009] 
		\draw    (335.5,61.83) .. controls (369.16,47.81) and (382.24,106.47) .. (359.94,147.75) ;
		\draw [shift={(359.25,149)}, rotate = 299.36] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Curve Lines [id:da9067721504500816] 
		\draw    (407.5,165.67) .. controls (457.99,165.67) and (445.75,240.15) .. (395.05,203.8) ;
		\draw [shift={(393.5,202.67)}, rotate = 36.87] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Curve Lines [id:da009290186164455871] 
		\draw    (309,179.83) .. controls (280.09,126.75) and (230.53,144.9) .. (202.21,156.15) ;
		\draw [shift={(200.5,156.83)}, rotate = 338.26] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Curve Lines [id:da05966341501035988] 
		\draw    (185.5,178.67) .. controls (219.45,195.16) and (236.46,227.64) .. (244.53,246.31) ;
		\draw [shift={(245.25,248)}, rotate = 247.08] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Curve Lines [id:da7465181846672535] 
		\draw    (195,278.83) .. controls (133.44,241.24) and (135.42,225.15) .. (149.59,189.32) ;
		\draw [shift={(150.25,187.67)}, rotate = 111.73] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		
		% Text Node
		\draw (142,147) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 1$};
		% Text Node
		\draw (280,50) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 2$};
		% Text Node
		\draw (354,170) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 3$};
		% Text Node
		\draw (238,269) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 4$};
		% Text Node
		\draw (119,65) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle p_{12}$};
		% Text Node
		\draw (377,76) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle p_{23}$};
		% Text Node
		\draw (442,183) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle p_{33}$};
		% Text Node
		\draw (236,121) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle p_{31}$};
		% Text Node
		\draw (232,195) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle p_{14}$};
		% Text Node
		\draw (119,229) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle p_{41}$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Generic example of a Markov chain}
	\end{figure}
	Thus, in the example of the oriented graph above, the only allowed transitions from the above $4$ states ($4 \times 4$ matrix) are those indicated by the arrows. So that the transition matrix is then simplified to:
	
	\end{tcolorbox}

The reader has seen in the previous example that we have the trivial property (by construction!) that the sum of the terms (probabilities) of a row of the matrix $P$ is always unitary (and therefore the sum of the terms of a column of the transpose of the matrix unit is still equal to the unit too):
	
and that the matrix is positive (meaning that all its terms are non-negative).

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
Remember that the sum of the probabilities of the columns is always equal to $1$ for the transpose of the stochastic matrix!
	\end{tcolorbox}	

The analysis of transient state (or: random walk) of a Markov chain consist to determine (or to impose!) to the column-matrix (vector) $p(n)$ to be in a given state at $n$-th step of the walk:
	
with the sum of the components that is always equal $1$ (since the sum of the probabilities of being in any of the vertices of the graph at given a time/step must be equal to $100\%$).

We frequently name this column matrix "\NewTerm{stochastic vector}\index{stochastic vector}" or "\NewTerm{probability measure on the vertex $i$}\index{probability measure}".

\begin{theorem}
We want to prove that the total probability of this stochastic vector is always equal to $1$.
\end{theorem}

\begin{dem}
If $p(n)$ is a stochastic vector, then its image:
	
is also a stochastic vector. Effectively, $p_i(n+1) \geq 0$ because:
	
is a sum of positive or zero values. Furthermore, we find:
	
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
\end{dem}
This probability vector whose components are positive or zero, depends (it's pretty intuitive) on the transition matrix $P$ and the vector of initial probabilities $p(0)$.

Although even if it is provable (Perron-Frobenius theorem), the reader may verify by a practical case (computerized or not!) that if we choose any vector state $p(n)$ then there exists for any stochastic matrix $P$ a unique probability vector traditionally noted $\pi$ as:
	
	Such a probability measure $\pi$ satisfying the above equation is named an "\NewTerm{invariant measure}\index{invariant measure}" or "\NewTerm{stationary measure}\index{stationary measure}\label{stationary measure}" or "\NewTerm{balanced measure}\index{balance measure}" which represents the equilibrium state of the system. In terms of linear algebra (see section of the same name page \pageref{linear algebra}) for the eigenvalue $1$, $\pi$ is an eigenvector of $P$ (\SeeChapter{see section Linear Algebra page \pageref{eigenvector}}).

	Iterations obtained before a Markov chain reaches the stationary measure are named "\NewTerm{burn-in}" or "\NewTerm{warmup}".

	We will see a trivial example in the Graph Theory section (page \pageref{adjacency matrix}) which will be redeveloped in detailed as in the section of Game and Decision Theory in the context of pharmaco-economics (page \pageref{markov decision process}) and in the section of Software Engineering (page \pageref{google pagerank algorithm}) when we will study the fundamentals of the Google PageRank algorithm. But also note that the Markov chains are used for example in meteorology (or in the case of computer passwords hacks):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=4cm,
	  thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]
		  \node[main node] (E1) {Sunny};
		  \node[main node] (E2) [below left of=E1] {Snow};
		  \node[main node] (E3) [below right of=E1] {Rain};
		  \path[every node/.style={font=\sffamily\small}]
		   (E1) edge [loop above] node {$80\%$} (E1)
		        edge node {$10\%$} (E2)
		        edge [bend left] node {$10\%$} (E3)
		   (E2) edge [bend left] node {$30\%$} (E1)
		        edge [loop left] node {$40\%$} (E2)
		        edge node {$30\%$} (E3)
		   (E3) edge node {$20\%$} (E1)
		        edge [bend left] node {$20\%$} (E2)
		        edge [loop right] node {$60\%$} (E3);
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Concrete example of a very simple Markov chain}
	\end{figure} 
	or in medicine, finance, transportation, marketing, etc.

Let us also point out a quite trivial equality to which the mathematician would have given the name of "\NewTerm{Chapman-Kolmogorov equation}\index{Chapman-Kolmogorov equation}". It is simply the following equality obtained by recurrence (which saves a lot of time in terms of applied calculations):


In the field of language analysis, from the frequency analysis of a sequence of words, computers are able to also build Markov chains and therefore propose a more correct semantic during grammatical computerized corrections or in written transcription of oral presentations.

\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] A Markov chain is said to be an "\NewTerm{irreducible Markov chain}\index{irreducible Markov chain}" if all states are bound to others (it's the case of the example in the figure above).

		\item[D2.] A Markov chain is said to be an "\NewTerm{absorbing Markov chain}\index{absorbing Markov chain}" if one of the states of the chain absorbs the transitions (so nothing comes out just to say things in a more simple way!).
	\end{enumerate}
	
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{90} & \pbox{20cm}{\score{4}{5} \\ {\tiny 27 votes, 51.11\%}} 
	\end{tabular} 
	\end{flushright}
	
	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Statistics}\label{statistics}
	\lettrine[lines=4]{\color{BrickRed}S}{tatistics} is a science that concerns the systematic grouping of facts or recurring events that lend themselves to a numerical or qualitative assessment over time according to a given law. In the industry and the economy in general, statistics is a science that helps in an uncertain environment to make valid inferences. Fondamentalement, les statistiques se préoccupent rarement de prouver quelque chose de vrai ou de faux dans la science expérimentale, mais plutôt d'évaluer à quel point les modèles/hypothèses/effets concurrents sont convaincants !

	You should know that among all areas of mathematics, the one that is used the most widely in business and research centers is Statistics and especially since softwares greatly facilitates the calculations! This is why this section is one of the biggest in this book even if only the basic concepts are presented!

	Note also that Statistics have a very bad reputation at the university because the notations are often confusing and vary greatly from one teacher to another, from one book to another, from one practitioner to another. Strictly speaking, it should comply with the vocabulary and notation of the ISO 3534-1:2006 \textit{Statistics - Vocabulary and symbols} standard and unfortunately this section was written before the publication of this standard... a certain period of adaptation will be necessary to obtain the full compliance.

	It is perhaps useless to precise that Statistic is widely used in engineering, robotics, theoretical physics, fundamental physics, econometrics, project management and in the industry of process, in the fields of life and non-life insurance, in the actuarial science or in the database analysis (with Microsoft Excel very often ... unfortunately ....) and the list goes on. We will also meet quite often the tools presented here in the chapters of Fluid Mechanics, Thermodynamics, Technical Management, Industrial Engineering and Economy (especially in the last two). The reader can then refer to them to have some concrete practical applications of the most important theoretical elements that will be seen here.

Note also that in addition to a few simple examples on these pages, many other application examples are given on the exercise server of the companion website in the categories Probability and Statistics, Industrial Engineering, Econometrics and Management Techniques.

\textbf{Definition (\#\thesection.\mydef):} The main purpose of Statistics is to determine the characteristics of a given population from the study of a part of the population, named "\NewTerm{sample}\index{sample}" or "\NewTerm{representative sample}\index{representative sample}". The determination of these characteristics should enable statistics to be a tool for the decision help!

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
The data processing concerns the "\NewTerm{descriptive statistics}\index{descriptive statistics}" or also named the "\NewTerm{descriptive analytics}\index{descriptive analytics}". The interpretation of data from estimators is named "\NewTerm{statistical inference}" (or "\NewTerm{inferential statistics}\index{inferential statistics}"), and mass data analysis "\NewTerm{statistical frequency}\index{statistical frequency}" as opposed to Bayesian inference (\SeeChapter{see section Probabilities page \pageref{bayesian inference}}).
	\end{tcolorbox}	

When we observe an event taking into account some given factors, there can happen that a second observation takes place in conditions that seem identical. By repeating these steps several times on different supposedly similar objects, we find that the observed results are statistically distributed around a mean value that is ultimately the most likely possible outcome. In practice, however, we sometimes perform a single measurement and then the goal is determine the value of the error we make by adopting it as average measure. This determination requires knowledge of the type of statistical distribution we are dealing with and that is on what we will focus (among others) to study here (at least the basics!). However, there are several common methodological approaches when we face the probabilistic events (less common are not mentioned yet):
	\begin{enumerate}
		\item A first is to simply ignore the random elements, for the simple reason that we do not know how to integrate them. We then use the "scenarios method" also named "deterministic simulation". This is typically the tool used by financial managers and non-graduates managers with tools like Microsoft Excel (which includes a scenarios management tool) or Microsoft Project (which includes a tool to manage the deterministic optimistic, pessimistic and expected scenarios).
		
		\item A second possible approach, when we do not know how to associate probabilities to specific future random events, is game theory (\SeeChapter{see section Game and Decision Theory page \pageref{game and decision theory}}) where semi-empirical criteria of selection are used as the criterion of maximax, minimax, Laplace, Savage, etc.
		
		\item Finally, when we can link probabilities to random events, whether these probabilities derived from calculations or measurements, whether they are based on experience from previous similar situations as the current situation, we can use descriptive and inferential statistics (contents of this chapter) to obtain usable and relevant information from this mass of acquired datas.
		
		\item A last approach when we know the relative probabilities from intervening events in response to strategic choices is the use of decision theory (\SeeChapter{see section Game and Decision Theory pages \pageref{extensive representation of a decision} or \pageref{graphical strategy with probabilities}, \pageref{hurwitz criteria}, \pageref{dove hawk game probabilities} and \pageref{markov decision process}}).
	\end{enumerate}
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.2]{img/arithmetics/love_statistics.jpg}
	\end{figure}
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
\textbf{R1.} Without mathematical statistics, a calculation on datas (e.g. an average), is a "\NewTerm{punctual indicator}\index{punctual indicator}". This is mathematical statistics which gives it the status of estimator whose bias, uncertainty and other statistical characteristics are controlled. We generally seek to ensure that the estimator is unbiased, convergent and efficient (we will see during our study of estimators further what is exactly all that stuff).\\

\textbf{R2.} When we communicate a statistic it should be an obligation to specify the confidence interval, the $p$-value and the size of the studied sample (absolute statistics) and its detailed characteristics and make available the sources and data protocol otherwise it has almost no scientific value (we will see all these concepts in detail further below). A common mistake is to communicate in relative values. For example, on a test group of 1,000 women, 5 women will die from breast cancer without screening check, with screening check 4 women will die. Some will say a little to quickly (typically physicians....) that screening checks saves 20\% of women (relative value as one the of five could have been saved...). In fact this is wrong since the absolute benefit of screening is insignificant!\\\\
\textbf{R3.} If you have a teacher or trainer who dares to teach you statistics and probabilities only with examples based on gambling (cards, dice, matches, coins, etc.) dispose or denounce him. Normally examples should be based on the industry, economy or R\&D, i.e. in areas used in daily by businesses!).
	\end{tcolorbox}	
	Let us introduce before continuing some definitions that will be useful for the future on the concept of samples and averages (we will come back on this basic definitions later in a more technical way!):
	
	\subsection{Samples}

During the statistical analysis of sets of information, the way to select the sample is as important as how to analyse it.The sample must be representative of the population (we do not necessarily make reference to human populations!). For this, the random sampling is the best way to achieve it.

\textbf{Definitions (\#\thesection.\mydef):}
\begin{enumerate}
	\item[D1.] The statistician always starts from the observation of a finite number of elements, which we name the "\NewTerm{population}\index{population}". The observed elements, in quantity $n$, are all of the same nature, but this nature can be very different from one population to another.
	\item[D2.] We are in the presence of a "\NewTerm{quantitative character}\index{quantitative character}" when each observed element is explicitly subject to the same measure. To a given quantitative character, we associate a "\NewTerm{quantitative variable}\index{quantitative variable}" continuous or discrete, which summarizes all the possible values that the measure can take (such information being represented by functions like the Gauss-Laplace distribution, the beta distribution, the Poisson distribution, etc.).
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
We will come back on the concept of "variable" and "distribution" a little further...
	\end{tcolorbox}	
	\item[D3.] We are in the presence of a "\NewTerm{qualitative character}\index{qualitative variable}" when each observed element is explicitly subject to a single connection to a "\NewTerm{modality}\index{modality}" from a set of exclusive modalities (e.g.: man $\mid$ woman) that permits to classify all studied elements in a given certain point of view (such information being represented by bar charts, sector charts, bubble charts, etc.). All modalities of a character can be established a priori before the survey (a list, a nomenclature, a code) or after the survey. A study population can be represented by a mixed character, or set of modalities such as gender, wage range, age, number of children, marital status for example for an individual.
	\item[D4.] A "\NewTerm{random sample}\index{random sample}" is by default (without more precision) a sample in which all individuals in a population have the same chance, or "\NewTerm{equally likely probability}\index{equally likely probability}" (and we emphasize that this probability must be equal), to end up in the sample.
	\item[D5.]  In the opposite in a sample whose elements were not chosen randomly, then we talk about a "\NewTerm{biased sample}\index{biased sample}" (in the opposite case we talk about a "\NewTerm{non-biased sample}\index{non-biases sample}").
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
A small representative sample is by far preferable to a large biased sample. But when the sample sizes are small, the probabilities are so that sometimes we can have a result worst than the biased one...
	\end{tcolorbox}	
\end{enumerate}

	\subsection{Averages}\label{averages}
	The concept of "\NewTerm{average}\index{average}" or "\NewTerm{central tendency}\index{central tendency}" (financial analysts name it a "measure of location"...) is with the notion of "variable" at the basis of statistics.

	This notion seems very familiar to us and we talk a lot about it without asking too many questions. But there are various qualifiers (we emphasize that this are only qualifiers!) to distinguish the way of the resolution of a problem of calculating the average.
	
	Thus, you must be very very careful about the calculations of averages because there is a tendency in business to rush and to systematically use the arithmetic mean without thinking, which can lead to serious errors! A nice example (for an analogy) is that a considerable number of laws require only moderate levels of pollution per year, while for example, smoking one cigarette per day during 365 days does not have the same impact as smoking 365 cigarettes in one day during one year when both have the same average taken over a year ... This is a clear evidence of statistical incompetence of the legislature.
	
	Here is a small sample of common mistakes:
	
	\begin{itemize}
		\item Consider that the arithmetic mean is the value that divides the population into two equal parts (although it is the median that does this).
	
		\item Consider that the average of the ratios of the type goals/realisations is equal to the ratio of the average of the goals and of the average of the realizations (although it is not the same thing!).
	
		\item Consider that the average salaries of different subsidiaries, is equal to the global average (while this is true if and only if there is the same number of employees in each subsidiary of the company).
	
		\item Consider that the average of the average of the rows in a table is always equal to the average of the columns of the same table (although this is true if and only if the cell contents are not empty).
	
		\item Calculate the arithmetic average growth of the revenue in \% (as the geometric mean must be used).
	
		\item etc.
	\end{itemize}
	
	We will see below different average with examples relative to arithmetic, to the enumeration, to physics, to econometrics, to geometry and sociology. The reader will find other practical examples by browsing the entire book.
	
	\textbf{Definitions (\#\thesection.\mydef):} As given $x_i$ real numbers, then we have:
	\begin{enumerate}
		\item[D1.] The "\NewTerm{arithmetic average}\index{arithmetic average}\label{arithmetic average}" or "\NewTerm{sample average}\index{sample average}" (the most commonly known) is defined as the quotient of the sum of $n$ observed $x_i$ values by the total size $n$ of the sample:
		
		and is very often written $\bar{x}$ or $\widehat{\mu}$  and is for any discrete or continuous statistical distribution an unbiased estimator of the mean.

	The arithmetic average represents a statistical measure expressing the magnitude that would have each member of a set of measures if the total must be equal to the product of the arithmetic average by the number of members.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Consider three products of respective prices $\{10.-,30.-,60.-\}$ whose price has increase of respectively $\{1.-,6.-,18.-\}$, hence respectively a total increase of $25.-$. The arithmetic average increase is therefore:
$$\mu_a=\dfrac{1+6+18}{3}=8.\bar{3} \Rightarrow 3\cdot 8.\bar{3}=25.-$$
	\end{tcolorbox}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The statistical fact that when the characteristics of individual objects within a population of similar objects are distributed according to a given law (curve) lead us to the observation that when we sample such a population we are more likely to find a property near the average than one out on either tail of the curve. This is named the "principle of mediocrity".
	\end{tcolorbox}

	If some values repeats more than once in the measurements, the arithmetic mean is then often formally denoted as following:
	
	and the last equality is named "\NewTerm{weighted average}\index{weighted average}\label{weighted mean}" and often denoted:
	
	Notice that the arithmetic average is a special case of the weighted average! Because we may have $w_i\neq n_i/n$ in some very common practical applications!
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Consider three products of respective prices $\{10.-,30.-,60.-\}$ (hence a total of $100.-$) whose price has increase of respectively $\{1.-,6.-,18.-\}$, ie $\{10\%,20\%,30\%\}$, hence respectivelly a total incrase of $25.-$ (from $100.-$ to $125.-$ hence it is immediate that the global increase is of $25\%$). The arithmetic average increase in percentage is therefore:
	$$\mu_a=\dfrac{10\%+20\%+30\%}{3}=20\%$$
	and the weighted average increase is of:
	$$\mu_w=\dfrac{10.-}{100.-}10\%+\dfrac{30.-}{100.-}20\%+\dfrac{60.-}{100.-}30\%=25\%$$
	It is then easy to identify that the result that makes sense in this case is the weighted average and not the arithmetic average!
	\end{tcolorbox}
	
	Finally, we could indicate that under this approach, the actual weighted average will be named "\NewTerm{mathematical mean}\index{mathematical mean}" or just "\NewTerm{mean}\index{mean}" in the field of study of probabilities.

	We may as well use the frequencies of occurrence of the observed values named "\NewTerm{classes frequencies}\index{class frequencies}":
	
	So that we get another equivalent definition named the "\NewTerm{weighted average by the classes frequencies}\index{weighted average by the classes frequencies}\label{weighted average by the classes frequencies}":
	
	Before continuing, it's important to know that in the field of statistics it is useful and often necessary to combine measurements/data in class intervals of a given width (see examples below). We often have to make several tries to choose the intervals even if there are semi-empirical formulas for choosing the number of classes when we have $n$ available values. One of these semi-empirical rules used by many practitioners is to retain the smallest integer $k$ of classes such as:
	
	the width of the class interval is then obtained by dividing the range (difference between the maximum and minimum measured value) by $k$. Thus:
	
	By convention and rigorously... (so rarely respected in the notations), a class interval is closed on the left and open on the right (\SeeChapter{see section Numbers page \pageref{domain of definition}}):
	
	This empirical rule is named the "\NewTerm{Sturges rule}\index{Sturges rule}" and is based on the following reasoning:

	We assume that the values of the binomial coefficient $C_k^i$ gives the number of individuals in an ideal histogram (we let the reader check this simply with a spreadsheet software like Microsoft Excel 11.8346 and the $\texttt{COMBIN(k,i)}$ function) of $k$ intervals for the $i$-th interval. As $k$ becomes large the histogram looks more and more like a continuous curve named the "Normal curve" or "bell curve" as we will see later.

	Therefore, based on the binomial theorem (\SeeChapter{see section Calculus page \pageref{binomial theorem}}), we have:
	
	Then, for each interval $i$ the practitioner will traditionally take the average between the lower and upper limit for the calculation and multiply it by the corresponding class frequency $f_i$. Therefore, the grouping of class frequencies implies that:
	\begin{enumerate}
		\item The weighted average by the frequencies differs from the arithmetic average.
		
		\item As the approximation seen above it will be a worst indicator compare to the arithmetic average...
		
		\item It is very sensitive to the choice of the number of classes, than very bad at this level.
	\end{enumerate}
	There are many other empirical rules for the discretization of random variables. For example, the software XLStat offers not less than 10 rules (constant amplitude, Fisher algorithm, $k$-means, 20/80, etc.).
	
	In practice we prefer to use ECDF (Empirical Cumulated Distribution Function) or QQ-plots because those latter do not rely on bin-size!

	Later, we will see two very important properties of the arithmetic average and of the mean that you will have to understand absolutely (the weighted average of deviations from the average and the average deviations from the average).

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
The "\NewTerm{mode}\index{mode}", denoted $\mathrm{Mod}$ or simply $M_0$, is defined as the value that appears most often in a set of data. In Microsoft Excel 11.8346, it is important to know that the $\texttt{MODE( )}$ function returns the first value in the order of values having the largest number of occurrences therefore assuming a unimodal distribution.
	\end{tcolorbox}	
		\item[D2.] The "\NewTerm{median}\index{median}" or "\NewTerm{middle value}\index{middle value}" $M_e$ is the value that cut the population values into two equal parts. In the case of a continuous statistical distribution $f(x)$ of a random variable $X$, it is the value that represents the value that has $50\%$ of cumulative probability to occur (we will see further in details the concept of statistical distribution):
		
In the case of a series of ordered values $x_1,x_2,\ldots,x_i,\ldots, x_n$, the median is therefore by definition the value such that we have the same number of values that are greater than or equal to it than the number of values that are less or equal to it.
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} The median is mainly used for skewed distributions, because it represent them better than the arithmetic average.\\
	
	\textbf{R2.} The median is in practice often not a single value (at least in the case where $n$ is even). Indeed, between the values corresponding to ranges $\dfrac{n}{2}$ and $\dfrac{n}{2}+1$ there is an infinite number of values to choose which cut the population in half. This is why for some people the median is not intuitive and especially when there are a lot of duplicates values. That's why in the case of duplicates, using in a spreadsheet software a formula like \texttt{=COUNTIF(range of cells,"<=Median)/COUNT(range of cells)} to calculate the proportion of cells having a value less than the median (or more than the median) and expecting a result of $50\%$ is obviously wrong!\\
	
	\textbf{R3.} Some corporations remove the duplicate values before calculating the Median. That's scientifically wrong but may be arguable in context of internal governance of the board committee (typically if evidence has shown that this lead to better decisions!).
	\end{tcolorbox}	
More rigorously:
	\begin{itemize}
		\item If the number of terms is odd, i.e. of the form $2n + 1$, the median of the series is the term of order $n + 1$ (that the terms are all distinct or not!).
		\item If the number of terms is even, i.e. of the form $2n$, the median of the series is half-sum (arithmetic average) of the values of the terms of rank $n$ and $n + 1$ (that the terms are all distinct or not!).	
	\end{itemize}

	In any case, by this definition, it follows that there are at least $50\%$ of the terms of the series that are smaller than or equal to the median, and at least $50\%$ of the terms of the series that are greater than or equal to the median.

	For example, consider the table of wages below:

	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|c|c|c|c|}
		\hline
		\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Employee N\degre}} & 
\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Wage}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Cumulated Employees}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{\%Cumulated Employees}}\\ \hline
		1 & 1,200 & 1 & 6\%\\ \hline
		2 & 1,220 & 2 & 12\%\\ \hline
		3 & 1,250 & 3 & 18\%\\ \hline
		4 & 1,300 & 4 & 24\%\\ \hline
		5 & 1,350 & 5 & 29\%\\ \hline
		6 & 1,450 & 6 & 35\%\\ \hline
		7 & 1,450 & 7 & 41\%\\ \hline
		8 & 1,560 & 8 & 47\%\\ \hline
\multicolumn{1}{|c|}{\cellcolor{green!30}9} & 
\multicolumn{1}{|c|}{\cellcolor{green!30}1,600} & \multicolumn{1}{|c|}{\cellcolor{green!30}9} & \multicolumn{1}{|c|}{\cellcolor{green!30}53\%}\\ \hline	
		10 & 1,800 & 10 & 59\%\\ \hline
		11 & 1,900 & 11 & 65\%\\ \hline
		12 & 2,150 & 12 & 71\%\\ \hline
		13 & 2,310 & 13 & 76\%\\ \hline
		14 & 2,610 & 14 & 82\%\\ \hline
		15 & 3,000 & 15 & 88\%\\ \hline
		16 & 3,400 & 16 & 94\%\\ \hline
		17 & 4,800 & 17 & 100\%\\ \hline
		\end{tabular}
		\caption{Identification of the median}
	\end{table}	

	There is in the table an odd number $2n + 1$ of values. So the median of the series is the term of rank $n + 1$. This is $1,600.-$ (result that give any spreadsheet software). The arithmetic average is in this case about $2,020.-$.

	In direct relation with the median it is important to define the following concept to understand the underlying mechanism:

	\item[D3.] Be given a statistical series $x_1,x_2,\ldots,x_i,\ldots,x_n$, we name "\NewTerm{dispersion of absolute differences}\index{dispersion of absolute differences}" around $x$ the number $\varepsilon '(x)$ defined by:
	
$\varepsilon '(x)$ is minimum for a value of $x$ closest to a given value $x_i$ in the sense of the absolute error value. The median is the value that achieves this minimum (extremum)! The idea will then be to study the variations of the function to find the position of this extremum.

Indeed, we can write:
	
Then by definition of the $x$ value:
	
What allows us to skip the absolute values is simply the choice of the index $r$ that is taken so that the series of values in practice can always be split into two parts: all that is less than the element indexed by $r$ and all that is superior to it (i.e.. the median by anticipation...).

$\varepsilon '(x)$ is also a piecewise (discrete) affine function (similar to the equation of a line for fixed values of $r$ and $n$) where we see that by analogy the factor:
	
is the slope of the function and:
	
the $Y$-intercept (ordinates at the origin).

The function is decreasing (negative slope) until $r$ is less than $\dfrac{n}{2}$ and increasing when $r$ is greater than $\dfrac{n}{2}$ (it passes trough an extremum!). Specifically, we distinguish two particularly cases of interest since $n$ is an integer:

	\begin{itemize}
		\item If $n$ is even, we can say that $n=2n'$, then the slope can be written $2(r-n')$ and it is equal to zero if $r=n$ and then, as the result is valid by construction only for $\forall x \in \left[ x_r,x_{r+1}\right] $ then $\varepsilon '(x)$ is constant on $\left[ x_{n'},x_{n'+1}\right]$ and we have an extremum necessarily in the middle of this range (arithmetic average of the two terms).
		
		\item If $n$ is odd, we can say that $n=2n'+1$ (we cut the series into two equal parts), then the slope can be written $(2r-2n'-1)$ and it is zero if $r=n'+\dfrac{1}{2}$, as the result is only valid for $\forall x \in \left[ x_r,x_{r+1}\right] $ then it is immediate that the middle value is the median $x_{n'+1}$.
	\end{itemize}
	We find out the median in both cases. We will also see later how the median is defined for a continuous random variable (the underlying idea is the same).

	There is another practical case where the statistician has at its disposal only the values grouped in intervals of statistical classes. The procedure for determining the median is then different:

	When we have at our disposal only a values grouped in intervals of statistical classes, the abscissa of the point of the median is generally within a class. To then get a more accurate value of the median, we perform a linear interpolation. This is what we name the "\NewTerm{linear interpolation method of the median}\index{linear interpolation method of the median}".

	The median value can be read from the graph or calculated analytically. Indeed, consider the graph of the cumulative probability $F(x)$ in class intervals as below where the bounds of the intervals were connected by straight lines: 
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,387); %set diagram left start at 0, and has height of 387
		
		%Shape: Axis 2D [id:dp44278814874178907] 
		\draw  (153.45,258.67) -- (448.95,258.67)(183,28.57) -- (183,284.23) (441.95,253.67) -- (448.95,258.67) -- (441.95,263.67) (178,35.57) -- (183,28.57) -- (188,35.57) (203,253.67) -- (203,263.67)(223,253.67) -- (223,263.67)(243,253.67) -- (243,263.67)(263,253.67) -- (263,263.67)(283,253.67) -- (283,263.67)(303,253.67) -- (303,263.67)(323,253.67) -- (323,263.67)(343,253.67) -- (343,263.67)(363,253.67) -- (363,263.67)(383,253.67) -- (383,263.67)(403,253.67) -- (403,263.67)(423,253.67) -- (423,263.67)(163,253.67) -- (163,263.67)(178,238.67) -- (188,238.67)(178,218.67) -- (188,218.67)(178,198.67) -- (188,198.67)(178,178.67) -- (188,178.67)(178,158.67) -- (188,158.67)(178,138.67) -- (188,138.67)(178,118.67) -- (188,118.67)(178,98.67) -- (188,98.67)(178,78.67) -- (188,78.67)(178,58.67) -- (188,58.67)(178,278.67) -- (188,278.67) ;
		\draw   ;
		%Straight Lines [id:da8270574333770748] 
		\draw [line width=2.25]    (162.5,258.67) -- (203.5,258.67) ;
		%Straight Lines [id:da14096066187415146] 
		\draw [line width=2.25]    (203.5,258.67) -- (219.5,219.67) ;
		%Straight Lines [id:da0227814997713065] 
		\draw [line width=2.25]    (219.5,219.67) -- (244.5,160.67) ;
		%Straight Lines [id:da8402919724456077] 
		\draw [line width=2.25]    (264.5,117.67) -- (244.5,160.67) ;
		%Straight Lines [id:da9975232219736176] 
		\draw [line width=2.25]    (264.5,117.67) -- (344.5,57.67) ;
		%Straight Lines [id:da03843924729608639] 
		\draw [line width=2.25]    (441.5,57.67) -- (344.5,57.67) ;
		%Straight Lines [id:da09680065279763395] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (180.5,57.67) -- (344.5,57.67) ;
		%Straight Lines [id:da3437392254684226] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (182.5,117.67) -- (264.5,117.67) ;
		%Straight Lines [id:da052675506494032964] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (264.5,117.67) -- (264.5,257.67) ;
		%Straight Lines [id:da32964148208349764] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (183.5,158.67) -- (440.5,158.67) ;
		%Straight Lines [id:da9489256743697856] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (244.5,160.67) -- (244.5,289.67) ;
		%Straight Lines [id:da5217435645161286] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (187.5,218.67) -- (219.5,218.67) ;
		%Straight Lines [id:da3230112560106908] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (223.5,257.67) -- (223.5,219.67) ;
		
		% Text Node
		\draw (154,210) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {$\displaystyle 0.2$};
		% Text Node
		\draw (154,151) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {$\displaystyle 0.5$};
		% Text Node
		\draw (154,110) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {$\displaystyle 0.7$};
		% Text Node
		\draw (153,51) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {$\displaystyle 1.0$};
		% Text Node
		\draw (161,265) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
		% Text Node
		\draw (218,268) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {$\displaystyle 2$};
		% Text Node
		\draw (258,268) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {$\displaystyle 4$};
		% Text Node
		\draw (339,268) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {$\displaystyle 8$};
		% Text Node
		\draw (220,289) node [anchor=north west][inner sep=0.75pt]   [align=left] {Median};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Graphical representation of the estimation of the median by linear interpolation}
	\end{figure}
	The value of the median $M_e$ is obviously located at the crossroads between the cumulated probability of 50\% (0.5) and the abscissa. Thus, by applying the basics of functional analysis, we have (just by observing that the slope in the interval containing the median is equal in the half-interval to the left and to right adjacent to the median):
	
	What we frequently write:
	
	Thus the value of the median:
		
	Consider the following table that we will see again much later in this chapter:

	\begin{table}[H]
		\centering
		\renewcommand{\arraystretch}{1.2}
		\small
		\begin{tabular}{cccc}\hline
		\rowcolor[gray]{0.75}\textbf{Price} &  \textbf{Number} & \textbf{Cumulated number} & \textbf{Relative frequencies}  \\[-3pt]
		\rowcolor[gray]{0.75}\textbf{of tickets} & \textbf{of tickets} & \textbf{of tickets} & \textbf{of tickets} \\ \hline % ne pas enlever les espaces vides entre les lignes!!!
		[0,50[ & 668 & 668 & 0.068 \\

		[50,100[ & 919 & 1,587 & 0.1587 \\

		[100,150[ & 1,498 & 3,085 & 0.3085 \\

		[150,200[ & 1,915 & 5,000 & 0.5000 \\

		[200,250[ & 1,915 & 6,915 & 0.6915\\

		[250,300[ & 1,498 & 8,413 & 0.8413\\

		[300,350[ & 919 & 9,332 & 0.9332 \\

		[350,400[ & 440 & 9,772 & 0.9772 \\

		[400 and + & 228 & 10,000 & 1 \\ \hline
		\end{tabular}
		\caption{Identification of the median and the mode}
	\end{table}
	We see that the "\NewTerm{median class}\index{median class}" is in the range $[150,200]$ because the cumulative value of $0.5$ is there (column at the right of the table) but the median has, using the previously established relation, the precise value of (it is trivial in the particular example of the table above, but we still do the calculation...):
	
and of course we can do the same with any other percentile!

	\textbf{Definition (\#\thesection.\mydef):} We can also give on the way a definition to determine the "\NewTerm{modal value}\index{modal value}" $M_0$ (the value that occurs the most frequently) if we are only in possession of the frequencies of class intervals. To see that we start with diagram below named "\NewTerm{grouped distribution}\index{grouped distribution}" in frequencies bar:
\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,387); %set diagram left start at 0, and has height of 387
		
		%Shape: Axis 2D [id:dp44278814874178907] 
		\draw  (153.45,258.67) -- (448.95,258.67)(183,28.57) -- (183,284.23) (441.95,253.67) -- (448.95,258.67) -- (441.95,263.67) (178,35.57) -- (183,28.57) -- (188,35.57) (203,253.67) -- (203,263.67)(223,253.67) -- (223,263.67)(243,253.67) -- (243,263.67)(263,253.67) -- (263,263.67)(283,253.67) -- (283,263.67)(303,253.67) -- (303,263.67)(323,253.67) -- (323,263.67)(343,253.67) -- (343,263.67)(363,253.67) -- (363,263.67)(383,253.67) -- (383,263.67)(403,253.67) -- (403,263.67)(423,253.67) -- (423,263.67)(163,253.67) -- (163,263.67)(178,238.67) -- (188,238.67)(178,218.67) -- (188,218.67)(178,198.67) -- (188,198.67)(178,178.67) -- (188,178.67)(178,158.67) -- (188,158.67)(178,138.67) -- (188,138.67)(178,118.67) -- (188,118.67)(178,98.67) -- (188,98.67)(178,78.67) -- (188,78.67)(178,58.67) -- (188,58.67)(178,278.67) -- (188,278.67) ;
		\draw   ;
		%Shape: Rectangle [id:dp702178376757342] 
		\draw  [line width=1.5]  (203.5,177.67) -- (263.5,177.67) -- (263.5,259) -- (203.5,259) -- cycle ;
		%Shape: Rectangle [id:dp8630127314728857] 
		\draw  [line width=1.5]  (263.5,92.67) -- (323.5,92.67) -- (323.5,259) -- (263.5,259) -- cycle ;
		%Shape: Rectangle [id:dp9251304352546621] 
		\draw  [line width=1.5]  (323.5,205.67) -- (383.5,205.67) -- (383.5,259) -- (323.5,259) -- cycle ;
		%Straight Lines [id:da831131273634528] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (263.5,92.67) -- (323.5,205.67) ;
		%Straight Lines [id:da6481400864191806] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (263.5,177.67) -- (323.5,92.67) ;
		%Straight Lines [id:da19308658438990545] 
		\draw [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (289.5,297.67) -- (289.5,141.67) ;
		
		% Text Node
		\draw (161,265) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
		% Text Node
		\draw (257,262) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle x_{i}$};
		% Text Node
		\draw (312,262) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle x_{i+1}$};
		% Text Node
		\draw (270,297) node [anchor=north west][inner sep=0.75pt]   [align=left] {Mode};
		% Text Node
		\draw (242,124) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \Delta _{1}$};
		% Text Node
		\draw (326,139) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \Delta _{2}$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Graphical representation of the estimation of the modal value with classes intervals}
	\end{figure}
	Using the Thales relations (\SeeChapter{see section Euclidean Geometry \pageref{thales theorem}}), it comes immediately, noting $M_0$ the modal value:
	
	As in a proportion, we do not change the value of the ratio by adding the numerators and adding the denominators, we get:
	
	We then have:
	
	With the previous example this gives then:
	
	The question that then arises is to the appropriateness of the choice of the mean, mode or median in terms of communication ... (normally we communicate them all three in corporate reports!).

	A good example is that of the labour market where in general, while the average wage and the median wage are quite different, the institutions of state statistics calculate the median than many traditional media then explicitly equate to he concept of "arithmetic average" in their news...

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	To avoid getting an arithmetic average having little sense, we often calculate a "\NewTerm{trimmed average}\index{trimmed average}", i.e. an arithmetic average calculated after removing outliers in the series (using Grubbs or Dixon tests or other some naive methods). See the section of Numerical Methods page \pageref{trimmed mean} for more details on this naive method.
	\end{tcolorbox}
	The "\NewTerm{quantile}\index{quantile}" generalize the concept of "median" by cutting the distribution in sets of equal parts (of the same cardinality we might say ...) or in other words in regular intervals. We define the "\NewTerm{quartiles}\index{quartiles}", the "\NewTerm{decile}\index{decile}" and "\NewTerm{percentile}\index{percentile}" on the population, ordered in ascending order, that we divide by $4$, $10$ or $100$ parts of the same size.	So we talk about the 90th percentile to indicate the value separating the first $90\%$ of the population and the $10\%$ remaining.
	
	Let $X$ be a random variable with cumulated distribution function $F(x)$. The $\tau$th quantile of $X$ is defined by:
	
	
	The "\NewTerm{midhinge}\index{midhinge}" is the average of the first and third quartiles and is thus a measure of location. Equivalently, it is the $25\%$ trimmed mid-range or $25\%$ midsummary; it is an $L$-estimator (see page \pageref{L-statistics}) defined explicitly by:
	
	The midhinge is related to the interquartile range (IQR), the difference of the third and first quartiles (i.e. $I Q R=Q_3-Q_1$), which is a measure of statistical dispersion. The two are complementary in sense that if one knows the midhinge and the IQR, one can find the first and third quartiles.

	The use of the term "hinge" for the lower or upper quartiles derives from John Tukey's work on exploratory data analysis in the late 1970 s, and "midhinge" is a fairly modern term dating from around that time. The midhinge is slightly simpler to calculate than the "\NewTerm{trimean}\index{trimean}" $(T M)$, which originated in the same context and equals the average of the median $\left(\tilde{X}=Q_2=P_{50}\right)$ and the midhinge:
	
	
	\textbf{Definition (\#\thesection.\mydef):} The "\NewTerm{$P$-th percentile}" ($0<P\leq 100$) of a list of $N$ ordered values (sorted from least to greatest) is the smallest value in the list such that no more than $P$ percent of the data is strictly less than the value and at least $P$ percent of the data is less than or equal to that value. This is obtained by first calculating the ordinal rank and then taking the value from the ordered list that corresponds to that rank. The ordinal rank $n$ is calculated using the relation (using the round to nearest lower integer notation):
	

	Note that in Microsoft Excel 11.8346 the functions \texttt{QUARTILE( )}, \texttt{PERCENTILE( )}, \texttt{MEDIAN( )}, \texttt{PERCENTRANK( )} are available and it can be useful that we specify that there are several variants (at least nine as detailed in the excellent paper \cite{HF96} and that are all available with the function \texttt{quantile( )} of the \texttt{R} statistical software!) of calculating these percentiles that explains possible variation between the results of different spreadsheet and statistical softwares\footnote{The \texttt{R} Statistical software for example include all nine methods through a parameter of it's function \texttt{quantile()}.}. When publishing scientific papers, scientists and engineers should always specify which on the nine methods of computing the quantiles was used!
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	The most famous example is to take the median for a better understanding of the vocabulary related to this topic! The median is indeed also the second quartile ($2\cdot 25\%$), but it's also at the same time the $0.5$ quantile and the $50\%$ percentile (i.e. centile) and the $5$th decile.
	\end{tcolorbox}
	More technically in the $\texttt{R}$ software the $9$ different types of quantile are defined as weighted averages of consecutive order statistics:
	
	where $1 \leq i \leq 9, \frac{j-m}{n} \leq p<\frac{j-m+1}{n}, x_j$ is the $j$th order statistic, $n$ is the sample size, the value of $\gamma$ Is a function of $j=\lfloor n p+m\rfloor$ and $g=n p+m-j$, and $m$ is a constant determined by the sample quantile type\footnote{Based on the definitions, type = 1,3,4 should always give the same result for probs = 0.5 (i.e. median), and type = 2,5,6,7,8,9 should always give the same result for probs = 0.5 (i.e. median).}.
		
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	About Microsoft Excel... and it's conditional formatting tool... his user must not confuse the option "percentile" that is based on row numbers on the sorted values of interest on that of "percentage" defined for a vector $\vec{X}$ of values by:
	
	\end{tcolorbox}
	This concept is very important in the context of confidence intervals that we will see much further in this section and very useful in the field of quality with the use of "\NewTerm{box plots}\index{box plots}" (also named "\NewTerm{Box \& Whiskers plots}\index{Box \& Whiskers plots}") to compare ("discriminate" as experts say) quickly two populations of data or more and especially to eliminate outliers (taking as reference the median will just make more sense!):
	\begin{figure}[H]
		\centering
		\resizebox{\textwidth}{!}{
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,607); %set diagram left start at 0, and has height of 607
		
		%Shape: Axis 2D [id:dp7379836375275177] 
		\draw  (65.37,402.78) -- (552.62,402.78)(77.55,149) -- (77.55,413.8) (545.62,397.78) -- (552.62,402.78) -- (545.62,407.78) (72.55,156) -- (77.55,149) -- (82.55,156)  ;
		%Straight Lines [id:da29837704309897406] 
		\draw    (193.34,402.03) -- (193.34,407.3) ;
		%Straight Lines [id:da2112608227263968] 
		\draw    (309.59,402.28) -- (309.59,407.55) ;
		%Straight Lines [id:da3118210118554774] 
		\draw    (424.34,402.53) -- (424.34,407.8) ;
		%Straight Lines [id:da9028286666751291] 
		\draw    (540.84,402.78) -- (540.84,408.05) ;
		%Straight Lines [id:da8712087036723855] 
		\draw    (77.39,372.46) -- (74.14,372.39) ;
		%Straight Lines [id:da5923683193106328] 
		\draw    (77.64,342.46) -- (74.39,342.39) ;
		%Straight Lines [id:da9881475572021068] 
		\draw    (77.64,312.21) -- (74.39,312.14) ;
		%Straight Lines [id:da9725497776198266] 
		\draw    (77.64,281.46) -- (74.39,281.39) ;
		%Straight Lines [id:da35340323889434777] 
		\draw    (77.64,250.46) -- (74.39,250.39) ;
		%Straight Lines [id:da42788250474540446] 
		\draw    (77.89,221.21) -- (74.64,221.14) ;
		%Straight Lines [id:da5171668660482798] 
		\draw    (77.89,190.96) -- (74.64,190.89) ;
		%Straight Lines [id:da2511055122980643] 
		\draw    (77.89,160.21) -- (74.64,160.14) ;
		%Shape: Rectangle [id:dp8177056950030852] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 125; green, 182; blue, 247 }  ,fill opacity=1 ] (106,306) -- (164.37,306) -- (164.37,345.28) -- (106,345.28) -- cycle ;
		%Shape: Rectangle [id:dp5423394123655236] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 125; green, 182; blue, 247 }  ,fill opacity=1 ] (221.84,227) -- (280.22,227) -- (280.22,305.28) -- (221.84,305.28) -- cycle ;
		%Shape: Rectangle [id:dp02983972067782803] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 125; green, 182; blue, 247 }  ,fill opacity=1 ] (337.84,308) -- (396.22,308) -- (396.22,342.28) -- (337.84,342.28) -- cycle ;
		%Shape: Rectangle [id:dp48866121319159794] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 125; green, 182; blue, 247 }  ,fill opacity=1 ] (452.84,235) -- (511.22,235) -- (511.22,309.28) -- (452.84,309.28) -- cycle ;
		%Straight Lines [id:da8730815597779045] 
		\draw [color={rgb, 255:red, 125; green, 182; blue, 247 }  ,draw opacity=1 ][line width=1.5]    (135.19,286.64) -- (135.19,364.64) ;
		\draw [shift={(135.19,364.64)}, rotate = 270] [color={rgb, 255:red, 125; green, 182; blue, 247 }  ,draw opacity=1 ][line width=1.5]    (0,6.71) -- (0,-6.71)   ;
		\draw [shift={(135.19,286.64)}, rotate = 270] [color={rgb, 255:red, 125; green, 182; blue, 247 }  ,draw opacity=1 ][line width=1.5]    (0,6.71) -- (0,-6.71)   ;
		%Straight Lines [id:da39033192532453276] 
		\draw [color={rgb, 255:red, 125; green, 182; blue, 247 }  ,draw opacity=1 ][line width=1.5]    (251.03,188.14) -- (251.03,340.28) ;
		\draw [shift={(251.03,340.28)}, rotate = 270] [color={rgb, 255:red, 125; green, 182; blue, 247 }  ,draw opacity=1 ][line width=1.5]    (0,6.71) -- (0,-6.71)   ;
		\draw [shift={(251.03,188.14)}, rotate = 270] [color={rgb, 255:red, 125; green, 182; blue, 247 }  ,draw opacity=1 ][line width=1.5]    (0,6.71) -- (0,-6.71)   ;
		%Straight Lines [id:da38809296141943617] 
		\draw [color={rgb, 255:red, 125; green, 182; blue, 247 }  ,draw opacity=1 ][line width=1.5]    (367.03,295.14) -- (367.03,356.28) ;
		\draw [shift={(367.03,356.28)}, rotate = 270] [color={rgb, 255:red, 125; green, 182; blue, 247 }  ,draw opacity=1 ][line width=1.5]    (0,6.71) -- (0,-6.71)   ;
		\draw [shift={(367.03,295.14)}, rotate = 270] [color={rgb, 255:red, 125; green, 182; blue, 247 }  ,draw opacity=1 ][line width=1.5]    (0,6.71) -- (0,-6.71)   ;
		%Straight Lines [id:da186782315581566] 
		\draw [color={rgb, 255:red, 125; green, 182; blue, 247 }  ,draw opacity=1 ][line width=1.5]    (482.03,229.14) -- (482.03,342.28) ;
		\draw [shift={(482.03,342.28)}, rotate = 270] [color={rgb, 255:red, 125; green, 182; blue, 247 }  ,draw opacity=1 ][line width=1.5]    (0,6.71) -- (0,-6.71)   ;
		\draw [shift={(482.03,229.14)}, rotate = 270] [color={rgb, 255:red, 125; green, 182; blue, 247 }  ,draw opacity=1 ][line width=1.5]    (0,6.71) -- (0,-6.71)   ;
		\draw  [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] (139.49,272.09) -- (130.88,263.35)(139.56,263.42) -- (130.82,272.03) ;
		\draw  [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] (139.49,255.09) -- (130.88,246.35)(139.56,246.42) -- (130.82,255.03) ;
		\draw  [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] (139.49,377.09) -- (130.88,368.35)(139.56,368.42) -- (130.82,377.03) ;
		\draw  [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] (139.49,399.09) -- (130.88,390.35)(139.56,390.42) -- (130.82,399.03) ;
		\draw  [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] (255.33,365.09) -- (246.72,356.35)(255.4,356.42) -- (246.66,365.03) ;
		\draw  [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] (255.33,180.09) -- (246.72,171.35)(255.4,171.42) -- (246.66,180.03) ;
		\draw  [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] (371.33,278.09) -- (362.72,269.35)(371.4,269.42) -- (362.66,278.03) ;
		\draw  [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] (371.33,375.09) -- (362.72,366.35)(371.4,366.42) -- (362.66,375.03) ;
		\draw  [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] (371.33,386.09) -- (362.72,377.35)(371.4,377.42) -- (362.66,386.03) ;
		\draw  [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] (486.33,380.09) -- (477.72,371.35)(486.4,371.42) -- (477.66,380.03) ;
		\draw  [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ] (486.33,217.09) -- (477.72,208.35)(486.4,208.42) -- (477.66,217.03) ;
		%Shape: Circle [id:dp09022729818294906] 
		\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ] (130.55,192.64) .. controls (130.55,190.08) and (132.62,188) .. (135.19,188) .. controls (137.75,188) and (139.83,190.08) .. (139.83,192.64) .. controls (139.83,195.2) and (137.75,197.28) .. (135.19,197.28) .. controls (132.62,197.28) and (130.55,195.2) .. (130.55,192.64) -- cycle ;
		%Shape: Circle [id:dp4585648391550994] 
		\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ] (130.55,184.64) .. controls (130.55,182.08) and (132.62,180) .. (135.19,180) .. controls (137.75,180) and (139.83,182.08) .. (139.83,184.64) .. controls (139.83,187.2) and (137.75,189.28) .. (135.19,189.28) .. controls (132.62,189.28) and (130.55,187.2) .. (130.55,184.64) -- cycle ;
		%Shape: Circle [id:dp8953554324213078] 
		\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ] (360.72,195.64) .. controls (360.72,193.08) and (362.8,191) .. (365.36,191) .. controls (367.92,191) and (370,193.08) .. (370,195.64) .. controls (370,198.2) and (367.92,200.28) .. (365.36,200.28) .. controls (362.8,200.28) and (360.72,198.2) .. (360.72,195.64) -- cycle ;
		%Straight Lines [id:da1908976510176783] 
		\draw    (106.19,323.64) -- (164.19,323.64) ;
		\draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ] (137.45,330.49) -- (132.92,325.9)(137.48,325.93) -- (132.89,330.46) ;
		%Straight Lines [id:da4232789589013324] 
		\draw    (222.03,300.64) -- (280.03,300.64) ;
		\draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ] (253.29,291.49) -- (248.77,286.9)(253.33,286.93) -- (248.73,291.46) ;
		%Straight Lines [id:da2060758450504041] 
		\draw    (338.03,323.64) -- (396.03,323.64) ;
		\draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ] (369.29,325.49) -- (364.77,320.9)(369.33,320.93) -- (364.73,325.46) ;
		%Straight Lines [id:da3713981776630302] 
		\draw    (453.03,301.14) -- (511.03,301.14) ;
		\draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ] (484.29,287.99) -- (479.77,283.4)(484.33,283.43) -- (479.73,287.96) ;
		%Straight Lines [id:da401611061501044] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (106.19,328.64) -- (164.19,328.64) ;
		%Straight Lines [id:da04389737442363639] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (222.03,289.14) -- (280.03,289.14) ;
		%Straight Lines [id:da2771849671058577] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (338.03,323.64) -- (396.03,323.64) ;
		%Straight Lines [id:da07420335385574006] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (453.03,285.71) -- (511.03,285.71) ;
		%Straight Lines [id:da15092197860048606] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (144.37,372.5) -- (179.23,372.5) ;
		\draw [shift={(141.37,372.5)}, rotate = 0] [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da8204015274908592] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (179.23,372.5) -- (179.23,439.64) ;
		%Straight Lines [id:da31590766860202923] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (135,172.5) -- (135,155.14) ;
		\draw [shift={(135,175.5)}, rotate = 270] [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da5862213743322677] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (135,155.14) -- (171.05,155.14) ;
		%Straight Lines [id:da9488774440131782] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (482.5,202) -- (482.5,179.64) ;
		\draw [shift={(482.5,205)}, rotate = 270] [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da05735995215346956] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (482.5,179.64) -- (518.55,179.64) ;
		%Straight Lines [id:da26687694511757054] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (285.12,223.01) -- (299.64,210.14) ;
		\draw [shift={(282.87,225)}, rotate = 318.46] [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da2811689832710982] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (399.35,346.53) -- (411.87,360.78) ;
		\draw [shift={(397.37,344.28)}, rotate = 48.69] [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da35574332767091676] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (519.12,297.01) -- (533.64,284.14) ;
		\draw [shift={(516.87,299)}, rotate = 318.46] [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da31597011434824385] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (217.49,286.05) -- (202.64,271.64) ;
		\draw [shift={(219.64,288.14)}, rotate = 224.14] [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da26920104318970295] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (479.15,349.84) -- (399.64,489.23) ;
		\draw [shift={(480.64,347.23)}, rotate = 119.7] [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da38598903670227913] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (126.71,279.72) -- (91.37,122.64) ;
		\draw [shift={(127.37,282.64)}, rotate = 257.32] [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		
		% Text Node
		\draw (112,405) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\scriptsize Quarter 1}};
		% Text Node
		\draw (227,405) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\scriptsize Quarter 2}};
		% Text Node
		\draw (339,405) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\scriptsize Quarter 3}};
		% Text Node
		\draw (458,405) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\scriptsize Quarter 4}};
		% Text Node
		\draw (65,405.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$0$};
		% Text Node
		\draw (58,366.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$20$};
		% Text Node
		\draw (58,337.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$40$};
		% Text Node
		\draw (282,424) node [anchor=north west][inner sep=0.75pt]   [align=left] {Quarter};
		% Text Node
		\draw (26.9,312.62) node [anchor=north west][inner sep=0.75pt]  [rotate=-269.15] [align=left] {Minutes};
		% Text Node
		\draw (58,306.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$60$};
		% Text Node
		\draw (58,275.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$80$};
		% Text Node
		\draw (51,244.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$100$};
		% Text Node
		\draw (51,217.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$120$};
		% Text Node
		\draw (51,183.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$140$};
		% Text Node
		\draw (51,156.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$160$};
		% Text Node
		\draw (224,110) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{Analysis of Train Arrival Delay}};
		% Text Node
		\draw (85,445) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,opacity=1 ] [align=left] {{\footnotesize light outlier (<Q1-1.5IQR in Microsoft Excel)}};
		% Text Node
		\draw (136,136.19) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,opacity=1 ] [align=left] {{\footnotesize extreme outlier (Q3+3IQR in some softwares)}};
		% Text Node
		\draw (399.5,161.19) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,opacity=1 ] [align=left] {{\footnotesize light outlier (<Q3+1.5IQR in Microsoft Excel)}};
		% Text Node
		\draw (268.5,192.69) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,opacity=1 ] [align=left] {{\footnotesize 3rd quartile Q3}};
		% Text Node
		\draw (376.5,358.69) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,opacity=1 ] [align=left] {{\footnotesize 1st quartile Q1}};
		% Text Node
		\draw (517,266.19) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,opacity=1 ] [align=left] {{\footnotesize median (2nd quartile)}};
		% Text Node
		\draw (176,254.19) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,opacity=1 ] [align=left] {{\footnotesize average}};
		% Text Node
		\draw (353.5,493.46) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textcolor[rgb]{0.61,0.61,0.61}{{\footnotesize whisker extends to}}\\\textcolor[rgb]{0.61,0.61,0.61}{{\footnotesize smallest data point within}}\\\textcolor[rgb]{0.61,0.61,0.61}{{\footnotesize 1.5IQR from 1st quartile}}};
		% Text Node
		\draw (59.5,68) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textcolor[rgb]{0.61,0.61,0.61}{{\footnotesize whisker extends to}}\\\textcolor[rgb]{0.61,0.61,0.61}{{\footnotesize biggest data point within}}\\\textcolor[rgb]{0.61,0.61,0.61}{{\footnotesize 1.5IQR from 3rd quartile}}};
		\end{tikzpicture}
		}
		\vspace*{3mm}
		\caption{Box \& Whiskers plot ideal chart type}
	\end{figure}
	Another very important mental representation of box plots is the following (it can get an idea of the asymmetry of the distribution as is able to do the \texttt{R} software):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,927); %set diagram left start at 0, and has height of 927
		
		%Shape: Polygon Curved [id:ds7619555609434494] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 186; green, 183; blue, 183 }  ,fill opacity=1 ] (308,67) .. controls (317.05,66.78) and (325.95,70.68) .. (345,110) .. controls (364.05,149.32) and (375.37,188.28) .. (403.37,228.28) .. controls (431.37,268.28) and (476.37,269.28) .. (476.37,269.28) .. controls (476.37,269.28) and (136.37,270.28) .. (137.37,269.28) .. controls (138.37,268.28) and (177.97,275.7) .. (211.37,231.28) .. controls (244.78,186.86) and (253,150.78) .. (272,111.78) .. controls (291,72.78) and (298.95,67.22) .. (308,67) -- cycle ;
		%Straight Lines [id:da254576529253034] 
		\draw    (153,273) -- (460.37,273) ;
		%Shape: Rectangle [id:dp8632686815937063] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 143; green, 190; blue, 243 }  ,fill opacity=1 ] (271,302) -- (346.37,302) -- (346.37,362.28) -- (271,362.28) -- cycle ;
		%Straight Lines [id:da7830904357733115] 
		\draw [color={rgb, 255:red, 143; green, 190; blue, 243 }  ,draw opacity=1 ]   (166,332.14) -- (451.37,332.14) ;
		\draw [shift={(451.37,332.14)}, rotate = 180] [color={rgb, 255:red, 143; green, 190; blue, 243 }  ,draw opacity=1 ][line width=0.75]    (0,11.18) -- (0,-11.18)   ;
		\draw [shift={(166,332.14)}, rotate = 180] [color={rgb, 255:red, 143; green, 190; blue, 243 }  ,draw opacity=1 ][line width=0.75]    (0,11.18) -- (0,-11.18)   ;
		%Straight Lines [id:da9364772267772354] 
		\draw    (308.69,302.28) -- (308.69,362.19) ;
		%Straight Lines [id:da4394141555509081] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (272,111.78) -- (272,290.28) ;
		%Straight Lines [id:da356615403666523] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (308,67) -- (308,293.28) ;
		%Straight Lines [id:da3348869647475474] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (345,110) -- (345,288.28) ;
		
		% Text Node
		\draw (258,367.4) node [anchor=north west][inner sep=0.75pt]    {$Q_{1}$};
		% Text Node
		\draw (338,367.4) node [anchor=north west][inner sep=0.75pt]    {$Q_{3}$};
		% Text Node
		\draw (311,326.4) node [anchor=north west][inner sep=0.75pt]    {$M$};
		% Text Node
		\draw (289,44) node [anchor=north west][inner sep=0.75pt]   [align=left] {Mode};
		\end{tikzpicture}
	\end{figure}
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,927); %set diagram left start at 0, and has height of 927
		
		%Shape: Polygon Curved [id:ds8644336824347836] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 186; green, 183; blue, 183 }  ,fill opacity=1 ] (390.91,43.23) .. controls (411.91,42.73) and (439.41,149.05) .. (484.41,202.64) .. controls (529.41,256.23) and (600.87,258.01) .. (600.87,258.01) .. controls (600.87,258.01) and (340.49,257.46) .. (343.49,257.46) .. controls (346.49,257.46) and (351.41,198.73) .. (354.91,173.73) .. controls (358.41,148.73) and (369.91,43.73) .. (390.91,43.23) -- cycle ;
		%Shape: Polygon Curved [id:ds04935333475055659] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 186; green, 183; blue, 183 }  ,fill opacity=1 ] (280.87,43.64) .. controls (301.87,43.64) and (309.73,142.78) .. (314.73,171.28) .. controls (319.73,199.78) and (324.23,257.19) .. (326.73,257.23) .. controls (329.23,257.28) and (68.87,257.65) .. (69.87,256.65) .. controls (70.87,255.65) and (146.87,262.64) .. (192.87,202.64) .. controls (238.87,142.64) and (259.87,43.64) .. (280.87,43.64) -- cycle ;
		%Shape: Rectangle [id:dp6161685604418481] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 143; green, 190; blue, 243 }  ,fill opacity=1 ] (200,287.73) -- (293.37,287.73) -- (293.37,346.74) -- (200,346.74) -- cycle ;
		%Shape: Rectangle [id:dp741987523611314] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 143; green, 190; blue, 243 }  ,fill opacity=1 ] (362.37,287.73) -- (457.37,287.73) -- (457.37,346.74) -- (362.37,346.74) -- cycle ;
		%Straight Lines [id:da9394464961572762] 
		\draw    (251.69,287.28) -- (251.69,347.19) ;
		%Straight Lines [id:da4007173019561856] 
		\draw    (401.69,287.28) -- (401.69,347.19) ;
		%Straight Lines [id:da259361111594151] 
		\draw [color={rgb, 255:red, 143; green, 190; blue, 243 }  ,draw opacity=1 ]   (92.37,317.23) -- (325.37,317.23) ;
		\draw [shift={(325.37,317.23)}, rotate = 180] [color={rgb, 255:red, 143; green, 190; blue, 243 }  ,draw opacity=1 ][line width=0.75]    (0,11.18) -- (0,-11.18)   ;
		\draw [shift={(92.37,317.23)}, rotate = 180] [color={rgb, 255:red, 143; green, 190; blue, 243 }  ,draw opacity=1 ][line width=0.75]    (0,11.18) -- (0,-11.18)   ;
		%Straight Lines [id:da5867835623246576] 
		\draw [color={rgb, 255:red, 143; green, 190; blue, 243 }  ,draw opacity=1 ]   (345.37,317.23) -- (577.37,317.23) ;
		\draw [shift={(577.37,317.23)}, rotate = 180] [color={rgb, 255:red, 143; green, 190; blue, 243 }  ,draw opacity=1 ][line width=0.75]    (0,11.18) -- (0,-11.18)   ;
		\draw [shift={(345.37,317.23)}, rotate = 180] [color={rgb, 255:red, 143; green, 190; blue, 243 }  ,draw opacity=1 ][line width=0.75]    (0,11.18) -- (0,-11.18)   ;
		%Straight Lines [id:da8478039556667585] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (290,53.37) -- (290,275.64) ;
		%Straight Lines [id:da7129655957623331] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (251.69,94.96) -- (251.69,271.64) ;
		%Straight Lines [id:da5388093209474654] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (202.69,190.96) -- (202.69,273.64) ;
		%Straight Lines [id:da04576014341081813] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (402,54.37) -- (402,276.64) ;
		%Straight Lines [id:da27236795743993913] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (455.69,162.96) -- (455.69,275.64) ;
		%Straight Lines [id:da5919238557869084] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (364.69,116.96) -- (364.69,277.64) ;
		%Straight Lines [id:da5208973575472966] 
		\draw    (93,264) -- (326.37,264) ;
		%Straight Lines [id:da4879749420913386] 
		\draw    (342,264) -- (584.37,264) ;
		
		% Text Node
		\draw (188,350.4) node [anchor=north west][inner sep=0.75pt]    {$Q_{1}$};
		% Text Node
		\draw (281,350.4) node [anchor=north west][inner sep=0.75pt]    {$Q_{3}$};
		% Text Node
		\draw (353,350.4) node [anchor=north west][inner sep=0.75pt]    {$Q_{1}$};
		% Text Node
		\draw (446,350.4) node [anchor=north west][inner sep=0.75pt]    {$Q_{3}$};
		% Text Node
		\draw (255,309.63) node [anchor=north west][inner sep=0.75pt]    {$M$};
		% Text Node
		\draw (404,309.63) node [anchor=north west][inner sep=0.75pt]    {$M$};
		% Text Node
		\draw (259,22.28) node [anchor=north west][inner sep=0.75pt]   [align=left] {Mode};
		% Text Node
		\draw (370,22.28) node [anchor=north west][inner sep=0.75pt]   [align=left] {Mode};
		\end{tikzpicture} 
		\vspace*{3mm}
		\caption{Graphical representation of the mode, median and 1st + 3rd quartile compared to a distribution}
	\end{figure}
	However notice that Box \& Whiskers plots have important issues as depicted below:
	\begin{figure}[H]
		\centering
		\resizebox{0.9\textwidth}{!}{
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,927); %set diagram left start at 0, and has height of 927
		
		%Straight Lines [id:da37667994536957283] 
		\draw [color={rgb, 255:red, 187; green, 182; blue, 182 }  ,draw opacity=1 ]   (110,86) -- (149.37,86) ;
		%Straight Lines [id:da7090778700724139] 
		\draw [color={rgb, 255:red, 187; green, 182; blue, 182 }  ,draw opacity=1 ]   (110,118) -- (149.37,118) ;
		%Straight Lines [id:da9792143217039238] 
		\draw [color={rgb, 255:red, 187; green, 182; blue, 182 }  ,draw opacity=1 ]   (110,151) -- (149.37,151) ;
		%Straight Lines [id:da7412925344502541] 
		\draw [color={rgb, 255:red, 187; green, 182; blue, 182 }  ,draw opacity=1 ]   (110,182) -- (149.37,182) ;
		%Straight Lines [id:da8995467181361456] 
		\draw [color={rgb, 255:red, 187; green, 182; blue, 182 }  ,draw opacity=1 ]   (110,214) -- (149.37,214) ;
		%Straight Lines [id:da2729903241296985] 
		\draw [color={rgb, 255:red, 187; green, 182; blue, 182 }  ,draw opacity=1 ]   (110,246) -- (149.37,246) ;
		%Shape: Rectangle [id:dp8518201175997533] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (119.5,132.36) -- (139.87,132.36) -- (139.87,231.64) -- (119.5,231.64) -- cycle ;
		%Straight Lines [id:da6159555060956317] 
		\draw [color={rgb, 255:red, 156; green, 195; blue, 237 }  ,draw opacity=1 ]   (129.69,92) -- (129.69,244.25) ;
		\draw [shift={(129.69,244.25)}, rotate = 270] [color={rgb, 255:red, 156; green, 195; blue, 237 }  ,draw opacity=1 ][line width=0.75]    (0,5.59) -- (0,-5.59)   ;
		\draw [shift={(129.69,92)}, rotate = 270] [color={rgb, 255:red, 156; green, 195; blue, 237 }  ,draw opacity=1 ][line width=0.75]    (0,5.59) -- (0,-5.59)   ;
		%Curve Lines [id:da06042297443596767] 
		\draw    (56.37,208.28) .. controls (53.81,225.12) and (79.06,240.49) .. (117.4,239.4) ;
		\draw [shift={(120.37,239.28)}, rotate = 177.14] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da4668399896245057] 
		\draw [color={rgb, 255:red, 187; green, 182; blue, 182 }  ,draw opacity=1 ]   (261.37,249.28) -- (261.37,86) ;
		%Straight Lines [id:da5865344799452248] 
		\draw [color={rgb, 255:red, 187; green, 182; blue, 182 }  ,draw opacity=1 ]   (292.37,249.28) -- (292.37,86) ;
		%Straight Lines [id:da812410948664662] 
		\draw [color={rgb, 255:red, 187; green, 182; blue, 182 }  ,draw opacity=1 ]   (323.37,249.28) -- (323.37,86) ;
		%Shape: Rectangle [id:dp3619181631198758] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (311.05,231.12) -- (311.12,245.12) -- (230.37,245.53) -- (230.3,231.53) -- cycle ;
		%Shape: Rectangle [id:dp9456621419439408] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (258.13,197.78) -- (258.22,214.64) -- (230.13,214.78) -- (230.05,197.92) -- cycle ;
		%Shape: Rectangle [id:dp37278632482933927] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (254.94,180.13) -- (255.03,196.39) -- (230.13,196.51) -- (230.05,180.26) -- cycle ;
		%Shape: Rectangle [id:dp09294594511022769] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (245.95,164.52) -- (246.02,178.79) -- (230.12,178.87) -- (230.05,164.6) -- cycle ;
		%Shape: Rectangle [id:dp7279388333002053] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (281.77,147.81) -- (281.85,162.99) -- (230.13,163.26) -- (230.05,148.07) -- cycle ;
		%Shape: Rectangle [id:dp3514373258118577] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (270.77,132.63) -- (270.84,146.34) -- (230.12,146.55) -- (230.05,132.83) -- cycle ;
		%Shape: Rectangle [id:dp128067249859247] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (264.77,117.47) -- (264.84,131.19) -- (230.12,131.37) -- (230.05,117.65) -- cycle ;
		%Shape: Rectangle [id:dp636093690216492] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (247.26,102.4) -- (247.33,116.12) -- (230.12,116.21) -- (230.05,102.49) -- cycle ;
		%Shape: Rectangle [id:dp1294604399275967] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (252.76,87.3) -- (252.83,101.02) -- (230.12,101.14) -- (230.05,87.42) -- cycle ;
		%Straight Lines [id:da9899034126616082] 
		\draw [color={rgb, 255:red, 187; green, 182; blue, 182 }  ,draw opacity=1 ]   (230.37,249.28) -- (230.37,86) ;
		%Curve Lines [id:da00582701973251476] 
		\draw    (318.1,200.82) .. controls (315.98,214.73) and (312.09,222.13) .. (309.6,226.3) ;
		\draw [shift={(308.1,228.82)}, rotate = 299.74] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da2303740035614179] 
		\draw [color={rgb, 255:red, 187; green, 182; blue, 182 }  ,draw opacity=1 ]   (364,90.05) -- (429.6,90.05) ;
		%Straight Lines [id:da1443758111031832] 
		\draw [color={rgb, 255:red, 187; green, 182; blue, 182 }  ,draw opacity=1 ]   (364,123.05) -- (429.6,123.05) ;
		%Straight Lines [id:da31925503151188517] 
		\draw [color={rgb, 255:red, 187; green, 182; blue, 182 }  ,draw opacity=1 ]   (364,154.05) -- (429.6,154.05) ;
		%Straight Lines [id:da898493596958283] 
		\draw [color={rgb, 255:red, 187; green, 182; blue, 182 }  ,draw opacity=1 ]   (364,186.05) -- (429.6,186.05) ;
		%Straight Lines [id:da6633789680971658] 
		\draw [color={rgb, 255:red, 187; green, 182; blue, 182 }  ,draw opacity=1 ]   (364,218.05) -- (429.6,218.05) ;
		%Shape: Rectangle [id:dp7983919995382809] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (373.5,155.41) -- (393.87,155.41) -- (393.87,189.41) -- (373.5,189.41) -- cycle ;
		%Straight Lines [id:da9439440463142785] 
		\draw [color={rgb, 255:red, 156; green, 195; blue, 237 }  ,draw opacity=1 ]   (383.69,124.91) -- (383.69,232.1) ;
		\draw [shift={(383.69,232.1)}, rotate = 270] [color={rgb, 255:red, 156; green, 195; blue, 237 }  ,draw opacity=1 ][line width=0.75]    (0,5.59) -- (0,-5.59)   ;
		\draw [shift={(383.69,124.91)}, rotate = 270] [color={rgb, 255:red, 156; green, 195; blue, 237 }  ,draw opacity=1 ][line width=0.75]    (0,5.59) -- (0,-5.59)   ;
		%Straight Lines [id:da010827851420569967] 
		\draw [color={rgb, 255:red, 187; green, 182; blue, 182 }  ,draw opacity=1 ]   (363.5,249.55) -- (429.6,249.55) ;
		%Straight Lines [id:da9708698849969031] 
		\draw    (119.6,162.73) -- (139.6,162.73) ;
		%Straight Lines [id:da5614877842647832] 
		\draw    (374.1,173.73) -- (393.6,173.73) ;
		%Shape: Rectangle [id:dp15147344736785628] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (404.5,155.91) -- (424.87,155.91) -- (424.87,189.91) -- (404.5,189.91) -- cycle ;
		%Straight Lines [id:da09299215504392344] 
		\draw [color={rgb, 255:red, 156; green, 195; blue, 237 }  ,draw opacity=1 ]   (414.69,127.41) -- (414.69,230.64) ;
		\draw [shift={(414.69,230.64)}, rotate = 270] [color={rgb, 255:red, 156; green, 195; blue, 237 }  ,draw opacity=1 ][line width=0.75]    (0,5.59) -- (0,-5.59)   ;
		\draw [shift={(414.69,127.41)}, rotate = 270] [color={rgb, 255:red, 156; green, 195; blue, 237 }  ,draw opacity=1 ][line width=0.75]    (0,5.59) -- (0,-5.59)   ;
		%Straight Lines [id:da23455215106360572] 
		\draw    (405.1,174.23) -- (424.6,174.23) ;
		%Curve Lines [id:da9344051074288542] 
		\draw    (436.1,108.78) .. controls (436.1,124.84) and (429.61,130.63) .. (426.04,134.1) ;
		\draw [shift={(424.1,136.32)}, rotate = 299.74] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da49923206457654157] 
		\draw [color={rgb, 255:red, 187; green, 182; blue, 182 }  ,draw opacity=1 ]   (524,90.05) -- (589.6,90.05) ;
		%Straight Lines [id:da7132823753545874] 
		\draw [color={rgb, 255:red, 187; green, 182; blue, 182 }  ,draw opacity=1 ]   (524,123.05) -- (589.6,123.05) ;
		%Straight Lines [id:da8110065491824006] 
		\draw [color={rgb, 255:red, 187; green, 182; blue, 182 }  ,draw opacity=1 ]   (524,154.05) -- (589.6,154.05) ;
		%Straight Lines [id:da8990330245009657] 
		\draw [color={rgb, 255:red, 187; green, 182; blue, 182 }  ,draw opacity=1 ]   (524,186.05) -- (589.6,186.05) ;
		%Straight Lines [id:da9777730498288295] 
		\draw [color={rgb, 255:red, 187; green, 182; blue, 182 }  ,draw opacity=1 ]   (524,218.05) -- (589.6,218.05) ;
		%Straight Lines [id:da6621667993815661] 
		\draw [color={rgb, 255:red, 187; green, 182; blue, 182 }  ,draw opacity=1 ]   (523.5,249.55) -- (589.6,249.55) ;
		%Shape: Circle [id:dp2931987666731306] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (542.32,126.39) .. controls (542.32,125.65) and (542.92,125.05) .. (543.66,125.05) .. controls (544.4,125.05) and (545,125.65) .. (545,126.39) .. controls (545,127.13) and (544.4,127.73) .. (543.66,127.73) .. controls (542.92,127.73) and (542.32,127.13) .. (542.32,126.39) -- cycle ;
		%Shape: Circle [id:dp24997698472650098] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (540.32,131.71) .. controls (540.32,130.97) and (540.92,130.37) .. (541.66,130.37) .. controls (542.4,130.37) and (543,130.97) .. (543,131.71) .. controls (543,132.45) and (542.4,133.05) .. (541.66,133.05) .. controls (540.92,133.05) and (540.32,132.45) .. (540.32,131.71) -- cycle ;
		%Shape: Circle [id:dp3809919560914736] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (534.32,136.39) .. controls (534.32,135.65) and (534.92,135.05) .. (535.66,135.05) .. controls (536.4,135.05) and (537,135.65) .. (537,136.39) .. controls (537,137.13) and (536.4,137.73) .. (535.66,137.73) .. controls (534.92,137.73) and (534.32,137.13) .. (534.32,136.39) -- cycle ;
		%Shape: Circle [id:dp6254973540295372] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (540.32,137.39) .. controls (540.32,136.65) and (540.92,136.05) .. (541.66,136.05) .. controls (542.4,136.05) and (543,136.65) .. (543,137.39) .. controls (543,138.13) and (542.4,138.73) .. (541.66,138.73) .. controls (540.92,138.73) and (540.32,138.13) .. (540.32,137.39) -- cycle ;
		%Shape: Circle [id:dp4381334417110416] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (540.32,141.39) .. controls (540.32,140.65) and (540.92,140.05) .. (541.66,140.05) .. controls (542.4,140.05) and (543,140.65) .. (543,141.39) .. controls (543,142.13) and (542.4,142.73) .. (541.66,142.73) .. controls (540.92,142.73) and (540.32,142.13) .. (540.32,141.39) -- cycle ;
		%Shape: Circle [id:dp8224393669342429] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (535.32,143.39) .. controls (535.32,142.65) and (535.92,142.05) .. (536.66,142.05) .. controls (537.4,142.05) and (538,142.65) .. (538,143.39) .. controls (538,144.13) and (537.4,144.73) .. (536.66,144.73) .. controls (535.92,144.73) and (535.32,144.13) .. (535.32,143.39) -- cycle ;
		%Shape: Circle [id:dp14481512738195734] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (537.32,144.39) .. controls (537.32,143.65) and (537.92,143.05) .. (538.66,143.05) .. controls (539.4,143.05) and (540,143.65) .. (540,144.39) .. controls (540,145.13) and (539.4,145.73) .. (538.66,145.73) .. controls (537.92,145.73) and (537.32,145.13) .. (537.32,144.39) -- cycle ;
		%Shape: Circle [id:dp6062487612115579] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (540.32,145.39) .. controls (540.32,144.65) and (540.92,144.05) .. (541.66,144.05) .. controls (542.4,144.05) and (543,144.65) .. (543,145.39) .. controls (543,146.13) and (542.4,146.73) .. (541.66,146.73) .. controls (540.92,146.73) and (540.32,146.13) .. (540.32,145.39) -- cycle ;
		%Shape: Circle [id:dp1498924382313327] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (537.32,147.39) .. controls (537.32,146.65) and (537.92,146.05) .. (538.66,146.05) .. controls (539.4,146.05) and (540,146.65) .. (540,147.39) .. controls (540,148.13) and (539.4,148.73) .. (538.66,148.73) .. controls (537.92,148.73) and (537.32,148.13) .. (537.32,147.39) -- cycle ;
		%Shape: Circle [id:dp24598739003003467] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (540.32,148.39) .. controls (540.32,147.65) and (540.92,147.05) .. (541.66,147.05) .. controls (542.4,147.05) and (543,147.65) .. (543,148.39) .. controls (543,149.13) and (542.4,149.73) .. (541.66,149.73) .. controls (540.92,149.73) and (540.32,149.13) .. (540.32,148.39) -- cycle ;
		%Shape: Circle [id:dp9532727300967332] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (537.32,149.39) .. controls (537.32,148.65) and (537.92,148.05) .. (538.66,148.05) .. controls (539.4,148.05) and (540,148.65) .. (540,149.39) .. controls (540,150.13) and (539.4,150.73) .. (538.66,150.73) .. controls (537.92,150.73) and (537.32,150.13) .. (537.32,149.39) -- cycle ;
		%Shape: Circle [id:dp8377776250624906] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (536.32,153.39) .. controls (536.32,152.65) and (536.92,152.05) .. (537.66,152.05) .. controls (538.4,152.05) and (539,152.65) .. (539,153.39) .. controls (539,154.13) and (538.4,154.73) .. (537.66,154.73) .. controls (536.92,154.73) and (536.32,154.13) .. (536.32,153.39) -- cycle ;
		%Shape: Circle [id:dp47226469378699854] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (534.32,153.39) .. controls (534.32,152.65) and (534.92,152.05) .. (535.66,152.05) .. controls (536.4,152.05) and (537,152.65) .. (537,153.39) .. controls (537,154.13) and (536.4,154.73) .. (535.66,154.73) .. controls (534.92,154.73) and (534.32,154.13) .. (534.32,153.39) -- cycle ;
		%Shape: Circle [id:dp4515506503503188] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (538.32,155.39) .. controls (538.32,154.65) and (538.92,154.05) .. (539.66,154.05) .. controls (540.4,154.05) and (541,154.65) .. (541,155.39) .. controls (541,156.13) and (540.4,156.73) .. (539.66,156.73) .. controls (538.92,156.73) and (538.32,156.13) .. (538.32,155.39) -- cycle ;
		%Shape: Circle [id:dp7947954279000087] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (533.32,150.39) .. controls (533.32,149.65) and (533.92,149.05) .. (534.66,149.05) .. controls (535.4,149.05) and (536,149.65) .. (536,150.39) .. controls (536,151.13) and (535.4,151.73) .. (534.66,151.73) .. controls (533.92,151.73) and (533.32,151.13) .. (533.32,150.39) -- cycle ;
		%Shape: Circle [id:dp45891166773174263] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (531.32,148.39) .. controls (531.32,147.65) and (531.92,147.05) .. (532.66,147.05) .. controls (533.4,147.05) and (534,147.65) .. (534,148.39) .. controls (534,149.13) and (533.4,149.73) .. (532.66,149.73) .. controls (531.92,149.73) and (531.32,149.13) .. (531.32,148.39) -- cycle ;
		%Shape: Circle [id:dp577146825211541] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (540.32,152.39) .. controls (540.32,151.65) and (540.92,151.05) .. (541.66,151.05) .. controls (542.4,151.05) and (543,151.65) .. (543,152.39) .. controls (543,153.13) and (542.4,153.73) .. (541.66,153.73) .. controls (540.92,153.73) and (540.32,153.13) .. (540.32,152.39) -- cycle ;
		%Shape: Circle [id:dp2517341414895775] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (542.32,152.39) .. controls (542.32,151.65) and (542.92,151.05) .. (543.66,151.05) .. controls (544.4,151.05) and (545,151.65) .. (545,152.39) .. controls (545,153.13) and (544.4,153.73) .. (543.66,153.73) .. controls (542.92,153.73) and (542.32,153.13) .. (542.32,152.39) -- cycle ;
		%Shape: Circle [id:dp4772907945071314] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (543.32,150.39) .. controls (543.32,149.65) and (543.92,149.05) .. (544.66,149.05) .. controls (545.4,149.05) and (546,149.65) .. (546,150.39) .. controls (546,151.13) and (545.4,151.73) .. (544.66,151.73) .. controls (543.92,151.73) and (543.32,151.13) .. (543.32,150.39) -- cycle ;
		%Shape: Circle [id:dp4770102766094797] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (544.32,147.39) .. controls (544.32,146.65) and (544.92,146.05) .. (545.66,146.05) .. controls (546.4,146.05) and (547,146.65) .. (547,147.39) .. controls (547,148.13) and (546.4,148.73) .. (545.66,148.73) .. controls (544.92,148.73) and (544.32,148.13) .. (544.32,147.39) -- cycle ;
		%Shape: Circle [id:dp0010768285257956034] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (535.32,157.39) .. controls (535.32,156.65) and (535.92,156.05) .. (536.66,156.05) .. controls (537.4,156.05) and (538,156.65) .. (538,157.39) .. controls (538,158.13) and (537.4,158.73) .. (536.66,158.73) .. controls (535.92,158.73) and (535.32,158.13) .. (535.32,157.39) -- cycle ;
		%Shape: Circle [id:dp9197394375965426] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (539.32,159.39) .. controls (539.32,158.65) and (539.92,158.05) .. (540.66,158.05) .. controls (541.4,158.05) and (542,158.65) .. (542,159.39) .. controls (542,160.13) and (541.4,160.73) .. (540.66,160.73) .. controls (539.92,160.73) and (539.32,160.13) .. (539.32,159.39) -- cycle ;
		%Shape: Circle [id:dp835268823004264] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (541.32,159.39) .. controls (541.32,158.65) and (541.92,158.05) .. (542.66,158.05) .. controls (543.4,158.05) and (544,158.65) .. (544,159.39) .. controls (544,160.13) and (543.4,160.73) .. (542.66,160.73) .. controls (541.92,160.73) and (541.32,160.13) .. (541.32,159.39) -- cycle ;
		%Shape: Circle [id:dp29879721630071376] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (536.32,163.39) .. controls (536.32,162.65) and (536.92,162.05) .. (537.66,162.05) .. controls (538.4,162.05) and (539,162.65) .. (539,163.39) .. controls (539,164.13) and (538.4,164.73) .. (537.66,164.73) .. controls (536.92,164.73) and (536.32,164.13) .. (536.32,163.39) -- cycle ;
		%Shape: Circle [id:dp4774524372254616] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (539.32,164.39) .. controls (539.32,163.65) and (539.92,163.05) .. (540.66,163.05) .. controls (541.4,163.05) and (542,163.65) .. (542,164.39) .. controls (542,165.13) and (541.4,165.73) .. (540.66,165.73) .. controls (539.92,165.73) and (539.32,165.13) .. (539.32,164.39) -- cycle ;
		%Shape: Circle [id:dp9834372097703028] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (534.32,165.39) .. controls (534.32,164.65) and (534.92,164.05) .. (535.66,164.05) .. controls (536.4,164.05) and (537,164.65) .. (537,165.39) .. controls (537,166.13) and (536.4,166.73) .. (535.66,166.73) .. controls (534.92,166.73) and (534.32,166.13) .. (534.32,165.39) -- cycle ;
		%Shape: Circle [id:dp4840994734708861] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (537.32,166.39) .. controls (537.32,165.65) and (537.92,165.05) .. (538.66,165.05) .. controls (539.4,165.05) and (540,165.65) .. (540,166.39) .. controls (540,167.13) and (539.4,167.73) .. (538.66,167.73) .. controls (537.92,167.73) and (537.32,167.13) .. (537.32,166.39) -- cycle ;
		%Shape: Circle [id:dp15457493067860417] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (541.32,166.39) .. controls (541.32,165.65) and (541.92,165.05) .. (542.66,165.05) .. controls (543.4,165.05) and (544,165.65) .. (544,166.39) .. controls (544,167.13) and (543.4,167.73) .. (542.66,167.73) .. controls (541.92,167.73) and (541.32,167.13) .. (541.32,166.39) -- cycle ;
		%Shape: Circle [id:dp23062374935166918] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (538.32,170.39) .. controls (538.32,169.65) and (538.92,169.05) .. (539.66,169.05) .. controls (540.4,169.05) and (541,169.65) .. (541,170.39) .. controls (541,171.13) and (540.4,171.73) .. (539.66,171.73) .. controls (538.92,171.73) and (538.32,171.13) .. (538.32,170.39) -- cycle ;
		%Shape: Circle [id:dp22048964257227088] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (534.32,171.39) .. controls (534.32,170.65) and (534.92,170.05) .. (535.66,170.05) .. controls (536.4,170.05) and (537,170.65) .. (537,171.39) .. controls (537,172.13) and (536.4,172.73) .. (535.66,172.73) .. controls (534.92,172.73) and (534.32,172.13) .. (534.32,171.39) -- cycle ;
		%Shape: Circle [id:dp5040779235003283] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (541.32,172.39) .. controls (541.32,171.65) and (541.92,171.05) .. (542.66,171.05) .. controls (543.4,171.05) and (544,171.65) .. (544,172.39) .. controls (544,173.13) and (543.4,173.73) .. (542.66,173.73) .. controls (541.92,173.73) and (541.32,173.13) .. (541.32,172.39) -- cycle ;
		%Shape: Circle [id:dp049620044011772] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (536.32,173.39) .. controls (536.32,172.65) and (536.92,172.05) .. (537.66,172.05) .. controls (538.4,172.05) and (539,172.65) .. (539,173.39) .. controls (539,174.13) and (538.4,174.73) .. (537.66,174.73) .. controls (536.92,174.73) and (536.32,174.13) .. (536.32,173.39) -- cycle ;
		%Shape: Circle [id:dp449375205138123] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (539.32,174.39) .. controls (539.32,173.65) and (539.92,173.05) .. (540.66,173.05) .. controls (541.4,173.05) and (542,173.65) .. (542,174.39) .. controls (542,175.13) and (541.4,175.73) .. (540.66,175.73) .. controls (539.92,175.73) and (539.32,175.13) .. (539.32,174.39) -- cycle ;
		%Shape: Circle [id:dp6318141925635392] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (538.32,174.39) .. controls (538.32,173.65) and (538.92,173.05) .. (539.66,173.05) .. controls (540.4,173.05) and (541,173.65) .. (541,174.39) .. controls (541,175.13) and (540.4,175.73) .. (539.66,175.73) .. controls (538.92,175.73) and (538.32,175.13) .. (538.32,174.39) -- cycle ;
		%Shape: Circle [id:dp13875854729338188] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (534.32,176.39) .. controls (534.32,175.65) and (534.92,175.05) .. (535.66,175.05) .. controls (536.4,175.05) and (537,175.65) .. (537,176.39) .. controls (537,177.13) and (536.4,177.73) .. (535.66,177.73) .. controls (534.92,177.73) and (534.32,177.13) .. (534.32,176.39) -- cycle ;
		%Shape: Circle [id:dp4591229499417986] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (538.32,177.39) .. controls (538.32,176.65) and (538.92,176.05) .. (539.66,176.05) .. controls (540.4,176.05) and (541,176.65) .. (541,177.39) .. controls (541,178.13) and (540.4,178.73) .. (539.66,178.73) .. controls (538.92,178.73) and (538.32,178.13) .. (538.32,177.39) -- cycle ;
		%Shape: Circle [id:dp1209017448426788] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (542.32,176.39) .. controls (542.32,175.65) and (542.92,175.05) .. (543.66,175.05) .. controls (544.4,175.05) and (545,175.65) .. (545,176.39) .. controls (545,177.13) and (544.4,177.73) .. (543.66,177.73) .. controls (542.92,177.73) and (542.32,177.13) .. (542.32,176.39) -- cycle ;
		%Shape: Circle [id:dp07461394456309867] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (535.32,180.39) .. controls (535.32,179.65) and (535.92,179.05) .. (536.66,179.05) .. controls (537.4,179.05) and (538,179.65) .. (538,180.39) .. controls (538,181.13) and (537.4,181.73) .. (536.66,181.73) .. controls (535.92,181.73) and (535.32,181.13) .. (535.32,180.39) -- cycle ;
		%Shape: Circle [id:dp30882008681920015] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (540.32,179.39) .. controls (540.32,178.65) and (540.92,178.05) .. (541.66,178.05) .. controls (542.4,178.05) and (543,178.65) .. (543,179.39) .. controls (543,180.13) and (542.4,180.73) .. (541.66,180.73) .. controls (540.92,180.73) and (540.32,180.13) .. (540.32,179.39) -- cycle ;
		%Shape: Circle [id:dp5533308637148151] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (538.32,181.39) .. controls (538.32,180.65) and (538.92,180.05) .. (539.66,180.05) .. controls (540.4,180.05) and (541,180.65) .. (541,181.39) .. controls (541,182.13) and (540.4,182.73) .. (539.66,182.73) .. controls (538.92,182.73) and (538.32,182.13) .. (538.32,181.39) -- cycle ;
		%Shape: Circle [id:dp3354193697226844] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (543.32,181.39) .. controls (543.32,180.65) and (543.92,180.05) .. (544.66,180.05) .. controls (545.4,180.05) and (546,180.65) .. (546,181.39) .. controls (546,182.13) and (545.4,182.73) .. (544.66,182.73) .. controls (543.92,182.73) and (543.32,182.13) .. (543.32,181.39) -- cycle ;
		%Shape: Circle [id:dp4487546916730427] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (537.32,184.39) .. controls (537.32,183.65) and (537.92,183.05) .. (538.66,183.05) .. controls (539.4,183.05) and (540,183.65) .. (540,184.39) .. controls (540,185.13) and (539.4,185.73) .. (538.66,185.73) .. controls (537.92,185.73) and (537.32,185.13) .. (537.32,184.39) -- cycle ;
		%Shape: Circle [id:dp9910309638558039] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (541.32,185.39) .. controls (541.32,184.65) and (541.92,184.05) .. (542.66,184.05) .. controls (543.4,184.05) and (544,184.65) .. (544,185.39) .. controls (544,186.13) and (543.4,186.73) .. (542.66,186.73) .. controls (541.92,186.73) and (541.32,186.13) .. (541.32,185.39) -- cycle ;
		%Shape: Circle [id:dp005695418582264633] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (539.32,189.39) .. controls (539.32,188.65) and (539.92,188.05) .. (540.66,188.05) .. controls (541.4,188.05) and (542,188.65) .. (542,189.39) .. controls (542,190.13) and (541.4,190.73) .. (540.66,190.73) .. controls (539.92,190.73) and (539.32,190.13) .. (539.32,189.39) -- cycle ;
		%Shape: Circle [id:dp19102038394790277] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (543.32,191.39) .. controls (543.32,190.65) and (543.92,190.05) .. (544.66,190.05) .. controls (545.4,190.05) and (546,190.65) .. (546,191.39) .. controls (546,192.13) and (545.4,192.73) .. (544.66,192.73) .. controls (543.92,192.73) and (543.32,192.13) .. (543.32,191.39) -- cycle ;
		%Shape: Circle [id:dp31554329008376025] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (534.32,193.39) .. controls (534.32,192.65) and (534.92,192.05) .. (535.66,192.05) .. controls (536.4,192.05) and (537,192.65) .. (537,193.39) .. controls (537,194.13) and (536.4,194.73) .. (535.66,194.73) .. controls (534.92,194.73) and (534.32,194.13) .. (534.32,193.39) -- cycle ;
		%Shape: Circle [id:dp38377102280088016] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (536.32,197.39) .. controls (536.32,196.65) and (536.92,196.05) .. (537.66,196.05) .. controls (538.4,196.05) and (539,196.65) .. (539,197.39) .. controls (539,198.13) and (538.4,198.73) .. (537.66,198.73) .. controls (536.92,198.73) and (536.32,198.13) .. (536.32,197.39) -- cycle ;
		%Shape: Circle [id:dp4247968792881671] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (538.32,195.39) .. controls (538.32,194.65) and (538.92,194.05) .. (539.66,194.05) .. controls (540.4,194.05) and (541,194.65) .. (541,195.39) .. controls (541,196.13) and (540.4,196.73) .. (539.66,196.73) .. controls (538.92,196.73) and (538.32,196.13) .. (538.32,195.39) -- cycle ;
		%Shape: Circle [id:dp514976031434385] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (534.32,201.39) .. controls (534.32,200.65) and (534.92,200.05) .. (535.66,200.05) .. controls (536.4,200.05) and (537,200.65) .. (537,201.39) .. controls (537,202.13) and (536.4,202.73) .. (535.66,202.73) .. controls (534.92,202.73) and (534.32,202.13) .. (534.32,201.39) -- cycle ;
		%Shape: Circle [id:dp049288930630815964] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (540.32,199.39) .. controls (540.32,198.65) and (540.92,198.05) .. (541.66,198.05) .. controls (542.4,198.05) and (543,198.65) .. (543,199.39) .. controls (543,200.13) and (542.4,200.73) .. (541.66,200.73) .. controls (540.92,200.73) and (540.32,200.13) .. (540.32,199.39) -- cycle ;
		%Shape: Circle [id:dp19349194715793838] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (543.32,197.39) .. controls (543.32,196.65) and (543.92,196.05) .. (544.66,196.05) .. controls (545.4,196.05) and (546,196.65) .. (546,197.39) .. controls (546,198.13) and (545.4,198.73) .. (544.66,198.73) .. controls (543.92,198.73) and (543.32,198.13) .. (543.32,197.39) -- cycle ;
		%Shape: Circle [id:dp5331502498520124] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (543.32,201.07) .. controls (543.32,200.33) and (543.92,199.73) .. (544.66,199.73) .. controls (545.4,199.73) and (546,200.33) .. (546,201.07) .. controls (546,201.81) and (545.4,202.41) .. (544.66,202.41) .. controls (543.92,202.41) and (543.32,201.81) .. (543.32,201.07) -- cycle ;
		%Shape: Circle [id:dp9234486001148388] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (538.32,204.07) .. controls (538.32,203.33) and (538.92,202.73) .. (539.66,202.73) .. controls (540.4,202.73) and (541,203.33) .. (541,204.07) .. controls (541,204.81) and (540.4,205.41) .. (539.66,205.41) .. controls (538.92,205.41) and (538.32,204.81) .. (538.32,204.07) -- cycle ;
		%Shape: Circle [id:dp1085911861787554] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (541.32,207.07) .. controls (541.32,206.33) and (541.92,205.73) .. (542.66,205.73) .. controls (543.4,205.73) and (544,206.33) .. (544,207.07) .. controls (544,207.81) and (543.4,208.41) .. (542.66,208.41) .. controls (541.92,208.41) and (541.32,207.81) .. (541.32,207.07) -- cycle ;
		%Shape: Circle [id:dp9530878033317625] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (537.32,209.07) .. controls (537.32,208.33) and (537.92,207.73) .. (538.66,207.73) .. controls (539.4,207.73) and (540,208.33) .. (540,209.07) .. controls (540,209.81) and (539.4,210.41) .. (538.66,210.41) .. controls (537.92,210.41) and (537.32,209.81) .. (537.32,209.07) -- cycle ;
		%Shape: Circle [id:dp6496558921649156] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (534.32,215.07) .. controls (534.32,214.33) and (534.92,213.73) .. (535.66,213.73) .. controls (536.4,213.73) and (537,214.33) .. (537,215.07) .. controls (537,215.81) and (536.4,216.41) .. (535.66,216.41) .. controls (534.92,216.41) and (534.32,215.81) .. (534.32,215.07) -- cycle ;
		%Shape: Circle [id:dp17965614599817337] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (539.32,213.07) .. controls (539.32,212.33) and (539.92,211.73) .. (540.66,211.73) .. controls (541.4,211.73) and (542,212.33) .. (542,213.07) .. controls (542,213.81) and (541.4,214.41) .. (540.66,214.41) .. controls (539.92,214.41) and (539.32,213.81) .. (539.32,213.07) -- cycle ;
		%Shape: Circle [id:dp6514421223504556] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (543.32,214.07) .. controls (543.32,213.33) and (543.92,212.73) .. (544.66,212.73) .. controls (545.4,212.73) and (546,213.33) .. (546,214.07) .. controls (546,214.81) and (545.4,215.41) .. (544.66,215.41) .. controls (543.92,215.41) and (543.32,214.81) .. (543.32,214.07) -- cycle ;
		%Shape: Circle [id:dp5224265273847672] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (544.32,215.07) .. controls (544.32,214.33) and (544.92,213.73) .. (545.66,213.73) .. controls (546.4,213.73) and (547,214.33) .. (547,215.07) .. controls (547,215.81) and (546.4,216.41) .. (545.66,216.41) .. controls (544.92,216.41) and (544.32,215.81) .. (544.32,215.07) -- cycle ;
		%Shape: Circle [id:dp6552929006564412] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (543.32,221.07) .. controls (543.32,220.33) and (543.92,219.73) .. (544.66,219.73) .. controls (545.4,219.73) and (546,220.33) .. (546,221.07) .. controls (546,221.81) and (545.4,222.41) .. (544.66,222.41) .. controls (543.92,222.41) and (543.32,221.81) .. (543.32,221.07) -- cycle ;
		%Shape: Circle [id:dp4447615265655125] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (541.32,224.07) .. controls (541.32,223.33) and (541.92,222.73) .. (542.66,222.73) .. controls (543.4,222.73) and (544,223.33) .. (544,224.07) .. controls (544,224.81) and (543.4,225.41) .. (542.66,225.41) .. controls (541.92,225.41) and (541.32,224.81) .. (541.32,224.07) -- cycle ;
		%Shape: Circle [id:dp7736393747975134] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (544.32,226.07) .. controls (544.32,225.33) and (544.92,224.73) .. (545.66,224.73) .. controls (546.4,224.73) and (547,225.33) .. (547,226.07) .. controls (547,226.81) and (546.4,227.41) .. (545.66,227.41) .. controls (544.92,227.41) and (544.32,226.81) .. (544.32,226.07) -- cycle ;
		%Shape: Circle [id:dp6904168266034414] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (542.32,228.07) .. controls (542.32,227.33) and (542.92,226.73) .. (543.66,226.73) .. controls (544.4,226.73) and (545,227.33) .. (545,228.07) .. controls (545,228.81) and (544.4,229.41) .. (543.66,229.41) .. controls (542.92,229.41) and (542.32,228.81) .. (542.32,228.07) -- cycle ;
		%Shape: Circle [id:dp43093492909944553] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (537.32,230.07) .. controls (537.32,229.33) and (537.92,228.73) .. (538.66,228.73) .. controls (539.4,228.73) and (540,229.33) .. (540,230.07) .. controls (540,230.81) and (539.4,231.41) .. (538.66,231.41) .. controls (537.92,231.41) and (537.32,230.81) .. (537.32,230.07) -- cycle ;
		%Shape: Circle [id:dp7572424359887395] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (543.32,233.07) .. controls (543.32,232.33) and (543.92,231.73) .. (544.66,231.73) .. controls (545.4,231.73) and (546,232.33) .. (546,233.07) .. controls (546,233.81) and (545.4,234.41) .. (544.66,234.41) .. controls (543.92,234.41) and (543.32,233.81) .. (543.32,233.07) -- cycle ;
		%Shape: Circle [id:dp11586287131938033] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (576.32,127.39) .. controls (576.32,126.65) and (576.92,126.05) .. (577.66,126.05) .. controls (578.4,126.05) and (579,126.65) .. (579,127.39) .. controls (579,128.13) and (578.4,128.73) .. (577.66,128.73) .. controls (576.92,128.73) and (576.32,128.13) .. (576.32,127.39) -- cycle ;
		%Shape: Circle [id:dp3364352443559102] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (573.32,130.39) .. controls (573.32,129.65) and (573.92,129.05) .. (574.66,129.05) .. controls (575.4,129.05) and (576,129.65) .. (576,130.39) .. controls (576,131.13) and (575.4,131.73) .. (574.66,131.73) .. controls (573.92,131.73) and (573.32,131.13) .. (573.32,130.39) -- cycle ;
		%Shape: Circle [id:dp3230156906044017] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (574.32,133.39) .. controls (574.32,132.65) and (574.92,132.05) .. (575.66,132.05) .. controls (576.4,132.05) and (577,132.65) .. (577,133.39) .. controls (577,134.13) and (576.4,134.73) .. (575.66,134.73) .. controls (574.92,134.73) and (574.32,134.13) .. (574.32,133.39) -- cycle ;
		%Shape: Circle [id:dp69693340396356] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (571.32,136.39) .. controls (571.32,135.65) and (571.92,135.05) .. (572.66,135.05) .. controls (573.4,135.05) and (574,135.65) .. (574,136.39) .. controls (574,137.13) and (573.4,137.73) .. (572.66,137.73) .. controls (571.92,137.73) and (571.32,137.13) .. (571.32,136.39) -- cycle ;
		%Shape: Circle [id:dp3891438647744305] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (578.32,135.39) .. controls (578.32,134.65) and (578.92,134.05) .. (579.66,134.05) .. controls (580.4,134.05) and (581,134.65) .. (581,135.39) .. controls (581,136.13) and (580.4,136.73) .. (579.66,136.73) .. controls (578.92,136.73) and (578.32,136.13) .. (578.32,135.39) -- cycle ;
		%Shape: Circle [id:dp5013186362873132] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (577.32,139.39) .. controls (577.32,138.65) and (577.92,138.05) .. (578.66,138.05) .. controls (579.4,138.05) and (580,138.65) .. (580,139.39) .. controls (580,140.13) and (579.4,140.73) .. (578.66,140.73) .. controls (577.92,140.73) and (577.32,140.13) .. (577.32,139.39) -- cycle ;
		%Shape: Circle [id:dp5812476467593732] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (572.32,140.39) .. controls (572.32,139.65) and (572.92,139.05) .. (573.66,139.05) .. controls (574.4,139.05) and (575,139.65) .. (575,140.39) .. controls (575,141.13) and (574.4,141.73) .. (573.66,141.73) .. controls (572.92,141.73) and (572.32,141.13) .. (572.32,140.39) -- cycle ;
		%Shape: Circle [id:dp5567491359028793] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (573.32,139.39) .. controls (573.32,138.65) and (573.92,138.05) .. (574.66,138.05) .. controls (575.4,138.05) and (576,138.65) .. (576,139.39) .. controls (576,140.13) and (575.4,140.73) .. (574.66,140.73) .. controls (573.92,140.73) and (573.32,140.13) .. (573.32,139.39) -- cycle ;
		%Shape: Circle [id:dp258780189057243] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (577.32,142.39) .. controls (577.32,141.65) and (577.92,141.05) .. (578.66,141.05) .. controls (579.4,141.05) and (580,141.65) .. (580,142.39) .. controls (580,143.13) and (579.4,143.73) .. (578.66,143.73) .. controls (577.92,143.73) and (577.32,143.13) .. (577.32,142.39) -- cycle ;
		%Shape: Circle [id:dp35349533211671513] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (576.32,144.39) .. controls (576.32,143.65) and (576.92,143.05) .. (577.66,143.05) .. controls (578.4,143.05) and (579,143.65) .. (579,144.39) .. controls (579,145.13) and (578.4,145.73) .. (577.66,145.73) .. controls (576.92,145.73) and (576.32,145.13) .. (576.32,144.39) -- cycle ;
		%Shape: Circle [id:dp9962851343512775] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (575.32,148.39) .. controls (575.32,147.65) and (575.92,147.05) .. (576.66,147.05) .. controls (577.4,147.05) and (578,147.65) .. (578,148.39) .. controls (578,149.13) and (577.4,149.73) .. (576.66,149.73) .. controls (575.92,149.73) and (575.32,149.13) .. (575.32,148.39) -- cycle ;
		%Shape: Circle [id:dp32099905381015614] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (577.32,149.39) .. controls (577.32,148.65) and (577.92,148.05) .. (578.66,148.05) .. controls (579.4,148.05) and (580,148.65) .. (580,149.39) .. controls (580,150.13) and (579.4,150.73) .. (578.66,150.73) .. controls (577.92,150.73) and (577.32,150.13) .. (577.32,149.39) -- cycle ;
		%Shape: Circle [id:dp8739434459700479] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (578.66,149.39) .. controls (578.66,148.65) and (579.26,148.05) .. (580,148.05) .. controls (580.74,148.05) and (581.34,148.65) .. (581.34,149.39) .. controls (581.34,150.13) and (580.74,150.73) .. (580,150.73) .. controls (579.26,150.73) and (578.66,150.13) .. (578.66,149.39) -- cycle ;
		%Shape: Circle [id:dp04732628378342829] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (571.32,149.39) .. controls (571.32,148.65) and (571.92,148.05) .. (572.66,148.05) .. controls (573.4,148.05) and (574,148.65) .. (574,149.39) .. controls (574,150.13) and (573.4,150.73) .. (572.66,150.73) .. controls (571.92,150.73) and (571.32,150.13) .. (571.32,149.39) -- cycle ;
		%Shape: Circle [id:dp38168084794674395] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (572.32,145.39) .. controls (572.32,144.65) and (572.92,144.05) .. (573.66,144.05) .. controls (574.4,144.05) and (575,144.65) .. (575,145.39) .. controls (575,146.13) and (574.4,146.73) .. (573.66,146.73) .. controls (572.92,146.73) and (572.32,146.13) .. (572.32,145.39) -- cycle ;
		%Shape: Circle [id:dp8236756245109615] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (573.32,143.39) .. controls (573.32,142.65) and (573.92,142.05) .. (574.66,142.05) .. controls (575.4,142.05) and (576,142.65) .. (576,143.39) .. controls (576,144.13) and (575.4,144.73) .. (574.66,144.73) .. controls (573.92,144.73) and (573.32,144.13) .. (573.32,143.39) -- cycle ;
		%Shape: Circle [id:dp38285756640827384] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (573.66,150.39) .. controls (573.66,149.65) and (574.26,149.05) .. (575,149.05) .. controls (575.74,149.05) and (576.34,149.65) .. (576.34,150.39) .. controls (576.34,151.13) and (575.74,151.73) .. (575,151.73) .. controls (574.26,151.73) and (573.66,151.13) .. (573.66,150.39) -- cycle ;
		%Shape: Circle [id:dp4955415892166637] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (571.66,152.39) .. controls (571.66,151.65) and (572.26,151.05) .. (573,151.05) .. controls (573.74,151.05) and (574.34,151.65) .. (574.34,152.39) .. controls (574.34,153.13) and (573.74,153.73) .. (573,153.73) .. controls (572.26,153.73) and (571.66,153.13) .. (571.66,152.39) -- cycle ;
		%Shape: Circle [id:dp5917128213323251] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (575.66,152.39) .. controls (575.66,151.65) and (576.26,151.05) .. (577,151.05) .. controls (577.74,151.05) and (578.34,151.65) .. (578.34,152.39) .. controls (578.34,153.13) and (577.74,153.73) .. (577,153.73) .. controls (576.26,153.73) and (575.66,153.13) .. (575.66,152.39) -- cycle ;
		%Shape: Circle [id:dp7403329886960426] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (579.66,152.39) .. controls (579.66,151.65) and (580.26,151.05) .. (581,151.05) .. controls (581.74,151.05) and (582.34,151.65) .. (582.34,152.39) .. controls (582.34,153.13) and (581.74,153.73) .. (581,153.73) .. controls (580.26,153.73) and (579.66,153.13) .. (579.66,152.39) -- cycle ;
		%Shape: Circle [id:dp6456778130539647] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (572.66,156.39) .. controls (572.66,155.65) and (573.26,155.05) .. (574,155.05) .. controls (574.74,155.05) and (575.34,155.65) .. (575.34,156.39) .. controls (575.34,157.13) and (574.74,157.73) .. (574,157.73) .. controls (573.26,157.73) and (572.66,157.13) .. (572.66,156.39) -- cycle ;
		%Shape: Circle [id:dp05890578475520991] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (574.66,159.39) .. controls (574.66,158.65) and (575.26,158.05) .. (576,158.05) .. controls (576.74,158.05) and (577.34,158.65) .. (577.34,159.39) .. controls (577.34,160.13) and (576.74,160.73) .. (576,160.73) .. controls (575.26,160.73) and (574.66,160.13) .. (574.66,159.39) -- cycle ;
		%Shape: Circle [id:dp1639238612012699] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (577.66,157.39) .. controls (577.66,156.65) and (578.26,156.05) .. (579,156.05) .. controls (579.74,156.05) and (580.34,156.65) .. (580.34,157.39) .. controls (580.34,158.13) and (579.74,158.73) .. (579,158.73) .. controls (578.26,158.73) and (577.66,158.13) .. (577.66,157.39) -- cycle ;
		%Shape: Circle [id:dp2936406603560169] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (579.66,156.39) .. controls (579.66,155.65) and (580.26,155.05) .. (581,155.05) .. controls (581.74,155.05) and (582.34,155.65) .. (582.34,156.39) .. controls (582.34,157.13) and (581.74,157.73) .. (581,157.73) .. controls (580.26,157.73) and (579.66,157.13) .. (579.66,156.39) -- cycle ;
		%Shape: Circle [id:dp6296497359980564] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (576.66,160.39) .. controls (576.66,159.65) and (577.26,159.05) .. (578,159.05) .. controls (578.74,159.05) and (579.34,159.65) .. (579.34,160.39) .. controls (579.34,161.13) and (578.74,161.73) .. (578,161.73) .. controls (577.26,161.73) and (576.66,161.13) .. (576.66,160.39) -- cycle ;
		%Shape: Circle [id:dp46482356850969375] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (579.66,160.39) .. controls (579.66,159.65) and (580.26,159.05) .. (581,159.05) .. controls (581.74,159.05) and (582.34,159.65) .. (582.34,160.39) .. controls (582.34,161.13) and (581.74,161.73) .. (581,161.73) .. controls (580.26,161.73) and (579.66,161.13) .. (579.66,160.39) -- cycle ;
		%Shape: Circle [id:dp2871444398124743] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (574.66,163.39) .. controls (574.66,162.65) and (575.26,162.05) .. (576,162.05) .. controls (576.74,162.05) and (577.34,162.65) .. (577.34,163.39) .. controls (577.34,164.13) and (576.74,164.73) .. (576,164.73) .. controls (575.26,164.73) and (574.66,164.13) .. (574.66,163.39) -- cycle ;
		%Shape: Circle [id:dp31459013115775236] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (577.66,163.39) .. controls (577.66,162.65) and (578.26,162.05) .. (579,162.05) .. controls (579.74,162.05) and (580.34,162.65) .. (580.34,163.39) .. controls (580.34,164.13) and (579.74,164.73) .. (579,164.73) .. controls (578.26,164.73) and (577.66,164.13) .. (577.66,163.39) -- cycle ;
		%Shape: Circle [id:dp6692247468867529] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (578.66,165.39) .. controls (578.66,164.65) and (579.26,164.05) .. (580,164.05) .. controls (580.74,164.05) and (581.34,164.65) .. (581.34,165.39) .. controls (581.34,166.13) and (580.74,166.73) .. (580,166.73) .. controls (579.26,166.73) and (578.66,166.13) .. (578.66,165.39) -- cycle ;
		%Shape: Circle [id:dp2990122373197981] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (574.66,167.39) .. controls (574.66,166.65) and (575.26,166.05) .. (576,166.05) .. controls (576.74,166.05) and (577.34,166.65) .. (577.34,167.39) .. controls (577.34,168.13) and (576.74,168.73) .. (576,168.73) .. controls (575.26,168.73) and (574.66,168.13) .. (574.66,167.39) -- cycle ;
		%Shape: Circle [id:dp43702882834968415] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (576.66,168.39) .. controls (576.66,167.65) and (577.26,167.05) .. (578,167.05) .. controls (578.74,167.05) and (579.34,167.65) .. (579.34,168.39) .. controls (579.34,169.13) and (578.74,169.73) .. (578,169.73) .. controls (577.26,169.73) and (576.66,169.13) .. (576.66,168.39) -- cycle ;
		%Shape: Circle [id:dp9574532804433131] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (578.66,169.39) .. controls (578.66,168.65) and (579.26,168.05) .. (580,168.05) .. controls (580.74,168.05) and (581.34,168.65) .. (581.34,169.39) .. controls (581.34,170.13) and (580.74,170.73) .. (580,170.73) .. controls (579.26,170.73) and (578.66,170.13) .. (578.66,169.39) -- cycle ;
		%Shape: Circle [id:dp1457062163338141] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (580.66,167.39) .. controls (580.66,166.65) and (581.26,166.05) .. (582,166.05) .. controls (582.74,166.05) and (583.34,166.65) .. (583.34,167.39) .. controls (583.34,168.13) and (582.74,168.73) .. (582,168.73) .. controls (581.26,168.73) and (580.66,168.13) .. (580.66,167.39) -- cycle ;
		%Shape: Circle [id:dp5553681979977159] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (574.66,170.39) .. controls (574.66,169.65) and (575.26,169.05) .. (576,169.05) .. controls (576.74,169.05) and (577.34,169.65) .. (577.34,170.39) .. controls (577.34,171.13) and (576.74,171.73) .. (576,171.73) .. controls (575.26,171.73) and (574.66,171.13) .. (574.66,170.39) -- cycle ;
		%Shape: Circle [id:dp37797658229943476] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (575.66,171.39) .. controls (575.66,170.65) and (576.26,170.05) .. (577,170.05) .. controls (577.74,170.05) and (578.34,170.65) .. (578.34,171.39) .. controls (578.34,172.13) and (577.74,172.73) .. (577,172.73) .. controls (576.26,172.73) and (575.66,172.13) .. (575.66,171.39) -- cycle ;
		%Shape: Circle [id:dp7396391950950918] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (579.66,173.39) .. controls (579.66,172.65) and (580.26,172.05) .. (581,172.05) .. controls (581.74,172.05) and (582.34,172.65) .. (582.34,173.39) .. controls (582.34,174.13) and (581.74,174.73) .. (581,174.73) .. controls (580.26,174.73) and (579.66,174.13) .. (579.66,173.39) -- cycle ;
		%Shape: Circle [id:dp8090620989456856] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (579.66,182.39) .. controls (579.66,181.65) and (580.26,181.05) .. (581,181.05) .. controls (581.74,181.05) and (582.34,181.65) .. (582.34,182.39) .. controls (582.34,183.13) and (581.74,183.73) .. (581,183.73) .. controls (580.26,183.73) and (579.66,183.13) .. (579.66,182.39) -- cycle ;
		%Shape: Circle [id:dp4250169444611942] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (571.66,181.39) .. controls (571.66,180.65) and (572.26,180.05) .. (573,180.05) .. controls (573.74,180.05) and (574.34,180.65) .. (574.34,181.39) .. controls (574.34,182.13) and (573.74,182.73) .. (573,182.73) .. controls (572.26,182.73) and (571.66,182.13) .. (571.66,181.39) -- cycle ;
		%Shape: Circle [id:dp10404101017665623] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (573.66,182.39) .. controls (573.66,181.65) and (574.26,181.05) .. (575,181.05) .. controls (575.74,181.05) and (576.34,181.65) .. (576.34,182.39) .. controls (576.34,183.13) and (575.74,183.73) .. (575,183.73) .. controls (574.26,183.73) and (573.66,183.13) .. (573.66,182.39) -- cycle ;
		%Shape: Circle [id:dp49041001945200446] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (571.66,182.39) .. controls (571.66,181.65) and (572.26,181.05) .. (573,181.05) .. controls (573.74,181.05) and (574.34,181.65) .. (574.34,182.39) .. controls (574.34,183.13) and (573.74,183.73) .. (573,183.73) .. controls (572.26,183.73) and (571.66,183.13) .. (571.66,182.39) -- cycle ;
		%Shape: Circle [id:dp24490263791516353] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (581.66,183.39) .. controls (581.66,182.65) and (582.26,182.05) .. (583,182.05) .. controls (583.74,182.05) and (584.34,182.65) .. (584.34,183.39) .. controls (584.34,184.13) and (583.74,184.73) .. (583,184.73) .. controls (582.26,184.73) and (581.66,184.13) .. (581.66,183.39) -- cycle ;
		%Shape: Circle [id:dp44332078543441966] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (580.66,185.39) .. controls (580.66,184.65) and (581.26,184.05) .. (582,184.05) .. controls (582.74,184.05) and (583.34,184.65) .. (583.34,185.39) .. controls (583.34,186.13) and (582.74,186.73) .. (582,186.73) .. controls (581.26,186.73) and (580.66,186.13) .. (580.66,185.39) -- cycle ;
		%Shape: Circle [id:dp7773564035761082] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (577.66,185.39) .. controls (577.66,184.65) and (578.26,184.05) .. (579,184.05) .. controls (579.74,184.05) and (580.34,184.65) .. (580.34,185.39) .. controls (580.34,186.13) and (579.74,186.73) .. (579,186.73) .. controls (578.26,186.73) and (577.66,186.13) .. (577.66,185.39) -- cycle ;
		%Shape: Circle [id:dp8565128767672991] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (577.66,183.39) .. controls (577.66,182.65) and (578.26,182.05) .. (579,182.05) .. controls (579.74,182.05) and (580.34,182.65) .. (580.34,183.39) .. controls (580.34,184.13) and (579.74,184.73) .. (579,184.73) .. controls (578.26,184.73) and (577.66,184.13) .. (577.66,183.39) -- cycle ;
		%Shape: Circle [id:dp09058414095009382] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (574.66,185.39) .. controls (574.66,184.65) and (575.26,184.05) .. (576,184.05) .. controls (576.74,184.05) and (577.34,184.65) .. (577.34,185.39) .. controls (577.34,186.13) and (576.74,186.73) .. (576,186.73) .. controls (575.26,186.73) and (574.66,186.13) .. (574.66,185.39) -- cycle ;
		%Shape: Circle [id:dp36203285305194144] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (575.66,183.39) .. controls (575.66,182.65) and (576.26,182.05) .. (577,182.05) .. controls (577.74,182.05) and (578.34,182.65) .. (578.34,183.39) .. controls (578.34,184.13) and (577.74,184.73) .. (577,184.73) .. controls (576.26,184.73) and (575.66,184.13) .. (575.66,183.39) -- cycle ;
		%Shape: Circle [id:dp4991054169783664] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (572.66,185.39) .. controls (572.66,184.65) and (573.26,184.05) .. (574,184.05) .. controls (574.74,184.05) and (575.34,184.65) .. (575.34,185.39) .. controls (575.34,186.13) and (574.74,186.73) .. (574,186.73) .. controls (573.26,186.73) and (572.66,186.13) .. (572.66,185.39) -- cycle ;
		%Shape: Circle [id:dp04510544576196973] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (578.66,186.39) .. controls (578.66,185.65) and (579.26,185.05) .. (580,185.05) .. controls (580.74,185.05) and (581.34,185.65) .. (581.34,186.39) .. controls (581.34,187.13) and (580.74,187.73) .. (580,187.73) .. controls (579.26,187.73) and (578.66,187.13) .. (578.66,186.39) -- cycle ;
		%Shape: Circle [id:dp6928591632861987] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (576.66,187.39) .. controls (576.66,186.65) and (577.26,186.05) .. (578,186.05) .. controls (578.74,186.05) and (579.34,186.65) .. (579.34,187.39) .. controls (579.34,188.13) and (578.74,188.73) .. (578,188.73) .. controls (577.26,188.73) and (576.66,188.13) .. (576.66,187.39) -- cycle ;
		%Shape: Circle [id:dp8575634454071555] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (576.66,218.39) .. controls (576.66,217.65) and (577.26,217.05) .. (578,217.05) .. controls (578.74,217.05) and (579.34,217.65) .. (579.34,218.39) .. controls (579.34,219.13) and (578.74,219.73) .. (578,219.73) .. controls (577.26,219.73) and (576.66,219.13) .. (576.66,218.39) -- cycle ;
		%Shape: Circle [id:dp530009259689467] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (574.66,220.39) .. controls (574.66,219.65) and (575.26,219.05) .. (576,219.05) .. controls (576.74,219.05) and (577.34,219.65) .. (577.34,220.39) .. controls (577.34,221.13) and (576.74,221.73) .. (576,221.73) .. controls (575.26,221.73) and (574.66,221.13) .. (574.66,220.39) -- cycle ;
		%Shape: Circle [id:dp4657441981579198] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (572.66,222.39) .. controls (572.66,221.65) and (573.26,221.05) .. (574,221.05) .. controls (574.74,221.05) and (575.34,221.65) .. (575.34,222.39) .. controls (575.34,223.13) and (574.74,223.73) .. (574,223.73) .. controls (573.26,223.73) and (572.66,223.13) .. (572.66,222.39) -- cycle ;
		%Shape: Circle [id:dp7053636525458711] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (579.66,220.39) .. controls (579.66,219.65) and (580.26,219.05) .. (581,219.05) .. controls (581.74,219.05) and (582.34,219.65) .. (582.34,220.39) .. controls (582.34,221.13) and (581.74,221.73) .. (581,221.73) .. controls (580.26,221.73) and (579.66,221.13) .. (579.66,220.39) -- cycle ;
		%Shape: Circle [id:dp3803321744501693] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (579.66,222.39) .. controls (579.66,221.65) and (580.26,221.05) .. (581,221.05) .. controls (581.74,221.05) and (582.34,221.65) .. (582.34,222.39) .. controls (582.34,223.13) and (581.74,223.73) .. (581,223.73) .. controls (580.26,223.73) and (579.66,223.13) .. (579.66,222.39) -- cycle ;
		%Shape: Circle [id:dp7611758884731528] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (575.66,224.39) .. controls (575.66,223.65) and (576.26,223.05) .. (577,223.05) .. controls (577.74,223.05) and (578.34,223.65) .. (578.34,224.39) .. controls (578.34,225.13) and (577.74,225.73) .. (577,225.73) .. controls (576.26,225.73) and (575.66,225.13) .. (575.66,224.39) -- cycle ;
		%Shape: Circle [id:dp4609930398001554] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (577.66,223.39) .. controls (577.66,222.65) and (578.26,222.05) .. (579,222.05) .. controls (579.74,222.05) and (580.34,222.65) .. (580.34,223.39) .. controls (580.34,224.13) and (579.74,224.73) .. (579,224.73) .. controls (578.26,224.73) and (577.66,224.13) .. (577.66,223.39) -- cycle ;
		%Shape: Circle [id:dp5745176775377865] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (580.66,224.39) .. controls (580.66,223.65) and (581.26,223.05) .. (582,223.05) .. controls (582.74,223.05) and (583.34,223.65) .. (583.34,224.39) .. controls (583.34,225.13) and (582.74,225.73) .. (582,225.73) .. controls (581.26,225.73) and (580.66,225.13) .. (580.66,224.39) -- cycle ;
		%Shape: Circle [id:dp4360063025836458] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (580.66,227.39) .. controls (580.66,226.65) and (581.26,226.05) .. (582,226.05) .. controls (582.74,226.05) and (583.34,226.65) .. (583.34,227.39) .. controls (583.34,228.13) and (582.74,228.73) .. (582,228.73) .. controls (581.26,228.73) and (580.66,228.13) .. (580.66,227.39) -- cycle ;
		%Shape: Circle [id:dp4431559060836632] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (577.66,227.39) .. controls (577.66,226.65) and (578.26,226.05) .. (579,226.05) .. controls (579.74,226.05) and (580.34,226.65) .. (580.34,227.39) .. controls (580.34,228.13) and (579.74,228.73) .. (579,228.73) .. controls (578.26,228.73) and (577.66,228.13) .. (577.66,227.39) -- cycle ;
		%Shape: Circle [id:dp7839759625022706] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (578.66,228.39) .. controls (578.66,227.65) and (579.26,227.05) .. (580,227.05) .. controls (580.74,227.05) and (581.34,227.65) .. (581.34,228.39) .. controls (581.34,229.13) and (580.74,229.73) .. (580,229.73) .. controls (579.26,229.73) and (578.66,229.13) .. (578.66,228.39) -- cycle ;
		%Shape: Circle [id:dp5257191852849448] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (573.66,227.39) .. controls (573.66,226.65) and (574.26,226.05) .. (575,226.05) .. controls (575.74,226.05) and (576.34,226.65) .. (576.34,227.39) .. controls (576.34,228.13) and (575.74,228.73) .. (575,228.73) .. controls (574.26,228.73) and (573.66,228.13) .. (573.66,227.39) -- cycle ;
		%Shape: Circle [id:dp5825767827705028] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (571.66,229.39) .. controls (571.66,228.65) and (572.26,228.05) .. (573,228.05) .. controls (573.74,228.05) and (574.34,228.65) .. (574.34,229.39) .. controls (574.34,230.13) and (573.74,230.73) .. (573,230.73) .. controls (572.26,230.73) and (571.66,230.13) .. (571.66,229.39) -- cycle ;
		%Shape: Circle [id:dp9070404413381465] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 156; green, 195; blue, 237 }  ,fill opacity=1 ] (571.66,226.39) .. controls (571.66,225.65) and (572.26,225.05) .. (573,225.05) .. controls (573.74,225.05) and (574.34,225.65) .. (574.34,226.39) .. controls (574.34,227.13) and (573.74,227.73) .. (573,227.73) .. controls (572.26,227.73) and (571.66,227.13) .. (571.66,226.39) -- cycle ;
		%Curve Lines [id:da13765307676019534] 
		\draw    (491.37,197.91) .. controls (498.2,212.54) and (531.64,206.25) .. (570.38,204.07) ;
		\draw [shift={(573.37,203.91)}, rotate = 177.14] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		
		% Text Node
		\draw (97,242.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0$};
		% Text Node
		\draw (95,209.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$20$};
		% Text Node
		\draw (95,177.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$40$};
		% Text Node
		\draw (95,146.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$60$};
		% Text Node
		\draw (95,114.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$80$};
		% Text Node
		\draw (89,81.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$100$};
		% Text Node
		\draw (30,160) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\scriptsize this looks}\\{\scriptsize like a small}\\{\scriptsize quantity...}};
		% Text Node
		\draw (225,254.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0$};
		% Text Node
		\draw (255,254.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$10$};
		% Text Node
		\draw (285,254.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$20$};
		% Text Node
		\draw (317,254.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$30$};
		% Text Node
		\draw (183,234.64) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle 0$ to$\displaystyle < 10$};
		% Text Node
		\draw (177,219.14) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle 10$ to$\displaystyle < 20$};
		% Text Node
		\draw (177,202.14) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle 20$ to$\displaystyle < 30$};
		% Text Node
		\draw (177,184.64) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle 30$ to$\displaystyle < 40$};
		% Text Node
		\draw (177,167.14) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle 40$ to$\displaystyle < 50$};
		% Text Node
		\draw (177,150.64) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle 50$ to$\displaystyle < 60$};
		% Text Node
		\draw (177,135.14) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle 60$ to$\displaystyle < 70$};
		% Text Node
		\draw (177,120.14) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle 70$ to$\displaystyle < 80$};
		% Text Node
		\draw (177,104.64) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle 80$ to$\displaystyle < 90$};
		% Text Node
		\draw (171,90.14) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle 90$ to$\displaystyle < 100$};
		% Text Node
		\draw (290,165) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {...but it's\\actually\\large :-(};
		% Text Node
		\draw (48.5,50.41) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {\textbf{Study participants by Age}};
		% Text Node
		\draw (199,50.41) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {\textbf{Study participants by Age}};
		% Text Node
		\draw (345.5,50.41) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {\textbf{Study participants by Age}};
		% Text Node
		\draw (487,50.41) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {\textbf{Study participants by Age}};
		% Text Node
		\draw (348,214.45) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$20$};
		% Text Node
		\draw (348,181.45) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$40$};
		% Text Node
		\draw (348,149.45) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$60$};
		% Text Node
		\draw (348,118.45) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$80$};
		% Text Node
		\draw (342,86.45) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$100$};
		% Text Node
		\draw (350,245.45) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0$};
		% Text Node
		\draw (385,70) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {box plots make\\the two groups\\look almost\\identical...};
		% Text Node
		\draw (369,255) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {\begin{minipage}[lt]{20.97pt}\setlength\topsep{0pt}
		\begin{center}
		test\\group
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (399,254) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {\begin{minipage}[lt]{24.14pt}\setlength\topsep{0pt}
		\begin{center}
		control\\group
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (508,214.45) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$20$};
		% Text Node
		\draw (508,181.45) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$40$};
		% Text Node
		\draw (508,149.45) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$60$};
		% Text Node
		\draw (508,118.45) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$80$};
		% Text Node
		\draw (502,86.45) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$100$};
		% Text Node
		\draw (510,245.45) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0$};
		% Text Node
		\draw (529,255) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {\begin{minipage}[lt]{20.97pt}\setlength\topsep{0pt}
		\begin{center}
		test\\group
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (559,254) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {\begin{minipage}[lt]{24.14pt}\setlength\topsep{0pt}
		\begin{center}
		control\\group
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (448,168.5) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {...but they\\definitely\\aren't :-(};
		\end{tikzpicture}
		}
	\end{figure}
	That's why we prefer in practice to use pirate plots (that are a mixed of violin plots and Box \& Whiskers plots):
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{img/arithmetics/pirate_plot.jpg}
	\end{figure}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	As we will prove it during our study of Inverse Transform Sampling in the section of Numerical Methods (see page \pageref{inverse transform sampling}), the percentiles of any distribution are always uniformly distributed.
	\end{tcolorbox}
	The concepts of median, outliers and confidence intervals that have yet been proved and/or just defined are so significant that there exists international standards for their proper use. First let us cite the norm ISO 16269-7:2001 \textit{Median - Estimation and confidence intervals} and also the norm ISO 16269-4:2010 \textit{Detection and treatment of outliers}.  For more details and more outliers detection tests the reader can take a look to an entire textbook dedicated to outliers identification as \cite{hawkins2013identification} or look at the Data Science mind map (see page \pageref{mindmap of data science}) where more than twenty different tests are classified.

		A beautiful demonstration on how governments can make statistics lie (hopefully now most of them has to give the data to the population for free) is for example the USA Federal Reserve that pointed out the fact a growth in average income since the 12010s (holocene calendar). But when we have access to the raw data:
		\begin{center}
			\url{https://www.federalreserve.gov/econres/scfindex.htm}
		\end{center}
		and plot the median versus the average with for example Minitab 17 we get (the years in the plot are based on the Gregorian calendar):
		\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/arithmetics/median_vs_average.jpg}
	\end{figure}
		and after some analysis you can point out that the increase in the average is largely due to increases in the highest $10\%$ of incomes. The decline in the median at the same time suggests that typical Americans are in reality not doing as well since 12010 (holocene calendar)... Q.E.D! 
		
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		Be careful with obvious conclusions when analyzing trends! As we will see during our study of Simpson's paradox (see further below page \pageref{Simpson paradox}), when a body of data displays one trend, yet when broken (i.e. stratified) into subgroups, the opposite trend may appear into view for each of those subgroups!
		\end{tcolorbox}
		
		\item[D4.] By analogy with the median, we define the "\NewTerm{medial}\index{medial}" as the value (in ascending order of values) that shares the (cumulative) sum values into two equal masses (i.e. the total sum divided by two).

		In the case of our wages example, while the median gives the $50\%$ of the salaries being below and above the medial gives how many employees share (and therefore the sharing wage) the first half and how many employees share the second half of the total of the wages costs.

		For example, to return to our table on wages:
		\begin{table}[H]
			\centering
			\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|c|c|c|c|}
			\hline
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Employee N\degre}} & 
	\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Wage}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Cumulated Wages}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{\%Cumulated Wages}}\\ \hline
			1 & 1,200 & 1,200 & 3.5\%\\ \hline
			2 & 1,220 & 2,420 & 7\%\\ \hline
			3 & 1,250 & 3,670 & 10.7\%\\ \hline
			4 & 1,300 & 4,970 & 14.5\%\\ \hline
			5 & 1,350 & 6,320 & 18.4\%\\ \hline
			6 & 1,450 & 7,770 & 22.6\%\\ \hline
			7 & 1,450 & 9,220 & 26.8\%\\ \hline
			8 & 1,560 & 10,780 & 31.4\%\\ \hline
			9 & 1,600 & 12,380 & 36.1\%\\ \hline
			10 & 1,800 & 14,180 & 41.3\%\\ \hline
	\multicolumn{1}{|c|}{\cellcolor{green!30}11} & 
	\multicolumn{1}{|c|}{\cellcolor{green!30}1,900} & \multicolumn{1}{|c|}{\cellcolor{green!30}16,080} & \multicolumn{1}{|c|}{\cellcolor{green!30}46.8\%}\\ \hline
	\multicolumn{1}{|c|}{\cellcolor{green!30}12} & 
	\multicolumn{1}{|c|}{\cellcolor{green!30}2,150} & \multicolumn{1}{|c|}{\cellcolor{green!30}18,230} & \multicolumn{1}{|c|}{\cellcolor{green!30}53.1\%}\\ \hline	
			13 & 2,310 & 20,540 & 59.8\%\\ \hline
			14 & 2,610 & 23,140 & 67.4\%\\ \hline
			15 & 3,000 & 26,140 & 76.1\%\\ \hline
			16 & 3,400 & 29,540 & 86\%\\ \hline
			17 & 4,800 & 34,340 & 100\%\\ \hline
			\end{tabular}
			\caption{Identification of the mediale}
		\end{table}	

		The sum of all wages is equal to $34,340$ and therefore the medial is $17,170$ then the medial is between the employees No. 11 and 12, while the median was $1,600$. We see then that the medial corresponds to $50\%$ of the aggregate. This is a very useful indicator in Pareto or Lorenz analysis (\SeeChapter{see section Quantitative Management page \pageref{pareto analysis}}).
		
		\item[D5.] Like the median, the  "\NewTerm{pseudomedian}\index{pseudomedian}", used in some nonparametric statistical tests, is a measure of centrality for a dataset. The pseudomedian is derived by taking the median of the set of the means of pairs of data points, where the first is less than or equal to the second. 
		
		So, if the data were $\{1,3,7\}$, the pseudomedian would be the median of $\{$ mean $(1,1)$, mean $(1,3)$, mean $(1,7)$, mean $(3,3)$, mean $(3,7)$, mean $(7,7)\}$, which is $3.5$. More technically, if the data are $x_{1}, x_{2}, \ldots, x_{n}$, then their pseudomedian is given by taking the median of the collection of $\frac{n}{2}(n+1)$ values of the form $w=\frac{x_{i}+x_{j}}{2}$ for $1 \leq i \leq j \leq n$.
	
		\item[D6.] The "\NewTerm{root mean square}\index{root mean square}" sometimes denoted simply $Q$ which comes from the general relation:
		
		but where we take $m=2$.
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		It is one of the best known averages in statistics because the standard deviation is a root mean square (see below)!
		\end{tcolorbox}
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		Consider a square of side $a$, and another square of side $b$. The average area of two squares equals one square of side:
		
		\end{tcolorbox}
		In Microsoft Excel 11.8346 you can combine the functions \texttt{SUMSQ( )}, \texttt{COUNT( )} and to quickly calculate the root mean square as following:
		\begin{center}
		\texttt{=(SUMSQ(...)/COUNT(...))\string^(1/COUNT(...))}
		\end{center}
	
		\item[D7.]  The "\NewTerm{harmonic mean}\index{harmonic mean}\label{harmonic mean}" sometimes simply denoted $H$ is defined by:
		
		It is little known but is often the result of simple and relevant arguments (typically the equivalent resistance of an electrical circuit with several resistors in parallel). There is a \texttt{HARMEAN( )} function in Microsoft Excel 11.8346 to calculate it.
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		Consider a distance $d$ travelled in one direction at the speed $v_1$ and in the other  direction (or not) at the speed $v_2$. The arithmetic average speed will be obtained by dividing the total distance $2d$ by the time of the travel:
		
		If we calculate the time it takes when travel $d$ with a speed $v_i$ that is simply the quotient:
		
		The total time is therefore:
		
		If the distance is not the same for the both velocities anyway each velocity remains the same this is why $d$ disappears!
		\end{tcolorbox}
		In other words: We use the harmonic mean when are given to us ratios. Let's see other important examples to illustrate this: 
		
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Examples:}\\\\
		E1. Three investments have a Price / Return ratio of respectively $104\%$, $106\%$ and $109\% $ (so we lost money in all three investments in this particular case!). Knowing that the price of the three investments was initially the same, we use the harmonic average:
		
		While an arithmetic average would give about $106.33\%$. Which makes a big difference when we manage millions in cash!\\
		
		Note on the way that the following development shows that the harmonic mean is a special case of the "\NewTerm{weighted arithmetic mean}\index{weighted arithmetic mean}":
		
		If all weights are equal-weighted then we fall back on the standard arithmetic average!\\
		
		E2. Consider that an investor puts us a disposition $300.-$ each month to buy a given asset. The first month this asset is worth $9.-$ so we can buy $33.333$ units of it, the second month the asset is worth $11.-$ so we can buy $ 27.27227$ units of it, finally the last and the third month the asset is worth $4.-$ so we can buy $75$ units of it. The question is, what is the average price of this asset in our portfolio? We then have:
		
		
		E3.\label{harmonic mean working rate} Alice can finish a task in $2$ hours and Bob takes $3$ hours to finish the same task, how long would it take them if they worked together?\\
		
		Alice works therefore a rate of $1 / 2$ (in on hour she can do $50\%$ of the task) and Bob therefore at a rate of $1 / 3$ (in one hour he can do $33.\bar{3}\%$ of the taks). Together they work therefore at a rate of:
		$$\dfrac{1}{2}+\dfrac{1}{3} =\dfrac{5}{6}$$
		So it would take them $6 / 5=1.2$ of an hour ($72$ minutes) to finish it if they worked together.
		\end{tcolorbox}
	
		\item[D8.] The "\NewTerm{geometric mean}\index{geometric mean}\label{geometric mean}" sometimes simply denoted $G$ is defined in the general case by:
		
		This average is often forgotten by undergraduate employees but famous it is famous in the field of finance (\SeeChapter{see section Economy page \pageref{geometric mean for finance}}) this is also why there is an \texttt{GEOMEAN( )} function in Microsoft Excel 11.8346 to calculate it.
		
		A geometric mean is often used when comparing different items – finding a single "figure of merit" for these items – when each item has multiple properties that have different numeric ranges. For example, the geometric mean can give a meaningful "average" to compare two companies which are each rated at $0$ to $5$ for their environmental sustainability, and are rated at $0$ to $100$ for their financial viability. If an arithmetic mean were used instead of a geometric mean, the financial viability is given more weight because its numeric range is larger so a small percentage change in the financial rating (e.g. going from $80$ to $90$) makes a much larger difference in the arithmetic mean than a large percentage change in environmental sustainability (e.g. going from $2$ to $5$). The use of a geometric mean "normalizes" the ranges being averaged, so that no range dominates the weighting, and a given percentage change in any of the properties has the same effect on the geometric mean. So, a $20\%$ change in environmental sustainability from $4$ to $4.8$ has the same effect on the geometric mean as a $20\%$ change in financial viability from $60$ to $72$.
		
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		In 12010 (holocene calendar), the geometric mean was introduced to compute the Human Development Index by United Nations Development Programme. Poor performance in any dimension is directly reflected in the geometric mean. That is to say, a low achievement in one dimension is not anymore linearly compensated for by high achievement in another dimension. The geometric mean reduces the level of substitutability between dimensions and at the same time ensures that a $1\%$  decline in index of, say, life expectancy has the same impact on the HDI as a $1\%$ decline in education or income index. Thus, as a basis for comparisons of achievements, this method is also more respectful of the intrinsic differences across the dimensions than a simple average.
		\end{tcolorbox}
		
		As for zero values, it is impossible to calculate the geometric mean with negative numbers! However, there are several workarounds for this problem, all of which require that the negative values be converted or transformed to a meaningful positive equivalent value. Most often this problem arises when it is desired to calculate the geometric mean of a percent change in a population or a financial return, which includes negative numbers.

		For example, to calculate the geometric mean of the values $+12\%, -8\%, 0\%$ and +$2\%$, instead calculate the geometric mean of their decimal multiplier equivalents of $1.12, 0.92, 1$ and $1.02$, to compute a geometric mean of $1.0125$. Subtracting $1$ from this value gives the geometric mean of $+1.25\%$ as a net rate of growth (or in financial circles is named the "\NewTerm{Compound Annual Growth Rate C.A.G.R.}\index{compound annual growth rate}").
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
		\textbf{{\Large \ding{45}}Example:}\\\\
		Suppose that a bank offers an investment opportunity and plans for the first year an interest (this is absurd, but this is an example) with a rate $(X-Y)\%$ but for the second year with an interest rate $(X+Y)\%$. At the same time another bank provides a constant interest rate for two years: $X\%$. We will say a little bit to fast that this is the same... In fact the two investments do not have the same profitability!\\
	
		In the first bank, a capital $C_0$ will give after the first year of interest:
		
		and the second year:
		
		In the other bank we will have after one year:
		
		and after the second year:
		
		and so on...\\
		
		As you can probably see it the placement will not be identical if $Y\neq 0$! $X\%$ is the not the arithmetic average of $(X-Y)\%$ and $(X+Y)\%$.\\

		Now if we write:
		
		What is in reality the average value of the global interest rate $r$?\\
	
		After $2$ years (for example), the capital is multiplied by $r_1 \cdot r_2$. If an average exists it will be denoted by $r$ and the capital will thus be multiplied by $r^2$. Then we have the relation:
		
		This is an example where we therefore see the geometric mean. Forgetting to use of the geometric mean a common mistake in companies where some employees calculate the arithmetic average rate of increase of a reference value.
		\end{tcolorbox}
		
		\item[D9.] The "\NewTerm{moving average}\index{moving average}\label{moving average}" of order $k$, also named sometimes "\NewTerm{rolling mean}\index{rolling mean}", is defined on a sequence $x_1,\ldots,x_i,\ldots,x_n$ of realizations by:
		
		The moving average is used particularly in economics, where it represents a trend of a series of values, where the number of points is equal to the total number of points of the series of values less the number that you specify for the period (for a detailed example see the section of Economy page \pageref{simple moving average}).
		
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		On some Internet pages and statistical softwares, the "\NewTerm{rolling mean}" of order $k$ (must be an odd positive number greater than $2$!) is defined for a sequence $x_1,\ldots,x_i,\ldots,x_n$ (with $n\geq k$) by:
		
		\end{tcolorbox}	
	
		A moving average in finance is calculated from the average of a stock price over a given period: each point of a moving average of $100$ sessions is the average of $100$ last current values. This curve, displayed simultaneously with the evolution of the curve of the values, smooths the daily changes in the value and gives the possibility to better see the trends.

		The moving averages can be calculated for different time periods, which can generate short-term trends MMC ($20$ sessions according to the habits of the domain), medium ($50$-$100$ sessions) or long-term MML (over $100$ sessions):

		\begin{figure}[H]
			\centering
			\includegraphics[scale=0.75]{img/analysis/time_serie.eps}
			\caption{Graphical representation of a few moving averages}
		\end{figure}
	
		The crosses of the moving averages with the price curve (cutted with a certain granularity) of the value generate purchase or sale (basic) signals depending on the case:
		\begin{itemize}
			\item Buy signal: when the price curve crosses the MM up.
			\item Sell signal: when the price curve crosses the MM down.
		\end{itemize}
		In addition to the moving average, note that there are a lot of other artificial indicators often used in finance (the \texttt{R} software has a package dedicated only to such indicators). As for example the "\NewTerm{upside/downside ratio}\index{upside/downside ratio}".

		The idea is the following: If you have a financial product (\SeeChapter{see section Economy  page \pageref{marketable assets}}) whose current price is $P_c$ for which you have a goal of high gain corresponding to a high price, which we will denote by $P_h$ (high price) and conversely, the potential loss that you feel is at a price $P_l$ (low price).
		
		Then the ratio:
		
		gives the upside/down ratio!
	
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Examples:}\\\\
		E1. For example, a financial product of $10.-$ with a low price of $5.-$ and a high price of $15.-$ has a ratio of $\text{UD}_R=1$ and therefore an identical speculative factor to allow a gain or loss of $5.-$.\\
		
		E2. A financial product of $10.-$ with a low price of $5.-$ and a high price of $20.-$ has a ratio of $\text{UD}_R=2$ and therefore twice the speculative potential gain compared to the loss.
		\end{tcolorbox}
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		Some financial institutions recommend to reject the $\text{UD}_R$ below $3$. Investors also tend to reject too high $\text{UD}_R$ that can be a sign of artificial inflation.
		\end{tcolorbox}	
	
		\item[D10.] The "\NewTerm{weighted average}\index{weighted average}" (the moving average and arithmetic average are just a special cases of the weighted average with $w_i=1$) is defined by:
		
Is used for example in geometry to locate the centroid of a polygon, in physics to determine the center of gravity or in statistics to calculate the mean and other advanced regression techniques and in project management for estimating task durations forecast.

In the general case the weights $w_i$ represents the weighted influence or arbitrary/empirical influence of the element $x_i$ relatively to the others one.

		\item[D11.] The "\NewTerm{functional mean}\index{functional mean}" or "\NewTerm{integral average}\index{integral average}\label{integral average}" is defined as:
		
		where $\mu_f$ depends of a function $f$ of a real integrated variable (\SeeChapter{see section Differential and Integral Calculus page \pageref{integral calculus}}) on a range $[a, b]$. It is often used in signal theory (electronics, electrotechnichs).
	\end{enumerate}
	
	\pagebreak
	\subsubsection{Laplace Smoothing}
	To come back to our class frequencies seen above and before proceeding with the study of some mathematical properties of the averages... you should know that when we work with discrete probabilities distributions it happens very (very) often that we meet a typical problem whose source is the size of the population.

	Consider as an example the case where we have $12$ documents and that we would like estimate the probability of occurrence of the word "Viagra". We have on a sample the following values:
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|c|c|}
				\hline
\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Document ID}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Word occurrences}} \\ \hline
		1 & 1 \\ \hline
		2 & 0 \\ \hline
		3 & 2 \\ \hline
		4 & 0 \\ \hline
		5 & 4 \\ \hline
		6 & 6 \\ \hline
		7 & 3 \\ \hline
		8 & 0 \\ \hline
		9 & 6 \\ \hline
		10 & 2 \\ \hline	
		11 & 0 \\ \hline
		12 & 1 \\ \hline
		\end{tabular}
		\caption[]{Class frequencies of the word}
	\end{table}
	Table that we can represent in another way:
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|c|c|c|}
		\hline
\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Word occurrences}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Documents}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Probability}} \\ \hline
		0 & 4 & 0.33\\ \hline
		1 & 2 & 0.17\\ \hline
		2 & 2 & 0.17\\ \hline
		3 & 1 & 0.083 \\ \hline
		4 & 1 & 0.083\\ \hline
		5 & 0 & 0\\ \hline
		6 & 2 & 0.17\\ \hline
		\end{tabular}
		\caption[]{Respective frequencies classes of documents}
	\end{table}
	And here we have a common phenomenon. There is no record with $5$ occurrences of the word of interest. The idea (very common in the field of Data Mining) is then to add artificially and empirically using a count using a technique named "\NewTerm{Laplace smoothing}\index{Laplace smoothing}" which involves adding $k$ units at each occurrence. Therefore the table becomes:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|}
		\hline
\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Word occurrences}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Documents}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Probability}} \\ \hline
		0 & 5 & 0.26\\ \hline
		1 & 3 & 0.16\\ \hline
		2 & 3 & 0.16\\ \hline
		3 & 2 & 0.11 \\ \hline
		4 & 2 & 0.11\\ \hline
		5 & 1 & 0.05\\ \hline
		6 & 3 & 0.16\\ \hline
		\end{tabular}	
		\caption[]{Frequencies classes of documents with smoothing}
	\end{table}
	Obviously this type of technique is debatable and beyond the scientific framework ... We even hesitated to introduce this technique in the chapter of Numerical Methods (with the rest of all the empirical numerical techniques) rather than here...

	\subsubsection{Means and Averages properties}\label{means and averages properties}
	Now we will see some relevant properties that connect some of these means and averages or are specific to a particular mean/average.

	\label{properties of the mean}The first properties are important so beware to understand them:
	\begin{enumerate}
		\item[P1.] The calculation of the arithmetic, root mean square and harmonic average/mean can be generalized using the following expression:
			
		where we see:
			\begin{enumerate}
				\item For $m=1$, we get the arithmetic average
				\item For $m=2$, we get the root mean square
				\item For $m=-1$ we get the harmonic mean
			\end{enumerate}
		\item[P2.] The arithmetic average has the property of linearity, that is to say (without proof because it is simple to check):
			
			This is the statistical version of the property of the mean in the field of probabilities that we will see further.
			\item[P3.] The weighted sum of the deviations from the arithmetic average is zero.
				\begin{dem}
					First, by definition, we know that:
						
					then we have:
						\thickmuskip=0mu
						\medmuskip=0mu
						
						\thickmuskip=3mu
						\medmuskip=3mu
					Thus, this tool can not be used as a measure of dispersion!

					By extension, the arithmetic average of the weighted deviations from the average is also equal to zero:
					
					\begin{flushright}
						$\blacksquare$  Q.E.D.
					\end{flushright}
				\end{dem}
				This result is quite important because it will further be useful for a better understanding of the concept of standard deviation and variance.
			\item[P4.] Now we would like to prove that:
				
					\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
The comparisons between the above means/averages and the median or the weighted or moving averages does not make sense this is why we won't compare them.
	\end{tcolorbox}
				\begin{dem}
					First, we consider two non-zero real numbers $x_1$ and $x_2$ as $x_2>x_1>0$ and then we write:
					\begin{enumerate}
						\item The arithmetic average:
							
						\item The geometric mean:
							
						\item The harmonic mean:
							
						\item The root mean square:
										
					\end{enumerate}
					We will start to prove that $\mu_g>\mu_h$ by contradiction by putting $\mu_g-\mu_h<0$:
					
					By convenience we will now put:
						
					and we know that $y>1$. We therefore have:
						
						and remember we search if it is possible that:
						
						 We can now easily check this statement from the following equivalences:
						 
						There is also a contradiction, and this validates our initial hypothesis:
							
						Let see if $\mu_g>\mu_a$.
						
						Under the hypothesis $x_2>x_1>0$. We search now to prove that:
							
						Now we have the following equivalences:
							
						and the last expression is obviously correct because the square of a (real) number is always positive which verifies our initial hypothesis:
							
						We will prove now that $\mu_q > \mu_a$ by contradiction by putting $\mu_q-\mu_a<0$:
						
						But the square of a (real) number is always positive which verifies our initial hypothesis:
						
						We then have:
						
						\begin{flushright}
							$\blacksquare$  Q.E.D.
						\end{flushright}
				\end{dem}
				Once these inequalities proved, we can then move on to a figure that we attribute to Archimedes to place three of these averages. The interest of this example is to show that there are some remarkable relations between statistics and geometry (coincidence??).
				
				\begin{figure}[H]
					\centering
					\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
					%uncomment if require: \path (0,511); %set diagram left start at 0, and has height of 511
					
					%Shape: Circle [id:dp1837559032226015] 
					\draw   (170,179.75) .. controls (170,95.94) and (237.94,28) .. (321.75,28) .. controls (405.56,28) and (473.5,95.94) .. (473.5,179.75) .. controls (473.5,263.56) and (405.56,331.5) .. (321.75,331.5) .. controls (237.94,331.5) and (170,263.56) .. (170,179.75) -- cycle ;
					%Shape: Right Triangle [id:dp0018942976946714207] 
					\draw  [line width=0.75]  (406.5,53.75) -- (170,179.75) -- (406.5,179.75) -- cycle ;
					%Straight Lines [id:da3593360414534994] 
					\draw    (170,179.75) -- (473.5,179.75) ;
					%Straight Lines [id:da2781063825375074] 
					\draw    (317.5,175) -- (326,184) ;
					%Straight Lines [id:da518748487654034] 
					\draw    (316.5,185) -- (325.25,174.75) ;
					%Straight Lines [id:da2859352867594811] 
					\draw    (406.5,53.75) -- (321.75,179.75) ;
					%Straight Lines [id:da2321702497723792] 
					\draw    (348.5,141) -- (406.5,179.75) ;
					%Shape: Circle [id:dp2797131151519534] 
					\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (318.75,179.75) .. controls (318.75,178.09) and (320.09,176.75) .. (321.75,176.75) .. controls (323.41,176.75) and (324.75,178.09) .. (324.75,179.75) .. controls (324.75,181.41) and (323.41,182.75) .. (321.75,182.75) .. controls (320.09,182.75) and (318.75,181.41) .. (318.75,179.75) -- cycle ;
					%Shape: Circle [id:dp2197974808621499] 
					\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (403.5,179.75) .. controls (403.5,178.09) and (404.84,176.75) .. (406.5,176.75) .. controls (408.16,176.75) and (409.5,178.09) .. (409.5,179.75) .. controls (409.5,181.41) and (408.16,182.75) .. (406.5,182.75) .. controls (404.84,182.75) and (403.5,181.41) .. (403.5,179.75) -- cycle ;
					%Shape: Circle [id:dp8501490899982711] 
					\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (403.5,53.75) .. controls (403.5,52.09) and (404.84,50.75) .. (406.5,50.75) .. controls (408.16,50.75) and (409.5,52.09) .. (409.5,53.75) .. controls (409.5,55.41) and (408.16,56.75) .. (406.5,56.75) .. controls (404.84,56.75) and (403.5,55.41) .. (403.5,53.75) -- cycle ;
					%Shape: Rectangle [id:dp9507329899615893] 
					\draw   (361.31,139.78) -- (356.33,146.66) -- (348.5,141) -- (353.48,134.11) -- cycle ;
					%Shape: Circle [id:dp9367304574790465] 
					\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (352.91,140.39) .. controls (352.91,139.28) and (353.8,138.39) .. (354.91,138.39) .. controls (356.01,138.39) and (356.91,139.28) .. (356.91,140.39) .. controls (356.91,141.49) and (356.01,142.39) .. (354.91,142.39) .. controls (353.8,142.39) and (352.91,141.49) .. (352.91,140.39) -- cycle ;
					%Shape: Rectangle [id:dp6937614552233597] 
					\draw   (414.26,179.99) -- (406.5,179.75) -- (406.74,172.09) -- (414.5,172.33) -- cycle ;
					%Shape: Arc [id:dp23086765956726363] 
					\draw  [draw opacity=0] (407.61,74.03) .. controls (407.24,74.06) and (406.87,74.08) .. (406.5,74.08) .. controls (402.84,74.08) and (399.52,72.28) .. (397.05,69.35) -- (406.5,56.75) -- cycle ; \draw   (407.61,74.03) .. controls (407.24,74.06) and (406.87,74.08) .. (406.5,74.08) .. controls (402.84,74.08) and (399.52,72.28) .. (397.05,69.35) ;  
					
					% Text Node
					\draw (153,170) node [anchor=north west][inner sep=0.75pt]   [align=left] {$A$};
					% Text Node
					\draw (477,172) node [anchor=north west][inner sep=0.75pt]   [align=left] {$C$};
					% Text Node
					\draw (314,184) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
					% Text Node
					\draw (408,37) node [anchor=north west][inner sep=0.75pt]   [align=left] {$D$};
					% Text Node
					\draw (332,123) node [anchor=north west][inner sep=0.75pt]   [align=left] {$H$};
					% Text Node
					\draw (409,112) node [anchor=north west][inner sep=0.75pt]   [align=left] {$B$};
					% Text Node
					\draw (393,75) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \alpha $};
					\end{tikzpicture}
					\vspace*{3mm}
					\caption{Starting point for the geometric representation for the various averages}
				\end{figure}
				We will first write $a=\overline{AB},b=\overline{BC}$ and $\text{O}$ is the midpoint of $\overline{AC}$. Thus, the circle is drawn with center $\text{O}$ and radius $\overline{\text{O}A}$. $D$ is the intersection of the perpendicular segment $\overline{AC}$ through $B$ and of the circle $\Omega$ (we can choose the intersection we want). $H$ is itself the orthogonal projection of $B$ on $\overline{\text{O}D}$.
				
				Archimedes says that $\overline{\text{O}A}$ is the arithmetic average of $a$ and $b$ and that $\overline{BD}$ is the geometric mean of $a$ and $b$, and $\overline{DH}$ is the harmonic mean of $a$ and $b$.
				
				We then prove that (could be trivial):
				
	Therefore $\overline{\text{O}A}$ is the arithmetic average $\mu_a$ of $a$ and $b$.

	We have in the right-angled triangle $ADB$:
	
Then we have in the right-angled triangle $BDC$:
	
	We then add these two relations, and we get:
	
	We know that $D$ is on a circle of diameter $\overline{AC}$, so $ADC$ is rectangle on $D$. Therefore:
	
	And then we replace $\overline{BA}$ and $\overline{BC}$ by $a$ and $b$:
	
	So finally:
	
	And therefore, $\overline{DB}$  is the geometric mean $\mu_g$ of $a$ and $b$.
	
	We have now to prove that $\overline{DH}$ is the harmonic mean of $a$ and $b$. We have in a first time using the orthogonal projection as studied in the section of Vector Calculus (page \pageref{orthogonal projection vector}):
		
		Then we also have (again orthogonal projection):
		
		Therefore we have:
		
		and since ${DB}=\sqrt{ab}$, we have then:
			
		$\overline{DH}$ is therefore the harmonic mean of $a$ and $b.$ Archimedes was not wrong!
	\end{enumerate} 
	
	\paragraph{Jensen Inequality}\mbox{}\\\\
	The "\NewTerm{Jensen's inequality}\index{Jensen's inequality}\label{jensen inequality}" is a particularly important relation (or property) in finance and insurance because it shows by the proof why an options seller will take the expected mean of pay-off rather than pay-off of the expect mean or why ultimately the insured will always pay more or equal than the differential of the expected mean of accident costs, and also in time series analysis why GARCH processes are naturally leptokurtic and also for the proof of the Kullback-Leibler divergence why that latter is strictly positive or null (\SeeChapter{see section Statistical Mechanics page \pageref{kullback-leibler divergence}}).

	\begin{theorem}
	Jensen's inequality will allow us to prove that for a convex function (\SeeChapter{see section Functional Analysis page \pageref{convex function}}):
	
	with $p_i>0$, $\sum_i p_i=1$. Either by using notations specific to the domain of statistics:
	
	and to the valuation of options (\SeeChapter{see section Economy page \pageref{jensen inequality finance}}):
	
	\end{theorem}
	Before proceeding further, let us recall that a function $f$ is concave if $-f$ is convex and vice versa. It is then immediate that:
	
	Either by using notations specific to the domain of statistics:
	
	Let's go to the proof and then we will make a small practical simplified example with derivatives in finance.
	\begin{dem}
	Let us see if: 
	
	is true by proceeding by induction. First for $m=2$, we fall back on the classical definition  of a convex function:
	
	Or written differently as $p_1+p_2=1$:
	
	Now, we assume the previous relation true for $m=n$, and we show that it is also true for $m=n+1$:
	
	Since is $f$ is convex, we can write:
	
	Let us put:
	
	for $i=1\ldots n$. We have $q_i>0$ for all $i=1\ldots n$ and:
	
	because for recall:
	
	and consequently:
	
	By hypothesis of recurrence we then have:
	
	And therefore:
	
	by simplifying the expression on the right we get indeed:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	We know that the pay-offs of a Call and a Put are respectively at maturity (from the point of view of a buyer and therefore by symmetry from the point of view of a seller also in absolute value):
	
	Let us consider a price at maturity of $102.-$ and that the possible strikes at maturity are $\{100,110,150\}$, we have then have:
	
	and:
	
	and so we have well for a Call from the point of view of a buyer or seller in absolute values:
	
	\end{tcolorbox}
	All what we have seen so far (and that we will see further!) is related to random values localized on a straight line. For statistics on a circle or a sphere, named "\NewTerm{directional statistics}", we strongly recommend the reader to look at \cite{mardia2014statistics}.

	\pagebreak
	\subsection{Type of variables}
	In talking about variables quantitative or qualitative variables, sometimes you hear variables being described as categorical (or sometimes nominal), or ordinal, or interval.  Below we will define these terms and explain why they are important.

	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] The "\NewTerm{discrete variables}\index{discrete variables}" (by counting) that belongs to $\mathbb{Z}$: Are analysed with statistical laws based on a countable definition domain always strictly positive (the Poisson or Hypergeoemetric distribution are such typical case in the industry). Are almost always represented graphically by histograms.

	\item[D2.] The "\NewTerm{continuous variables}\index{continuous variable}" (by measure) that belong to $\mathbb{R}$: Are analysed with statistical laws based on an uncountable domain of definition strictly positive or may take any positive or negative value (typically the Normal distribution in the industry). Are almost always represented graphically by histograms with class intervals.

	\item[D3.] The "\NewTerm{attribute variables}\index{attribute variable}" (by classification): They are not digital data (only when they are coded with digits!) but qualitative data type \{Yes, No\}, \{Passed, Failed\}, \{On time, Late\}, \{red, green blue, black\}, etc. The binary data type attribute follow typically a Bernoulli or Binomial distribution while higher order qualitative variables have no average or standard deviation (indeed... try to calculate the mean and standard deviation between the qualitative variables Red, Green and Pink...).
	
	In attribute variable we mainly distinct two subtypes of variables:
		\begin{enumerate}
			\item A "\NewTerm{categorical variable}\index{categorical variable}" (sometimes named a "\NewTerm{nominal variable}\index{nominal variable}", or "\NewTerm{multicategorical variable}\index{multicategorical variable}" or "\NewTerm{polytomous variable}\index{polytomous variable}") is one that has two or more categories, but there is no intrinsic ordering to the categories.  For example, gender is a categorical variable having two categories (male and female) and there is no intrinsic ordering to the categories.  Hair color is also a categorical variable having a number of categories (blonde, brown, brunette, red, etc.) and again, there is no agreed way to order these from highest to lowest.  A purely categorical variable is one that simply allows you to assign categories but you cannot clearly order the variables.  If the variable has a clear ordering, then that variable would be an ordinal variable, as described below.
			
			\item An "\NewTerm{ordinal variable}\index{ordinal variable}" is similar to a categorical variable.  The difference between the two is that there is a clear ordering of the variables.  For example, suppose you have a variable, named\textit{ economic status}, with three categories: \{low, high, medium\}.  In addition to being able to classify people into these three categories, you can order the categories as low (1) $<$ medium (2) $<$ high (3). Now consider a variable like \textit{educational experience} (with values such as: \{high school graduate, college graduate, some college, elementary school graduate\}). These also can be ordered as: elementary school graduate (1) $<$ high school graduate (2) $<$ some college (3) $<$ college graduate (4).
		\end{enumerate}
	\end{enumerate}
	A pictorial and more complete summary of variable types in statistics is given in the figure below:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/arithmetics/type_of_variables.jpg}	
		\caption{Type of variables in statistics}
	\end{figure}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In principle, any continuous variable or any interval or ratio variable with more than two observed values is a polytomous variable, but the term is usually reserved for nominal and ordinal variables, named "unordered and ordered polytomous variables".
	\end{tcolorbox}
	Understanding the different types of data is an important discipline for the engineers because it has important implications for the type of analysis tools and techniques that will be used.

	A common question regarding the collection of data is what is the amount that should be collected. In fact it depends on the desired level of accuracy. We will see much further in this section (with proof!) how to mathematically determine the amount of data to collect.

	Now that we are relatively familiar with the concept of average (mean), we can discuss on more formal calculations and that will make sense.

	\subsubsection{Discrete Variables and Moments}
	
	Consider $X$ is an independent variable (an individual of a sample, whose property is independent of other individuals) that can take discrete random values (realizations of the vector $(X_1,X_2,\ldots ,X_n)$) with respective probabilities $(p_1,p_2,\ldots,p_n)$ where, by the axioms of probabilities (\SeeChapter{see section Probabilities page \pageref{kolmogorov axioms}}):
		
	
	Let $X$ be a numeric (quantitative) random variable (r.v.). It will be fully described in practice most of time by the value of the probability (for discrete variables) for a realization of this variable or by:
	
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] The probability corresponding to its value, named "\NewTerm{probability density function}\footnote{For discrete variable some use instead the terminology: "\NewTerm{probability mass function}" (PMF).}\index{probability density  function}" (P.D.F.) and denoted by:
		
		also sometimes (and more properly) denoted:
		
		keeping in mind that $\sum f_X(x)$ must be equal to $1$.
		
		\item[D2.]  The cumulative probability (for discrete AND continuous variables) to be typically less than or equal $X$ for all realizations $x$. This "\NewTerm{cumulative distribution function}\index{cumulative distribution function}" (C.D.F.) is denoted by:
		
		also sometimes (and more properly) denoted (for recall $D$ is the domain of definition as defined on page \pageref{domain of definition} in the section of Functional Analysis):
		
		with:
		
		where $F(x)$ is also named the "\NewTerm{repartition function}\index{repartition function}" of the random variable $X$. It is the theoretical proportion of the population whose value is less than or equal to $x$. It follows for example:
		
		More generally, for any two numbers $a$ and $b$ with $a<b$, we have:
		
		\begin{figure}[H]
			\centering
			\resizebox{\textwidth}{!}{\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
			%uncomment if require: \path (0,1076); %set diagram left start at 0, and has height of 1076
	
			%Shape: Rectangle [id:dp698151577959981] 
			\draw  [draw opacity=0][fill={rgb, 255:red, 242; green, 212; blue, 184 }  ,fill opacity=1 ] (361.8,279.28) -- (388.86,279.28) -- (388.86,282.02) -- (361.8,282.02) -- cycle ;
			%Shape: Rectangle [id:dp5839599555029737] 
			\draw  [draw opacity=0][fill={rgb, 255:red, 242; green, 212; blue, 184 }  ,fill opacity=1 ] (388.86,275.28) -- (417.86,275.28) -- (417.86,282.02) -- (388.86,282.02) -- cycle ;
			%Shape: Rectangle [id:dp8826703003907141] 
			\draw  [draw opacity=0][fill={rgb, 255:red, 242; green, 212; blue, 184 }  ,fill opacity=1 ] (417.86,251.28) -- (442.86,251.28) -- (442.86,282.02) -- (417.86,282.02) -- cycle ;
			%Shape: Rectangle [id:dp810063925932869] 
			\draw  [draw opacity=0][fill={rgb, 255:red, 242; green, 212; blue, 184 }  ,fill opacity=1 ] (442.86,211.28) -- (465.86,211.28) -- (465.86,282.02) -- (442.86,282.02) -- cycle ;
			%Shape: Rectangle [id:dp34477175994958587] 
			\draw  [draw opacity=0][fill={rgb, 255:red, 242; green, 212; blue, 184 }  ,fill opacity=1 ] (465.86,179.28) -- (490.86,179.28) -- (490.86,282.02) -- (465.86,282.02) -- cycle ;
			%Shape: Rectangle [id:dp051854600792217154] 
			\draw  [draw opacity=0][fill={rgb, 255:red, 242; green, 212; blue, 184 }  ,fill opacity=1 ] (490.86,143.28) -- (515.86,143.28) -- (515.86,282.02) -- (490.86,282.02) -- cycle ;
			%Shape: Rectangle [id:dp8332874265823342] 
			\draw  [draw opacity=0][fill={rgb, 255:red, 242; green, 212; blue, 184 }  ,fill opacity=1 ] (515.86,120.28) -- (540.86,120.28) -- (540.86,282.02) -- (515.86,282.02) -- cycle ;
			%Shape: Rectangle [id:dp23705704294826524] 
			\draw  [draw opacity=0][fill={rgb, 255:red, 242; green, 212; blue, 184 }  ,fill opacity=1 ] (540.86,99.28) -- (566.86,99.28) -- (566.86,282.02) -- (540.86,282.02) -- cycle ;
			%Shape: Rectangle [id:dp5054912550402186] 
			\draw  [draw opacity=0][fill={rgb, 255:red, 242; green, 212; blue, 184 }  ,fill opacity=1 ] (566.86,95.28) -- (593.86,95.28) -- (593.86,282.02) -- (566.86,282.02) -- cycle ;
			%Shape: Rectangle [id:dp5982579208394565] 
			\draw  [draw opacity=0][fill={rgb, 255:red, 242; green, 212; blue, 184 }  ,fill opacity=1 ] (593.86,90.28) -- (618.86,90.28) -- (618.86,282.02) -- (593.86,282.02) -- cycle ;
			%Shape: Rectangle [id:dp45943668567070794] 
			\draw  [draw opacity=0][fill={rgb, 255:red, 242; green, 212; blue, 184 }  ,fill opacity=1 ] (618.86,87.28) -- (641.86,87.28) -- (641.86,282.02) -- (618.86,282.02) -- cycle ;
			%Shape: Rectangle [id:dp3638646955142584] 
			\draw  [draw opacity=0][fill={rgb, 255:red, 228; green, 202; blue, 255 }  ,fill opacity=1 ] (252.39,204.19) -- (263.45,204.19) -- (263.45,282.02) -- (252.39,282.02) -- cycle ;
			%Shape: Rectangle [id:dp24646537076917596] 
			\draw  [draw opacity=0][fill={rgb, 255:red, 228; green, 202; blue, 255 }  ,fill opacity=1 ] (227.02,140.19) -- (238.08,140.19) -- (238.08,282.02) -- (227.02,282.02) -- cycle ;
			%Shape: Rectangle [id:dp21396869574212296] 
			\draw  [draw opacity=0][fill={rgb, 255:red, 228; green, 202; blue, 255 }  ,fill opacity=1 ] (201.65,89.19) -- (212.71,89.19) -- (212.71,282.02) -- (201.65,282.02) -- cycle ;
			%Shape: Rectangle [id:dp1806706358103709] 
			\draw  [draw opacity=0][fill={rgb, 255:red, 228; green, 202; blue, 255 }  ,fill opacity=1 ] (96.17,171.01) -- (108.23,171.01) -- (108.23,282.02) -- (96.17,282.02) -- cycle ;
			%Shape: Rectangle [id:dp39712777638196384] 
			\draw  [draw opacity=0][fill={rgb, 255:red, 228; green, 202; blue, 255 }  ,fill opacity=1 ] (69.8,231.01) -- (81.86,231.01) -- (81.86,282.02) -- (69.8,282.02) -- cycle ;
			%Shape: Rectangle [id:dp480777299526411] 
			\draw  [draw opacity=0][fill={rgb, 255:red, 228; green, 202; blue, 255 }  ,fill opacity=1 ] (122.54,97.19) -- (134.6,97.19) -- (134.6,282.02) -- (122.54,282.02) -- cycle ;
			%Shape: Rectangle [id:dp029234119737204356] 
			\draw  [draw opacity=0][fill={rgb, 255:red, 228; green, 202; blue, 255 }  ,fill opacity=1 ] (148.91,61.37) -- (160.97,61.37) -- (160.97,282.02) -- (148.91,282.02) -- cycle ;
			%Shape: Rectangle [id:dp134517437005246] 
			\draw  [draw opacity=0][fill={rgb, 255:red, 228; green, 202; blue, 255 }  ,fill opacity=1 ] (175.28,61.37) -- (187.34,61.37) -- (187.34,282.02) -- (175.28,282.02) -- cycle ;
			%Shape: Rectangle [id:dp2510403165877322] 
			\draw  [draw opacity=0][fill={rgb, 255:red, 228; green, 202; blue, 255 }  ,fill opacity=1 ] (277.75,245.19) -- (288.8,245.19) -- (288.8,282.02) -- (277.75,282.02) -- cycle ;
			%Shape: Rectangle [id:dp6602868118206136] 
			\draw  [draw opacity=0][fill={rgb, 255:red, 228; green, 202; blue, 255 }  ,fill opacity=1 ] (303.8,259.82) -- (316.86,259.82) -- (316.86,282.02) -- (303.8,282.02) -- cycle ;
			%Shape: Rectangle [id:dp20465340606567062] 
			\draw  [draw opacity=0][fill={rgb, 255:red, 228; green, 202; blue, 255 }  ,fill opacity=1 ] (44.8,273.82) -- (57.86,273.82) -- (57.86,282.02) -- (44.8,282.02) -- cycle ;
			%Shape: Axis 2D [id:dp4863373565746636] 
			\draw  (36.86,282.02) -- (325.86,282.02)(50.46,67.28) -- (50.46,298.01) (318.86,277.02) -- (325.86,282.02) -- (318.86,287.02) (45.46,74.28) -- (50.46,67.28) -- (55.46,74.28) (63.46,277.02) -- (63.46,287.02)(76.46,277.02) -- (76.46,287.02)(89.46,277.02) -- (89.46,287.02)(102.46,277.02) -- (102.46,287.02)(115.46,277.02) -- (115.46,287.02)(128.46,277.02) -- (128.46,287.02)(141.46,277.02) -- (141.46,287.02)(154.46,277.02) -- (154.46,287.02)(167.46,277.02) -- (167.46,287.02)(180.46,277.02) -- (180.46,287.02)(193.46,277.02) -- (193.46,287.02)(206.46,277.02) -- (206.46,287.02)(219.46,277.02) -- (219.46,287.02)(232.46,277.02) -- (232.46,287.02)(245.46,277.02) -- (245.46,287.02)(258.46,277.02) -- (258.46,287.02)(271.46,277.02) -- (271.46,287.02)(284.46,277.02) -- (284.46,287.02)(297.46,277.02) -- (297.46,287.02)(310.46,277.02) -- (310.46,287.02)(45.46,269.02) -- (55.46,269.02)(45.46,256.02) -- (55.46,256.02)(45.46,243.02) -- (55.46,243.02)(45.46,230.02) -- (55.46,230.02)(45.46,217.02) -- (55.46,217.02)(45.46,204.02) -- (55.46,204.02)(45.46,191.02) -- (55.46,191.02)(45.46,178.02) -- (55.46,178.02)(45.46,165.02) -- (55.46,165.02)(45.46,152.02) -- (55.46,152.02)(45.46,139.02) -- (55.46,139.02)(45.46,126.02) -- (55.46,126.02)(45.46,113.02) -- (55.46,113.02)(45.46,100.02) -- (55.46,100.02)(45.46,87.02) -- (55.46,87.02) ;
			\draw   ;
			%Shape: Axis 2D [id:dp17737995161743947] 
			\draw  (361.86,282.02) -- (652.86,282.02)(375.56,67.28) -- (375.56,298.01) (645.86,277.02) -- (652.86,282.02) -- (645.86,287.02) (370.56,74.28) -- (375.56,67.28) -- (380.56,74.28) (388.56,277.02) -- (388.56,287.02)(401.56,277.02) -- (401.56,287.02)(414.56,277.02) -- (414.56,287.02)(427.56,277.02) -- (427.56,287.02)(440.56,277.02) -- (440.56,287.02)(453.56,277.02) -- (453.56,287.02)(466.56,277.02) -- (466.56,287.02)(479.56,277.02) -- (479.56,287.02)(492.56,277.02) -- (492.56,287.02)(505.56,277.02) -- (505.56,287.02)(518.56,277.02) -- (518.56,287.02)(531.56,277.02) -- (531.56,287.02)(544.56,277.02) -- (544.56,287.02)(557.56,277.02) -- (557.56,287.02)(570.56,277.02) -- (570.56,287.02)(583.56,277.02) -- (583.56,287.02)(596.56,277.02) -- (596.56,287.02)(609.56,277.02) -- (609.56,287.02)(622.56,277.02) -- (622.56,287.02)(635.56,277.02) -- (635.56,287.02)(370.56,269.02) -- (380.56,269.02)(370.56,256.02) -- (380.56,256.02)(370.56,243.02) -- (380.56,243.02)(370.56,230.02) -- (380.56,230.02)(370.56,217.02) -- (380.56,217.02)(370.56,204.02) -- (380.56,204.02)(370.56,191.02) -- (380.56,191.02)(370.56,178.02) -- (380.56,178.02)(370.56,165.02) -- (380.56,165.02)(370.56,152.02) -- (380.56,152.02)(370.56,139.02) -- (380.56,139.02)(370.56,126.02) -- (380.56,126.02)(370.56,113.02) -- (380.56,113.02)(370.56,100.02) -- (380.56,100.02)(370.56,87.02) -- (380.56,87.02) ;
			\draw   ;
			%Straight Lines [id:da9064582210098973] 
			\draw [color={rgb, 255:red, 144; green, 19; blue, 254 }  ,draw opacity=1 ][line width=1.5]    (44.8,273.82) -- (57.86,273.82) ;
			%Straight Lines [id:da08337324306581961] 
			\draw [color={rgb, 255:red, 144; green, 19; blue, 254 }  ,draw opacity=1 ][line width=1.5]    (69.8,230.01) -- (81.86,230.01) ;
			%Straight Lines [id:da21095592221418347] 
			\draw [color={rgb, 255:red, 144; green, 19; blue, 254 }  ,draw opacity=1 ][line width=1.5]    (96.17,170.01) -- (108.23,170.01) ;
			%Straight Lines [id:da1992889369772206] 
			\draw [color={rgb, 255:red, 144; green, 19; blue, 254 }  ,draw opacity=1 ][line width=1.5]    (122.54,96.19) -- (134.6,96.19) ;
			%Straight Lines [id:da523675754106169] 
			\draw [color={rgb, 255:red, 144; green, 19; blue, 254 }  ,draw opacity=1 ][line width=1.5]    (148.91,60.37) -- (160.97,60.37) ;
			%Straight Lines [id:da8351851242968904] 
			\draw [color={rgb, 255:red, 144; green, 19; blue, 254 }  ,draw opacity=1 ][line width=1.5]    (175.28,60.37) -- (187.34,60.37) ;
			%Straight Lines [id:da2818634450587314] 
			\draw [color={rgb, 255:red, 144; green, 19; blue, 254 }  ,draw opacity=1 ][line width=1.5]    (201.65,88.19) -- (212.71,88.19) ;
			%Straight Lines [id:da113241178541728] 
			\draw [color={rgb, 255:red, 144; green, 19; blue, 254 }  ,draw opacity=1 ][line width=1.5]    (227.02,139.19) -- (238.08,139.19) ;
			%Straight Lines [id:da7315196104616244] 
			\draw [color={rgb, 255:red, 144; green, 19; blue, 254 }  ,draw opacity=1 ][line width=1.5]    (252.39,203.19) -- (263.45,203.19) ;
			%Straight Lines [id:da03631873093421634] 
			\draw [color={rgb, 255:red, 144; green, 19; blue, 254 }  ,draw opacity=1 ][line width=1.5]    (277.75,245.19) -- (288.8,245.19) ;
			%Straight Lines [id:da5101584949993043] 
			\draw [color={rgb, 255:red, 144; green, 19; blue, 254 }  ,draw opacity=1 ][line width=1.5]    (303.8,259.82) -- (316.86,259.82) ;
			%Straight Lines [id:da48757066760438894] 
			\draw [color={rgb, 255:red, 212; green, 137; blue, 87 }  ,draw opacity=1 ]   (417.86,275.28) -- (417.86,282.02) ;
			%Straight Lines [id:da9896559965428404] 
			\draw [color={rgb, 255:red, 212; green, 137; blue, 87 }  ,draw opacity=1 ]   (442.86,251.28) -- (442.86,282.02) ;
			%Straight Lines [id:da702167940061516] 
			\draw [color={rgb, 255:red, 212; green, 137; blue, 87 }  ,draw opacity=1 ]   (465.86,211.28) -- (465.86,282.02) ;
			%Straight Lines [id:da803845119851222] 
			\draw [color={rgb, 255:red, 212; green, 137; blue, 87 }  ,draw opacity=1 ]   (490.86,179.28) -- (490.86,282.02) ;
			%Straight Lines [id:da26101766522735037] 
			\draw [color={rgb, 255:red, 212; green, 137; blue, 87 }  ,draw opacity=1 ]   (515.86,143.28) -- (515.86,282.02) ;
			%Straight Lines [id:da6450067505511707] 
			\draw [color={rgb, 255:red, 212; green, 137; blue, 87 }  ,draw opacity=1 ]   (540.86,120.28) -- (540.86,282.02) ;
			%Straight Lines [id:da2889596205339655] 
			\draw [color={rgb, 255:red, 212; green, 137; blue, 87 }  ,draw opacity=1 ]   (566.86,99.28) -- (566.86,282.02) ;
			%Straight Lines [id:da6838982027502796] 
			\draw [color={rgb, 255:red, 212; green, 137; blue, 87 }  ,draw opacity=1 ]   (593.86,95.28) -- (593.86,282.02) ;
			%Straight Lines [id:da8368385569412817] 
			\draw [color={rgb, 255:red, 212; green, 137; blue, 87 }  ,draw opacity=1 ]   (618.86,90.28) -- (618.86,282.02) ;
			%Straight Lines [id:da8181037415842498] 
			\draw [color={rgb, 255:red, 212; green, 137; blue, 87 }  ,draw opacity=1 ]   (641.86,87.28) -- (641.86,282.02) ;
			%Straight Lines [id:da7671503178884753] 
			\draw [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][line width=1.5]    (361.8,279.28) -- (387.25,279.28) ;
			\draw [shift={(388.86,279.28)}, rotate = 0] [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][line width=1.5]      (0, 0) circle [x radius= 2.61, y radius= 2.61]   ;
			\draw [shift={(361.8,279.28)}, rotate = 0] [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][fill={rgb, 255:red, 139; green, 87; blue, 42 }  ,fill opacity=1 ][line width=1.5]      (0, 0) circle [x radius= 2.61, y radius= 2.61]   ;
			%Straight Lines [id:da11814974670515399] 
			\draw [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][line width=1.5]    (388.86,275.28) -- (416.25,275.28) ;
			\draw [shift={(417.86,275.28)}, rotate = 0] [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][line width=1.5]      (0, 0) circle [x radius= 2.61, y radius= 2.61]   ;
			\draw [shift={(388.86,275.28)}, rotate = 0] [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][fill={rgb, 255:red, 139; green, 87; blue, 42 }  ,fill opacity=1 ][line width=1.5]      (0, 0) circle [x radius= 2.61, y radius= 2.61]   ;
			%Straight Lines [id:da26947472543873996] 
			\draw [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][line width=1.5]    (417.86,251.28) -- (441.25,251.28) ;
			\draw [shift={(442.86,251.28)}, rotate = 0] [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][line width=1.5]      (0, 0) circle [x radius= 2.61, y radius= 2.61]   ;
			\draw [shift={(417.86,251.28)}, rotate = 0] [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][fill={rgb, 255:red, 139; green, 87; blue, 42 }  ,fill opacity=1 ][line width=1.5]      (0, 0) circle [x radius= 2.61, y radius= 2.61]   ;
			%Straight Lines [id:da4739511038660622] 
			\draw [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][line width=1.5]    (442.86,211.28) -- (464.25,211.28) ;
			\draw [shift={(465.86,211.28)}, rotate = 0] [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][line width=1.5]      (0, 0) circle [x radius= 2.61, y radius= 2.61]   ;
			\draw [shift={(442.86,211.28)}, rotate = 0] [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][fill={rgb, 255:red, 139; green, 87; blue, 42 }  ,fill opacity=1 ][line width=1.5]      (0, 0) circle [x radius= 2.61, y radius= 2.61]   ;
			%Straight Lines [id:da2802809937768298] 
			\draw [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][line width=1.5]    (467.86,179.28) -- (489.25,179.28) ;
			\draw [shift={(490.86,179.28)}, rotate = 0] [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][line width=1.5]      (0, 0) circle [x radius= 2.61, y radius= 2.61]   ;
			\draw [shift={(467.86,179.28)}, rotate = 0] [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][fill={rgb, 255:red, 139; green, 87; blue, 42 }  ,fill opacity=1 ][line width=1.5]      (0, 0) circle [x radius= 2.61, y radius= 2.61]   ;
			%Straight Lines [id:da9143367156902595] 
			\draw [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][line width=1.5]    (490.86,143.28) -- (512.25,143.28) ;
			\draw [shift={(513.86,143.28)}, rotate = 0] [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][line width=1.5]      (0, 0) circle [x radius= 2.61, y radius= 2.61]   ;
			\draw [shift={(490.86,143.28)}, rotate = 0] [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][fill={rgb, 255:red, 139; green, 87; blue, 42 }  ,fill opacity=1 ][line width=1.5]      (0, 0) circle [x radius= 2.61, y radius= 2.61]   ;
			%Straight Lines [id:da841814257181468] 
			\draw [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][line width=1.5]    (515.86,120.28) -- (537.25,120.28) ;
			\draw [shift={(538.86,120.28)}, rotate = 0] [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][line width=1.5]      (0, 0) circle [x radius= 2.61, y radius= 2.61]   ;
			\draw [shift={(515.86,120.28)}, rotate = 0] [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][fill={rgb, 255:red, 139; green, 87; blue, 42 }  ,fill opacity=1 ][line width=1.5]      (0, 0) circle [x radius= 2.61, y radius= 2.61]   ;
			%Straight Lines [id:da7440693394128206] 
			\draw [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][line width=1.5]    (540.86,99.28) -- (562.25,99.28) ;
			\draw [shift={(563.86,99.28)}, rotate = 0] [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][line width=1.5]      (0, 0) circle [x radius= 2.61, y radius= 2.61]   ;
			\draw [shift={(540.86,99.28)}, rotate = 0] [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][fill={rgb, 255:red, 139; green, 87; blue, 42 }  ,fill opacity=1 ][line width=1.5]      (0, 0) circle [x radius= 2.61, y radius= 2.61]   ;
			%Straight Lines [id:da07418044238474675] 
			\draw [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][line width=1.5]    (566.86,95.28) -- (588.25,95.28) ;
			\draw [shift={(589.86,95.28)}, rotate = 0] [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][line width=1.5]      (0, 0) circle [x radius= 2.61, y radius= 2.61]   ;
			\draw [shift={(566.86,95.28)}, rotate = 0] [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][fill={rgb, 255:red, 139; green, 87; blue, 42 }  ,fill opacity=1 ][line width=1.5]      (0, 0) circle [x radius= 2.61, y radius= 2.61]   ;
			%Straight Lines [id:da7428952122941432] 
			\draw [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][line width=1.5]    (595.86,90.28) -- (617.25,90.28) ;
			\draw [shift={(618.86,90.28)}, rotate = 0] [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][line width=1.5]      (0, 0) circle [x radius= 2.61, y radius= 2.61]   ;
			\draw [shift={(595.86,90.28)}, rotate = 0] [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][fill={rgb, 255:red, 139; green, 87; blue, 42 }  ,fill opacity=1 ][line width=1.5]      (0, 0) circle [x radius= 2.61, y radius= 2.61]   ;
			%Straight Lines [id:da7418952471137537] 
			\draw [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][line width=1.5]    (618.86,87.28) -- (640.25,87.28) ;
			\draw [shift={(641.86,87.28)}, rotate = 0] [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][line width=1.5]      (0, 0) circle [x radius= 2.61, y radius= 2.61]   ;
			\draw [shift={(618.86,87.28)}, rotate = 0] [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][fill={rgb, 255:red, 139; green, 87; blue, 42 }  ,fill opacity=1 ][line width=1.5]      (0, 0) circle [x radius= 2.61, y radius= 2.61]   ;
			
			% Text Node
			\draw (38,285.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
			% Text Node
			\draw (97,290) node [anchor=north west][inner sep=0.75pt]   [align=left] {2};
			% Text Node
			\draw (147,290) node [anchor=north west][inner sep=0.75pt]   [align=left] {4};
			% Text Node
			\draw (200,290) node [anchor=north west][inner sep=0.75pt]   [align=left] {6};
			% Text Node
			\draw (252,290) node [anchor=north west][inner sep=0.75pt]   [align=left] {8};
			% Text Node
			\draw (300,290) node [anchor=north west][inner sep=0.75pt]   [align=left] {10};
			% Text Node
			\draw (7,209.4) node [anchor=north west][inner sep=0.75pt]    {$0.05$};
			% Text Node
			\draw (7,145.4) node [anchor=north west][inner sep=0.75pt]    {$0.10$};
			% Text Node
			\draw (7,78.4) node [anchor=north west][inner sep=0.75pt]    {$0.15$};
			% Text Node
			\draw (363,285.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
			% Text Node
			\draw (422,290) node [anchor=north west][inner sep=0.75pt]   [align=left] {2};
			% Text Node
			\draw (472,290) node [anchor=north west][inner sep=0.75pt]   [align=left] {4};
			% Text Node
			\draw (525,290) node [anchor=north west][inner sep=0.75pt]   [align=left] {6};
			% Text Node
			\draw (577,290) node [anchor=north west][inner sep=0.75pt]   [align=left] {8};
			% Text Node
			\draw (624,290) node [anchor=north west][inner sep=0.75pt]   [align=left] {10};
			% Text Node
			\draw (343,233.4) node [anchor=north west][inner sep=0.75pt]    {$0.2$};
			% Text Node
			\draw (343,155.9) node [anchor=north west][inner sep=0.75pt]    {$0.6$};
			% Text Node
			\draw (343,78.4) node [anchor=north west][inner sep=0.75pt]    {$1.0$};
			% Text Node
			\draw (343,194.65) node [anchor=north west][inner sep=0.75pt]    {$0.4$};
			% Text Node
			\draw (343,117.15) node [anchor=north west][inner sep=0.75pt]    {$0.8$};
			% Text Node
			\draw (161,19) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{{\Large PDF}}};
			% Text Node
			\draw (482,19) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{{\Large CDF}}};
			\end{tikzpicture}}
		\end{figure}
		This definition obviously also applies to continuous variables!
	
		\item[D3.] The "\NewTerm{empirical cumulated distribution function}\index{empirical cumulated distribution function}" (E.C.D.F.) is naturally defined by (we have indicated the different notations that you can find in the literature):
		
		with $\hat{F}_n(x)\in [0,1]$ and associated with the sample of independent and identically distributed variables which as we know is named a "random vector" denoted by $\vec{X}=(x_1,x_2,\ldots ,x_n)$.
		
		It is simply the cumulative frequencies of appearance normalized to unity below a certain fixed value (approach that the majority of human beings are naturally using when seeking the repartition function).
		
		So if we take again the example of wages already used above, then we have for example for $x$ fixed to 1,800:
		\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|c|c|}
					\hline
	\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Ordered Wages $x_i \leq x$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Frequencies $1_{x_i \leq x}$}} \\ \hline
			$1,200$ & $1$ \\ \hline
			$1,220$ & $1$ \\ \hline
			$1,250$ & $1$ \\ \hline
			$1,300$ & $1$ \\ \hline
			$1,350$ & $1$ \\ \hline
			$1,450$ & $1$ \\ \hline
			$1,450$ & $1$ \\ \hline
			$1,560$ & $1$ \\ \hline
			$1,600$ & $1$ \\ \hline
			$1,800$ & $1$ \\ \hline	
			$1,900$ & $0$ \\ \hline
			$2,150$ & $0$ \\ \hline
			$2,310$ & $0$ \\ \hline
			$2,600$ & $0$ \\ \hline
			$3,000$ & $0$ \\ \hline
			$3,400$ & $0$ \\ \hline
			$4,800$ & $0$ \\ \hline
		\end{tabular}
		\end{center}
		\caption[]{Example of the empirical repartition function}
		\end{table}
		And then:
		
		The repartition function is clearly a monotonically increasing function (or more precisely "non-decreasing") whose values range from $0$ to $1$.
		
		Let's look at a property that we will use several times and which is important in practice (property valid for both discrete and continuous random variables):

		Let $X$ be a random variable of repartition function $F_{X}$ and density function $f_{X}$ (let us recall that: $ f_{X} = F_{X}^{\prime}$) and $c^{te} \in \mathbb{R}^{*}$ a constant such that $y=c^{te}X$. We want to determine the repartition function and the density function of the random variable $Y$ from $F_{X}$ and $f_{X}$ respectively.
		
		By definition for $y \in \mathbb{R}$:
		
		Therefore:
		
		With regard to the density function we have:
		
		Therefore:
		
	\end{enumerate}
	
	\paragraph{Mean and Variance of Discrete Random Variables}\mbox{}\\\\
	\textbf{Definition (\#\thesection.\mydef):} We define the "\NewTerm{expectation}\index{expectation}" or "\NewTerm{mean}\index{mean}", also named "\NewTerm{moment of order $1$}\index{moment of order $1$}", of the random variable $X$ by the relation (with various notations)\label{expected mean discrete variable}:
	
also sometimes named "\NewTerm{parts rule}\index{parts rule}".

	In other words, we know that for every event in the sample space is associated with a probability that we also associate with a value (given by the random variable). The question then is to know what value we can get on the long term? The expected value (the mean...) is then the weighted average, by the probability, of all values of the events of sample space.

	If the probability is given by a discrete distribution function $f(x_i)$ (see the definitions of distribution functions further below in the text) of the random variable, we then have: 
	

	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} The mean $\mu_X$ can also be written just simply $\mu$ if there are no possible confusion on the random variable.\\

	\textbf{R2.} If we consider each realization of the random variables $(x_1,x_2,\ldots ,x_n)$ as the components of a vector $\vec{x}$ and each associated probability (or ponderation) $(p_1,p_2,\ldots ,p_n)$ as the components of a vector $\vec{p}$ we can write the mean in a technical way using the scalar product (\SeeChapter{see section Vector Calculus page \pageref{dot product}}) often written:
	
	\end{tcolorbox}	
Here are the most important mathematical properties of the mean for any random variable (whatever the distribution law!) and that we will often use throughout this section (and many other involving statistics):
	\begin{enumerate}
		\item[P1.] Multiplication by a constant (homogeneous):
			
		\item[P2.] Sum of two random variables (independent or not!):
			
			Where we used in the 4th line, the property view in the section of Probabilities (page \pageref{disjoint probability}):
			
			We deduce that for $n$ random variables $X_i$ following any probability distribution:
			
			
			\item[P3.] Then mean of a constant $a$ is equal to the constant itself:
				
		
			\item[P4.] Mean of a product of two random variables:
				
				And if the two random variables are independent, then the probability is equal to the joint probability (\SeeChapter{see section Probabilities page \pageref{joint probability}}). Therefore we have:
				
	\end{enumerate}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	For a unique dice the expected value is:
	$$\text{E}(X)=\dfrac{1}{6}\cdot 1+\dfrac{1}{6}\cdot 2+\dfrac{1}{6}\cdot 3+\dfrac{1}{6}\cdot 4+\dfrac{1}{6}\cdot 5+\dfrac{1}{6}\cdot 6=\dfrac{1}{6}\cdot 21=3.5$$
	Using the property of linearity of the mean, we have then for the sum of $n$ dices:
	$$\text{E}_n(X)=n\text{E}(X)=n\cdot 3.5$$
	For the special case of two dices, there the most likely sum will be:
	$$\text{E}_2(X)=2\text{E}(X)=2\cdot 3.5=7$$
	\end{tcolorbox}
	
	So the mean of the product of independent random variables is always equal to the product of their means.

	We will assume as obvious that these four properties extend to the continuous case!

	\textbf{Definition (\#\thesection.\mydef):} After having translated the trend by the mean it is interesting to have and indicator that reflects the dispersion or "\NewTerm{dispersion}\index{dispersion}" around the mean by a value named "\NewTerm{variance of $X$}\index{variance}\label{variance discrete variable}" or "\NewTerm{second-order centered moment}\index{second-order centered moment}" or "\NewTerm{mean square error (MSE)}\index{mean square error}, written $\text{V}(X)$ or $\sigma_X^2$ (read "sigma square") and given in its discrete form by:
	
	In other words the variance measure of how much a distribution of values is spread from the mean.
	
	The variance is however not directly comparable to the mean because of the fact that the units of the variance are the square of the unit of the random variable, which follows directly from its definition. To have an indicator of dispersion that can be compared to the parameters of central tendency (mean, median and ... mode), it then suffices to take the square root of the variance.

For convenience, we define the "\NewTerm{standard deviation}" of $X$ by:
	
	The standard deviation is therefore the root mean square of the deviations (or "mean squared difference") between the observations and their mean.
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} The standard deviation $\sigma_X$ of the random variable $X$ can be simply denoted $\sigma$ if there is no possible confusion.\\

	\textbf{R2.} The standard deviation and variance are, in the literature, often named "\NewTerm{dispersion parameters}\index{dispersion parameters}" or "\NewTerm{scale parameters}\index{scale parameters}"  as opposed to the mean, mode and median that are named "\NewTerm{positional parameters}\index{propositional parameters}". 
	\end{tcolorbox}	
	Notice that when two different types of measurement methods give different statistical distributions of the variable of interest, then in physics and statistics, the distance between the two peaks of some unimodal distributions is named the "\NewTerm{tension}\index{tension}". For example in the year 12021 (holocene calendar), the tension between the cosmological radio-wave background measurement method and the direct measurement method of the Hubble constant has a tension of $5\sigma$ as illustrated below:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{img/arithmetics/tension.jpg}
		\caption{Illustration of "tension" in statistics}
	\end{figure}
	
	\textbf{Definition (\#\thesection.\mydef):} The ratio (expressed in \%):
	
is often used in business to compare the mean and the standard deviation and is named the "\NewTerm{coefficient of variation C.V.}\index{coefficient of variation}\label{coefficient of variation}" because it has no units (which is it's main advantage!) and because many industrial statistical methods consider that a good C.V should ideally be just about a few \% only.

	More generally for any statistics estimator $\hat{\theta}$ (sum, average, median, etc.) we can build a coefficient of variation such that:
	
	Thus, in practice we consider that:
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|p{3.5cm}|p{3cm}|}
			\hline
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Coefficient of variation}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Quality}} \\ \hline
			\centering\arraybackslash\ 20\% & \centering\arraybackslash\ Poor \\ \hline
			\centering\arraybackslash\ 10\% & \centering\arraybackslash\ Acceptable  \\ \hline
			\centering\arraybackslash\ 5\% & \centering\arraybackslash\ Controlled  \\ \hline
			\centering\arraybackslash\ 2.5\% & \centering\arraybackslash\ Excellent  \\ \hline
			\centering\arraybackslash\ 1.25\% & \centering\arraybackslash\ World Class  \\ \hline
			\centering\arraybackslash\ 0.0625\% & \centering\arraybackslash\ Rarely achieved \\ \hline
		\end{tabular}
		\caption{Qualitative judgements of C.Vs commonly accepted}
	\end{table}	
	Why do we find a square (respectively a square root) in the definition of the variance? The intuitive reason is simple (the rigorous much less ...). Remember, that we have shown above that the sum of the deviations from the actual weighted average is always zero:
	
If we assimilate the size of each sample by the probability by normalizing the sample size with respect to $n$, we come upon a relation that is the same as the variance with the difference that the term in brackets is not squared. And then we immediately see the problem... the dispersion measure is always zero, hence the need to bring this to the square.

We could, however, imagine to use the absolute value of deviations from the mean, but for a number of reasons that we will see later during our study of estimators, the choice of squaring is quite natural.

Note, however, the common use in the industry of two common other indicators of dispersion:
	\begin{enumerate}
		\item The "\NewTerm{mean absolute deviation}\index{mean absolute deviation}" (mean of the absolute values of deviations from the mean):
			
			Which is a very elementary indicator used when we do not want to make statistical inference on a series of measures. This deviation can be easily calculated in the English version of Microsoft Excel 11.8346 using the \texttt{AVEDEV( )} function.
			
		\item The "\NewTerm{median absolute deviation}\index{median absolute deviation}" denoted MAD (median of absolute values of deviations from the median):
			
			which is considered as a more robust measure of dispersion than those given by the mean absolute deviation or the standard deviation (unfortunately this indicator is not natively integrated in spreadsheets softwares). That latter must not be confused with the "\NewTerm{mean median absolute deviation}\index{mean median absolute deviation}" defined by:
			
			\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
			In order to use the MAD as a consistent estimator for the estimation of the standard deviation $\sigma$ (see further below for the definition of the standard deviation) as we will need it for our study of Control Charts (\SeeChapter{see section of the corresponding name page \pageref{quality control charts}}), one takes:
			
			where $k$ is a constant scale factor, which depends on the distribution.\\
			
			For normally distributed data as we will see during our study of the half-normal distribution (see page \pageref{half normal distribution}) we take:
			 
			Hence:
			
			\end{tcolorbox}	
	\end{enumerate}
	Having seen the definition of the both concepts let us see now through an example  how to calculate them and their difference on a same set of data!
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\textbf{{\Large \ding{45}}Example:}\\\\
Consider the following measure of a random variable $X$:
	
	and where the median value is given as we know by:
	
	The absolute deviations from the median are then:
	
	Placed in ascending order, we then have:
	
	where we easily identify the absolute deviation from the median, which is:
	
	\end{tcolorbox}
	
	In the case where we have at disposition a series of measures, we can estimate the experimental value of the mean (expectation) and of the variance with the following estimators (it is simply the of average and standard deviation of a sample when the events are equally likely) with the specific notation:
		
		\begin{dem}
			First for the mean:
			
			And for the variance:
			
			\begin{flushright}
				$\blacksquare$  Q.E.D.
			\end{flushright}
		\end{dem}
		\begin{theorem}
			Let us prove now a very nice little property as the arithmetic average is an optimum for the sum of squared errors.
		\end{theorem}
		\begin{dem}
			
			And if we search for $\alpha$ as the derivative of the above expression is equal to zero:
			
			then $\alpha$ is an optimum. We have therefore:
			
			or after rearrangement and an elementary simplification we get:
						
			\begin{flushright}
				$\blacksquare$  Q.E.D.
			\end{flushright}		
		\end{dem}
		It is effectively the arithmetic average! Now to see if it is a maximum extrema or minimum extrema we just need calculate the second derivative (\SeeChapter{see section Differential and Integral Calculus page \pageref{second derivative}}) and see if it gives a positive constant (i.e. the first derivative increases when $\alpha$ increase). Therefore we immediately see that it is effectively a minimum extrema!!!
		
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
The term of the sum that we see in the expression of the variance (standard deviation) is named the "\NewTerm{sum of squared deviations from the mean}\index{sum of squared deviations from the mean}" or "\NewTerm{sum of squared errors from the mean}\index{sum of squared errors from the mean}". We also name it the "\NewTerm{total sum of squares}\index{total sum of squares}", or "\NewTerm{total variation}\index{total variation}", or "\NewTerm{sum of square errors}\index{sum of square errors}" in the context of the study of the ANOVA (see the further below)
	\end{tcolorbox}	
	Before that we continue, let us recall the concept of geometric mean seen above (widely used for returns in finance or growth analyses in \% of sales):
	
	It's fine but employees in financial departments also need to calculate the standard deviation of this average. The idea is then to take the logarithm to reduce it to a simple arithmetic mean (it is still obviously an estimator!):
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	For the section of Finance, if we denote the decimal multiplier equivalent $R_i=1+r_i$ or a random variable return $r_i$ then:
	
	However if $\mu_g(R)\cong 1$ then using first order Taylor expansion $\ln(x)\cong x-1$ (around $x\cong 1$ obviously!) we get:
	
	So in other words: the arithmetic average of the log-returns (their decimal multiplier equivalent) is approximately equal to geometric mean of the returns (their decimal multiplier equivalent). It is an approximation you can often found in many financial computer codes.
	\end{tcolorbox}
	Therefore, since taking the logarithm of the values we have the arithmetic mean of the log values, then the logarithm of the geometric standard deviation (with physicist reasoning like...) will be:
	
	Then we just take the exponential of the standard deviation of the logarithms of the values to have the "\NewTerm{geometric standard deviation}\index{geometric standard deviation}":
	
	It can be easily proved, using the same method as earlier, that the geometric standard deviation of the returns (the last relation above) is approximately equal to the Pearson standard deviation of the log-returns.
	
	By the way, remember that we have proved earlier that $\mu_g < \mu_a$.
	
	The variance can also be written in the very important way named the "\NewTerm{Huygens relation}\index{Huygens relation}\label{huygens relation}" or "\NewTerm{König-Huygens theorem}\index{König-Huygens theorem}" or "\NewTerm{Steiner translation theorem}\index{Steiner translation theorem}" that we will reuse several times thereafter. Let's see what it is:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The reader may found interesting to know that in many business applications of statistics (especially in "Machine Learning"), when the statistics must be updated "on the fly" each time new data appear we speak then of "\NewTerm{data flow statistics}" or "\NewTerm{data stream mining}" or even "\NewTerm{online learning}". So an efficient algorithmic way must be found to update the calculations without recalculating everything from scratch! The simplest typical example is the variance! Updating the sample variance calculation looks obviously quite consuming if we just add new data according to:
	
	But using the Huygens relation, we see that this sample variance becomes much less consuming to calculate as it becomes only a problem of updating two sums:
	
	\end{tcolorbox}
	Let us now do a relatively small hook to a common scenario generator of errors in business when several statistical series are handled (very common case in the industry as well as in insurance or finance)!
	
	Consider two data series on the same character:
	\begin{itemize}
		\item $(x_1,n_1),(x_2,n_2),\ldots ,(x_p,n_p)$ sample of total size $n$, arithmetic average $\bar{x}$, standard deviation $\sigma_x$.
		\item $(y_1,m_1),(y_2,m_2),\ldots ,(y_p,m_q)$ sample of total size $m$, arithmetic average $\bar{y}$, standard deviation $\sigma_y$.
	\end{itemize}
	with $n+m=N$.
	
	We then have:
		
		So the average of the averages is not equal to the overall average (first common mistake in business) except if the two data series have the same sample size ($n=m=\sfrac{1}{2}N$)!!!
		
		Let have a look at the standard deviation always with the same situation. First remember that we have:
		
		To continue, recall that we have previously proved the Huygens theorem and therefore:
		
		Therefore we have\label{variance of two statistical series}:
		
		Redoing exactly the same developments with the unbiased variance we would get:
		
		So we see that the overall variance is not equal to the sum of the variance (second common mistake in business)!!!

	Consider now $X$ being a random variable of mean $\mu$ (constant and determined value) and variance $\sigma^2$ (constant and determined value), we define the "\NewTerm{reduced centered variable}\index{reduced centered variable}\label{reduced centered variable}" by the relation:
	 
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	When the real mean is unknowns and same for the variance, we denoted the above relation as following (various notation are represented):
	
	and this is named a "\NewTerm{studentized variable}\index{studentized  variable}".
	\end{tcolorbox}
	\begin{theorem}
	We prove in a very simple way by using the property of linearity of the mean and property of scalar multiplication of the variance that:
	
\end{theorem}
\begin{dem}
For the proof we just use the definitions of the expected mean and variance (using Huygens theorem for this latter). So let us begin with the mean:
	
	And now with the variance using the Huygens theorem:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	Thus, any statistical distribution defined by a mean and standard deviation can be transformed into another distribution often easier to analyse statistical. Therefore making this transformation, we obtain a random variable for which the parameters of the distribution low are now useless to know. When we do that with other laws, and in the general case, when we speak of "\NewTerm{pivotal variables}\index{pivot variables}".

Here are some important mathematical properties of the variance\label{properties of the variance}:
	\begin{enumerate}
		\item[P1.] Multiplication by a constant:
		
		
		\item[P2.] Sum of two random variables:
		
		Where we meet for the first time the concept of "\NewTerm{covariance}\index{covariance}\label{covariance}" denoted by $\text{cov}()$. We will see further below at page \pageref{variance sum multiple random variables} the general case to $n$ random variables!
		
		\item[P3.] Affine transformation:
		
		without too much surprises...
		
		\item[P4.] Product of two random variables (using the Huygens theorem):
		
		And if the two random variables are independent, we get:
		
		What we can rewrite using once again the Huygens theorem:
		
	\end{enumerate}
	We will assume as obvious that these four properties extend to the continuous case!
	
	\subparagraph{Semivariance}\mbox{}\\\\
	The "\NewTerm{semi-variance}\index{semivariance}" is an indicator that may have been proposed by Harry Markowitz instead of the variance for portfolio optimization especially.

Analyses in the labs may have shown that portfolios optimized for semivariance minimization (\SeeChapter{see section Economy page \pageref{portfolio efficient diversification models}}) have better performance than those based on variance. Indeed, the variance considers extremes of positive and negative returns as having equal undesirability, whereas as we shall see, semi-variance considers only the negative part. On the other hand, if the distribution of returns tends to a perfect symmetry, the two methods tend (obviously) to give exactly the same result.
	
	\textbf{Definition (\#\thesection.\mydef):} The "\NewTerm{semi-variance}", also named "\NewTerm{Lower Partial Standard Deviation}\index{lower partial standard deviation}" (LSPD), is a measure of the dispersion only of the realization of a random variables $X$ which are less than the expected mean of this same random variable according to:
	
	or denoted in the common discrete biased case, as standard deviation:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	By the way the following trap must be avoid:
	
	\end{tcolorbox}
	There trough this definition we see that the semivariance is similar to variance. However, it only considers observations below the mean. A useful tool in portfolio or asset analysis, semivariance provides a measure for "\NewTerm{downside risk}\index{downside risk}". While standard deviation and variance provide measures of volatility, semivariance only looks at the negative fluctuations of an asset. By neutralizing all values above the mean, or an investor's target return, semivariance estimates the average loss that a portfolio could incur. 

	For risk averse investors, solving for optimal portfolio allocations by minimizing semivariance would limit the likelihood of a large loss.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us consider the small following table:
	\begin{table}[H]
	\centering
	\begin{tabular}{|l|c|c|c|}
	\hline
	\rowcolor[gray]{0.75} 
	\multicolumn{1}{|c|}{\cellcolor[gray]{0.75}\textbf{Date}} & \textbf{Price} & \textbf{Return ($\pmb{X}$)} & \textbf{$\pmb{\left(\min(X-\text{E}(X),0)\right)^2}$} \\ \hline
	March & $20$ &  & \multicolumn{1}{l|}{} \\ \hline
	April & $19$ & $-5\%$ & $(-4.6\%)^2$ \\ \hline
	May & $17.10$ & $-10\%$ & $(-9.6\%)^2$ \\ \hline
	June & $18.81$ & $+10\%$ & $0^2$ \\ \hline
	July & $19.25$ & $+5\%$ & $0^2$ \\ \hline
	August & $19.36$ & $-1.99\%$ & $(-1.59\%)^2$ \\ \hhline{|=|=|=|=|}
	\multicolumn{2}{|l|}{\textbf{Arithmetic mean:}} & $\text{E}(X)\cong 0.4\%$ &  \\ \hline
	\end{tabular}
	\end{table}
	The semi-variance value is then:
	
	\end{tcolorbox}

	\paragraph{Discrete Covariance}\mbox{}\\\\ 
	We have seen in one of the last equations earlier above the concept of "\NewTerm{covariance}" for which we will determine a more convenient expression later:
	
	The concept of covariance can be understood geometrically as illustrated below (we will define further below the concept of correlation abbreviated Corr in the figure):
	\begin{itemize}
		\item The rectangle is green when either:
		\begin{itemize}
			\item Both $x$ and $y$ are greater than their respective means
			\item Both $x$ and $y$ are less than their respective means
		\end{itemize}
	
		\item The rectangle is red when either:
		\begin{itemize}
			\item $x$ is greater than $\mu_X$ but $y$ is less than $\mu_Y$
			\item $x$ is less than $\mu_X$ but $y$ is greater than $\mu_Y$
		\end{itemize}
	\end{itemize}
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/arithmetics/covariance.jpg}
		\caption{Geometric interpretation of covariance}
	\end{figure}
		
	We introduce now a more general and very important expression of the covariance in many application fields\label{variance sum multiple random variables}:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Some readers may have noticed that with $1$ variable, we have $0$ covariance terms, with $2$ variables we have $1$ covariance term, with $3$ variables we have $2+2+2=6$ covariance terms, and continuing we would see that we have in the general case $n(n-1)$ covariance terms for $n$ variables.
	\end{tcolorbox}	
	Now we change the notation to simplify even more:
	
	Therefore in the general case:
	
	Or using standard deviation:
	
	Using the properties of the mean (especially $\text{E}(X)=c^{te}$ and $\text{E}(c^{te})=c^{te}$) we can write the covariance in a much simpler way for calculation purposes:
	
	and we obtain the relation widely used in statistics and finance in  practice named the "\NewTerm{covariance formula}"...:
	
	which is however best known when written as:
	
	
	If $X=Y$ (equivalent to a univariate covariance) we fall back again on the Huygens theorem:
	

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Statistics can be partitioned according to the number of random variables we study. Thus, when a single random variable is studied, we speak of "\NewTerm{univariate statistics}\index{univariate statistics}", for two random variables of "\NewTerm{bivariate statistics}\index{bivariate statistics}" and in general, of "\NewTerm{multivariate statistics}\index{multivariate statistics}". 
	\end{tcolorbox}	
	
	If and only if the variables are equally likely, we find the covariance in the literature in the following form, sometimes named "\NewTerm{Pearson covariance}\index{Pearson covariance}", which derives from calculations that we have done previously with the mean:
	
	Covariance is a measure of the simultaneous variation of $X$ and $Y$. Indeed, if $X$ and $Y$ generally grow simultaneously, the products $(y_i-\mu_Y)(x_i-\mu_X)$ will be positive (positively correlated), whereas if $Y$ decreases as $X$ increases, these same products will be negative (negative correlation).
	
	Note that if we distribute the terms of the last equation, we have:
	
	and we have already shown that the sum of the deviations from the mean is zero. Hence we get another common way to write the covariance:
	
	and by symmetry:
	
	So in the end, in the equiprobable case, we finally have the equivalent three important relations used in various sections of this book:
	\begin{equation}
  	\addtolength{\fboxsep}{5pt}
   \boxed{
   \begin{gathered}
		\begin{aligned}
			\text{c}_{X,Y}&=\text{cov}(X,Y)=\dfrac{1}{n}\sum_{i=1}^n(y_i-\bar{y})(x_i-\bar{x})\\
			\text{c}_{X,Y}&=\text{cov}(X,Y)=\dfrac{1}{n}\sum_{i=1}^nx_iy_i-\bar{x}\bar{y}\\
			\text{c}_{X,Y}&=\text{cov}(X,Y)=\dfrac{1}{n}\sum_{i=1}^n x_i(y_i-\bar{y})=\dfrac{1}{n}\sum_{i=1}^n y_i(x_i-\bar{x})
		\end{aligned}
   \end{gathered}
   }
	\end{equation}
	In the section Theoretical Computing for the study of linear regression (page \pageref{least squares method}) and factor analysis we will need the explicit expression of the bilinearity\label{bilinearity of the variance} property of the variance. To see what it is exactly, consider three random variables $X, Y$ and $Z$ and two constants $a$ and $b$. Then using the third relation given above, we have:
	
	The last relation is also important and will be used in several sections of this book (Economy page \pageref{economy}, Numerical Methods page \pageref{numerical methods}). It also allows us to directly obtain the covariance for the sums of various random variables.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
If $X, Y, Z, T$ are four random variables defined on the same population, we want to compute the following covariance:
	
	We will develop that in two phases (this is also why we name that "bilinearity property"). First with respect to the second argument (random choice!):
	
	And then with respect to the first:
	
	So in the end:
	
	\end{tcolorbox}
	There is an elegant and simple method to remember how to calculate the covariance of two random variables. Indeed, remembering that we have proved just earlier that:
	 
	It comes by subtracting these two relations and simplifying / reordering:
	 
	and that's it! A simple way to remember the compute the covariance.
	
	Now, consider a set of random vectors $\vec{X}_i:=X_i$ of components $(x_1,x_2,\ldots ,x_n)_i$. The calculation of the covariance of the components by pairs gives what is named the "\NewTerm{covariance matrix}\index{covariance matrix}" (a tool widely used in finance, management and statistical numerical methods!).

	Indeed, we define the component $(m,n)$ of the covariance matrix by:
	
	We can therefore write a symmetric matrix (usually in practice it must be a square matrix...) in the form:
	
	where $\Sigma$ is the usual tradition letter to denote the "\NewTerm{covariance matrix}\index{covariance matrix}".
	
	By symmetry and because it is a square $n$ by $n$ matrix only the number $\dfrac{n(n+1)}{2}$ of components is useful for us to determine the whole matrix (trivial but important information for when we will study the structural equation modelling in the Numerical Methods section).
	
	This matrix has the remarkable property that if we take the set of all random vectors and we calculate the covariance matrix, then the diagonal will give us obviously the variances of each pair of vectors (see examples in the chapters Economics, Numerical Methods or Industrial Engineering) because we have for recall:
	
	Hence the name "\NewTerm{variance-covariance matrix}\index{variance-covariance matrix}\label{variance covariance matrix}":
	
	And this is a little bit abusively sometimes written as:
	
	This matrix has the advantage of quickly showing what pairs of random variables have a negative covariance and there... for which random variable the variance of the sum is smaller than the sum of the variances!
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	As we already mention it, this matrix is very important and we will often see it again in the section Economy during our study of modern portfolio theory (page \pageref{portfolio efficient diversification models}) and also for data mining techniques in the section of Theoretical Computing (page \pageref{data mining}) and also for principal components analysis (page \pageref{principal component analysis}) and also in Industrial Engineering during our study of bivariate control charts (page \pageref{quality control charts}).
	\end{tcolorbox}	
	
	Recall now that we have an axiom in probability (\SeeChapter{see section Probabilities page \pageref{kolmogorov axioms}}) which lead us to the fact that two events $A$ and $B$ are independent if and only if:
	
	Similarly, by extension, we define the independence of discrete random variables.
	
\textbf{Definition (\#\thesection.\mydef):} Let $X, Y$ be two discrete random variables. We say that $X, Y$ are independent if and only if:
	
	More generally, the discrete variables $X_1,X_2,\ldots,X_n$ are independent (in block) if:
	
	\begin{theorem}
		The independence of two random variables implies that their covariance is zero (the opposite is false!). 
	\end{theorem}
	\begin{dem}
		We will prove this in the case where the random variables take only a finite number of values $\left\lbrace x_i \right\rbrace_I$ and $\left\lbrace y_j \right\rbrace_J$ , respectively, with $I, J$ finite sets.
		
		For the proof let us recall that:
		
		and therefore:
		
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	So smaller is the covariance (near to zero), more the series are independent. Conversely, the greater the covariance (in absolute value) higher the series are dependent.
	\end{tcolorbox}	
	Given that:
	
	and the fact that if $X$ and $Y$ are independent we have $c_{X,Y}=0$. Then:
	
	More generally if $X_1,\ldots,X_n$ are independent (in block) then for any discrete or continuous statistical distribution law (!) we have using the two most common notations:
	
	Or using the standard deviation:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	There is another important relation that we will need for our study of Data Mining/Machine Learning and especially to compare different models and involving the covariance matrix.
	
	Let $X=(X_i)$ be an $n\times 1$ random vector and let $A$ be an $n\times n$ symmetric matrix. If $\text{E}(X)=\mu$ and $\text{V}(X)=\Sigma=\sigma_{ij}$ then $\text{E}[X^TAX]=\text{tr}(A\sigma)+\mu^TA\mu$ (keeping in mind that $X^TAX$ is a $1\times 1$ matrix, or in other words: a scalar!).
	
	To prove this first notice that:
	
	So what is its expectation? Clearly we have first by using the linearity proper of the expected mean:
	
	Applying the covariance relation proved just earlier above:
	
	then we get:
	
	As $\Sigma$ is symmetric:
		
	The following relation will be important to us during our study of the Akaike information criterion (\SeeChapter{see section Numerical Methods page \pageref{Akaike information criterion}}):
	
	Notice that the notation above is common in the field of statistics but more rigorously we should write\label{quadratic relation for akaike information criterion}:
	
	We need also another property of the covariance matrix for our study of Akaike information criterion (\SeeChapter{see section Numerical Methods page \pageref{Akaike information criterion}}). Let us consider $\vec{x}\in \mathbb{R}^{p}$ be a random (column) vector with covariance matrix $\sigma \in \mathbb{R}^{p\times p}$. Let $A\in \mathbb{R}^{p\times p}$ be a non-random matrix. Then: 
	
	We get the same result if we replace the vector $\vec{x}$ by a matrix $X$ such that:
	
	
	\subparagraph{Quasi-correlation matrix}\mbox{}\\\\
	In practice, some financial analysts estimate qualitatively the matrix of correlations based on their experience, we then talk about the "\NewTerm{quasi-correlation matrix}\index{quasi-correlation matrix}\label{quasi-correlation matrix}", and remember that the variance-covariance matrix and the matrix of correlations are the same thing but with two different perspectives, however it is not possible to make an estimate randomly (without rules).

	Indeed, let us consider $n$ real random variables $X_1,\ldots,X_n$ of respective means $\mu_1,\ldots,\mu_n$. Let us denote exceptionally by $\Sigma$ the symmetric matrix of variances-covariances defined by:
	
	Let us prove that $\Sigma$ is positive semi-definite (\SeeChapter{see section Linear Algebra page \pageref{positive semidefinite matrix}})\label{positive semi-definitiveness of covariance matrix}:
	\begin{dem}
	Given $\vec{\alpha}=(\alpha_1,\ldots,\alpha_n)^T$ a vector of $\mathbb{R}^n$. We have:
	
	Therefore:
	
	Since the variance is positive or zero by construction! Which means that $\Sigma$ is semi-positive. This is a very important property for many statistical applications (Mahalonobis distance, Cholesky decomposition, etc.).
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Now, by the generality of $\vec{\alpha}$, let us admit (nothing prevents us to do so!) that the latter is an eigenvector (\SeeChapter{see section Linear Algebra page \pageref{eigenvector}}). Then, since:
	
	and since by definition of their eigenvalues and eigenvectors:
	
	Then we have:
	
	And let us concentrate on the right term:
	
	Since the eigenvalue is a scalar we can rearrange this for example in the following form (commutativity):
	
	and as $\vec{\alpha}^T\vec{\alpha}$ is necessarily positive then the $\lambda_i$ can only be positive or null.
	
	So we got a way to check that our matrix of variances-covariances (or correlation) is indeed what it is supposed to be !!!
	
	Moreover, as we have demonstrated in our study of the spectral theorem in the Linear Algebra section page \pageref{spectral theorem} that for a matrix such that is $\Sigma$ (symmetric, real, positive semi-definite):
	
	\label{positive semi-definite matrix not always invertible} Then another way of controlling is $\Sigma$ is positive semi-definite is that the determinant of the variance-covariance matrix is positive or zero. And keep in mind that if the determinant is zero, then the matrix is not invertible! Hence the variance-covariance matrix is not always invertible!
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	However, the reciprocal is false (obviously!), we can have a positive determinant without the matrix being positive. While for eigenvalues there is equivalence: positive matrix is equivalent to all eigenvalues are positive.
	\end{tcolorbox}
	
	\pagebreak
	\subparagraph{Anscombe's famous quartet}\mbox{}\\\\
	Anscombe's quartet comprises four datasets that have nearly identical elementary statistical properties, yet appear very different when graphed or analysed with undergraduate statistics rather than high-school one. Each dataset consists of eleven $(x,y)$ points. They were constructed in 11973 (holocene calendar) by the statistician Francis Anscombe to demonstrate both the importance of graphing data before analysing it and the effect of outliers on statistical properties. This quartet is also used to test if an analytical tool can be accepted a "statistics-compliant" (as the six corresponding used statistics should be the minimum provided by any high-school level analytical tool!).
	
	The datasets are as follows. The $x$ values are the same for the first three datasets:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|c|c|c|c|}
			\hline
			\multicolumn{2}{|c|}{\cellcolor[gray]{0.75}I} & \multicolumn{2}{|c|}{\cellcolor[gray]{0.75}II} & \multicolumn{2}{|c|}{\cellcolor[gray]{0.75}III} & \multicolumn{2}{|c|}{\cellcolor[gray]{0.75}IV} \\ \hline
			\cellcolor[gray]{0.75} $x$ & \cellcolor[gray]{0.75} $y$ & \cellcolor[gray]{0.75} $x$ & \cellcolor[gray]{0.75} $y$ & \cellcolor[gray]{0.75} $x$ & \cellcolor[gray]{0.75} $y$ & \cellcolor[gray]{0.75} $x$ & \cellcolor[gray]{0.75} $y$ \\ \hline
			$10.0$ & $8.04$ & $10.0$ & $9.14$ & $10.0$ & $7.46$ & $8.0$ & $6.58$\\
			$8.0$ & $6.95$ & $8.0$ & $8.14$ & $8.0$ & $6.77$ & $8.0$ & $5.76$\\
			$13.0$ & $7.58$ & $13.0$ & $8.74$ & $13.0$ & $12.74$ & $8.0$	& $7.71$\\
			$9.0$ & $8.81$ & $9.0$ & $8.77$ & $9.0$ & $7.11$ & $8.0$ & $8.84$\\
			$11.0$ & $8.33$ & $11.0$ & $9.26$ & $11.0$ & $7.81$ & $8.0$ & $8.47$\\
			$14.0$ & $9.96$ & $14.0$ & $8.10$ & $14.0$ & $8.84$ & $8.0$ & $7.04$\\
			$6.0$ & $7.24$ & $6.0$ & $6.13$ & $6.0$ & $6.08$ & $8.0$ & $5.25$\\
			$4.0$ & $4.26$ & $4.0$ & $3.10$ & $4.0$ & $5.39$ & $19.0$ & $12.50$\\
			$12.0$ & $10.84$ & $12.0$ & $9.13$ & $12.0$ & $8.15$ & $8.0$ & $5.56$\\
			$7.0$ & $4.82$ & $7.0$ & $7.26$ & $7.0$ & $6.42$ & $8.0$ & $7.91$\\
			$5.0$ & $5.68$ & $5.0$ & $4.74$ & $5.0$ & $5.73$ & $8.0$ & $6.89$\\ \hline
		\end{tabular}
		\caption{Anscombe's quartet}
	\end{table}
	The quartet is still often used to illustrate the importance of looking at a set of data graphically before starting to analyse according to a particular type of relation, and the inadequacy of basic statistic properties for describing realistic datasets.
	
	With Microsoft Excel 14.0.7166 we get:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.85]{img/arithmetics/anscombe_quartet.jpg}
		\caption{Anscombe's quartet statistics summary}
	\end{figure}
	As we can see with elementary statistical indicators it is almost impossible to guess a difference between the four data sets. But if we use the skewness or the kurtosis (see page \pageref{skewness and kurtosis}) this change everything! The mutual information criterion (\SeeChapter{see section Statistical Mechanics page \pageref{mutual information}}) can also discriminate between the four plots.
	
	Looking to the corresponding charts we get the same conclusion:
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.8]{img/arithmetics/anscombe_quartet_chart.jpg}
		\end{center}	
		\caption{Anscombe's quartet graphs summary}
	\end{figure}

	
	\paragraph{Mean and Variance of the Average}\mbox{}\\\\
	Often in statistics, it is (verrrrry!) useful to determine the standard deviation of the sample mean and to work with it to get important analytical results in management and manufacturing. Let's see what it is!
	
	Given the average of a series of terms, each determined by the measurement of several values (it is in fact its estimator in a particular case as we will see later):
	
	then using the properties of the mean:
	
	and if all the random variables are independent and identically distributed then we have:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
We will prove much further below that if all the random variables are independent and identically distributed with finite variance, then the mean follows asymptotically what we name a "Normal distribution".
	\end{tcolorbox}
	For the variance, the same reasoning applies:
	
	And if the random variables are independent and identically distributed (we will study further the very important case current in practice where the last condition is not satisfied):
	
	Then we get the "\NewTerm{standard deviation of the mean}\index{standard deviation of the mean}" also named "\NewTerm{standard error}\index{standard error}\label{standard error}" or "\NewTerm{non-systematic variation}\index{non-systematic variation}":
	
	and this is strictly the standard deviation of the estimator of mean!
	
	The more intuitive form to express the Standard Error in terms of percent for non-analytical workers, managers and chief executives is named "\NewTerm{Relative Standard Error (RSE)}\index{relative standard error}" which is the expression of the Standard Error as percent, that is:
	
	The latter is quite useful when we have to deal with many variables with different units!!
	
	The value of $\sigma_{\bar{X}}$ is available in many softwares including Microsoft Excel charts (but there is no built-in function in Microsoft Excel) and is written with the standard deviation (as above) or with the notation of the variance (then we only have to take the square root...).

	Note that the last relation can be used even if the average of $n$ random variables is not the same! The main condition is just that the standard deviations are all equal and this is the case in the industry (production).

	We then have:
	\begin{equation}
  	\addtolength{\fboxsep}{5pt}
   	\boxed{
   	\begin{gathered}
   		\begin{aligned}
		\text{E}(S_n)&=n\mu & \text{E}(M_n)&=\mu\\
		\text{V}(S_n)&=\sigma_{S_n}^2=n\sigma^2 & \text{V}(M_n)&=\sigma_{M_n}^2=\dfrac{\sigma^2}{n}
   		\end{aligned}
   	\end{gathered}
   	}
	\end{equation}
	where $S_n$ is the sum of $n$ independent identically distributed random variables and $M_n$ their estimated average.
	
	The reduced centered variable that we introduced earlier:
	
	can then be written in several very useful ways:
	
	Furthermore, assuming that the reader already knows what is a Normal distribution $\mathcal{N}(\mu,\sigma)$, we will show later in detail because it is extremely important (!) that the probability of the random variable $\bar{X}$, average of $n$ identically distributed and linearly independent random variables, has for law (obviously):
	
	
	
	\pagebreak
	\paragraph{Coefficient of Correlation}\label{coefficient of correlation}\mbox{}\\\\
	Now consider $X$ and $Y$ two random variables having for covariance:
	
	\begin{theorem}
	We have:
	
	We will prove this relation immediately because the use of the covariance alone for data analysis is not always great because it is not strictly limited and easy to use (at interpretation). We will construct an indicator easier to use in business.
	\end{theorem}
	\begin{dem}
		We choose any constant $a$ and we calculate the variance of:
		
		We can then immediately write using the properties of the variance and the of the mean:
		
		The right quantity is positive or null for any $a$ by construction of the variance (left). So the discriminant of the expression (\SeeChapter{see section Calculus page \pageref{discriminant}}), seen as a polynomial in $a$ is of the type:
		
		Because $P(a)$ is positive for any $a$ we have as only possibility that:
		
		Therefore after simplification:
		
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
	\end{dem}
	This gives us also:
	
	Finally we get some statistical inequality (it's a special form of the Cauchy-Schwartz inequality that we will prove later in the section of Vector Calculus page \pageref{cauchy-schwarz inequality}):
	
	If the variances of $X$ and $Y$ are non-zero, the correlation between $X$ and $Y$ is defined by the "\NewTerm{linear correlation coefficient}\index{linear correlation coefficient}\label{linear correlation coefficient}" (it is a standardized covariance so that its amplitude does not depend on the chosen unit measure) and written:
	
	and also named "\NewTerm{product moment correlation coefficient}\index{product moment correlation coefficient}" or simply  "\NewTerm{correlation coefficient}\index{correlation coefficient}".
	
	Which can also be written in an expanded form (using Huygens theorem):
	
	or more condensed:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
Note that normally, the letter $R$ is reserved to say that this is an estimator of the correlation coefficient but the definition above is not an estimator (the variances doesn't have the small hat...) and that, strictly speaking, we should then write $\rho_{X,Y}$ according to the traditions of use.
	\end{tcolorbox}	
	
	Whatever the units and the orders of magnitude, the correlation coefficient is a number between $-1$ and $+1$ without units (so its value does not depend on the unit of measure, which is by far not the case for all statistical indicators!). It reflects more or less the linear dependence of $X$ and $Y$ or geometrically more or less the flatness magnitude. We can therefore say that a coefficient of correlation of zero or close to $0$ correlation means that there is no linear relation between the characters. But it does not involve any notion of more general independence!
	
	So when the correlation is zero:
	
	Then:
	
	Hence:
	
	And we know that this relations holds when random variables are independent. But think that in the general case "\textit{uncorrelated}" means "\textit{independence}" is totally wrong! Only independence implies uncorrelation! The opposite is not true (if $X$ and $Y$ are uncorrelated, then they can still be dependent)!
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Consider as example of uncorrelated variables that are not independent, the following example given (useful example in finance!) by:
	
	$Y$ is clearly dependent upon $X$. As we will prove later, during our study of the Normal centered reduced distribution, we have:
	
	And as:
	
	Then we have indeed dependent variables that are uncorrelated!
	\end{tcolorbox}
	The only general case when lack of correlation implies independence is when the joint distribution of $X$ and $Y$ is Gaussian!
	
	The figure below illustrates some common cases and the last row highlights well situations with dependent but uncorrelated variables\label{correlations figure}:
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/arithmetics/correlation_coefficients.jpg}
		\vspace*{2mm}
		\caption[Some Pearson correlations]{Several sets of $(x, y)$ points, with the corresponding Pearson correlation (source: Wikipedia, author: Denis Boigelot)}
	\end{figure}
	When the correlation coefficient is near $+1$ or $-1$, the characters are said to be strongly correlated. We must be careful with the frequent confusion between correlation and causality. Thus, two phenomena that are correlated does not imply in any way that one is the cause of the other (this fallacy is also known as "\NewTerm{spurious correlation}" or "\NewTerm{cum hoc ergo propter hoc}\footnote{\textit{Post hoc ergo propter hoc} (latin of "after this, therefore because of this:) is an informal fallacy that states: "Since event $Y$ followed event $X$, event $Y$ must have been caused by event $X$." It is often shortened simply to \textit{post hoc fallacy}. A logical fallacy of the questionable cause variety, it is subtly different from the fallacy \textit{cum hoc ergo propter hoc} ("with this, therefore because of this"), in which two events occur simultaneously or the chronological ordering is insignificant or unknown. Post hoc is a logical fallacy in which an event says to be the cause of a later event because it occurred earlier!}", latin for "with this, therefore because of this," and "false cause")!!!!
	
	Indeed, for any two correlated events, $A$ and $B$, the different possible relationships include:
	\begin{itemize}
		\item $A$ causes $B$ (direct causation);
		\item $B$ causes $A$ (reverse causation);
		\item $A$ and $B$ are consequences of a common cause, but do not cause each other;
		\item $A$ causes $B$ and $B$ causes $A$ (bidirectional or cyclic causation);
		\item $A$ causes $C$ which causes $B$ (indirect causation);
		\item There is no connection between $A$ and $B$;
		\item The correlation is a coincidence.
	\end{itemize}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	A situation in which a measure of association or relationship between exposure and outcome is distorted by the presence of another variable is named a "\NewTerm{confounding}\index{confounding}". An extraneous variable that wholly or partially accounts for the observed effect is named a "\NewTerm{confounder variable}\index{confounder variable}".
	\end{tcolorbox}
	So we know that correlation doesn't imply necessarily causation and that in practice times we can't always have mathematical (i.e. statistical) evidence that the correlation implies a causation! However there is a consensus in the scientific field that a correlation implies a causation when the following so named "\NewTerm{Braford Hill Criterion}\index{Braford Hill Criterion}" are fulfilled\footnote{Keep in mind that a "systematic review" answers a defined research question by collecting and summarising all empirical evidence that fits pre-specified eligibility criteria. A "meta-analysis" is the use of statistical methods to summarise the results of these studies.}:
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|p{10cm}|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|c|}{\cellcolor[gray]{0.75}\textbf{Criteria}} & \multicolumn{1}{c|}{\cellcolor[gray]{0.75}\textbf{Meaning}} \\ \hline
		\textbf{Strength of Association} & A strong association is more likely to have a causal component than is a modest association. Strength of the association is determined by the types of existing studies. The highest-level studies from the evidence pyramid would represent the strongest associations, (i.e., Systematic Reviews with meta-analysis and RCTs). Results from these studies must demonstrate an Odds Ratio (OR) or Relative Risk (RR) of at least $2$ or above in order to be meaningful. Anything between $1$ and $2$ is weak, while $>2$ is moderate and $>4$ is considered strong. \\ \hline
		\textbf{Consistency} & A relationship is repeatedly observed in all available studies. \\ \hline
		\textbf{Specificity} & A factor influences specifically a particular outcome or population. The more specific an association between a factor and an effect, the greater the probability that it is causal. \\ \hline
		\textbf{Temporality} & The cause must precede the outcome it is assumed to affect (e.g. smoking before the appearance of lung cancer) when the outcome is measured over time (longitudinal study). \\ \hline
		\textbf{\begin{tabular}[c]{@{}l@{}}Biological Gradient\\ (dose-response)\end{tabular}} & The outcome increases monotonically with increasing dose of exposure or according to a function predicted by a substantive theory (i.e. the more cigarettes one smokes, the greater the chance of the cancer occurring).\\ \hline
		\textbf{Plausability} & The observed association can be plausibly explained by substantive matter, i.e., biologically possible. \\ \hline
		\textbf{Coherence} & A causal conclusion should not fundamentally contradict present substantive knowledge (studies must not contradict each other). \\ \hline
		\textbf{Experiment} & Causation is more likely if evidence is based on randomized experiments or a systematic review with a meta-analysis of randomized experiments. However, if RCT's are not ethically possible then prospective cohort studies may provide the highest level of evidence. \\ \hline
		\textbf{Analogy} & For analogous exposures and outcomes, an effect has already been shown, e.g., effects first demonstrated on animals or an effect previously occurring on humans such as the effects of thalidomide on a foetus during pregnancy.\\ \hline
		\end{tabular}
		\caption{Bradford Hill correlation-causation criterion}
	\end{table}
	Coming back to the mathematical aspect of the correlation:
	\begin{itemize}
		\item If $R_{X,Y}=-1$ we are dealing with a "\NewTerm{pure negative correlation}\index{pure negative correlation}" (in the case of a linear relation all measurement points are located on a straight line with a negative slope).
		
		\item If $-1<R_{X,Y}<+1$ we are dealing with a negative or positive correlation named "\NewTerm{imperfect correlation}\index{imperfect correlation}" (in the case of a linear relation all measurement points are located on a straight line negative or positive slope respectively).
		
		 \item If $R_{X,Y}=0$ the correlation is zero... (in the case of a linear relation all the measurement points are located on a straight line of slope zero).
		 
		 \item If $R_{X,Y}=+1$ we are dealing with a "\NewTerm{pure positive correlation}\index{pure positive correlation}" (in the case of a linear relation all measurement points are located on a straight positive slope).
	\end{itemize}
	
	The analysis of the correlation coefficient has the objective of determining the degree of association between variables: it is often expressed as the coefficient of determination, which is the square of the correlation coefficient. The coefficient of determination thus measures the contribution of a variable to the explanation of the second. However, asking «\textit{how high should $R$ (or $R^2$) be?} doesn't make sense. A low $R$ doesn't negate a significant predictor or change the meaning of its coefficient. $R$ is simply whatever value it is, and it doesn't need to be any particular value to allow for a valid interpretation». 

Using the expressions of mean and standard deviation of equiprobable variables as demonstrated above (thus the idea of computing the correlation of two random variables is a good idea if they are jointly gaussian), we start:
	
	To obtain the estimator of the coefficient of correlation:
	
	where we see that the covariance becomes the average of the products minus the product of averages.
	
	Thus after simplification we get a famous expression:
	
	The correlation coefficient can be calculated in the English version of Microsoft Excel 11.8346 and others with the integrated \texttt{CORREL( )} function.
	
	We will see in the section of Numerical Methods a more general expression of the correlation coefficient (page \pageref{correlation coefficient numerical methods}) and in the section of Statistical Mechanics an alternative indicator (page \pageref{mean mutual information}).
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} In the literature, the experimental correlation coefficient is often named "\NewTerm{sampling Pearson coefficient}\index{sampling Pearson coefficient}" (in the equiprobable case), or  "\NewTerm{Pearson product-moment}\index{Pearson product-moment}", and when we carry it to the square, then we name it the "\NewTerm{coefficient of determination}\index{coefficient of determination}".\\

	\textbf{R2.} Often the square of the coefficient is somewhat improperly interpreted as the \% of variation explained in the response variable $Y$ by the explanatory variable $X$. 
	\end{tcolorbox}
	
	Finally, note that we have the following relation which is used a lot in practice (see the section Economics page \pageref{portfolio efficient diversification models} for famous detailed examples!):
	
	or the version with the standard deviation even more famous:
	
	It is a relation that we can often see in finance in the calculation of the VaR (Value at Risk) according to RiskMetrics methodology proposed by JP Morgan (\SeeChapter{see section Economy page \pageref{parametric VaR}}).
	
	Let us see a small application example of the correlation but that has nothing to do with VaR (at least for the moment...).
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	An airline company has $120$ seats available that she reserves for connecting passengers from two flights arrived earlier in the journey and that have to go to Frankfurt. The first flight arrived from Manila and the number of passengers on board follows a Normal distribution with mean $50$ and variance $169$. The second flight arrives in Taipei and the number of passengers on board follows a Normal distribution with mean $45$ and variance $196$.\\

	The linear correlation coefficient between the number of passengers of both flights was measured as:
	
	The law that follows the number of passengers for Frankfurt if we assume that the law of the couple also follows a Normal distribution (according to statement!) is:
	
	with:
	
	This is a bad start for customer satisfaction in the long term...
	\end{tcolorbox}

	We have proved earlier that for a sum of independent and identically distributed random variables, the standard-deviation of their mean was given by the standard-error:
		
	And what if their are not independent but still identically distributed of variance $\sigma^2$ (important case in Machine Learning but not only!)? First let us recall that:
	
	Therefore:
	
	Using the definition of correlation seen just above:
	
	We get (keeping in mind that all variances are assumed to be the same such that $\sigma_{X_i}\sigma_{Y_i}=\sigma$):
	
	Let us now assume that all pairwise correlations are equals (such that $R_{X_i,Y_i}=R$), then we get:
	
	That latter relation is often denoted by data scientists as following\label{variance mean correlated variables}:
		
	
	Finally, let us recall that the variance-covariance matrix is given by:
	
	Dividing each term by the corresponding terms $\sqrt{\text{V}_i}\sqrt{\text{V}_j}$ gives us the "\NewTerm{correlation matrix}\index{correlation matrix}\label{correlation matrix}":
	
	Notice that if the variances are all equal to the unit ($\text{V}_i=1$) then the covariance matrix is equal to the correlation matrix, ie $R=\Sigma$ (this is a useful property that we will use much later during our study of the variance inflation factor for multicollinearity detection)!
	
	Let us see what we just said in another way! As we know the equation for the standardization of a variable is written as:
	
	We also know that the original covariance matrix is defined as (using unbiased estimator):
	
	And after standardizing both variables:\\
	
	we get:
	
	So finally:
	
	So the variance-covariance matrix becomes indeed the correlation matrix with standardized variables!
	
	\subparagraph{Geometric interpretation}\mbox{}\\\\
	A measure of the relationship between two random variables is the covariance as we know given by:
	
	Covariance normalized to unity is the correlation coefficient as we have just seen it:
	
	Hence:
	
	The resulting expression:
	
	is the ratio of two elements. The numerator is the scalar product of two vectors, while the denominator is the product of their lengths: 
	
	This expression shows the formal identity between the correlation coefficient, and the cosine of the angle between two random vectors.
	
	\subparagraph{Point biserial correlation coefficient}\mbox{}\\\\
	A "\NewTerm{point-biserial correlation}\index{point-biserial correlation}", denoted $r_{\text{pb}}$, is used to measure the strength and direction of the association that exists between one continuous variable and one dichotomous variable. It is a special case of the Pearson’s product-moment correlation, which is applied when you have two continuous variables, whereas in this case one of the variables is measured on a dichotomous scale.

	For example, we could use a point-biserial correlation to determine whether there is an association between salaries, measured in US dollars, and gender (i.e., your continuous variable would be "salary" and your dichotomous variable would be "gender", which has two categories: "males" and "females").
	
	To calculate $r_{\text{pb}}$ we assume that the dichotomous variable $Y$ has the two values $0$ and $1$. If we divide the data set into two groups, group $1$ which received the value "$1$" on $Y$ and group $2$ which received the value "$0$" on $Y$, then the point-biserial correlation coefficient is calculated as follows:
	
	where ${\sigma}_{n}$ is the standard deviation used when data are available for every member of the population (obviously there are version that use the estimator of the standard deviation instead of the biased standard deviation!):
	
	$\mu_1$ being the mean value on the continuous variable $X$ for all data points in group $1$, and $\mu_0$ the mean value on the continuous variable $X$ for all data points in group $2$. Further, $n_1$ is the number of data points in group $1$, $n_0$ is the number of data points in group $2$ and $n$ is the total sample size. 
	
	\begin{dem}
	Let the $n$ data consist of $n_{0}>0(x, 0)$ pairs and $n_{1}>0(x, 1)$ pairs. Their Pearson correlation coefficient will be the same as the reversed data consisting of corresponding $(0, x)$ and $(1, x)$ pairs. Because there are exactly two distinct values of the first coordinates, the regression line of the reversed data must pass through the mean points $\left(0, \mu_{0}\right)$ and $\left(1, \mu_{1}\right),$ whence it has slope:
	
	We have proved (\SeeChapter{see section Numerical Methods page \pageref{slope and correlation coefficient relation}}) that the correlation coefficient for a univariate regression is also given by:
	
	Therefore the slope must be multiplied by the standard deviation of the first coordinates and divided by the standard deviation of the second coordinates (the original $x$ values), denoted traditionally (sadly) by $\hat{\sigma}_{n}$. The standard deviation of the first coordinates is readily computed from the fact that they consist of $n_{0}$ zeros (0) and $n_{1}$ ones (1). Considering this set of values as a Bernoulli random variable, we have proved earlier above during our study of the Bernoulli distribution that:
	
	Therefore:
	
	Consequently the point biserial Pearson correlation coefficient is:
	
	So the point biserial correlation coefficient is just a special case of the Pearson correlation coefficient. However remember that in most statistical computer softwares the Pearson correlation coefficient is calculated using the unbiased variance and the Point biserial correlation coefficient used the biased one, this is why we don't get the same numerical results when using both corresponding functions in softwares!
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	The idea is depicted here:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1076); %set diagram left start at 0, and has height of 1076
		
		%Shape: Rectangle [id:dp94805615582941] 
		\draw   (222,64) -- (409.86,64) -- (409.86,243.28) -- (222,243.28) -- cycle ;
		%Straight Lines [id:da619065946250478] 
		\draw    (222,96) -- (208.86,96) ;
		%Straight Lines [id:da8536316045920995] 
		\draw    (222,151) -- (208.86,151) ;
		%Straight Lines [id:da8230536321875317] 
		\draw    (222,207) -- (208.86,207) ;
		%Straight Lines [id:da3853411604952772] 
		\draw    (227.86,244) -- (227.86,254.28) ;
		%Straight Lines [id:da6378665961154253] 
		\draw    (264.1,244) -- (264.1,254.28) ;
		%Straight Lines [id:da30031905175730556] 
		\draw    (298.32,244) -- (298.32,254.28) ;
		%Straight Lines [id:da5088995355982853] 
		\draw    (333.55,244) -- (333.55,254.28) ;
		%Straight Lines [id:da8874335889563825] 
		\draw    (369.78,244) -- (369.78,254.28) ;
		%Straight Lines [id:da5521566012047592] 
		\draw    (403,244) -- (403,254.28) ;
		%Shape: Circle [id:dp4663191925772725] 
		\draw   (226.23,131.7) .. controls (226.23,129.91) and (227.68,128.46) .. (229.47,128.46) .. controls (231.26,128.46) and (232.71,129.91) .. (232.71,131.7) .. controls (232.71,133.49) and (231.26,134.95) .. (229.47,134.95) .. controls (227.68,134.95) and (226.23,133.49) .. (226.23,131.7) -- cycle ;
		%Shape: Circle [id:dp61402633980679] 
		\draw   (226.23,158.04) .. controls (226.23,156.25) and (227.68,154.79) .. (229.47,154.79) .. controls (231.26,154.79) and (232.71,156.25) .. (232.71,158.04) .. controls (232.71,159.83) and (231.26,161.28) .. (229.47,161.28) .. controls (227.68,161.28) and (226.23,159.83) .. (226.23,158.04) -- cycle ;
		%Shape: Circle [id:dp08000340343415435] 
		\draw   (226.23,184.7) .. controls (226.23,182.91) and (227.68,181.46) .. (229.47,181.46) .. controls (231.26,181.46) and (232.71,182.91) .. (232.71,184.7) .. controls (232.71,186.49) and (231.26,187.95) .. (229.47,187.95) .. controls (227.68,187.95) and (226.23,186.49) .. (226.23,184.7) -- cycle ;
		%Shape: Circle [id:dp9541036518775718] 
		\draw   (226.23,191.19) .. controls (226.23,189.4) and (227.68,187.95) .. (229.47,187.95) .. controls (231.26,187.95) and (232.71,189.4) .. (232.71,191.19) .. controls (232.71,192.98) and (231.26,194.43) .. (229.47,194.43) .. controls (227.68,194.43) and (226.23,192.98) .. (226.23,191.19) -- cycle ;
		%Shape: Circle [id:dp9613385540704549] 
		\draw   (226.23,197.67) .. controls (226.23,195.88) and (227.68,194.43) .. (229.47,194.43) .. controls (231.26,194.43) and (232.71,195.88) .. (232.71,197.67) .. controls (232.71,199.46) and (231.26,200.91) .. (229.47,200.91) .. controls (227.68,200.91) and (226.23,199.46) .. (226.23,197.67) -- cycle ;
		%Shape: Circle [id:dp3685406507955671] 
		\draw   (226.23,201.91) .. controls (226.23,200.12) and (227.68,198.67) .. (229.47,198.67) .. controls (231.26,198.67) and (232.71,200.12) .. (232.71,201.91) .. controls (232.71,203.71) and (231.26,205.16) .. (229.47,205.16) .. controls (227.68,205.16) and (226.23,203.71) .. (226.23,201.91) -- cycle ;
		%Shape: Circle [id:dp5404603606971947] 
		\draw   (226.56,210.58) .. controls (226.56,208.79) and (228.01,207.34) .. (229.8,207.34) .. controls (231.59,207.34) and (233.05,208.79) .. (233.05,210.58) .. controls (233.05,212.37) and (231.59,213.82) .. (229.8,213.82) .. controls (228.01,213.82) and (226.56,212.37) .. (226.56,210.58) -- cycle ;
		%Shape: Circle [id:dp13039303102395028] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (226.23,187.95) .. controls (226.23,186.15) and (227.68,184.7) .. (229.47,184.7) .. controls (231.26,184.7) and (232.71,186.15) .. (232.71,187.95) .. controls (232.71,189.74) and (231.26,191.19) .. (229.47,191.19) .. controls (227.68,191.19) and (226.23,189.74) .. (226.23,187.95) -- cycle ;
		%Shape: Circle [id:dp8290419408264647] 
		\draw   (226.56,236.91) .. controls (226.56,235.12) and (228.01,233.67) .. (229.8,233.67) .. controls (231.59,233.67) and (233.05,235.12) .. (233.05,236.91) .. controls (233.05,238.71) and (231.59,240.16) .. (229.8,240.16) .. controls (228.01,240.16) and (226.56,238.71) .. (226.56,236.91) -- cycle ;
		%Shape: Circle [id:dp5534610741071184] 
		\draw   (399.56,192.25) .. controls (399.56,190.46) and (401.01,189.01) .. (402.8,189.01) .. controls (404.59,189.01) and (406.05,190.46) .. (406.05,192.25) .. controls (406.05,194.04) and (404.59,195.49) .. (402.8,195.49) .. controls (401.01,195.49) and (399.56,194.04) .. (399.56,192.25) -- cycle ;
		%Shape: Circle [id:dp4587444125697293] 
		\draw   (399.56,155.91) .. controls (399.56,154.12) and (401.01,152.67) .. (402.8,152.67) .. controls (404.59,152.67) and (406.05,154.12) .. (406.05,155.91) .. controls (406.05,157.71) and (404.59,159.16) .. (402.8,159.16) .. controls (401.01,159.16) and (399.56,157.71) .. (399.56,155.91) -- cycle ;
		%Shape: Circle [id:dp2453528928895552] 
		\draw   (399.23,135.91) .. controls (399.23,134.12) and (400.68,132.67) .. (402.47,132.67) .. controls (404.26,132.67) and (405.71,134.12) .. (405.71,135.91) .. controls (405.71,137.71) and (404.26,139.16) .. (402.47,139.16) .. controls (400.68,139.16) and (399.23,137.71) .. (399.23,135.91) -- cycle ;
		%Shape: Circle [id:dp9096584683587261] 
		\draw   (399.56,108.58) .. controls (399.56,106.79) and (401.01,105.34) .. (402.8,105.34) .. controls (404.59,105.34) and (406.05,106.79) .. (406.05,108.58) .. controls (406.05,110.37) and (404.59,111.82) .. (402.8,111.82) .. controls (401.01,111.82) and (399.56,110.37) .. (399.56,108.58) -- cycle ;
		%Shape: Circle [id:dp2598086329963085] 
		\draw   (399.56,98.25) .. controls (399.56,96.46) and (401.01,95.01) .. (402.8,95.01) .. controls (404.59,95.01) and (406.05,96.46) .. (406.05,98.25) .. controls (406.05,100.04) and (404.59,101.49) .. (402.8,101.49) .. controls (401.01,101.49) and (399.56,100.04) .. (399.56,98.25) -- cycle ;
		%Shape: Circle [id:dp7240401966558581] 
		\draw   (399.56,81.91) .. controls (399.56,80.12) and (401.01,78.67) .. (402.8,78.67) .. controls (404.59,78.67) and (406.05,80.12) .. (406.05,81.91) .. controls (406.05,83.71) and (404.59,85.16) .. (402.8,85.16) .. controls (401.01,85.16) and (399.56,83.71) .. (399.56,81.91) -- cycle ;
		%Shape: Circle [id:dp8571322259984218] 
		\draw   (399.56,79.91) .. controls (399.56,78.12) and (401.01,76.67) .. (402.8,76.67) .. controls (404.59,76.67) and (406.05,78.12) .. (406.05,79.91) .. controls (406.05,81.71) and (404.59,83.16) .. (402.8,83.16) .. controls (401.01,83.16) and (399.56,81.71) .. (399.56,79.91) -- cycle ;
		%Shape: Circle [id:dp8722588207370814] 
		\draw   (399.56,70.91) .. controls (399.56,69.12) and (401.01,67.67) .. (402.8,67.67) .. controls (404.59,67.67) and (406.05,69.12) .. (406.05,70.91) .. controls (406.05,72.71) and (404.59,74.16) .. (402.8,74.16) .. controls (401.01,74.16) and (399.56,72.71) .. (399.56,70.91) -- cycle ;
		%Shape: Circle [id:dp49291465982863514] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (399.56,115.07) .. controls (399.56,113.28) and (401.01,111.82) .. (402.8,111.82) .. controls (404.59,111.82) and (406.05,113.28) .. (406.05,115.07) .. controls (406.05,116.86) and (404.59,118.31) .. (402.8,118.31) .. controls (401.01,118.31) and (399.56,116.86) .. (399.56,115.07) -- cycle ;
		%Straight Lines [id:da031951260025781636] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (223.02,191.13) -- (410.77,112.28) ;
		
		% Text Node
		\draw (151,145.4) node [anchor=north west][inner sep=0.75pt]    {$x$};
		% Text Node
		\draw (312,301.4) node [anchor=north west][inner sep=0.75pt]    {$y$};
		% Text Node
		\draw (182,198.4) node [anchor=north west][inner sep=0.75pt]    {$4.0$};
		% Text Node
		\draw (182,143.4) node [anchor=north west][inner sep=0.75pt]    {$4.5$};
		% Text Node
		\draw (182,87.4) node [anchor=north west][inner sep=0.75pt]    {$5.0$};
		% Text Node
		\draw (218,259.4) node [anchor=north west][inner sep=0.75pt]    {$0.0$};
		% Text Node
		\draw (289,259.4) node [anchor=north west][inner sep=0.75pt]    {$0.4$};
		% Text Node
		\draw (359,259.4) node [anchor=north west][inner sep=0.75pt]    {$0.8$};
		\end{tikzpicture}
	\end{figure}
	The heights of the red dots depict the mean values $\mu_0$ and $\mu_1$ of each vertical strip of points. The dashed gray line is the regression line.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	R software native function computes the point biserial correlation coefficient with the corr and corr.test function because the point-biserial correlation is simply a special case of Pearson’s product-moment correlation, which is applied when you have two continuous variables, whereas in this case one of the variables is measured on a dichotomous scale. But its also available as biserial.cor in various packages.
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us imagine that the level of operational risk in an institution's business lines is graded as high or low based on the opinion of an operational risk analyst. Suppose we are interested in assessing the degree of correlation between operational  risk and gross income of the business lines. Since in this case operational risk is a binary variable and gross income a continuous variable, we can use the biserial coefficient to estimate correlation!
	\end{tcolorbox}
	
	
	\subparagraph{Partial correlation and semipartial correlation}\mbox{}\\\\
	Partial correlation measures the degree of association between two random variables, with the effect of a set of controlling random variables removed. If we are interested in finding whether or to what extent there is a numerical relationship between two variables of interest, using their correlation coefficient will give misleading results if there is another, "\NewTerm{confounding}\index{confounding}", variable that is numerically related to both variables of interest. This misleading information can be avoided by controlling (i.e. cancelling) for the confounding variable, which is done by computing the partial correlation coefficient.
	
	When in the case of three variables we want to control (cancel) one of the variable and that this one is the unique that is qualitative, the strategy is then simple as it consist to calculate the correlation between the two remaining variable for each level of the qualitative variable. But things start to be more complicated when the variable to control (cancel) is continuous. This is why we need to build a more sophisticated approach!
	
	The "\NewTerm{partial correlation coefficient}\index{partial correlation coefficient}\label{partial correlation coefficient}", denoted here $R_ {AB.C}$, makes it possible to know the value of the correlation between two variables $A$ and $B$, if the variable $C$ had remained constant for the series of observations considered.

	In other words, the partial correlation coefficient $R_{AB.C}$ is the total correlation coefficient between variables $A$ and $B$ when their best linear explanation has been removed in terms of $C$. It is given by the relation (in the obvious special case of three variables!):
	
	The quickest proof of this relation is to rely on the geometric interpretation of the correlation (cosine). 
	
	But notice before that like the correlation coefficient, the partial correlation coefficient takes on a value in the range from $-1$ to $+1$. The value $-1$ conveys a perfect negative correlation controlling for some variables (that is, an exact linear relationship in which higher values of one variable are associated with lower values of the other); the value $+1$ conveys a perfect positive linear relationship, and the value $0$ conveys that there is no linear relationship.
	
	Also notice that if $R_{AC}$ and $R_{BC}$ are both equal to zero (i.e. there is no confounding variable) then we have the equality:
	
	So it is easy to understand that in general when there is somewhere a correlation between some of the variables, then $R_{AB.C}>R_{AB}$. For other configuration values, the reader must be very precautions on the interpretation as the result depends obviously on the value of $R_{AB}$!
	
	Ok now let's go for the proof in the special case of three variables!
	
	
	The series of observations $A$, $B$ and $C$, once centered reduced (thus of radius $1$), are centered vectors $\vec{A}$, $\vec{B}$, $\vec{C}$ of unit length on the same sphere:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1467); %set diagram left start at 0, and has height of 1467
		
		% Gradient Info
		\tikzset {_ok2tyzru4/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformscale{1 }  }}}
		\pgfdeclareradialshading{_lc29vhjv8}{\pgfpoint{0bp}{0bp}}{rgb(0bp)=(0.29,0.56,0.89);
		rgb(0bp)=(0.29,0.56,0.89);
		rgb(25bp)=(0.93,0.97,1);
		rgb(400bp)=(0.93,0.97,1)}

		
		%Shape: Circle [id:dp8834577773441976] 
		\draw  [draw opacity=0][shading=_lc29vhjv8,_ok2tyzru4] (158,191) .. controls (158,104.29) and (228.29,34) .. (315,34) .. controls (401.71,34) and (472,104.29) .. (472,191) .. controls (472,277.71) and (401.71,348) .. (315,348) .. controls (228.29,348) and (158,277.71) .. (158,191) -- cycle ;
		%Shape: Arc [id:dp18102115274748698] 
		\draw  [draw opacity=0] (471.72,196.04) .. controls (466.8,217.4) and (398.45,234.33) .. (314.85,234.33) .. controls (228.05,234.33) and (157.69,216.08) .. (157.69,193.58) .. controls (157.69,192.7) and (157.8,191.84) .. (158.01,190.98) -- (314.85,193.58) -- cycle ; \draw   (471.72,196.04) .. controls (466.8,217.4) and (398.45,234.33) .. (314.85,234.33) .. controls (228.05,234.33) and (157.69,216.08) .. (157.69,193.58) .. controls (157.69,192.7) and (157.8,191.84) .. (158.01,190.98) ;  
		%Shape: Arc [id:dp3776002482076839] 
		\draw  [draw opacity=0][dash pattern={on 4.5pt off 4.5pt}] (158.01,190.98) .. controls (162.92,169.62) and (231.27,152.69) .. (314.86,152.69) .. controls (401.65,152.69) and (472,170.94) .. (472,193.44) .. controls (472,194.32) and (471.89,195.18) .. (471.69,196.04) -- (314.86,193.44) -- cycle ; \draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (158.01,190.98) .. controls (162.92,169.62) and (231.27,152.69) .. (314.86,152.69) .. controls (401.65,152.69) and (472,170.94) .. (472,193.44) .. controls (472,194.32) and (471.89,195.18) .. (471.69,196.04) ;  
		%Straight Lines [id:da816658962411912] 
		\draw    (313.48,191) -- (161,191) ;
		\draw [shift={(158,191)}, rotate = 360] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da27573146230557555] 
		\draw [color={rgb, 255:red, 18; green, 103; blue, 207 }  ,draw opacity=1 ][line width=1.5]    (313.48,191) -- (414.67,221.83) ;
		\draw [shift={(418.5,223)}, rotate = 196.95] [fill={rgb, 255:red, 18; green, 103; blue, 207 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (11.61,-5.58) -- (0,0) -- (11.61,5.58) -- cycle    ;
		%Curve Lines [id:da9642410683533935] 
		\draw [line width=1.5]    (257.5,92) .. controls (270.5,84) and (281.5,82) .. (302.5,82) ;
		%Curve Lines [id:da1400654959170169] 
		\draw [line width=1.5]    (302.5,82) .. controls (361.5,113) and (405.5,186) .. (418.5,223) ;
		%Straight Lines [id:da6648160698143346] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (313.48,191) -- (259.47,95.48) ;
		\draw [shift={(257.5,92)}, rotate = 60.52] [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (11.61,-5.58) -- (0,0) -- (11.61,5.58) -- cycle    ;
		%Straight Lines [id:da4033676655809191] 
		\draw [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ][line width=1.5]    (313.48,191) -- (302.9,85.98) ;
		\draw [shift={(302.5,82)}, rotate = 84.25] [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (11.61,-5.58) -- (0,0) -- (11.61,5.58) -- cycle    ;
		%Flowchart: Summing Junction [id:dp5201158147851153] 
		\draw   (309.19,191) .. controls (309.19,188.63) and (311.11,186.71) .. (313.48,186.71) .. controls (315.85,186.71) and (317.77,188.63) .. (317.77,191) .. controls (317.77,193.37) and (315.85,195.29) .. (313.48,195.29) .. controls (311.11,195.29) and (309.19,193.37) .. (309.19,191) -- cycle ; \draw   (310.44,187.97) -- (316.51,194.03) ; \draw   (316.51,187.97) -- (310.44,194.03) ;
		%Curve Lines [id:da9603770393925222] 
		\draw [line width=1.5]    (257.5,92) .. controls (316.5,123) and (388.5,184) .. (418.5,223) ;
		
		% Text Node
		\draw (110,182.4) node [anchor=north west][inner sep=0.75pt]    {$R=1$};
		% Text Node
		\draw (299,194) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
		% Text Node
		\draw (420.5,226.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,opacity=1 ]  {$\vec{B}$};
		% Text Node
		\draw (240.5,75.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ]  {$\vec{A}$};
		% Text Node
		\draw (294.5,51.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,opacity=1 ]  {$\vec{C}$};
		% Text Node
		\draw (271,66.4) node [anchor=north west][inner sep=0.75pt]    {$b$};
		% Text Node
		\draw (341,156.4) node [anchor=north west][inner sep=0.75pt]    {$c$};
		% Text Node
		\draw (360,115.4) node [anchor=north west][inner sep=0.75pt]    {$a$};
		\end{tikzpicture}
	\end{figure}
	Their extremities (vertices) determine a spherical triangle $ABC$, whose sides $a$, $b$ and $c$ are the arcs of large circles $\wideparen{BC}$, $\wideparen{AC}$ and $\wideparen{AB}$. The correlation coefficients between these vectors are (let us recall that for a unitary circle/sphere we have $\cos(\alpha)=\cos(a)$ as seen in the section of Trigonometry):
	  
	We have then by the cosine formula of the spherical triangles (\SeeChapter{see section Trigonometry page \pageref{cosine formula}}) the following value for the opening angle of $C$:
	 
	Just as $c$ is the angle between the points $A$ and $B$ (and on a unite sphere also the arclength $\wideparen{AB}$), seen from the center of the sphere, $\hat{C}$ is the spherical angle between points $A$ and $B$, seen from point $C$ to the surface of the sphere, and:
		
	is the "partial correlation" between $A$ and $B$ when $C$  is fixed. 
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} If we partial one variable out of a correlation, that partial correlation is named a "\NewTerm{first order partial correlation}". If we partial out $2$ variables from that correlation (e.g., $R_{12.34}$), we have a "\NewTerm{second order partial correlation}", and so forth. It is customary to refer to "\NewTerm{unpartialed (raw, as it were) correlations}" as "\NewTerm{zero order correlations}". \\
	
	\textbf{R2.} Keep in mind that as for the correlation, the partial correlation when high, doesn't necessarily imply causation!
	\end{tcolorbox} 
	With partial correlation, we find the correlation between $A$ and $B$ holding $C$ constant for both $A$ and $B$. Sometimes, however, we want to hold $C$ constant for just $A$ or just $B$. In that case, we compute a "\NewTerm{semipartial correlation coefficient}\index{semipartial correlation coefficient}\label{semipartial correlation}". 

	Let's compare the correlational formulas for the partial and semipartial:
	\begin{itemize}
		\item Partial:
		
		
		\item Semipartial:
		
	\end{itemize}
	Note that the partial and semipartial correlation formulas are the same in the numerator and almost the same in the denominator. The partial contains something extra, that is, something missing from the semipartial correlation in the denominator. This means that the partial correlation is going to be larger in absolute value than the semipartial. This will be true except when the controlling or partialling variable is uncorrelated with the variable to be controlled or residualized; this is a trivial case.
	
	\pagebreak
	\subparagraph{Intraclass coefficient of correlation}\mbox{}\\\\
	The "\NewTerm{intraclass correlation coefficient}\index{intraclass correlation coefficient}", denoted ICC, is a metric that can be used when quantitative measurements are made on measures that are organized into groups. It describes how the measurements of the same group are similar (thus it is still one of the many indicators of the family of repeatability and reproducibility studies like Cohen's kappa of approval that we will introduce later). This statistic is considered as a type of correlation and, unlike most other correlation measures, it therefore works on data structured in groups.
	
	An important application is the evaluation of the consistency or the reproducibility of quantitative measurements made by different observers measuring the same quantity (R\&R Gage studies - GRR). For example, if several doctors are asked to mark the results of a scan for signs of cancer progression, we may wonder how the scores are consistent from one physician to another (i.e. ANOVA with random factors).
	
	An important aspect of this statistic (and also a weakness) is that it quantifies both inter-observer variability and intra-observer variability. Inter-observer variability refers to systematic differences between observers - for example, a physician may always mark patients at a higher risk level than other physicians. Intra-observer variability refers to measurement deviations of a single observer.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Since the intraclass correlation coefficient gives a composition of inter- and intra-observer variability, its results are sometimes considered difficult to interpret when observers are not exchangeable (random). Alternative measures such as the Cohen approval kappa statistic (see below), the Fleiss kappa and the correlation coefficient of concordance were proposed as more appropriate measures of agreement between non-exchangeable observers.
	\end{tcolorbox}	
	There are several kinds of intraclass correlations. They relate to different ways of conceptualizing the variables that are often referred to generically as "subjects" and "judges." Either of these variables may be a random variable (their levels were chosen at random from a population of possible levels) or a fixed variable, the different levels of which were specifically selected for this design, and the same levels would be selected again in a replication. In addition, we could use individual ratings, or we could take the mean of a set of ratings by different raters. Doing so will alter the intraclass correlation coefficient by providing a more reliable measure.
	
	Here is a figurative example of a ICC (for numerical application see the Minitab or \texttt{R} companion books):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.9]{img/arithmetics/intra_class_correlation_coefficient.jpg}
		\vspace*{3mm}
		\caption[Intra-class correlation coefficient illustration]{Intra-class correlation coefficient illustration (source: Wikipedia)}
	\end{figure}
	The intra-class correlation coefficient that will interest us here, denoted ICC1, that is most often used in practice is to consider a fixed factor (non-random) and the other random (there are therefore other variants that must be applied with caution!).
	
	Indeed, imagine a study concerning the productivity of employees of a large company. The management committee wants to have an idea of the daily production and how it depends on the production machines used by the employees. If we take for the study a random sample of $n$ employees among $N$, with all the machines at a quantity of $k$ then we have a model with one fixed factor (machines) and the other one that is random (employees).

	The empirical idea is then to write the model in the following very specific form (for the other more general models it will be necessary to wait until we write the developments!):
	
	where $i$ is the measure of the fixed factor (often as rows in ICC analysis tables) and $j$ is the random factor represented by the judges (often as columns in ICC analysis tables) and thus $\varepsilon_{ij}$ describes the variability of measurement of the elements in a given level of random factor and the error of the judges is inseparable from the element measured. The previous relation is also sometimes denoted:
	
	where obviously $\mu_j=\mu+\tau_j$.

	As usual (...) we assume that:
	
	So the variances of the $\alpha_j$ are assumed to be identical (which is nevertheless quite a strong hypothesis in practice!).

	We have, therefore, from the linearity of the mean:
	
	The decomposition of the variance is immediate if the errors are independent of the random factors:
	
	We have therefore:
	
	For two elements (observations) taken under the same level of factor $i$, we have:
	
	But as the variances of all $\alpha_j$ are assumed to be identical and the errors independent of the fixed effects, the latter reduce to:
	
	And their correlation is then:
	
	This is often denoted in textbooks as:
	
	Where $S_w^2$ is the pooled variance (see page \pageref{pooled variance}) within subjects, and $S^2_b$ is the variance of the measurements between subjects. $S^2_b + S^2_w$ is the total variance.
	
	It is therefore the ratio of the variance of the random factors to the total variance (the denominator including the variance of the error) which is often denoted ICC1 or ICC(1,1) to say that it is the Intraclass correlation coefficient for 1 random factor and 1 fixed factor.

	So if $\sigma_\varepsilon\rightarrow 0$, the intraclass correlation is unitary and this is then and indicator of good reproducibility!
	
	Let us now recall from our study of one fixed factor ANOVA (page \pageref{anova one way fixed factor}) that we had the following decomposition in sum of the squares:
	
	If $n_i=n$ (balanced experiment), then we have obviously:
	
	This is often written in the context of the use of the intra-class correlation coefficient:
	
	where for recall $N=kn$. Here we assume therefore that the judges are indexed by $i$, the observations by $j$ and that we have $n$ judges and $k$ observations!
	
	And we also regularly have the following notations in some textbooks:
	
	or:
	
	where SSW stands for "Sum of Squares Within" and SSB for "Sum of Squares Between" or (...):
	
	where MSE stands for "Mean Square for Error" and MSk for "Mean Square for treatments". Therefore:
	
	Hence:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	As the reader have maybe notice it, the numerator may be negative and therefore so may be $\sigma^2_\alpha$... If we get negative variance components, the following are possible ways to deal with the negative estimates:
	\begin{itemize}
		\item Accept the estimate as evidence of a true value of zero and use zero as the estimate, recognizing that the estimator will no longer be unbiased.
	
		\item Retain the negative estimate, recognizing that subsequent calculations using the results might not make much sense.
	
		\item Interpret that the negative component estimate indicates an incorrect statistical model.
	
		\item Collect more data and analyse them separately or in conjunction with the existing data and hope that increased information will yield positive estimates.
	\end{itemize}
	For example, when a variance component estimate is less than zero, the software Minitab displays the negative estimate, but sets the estimate to zero in order to calculate the percent of total variability.
	\end{tcolorbox}
	This is the reasons why we often find the intraclass correlation coefficient in the following common form in the textbooks because it allows a sometimes a more intuitive approach to the concept:
	
	That is also often written is some textbooks as:
	
	Dominic V. Cicchetti (11994 according to holocene calendar) gives the following often quoted guidelines for interpretation for kappa or ICC inter-rater agreement measures:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\textbf{ICC value} & \textbf{Criteria} \\ \hline
		$0.4<$ & Poor \\ \hline
		$[0.4,0.59]$ & Fair \\ \hline
		$[0.60,0.74]$ & Good \\ \hline
		$>0.74$ & Excellent \\ \hline
		\end{tabular}
		\caption{Interpretation for kappa or ICC inter-rater agreement}
	\end{table}
	
	\pagebreak
	\subsubsection{Continuous Variables and Moments}
	
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] We say that $X$ is a continuous variable if its "\NewTerm{cumulative distribution function}\index{cumulative distribution function}" (C.D.F.) is continuous (already defined above). The distribution function of $X$ is defined by for $x \in \mathbb{R}$ or a truncated subset of $\mathbb{R}$:
		
		that is the cumulative probability that the random variable $X$ is smaller than or equal to the set value $x$. More properly, as for discrete variables, this is denoted by:
		
		with:
		
		
		\item[D2.] We denote by:
		
		the "\NewTerm{survival function}\index{survival function}" or "\NewTerm{tail function}\index{tail function}".
		
		\item[D3.] If furthermore the distribution function $F$ of $X$ is continuously differentiable of derivative $f$ (or sometimes denoted by $\rho$) named "\NewTerm{probability density function}\footnote{Keep in mind that the expression "\NewTerm{probability mass function}\index{probability mass function}" is reserved to discrete variables.}\index{probability density function}" or just simply "\NewTerm{distribution function}\index{distribution function}\label{distribution function}" then we say that $X$ is absolutely continuous and in this case we have:
		
		with the normalization condition:
		
		Any probability distribution function must satisfy the integral of normalization in its domain of definition!
	\end{enumerate}
	
	\label{random variable transformation}Let us consider a typical transformation of random variables that is quite common to generalize some special cases to more generic cases (typically from the standard Normal law to the general Normal law or from the standard Cauchy law to the generalized Cauchy law)! Let $X$ be a continuous random variable be then set $Y=a+b X$ where $a$ is named the "location parameter" and with $b>0$ named the "scale parameter" and let us denote $F$ the distribution function of $X$ and $G$ the distribution function of $Y$. Then:
	
	Now we differentiate and we get the important and trivial result:
	
	where $g$ is the density function for $Y$ and $f$ is the density function for $X$. 
	
	We can also consider the case where $b<0$. Then:
	
	differentiating:
	
	They reader must try to keep in mind these relations if possible.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
It is interesting to note that the definition implies that the probability that a completely continuous random variable takes a given value tends to zero! So it is not because an event has almost a zero probability that it can not happen!!!
	\end{tcolorbox}	
	The average being defined by a sum weighted by probabilities for a discrete variable, it becomes an integral for a continuous variable\index{expected mean continuous variable}\label{expected mean continuous variable} (the upper and lower integration limits are not always from $-\infty$ to $+\infty$ obviously! It depends of the definition domain of the random variable of interest):
	
	and therefore the variance for a continuous variable\label{variance continuous variable}: is written as:
	
	Then we have also the median that is logically redefined in the case of a continuous random variable by:
	
	and it rarely coincides with the average!
	
	And the modal value is given by the value of $x$ where:
	
	
	Statisticians often use the following notations for the expected mean of a continuous variable:
	
	and for the variance:
	
	That is the same as for the moment of discrete variable.

	Thereafter, we will calculate these different moments indicators with detailed proofs only for the most used cases.
	
	But before... let us discuss about some other well-knows moments and highly discussed...!
	
	\pagebreak
	\label{skewness and kurtosis}
	\paragraph{Skewness and Kurtosis}\mbox{}\\
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Caution! I have hesitated during many years (more than $10$ years!) to write something about the skewness and kurtosis in this book as they are most of time misleading concepts and that they are difficult (or even impossible...) to introduce with mathematical rigour. I changed my mind when I saw that my students were expecting something more complete and robust than Wikipedia. The reader must keep in mind that these both indicators must be use with caution and are good tools if we know that we are working with Normally distributed data.
	\end{tcolorbox}
	As already mentioned in the previous warning, skewness and kurtosis are somewhat vague notions. The reader should therefore not expect any proof of their origin here... Furthermore, the concept of "symmetry", is mathematically precise, but skewness by contrast is surprisingly slippery. Kurtosis is perhaps even more so.
	
	The underlying ideas corresponding definitions are the following (the definitions are given for discrete variable but can easily be extend to continuous variable by converting the sums into integrals!):
	
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] If the "\NewTerm{skewness}\index{skewness}", denoted in general $\gamma$, and associated most of time with the moment of order $3$ also named the "\NewTerm{Fisher-Pearson coefficient of skewness}\index{Fisher-Pearson coefficient of skewness}":
		
		is zero, the distribution is symmetric (this does not mean that the symmetry takes place on a peak of the distribution because in the bimodal case the axis of symmetry may be between the two symmetric modal values). If the skewness is positive (right-modal), the distribution (modal / median value) is lean to the right (or there are extreme values to the right). If the skewness is negative (left-modal), the distribution (the modal / median value) is leaned to the left (or there are extreme values to the left).
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		A common question is why the skewness is defined as third central moment, and not fifth or any other number? What's the logic behind it? In fact nothing special! We could choose any odd power but the thing is that the moment of order $1$ is the expected mean (already taken...) and the next smallest order easy to manage for further calculations is the power of $3$ (we let the reader imagine the integration of distribution function with variables of order like $5$ or bigger...).
		\end{tcolorbox}
		It's possible for a distribution to be symmetric but not have zero third moment. A simple counterexample is any $T$-distribution with $3$ or fewer degrees of freedom (see proof during our study of the corresponding distribution). In samples, however, symmetry implies zero third moment - but samples are almost never perfectly symmetric, so it's not much use there either. It's also possible to build asymmetric distributions that have non-zero third moment (see our \texttt{R} companion book!)!!! So symmetry doesn't necessarily imply zero third moment and zero third moment doesn't necessarily imply symmetry.
	
		\item[D2.] If the "\NewTerm{kurtosis}\index{kurtosis}", denoted in general $\kappa$ (see the maths just after the definitions), and associated most of time with the moment of order $4$ (hence always positive or equal to zero):
		
		and if we define the "\NewTerm{excess kurtosis}\index{excess kurtosis}" as $\kappa_1-3$. There are three distinct regimes as described below:
		\begin{itemize}
			\item The excess kurtosis is zero, ie "\NewTerm{mesokurtic}\index{mesokurtic}", then the flattening is similar to that of a Normal distribution\footnote{Rigorously a Normal distribution has a zero kurtosis. But the opposite is not true! Distributions that don't look Normal at all can have zero kurtosis!}.
			
			\item  If the value is greater than zero, ie "\NewTerm{leptokurtic}\index{leptokurtic}", then the distribution of interest is higher than that of a Normal distribution with equal mean.
			
			\item The excess kurtosis is smaller than zero, ie "\NewTerm{platikurtic}\index{platikurtic}",  then the distribution of interest is lower than that of a Normal distribution with equal mean.
		\end{itemize}
		Rigorously kurtosis measure how far the underlying distribution is from being symmetric and bimodal (algebraically, a perfectly symmetric and bimodal distribution will have a kurtosis of $1$, which is the smallest possible value the kurtosis can have). Most elementary texts describe kurtosis as a measure of the "peakedness" of a distribution. This term is misleading, and that a far better term for describing kurtosis is "bimodality," where the lower the kurtosis, the greater the bimodality.
		
		Notice that the kurtosis may seems similar to the idea of measuring the dispersion (i.e. variance). However, one way to look at kurtosis is that it measures the width of a random distribution compared to that of a  Normal distribution with the same mean and variance as the distribution in question. If the distribution is symmetric about the mean, a distribution with a positive kurtosis will have a sharper peak and much longer tails than does the Normal distribution. The opposite is the case for distributions with a negative kurtosis.
	\end{enumerate}
	Here is an illustration:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/arithmetics/moments.pdf}
		\caption[Kurtosis and Skewness range map of some distributions]{Kurtosis and Skewness range map of some distributions (author: Simon H. Hess)}
	\end{figure}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	If one does image searches for the "kurtosis" on Google, quite a few images show the kurtosis as being only a Gaussian distributions with a greater or smaller second order moment (i.e. the Gaussian is just larger or smaller). Such images are misleading representations of kurtosis.
	\end{tcolorbox}
	There have been a fairly large number of attempts to give measures corresponding to these notions, but these often-useful measures can be surprisingly counter-intuitive at times. For example, the moment-based skewness can be zero when the distribution is asymmetric (contradicting an assertion one can surprisingly often find when reading elementary texts which discuss skewness).
	
	In facts moments of a continuous distribution, and functions of them like the kurtosis, tell you extremely little about the graph of its density function!!!! It should be abundantly clear that kurtosis and skewness is not an easily interpretable or intuitive measure of symmetry, unimodality, bimodality, convexity, or any other familiar geometric characterization of a curve. Functions of moments, therefore (and kurtosis as a special case) do not describe geometric properties of the graph of the probability density function. This intuitively makes sense: because a probability density function represents probability by means of area, we can almost freely shift probability density around from one location to another, radically changing the appearance of the  probability density function, while fixing any finite number of pre-specified moments.
	
	Here are the skewness used by \texttt{R} (implements $\gamma_1$, $\gamma_2$, $\gamma_3$), MATLAB™ (implements $\gamma_1$, $\gamma_2$), Mathematica (implements $\gamma_1$), and MINITAB/IBM SPSS and Microsoft Excel (that implement $\gamma_3$):
	
	and the kurtosis used by \texttt{R} (implements $\kappa_3$, $\kappa_4$, $\kappa_5$), MATLAB™ (implements $\kappa_1$, $\kappa_2$), Mathematica (implements $\kappa_1$), and MINITAB/IBM SPSS and Microsoft Excel (that implement $\kappa_3$):
	
	From the relations above, only $\gamma_1$, $\gamma_2$, $\kappa_1$ and $\kappa_3$ are actually in the ISO 3534-1 standard.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	It exist at least 11 methods to calculate the Skewness. See the study by Tabor, J. (12010 according to holocene calendar), \textit{Investigating the Investigative Task: Testing for Skewness - An Investigation of Different Test Statistics and their Power to Detect Skewness}, Journal of Statistics Education, 18, 1-13. www.amstat.org/publications/jse/v18n2/tabor.pdf
	\end{tcolorbox}
	Skewness $\gamma_1$ can be expressed in terms of the non-central moment $\text{E}[X^3]$ by expanding its general definition (this form is very useful in practice to calculate the explicit skewness of a given distribution!):
	
	For a sample of $n$ values, a natural method of moments estimator of the population skewness $\gamma_1$ is obviously as already defined earlier above:
	
	Same for the Kurtosis:
	
	Here is a figure giving and idea of the range values of skewness squared and kurtosis for some distributions:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1076); %set diagram left start at 0, and has height of 1076
		
		%Shape: Polygon [id:ds8321723693979117] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 196; green, 180; blue, 221 }  ,fill opacity=1 ] (153.86,456.28) -- (388.2,54.61) -- (153.94,54.8) -- cycle ;
		%Shape: Polygon [id:ds9978572914367962] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 196; green, 180; blue, 221 }  ,fill opacity=1 ] (153.86,489.28) -- (520.86,132.28) -- (520.86,65.28) -- (154.86,464.28) -- cycle ;
		%Shape: Polygon [id:ds7339106774986031] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 165; green, 199; blue, 245 }  ,fill opacity=1 ] (154.02,465.37) -- (520.86,65.28) -- (521.28,53.3) -- (388.2,54.61) -- (154.02,455.7) -- cycle ;
		%Straight Lines [id:da3048566841518985] 
		\draw    (148.86,489.28) -- (535.86,489.28) ;
		\draw [shift={(537.86,489.28)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da394237738245544] 
		\draw    (195,484.28) -- (195,494.55) ;
		%Straight Lines [id:da40282908281569285] 
		\draw    (235.63,484.28) -- (235.63,494.55) ;
		%Straight Lines [id:da3981551093197515] 
		\draw    (276.26,484.28) -- (276.26,494.55) ;
		%Straight Lines [id:da21806982059087576] 
		\draw    (316.89,484.28) -- (316.89,494.55) ;
		%Straight Lines [id:da4081931975517423] 
		\draw    (357.52,484.28) -- (357.52,494.55) ;
		%Straight Lines [id:da6720456145634102] 
		\draw    (398.15,484.28) -- (398.15,494.55) ;
		%Straight Lines [id:da7985215700671673] 
		\draw    (438.78,484.28) -- (438.78,494.55) ;
		%Straight Lines [id:da15300399364913742] 
		\draw    (479.41,484.28) -- (479.41,494.55) ;
		%Straight Lines [id:da6494000378342348] 
		\draw    (520,484.28) -- (520,494.55) ;
		%Straight Lines [id:da9876597285634889] 
		\draw [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,draw opacity=1 ]   (154,94.37) -- (520.86,94.37) ;
		%Straight Lines [id:da09453074304276043] 
		\draw [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,draw opacity=1 ]   (154,134.28) -- (520.86,134.28) ;
		%Straight Lines [id:da03644593262829976] 
		\draw [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,draw opacity=1 ]   (154,174.28) -- (476.86,174.28) ;
		%Straight Lines [id:da9034815389983286] 
		\draw [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,draw opacity=1 ]   (154,214.28) -- (435.86,214.28) ;
		%Straight Lines [id:da20093093308089527] 
		\draw [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,draw opacity=1 ]   (153.9,254.54) -- (396.86,254.54) ;
		%Straight Lines [id:da3743227965607874] 
		\draw [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,draw opacity=1 ]   (153.9,294.54) -- (355.86,294.54) ;
		%Straight Lines [id:da6643487347427188] 
		\draw [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,draw opacity=1 ]   (153.9,334.54) -- (315.86,334.54) ;
		%Straight Lines [id:da30321986833076675] 
		\draw [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,draw opacity=1 ]   (153.9,374.54) -- (272.86,374.54) ;
		%Straight Lines [id:da3585910889647572] 
		\draw [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,draw opacity=1 ]   (153.86,414.73) -- (231.86,414.73) ;
		%Straight Lines [id:da508534274794348] 
		\draw [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,draw opacity=1 ]   (154.02,454.7) -- (189.86,454.7) ;
		%Straight Lines [id:da36424381643870474] 
		\draw    (153.86,494.55) -- (153.86,25.01) (149.86,454.55) -- (157.86,454.55)(149.86,414.55) -- (157.86,414.55)(149.86,374.55) -- (157.86,374.55)(149.86,334.55) -- (157.86,334.55)(149.86,294.55) -- (157.86,294.55)(149.86,254.55) -- (157.86,254.55)(149.86,214.55) -- (157.86,214.55)(149.86,174.55) -- (157.86,174.55)(149.86,134.55) -- (157.86,134.55)(149.86,94.55) -- (157.86,94.55)(149.86,54.55) -- (157.86,54.55) ;
		\draw [shift={(153.86,23.01)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da03718765531993862] 
		\draw [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,draw opacity=1 ]   (195,451.19) -- (195,55.64) ;
		%Straight Lines [id:da31265299829833526] 
		\draw [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,draw opacity=1 ]   (235.63,410.91) -- (235.63,54.91) ;
		%Straight Lines [id:da13325882507197084] 
		\draw [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,draw opacity=1 ]   (276.26,374.54) -- (276.26,54.91) ;
		%Straight Lines [id:da8581731274112063] 
		\draw [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,draw opacity=1 ]   (315.86,334.54) -- (315.86,52.91) ;
		%Straight Lines [id:da185487316145575] 
		\draw [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,draw opacity=1 ]   (357.52,292.91) -- (357.52,54.91) ;
		%Straight Lines [id:da33062101046428927] 
		\draw [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,draw opacity=1 ]   (398.15,252.91) -- (398.15,54.91) ;
		%Straight Lines [id:da051455349509653914] 
		\draw [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,draw opacity=1 ]   (438.78,214.28) -- (438.78,53.91) ;
		%Straight Lines [id:da032832962697630697] 
		\draw [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,draw opacity=1 ]   (479.41,174.28) -- (479.41,54.91) ;
		%Straight Lines [id:da674482156733818] 
		\draw [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,draw opacity=1 ]   (520,134.28) -- (520,53.3) ;
		%Straight Lines [id:da6320677338595453] 
		\draw [line width=1.5]    (153.86,489.28) -- (520.86,132.28) ;
		%Straight Lines [id:da27159530729005965] 
		\draw [line width=1.5]    (153.86,355.64) -- (287.19,55.05) ;
		%Straight Lines [id:da1651412882849188] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (153.86,414.73) -- (395.54,58.11) ;
		\draw [shift={(397.78,54.8)}, rotate = 124.12] [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (11.61,-5.58) -- (0,0) -- (11.61,5.58) -- cycle    ;
		%Shape: Circle [id:dp8166012697566929] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (313.3,174.28) .. controls (313.3,172.55) and (314.7,171.14) .. (316.43,171.14) .. controls (318.16,171.14) and (319.57,172.55) .. (319.57,174.28) .. controls (319.57,176.01) and (318.16,177.41) .. (316.43,177.41) .. controls (314.7,177.41) and (313.3,176.01) .. (313.3,174.28) -- cycle ;
		%Shape: Circle [id:dp4032248891101555] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (150.73,414.73) .. controls (150.73,413) and (152.13,411.6) .. (153.86,411.6) .. controls (155.6,411.6) and (157,413) .. (157,414.73) .. controls (157,416.47) and (155.6,417.87) .. (153.86,417.87) .. controls (152.13,417.87) and (150.73,416.47) .. (150.73,414.73) -- cycle ;
		%Shape: Circle [id:dp3042556330662254] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (150.88,454.7) .. controls (150.88,452.97) and (152.28,451.57) .. (154.02,451.57) .. controls (155.75,451.57) and (157.15,452.97) .. (157.15,454.7) .. controls (157.15,456.43) and (155.75,457.84) .. (154.02,457.84) .. controls (152.28,457.84) and (150.88,456.43) .. (150.88,454.7) -- cycle ;
		%Straight Lines [id:da8376911715980462] 
		\draw    (331.86,407.73) -- (226.44,324.97) ;
		\draw [shift={(224.86,323.73)}, rotate = 38.13] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da17961250471290047] 
		\draw    (331.86,407.73) -- (229.7,363.53) ;
		\draw [shift={(227.86,362.73)}, rotate = 23.4] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da6159332367152035] 
		\draw    (331.86,407.73) -- (241.8,385.22) ;
		\draw [shift={(239.86,384.73)}, rotate = 14.04] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		
		% Text Node
		\draw (137,480.68) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (137,445.68) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (137,405.68) node [anchor=north west][inner sep=0.75pt]    {$3$};
		% Text Node
		\draw (137,365.68) node [anchor=north west][inner sep=0.75pt]    {$4$};
		% Text Node
		\draw (137,326.68) node [anchor=north west][inner sep=0.75pt]    {$5$};
		% Text Node
		\draw (137,285.68) node [anchor=north west][inner sep=0.75pt]    {$6$};
		% Text Node
		\draw (137,247.68) node [anchor=north west][inner sep=0.75pt]    {$7$};
		% Text Node
		\draw (137,206.68) node [anchor=north west][inner sep=0.75pt]    {$8$};
		% Text Node
		\draw (137,164.68) node [anchor=north west][inner sep=0.75pt]    {$9$};
		% Text Node
		\draw (130,126.68) node [anchor=north west][inner sep=0.75pt]    {$10$};
		% Text Node
		\draw (130,86.68) node [anchor=north west][inner sep=0.75pt]    {$11$};
		% Text Node
		\draw (106,90) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {kurtosis};
		% Text Node
		\draw (460,516) node [anchor=north west][inner sep=0.75pt]   [align=left] {skewness squared};
		% Text Node
		\draw (129,45.68) node [anchor=north west][inner sep=0.75pt]    {$12$};
		% Text Node
		\draw (146.86,496.95) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (189.86,496.95) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (229.86,496.95) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (270.86,496.95) node [anchor=north west][inner sep=0.75pt]    {$3$};
		% Text Node
		\draw (310.86,496.95) node [anchor=north west][inner sep=0.75pt]    {$4$};
		% Text Node
		\draw (351.86,496.95) node [anchor=north west][inner sep=0.75pt]    {$5$};
		% Text Node
		\draw (392.86,496.95) node [anchor=north west][inner sep=0.75pt]    {$6$};
		% Text Node
		\draw (431.86,496.95) node [anchor=north west][inner sep=0.75pt]    {$7$};
		% Text Node
		\draw (473.86,496.95) node [anchor=north west][inner sep=0.75pt]    {$8$};
		% Text Node
		\draw (512.86,496.95) node [anchor=north west][inner sep=0.75pt]    {$9$};
		% Text Node
		\draw (186.47,230.61) node [anchor=north west][inner sep=0.75pt]  [rotate=-293.47] [align=left] {limit for Pearson distributions};
		% Text Node
		\draw (154.46,364.43) node [anchor=north west][inner sep=0.75pt]  [rotate=-300.34] [align=left] {mound-shaped distributions};
		% Text Node
		\draw (287.35,174.77) node [anchor=north west][inner sep=0.75pt]  [rotate=-301.23] [align=left] {Gamma distributions};
		% Text Node
		\draw (237.7,323.49) node [anchor=north west][inner sep=0.75pt]  [rotate=-310.03] [align=left] {$\displaystyle J$-shaped distributions};
		% Text Node
		\draw (324.88,287.75) node [anchor=north west][inner sep=0.75pt]  [rotate=-315.87] [align=left] {bimodal distributions};
		% Text Node
		\draw (440.88,220.75) node [anchor=north west][inner sep=0.75pt]  [rotate=-315.87] [align=left] {impossible zone};
		% Text Node
		\draw (324,163.91) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\footnotesize exponential}};
		% Text Node
		\draw (159,406.91) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {Normal};
		% Text Node
		\draw (160,448.91) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {Uniform};
		% Text Node
		\draw (335,389.46) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {Beta distributions\\fill the whole region\\below the Gammas};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{The Beta and Gamma families of distributions on the shape characterization plane}
	\end{figure}
	Later during the study of some important distributions, we will prove in details the values of the skewness and kurtosis for: the Uniform, Triangular, Normal, Student and the Beta distribution. 
	
	But before doing this let us prove an interesting theorem!
	\begin{theorem}
	Given a random variable $X$ with symmetric probability density function $f(x)$, we have $\text{E}(X)=a\text{E}(X)=a$ where $a$ is the point of symmetry assuming that $\text{E}(X)$ exists.
	\end{theorem}
	
	\begin{dem}
	First, it is obvious that symmetry about $a$ means according to the assumptions of the theorem statement that:
	
	for all $z$.
	
	Suppose that the expectation $\text{E}(X)$ exists. Then we know that:
	
	Let us rewrite this as:
	
	which is equal to:
	
	The first integral is obviously equal to $a$ as $f(x)$ is a density function. 
	
	In the second integral we make the change of variable $y=x-a$. Then this second integral becomes:
	
	Break up at $y=0$. We get:
	
	
	For the first integral, make the change of variable $z=-y$. We get:
	
	Using $f(a-z)=f(a+z)$, and some minor fooling with minus signs, we end up with:
	
	Therefore:
	
	becomes:
	
	And this is equal to zero! Then we have:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The skewness and kurtosis are fundamental moments to develop the famous Jarque-Bera test of normality or the famous Cornish-Fisher value at risk (we will see all that later in this book).
	\end{tcolorbox}	
	
	\subsection{Fundamental postulate of statistics}
	One of the ultimate goals of statistics is, starting from a sample, to find the analytical distribution function that gave birth to the sample. This goal will be presented in this book as a postulate (even if this assumption is quite difficult to apply in practice).
	
	Postulate: For any empirical distribution function $\hat{F}_n(x)$ of the $n$-th measurement of the $x$ random variable we can associate a theoretical distribution function $F(x)$ to which it converges when the sample size is large enough if:
	
	is the random variable defined as the largest difference (in absolute value) between $\hat{F}_n(x)$ and $F(x)$ (observed for all values of $x$ for a given sample), then $X_n$ converges to $0$ almost surely.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
Mathematicians of Statistics prove this postulate rigorously as a theorem named the "\NewTerm{fundamental theorem of statistics}\index{fundamental theorem of statistics}" or the "\NewTerm{Glivenko-Cantelli theorem}\index{Glivenko-Cantelli theorem}" regarding continuous functions. Personally, even if we offends the experts, we think that this proof is not one because it is very far away from the practical reality (yes this is our physicist side that emerges...) and this theoretical result leads many practitioners do their utmost (excluding data, transformations and other abominations) to find a known distribution law that they can adjust to their measured data.
	\end{tcolorbox}	
	
	\subsection{Diversity Index}\label{diversity index}
	It happens in the field of biology or business that you it is asked to a statistician or analyst to measure the diversity of a number of predefined elements. For example, imagine a multinational with a range of well-defined products and some of the stores (customers) in the world can choose a subset of this range for their business sales. The request is then to make a ranking of stores that sell the widest range of branded products and that by taking also into account the quantity.

	For example, we have a list of a total 4 products in our catalogue. By hazard, three of our customers sell our 4 products but we would like to know which customers sells the greatest diversity and this by taking into account the quantities.

	We have the following sales data by product for the customer $1$:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|}
			\hline
			\multicolumn{2}{|c|}{\cellcolor[gray]{0.75}Customer 1} \\ \hline
			Product 1 & 5 \\ \hline
			Product 2 & 5 \\ \hline
			Product 3 & 5 \\ \hline
			Product 4 & 5 \\ \hline
		\end{tabular}
	\end{table}
	
	For the customer $2$:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|}
			\hline
			\multicolumn{2}{|c|}{\cellcolor[gray]{0.75}Customer 2} \\ \hline
			Product 1 & 1 \\ \hline
			Product 2 & 1 \\ \hline
			Product 3 & 1 \\ \hline
			Product 4 & 17 \\ \hline
		\end{tabular}
	\end{table}
	and for the customer $3$:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|}
			\hline
			\multicolumn{2}{|c|}{\cellcolor[gray]{0.75}Customer 3} \\ \hline
			Product 1 & 2 \\ \hline
			Product 2 & 2 \\ \hline
			Product 3 & 2 \\ \hline
			Product 4 & 34 \\ \hline
		\end{tabular}
	\end{table}
	A measure of information (diversity of states) that is well suited to this purpose is the Shannon formula introduced in the section of Statistical Mechanics (page \pageref{shannon entropy}) whose mean:
	
	Arbitrarily, we will take $\lambda=1$ and the logarithm in base $10$ (so, if we have $10$ equiprobable variables, entropy is unitary for example...).
	
	Therefore we have:
	
	
	We will rewrite this more adequately for the application in business. Thus, if $n$ is the number of products and $p_i$ the proportion (or "relative frequency") of sales of product $i$ from all sales $N$ then:
	
	Then we have:
	
	This gives for the customer 1 (we stay in base $10$ for the logarithm):
	
	which is the maximum possible value (each state is equally likely). And for customer 2 we have:
	
	And finally for customer 3:
	
	Thus, the customer that has the greatest diversity is the first one. We also see an interesting property of the Shannon formula with customer 2 and 3 and this is that the quantity does not affect diversity (since the only difference between the two customers is that the quantity is multiplied by a factor of 2 and not diversity)!
	
	Notice that if we put all the products in only one customer (set), we get:
	
	Therefore we guess that (without general proof):
	
	Hence the fact that it is better to have various product in one place than in multiple place (result used for example in the field of document management where the purpose is to avoid to have the same type of documents saved in different share drive or libraries).
	
	\pagebreak
	\subsection{Distribution Functions (probabilities laws)}\label{statistical distributions}
	When we observe probabilistic phenomena, and we take note of the values taken by them and that we report them graphically, we can observe that the individual measurements follow a typical characteristic which is sometimes adjustable theoretically with a good level of quality.

In the field of probabilities and statistics, we name these characteristics "\NewTerm{distribution functions}" because they indicate the frequency with which the random variable appears for given values.

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
We sometimes simply use the term "function" or "law" to describe these characteristics.
	\end{tcolorbox}	
	
	These functions are in practice bounded by what we name the "\NewTerm{range of the distribution}\index{range of a distribution}" which is the difference between the maximum value (on the right) and the minimum value (on the left) of the observed values:
	
	In theory they are not necessarily bounded and then we talk (\SeeChapter{see section Functional Analysis page \pageref{natural domain of definition}}) about a "\NewTerm{domain of definition}" or more simply about the "\NewTerm{support}\index{support}" of the function.

If the observed values are distributed in a certain way then there is a probability (or "cumulative probability" in the case of continuous distribution functions) to have a certain value of the distribution function.

In industrial practice (\SeeChapter{see section Industrial Engineering page \pageref{range six sigma}}), the range of statistical values is important (as well as the standard deviation) because it gives an indication of the variation of a process (variability).

If $L$ denote any possible univariate distribution function the range of the function is simply denoted by $L$ if its domain of definition is $\mathbb{R}$  otherwise if it is bounded you will typically see something like $L_{]a,b]}$.

\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] The mathematical relation that gives the probability of a given value of the distribution function a random variable is named the "\NewTerm{density function}\index{density function}" (or "\NewTerm{probability density function}\index{probability density function}\label{probability density function}"), "\NewTerm{mass function}\index{mass function}" or "\NewTerm{marginal function}\index{marginal function}".
		
		\item[D2.] The mathematical relation that gives the cumulative probability that a random variable to be lower than or equal to a certain value of the distribution function is referred to as the "\NewTerm{repartition function}\index{repartition function}" or "\NewTerm{cumulative function}\index{cumulative function}" or "\NewTerm{cumulative distribution function}\index{cumulative distribution function}".
		
		\item[D3.] Random variables are "\NewTerm{independent and identically distributed (i.i.d.)}\index{independent and identically distributed variables}" if they all follow the same distribution function, with the same parameters values and that they are independent.
	\end{enumerate}
	Such functions are very numerous, we offer then here to the reader a detailed study of the most known only.

Before going any further it could be useful to know that if $X$ is a continuous or discrete random variable, then are several tradition of notation in the literature to indicate that it follows a given probability distribution $L$. Here are the most common:
	
	In this section and throughout the book in general, we will use the last notation!

Here is the list of the distribution functions that we will see here as well as distribution functions commonly used in the industry and located in other chapters/section and those whose proof has yet still to be written:

	\begin{itemize}[noitemsep,nolistsep]
		\item Discrete Uniform distribution $\mathcal{U}(a,b)$ (see below)
		\item Bernoulli distribution $\text{B}(1,p)$ (see below)
		\item Geometric distribution $\mathcal{G}(N)$ (see below)
		\item Binomial distribution $\mathcal{B}(N,k)$ (see below)
		\item Binomial Negative distribution $\text{NB}(N,k,p)$ (see below)
		\item Hypergeometric distribution $\mathcal{H}(n,p,m,k)$ (see below)
		\item Multinomial distribution $\mathcal{H}(\vec{k},\vec{p},m)$ (see below)
		\item Poisson distribution $\mathcal{P}(\mu,k)$ (see below)
		\item Gauss-Laplace/Normal distribution $\mathcal{N}(\mu,\sigma)$ (see below)
		\item General Folded Normal distribution (see below)
		\item Half-Normal distribution (see below)
		\item Log-Normal distribution $\mathcal{LN}(\mu,\sigma)$ (see below)
		\item Continuous Uniform distribution (see below)
		\item Triangular distribution (see below)
		\item Pareto distribution (see below)
		\item Exponential distribution (see below)
		\item Weibull distribution (see section Industrial Engineering page \pageref{weibull distribution}) 
		\item Generalized Exponential distribution (still must be written)
		\item Erlang/Erlang-B/Erlang-C distributions (see section Quantitative Management page \pageref{erlang distribution})
		\item Cauchy distribution (see below)
		\item Beta distribution (below and section Quantitative Management page \pageref{beta distribution application})
		\item Gamma distribution (see below)
		\item Chi-squared distribution (see below)
		\item Student distribution (see below)
		\item Fisher-Snedecor distribution (see below)
		\item Benford distribution (see below)
		\item Logistic distribution (see section Numerical Methods page \pageref{logistic distribution})	
		\item Laplace distribution $\mathcal{L}(\mu,b)$ (see below)
		\item Square Gauss Distribution (see below)
		\item Extreme value Distribution (still must be written)
		\item Vasicek distribution (already written but need to be reviewed)
	\end{itemize}
	For a more exhaustive list (almost a hundred!) and classification of distribution functions, the reader can see refer to the Data Science mind map at page \pageref{mindmap of data science}.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The reader will find the mathematical developments of the Weibull distribution function in the section on Industrial Engineering (page \pageref{weibull distribution}), and the logistic distribution function in the section of Numerical Methods (page \pageref{logistic distribution}).
	\end{tcolorbox}	
	
	\subsubsection{Discrete Uniform Distribution}
	
	If we accept that it is possible to associate a probability to an event, we can conceive of situations where we can assume a priori that all elementary events are equally likely (that is to say, they have the same probability to occur). We then use the ratio between the number of favourable cases and the number of possible cases to calculate the probability of all events in the Universe of events $U$. More generally, if $U$ is a finite set of equally likely events and $A$ is part of $U$, then we have using set theory notation (\SeeChapter{see section Set Theory page \pageref{cardinal}}):
	 
	 More commonly, if $e$ is an event that may have $N$ equally likely possible outcomes. Then the probability of observing the outcome of this given event follows a "\NewTerm{discrete uniform function}\index{discrete uniform function}" (or "\NewTerm{discrete uniform law}\index{discrete uniform law}") given by the relation:
	 
	 Whose mean (or average) is given by:
	  
	 If we put ourselves in the particular case where $x_i=1$ with $i=1\ldots N$. We then have (\SeeChapter{see section Sequences and Series page \pageref{gauss series}}):
	 
	 If the random variable $e$ take all values between $[a,b]$ (another special case) such the distribution will be now denoted by $\mathcal{U}(a,b)$ then it should be obvious that we have for the expected mean:
	 
	 
	 For the variance we have (always using the results of the section on Sequences and Series page \pageref{sum of squares integers}):
	 
	 
	 If the random variable $e$ take all values between $[a,b]$ (another special case) such the distribution will be now denoted by $\mathcal{U}(a,b)$ then it should be obvious that we have for the variance:
	 
	 
	 By symmetry of the distribution if all values of the domain of definition $[a,b]$ are taken by the random variable we have for the median:
	 
	 
	 Here is a plot example of the mass distribution function and cumulative distribution function respectively for discrete uniform law of parameters $\left\lbrace 1,5,8,11,12\right\rbrace$ (we see that each value is equally likely):
	 
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.75]{img/arithmetics/law_uniform.jpg}
		\end{center}	
		\caption{Uniform law $\mathcal{U}$ (density and cumulative distribution function)}
	\end{figure}
	
	As we can see in the above diagram the cumulative distribution function can be written:
	
 
	 \begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
For sure the discrete uniform distribution has no specific modal value $M_0$!
	\end{tcolorbox}	
	

	\subsubsection{Bernoulli Distribution}\label{bernoulli distribution}
	
	If we are dealing with a binary observation then the probability of an event is constant from one observation to the other if there is no memory effect (in other words: a sum of Bernoulli variables, two by two independent).
	
	We name this kind of observations where the random variables takes the values $0$ (false) or $1$ (true), with probability $q=1-p$ respectively $p$, "\NewTerm{Bernoulli trials}\index{Bernoulli trials}" with "\NewTerm{contrary events with contrary probabilities}".
	
	Thus, a random variable $X$ follows a "\NewTerm{Bernoulli function}\index{Bernoulli function}" $\text{B}(1,p)$ (or "\NewTerm{Bernoulli law}\index{Bernoulli law}") if it can take only the values $0$ or $1$ , associated with probabilities $p$ and $q$ and so that $q+p=1$ and:
	
	The classic example of such a process is the game of piece face or sampling with replacement or be considered as such (this last case is very important in industrial practice). There certainly is no need for the reader to formally verify that the cumulative probability is unitary...
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
The introduction above is perhaps not relevant for business, but we will see in the section of Quantitative Techniques (page \pageref{queueing theory}) that the Bernoulli function naturally appears at the beginning of our study of queuing theory.
	\end{tcolorbox}	
	
	Note that, by extension, if we consider $N$ events where we get in a particular order $k$ times one possible outcomes (success) and the other $N-k$ (fail) times, then the probability of such a series ($k$ successes and $N-k$ failures ordered in any particular way) is given intuitively by (applying independent events rule):
	
	with $N \in \mathbb{N}^{*}$.

Here is an example plot of the cumulative distribution function for $q=0.3$:

	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.75]{img/arithmetics/law_bernoulli.jpg}
		\end{center}	
		\caption{Bernoulli law $\text{B}$ (cumulative distribution function)}
	\end{figure}
	The Bernoulli function has therefore for expected mean (average) choosing $p$ as the probability of the event of interest:
	
	and for variance (we use the Huygens theorem proved above):
	
	
	The modal value $M_0$ of the Bernoulli law depends on the values of $p$ or $q$. So we have (it could be obvious for the reader):
	
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
For sure the Bernoulli distribution has no specific median value $M_e$!
	\end{tcolorbox}

	
	\subsubsection{Geometric Distribution}\label{geometric distribution}
	The geometric law $\mathcal{G}(N)$ or "\NewTerm{Pascal's law}\index{Pascal's law}" consist in a Bernoulli trial, where the probability of success is $p$ and that of failure $q=1-p$ are constant, that we renew independently until the first success.
	
	Remember that during our presentation of the Bernoulli law we have deduce an extension to $N$ such that:
	
	Therefore the probability to get the first success $k=1$ after $N$ trials is:
	
	with $N \in \mathbb{N}^{*}$.
	
	As you can see, greater is $N$, smaller is the probability $\mathcal{G}(N)$. This can be seem non-logic but in fact it is! Indeed in the sentence «\textit{the probability to get the first success after $N$ trials}», you must not forget that it is written \underline{after} and not \underline{during}.
	
	Therefore for sure... the probability to have $N-1$ failures followed by $1$ success will be always be smaller when $N$ increase (have a look to the figure a little bit further below for $p=0.5$, it may help to understand).
	
	This law has for expected mean:
	
	However, the last relation can also be written:
	
	Indeed, we proved in the section of Sequences and Series (page \pageref{geometric series}) during our study of geometric series that:
	
	Taking the limit $n\rightarrow +\infty$ when we get:
	
	because $0\leq q < 1$. 
	Then we just derive both members of equality with respect to $q$ and we get:
	
	This done let us continue...
	
	We have then the average number of trials $X$ it takes to get the first success (or in other words, the expected rank - number of expected trials - to see the first success):
	
	
	Now we calculate the variance and reminding once again (Huygens theorem):
	
	So let's start by calculating $\text{E}(X^2)$:
	
	The last term of this expression is equivalent to the expected mean calculated previously. Thus:
	
	It remains to calculate:
	
	We have:
	
	But deriving the following equality:
	
	We get:
	
	Therefore:
	
	Thus:
	
	Finally when it comes to ranking the expected variance of the first success (i.e.: the variance expected number before the first successful trials):
	
	The modal value is easy to get because we need to find the value of $N$ that maximize the definition of the geometric law:
	
	and we hope that it is immediate to the reader that this is satisfy when $N=1$ therefore:
	
	Now let us determine the median $M_e$. For this, by definition we know we must have:
	
	But we can rewrite:
	
	Therefore (in base $10$):
	
	Finally, based on our definition of the median, we get:
	
	Now we determine the cumulative function of the geometrical law. We start from:
	
	Then we have, by definition of the cumulative probability, that the experience is successful in the first $N$ trials:
	
	with $N$ being for sure an integer of values $0,1,2,\ldots$.
	
	We write: 
	
	We then have for the cumulated distribution function:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\textbf{{\Large \ding{45}}Example:}\\\\
	You try late at night and in the dark, to open a lock with a bunch of five keys, without attention, because you are a little tired (or a little tipsy ...) you will try each key. Knowing that only one key will work, what is the probability of using the right key at the $N$-th test?\\
	
	The solution is:
	
	\end{tcolorbox}
	
	Plot of the mass function and cumulative distribution function for the Geometric law with parameter $p=0.5$:
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/arithmetics/law_geometric.jpg}
		\end{center}	
		\caption{Geometric law $\mathcal{G}$ (mass and cumulative distribution function)}
	\end{figure}
	
	\subsubsection{Binomial Distribution}\label{binomial distribution}
	
	We come back now to our Bernoulli experiment. More generally, any particular $N$-tuple consisting of $k$ successes and of $N-k$ failures will have for probability (within a sampling with replacement or without replacement if the population is large ... in a first approximation):
	
	to be drawn (or appear) whatever the order of appearance of successes and failures (the reader will have perhaps notice that this is a generalization of the geometric distribution, just write $k = 1$ to find the geometric distribution back).
	
	But we know that the combinatorial determines the number of $N$-tuples of this type (the number of ways to order the appearance of failures and successes). The number of possible arrangements is, as we proved it (\SeeChapter{see section Probabilities page \pageref{choice function}}), given by the binomial coefficient (we recall that the notation in this book does not comply with ISO standard 31-11):
	
	So as the probability of obtaining a given series of $k$ successes and $N-k$ failures is always the same (regardless of the order) then we have just to multiply the probability of a particular series by the binomial coefficient (this is equivalent to a sum) such that:
	
	to get the total probability to obtain any of these possible series (since each is possible).
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
This is equivalent to the study of a sampling with simple replacement (\SeeChapter{see section Probabilities page \pageref{choice function}}) with constraint on the order or to the study of a series of successes and failures. We will use this relation in the context of the queuing theory or reliability (\SeeChapter{see section Industrial Engineering page \pageref{industrial engineering}}). Note that in the case of large populations, even if the sampling is not with replacement it can be considered as with...
	\end{tcolorbox}
		
	Written in another way this gives the "\NewTerm{binomial function}\index{binomial function}" (or "\NewTerm{binomial law}\index{binomial law}"), also known as the following distribution function:
	
	and sometimes also denoted by $\beta(n,p)$ with a lowercase $n$ or uppercase $N$ (it does not really matter...) and can be calculated in the English version of Microsoft Excel 11.8346 using the \texttt{BINOMDIST( )} function.
	
	We sometimes say that the binomial law is not exhaustive as the size of the initial population is not apparent in the expression of the law.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The Binomial distribution is named "\NewTerm{Symmetric Binomial Distribution}\index{symmetric binomial distribution}" when $p=0.5$.
	\end{tcolorbox} 
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. We want to test the alternator of a generator. The probability of failure at solicitation of this material is estimated to be 1 failure per $1,000$ starts.\\

	We decided to test $100$ starts. The probability of observing one failure in this test is:
	
	E2.\label{prosecutor fallacy frequentist example} Maybe you remember the prosecutor's fallacy we have introduced at page \pageref{prosecutor fallacy}. Let us come back on the example related to the crime-scene DNA sample. Now that we have introduced the binomial distribution, we can calculate the probability of getting at least one match among the record:
	
	So, this evidence alone is an uncompelling data hacking result. If the culprit was in the database then he and one or more other men would probably be matched; in either case, it would be a fallacy to ignore the number of records searched when weighing the evidence. "Cold hits" like this on DNA databanks are now understood to require careful presentation as trial evidence.
	\end{tcolorbox}
	We obviously have for the cumulative distribution function (very useful in practice for suppliers batch control or reliability as we will see in the section of Industrial Engineering page \pageref{sampling plans}!):
	
	Indeed, we have proved in the section Calculus (page \pageref{binomial theorem}) the "\NewTerm{binomial theorem}\index{binomial theorem}":
	
	Therefore:
	
	Instead of calculating such cumulated probability rather than hand it is better to use Microsoft Excel 11.8346 (or any other widely known software) with the function \texttt{CRITBINOM()} to not bother to calculate these type of values.
	
	The expected mean (average) of $\mathcal{B}(N,k)$ is given by:
	
	
	But having:
	
	We finally get:
	
	that gives the average number of times that we will get the desired outcome of probability $p$ after $N$ trials.
	
	The mean of the binomial distribution is sometimes noted in the specialized literature with the following notation if $r$ is the potential number of possible expected outcomes in a population of size $n$:
	
	Before calculating the variance, we need to introduce the following equality:
	
	Indeed, let us proof this relation using the previous developments:
	
	We recognize in the last equality the cumulative distribution function that is equal to 1. Therefore:
	
	
	We start now the (long) calculation of the variance of the binomial distribution by using the previous results:
	
	Finally:
	
	The standard deviation of the binomial distribution is sometimes noted in the specialized literature in the following way if $r$ is the potential number of expected outcomes in a population of size $n$ and $s$ the not expected one:
	
	Here is a plot example of the binomial $\mathcal{B}(10,0.5)$ distribution and cumulative distribution function:
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/arithmetics/law_binomial.jpg}
		\end{center}	
		\caption{Binomial law $\mathcal{B}$ (mass and cumulative distribution function)}
	\end{figure}
	
	\label{normalized variance and mean of binomial distribution}It could be useful to note that some employees in companies (especially physicians in the field of survival analysis - clinical studies - as we will see later!) normalize the calculation of the mean and standard deviation to the unit of $N$. Then we have:
	
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\textbf{{\Large \ding{45}}Example:}\\\\
	In a sample of $100$ workers, $25\%$ are late at least once a week. The mean number and variance of late people is then:
	
	Normalized to the unit of $N$ this give us:
	
	\end{tcolorbox}
	Let us now calculate the mode. Because the function is discrete we can not use derivative. Then we will use a hint. We compute the ratio:
		
	 and we check that this ratio is $>1$ for every $k<k^{*}$ and $\leq 1$ for every $k\geq k^{*}$, for some integer $k^{*}$ that is the $k$ value corresponding to the modal value. 
	 
	 Let $a_k=P(X=k)$. We have:
	 
	 We calculate the ratio $\dfrac{a_{k+1}}{a_k}$. Note that:
	 
	 What is important now is to analyse:
	  
	 depending on the value of $k$. First we can see that this ratio is equal to $1$ and therefore we have two modes if:
	 
	 That is to say if $k=np+p-1=p(n+1)-1$. This can be seen as the limit point of interest. But don't forget we are looking for the $k$ such that the ratio is less than $1$. So we try two values: 
	 
	 Injecting this in our ratio we see that:
	 
	 Is the value we were looking for! Finally there are two possible values for the modes. A unique modal value and a double modal value.
	
	As we know, the median value is the value of $X$ such that we have:
	
	But we did not yet found an easy proof to determine $M_e$ in the general case for the Binomial law.
	
	To conclude on the binomial law, we will develop now a result that we will need to build the McNemar paired test for a square contingency table (and as it is squared it is also dichotomous) that we will study further below (page \pageref{mcnemar test}).

	We need for this test to calculate the covariance of two-paired binomial random variables (this is why the covariance is non-zero):
	
	As they are paired, this means that:
	
	And therefore:
	
	Now comes the difficulty that is to calculate $\text{E}(n_in_j)$. To calculate this term it does not exist to our knowledge other methods than looking for the law of the pair (sometimes we can get around such approach). In this case it is a multinomial distribution (more precisely: trinomial) that it is customary to write in the following way by construction:
	
	that we will write now temporarily as following to condense the expression:
	
	So we have a trinomial law as we are looking for the number of times we have the event $k$, the event $l$ and neither one nor the other (so the rest of the time).  
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/trinomial_distribution.pdf}	
		\caption{Trinomoial distribution with $p=1/5,q=2/5$ and $n=5$}
	\end{figure}
	We then get:
	
	If $k \geq 1$ and $l \geq 1$, we obtain:
	
	Now we use this relation in the joint mean:
	
	Consider now the special case where $n$ is equal $2$. We then have: 
	
	where the sum is reduced to only one term because if we take for example $k=2,l=1$ we get a negative factorial at the denominator.
	
	For $n$ equal $3$, the result will be also $1$, and so on (we will assume to simplify... that some numerical examples will suffice to convince the reader of the generality of this property because it is very boring to write with \LaTeX).
	
	Then we have:
	
	So in the end\label{covariance trinomial distribution}:
	
	It does make sense for the covariance to be negative, since the more there are in category $1$, the fewer are available for category $2$. And this is the major result we will need for the study of the McNemar test. 
	
	Let $X$ and $Y$ be independent binomial random variables with parameters $(n,p)$ and $(m,p)$, respectively. Let $Z=X+Y$. Then the distribution of $Z$ is given by:
	
	Hence the binomial distribution is indeed stable by addition:
	
	Doing the same development with different probabilities and without forgetting that $p_i+p_j=1$ then we have:
	

	\subsubsection{Negative Binomial Distribution}\label{negative binomial distribution}
	
	The negative binomial distribution is applied in the same situation as for binomial distribution, but it gives the probability to have $E$ failures before the $R$-th success when the probability of success is $p$ (or, at contrary, the probability to have $R$ success before $E$-th failure when the failure probability is $p$).

	We will introduce this important distribution with an example. Consider this for this purpose the following probabilities:
	
	Imagine that we have done $10$ trials and we wanted to stop at the third success and that the 10th trial is the third successful one! We will write this:
	
	Now we highlight what we will consider as the successes (R) and failures (E):
	

We have also $7$ failures and $3$ successes. In an experiment where the draws are independent (or can be considered as independent...), the probability that we get this particular result is:
	
	But the order of successes and failures in the bracketed part is irrelevant. So as we have $2$ success among the $9$ trial in brackets it follows that the probability of obtaining the same result regardless of the order is then using combinatorics:
	
	Which corresponds to the probability of having $7$ failures before the 3rd success (or otherwise seen: 3 successes after $10$ trials). This can be written with Microsoft Excel 14.0.6123 or later ($7+3 = 10$ trials, $7$ failures including $3$ successes):
	\begin{center}
		\texttt{=NEGBINOMDIST(7,3,0.2,0)=0.0604}
	\end{center}
	We now generalize the prior-previous notation by writing the number of failures $k, N$ the total number of trials and $p$ the probability of success:
	
	However, there are several possible notations because the previous relation is not very intuitive to practice as may have perhaps noticed the reader. Thus, if we denote $k$ as the number of successes and not the number of failures, then we have (the most common writing way from my point of view among a lot of others notations) the following probability of having $N-k$ success before having a number $k$ of failures with a probability $p$ (or of failures before having $k$ successes ... it's symmetrical!):
	
	therefore the comparison with the formulation of the binomial distribution proved above is then perhaps more obvious!
	
	However, it is more common to write the previous relation by removing $N$ because for the moment the notation is still perhaps not very clear. For this, we note $R$ the number of successes, $E$ the number of failures, $p$ the probability of success and then comes the probability of having $R$ success after $E$ failures (this is perhaps much more clear...):
	
	We sometimes find this last relation with another relation using explicitly the binomial coefficient:
	
	The cumulative probability that we have at least $R$ successes before the $E$-th failure is obviously given by:
	
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The name of this law comes from the fact that some statisticians use a definition of the binomial coefficient with a negative value for the expression of the function. Since this is a rather a rare notation, we do not want lose time to prove the origin of the name. You should also know that this law is also known as the "\NewTerm{Pascal's law}\index{Pascal's law}" (as well as the geometric distribution ...) in honour of Blaise Pascal and also as "\NewTerm{Polya's law}\index{Polya's law}" in honour of George Pólya. 
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. A long-term quality control has enabled us to compute the estimator of the proportion $p$ of nonconforming pieces as equal to $2\%$ at the output of a production line. We would like to know the cumulative probability to have $200$ pieces before the 3rd defective piece appears. With Microsoft Excel 14.0.6123 or later it comes using the negative binomial distribution:
	\begin{center}
		\texttt{=NEGBINOM.DIST(200,3,0.02,1)=77.35\%}
	\end{center}
	E2. To compare with the binomial distribution, we can ask ourselves what is the cumulative probability of drawing 198 non-defective parts from 201 using Microsoft Excel 14.0.6123 or later:
	\begin{center}
		\texttt{=BINOM.DIST(198,201,0.98,1)=76.77\%}
	\end{center}
	Therefore we see that the difference is small. In fact the difference between the two laws is in practice so small that we then use almost always the binomial law (but you should still be careful with this choice!).
	\end{tcolorbox}
	As usual, we will now determine the variance and mean of this law. Let's start with the mean of having $R$ successes when the $E$-th failure appears knowing that the probability of a failure is $p$. For this we will use a very simple and ingenious trick (all art was thinking about it...). If we return to our initial example:
	
	and we rewrite this example as follows:
	
	We then notice that the third success $R$ of the first notation can be decomposed into the sum of three geometric random variables such that:	
	
	With in the case of this particular example $n=3$ corresponding in fact to $E=3$. So quite generally the sum of $n$ random geometric variables always gives a negative binomial distribution if the probability $p$ is equal for each geometric variable!!! Anyway... as we have proved the expression of the mean and variance of the Geometric law as (thus giving us the mean rank of the first failure):
	
	since the random variables are independent and of same parameters then it comes for the negative binomial the mean of the rank of $E$-th failure using the property of the mean:
	
	And therefore for the variance of the negative binomial distribution:
	
	So the mean and variance of the rank (corresponding to the number of trials $N$ or from another point of view: the mean number of successes by the simple subtraction $X - E$) to have the $E$-th failure is then to summarize:
	
	Thus, putting $E = 1$, we fall back on the mean and variance of the geometric distribution!
	
	Now, let $Y$ be the random variable representing the number of trials \underline{before} the $R$-th success. We then have the following expressions for the variance and the mean that are very common in the literature (these expressions of mean and variance corresponds to what we can find for the negative binomial law in Wikipedia for example):
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\textbf{{\Large \ding{45}}Example:}\\\\
		What is the expected number (mean) of trials we can expect before we fall on the third non-conforming part, knowing that the probability of a non-conforming part is $2\%$?
		
		and for the standard deviation:
		
	\end{tcolorbox}
	Like always the reader will find below a plot example of the distribution and cumulative distribution function for the negative binomial law of parameters $\text{NB}(N,k,p)=P(N,3,0.6)$ based on the example of the begging, but where the only difference is the probability of success where we he have taken $60\%$ instead of $20\%$.
	
	Thus, there is $21.6\%$ of probability of having the third success after the third successive trial (i.e. $0$ trials more than the number of successes), $25.92\%$ of probability of having the third success after the fourth successive trial (i.e. one trial more than the number of successes), $20.7\%$ of probability of having the third success after the fifth successive trial (i.e. two trial more than the number of successes) and so on...:
	
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=1.2]{img/arithmetics/law_binomial_negative.jpg}
		\end{center}	
		\caption{Negative Binomial law $\texttt{NB}$ (mass and cumulative distribution function)}
	\end{figure}
	
	The above distributions are truncated to $9$ (corresponding to $12$ trials) but they theoretically continue indefinitely. What particularly distinguishes the binomial and geometric distributions from the negative binomial are the tails of the distribution.

	The binomial negative distribution has an important place in a special regression technique that we will see in the section of Theoretical Computing at page \pageref{regression techniques}.

	\subsubsection{Hypergeometric Distribution}\label{hypergeometric distribution}
	
	We consider to approach this function a simple example (but not very interesting in practice) that is this of an urn containing $n$ balls where $m$ are black and the other $m'$ white (for several important examples used in the industry refer to the sections of Industrial Engineering page \pageref{quality control}). We take successively, and without replacement in the urn, $p$ balls. The question is to find the probability that among the $p$ balls, there is $k$ that are black (in this statement the order of the drawing does not interest us).
	
	We often talk about "exhaustive sampling" with the hypergeometric distribution because at the opposite of to the binomial distribution, the size of the lot which is the basis for the sampling will appear in the law.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		This is also equivalent to a non-ordered sampling without replacement/repetition (\SeeChapter{see section Probabilities page \pageref{choice function}}) with constraint on the occurrences sometimes named "simultaneous sampling". We will often use the hypergeometric distribution in the field of quality and reliability where the black balls are associated to items with defects and the white one to items without defects.
	\end{tcolorbox}
	
	The $p$ balls can be chosen among $n$ balls in $C_p^n$ ways (thus representing the number of possible different outcomes) with as reminder (\SeeChapter{see section Probabilities page \pageref{choice function}}):
	
	
	The $k$ black balls can be chosen among the $m$ black in $C_k^m$ ways. The $p-k$ white balls can be chosen in $C_{p-k}^{n-m}$ ways. There is therefore $C_k^mC_{p-k}^{n-m}$ ways to have $k$ black balls and $p-k$ white balls.
	
	The searched probability is therefore given by (we will see an alternative notation in the section of Industrial Engineering page \pageref{quality control}):
	
	and is said to follow a "\NewTerm{Hypergeometric distribution}\index{hypergeometric distribution}" (or "\NewTerm{Hypergeometric law}"\index{hypergeometric law}) and can be obtained fortunately directly in Microsoft Excel 14.0.7153 with the function \texttt{HYPGEOM.DIST()}.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. We want to develop a small computer program of $10,000$ lines of code ($n$). The return on experience shows that the probability of failure is one bug per $1,000$ lines of code (or $0.1\%$ of $10,000$ lines) that corresponds to the value $m$.\\
	
		We test about $50\%$ of the functionality of the software randomly before sending it to the customer (corresponding to the equivalent of $5,000$ lines that is $p$). The probability of observing $5$ bugs ($k$) is then given with Microsoft Excel 14.0.715:
	\begin{center}
		\texttt{HYPGEOMDIST(k,p,m,n)=HYPGEOMDIST(5,5000,1\%*10000,10000)=24.62\%}
	\end{center}

	E2. In a small single production of a batch of $1,000$ pieces we know that $30\%$ on average are bad because of the complexity of the pieces and by return on experience from a previous similar manufacturing. We know that a customer will randomly draw $20$ pieces to decide whether to accept or reject the lot. He will not reject the lot if he finds zero defective pieces on the $20$. What is the probability of having exactly $0$ defective?
	\begin{center}
		\texttt{=HYPGEOMDIST(0,20,300,1000)=0.073\%}
	\end{center}
	and as we require a null draw drawing result, the calculation of the hypergeometric distribution simplifies manually to: 
	
	\end{tcolorbox}
	It is not forbidden to make direct calculation of the mean and variance of the hypergeometric distribution, but the reader will without much trouble guess that this calculation will be ... relatively indigestible. Then we can use an indirect method that is much more interesting!

	First, the reader will perhaps, even certainly, have noticed that experienced of the hypergeometric distribution is a series of Bernoulli trials (without replacement of course!).

	So we will cheat by using initially the property of linearity of the mean. We define for this purpose a new variable corresponding implicitly in fact to the experience of the hypergeometric distribution (a sequence of $k$ Bernoulli trials!):
	
	where $X_i$ is the success of obtaining at the $i$-th drawing a black ball (either $0$ or $1$). But, we know that for all $i$ the random variable $X_i$ follows a Bernoulli function for which we have proved in our study of the Bernoulli distribution that $\text{E}(X_i)=p$.
	
	Therefore, by the property of linearity of the mean we have (caution! here $p$ is not the number of balls, but the probability associated with an expected event!):
	
	In the Bernoulli trial, $p$ is the probability of obtaining the desired item or event (for reminder...). In the hypergeometric distribution what interests us is the probability of a black ball (which are in quantity $m$, therefore with $m'$ white balls) compared to the total amount of $n$ balls. And the ratio obviously gives us this probability. Thus, we have:
	
	where $k$ is the number of trials (do not confuse with the notation of the initial statement where it was by the variable $p!$). The mean gives then the average number of black balls in a drawing of $k$ balls among $n$, where $m$ are known as being black. The reader will have noticed that the mean of the hypergeometric distribution is the same as the binomial distribution!
	
	To determine the variance, we use the variance of the Bernoulli distribution and the following relation proved during the introduction of the mean and covariance at the beginning of this chapter:
	
	Recalling that we have $X=\displaystyle\sum_{i=1}^k X_i$, we get:
	
	However, for the Bernoulli law, we have:
	
	Then we first already get:
	
	Afterwards, we have easily:
	
	The calculation of $\text{E}(X_iX_j)$ requires a good understanding of probabilities (this will be a good refresh!).
	
	The mean $\text{E}(X_iX_j)$ is given (implicitly), as we know, by the weighted sum of the probabilities that two events occur at the same time. However, our events are binary: either it is a black ball ($1$) or it is a white ball ($0$). So all terms of the sum without two consecutive black balls consecutively will be null!
	
	The problem is then to calculate the probability of having two consecutive black balls and it is thus written:
	
	So we finally have:
	
	Therefore:
	
	Finally (using the result of Gauss series seen in the section of Sequences and Series page \pageref{gauss series}):
	
	where we have used the fact that:
	
	is composed of:
	
	terms that correspond to the number of ways there are to choose a pair $(i, j)$ with $i<j$.
	
	Because:
	
	We can write:
	
	In the specialized literature, we often find the variance written in the following way by noting the expected event $r$ and the non-expected event $s$:
	
	so with $l = n - k$. This last notation will be very useful further below during our study of the Mantel-Haenszel test (page \pageref{cochran mantel test}).
	Furthermore, we see that in:
	
	there is the standard deviation as the binomial distribution, at the difference of a factor that is noted:
	
	that we found often in statistics and is named "\NewTerm{finite population correction factor}\index{finite population correction factor}".

	Here is an example plot example of the distribution function and cumulative distribution for the Hypergeometric function of parameters $(n,p,m,k)=(10,6,5,k)$:
	\begin{figure}[H]
	\begin{center}
			\includegraphics{img/arithmetics/law_hypergeometric.jpg}
		\end{center}	
		\caption{Hypergeometric law $\mathcal{H}$ (mass and cumulative distribution function)}
	\end{figure}
	We will prove now that the Hypergeometric distribution tends to a binomial distribution since this property is used many times in different sections of this book (especially the section of Industrial Engineering page \pageref{quality control}).

	To do this, we decompose:
	
	We then get:
	
	For the second term:
	
	For $m\rightarrow +\infty$ (...) all the terms are of then of the order of $m$. Then we have:
	
	For the third term, a identical development to the previous one provides (for sure we need that also $n \rightarrow +\infty$ (...)):
	
	And for sure... we can debate therefore about $n-m$ when both terms tends to infinity...
	
	Ditto for the fourth term:
	
	In conclusion we have:
	
	We change the notation by writing $p$ (the number of individuals drawn) as being $N$. We get then:
	
	We make another change of notation by writing $b$ the black balls and $w$ the white balls. We get then:
	
	Finally, we note $p$ the proportion of black balls and $q$ that of white balls in the lot. We then get:
	
	We find out the binomial distribution! In practice, it is common to approximate the hypergeometric distribution with a binomial distribution when the ratio of the number of individuals from the total number of individuals is less than $10\%$ (that is to say when the sample is $10$ times smaller than the whole population). It follows that the hypergeometric distribution tends also (as we will show later) to a Normal distribution when the population tends to infinity and that the sample is small.

	In practice, Monte Carlo simulations with testing adjustments (see later in this chapter) have shown that the hypergeometric distribution could be approximated by a Normal distribution (very important case in contingency statistical tests that we will study further below in this section) if the following three conditions are met simultaneously:
	
	Thus graphically and approximately...:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,674); %set diagram left start at 0, and has height of 674
		
		%Straight Lines [id:da47843249344059235] 
		\draw    (90,413) -- (90,54) (86,356) -- (94,356)(86,299) -- (94,299)(86,242) -- (94,242)(86,185) -- (94,185)(86,128) -- (94,128)(86,71) -- (94,71) ;
		\draw [shift={(90,52)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da8219069354074391] 
		\draw    (82,407) -- (580,407) (179,403) -- (179,411)(276,403) -- (276,411)(373,403) -- (373,411)(470,403) -- (470,411)(567,403) -- (567,411) ;
		\draw [shift={(582,407)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da9837528012409398] 
		\draw [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ]   (345,65) -- (302.24,119.43) ;
		\draw [shift={(301,121)}, rotate = 308.16] [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da047189581735003205] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ]   (489,335) -- (451.08,275.69) ;
		\draw [shift={(450,274)}, rotate = 57.41] [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da48300715476791223] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (135,304) -- (182.66,251.48) ;
		\draw [shift={(184,250)}, rotate = 132.22] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Diamond [id:dp026032481894895376] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (205,138) -- (210,143) -- (205,148) -- (200,143) -- cycle ;
		%Shape: Diamond [id:dp2976493597318288] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (323,233) -- (328,238) -- (323,243) -- (318,238) -- cycle ;
		%Shape: Diamond [id:dp22596148816614736] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (535,237) -- (540,242) -- (535,247) -- (530,242) -- cycle ;
		%Shape: Diamond [id:dp4366865705019347] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (183,350) -- (188,355) -- (183,360) -- (178,355) -- cycle ;
		%Straight Lines [id:da7541468417327413] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (93,85) -- (558,85) ;
		%Straight Lines [id:da9788777837578451] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (93,99.27) -- (110,99.27) -- (558,99.27) ;
		%Straight Lines [id:da16205290111838377] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (93,113.54) -- (558,113.54) ;
		%Straight Lines [id:da26932448376014695] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (94,142.08) -- (159,142.08) ;
		%Straight Lines [id:da6844226011080028] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (492,142.08) -- (557,142.08) ;
		%Straight Lines [id:da40262642244750735] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (92,156.35) -- (161,156.35) ;
		%Straight Lines [id:da22281448903293843] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (492,156.35) -- (557,156.35) ;
		%Straight Lines [id:da8963483303345963] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (93,170.62) -- (164,170.62) ;
		%Straight Lines [id:da6538918684838575] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (488,170.62) -- (557,170.62) ;
		%Straight Lines [id:da9321007556481158] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (92,184.89) -- (168,184.89) ;
		%Straight Lines [id:da3565410314053854] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (480,184) -- (557,184) ;
		%Straight Lines [id:da6847518073752286] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (92,199.16) -- (173,199.16) ;
		%Straight Lines [id:da7290129251167321] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (477,199.16) -- (557,199.16) ;
		%Straight Lines [id:da599528799936442] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (92,213.43) -- (177,213.43) ;
		%Straight Lines [id:da16384526848656478] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (471,213.43) -- (557,213.43) ;
		%Straight Lines [id:da6711446482537646] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (92,227.7) -- (115,227.7) -- (182,227.7) ;
		%Straight Lines [id:da27390684799153164] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (469,227.7) -- (557,227.7) ;
		%Straight Lines [id:da810187906563703] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (91,241.97) -- (190,241.97) ;
		%Straight Lines [id:da5236653318641444] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (463,241.97) -- (557,241.97) ;
		%Straight Lines [id:da03327112600108961] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (93,256.24) -- (198,256.24) ;
		%Straight Lines [id:da7283817638168508] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (453,256.24) -- (517,256.24) -- (557,256.24) ;
		%Straight Lines [id:da27382602393384525] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (93,270.51) -- (207,270.51) ;
		%Straight Lines [id:da5285872426520581] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (93,284.78) -- (217,284.78) ;
		%Straight Lines [id:da036919418756277356] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (446,270.51) -- (519,270.51) -- (557,270.51) ;
		%Straight Lines [id:da6075383872818545] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (434,284.78) -- (557,284.78) ;
		%Straight Lines [id:da0451328720971953] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (422,299.05) -- (557,299.05) ;
		%Straight Lines [id:da581557269077595] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (92,313.32) -- (247,313.32) ;
		%Straight Lines [id:da7021916330599485] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (404,313.32) -- (557,313.32) ;
		%Straight Lines [id:da9100434863039237] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (93,327.59) -- (270,327.59) ;
		%Straight Lines [id:da06821639193257045] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (388,327.59) -- (557,327.59) ;
		%Straight Lines [id:da04429559883233525] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (93.5,356.13) -- (313,356.13) ;
		%Straight Lines [id:da5273141348858117] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (338,356.13) -- (557,356.13) ;
		%Straight Lines [id:da24156741849429264] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (94.5,384.67) -- (557,384.67) ;
		%Straight Lines [id:da7038968774795304] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (93.5,399) -- (556,399) ;
		%Straight Lines [id:da663082885563399] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (93.5,341.86) -- (290,341.86) ;
		%Straight Lines [id:da6206045339473067] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (367,342) -- (557,342) ;
		%Straight Lines [id:da7321027891395482] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (92.5,370.4) -- (555,370.4) ;
		%Straight Lines [id:da857835284922241] 
		\draw [color={rgb, 255:red, 235; green, 235; blue, 235 }  ,draw opacity=1 ] [dash pattern={on 4.5pt off 4.5pt}]  (93,299.05) -- (231,299.05) ;
		%Curve Lines [id:da12447159736965863] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (556,405) .. controls (321,378) and (175,356) .. (145,71) ;
		%Straight Lines [id:da3279376252404347] 
		\draw [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ][line width=2.25]    (92,127.81) -- (557,127.81) ;
		%Curve Lines [id:da15646770323409642] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (89,407) .. controls (332,374) and (468,363) .. (503,70) ;
		
		% Text Node
		\draw (73,33.4) node [anchor=north west][inner sep=0.75pt]    {$k$};
		% Text Node
		\draw (590,405.4) node [anchor=north west][inner sep=0.75pt]    {$m$};
		% Text Node
		\draw (54,63.4) node [anchor=north west][inner sep=0.75pt]    {$120$};
		% Text Node
		\draw (54,121) node [anchor=north west][inner sep=0.75pt]    {$100$};
		% Text Node
		\draw (62,179) node [anchor=north west][inner sep=0.75pt]    {$80$};
		% Text Node
		\draw (62,236) node [anchor=north west][inner sep=0.75pt]    {$60$};
		% Text Node
		\draw (62,290.72) node [anchor=north west][inner sep=0.75pt]    {$40$};
		% Text Node
		\draw (62,349) node [anchor=north west][inner sep=0.75pt]    {$20$};
		% Text Node
		\draw (69,395.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (86,418.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (165,416.9) node [anchor=north west][inner sep=0.75pt]    {$200$};
		% Text Node
		\draw (263,416.9) node [anchor=north west][inner sep=0.75pt]    {$400$};
		% Text Node
		\draw (360,416.9) node [anchor=north west][inner sep=0.75pt]    {$600$};
		% Text Node
		\draw (457,416.9) node [anchor=north west][inner sep=0.75pt]    {$800$};
		% Text Node
		\draw (547,416.9) node [anchor=north west][inner sep=0.75pt]    {$1000$};
		% Text Node
		\draw (332,34.4) node [anchor=north west][inner sep=0.75pt]    {$\textcolor[rgb]{0.49,0.83,0.13}{k< }\textcolor[rgb]{0.49,0.83,0.13}{\dfrac{n}{10}}$};
		% Text Node
		\draw (473,329.4) node [anchor=north west][inner sep=0.75pt]    {$\textcolor[rgb]{0.29,0.56,0.89}{k >9}\textcolor[rgb]{0.29,0.56,0.89}{\dfrac{m}{n-m}}$};
		% Text Node
		\draw (110,294.4) node [anchor=north west][inner sep=0.75pt]    {$\textcolor[rgb]{0.82,0.01,0.11}{k >9}\textcolor[rgb]{0.82,0.01,0.11}{\dfrac{n-m}{m}}$};
		% Text Node
		\draw (229,136) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{150pt}\setlength\topsep{0pt}
		\begin{center}
		acceptable combinations of $\displaystyle k$\\and $\displaystyle m$ for an approximation by\\a hypergeometric distribution\\for $\displaystyle n=1,000$
		\end{center}
		\end{minipage}};
		\end{tikzpicture}	
		\vspace*{3mm}
		\caption{Conditions of application of the approximation by a Normal distribution}
	\end{figure}
	Thus:
	
	
	
	
	\subsubsection{Multinomial Distribution}\label{multinomial distribution}
	The multinomial distribution (so named because it involves several times the binomial coefficient) is a law applicable to $n$ distinguishable events, each with a given probability, which occur one or more times and it is not necessarily ordered. This is a frequent case in marketing research and that will be useful to build the statistical McNemar test that we will study much later (see further below page \pageref{mcnemar test}) and also the Multinomial logistic regression (see further below page \pageref{multinomial logistic regression}). 
	
	The multinomial law is a generalization of the binomial law applicable for example to $n$ trials of a six-sided die but with the difference that probabilities are not equal!
	
	More technically, consider the space of events $\Omega=\left\lbrace 1,2,\ldots ,m \right\rbrace$ with the probabilities $P(\left\lbrace i\right\rbrace)=p_i,i=1,2,\ldots ,n$. We take $n$ times a given element of $\Omega$ with replacement with the probability $p_i,i=1,2,\ldots ,n$. We will search what is the probability of getting in a non-necessarily ordered way the event $1$, $k_1$ times, event $2$, $k_2$ times and this on a sequence of $n$ drawings.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
This is equivalent to the study of a sampling with replacement AND constraints on the occurrences! So without constraints we will see with an example that we fall back on a sampling with simple replacement.
	\end{tcolorbox}	
	
	We know that (\SeeChapter{see section Probabilities page \pageref{choice function}}) if we take a set of events with multiple outcomes, then the different combinations of sequences we can get taking $p$ selected elements among $n$ is given by:
	
	We have therefore:
	
	different ways to get $k_1$ times a given event. Thus an associated probability of (it's the binomial distribution):
	
	Now comes the particularity of the multinomial distribution!: there are no failures in contrast to the binomial distribution. Each "pseudo-failure" can be considered as a subset draw of $k_2$ items from the $n-k_1$ remaining elements.
	
	Thus the term:
	
	will be written on the whole experience if we consider a particular case limited to two types of events:
	
	so with:
	
	which gives us the number of different times to get $k_2$ times a second event because in the whole sequence of $n$ elements $k_1$ of them have already been taken so have now only $n-k_1$ remaining on which we can get the $k_2$ desired.
	
	These relations then show us that this is a situation where each event probability is considered as a binomial (hence its name ...).

So we have in the case of two sets of $t$-uples:
	
	and because:
	
	we get:
	
	and we see that the construction of this distribution therefore requires that:
	
	Thus, by induction we have the probability $\mathcal{M}$ we were looking for and named the "\NewTerm{Multinomial function}\index{multinomial function}" and using the previous relation given by:
	
	In other words if there are $m$ random variables, i.e. $X_i$, $i\in[1,n]$, where $X_i$ represents the number of occurrences of item $i$ in a choice of $n$ items, with entry $i$ in the vector of probabilities $\vec{p}$, $p_i$ giving the probability of drawing item $i$. The probability of selecting $k_i$ of item $k_1\ldots k_m$ of item $m$ is then given by $\mathcal{M}(\vec{k},\vec{p},m)$ above (if this explanation is not clear, see the example further below!).
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Note that if $m=2$  then the above relation becomes:
	
	But as $k_1+k_2=n$ then:
	
	and as $p_1+p_2=1$ we finally have:
	
	We recognize here the binomial distribution!
	\end{tcolorbox}
	
	In a spreadsheet software Microsoft Excel 11.8346, the term:
	
	is named "\NewTerm{multinomial coefficient}\index{multinomial coefficient}" and is available under the name of the function \texttt{MULTINOMIAL( )}. In the literature we also find this term sometimes under the following respective notations:
	
	Which gives then for the expression of the multinomial distribution:
	
	\begin{theorem}	
	We will show now that the multinomial distribution is effectively a probability distribution (because we could doubt ...). If this is the case, as we know it, the sum of the probabilities must be equal to $1$.
	\end{theorem}	
	\begin{dem}
		Let us recall that we proved the binomial theorem (\SeeChapter{see section Calculus page \pageref{binomial theorem}}):
		
		Now do a little bit of notation:
		
		and this time a change of variables:
		
		This last relation (which is a special case with two terms of the "\NewTerm{multinomial theorem}\index{multinomial theorem}") will be useful to us to show that the multinomial distribution is effectively a probability distribution. We also take the special case with two groups of drawing:
		
		which can is also written by the construction of the multinomial distribution:
		
		and therefore, the sum must be equal to the unit such that:
		
		To check this we use the multinomial theorem shown above:
		
		However, by construction of the multinomial sum of probabilities is unitary, we have effectively:
		
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
	\end{dem}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Examples:}\\\\
		E1. We launch an unbiased die $12$ times. What is the probability that all $6$ faces appear the same number of times (not necessarily consecutively!) that means twice for each:
		
		where we see well that $m$ is the number of success groups.\\
		
		E2. We launch an unbiased die $12$ times. What is the probability that a single unique face appears $12$ times (hence the "$1$" appears $12$ times, or the "$2$" or the "$3$", etc.):
		
		So we end up with this last example known a being a binomial distribution result.
	\end{tcolorbox}
	Let us recall that we have proved just earlier above (we just changes the notation a bit):
	
	And then we have obviously:
	
	The mean of the multinomial distribution is in fact a mean of vector. But as the mean of a vector is equal to the mean of its components we want to calculate $\text{E}(x_i)$. For this, without loss of generality, let us calculate $\text{E}(x_1)$. To do this let us recall that by definition of the mean:
	
	But for $x_1=0$ the corresponding terms is equal to zero. Then we can directly write:
	
	Now as the sum already acts on the term $P(X_1=x_1)$, for that latter we have only to sum on all terms excepted $x_1$ such that:
	
	Now we make the following change of variables $x_1^\prime=x_1-1$. Then:
	
	But the second summation is knows to us! Indeed, it's the cumulated probability on all $n-1$ (instead of the classical $n$). And that one is equal to $1$. Therefore we have:
	
	Therefore:
	
	Now let us see the variance (it's also a vector of variance in fact!). The idea is quite the same as for the mean. First we will use the Huygens relation:
	
	So only the first term $\text{E}(x_i^2)$ is unknown to us. Without loss of generality, and again by taking the definition of the expected mean, let us consider:
	
	Let us do now the change of variable:
	
	Therefore:
	
	So finally:
	

	\pagebreak
	\subsubsection{Poisson Distribution}\label{poisson distribution}
	
	For some rare events, the probability $p$ is very small and tends to zero. However, the average value $np$ tends to a fixed value as $n$ tends to infinity.
	
	We start from a binomial distribution with mean $\mu=np$ that we will assume finite when $n$ tends to infinity.
	
	The probability of $k$ successes after $n$ trials is (Binomial distribution):
	
	By writing $p=\dfrac{m}{n}$ (where $m$ will be temporarily the new notation for the mean according to $\mu=np$), this expression can be rewritten as:
	
	
	By grouping the terms, we can put the value $\mathcal{B}(n,k)$ under the form:
	
	We recognize that when $n$ tends to infinity, the second factor of the product has for limit $e^{-\mu}$ (\SeeChapter{see Functional Analysis page \pageref{Euler number}}). The third factor, since we are interested to the small values of $k$ (the probability of success is very small), its limit for $n$ tending to infinity is equal to $1$.
	
	This technique of passing to the limit is sometimes named in this context: the "\NewTerm{Poisson limit theorem}\index{Poisson limit theorem}".
	
	So we get the "\NewTerm{Poisson distribution}\index{Poisson distribution}" (or "\NewTerm{Poisson law}\index{Poisson law}"), also sometimes named the "\NewTerm{law of rare events}\index{law of rare events}" therefore given by:
	
	which can be obtained in Microsoft Excel 11.8346 with the function \texttt{POISSON( )} and in practice and the specialized literature is often indicated by the letter $u$.
	
	It is indeed a probability distribution since using the Taylor series (\SeeChapter{see section Sequences and Series page \pageref{usual maclaurin developments}}), we show that the sum of the cumulative probabilities is:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	We will frequently encounter this distribution in different sections of the book such as in the study of preventive maintenance in the section of Industrial Engineering page \pageref{preventive maintenance} or in the section of Quantitative Management for the study of queuing theory page \pageref{queueing theory} (the reader can refer to them for interesting and pragmatic examples) and finally in the field of life and non-life insurance.
	\end{tcolorbox}	
	Here is a plot example of the Poisson distribution and cumulative distribution function with parameter $\mu=3$:
	\begin{figure}[H]
	\begin{center}
		\includegraphics{img/arithmetics/law_poisson.jpg}
		\end{center}	
		\caption{Poisson law $\mathcal{P}$ (mass and cumulative distribution function)}
	\end{figure}
	This distribution is important because it describes many processes whose probability is small and constant. It is often used in the "queuing theory" (waiting time), acceptability and reliability test and statistical quality control. Among other things, it applies to processes such as the emission of light quanta by excited atoms, the number of red blood cells seen under the microscope, the number of incoming calls to a call center. The Poisson distribution is valid for many observations in nuclear and particle physics.
	
	The mean (average) of the Poisson distributions is (we use the Taylor series of the exponential):
	
	and gives the average number of times that you get the desired outcome.
	
	This result may seem confusing .... the mean is expressed by the mean?? Yes must simply not forget that it is given since the beginning by:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	For more details the reader may also refer to the subsection on "estimators" page \pageref{likelihood estimators}. 
	\end{tcolorbox}	
	The variance of the Poisson distribution function is itself given by (again we use the Taylor series):
	
	always with:
	
	The important fact for the Poisson distribution is that the variance that is equal to the mean is named the "\NewTerm{equidispersion property of the Poisson distribution}\index{equidispersion of the Poisson distribution}". This is a property often used in practice as an indicator to identify whether the data (with discrete support) are distributed according to a Poisson distribution.

	The theoretical laws of statistical distributions are determined assuming completion of an infinite number of measurements. It is obvious that we can only perform in practice a finite number $N$. Hence the utility to establish correspondence between the theoretical and experimental values. For the experimental values we obviously obtain only an approximation whose validity is, however, often accepted as sufficient.

	Now we will prove an important property of the Poisson distribution in the field of engineering and statistics that we name "stability by addition". The idea is as follows:
	
	
	Let $X$ and $Y$ be two independent random variables with Poisson distribution of respective parameters $\lambda$ and $\mu$. We want to ensure that their sum is also a Poisson distribution:
	
	See this:
	
	because the events are independent. Then we have:
	
	However, by applying the binomial theorem (\SeeChapter{see section Calculus page \pageref{binomial theorem}}):
	
	So in the end:
	
	and therefore the Poisson distribution is stable by addition. So any Poisson distribution where the parameter is verbatim indefinitely dividable into a finite or infinite sum of independent Poisson distributions.
	
	\newpage
	For the people who may not have access to a spreadsheet or statistical software here are some useful tables:
	
	\begin{center}
	POISSON CUMULATIVE DISTRIBUTION FUNCTION
	\end{center}

	\begin{center}
	\begin{tabular}{rr@{\ \,}r@{\ \,}r@{\ \,}r@{\ \,}r@{\ \,}r@{\ \,}r@{\ \,}r@{\ \,}r@{\ \,}r@{\ \,}r}
	$k/\mu$&
	\multicolumn{1}{c}{0.1}&\multicolumn{1}{c}{0.2}&
	\multicolumn{1}{c}{0.3}&\multicolumn{1}{c}{0.4}&
	\multicolumn{1}{c}{0.5}&\multicolumn{1}{c}{0.6}&
	\multicolumn{1}{c}{0.7}&\multicolumn{1}{c}{0.8}&
	\multicolumn{1}{c}{0.9}&\multicolumn{1}{c}{1.0}\\
	\ \\
	0&0.905&0.819&0.741&0.670&0.607&0.549&0.497&0.449&0.407&0.368\\
	1&0.995&0.982&0.963&0.938&0.910&0.878&0.844&0.809&0.772&0.736\\
	2&1.000&0.999&0.996&0.992&0.986&0.977&0.966&0.953&0.937&0.920\\
	3&1.000&1.000&1.000&0.999&0.998&0.997&0.994&0.991&0.987&0.981\\
	4&1.000&1.000&1.000&1.000&1.000&1.000&0.999&0.999&0.998&0.996\\
	5&1.000&1.000&1.000&1.000&1.000&1.000&1.000&1.000&1.000&0.999\\
	6&1.000&1.000&1.000&1.000&1.000&1.000&1.000&1.000&1.000&1.000\\
	\ \\
	$k/\mu$&
	\multicolumn{1}{c}{1.1}&\multicolumn{1}{c}{1.2}&
	\multicolumn{1}{c}{1.3}&\multicolumn{1}{c}{1.4}&
	\multicolumn{1}{c}{1.5}&\multicolumn{1}{c}{1.6}&
	\multicolumn{1}{c}{1.7}&\multicolumn{1}{c}{1.8}&
	\multicolumn{1}{c}{1.9}&\multicolumn{1}{c}{2.0}\\
	\ \\
	0&0.333&0.301&0.273&0.247&0.223&0.202&0.183&0.165&0.150&0.135\\
	1&0.699&0.663&0.627&0.592&0.558&0.525&0.493&0.463&0.434&0.406\\
	2&0.900&0.879&0.857&0.833&0.809&0.783&0.757&0.731&0.704&0.677\\
	3&0.974&0.966&0.957&0.946&0.934&0.921&0.907&0.891&0.875&0.857\\
	4&0.995&0.992&0.989&0.986&0.981&0.976&0.970&0.964&0.956&0.947\\
	5&0.999&0.998&0.998&0.997&0.996&0.994&0.992&0.990&0.987&0.983\\
	6&1.000&1.000&1.000&0.999&0.999&0.999&0.998&0.997&0.997&0.995\\
	7&1.000&1.000&1.000&1.000&1.000&1.000&1.000&0.999&0.999&0.999\\
	8&1.000&1.000&1.000&1.000&1.000&1.000&1.000&1.000&1.000&1.000\\
	\ \\
	$k/\mu$&
	\multicolumn{1}{c}{2.2}&\multicolumn{1}{c}{2.4}&
	\multicolumn{1}{c}{2.6}&\multicolumn{1}{c}{2.8}&
	\multicolumn{1}{c}{3.0}&\multicolumn{1}{c}{3.2}&
	\multicolumn{1}{c}{3.4}&\multicolumn{1}{c}{3.6}&
	\multicolumn{1}{c}{3.8}&\multicolumn{1}{c}{4.0}\\
	\ \\
	0&0.111&0.091&0.074&0.061&0.050&0.041&0.033&0.027&0.022&0.018\\
	1&0.355&0.308&0.267&0.231&0.199&0.171&0.147&0.126&0.107&0.092\\
	2&0.623&0.570&0.518&0.469&0.423&0.380&0.340&0.303&0.269&0.238\\
	3&0.819&0.779&0.736&0.692&0.647&0.603&0.558&0.515&0.473&0.433\\
	4&0.928&0.904&0.877&0.848&0.815&0.781&0.744&0.706&0.668&0.629\\
	5&0.975&0.964&0.951&0.935&0.916&0.895&0.871&0.844&0.816&0.785\\
	6&0.993&0.988&0.983&0.976&0.966&0.955&0.942&0.927&0.909&0.889\\
	7&0.998&0.997&0.995&0.992&0.988&0.983&0.977&0.969&0.960&0.949\\
	8&1.000&0.999&0.999&0.998&0.996&0.994&0.992&0.988&0.984&0.979\\
	9&1.000&1.000&1.000&0.999&0.999&0.998&0.997&0.996&0.994&0.992\\
	10&1.000&1.000&1.000&1.000&1.000&1.000&0.999&0.999&0.998&0.997\\
	11&1.000&1.000&1.000&1.000&1.000&1.000&1.000&1.000&0.999&0.999\\
	12&1.000&1.000&1.000&1.000&1.000&1.000&1.000&1.000&1.000&1.000\\
	\end{tabular}
	\end{center}
	
	\newpage
	
	\begin{center}
	\begin{tabular}{rr@{\ \,}r@{\ \,}r@{\ \,}r@{\ \,}r@{\ \,}r@{\ \,}r@{\ \,}r@{\ \,}r@{\ \,}r@{\ \,}r}
	$k/\mu$&
	\multicolumn{1}{c}{4.2}&\multicolumn{1}{c}{4.4}&
	\multicolumn{1}{c}{4.6}&\multicolumn{1}{c}{4.8}&
	\multicolumn{1}{c}{5.0}&\multicolumn{1}{c}{5.2}&
	\multicolumn{1}{c}{5.4}&\multicolumn{1}{c}{5.6}&
	\multicolumn{1}{c}{5.8}&\multicolumn{1}{c}{6.0}\\
	\ \\
	0&0.015&0.012&0.010&0.008&0.007&0.006&0.005&0.004&0.003&0.002\\
	1&0.078&0.066&0.056&0.048&0.040&0.034&0.029&0.024&0.021&0.017\\
	2&0.210&0.185&0.163&0.143&0.125&0.109&0.095&0.082&0.072&0.062\\
	3&0.395&0.359&0.326&0.294&0.265&0.238&0.213&0.191&0.170&0.151\\
	4&0.590&0.551&0.513&0.476&0.440&0.406&0.373&0.342&0.313&0.285\\
	5&0.753&0.720&0.686&0.651&0.616&0.581&0.546&0.512&0.478&0.446\\
	6&0.867&0.844&0.818&0.791&0.762&0.732&0.702&0.670&0.638&0.606\\
	7&0.936&0.921&0.905&0.887&0.867&0.845&0.822&0.797&0.771&0.744\\
	8&0.972&0.964&0.955&0.944&0.932&0.918&0.903&0.886&0.867&0.847\\
	9&0.989&0.985&0.980&0.975&0.968&0.960&0.951&0.941&0.929&0.916\\
	10&0.996&0.994&0.992&0.990&0.986&0.982&0.977&0.972&0.965&0.957\\
	11&0.999&0.998&0.997&0.996&0.995&0.993&0.990&0.988&0.984&0.980\\
	12&1.000&0.999&0.999&0.999&0.998&0.997&0.996&0.995&0.993&0.991\\
	13&1.000&1.000&1.000&1.000&0.999&0.999&0.999&0.998&0.997&0.996\\
	14&1.000&1.000&1.000&1.000&1.000&1.000&1.000&0.999&0.999&0.999\\
	15&1.000&1.000&1.000&1.000&1.000&1.000&1.000&1.000&1.000&0.999\\
	16&1.000&1.000&1.000&1.000&1.000&1.000&1.000&1.000&1.000&1.000\\
	\ \\
	$k/\mu$&
	\multicolumn{1}{c}{6.5}&\multicolumn{1}{c}{7.0}&
	\multicolumn{1}{c}{7.5}&\multicolumn{1}{c}{8.0}&
	\multicolumn{1}{c}{8.5}&\multicolumn{1}{c}{9.0}&
	\multicolumn{1}{c}{9.5}&\multicolumn{1}{c}{10.0}&
	\multicolumn{1}{c}{10.5}&\multicolumn{1}{c}{11.0}\\
	\ \\
	0&0.002&0.001&0.001&0.000&0.000&0.000&0.000&0.000&0.000&0.000\\
	1&0.011&0.007&0.005&0.003&0.002&0.001&0.001&0.000&0.000&0.000\\
	2&0.043&0.030&0.020&0.014&0.009&0.006&0.004&0.003&0.002&0.001\\
	3&0.112&0.082&0.059&0.042&0.030&0.021&0.015&0.010&0.007&0.005\\
	4&0.224&0.173&0.132&0.100&0.074&0.055&0.040&0.029&0.021&0.015\\
	5&0.369&0.301&0.241&0.191&0.150&0.116&0.089&0.067&0.050&0.038\\
	6&0.527&0.450&0.378&0.313&0.256&0.207&0.165&0.130&0.102&0.079\\
	7&0.673&0.599&0.525&0.453&0.386&0.324&0.269&0.220&0.179&0.143\\
	8&0.792&0.729&0.662&0.593&0.523&0.456&0.392&0.333&0.279&0.232\\
	9&0.877&0.830&0.776&0.717&0.653&0.587&0.522&0.458&0.397&0.341\\
	10&0.933&0.901&0.862&0.816&0.763&0.706&0.645&0.583&0.521&0.460\\
	11&0.966&0.947&0.921&0.888&0.849&0.803&0.752&0.697&0.639&0.579\\
	12&0.984&0.973&0.957&0.936&0.909&0.876&0.836&0.792&0.742&0.689\\
	13&0.993&0.987&0.978&0.966&0.949&0.926&0.898&0.864&0.825&0.781\\
	14&0.997&0.994&0.990&0.983&0.973&0.959&0.940&0.917&0.888&0.854\\
	15&0.999&0.998&0.995&0.992&0.986&0.978&0.967&0.951&0.932&0.907\\
	16&1.000&0.999&0.998&0.996&0.993&0.989&0.982&0.973&0.960&0.944\\
	17&1.000&1.000&0.999&0.998&0.997&0.995&0.991&0.986&0.978&0.968\\
	18&1.000&1.000&1.000&0.999&0.999&0.998&0.996&0.993&0.988&0.982\\
	19&1.000&1.000&1.000&1.000&0.999&0.999&0.998&0.997&0.994&0.991\\
	20&1.000&1.000&1.000&1.000&1.000&1.000&0.999&0.998&0.997&0.995\\
	21&1.000&1.000&1.000&1.000&1.000&1.000&1.000&0.999&0.999&0.998\\
	22&1.000&1.000&1.000&1.000&1.000&1.000&1.000&1.000&0.999&0.999\\
	23&1.000&1.000&1.000&1.000&1.000&1.000&1.000&1.000&1.000&1.000
	\end{tabular}
	\end{center}

	\subsubsection{Normal \& Gauss-Laplace Distribution}\label{gauss distribution}
	This is the most important function of distribution in the field of statistics following a famous theorem named the "central limit theorem", which as we will see later, permits to prove (among other things) that any sum of independent identically distributed random variables with a finite mean and variance converges to a Laplace-Gaussian function (Normal distribution).
	
	It is therefore very important to focus your attention on the developments that will be presented right now!

	Let start from a binomial function and make tender the number of trials $n$ to infinity. If $p$ is set from the beginning , the mean $\mu=np$ also tends to infinity, furthermore the standard deviation $\sigma=npq$ also tends to infinity.

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The case where $p$ varies and tends to $0$ while keeping fixed the mean has already been studied during the study of the Poisson function.
	\end{tcolorbox}
	
	If we want to calculate the limit of the binomial function, it will then be necessary to make a change of origin, which stabilizes the mean, to $0$ for example, and a change of unit change that stabilizes the standard deviation to $1$ for example.
	
	Let us now denote by $P_n(k)$ the binomial probability of $k$ success and let's see first how $P_n(k)$ vary with $k$ and calculate the difference:
	
	We conclude that $P_n(k)$ is an increasing function of $k$, as $np-k-q$ is positive (for $n, p$ and $q$ fixed). Too see it, just take a few values (of the right term of the equality) or to observe the graph of the binomial distribution function, remembering that:
	
	As $q<1$ it is therefore evident that the value of $k$ close to the mean $\mu=np$ of the binomial distribution is the maxima of $P_n(k)$.
	
	On the other hand the difference $P_n(k+1)-P_n(k)$ is the increase rate of the function $P_n(k)$. We can then write:
	
	as being the slope of the function.

	We now define a new random variable such that its average is zero (negligible variations) and its standard deviation equal to the unit (this will be a centered-reduced variable in other words). Then we have:
	
	Then we also have with this new random variable:
	
	Let us denote $F(x)$ as being $P_n(k)$ calculated using the new random variable with zero mean and unit standard deviation which we seek the expression when $n$ tends to infinity.
	
	Let's go back to:
	
	
	To simplify the study of this relation when $n$ tends to infinity and $k$ to mean $\mu=np$, multiply both sides by $npq/\sqrt{npq}$:
	
	We rewrite now the right-hand side of this equality. Then we get:
	
	And now let us rewrite the left term of the prior-previous relation. We then get:
	
	After passing to the limit for $n$ tending to infinity we have in a first time for the denominator of the second term of the prior-previous relation:
	
	the following simplification:
	
	Thus:
	
	and in a second time, taking into account that the considered values of $k$ are then in the neighborhood of the mean $np$, we get:
	
	and:
	
	Thus:
	
	and as:
	
	where $F(x)$ represents (awkwardly) for the few lines that follow, the density function as $n$ tends to infinity.
	
	Finally we have:
	
	This relation can also be rewritten rearranging the terms:
	
	and by integrating both sides of this equality we get (\SeeChapter{see section Differential and Integral Calculus page \pageref{integral calculus}}):
	
	The following function is a solution of the above equation:
	
	Effectively:
	
	The constant is determined by the condition that:
	
	which represents the sum of all probabilities, that mus be equal to $1$. We prove for this that we need to have:
	
	\begin{dem}
		We have:
		
		So let us focus on the last term of this equality\label{Gauss integral}\index{Gauss integral} named the "\NewTerm{Gauss integral}\index{Gauss integral}" or "\NewTerm{Gaussian integral}\index{Gaussian integral}" (in fact the case below is a special case of that integral) or also "\NewTerm{probability integral}\index{probability integral}". Thus:
		
		since $e^{-x^2}$ is an even function (\SeeChapter{see section Functional Analysis page \pageref{even function}}). Now we write the square of the integral as follows:
		
		and make a change of variable passing in polar coordinates, therefore we also use the Jacobian in these same coordinates (\SeeChapter{see section Differential and Integral Calculus page \pageref{jacobian}}):
		
		Therefore:
		
		By extension for $e^{-\frac{x^2}{2}}$ we have:
		
		Hence, the reader must keep in mind for further developments that:
		
		and that:
		
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
	\end{dem}
	We thus obtain the "\NewTerm{standard Normal distribution}\index{standard Normal distribution}" denoted as probability density function (with the capital letter $F$ that may unfortunately lead to confusion in the present development with the notation of the cumulative distribution function... we apologize...):
	
	which can be calculated in Microsoft Excel 11.8346 with the function \texttt{NORMSDIST( )}. Notice also that is some textbooks that latter relation is denoted by $\phi(x)$.
		
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Originally Gauss used the term "normal" (in the sense of "orthogonal") referring to the geometric interpretation of a system of linear (normal) equations from which the distribution bearing his name was derived. Pearson in 11920 (holocene calendar) credited himself for first using the term "normal curve" noting that: « \textit{while it avoids an international question of priority [of Gauss vs Laplace], has the disadvantage of leading people to believe that all other distributions are 'abnormal'.}»
	\end{tcolorbox}
		
		For information, a random variable following a Normal centered reduced distribution is by tradition often denoted by the letter $Z$ ("Zentriert" in German meaning "centered" in English) and its realization by a lowercase $z$. 
		
		Returning to non-normalized variables (i.e. non-standardized)\label{normal centered reduced variable} by remembering that:
		
		We get the "\NewTerm{Gauss-Laplace function}\index{Gauss-Laplace function}" (or "\NewTerm{Gauss-Laplace law}\index{Gauss-Laplace law}") or also named "\NewTerm{Normal distribution}\index{Normal distribution}"\index{Normal law} given in the form of probability density in this book by:
		 
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		So in general the reader must keep in mind that when a random variable $x$ follows a $\mathcal{N}(\mu,\sigma)$, then standardizing it by subtracting it's mean (i.e. a "\NewTerm{mean normalization}\index{mean normalization}") and dividing it by it's own standard deviation, make him follow a Normal centered reduced distribution $Z$ so that its realization will be denoted:
		
		\end{tcolorbox}

		The cumulative probability (distribution cumulative function) to have a certain value $k$ is obviously given in this special case by:
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.6\textwidth]{img/arithmetics/normal_distribution_cumulated_area.pdf}	
			\caption[Gaussian distribution cumulated area]{Gaussian distribution cumulated area (author: Izaak Neutelings)}
		\end{figure}
	
		Here is a plot example of the distribution and cumulative distribution function for the Normal law with the parameters example $(\mu,\sigma)=(0,1)$ that is therefore the standard centered reduced Normal distribution:
	\begin{figure}[H]
	\begin{center}
		\includegraphics{img/arithmetics/law_normal.jpg}
		\end{center}	
		\caption{Normal law $\mathcal{N}$ (mass and cumulative distribution function)}
	\end{figure}
	This law governs under very general conditions, and often encountered, in many random phenomena. It is also symmetrical with respect to the mean (this is important to remember!).
	
	We will now show that $\mu$ well represents the mean (or average) of $x$ (this is a bit silly but we can still check ...):
	
	We put:
	
	We then have:
	
	Let us now calculate the first integral:
	
	So we finally get:
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} The reader might find confusing at first that the parameter of a function is one of the results that we seek of this same function (as for the Poisson distribution). What bothers is to put in practice such a thing. In fact, everything will be more clear when we will discuss later in this chapter the concepts of "likelihood estimators".\\

	\textbf{R2.} It could be interesting to know for the reader that in practice (finance, quality assurance, etc.) it is common to have to calculate only mean only positive values of the random variable which is then naturally defined as "positive mean" and given by:
	
	We will see a practical example of this last relation in the section Economy during our study of the theoretical model of speculation of Louis Bachelier (page \pageref{theory of speculation}).
	\end{tcolorbox}
	Also we will prove now (...) that $\sigma$ is the standard deviation of $X$ (in other words to prove that $\text{V}(X)=\sigma^2$) and for this we recall that we had prove that (Huygens relation):
	
	We already know that at the level of the notations we have:
	
	then we first calculate $\text{E}(X^2)$:
	
	Let $y=(x-\mu)/\sqrt{2}\sigma$ that therefore leads us to:
	
	And we know that (already proved above):
	
	It remains to calculate therefore only the first integral. To do this, we proceed by integration by parts (\SeeChapter{see section Differential and Integral Calculus page \pageref{integration by parts}}):
	
	leads us to:
	
	Then we get:
	
	And finally:
	
	An additional signification of the standard deviation of Gauss-Laplace distribution is a measure of the width of the distribution as (this can be checked only with the aid of integration by using numerical methods) for any non-zero mean and standard deviation we have (thanks to John Cannin for the \LaTeX{} figure):
	\begin{figure}[H]
		\centering
		\pgfplotsset{compat=1.7}
		\pgfmathdeclarefunction{gauss}{2}{\pgfmathparse{1/(#2*sqrt(2*pi))*exp(-((x-#1)^2)/(2*#2^2))}%
		}
		\begin{tikzpicture}
		\begin{axis}[no markers, domain=0:10, samples=100,
		axis lines*=left, xlabel=Standard deviations, ylabel=Frequency,,
		height=6cm, width=10cm,
		xtick={-3, -2, -1, 0, 1, 2, 3}, ytick=\empty,
		enlargelimits=false, clip=false, axis on top,
		grid = major]
		\addplot [fill=cyan!20, draw=none, domain=-3:3] {gauss(0,1)} \closedcycle;
		\addplot [fill=orange!20, draw=none, domain=-3:-2] {gauss(0,1)} \closedcycle;
		\addplot [fill=orange!20, draw=none, domain=2:3] {gauss(0,1)} \closedcycle;
		\addplot [fill=blue!20, draw=none, domain=-2:-1] {gauss(0,1)} \closedcycle;
		\addplot [fill=blue!20, draw=none, domain=1:2] {gauss(0,1)} \closedcycle;
		\addplot[] coordinates {(-1,0.4) (1,0.4)};
		\addplot[] coordinates {(-2,0.3) (2,0.3)};
		\addplot[] coordinates {(-3,0.2) (3,0.2)};
		\node[coordinate, pin={68.2\%}] at (axis cs: 0, 0.4){};
		\node[coordinate, pin={95\%}] at (axis cs: 0, 0.3){};
		\node[coordinate, pin={99.7\%}] at (axis cs: 0, 0.2){};
		\node[coordinate, pin={34.1\%}] at (axis cs: -0.5, 0){};
		\node[coordinate, pin={34.1\%}] at (axis cs: 0.5, 0){};
		\node[coordinate, pin={13.6\%}] at (axis cs: 1.5, 0){};
		\node[coordinate, pin={13.6\%}] at (axis cs: -1.5, 0){};
		\node[coordinate, pin={2.1\%}] at (axis cs: 2.5, 0){};
		\node[coordinate, pin={2.1\%}] at (axis cs: -2.5, 0){};
		\end{axis}
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Sigma intervals for the Normal distribution}
	\end{figure}	
	The width of the interval has a great importance in the interpretation of uncertainties measurement. The presentation of a result like $\bar{N}\pm \sigma$ has for signification that the average value has about $68.3\%$ chance (probability) to lie between the limits of $\bar{N}- \sigma$ and $\bar{N}+ \sigma$ or has approximately $95.4\%$ to lie between the limits of $\bar{N}- 2\sigma$ and $\bar{N}+2\sigma$ etc.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		This concept is widely used in quality management in industrial business especially with the Six Sigma methodology (\SeeChapter{see Industrial Engineering page \pageref{six sigma}}) which requires a mastery of $6$ around each side of the mean (!) of the manufacturing (or anything else whose deviation is measured).
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|c|}{\cellcolor[gray]{0.75}\textbf{\begin{tabular}[c]{@{}c@{}}Sigma Quality\\ Level\end{tabular}}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{\begin{tabular}[c]{@{}c@{}}Insured non-defection \\ rate in \%\end{tabular}}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{\begin{tabular}[c]{@{}c@{}}Defection rate in \\ parts per million\end{tabular}}} \\ \hline
		$1\sigma$ & $68.26894$ & $317,311$ \\ \hline
		$2\sigma$ & $95.4499$ & $45,500$ \\ \hline
		$3\sigma$ & $99.73002$ & $2,700$ \\ \hline
		$4\sigma$ & $99.99366$ & $63.4$ \\ \hline
		$5\sigma$ & $99.999943$ & $0.57$ \\ \hline
		$6\sigma$ & $99.9999998$ & $0.002$ \\ \hline
		\end{tabular}
		\caption{Sigma quality level with defection / non-defection rate}
	\end{table}
	The second column of the table can easily be obtained with Maple 4.00b (or also with the spreadsheet software from Microsoft). For example for the first line:\\

	\texttt{>S:=evalf(int(1/sqrt(2*Pi)*exp(-x\string^ 2/2),x=-1..1));}\\

	and the first row of the third column:\\

	\texttt{>(1-S)*1E6;}\\

	If the Normal distribution was not centered, then we just would write for the second column:\\

	\texttt{>S:=evalf(int(1/sqrt(2*Pi)*exp(-(x-mu)\string^ 2/2),x=-1..1));}\\

	and so on for any deviation and mean we will then obtain exactly the same intervals!!!
	\end{tcolorbox}
	Let us now derive the skewness of the centered reduced Normal distribution define for recall as:
		
	And hence for the Normal centered reduced distribution $\mathcal{N}(0,1)$:
	
	Let now $Y=X-a$ be a random variable. Now note that due to symmetry of the Normal law $Y$ and $-Y$ have the same distribution. That implies:
	
	This implies that the only possible solution is $\text{E}[Y^3]=0$. Hence for the Normal Law:
	
	Let us now derive the kurtosis $\kappa_1$ and $\kappa_4$ for the centered reduced Normal distribution\label{kurtosis normal distribution}:
	
	For this purpose we use the following usual primitive (\SeeChapter{see section Differential and Integral Calculus page \pageref{usual primitives}}):
	
	Therefore using also the Gauss integral:
	
	Therefore, the kurtosis is:
	
	Hence the normalized kurtosis:
	
	The Gauss-Laplace distribution is also not only a tool for data analysis but also for data generation. Indeed, this distribution is one of the largest used in the world of multinationals that use statistical tools for risk management, project management and simulation where a large number of random variables are to be controlled. The best examples of applications use the softwares CrystalBall or Palisade @Risk (this last one being my favorite...).
	
	In this context of application (project management), it is also very common to use the sum (task duration) or the product of random variables (customer uncertainty) following Gauss-Laplace distributions. We will see now how to calculate this:

	\paragraph{Sum of two random Normal variables}\label{sum of two random normal variables}\mbox{}\\\\
	Let $X, Y$ be two independent random variables. Suppose that $X$ follows the distribution $\mathcal{N}(\mu_1,\sigma_1)$ and that $Y$ follows the distribution $\mathcal{N}(\mu_2,\sigma_2)$. Then the random variable $Z=X+Y$ has a density equal to the convolution product of $f_x$ and $f_y$ (\SeeChapter{see section Functional Analysis page \pageref{convolution}}). That is to say:
	
	which is equivalent to the joint product (\SeeChapter{see section Probabilities page \pageref{joint probability}}) of the probabilities of occurrence of the two continuous variables (remember the same kind of calculation in discrete form!).
	
	To simplify the expression, make the change of variable $t=x-\mu_1$ and let us write: 
	
	As:
	
	we get after a hard to guess rearrangement trick:
	
	We write:
	
	Then:
	
	Knowing that (already proved earlier above!):
	
	and:
	
	our relation becomes:
	
	We recognize the expression of the Gauss-Laplace distribution (Normal law) of mean $\mu_1+\mu_2$ and standard deviation $\sigma=\sqrt{\sigma_1^2+\sigma_2^2}$.
	
	Therefore, $X+Y$ follows the distribution as written by physicists (both argument have same units):
	
	and as noted by most mathematicians and statisticians:
	
	The fact that the sum of two Normal random variables always give also a Normal distribution is what we name in statistics "\NewTerm{stability of the sum}\index{stability of the sum (statistics)}\label{stability of the sum in statistics}" of the Gauss-Laplace distribution (Normal law). We will find such properties for other distribution that will be discussed later.

	So as well as for Poisson random variables, any Normal random variable whose parameters are known is verbatim indefinitely divisible into a finite or infinite number of independent Normal variables that are summed as:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The families of stable distribution by the sum is an important field of study in physics, finance and statistics named "\NewTerm{Levy alpha-stable distribution}\index{Levy alpha-stable}". If time permits, I will present the details of this extremely important study in this chapter.
	\end{tcolorbox}
	Notice that from what we have derive above, we can conclude that if we have a random variable $X$ that is Normally distributed and we want to keep it's mean but we want to increase it's standard deviation by a constant $c$, then we just have to add to itself the quantity $c\cdot(X-\mu)/\sigma$.
	
	\paragraph{Product of two random Normal variables}\mbox{}\\\\
	Let $X, Y$ be two real independent random variables. We denote by $f_X$ and $f_Y$ the corresponding densities and we seek to determine the density of the variable $Z=XY$ (very important case, particularly in engineering).
	
	Let $f$ denote the density function of the pair $(X, Y)$. Since $X, Y$ are independent (\SeeChapter{see section Probabilities page \pageref{joint probability}}):
	
	The distribution function of $Z$ is:
	
	where $D=\left\lbrace(x,y) \vert xy<z\right\rbrace$. 
	
	$D$ can be rewritten as a disjoint union (we do this for anticipating in the future change of variables a division by zero):
	
	with:
	
	We have:
	
	The last integral is equal to zero because $D_3$ is of measurement (thickness) zero for the integral along $x$.
	
	We then perform the following change of variable:
	
	The Jacobian of the transformation (\SeeChapter{see section Differential and Differential Calculus page \pageref{jacobian}}) is:
	
	Thus:
	
	Let $f_Z$ be the density of the variable $Z$. By definition:
	
	On the other hand:
	
	as we have seen. Therefore:
	
	What is a bit sad is that in the case of a Gauss-Laplace distribution (Normal distribution), this integral can only be easily calculated numerically ... it is then necessary to use Monte Carlo integration type methods (\SeeChapter{see section Numerical Methods page \pageref{monte carlo integration}}).

	However according to some research done on the Internet, but without certainty, this integral may be calculated and give a new distribution named "\NewTerm{Bessel distribution}\index{Bessel distribution}".
	
	\paragraph{Bivariate Normal Distribution}\label{bivariate normal distribution}\mbox{}\\\\
	If two Normal distributed random variables are independent, we know that their joint probability is equal to the product of their probabilities. So we have:
	
	Now comes an approach that we will often find in the follow developments: to generalize simple algebra models, you have to think in a Linear Algebra way! Therefore we are left with two vectors involving a scalar product:
	
	But we can do even better because for the moment there is no added value to this notation! Effectively a subtle idea is to involve the determinant of a matrix (\SeeChapter{see section Linear Algebra  page \pageref{determinant}}) and the inverse of this same matrix in the previous relation:
	
	We thus find a particular case of the variance-covariance matrix. In the field of the bivariate Normal distribution is it is customary to write this last relation in the following form:
	
	If we make a plot of this function we get:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/law_bivariate_normal_perspective.jpg}
		\caption{Plot of the bivariate Normal function in MATLAB™}
	\end{figure}
	or in high definition with TikZ:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}
	    \begin{axis}[scale=1.5]
	    \def\centerx{0}
		\def\centery{0}
	    \addplot3[surf,domain=-3.5:3.5,domain y=-3.5:3.5] 
	        {1/(2*pi)*exp(-0.5*( (x-\centerx)^2 + (y-\centery)^2) )};
	    \node[circle,inner sep=1pt,fill=blue,pin=90:$\vec\mu$] 
	        at (axis cs:\centerx,\centery,0.16) {};
	    \end{axis}
		\end{tikzpicture}
	\end{figure}
	
	or another one (not with the same values) with corresponding projections:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/arithmetics/normal_bivariate_projection.jpg}
		\caption{Plot of the bivariate Normal function with pgfplots}
	\end{figure}
	Now consider the important case in engineering, astronomy and quantum physics by returning to the following notation:
	
	and by focusing on to the iso-lines such that for any pair of values of the two random variables, we have:
	
	By doing some very basic algebraic manipulations, we get:
	
	Thus:
	
	and we get:
	
	We recognize here the analytical equation of an ellipse (\SeeChapter{see section Analytical Geometric page \pageref{analytical expression ellipse}})!
	
	A plot of iso-lines with $\mu=\begin{pmatrix}3\\2\end{pmatrix},\Sigma=\begin{bmatrix}25 & 0\\0 & 9\end{bmatrix}$ give us:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/law_bivariate_normal_isolines.jpg}
		\caption{Plot of the iso-lines of the bivariate Normal function (non-correlated case)}
	\end{figure}
	But now recall that when we got:
	
	the variance-covariance matrix was zero everywhere except on the diagonal, implying verbatim the independence of the two random variables. We can obviously guess that the generalization is that the variance-covariance matrix is non-zero in the diagonal and then the two random variables are correlated. Consequently, the iso-lines become with values such as $\mu=\begin{pmatrix}3\\2\end{pmatrix},\Sigma=\begin{bmatrix}10 & 5\\5 & 5\end{bmatrix}$:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/law_bivariate_normal_isolines_correlation.jpg}
		\caption{Plot of the iso-lines of the bivariate Normal function (correlated case)}
	\end{figure}
	So the correlation rotates the axis of the ellipses! Note that we have therefore:
	
	and thus verbatim:
	
	Recall that we saw during our study of the correlation coefficient that (well... normally... the $R$ notation for the correlation is used only if the variances are estimated but as it is the most common notation in practice we will still us it...):
	
	Thus:
	
	and the exponent of the exponential of the bivariate Normal takes a form that we can find very often in the literature:
	
	Note that if the random variables are centered reduced, then we have:
	
	and thus the exponent of the exponential of the bivariate Normal distribution becomes:
	
	Thus, the density function of the bivariate Normal centered reduced distribution will be written:
	
	Thus, we can see that a bivariate Normal reduced centered distribution function normal can be constructed by the multiplication of two Normal centered and reduced distributions themselves multiplied by a term that depends mainly on the correlation parameter. The latter term includes the nature of the dependence of the two random variables and provides the link between the marginal distributions (both Normal centered and reduced) to obtain the joint bivariate Normal distribution.

	If necessary (this can be very useful in practice), here is the Maple 4.00b code to plot a bivariate Normal function (taking the last example) even if it is also quite simple to do with a spreadsheet software like Microsoft Excel:

	\texttt{>f:=(x,y,rho,mu1,mu2,sigma1,sigma2)->(1/(2*Pi*sqrt(sigma1*sigma2*(1-rho\string^2))))}\\
	\texttt{*exp((-1/(2*(1-rho\string^2)))*(((x-mu1)/sqrt(sigma1))\string^2+((y-mu2)/sqrt(sigma2))\string^2}\\
	\texttt{-2*rho*((x-mu1)/sqrt(sigma1))*((y-mu2)/sqrt(sigma2))));}\\

	\texttt{>plot3d(f(x,y,5/sqrt(10*5),3,2,10,5),x=-4..10,y=-4..9,grid=[40,40]);}

	and for the plot with the iso-lines:

	\texttt{>with(plots):}\\
	\texttt{>contourplot(f(x,y,5/sqrt(10*5),3,2,10,5),x=-4..10,y=-4..9,grid=[40,40]);}

	and we can check that it is a probability density function by writing:

	\texttt{>int(int(f(x,y,5/sqrt(10*5),3,2,10,5),x=-infinity...+infinity)}\\
	\texttt{,y=-infinity...+infinity);}

	or calculate the cumulative probability between two intervals:

	\texttt{>evalf(int(int(f(x,y,5/sqrt(10*5),3,2,10,5),x=-3...+4),y=-5...+2));}
	
	Notice that it is only in 12014 (holocene calendar) that the "\NewTerm{Gaussian correlation conjecture}\index{Gaussian correlation conjecture}" (also named "\NewTerm{Gaussian correlation inequality}\index{Gaussian correlation inequality}") was proven by Thomas Royen, a retired German professor of statistics. This conjecture can be summarized with the following figure:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/gaussian_conjecture.jpg}
	\end{figure}
	We will not provide the proof here as I never needed it for my business or also my customers business but it is quite funny that to know that an inequality that seems quite obvious needed so many years before someone found the proof.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	On July 17, 12014 (holocene calendar), a few years after his retirement, when brushing his teeth, Royen had a flash of insight: how to use the Laplace transform of the multivariate gamma distribution to achieve a relatively simple proof for the Gaussian correlation inequality, a conjecture on the intersection of geometry, probability theory and statistics, formulated after work by Dunnett and Sobel (11955 according to holocene calendar) and the American statistician Olive Jean Dunn (11958 according to holocene calendar) that had remained unsolved since then. He sent a copy of his proof to Donald Richards, an acquainted American mathematician, who worked on a proof of the GCI for 30 years. Richards immediately saw the validity of Royen's proof and subsequently helped him to transform the mathematical formulas into \LaTeX{} (hence the importance to lean \LaTeX{} when we are engineers or scientists!). When Royen contacted other reputed mathematicians, though, they didn't bother to investigate his proof, because Royen was relatively unknown, and these mathematicians therefore estimated the chance that Royen's proof would be false as very high (especially also by sending a Microsoft Word file for peer-review your chances to be read are quite low).
	\end{tcolorbox}	
	
	\paragraph{Multivariate Normal distribution}\label{normal multivariate distribution}\mbox{}\\\\
	The bivariate Normal distribution is obviously a special case of the multivariate one. We will see now some important properties of the multivariate Normal distribution that we will need later for various statistical tests and methods.
	
	Let us consider the following multivariate Normal distribution $P(\vec{X})=\mathcal{N}(\vec{\mu}_X,\Sigma_X)$. We want to prove the following multivariate affine transformation that will be useful to us for the study of Factor Analysis later:
	
	where:
	
	\begin{dem}
	Let us see first for the mean:
	
	and now for the variance-covariance matrix:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Notice some special cases:
	\begin{itemize}
		\item If $A=\mathds{1}$, then:
		
		
		\item If $\vec{b}=\vec{0}$, then:
		
		
		\item If $P(\vec{X})=\mathcal{N}\left([\mu_1,\ldots,\mu_n]^T,\text{diag}[\sigma_1^2,\ldots,\sigma_2^2]\right)$, then:
		
	\end{itemize}
	
	Now let us move to another property of multivariate Normal distribution.
	
	For this, let us recall first that the product of two univariate gaussian probability density functions is given by:
	
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Do not confuse the product of the Normal distributed variables, and the product of their respective probability density functions!!!!\\
	
	Indeed, the product of the probability density functions of two random variables $X$ and $Y$ will give the joint distribution of the vector-valued random variable $(X,Y)$ in the case that $X$ and $Y$ are independent. Therefore, if $X$ and $Y$ are normally distributed independent random variables, the product of their respective probability density function is bivariate normal with zero correlation (but the result isn't technically speaking a probability density function!).\\

	On the other hand, even in the case that $X$ and $Y$ are independent and identically standard Normal random variables, their product is not itself Normal (it's easy to check in the case where we put $Y=X$ as $X^2$ cannot be negative, hence it's not Normally distributed!).
	\end{tcolorbox}
	Now let us consider for further needs the following multivariate gaussian variable notation:
	
	where:
	
	Now for the need of our study of Factor Analysis, let us consider a same random variable $\vec{x}$ inject into two multivariate Normal distribution with different mean and variance-covariance matrix:
	
	Here we have defined:
	
	and:
	
	i.e.:
	
	We see then that the product of two multivariate gaussians with same random vector is also a scaled Gaussian:
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/arithmetics/product_of_gaussians.jpg}
	\end{figure} 
	
	\subparagraph{Marginal and conditional distributions of multivariate Normal distributions}\label{marginal and conditional distributions of multivariate Normal distributions}\mbox{}\\\\
	For our study later of Factor Analysis we will also need what follows!
	
	A random vector ${\vec x}=[{\vec x}_1^T,{\vec x}_2^T]^T$ with a Normal distribution ${\mathcal N}({\vec x},{\vec \mu},{\Sigma})$ can be partitioned into two subvectors ${\vec x}_1$ and ${\vec x}_2$:
	
	where:
	
	Given this Normal joint distribution of ${\vec x}_1$ and ${\vec x}_2$, we can find the Normal conditional distribution of ${\vec x}_i$ given ${\vec x}_j$:
	
	in terms of the mean vector and the covariance matrix:
	
	
	\begin{dem}
	The joint distribution of $\vec{x}$ is defined as:
	
	Here we have defined:
	
	where we put:
	
	According to the property of the inverse of a partitioned symmetric matrix (\SeeChapter{see section Linear Algebra page \pageref{inverse of a partitioned symmetric matrix}}), we have:
	
	Substituting the second expression for ${\Sigma}^{11}$, the first expression for ${\Sigma}^{22}$, and ${\Sigma}^{12}$ into $Q({\vec x}_1,{\vec x}_2)$ we get:
	
	The last equal sign is due to the following equations for any vectors $\vec{u}$ and $\vec{v}$ and a symmetric matrix $A=A^T$:
	\begin{equation}
		\begin{aligned}
		 \vec{u}^{T} A \vec{u}-2 \vec{u}^{T} A \vec{v}+\vec{v}^{T} A \vec{v}&=\vec{u}^{T} A \vec{u}-\vec{u}^{T} A \vec{v}-\vec{u}^{T} A \vec{v}+\vec{v}^{T} A \vec{v} \\
		&= \vec{u}^{T} A(\vec{u}-\vec{v})-(\vec{u}-\vec{v})^{T} A \vec{v}=\vec{u}^{T} A(\vec{u}-\vec{v})-\vec{v}^{T} A(\vec{u}-\vec{v}) \\
		&=(\vec{u}-\vec{v})^{T} A(\vec{u}-\vec{v})=(\vec{v}-\vec{u})^{T} A(\vec{v}-\vec{u})
		\end{aligned}
	\end{equation}
	To simplify the notations, let us now put:
	\begin{equation}
		\left\{\begin{array}{l}
		{\vec{b}=\vec{\mu}_{2}+{\Sigma}_{21} {\Sigma}_{11}^{-1}\left(\vec{x}_{1}-\vec{\mu}_{1}\right)} \\
		{{A}={\Sigma}_{22}-{\Sigma}_{21} {\Sigma}_{11}^{-1} {\Sigma}_{12}}
		\end{array}\right.
	\end{equation}
	and also:
	\begin{equation}
		\left\{\begin{aligned}
		Q_{1}\left(\vec{x}_{1}\right) &=\left(\vec{x}_{1}-\vec{\mu}_{1}\right)^{T} {\Sigma}_{11}^{-1}\left(\vec{x}_{1}-\vec{\mu}_{1}\right) \\
		Q_{2}\left(\vec{x}_{1}, \vec{x}_{2}\right) &=\left[\left(\vec{x}_{2}-\vec{\mu}_{2}\right)-{\Sigma}_{21} {\Sigma}_{11}^{-1}\left(\vec{x}_{1}-\vec{\mu}_{1}\right)\right]^{T}\left({\Sigma}_{22}-{\Sigma}_{21} {\Sigma}_{11}^{-1} {\Sigma}_{12}\right)^{-1}\left[\left(\vec{x}_{2}-\vec{\mu}_{2}\right)-{\Sigma}_{21} {\Sigma}_{11}^{-1}\left(\vec{x}_{1}-\vec{\mu}_{1}\right)\right] \\
		&=\left(\vec{x}_{2}-\vec{b}\right)^{T} {A}^{-1}\left(\vec{x}_{2}-\vec{b}\right)
		\end{aligned}\right.
	\end{equation}
	Then the expression above for $Q({\vec x}_1,{\vec x}_2)$ can be written as:
	\begin{equation}
		Q\left(\vec{x}_{1}, \vec{x}_{2}\right)=Q_{1}\left(\vec{x}_{1}\right)+Q_{2}\left(\vec{x}_{1}, \vec{x}_{2}\right)
	\end{equation}
	and the joint distribution can be written as:
	\begin{equation}
		\begin{aligned}
		P(\vec{x}) &=P\left(\vec{x}_{1}, \vec{x}_{2}\right)=\frac{1}{(2 \pi)^{n / 2}|\Sigma|^{1 / 2}} \exp \left[-\frac{1}{2} Q\left(\vec{x}_{1}, \vec{x}_{2}\right)\right] \\
		&=\frac{1}{(2 \pi)^{n / 2}\left|{\Sigma}_{11}\right|^{1 / 2}\left|{\Sigma}_{22}-{\Sigma}_{21} {\Sigma}_{11}^{-1} {\Sigma}_{12}\right|^{1 / 2}} \exp \left[-\frac{1}{2} Q_{1}\left(\vec{x}_{1}\right)\right] \exp \left[-\frac{1}{2} Q_{2}\left(\vec{x}_{1}, \vec{x}_{2}\right)\right] \\
		&=\frac{1}{(2 \pi)^{p / 2}\left|{\Sigma}_{11}\right|^{1 / 2}} \exp \left[-\frac{1}{2}\left(\vec{x}_{1}-\vec{\mu}_{1}\right)^{T} {\Sigma}_{11}^{-1}\left(\vec{x}_{1}-\vec{\mu}_{1}\right)\right] \frac{1}{(2 \pi)^{q / 2}|{A}|^{1 / 2}} \exp \left[-\frac{1}{2}\left(\vec{x}_{2}-\vec{b}\right)^{T} {A}^{-1}\left(\vec{x}_{2}-\vec{b}\right)\right] \\
		&=\mathcal{N}\left(\vec{x}_{1}, \vec{\mu}_{1}, {\Sigma}_{11}\right) \mathcal{N}\left(\vec{x}_{2}, \vec{b}, {A}\right)
		\end{aligned}
	\end{equation}
	where we have used the expression of the determinant of a partitioned matrix (\SeeChapter{see section Linear Algebra page \pageref{determinant of a partitioned symmetric matrix}}):
	
	The marginal distribution of $\vec{x}_1$ is then:
	
	and the conditional distribution of ${\vec x}_2$ given ${\vec x}_1$ is then:
	
	with for recall:
	\begin{equation}
		\left\{\begin{array}{l}
		\vec{b}=\vec{\mu}_{2|1}=\vec{\mu}_2+\Sigma_{21}\Sigma_{11}^{-1}(\vec{x}_1-\vec{\mu}_1)\\
		A=\Sigma_{2|1}=\Sigma_{22}-\Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12}
		\end{array}\right.
	\end{equation}
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem} 
	Then we can deduce two famous results for the special bivariate case (using $\sigma_{12}=\text{cov}(X_1,X_2)=R_{X_1,X_2}\sigma_{11}\sigma_{22}$), ie (with the traditional notation in the field of statistics):
	\begin{equation}
		\left\{\begin{array}{l}
		\text{E}(Y|X)=\vec{\mu}_{2|1}=\vec{\mu}_2+\Sigma_{21}\Sigma_{11}^{-1}(\vec{x}_1-\vec{\mu}_1)=\mu_Y+\rho\dfrac{\sigma_Y}{\sigma_X}(x-\mu_X)\\
		\text{V}(Y|X)=\Sigma_{2|1}=\Sigma_{22}-\Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12}=\sigma^2_Y-\sigma^2_Y\rho^2=\sigma^2_Y(1-\rho^2)
		\end{array}\right.
	\end{equation}
	
	\paragraph{Normal Reduced Centered Distribution}\label{normal reduced centered distribution}\mbox{}\\\\
	The Gauss-Laplace distribution is not tabulated as we must then have so many numerical tables as possible values for the mean $\mu$ and standard deviation $\sigma$ (which are the parameters of the function as we have seen it).

	Therefore, by a change of variable, the Normal distribution becomes the Normal reduced centered distribution more often named the "\NewTerm{standard Normal distribution}\index{standard Normal distribution}" where:
	\begin{enumerate}
		\item "Centered" refers to subtracting the mean $\mu$ to the measures (thus the distribution function is symmetric to the vertical axis).
		
		\item "Reduced" refers to the division by the standard deviation $\sigma$ (thus the distribution function has a unit variance).
	\end{enumerate}
	By this change of variable, the variable $k$ is replaced by the reduced centered random variable:
	
	If the variable $k$ has for mean $\mu$ and standard deviation $\sigma$ then the variable $k^{*}$ has a mean of $0$ and standard deviation of $1$ (this last variable is usually denoted by the letter $Z$).
	
	Thus the relation:
	
	is therefore written (trivially) more simply:
	
	which is just the explicit expression of the reduced centered Normal distribution ("standard Normal") often denoted $\mathcal{N}(0,1)$ which we will find very often in the sections of physics, finance, quantitative management and engineering!
	
	The derivation of the inverse of the standard normal PDF is simple (it is useful to know it as not implemented by default in all statistical softwares):
	
	
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Calculate the integral of the previous relation for an interval can not be done accurately formally speaking. One possible and simple idea is then to express the exponential in a Taylor series and then be integrated term by term of the series (making sure to take enough terms for convergence!).
	\end{tcolorbox}	
	
	\paragraph{Henry's Line}\label{Henri line}\mbox{}\\\\
	Often in business it is the Gauss-Laplace (Normal) distribution that is analysed but common and easily accessible software like Microsoft Excel are unable to verify that the measured data follow a Normal distribution when we do the frequency analysis (there are no default integrated tool allowing users to check this assumption) and we do not have the original ungrouped data.

	The trick then is then to use the reduced centered variable that is build as we have seen above with the following relation:
	
	The idea of the Henry's Line is then to use the linear relation between $k$ and $k^*$ given by the equation of the line:
	
	and which can be plotted to determine the mean and the standard deviation of the Normal distribution.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
\textbf{{\Large \ding{45}}Example:}\\\\
	Suppose we have the following frequencies analysis of $10,000$ receipts in a supermarket:
	\begin{table}[H]
		\centering
		\renewcommand{\arraystretch}{1.2}
		\small
		\begin{tabular}{cccc}\hline
		\rowcolor[gray]{0.75}\textbf{Price} &  \textbf{Number} & \textbf{Cumulated number} & \textbf{Relative frequencies}  \\[-3pt]
		\rowcolor[gray]{0.75}\textbf{of receipts} & \textbf{of receipts} & \textbf{of receipts} & \textbf{of receipts} \\ \hline % ne pas enlever les espaces vides entre les lignes!!!
		$[0,50[$ & $668$ & $668$ & $0.068$ \\

		$[50,100[$ & $919$ & $1,587$ & $0.1587$ \\

		$[100,150[$ & $1,498$ & $3,085$ & $0.3085$ \\

		$[150,200[$ & $1,915$ & $5,000$ & $0.5000$ \\

		$[200,250[$ & $1,915$ & $6,915$ & $0.6915$\\

		$[250,300[$ & $1,498$ & $8,413$ & $0.8413$\\

		$[300,350[$ & $919$ & $9,332$ & $0.9332$ \\

		$[350,400[$ & $440$ & $9,772$ & $0.9772$ \\

		$[400$ and $+$ & $228$ & $10,000$ & $1$ \\ \hline
		\end{tabular}
		\caption[]{Supermarket receipt amount distribution}
	\end{table}
	If we now plot this in Microsoft Excel 11.8346 we get:
	\begin{figure}[H]
		\centering
		\fbox{\includegraphics{img/arithmetics/distribution_example_henry_law.jpg}}
		\caption[]{Distribution of receipts amount}
	\end{figure}
	What looks terribly like a Normal distribution, thus the authorization, without too much risk to use in this example the technique of Henry's line.\\

	But what can we do now? Well... now that we know the cumulative frequency, it remains for us to calculate each $k^*$ using numerical tables or the \texttt{NORMSINV( )} function of Microsoft Excel 11.8346 (remember that formal integration of the Gaussian function is not easy...).\\

	This will give us the values of the standard Normal distribution $\mathcal{N}(0,1)$ of these respective cumulative frequencies (cumulative distribution function). So we get (we leave to the reader to take its statistic table or open its favorite software...):
	\begin{table}[H]
		\centering
		\renewcommand{\arraystretch}{1.2}
		\small
		\begin{tabular}{cccc}\hline
		\rowcolor[gray]{0.75}\textbf{Upper limit} &  \textbf{Cumulated} &\textbf{ Correspondence for} $k^*$  \\[-3pt]
		\rowcolor[gray]{0.75}\textbf{of the interval} & \textbf{relative frequencies} & \textbf{of} $\mathcal{N}(0,1)$ \\ \hline % ne pas enlever les espaces vides entre les lignes!!!
		$50$ & $0.068$ & $-1.5$ \\

		$100$ & $0.1587$ & $-1$ \\

		$150$ & $0.3085$ & $-0.5$ \\

		$200$ & $0.5000$ & $0$ \\

		$250$ & $0.6915$ & $0.5$ \\

		$300$ & $0.8413$ & $1$\\

		$350$ & $0.9332$ & $1.5$ \\

		$400$ & $0.9772$ & $2$ \\

		- & 1 & - \\ \hline
		\end{tabular}
		\caption[]{Cumulative relative frequencies to the Henry's line}
	\end{table}
	Note that in the type of table above, in Microsoft Excel, the null and unit cumulative frequencies will generated some errors. You should then play a little bit...

	As we specified earlier, we have under discrete form:
	
	So graphically in Microsoft Excel 11.8346 we can thanks to our table plot the following chart (obviously we could do strictly a linear regression in the rules of art as seen in the chapter of Numerical Methods with confidence, prediction intervals and other stuffs...):
	\begin{figure}[H]
		\centering
		\fbox{\includegraphics{img/arithmetics/linearized_distribution_for_henry.jpg}}
		\caption[]{Linearized form of the distribution}
	\end{figure}
	So thanks to the linear regression given by Microsoft Excel 11.8346 (or calculated by you using the techniques of linear regressions seen in the section of Numerical Methods page \pageref{regression techniques}). It comes:
	
	we immediately deduce that:
	
	This is thus a particular technique for a particular distribution! Similar techniques more or less simple (or complicated depending on the case...) exist for others distributions.\\

	See now another approximate approach to solve this problem. Let's take again our table for this example:
	\begin{table}[H]
		\centering
		\renewcommand{\arraystretch}{1.2}
		\small
		\begin{tabular}{cccc}\hline
		\rowcolor[gray]{0.75}\textbf{Price} &  \textbf{Upper limit} & \textbf{Center} & \textbf{Relative cumulative}  \\[-3pt]
		\rowcolor[gray]{0.75}\textbf{of receipts} & \textbf{of the interval} &  & \textbf{frequencies in} \% \\ \hline % ne pas enlever les espaces vides entre les lignes!!!
		$[0,50[$ & $50$ & $25$ & $6.8$ \\

		$[50,100[$ & $100$ & $75$ & $15.87$ \\

		$[100,150[$ & $150$ & $125$ & $30.85$ \\

		$[150,200[$ & $200$ & $175$ & $50.00$ \\

		$[200,250[$ & $250$ & $225$ & $69.15$\\

		$[250,300[$ & $300$ & $275$ & $84.13$\\

		$[300,350[$ & $350$ & $325$ & $93.32$ \\

		$[350,400[$ & $400$ & $375$ & $97.72$ \\

		$[400$ and $+$ & $-$ & $-$ & $100$ \\ \hline
		\end{tabular}
	\end{table}
	The average is now calculated using the central value of the intervals and sample sizes according to the relation we have seen at the beginning of this section (page \pageref{arithmetic average}):
	
		\begin{table}[H]
		\centering
		\renewcommand{\arraystretch}{1.2}
		\small
		\begin{tabular}{cccc}\hline
		\rowcolor[gray]{0.75}\textbf{Price} &  \textbf{Center} & \textbf{Relative cumulative} & \textbf{Calculation}  \\[-3pt]
		\rowcolor[gray]{0.75}\textbf{of receipts} &  & \textbf{frequencies} &  \\ \hline % ne pas enlever les espaces vides entre les lignes!!!
		$[0,50[$ & $25$ & $668$ & $16,700$ \\

		$[50,100[$ & $75$ & $919$ & $68,925$ \\

		$[100,150[$ & $125$ & $1,498$ & $187,250$ \\

		$[150,200[$ & $175$ & $1,915$ & $335,125$ \\

		$[200,250[$ & $225$ & $1,915$ & $430,875$\\

		$[250,300[$ & $275$ & $1,498$ & $411,950$\\

		$[300,350[$ & $325$ & $919$ & $411,950$ \\

		$[350,400[$ & $375$ & $440$ & $165,000$ \\

		$[400$ and $+$ & $-$ & $-$ & $-$ \\ \hline
		 & Sum: & $9,772$ & $1,914,500$ \\ \hline
		 & & Average: & $\dfrac{1,914,500}{9,772}=195.92$ \\ \hline
		\end{tabular}
	\end{table}
	The average that we have calculated yet is also quite close to the average obtained previously with the Henry's line.\\
	
	The standard deviation will now be calculated using also the central value of the intervals and sample sizes according to the relation seen at the beginning of this chapter:
	
	\begin{table}[H]
		\centering
		\renewcommand{\arraystretch}{1.2}
		\small
		\begin{tabular}{cccc}\hline
		\rowcolor[gray]{0.75}\textbf{Price} &  \textbf{Center} & \textbf{Relative cumulative} & \textbf{Calculation}  \\[-3pt]
		\rowcolor[gray]{0.75}\textbf{of receipts} &  & \textbf{frequencies} & \% \\ \hline % ne pas enlever les espaces vides entre les lignes!!!
		$[0,50[$ & $25$ & $668$ & $16,700$ \\

		$[50,100[$ & $75$ & $919$ & $68,925$ \\

		$[100,150[$ & $125$ & $1,498$ & $187,250$ \\

		$[150,200[$ & $175$ & $1,915$ & $335,125$ \\

		$[200,250[$ & $225$ & $1,915$ & $430,875$\\

		$[250,300[$ & $275$ & $1,498$ & $411,950$\\

		$[300,350[$ & $325$ & $919$ & $411,950$ \\

		$[350,400[$ & $375$ & $440$ & $165,000$ \\

		$[400$ and $+$ & $-$ & $228$ & $-$ \\ \hline
		 &  & Variance: & $8364.16$ \\ \hline
		 & & Standard Deviation: & $91.45$ \\ \hline
		\end{tabular}
	\end{table}
	The standard deviation that we have calculated yet is also quite close to the standard deviation obtained with the method of the Henry's line.
	\end{tcolorbox}
	
	\paragraph{Q-Q plot}\mbox{}\\\\
	Another way to judge of the quality of fit of experimental data with a theoretical distribution (whatever that is!) is the use of a "\NewTerm{quantile-quantile plot}\index{quantile-quantile plot}" or simply named "\NewTerm{q-q plot}\index{q-q plot}".

	The idea is pretty simple, it based on the comparison the experimental data relatively to the theoretical data that are supposed to follow a particular distribution. Thus, in the case of our example, if we take the values of the mean ($\sim 200$) and standard deviation ($\sim 100$) obtained with the Henry's line as theoretical parameters for the Normal distribution, we get:
		\begin{table}[H]
		\centering
		\renewcommand{\arraystretch}{1.2}
		\small
		\begin{tabular}{cccc}\hline
		\rowcolor[gray]{0.75}\textbf{Price} &  \textbf{Upper experimental} & \textbf{Relative cumulative} & \textbf{Upper theoretical}   \\[-3pt]
		\rowcolor[gray]{0.75}\textbf{of receipts} & \textbf{limit (imposed)} & \textbf{frequencies in} \% & \textbf{limit (calculated)}  \\ \hline % ne pas enlever les espaces vides entre les lignes!!!
		$[0,50[$ & $50$ & $6.80\%$ & $50.91$ \\

		$[50,100[$ & $100$ & $15.87\%$ & $100.02$ \\

		$[100,150[$ & $150$ & $30.85\%$ & $149.99$ \\

		$[150,200[$ & $200$ & $50.00\%$ & $200$ \\

		$[200,250[$ & $250$ & $69.15\%$ & $250.01$\\

		$[250,300[$ & $300$ & $84.13\%$ & $299.98$\\

		$[300,350[$ & $350$ & $93.32\%$ & $350.00$ \\

		$[350,400[$ & $400$ & $97.72\%$ & $399.90$ \\

		$[400$ and $+$ & $-$ & $100\%$ & $-$ \\ \hline
		\end{tabular}
	\end{table}
	Plotted, this gives us the famous Q-Q plot:
	\begin{figure}[H]
		\centering
		\fbox{\includegraphics{img/arithmetics/q_q_plot.jpg}}
		\caption{Q-Q plot of the distribution}
	\end{figure}
	And of course we can compare the observed quantiles with the supposed theoretical distribution. More the points will be aligned on the line of unit slope and zero intercept origin, the better will be the fit! It's very visual, very simple and widely used by non-specialists in business statistics.
	
	\pagebreak
	For the people who may not have access to a spreadsheet or statistical software here is a table that may be useful relatively to the Normal distribution:
	\begin{center}
		\begin{tabular}{rr@{\ }r@{\ }r@{\ }r@{\ }r@{\ }r@{\ }r@{\ }r@{\ }r@{\ }r@{\ }r}
		\multicolumn{11}{c}{NORMAL CUMULATIVE DISTRIBUTION FUNCTION}\\
		\ \\
		$x$&0.00&0.01&0.02&0.03&0.04&0.05&0.06&0.07&0.08&0.09\\
		\ \\
		0.0&0.5000&0.5040&0.5080&0.5120&0.5160&0.5199&0.5239&0.5279&0.5319&0.5359\\
		0.1&0.5398&0.5438&0.5478&0.5517&0.5557&0.5596&0.5636&0.5675&0.5714&0.5753\\
		0.2&0.5793&0.5832&0.5871&0.5910&0.5948&0.5987&0.6026&0.6064&0.6103&0.6141\\
		0.3&0.6179&0.6217&0.6255&0.6293&0.6331&0.6368&0.6406&0.6443&0.6480&0.6517\\
		0.4&0.6554&0.6591&0.6628&0.6664&0.6700&0.6736&0.6772&0.6808&0.6844&0.6879\\
		0.5&0.6915&0.6950&0.6985&0.7019&0.7054&0.7088&0.7123&0.7157&0.7190&0.7224\\
		0.6&0.7257&0.7291&0.7324&0.7357&0.7389&0.7422&0.7454&0.7486&0.7517&0.7549\\
		0.7&0.7580&0.7611&0.7642&0.7673&0.7703&0.7734&0.7764&0.7794&0.7823&0.7852\\
		0.8&0.7881&0.7910&0.7939&0.7967&0.7995&0.8023&0.8051&0.8078&0.8106&0.8133\\
		0.9&0.8159&0.8186&0.8212&0.8238&0.8264&0.8289&0.8315&0.8340&0.8365&0.8389\\
		1.0&0.8413&0.8438&0.8461&0.8485&0.8508&0.8531&0.8554&0.8577&0.8599&0.8621\\
		1.1&0.8643&0.8665&0.8686&0.8708&0.8729&0.8749&0.8770&0.8790&0.8810&0.8830\\
		1.2&0.8849&0.8869&0.8888&0.8907&0.8925&0.8944&0.8962&0.8980&0.8997&0.9015\\
		1.3&0.9032&0.9049&0.9066&0.9082&0.9099&0.9115&0.9131&0.9147&0.9162&0.9177\\
		1.4&0.9192&0.9207&0.9222&0.9236&0.9251&0.9265&0.9279&0.9292&0.9306&0.9319\\
		1.5&0.9332&0.9345&0.9357&0.9370&0.9382&0.9394&0.9406&0.9418&0.9429&0.9441\\
		1.6&0.9452&0.9463&0.9474&0.9484&0.9495&0.9505&0.9515&0.9525&0.9535&0.9545\\
		1.7&0.9554&0.9564&0.9573&0.9582&0.9591&0.9599&0.9608&0.9616&0.9625&0.9633\\
		1.8&0.9641&0.9649&0.9656&0.9664&0.9671&0.9678&0.9686&0.9693&0.9699&0.9706\\
		1.9&0.9713&0.9719&0.9726&0.9732&0.9738&0.9744&0.9750&0.9756&0.9761&0.9767\\
		2.0&0.9772&0.9778&0.9783&0.9788&0.9793&0.9798&0.9803&0.9808&0.9812&0.9817\\
		2.1&0.9821&0.9826&0.9830&0.9834&0.9838&0.9842&0.9846&0.9850&0.9854&0.9857\\
		2.2&0.9861&0.9864&0.9868&0.9871&0.9875&0.9878&0.9881&0.9884&0.9887&0.9890\\
		2.3&0.9893&0.9896&0.9898&0.9901&0.9904&0.9906&0.9909&0.9911&0.9913&0.9916\\
		2.4&0.9918&0.9920&0.9922&0.9925&0.9927&0.9929&0.9931&0.9932&0.9934&0.9936\\
		2.5&0.9938&0.9940&0.9941&0.9943&0.9945&0.9946&0.9948&0.9949&0.9951&0.9952\\
		2.6&0.9953&0.9955&0.9956&0.9957&0.9959&0.9960&0.9961&0.9962&0.9963&0.9964\\
		2.7&0.9965&0.9966&0.9967&0.9968&0.9969&0.9970&0.9971&0.9972&0.9973&0.9974\\
		2.8&0.9974&0.9975&0.9976&0.9977&0.9977&0.9978&0.9979&0.9979&0.9980&0.9981\\
		2.9&0.9981&0.9982&0.9982&0.9983&0.9984&0.9984&0.9985&0.9985&0.9986&0.9986\\
		3.0&0.9987&0.9987&0.9987&0.9988&0.9988&0.9989&0.9989&0.9989&0.9990&0.9990\\
		3.1&0.9990&0.9991&0.9991&0.9991&0.9992&0.9992&0.9992&0.9992&0.9993&0.9993\\
		3.2&0.9993&0.9993&0.9994&0.9994&0.9994&0.9994&0.9994&0.9995&0.9995&0.9995\\
		3.3&0.9995&0.9995&0.9995&0.9996&0.9996&0.9996&0.9996&0.9996&0.9996&0.9997\\
		3.4&0.9997&0.9997&0.9997&0.9997&0.9997&0.9997&0.9997&0.9997&0.9997&0.9998\\
		3.5&0.9998&0.9998&0.9998&0.9998&0.9998&0.9998&0.9998&0.9998&0.9998&0.9998\\
		3.6&0.9998&0.9998&0.9999&0.9999&0.9999&0.9999&0.9999&0.9999&0.9999&0.9999\\
		3.7&0.9999&0.9999&0.9999&0.9999&0.9999&0.9999&0.9999&0.9999&0.9999&0.9999\\
		3.8&0.9999&0.9999&0.9999&0.9999&0.9999&0.9999&0.9999&0.9999&0.9999&0.9999\\
		3.9&1.0000&1.0000&1.0000&1.0000&1.0000&1.0000&1.0000&1.0000&1.0000&1.0000\\
		\end{tabular}
	\end{center}

	\pagebreak
	\subsubsection{Log-Normal Distribution}\label{log normal distribution}
	We say that a positive random variable $X$ follows a "\NewTerm{log-normal function}\index{log-normal function}" (or "\NewTerm{log-normal distribution}\index{log-normal distribution}") if by writing:
	
	we see that $y$ follows a Normal distribution of mean $\mu$ and variance $\sigma^2$ (moments of the Normal distribution). 
	
	Verbatim by the properties of logarithms, a variable can be modelled by a log-normal distribution if it results of the multiplication of many small independent factors (property of the product in sum of the logarithms and stability of the Normal distribution by the addition).
	
	The density function of $X$ for $x \geq 0$ is then (this will be justified just further below):
	
	that can be calculated in Microsoft Excel 11.8346 with the \texttt{LOGNORMDIST( )} function or its inverse by \texttt{LOGINV( )}.

	This type of scenario is especially frequent in financial markets in the options pricing model (page \pageref{ito process}). There is also an important remark with respect to the log-normal distribution further when we will develop the central limit theorem!

	Let us show that the cumulative probability function corresponds to a Normal distribution if we make the change of variables mentioned above:
	
	by writing:
	
	and (by definition):
	
	we then get:
	
	So we indeed fall back on the Normal distribution!

	The mean (average) of $X$ is then given by (the natural logarithm being not defined for $x<0$ we start the integral from zero):
	
	where we performed the change of variable:
	
	The expression:
	
	moreover being equal to:
	
	the last integral also becomes:
	
	and where we used the property that emerged during our study of the Normal distribution, that is to say that any integral of the form:	
	
	always has the same value!
	
	To calculate the variance, recall that for a random variable $X$, we have the Huygens theorem:
	
	
	Let us calculate $\text{E}(X^2)$ by performing similarly to previous developments:
	
	where once again we have the change of variable:
	
	and where we transformed the expression:
	
	as:
	
	Then:
	
	Here is a plot example of the distribution and cumulative distribution of the Log-Normal function of parameters $(\mu,\sigma)=(0,1)$:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/law_log_normal.jpg}
		\caption{Log-Normal law (mass and cumulative distribution function)}
	\end{figure}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Notice that if $y=\log(x)$ follows a $\mathcal{N}(\mu,\sigma)=\mathcal{N}(0,1)$ distribution. Then we have for the mean and variance of the log-normal distribution:
	
	\end{tcolorbox}
	The Median of the log-normal distribution can be obtained quite straightforward. Let us recall that $M_e$ is given by the value of $x$ for which the first derivative cancels, ie:
	
	We get then immediately:
	
	
	\subsubsection{Continuous Uniform Distribution}
	Let us choose $a<b$. We define the continuous uniform distribution function or "\NewTerm{uniform function}\index{uniform distribution}" by the relation:
	
	where $1_{[a,b]}$ means that outside the domain of definition $[a, b]$ the distribution function is zero. We will find this type of notation later in some other distribution functions.
	
	So we have for the cumulative distribution function:
	
	It is indeed a distribution function because it satisfies (simple integral):
	
	The continuous uniform function has for expected mean:
	
	and for the variance using the Huygens theorem:
	
	Here is a plot example of the distribution and cumulative distribution of the continuous uniform function of parameters $(a,b)=(0,1)$:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/law_uniform_continuous.jpg}
		\caption{Uniform continuous law (mass and cumulative distribution function)}
	\end{figure}
	Using the same method above we can derive a quite useful special case (that we will use in the chapter of Theoretical Computing). That is the excepted mean of $\mathcal{U}_{0,1}(x)$ \label{mean squared uniform random variable}:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The uniform function $\mathcal{U}_{a,b}(x)$ is often used in business simulation to indicate that the random variable has equal probabilities to have a value within a certain interval (typically in portfolio returns or in the estimation of project durations). The best example of application is again CrystalBall or @Risk software that integrate with Microsoft Project. 
	\end{tcolorbox}

	Let us see an interesting result of the continuous uniform distribution (and that applies also to the discrete one as well...).

	I often hear managers (who consider themselves at high level) that if we have a measure with an equal probability to occur in a closed given interval, then the sum of two such independent random variables have also the same equal probability in the same interval!

	Now we will prove here that this is not the case (if someone has a more elegant proof I'm interested)!
	\begin{dem}
	Consider two independent random variables $X$ and $Y$ that follow a uniform distribution in a closed interval $[0, a]$. We are searching the density of their sum will be written:
	
	Then we have:
	
	with the variable:
	
	To calculate the distribution of the sum, remember that we know that in discrete terms this is equivalent to the joint product of probabilities (\SeeChapter{see section Probabilities page \pageref{joint probability}}) of the occurrence of two continuous variables (remember the same kind of calculation in the discreet form!).

	That is to say:
	
	As $f_Y(y)=1$ if $0\leq y \leq a$ and $0$ otherwise then the product of the previous convolution reduces to:
	
	The integrand is by definition $0$ except by construction in the interval $0\leq z-y \leq a$ it is then $1$.
	
	Let us focus on the limits of the integral that is in this case the only one that is interesting ....

	First we make a change of variables by writing:
	
	thus:
	
	The integral can be then written in this interval after the change of variable:
	
	Remembering that we have seen at the beginning that $0\leq z \leq 2a$, then we have immediately if $z<0$ and $z >2a$ that the integral is zero.
	
	We will consider two cases for the interval because the convolution of these two rectangular functions can be distinguished according to the situation where at first they cross (nest), that is to say where $0\leq z \leq a$, and then recede from each other, that is to say $a< z \leq 2a$.
	
	\begin{itemize}
		\item In the first case (nest) where $0\leq z \leq a$:
		
		where we changed the lower bound to $0$ because anyway $f_X(u)$ is zero for any negative value (and when $0\leq z \leq a$,$z-a$ is precisely zero or negative!).
		
		\item In the second case (dislocation) where $a< z \leq 2a$:
		
		where we changed the upper terminal $a$ because anyway $f_X(u)$ is zero for any higher value (and when $a \leq  z \leq 2a$, $z$ is just larger than $a$).
		
		So in the end, we have:
		
	\end{itemize}
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	This is a particular case, deliberately simplified, of the triangular distribution that we will discover just after...

	This result (which may seems perhaps not intuitive) can be check in a few seconds with a spreadsheet software like Microsoft Excel 11.8346 using the \texttt{RANDBETWEEN()} and the \texttt{FREQUENCY( )} functions.
	
	\pagebreak
	\subsubsection{Triangular Distribution}\label{triangular distribution}
	
	Let $a<c<b$. We define the "\NewTerm{triangular distribution}\index{triangular distribution}" (or "\NewTerm{triangular function}\index{triangular function}") by construction based on the following two distribution functions:
	
	where $a$ is often assimilated with the optimistic value, $c$ to the modal value and $b$ the pessimistic value.

	It is also the only way to write this distribution function if the reader keeps in mind that the base of a triangle of length $c-a$ must have a height $h$ equal to $2/(c-a)$ as its total area is equal to unity (we will soon prove it).

	Here is a plot example of the triangular distribution and cumulative distribution for the parameters $(a, c, b) = (0, 3, 5)$:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/law_triangular.jpg}
		\caption{Triangular law (mass and cumulative distribution function)}
	\end{figure}
	The slope of the first straight line (increasing from left) is obviously:
	
	and the slope of the second straight line (decreasing to the right):
	
	This function is a distribution function if it satisfies:
	
	It is in this case, simply the area of the triangle which we recall is simply the base multiplied by the height divided by $2$ (\SeeChapter{see section Geometric Shapes page \pageref{unspecified triangle}}):
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	This function is widely used in project management in the context of task duration estimations or in industrial simulations. Where $a$ corresponds to the optimistic value, $c$ to the expected value (mode) and the value $b$ to the pessimistic value. The best example of application is again the softwares CrystalBall or @Risk that are add-ins for Microsoft Project.
	\end{tcolorbox}
	The triangular function has also for mean (average):
	
	
	and for variance:
	
	We can replace $\mu$ by the result obtained before and we get after simplification (it is boring algebra...):
	
	We can show that the sum of two independent random variables, each uniformly distributed on $[a, b]$ (i.e. independent and identically distributed) follows a triangular distribution on $[2a, 2b]$ but if they do not have the same limits, then their sum gives something that has no name to my knowledge...
	
	As already mentioned earlier in the previous remark box, the triangular distribution is used a lot in Risk Management by beginners (the level above consisting in using the beta distribution that we will see further below). A quite common figure you can find in project management textbooks for beginners is the impact/probability matrix give below ("impact" is modelled according to the distance between optimistic and pessimistic points):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,659); %set diagram left start at 0, and has height of 659
		
		%Shape: Rectangle [id:dp8248438249726975] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 213; green, 6; blue, 32 }  ,fill opacity=1 ] (384.74,229.78) -- (505.37,229.78) -- (505.37,293.06) -- (384.74,293.06) -- cycle ;
		%Shape: Rectangle [id:dp9842359541958616] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 237; green, 52; blue, 74 }  ,fill opacity=1 ] (384.74,166.5) -- (505.37,166.5) -- (505.37,229.78) -- (384.74,229.78) -- cycle ;
		%Shape: Rectangle [id:dp6209709722088212] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 237; green, 52; blue, 74 }  ,fill opacity=1 ] (266.37,229) -- (384.74,229) -- (384.74,292.28) -- (266.37,292.28) -- cycle ;
		%Shape: Rectangle [id:dp1596029600650244] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 248; green, 197; blue, 99 }  ,fill opacity=1 ] (148,229) -- (266.37,229) -- (266.37,292.28) -- (148,292.28) -- cycle ;
		%Shape: Rectangle [id:dp10393178333800979] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 248; green, 197; blue, 99 }  ,fill opacity=1 ] (384.74,104) -- (506.37,104) -- (506.37,167.28) -- (384.74,167.28) -- cycle ;
		%Shape: Rectangle [id:dp41972714299957814] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 248; green, 197; blue, 99 }  ,fill opacity=1 ] (266.37,166.5) -- (384.74,166.5) -- (384.74,229.78) -- (266.37,229.78) -- cycle ;
		%Shape: Rectangle [id:dp8045035315679419] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 169; green, 233; blue, 95 }  ,fill opacity=1 ] (148,167.28) -- (266.37,167.28) -- (266.37,230.56) -- (148,230.56) -- cycle ;
		%Shape: Rectangle [id:dp19146032089804899] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 169; green, 233; blue, 95 }  ,fill opacity=1 ] (266.37,104) -- (384.74,104) -- (384.74,167.28) -- (266.37,167.28) -- cycle ;
		%Shape: Rectangle [id:dp6108724798839429] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 248; blue, 177 }  ,fill opacity=1 ] (148,104) -- (266.37,104) -- (266.37,167.28) -- (148,167.28) -- cycle ;
		%Shape: Rectangle [id:dp6751931797233031] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] (148,104) -- (505.37,104) -- (505.37,292.28) -- (148,292.28) -- cycle ;
		%Shape: Rectangle [id:dp17916742513300976] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] (148,104) -- (505.37,104) -- (505.37,167.28) -- (148,167.28) -- cycle ;
		%Shape: Rectangle [id:dp09402133727745432] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] (148,104) -- (505.37,104) -- (505.37,167.28) -- (148,167.28) -- cycle ;
		%Shape: Rectangle [id:dp7215827067315792] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] (148,167.28) -- (505.37,167.28) -- (505.37,229.28) -- (148,229.28) -- cycle ;
		%Shape: Rectangle [id:dp7375575543117718] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] (148,229.28) -- (505.37,229.28) -- (505.37,292.56) -- (148,292.56) -- cycle ;
		%Shape: Rectangle [id:dp7234703941457603] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] (148,104) -- (266.37,104) -- (266.37,292.28) -- (148,292.28) -- cycle ;
		%Shape: Rectangle [id:dp61007774773711] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] (266.37,104) -- (384.74,104) -- (384.74,292.28) -- (266.37,292.28) -- cycle ;
		%Shape: Rectangle [id:dp937481430908327] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] (384.74,104) -- (505.37,104) -- (505.37,292.28) -- (384.74,292.28) -- cycle ;
		%Right Arrow [id:dp6056932443763072] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 194; green, 194; blue, 194 }  ,fill opacity=1 ] (151,44) -- (482.37,44) -- (482.37,34) -- (504.37,54) -- (482.37,74) -- (482.37,64) -- (151,64) -- cycle ;
		%Right Arrow [id:dp663506976197906] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 194; green, 194; blue, 194 }  ,fill opacity=1 ] (93.66,102.28) -- (93.73,282.33) -- (103.73,282.32) -- (83.73,294.28) -- (63.73,282.34) -- (73.73,282.33) -- (73.66,102.28) -- cycle ;
		%Straight Lines [id:da017488965185608052] 
		\draw    (163.5,152) -- (243.87,152) ;
		%Straight Lines [id:da43340329786858556] 
		\draw    (284.87,280) -- (365.24,280) ;
		%Shape: Triangle [id:dp01634032547335762] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 187; green, 184; blue, 184 }  ,fill opacity=1 ] (205.87,111) -- (226.37,151) -- (185.37,151) -- cycle ;
		%Shape: Triangle [id:dp4774164864375896] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 187; green, 184; blue, 184 }  ,fill opacity=1 ] (326.37,111.64) -- (355.37,151.64) -- (297.37,151.64) -- cycle ;
		%Straight Lines [id:da5404973477901664] 
		\draw    (284.87,152) -- (365.24,152) ;
		%Shape: Triangle [id:dp07890081516279701] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 187; green, 184; blue, 184 }  ,fill opacity=1 ] (445.81,112) -- (486.37,152) -- (405.24,152) -- cycle ;
		%Straight Lines [id:da8267207953324205] 
		\draw    (399.8,152.64) -- (492.81,152.64) ;
		%Straight Lines [id:da40927616692364177] 
		\draw    (399.24,280) -- (493.24,280) ;
		%Shape: Triangle [id:dp9683449167142861] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 187; green, 184; blue, 184 }  ,fill opacity=1 ] (198.5,177.14) -- (228.37,217.14) -- (187.37,217.14) -- cycle ;
		%Straight Lines [id:da4570129916033687] 
		\draw    (163.5,217) -- (243.87,217) ;
		%Shape: Triangle [id:dp6348780192924832] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 187; green, 184; blue, 184 }  ,fill opacity=1 ] (317.55,177.14) -- (358.47,217.14) -- (295.37,217.14) -- cycle ;
		%Straight Lines [id:da6931127012741924] 
		\draw    (288.37,217.64) -- (365.24,217) ;
		%Shape: Triangle [id:dp9081091702497472] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 187; green, 184; blue, 184 }  ,fill opacity=1 ] (419.59,177) -- (489.24,217) -- (404.68,217) -- cycle ;
		%Straight Lines [id:da7908586830402546] 
		\draw    (399.8,217) -- (492.81,217) ;
		%Shape: Rectangle [id:dp6591260934759227] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 187; green, 184; blue, 184 }  ,fill opacity=1 ] (187.69,240.64) -- (226.69,240.64) -- (226.69,280.64) -- (187.69,280.64) -- cycle ;
		%Straight Lines [id:da5725091968036204] 
		\draw    (163.5,280) -- (243.87,280) ;
		%Shape: Rectangle [id:dp8052906812363181] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 187; green, 184; blue, 184 }  ,fill opacity=1 ] (295.37,239.14) -- (356.37,239.14) -- (356.37,279.14) -- (295.37,279.14) -- cycle ;
		%Shape: Rectangle [id:dp8492952723805156] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 187; green, 184; blue, 184 }  ,fill opacity=1 ] (404.68,239) -- (489.37,239) -- (489.37,279) -- (404.68,279) -- cycle ;
		
		% Text Node
		\draw (191,78) node [anchor=north west][inner sep=0.75pt]   [align=left] {Low};
		% Text Node
		\draw (297,78) node [anchor=north west][inner sep=0.75pt]   [align=left] {Medium};
		% Text Node
		\draw (425,78) node [anchor=north west][inner sep=0.75pt]   [align=left] {High};
		% Text Node
		\draw (108,151) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Low};
		% Text Node
		\draw (108,226) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Medium};
		% Text Node
		\draw (108,281) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {High};
		% Text Node
		\draw (299,47) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{IMPACT}};
		% Text Node
		\draw (76,251) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {\textbf{PROBABILITY}};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[Naive risk matrix in project management]{Naive risk matrix in project management (author: Jose Barato)}
	\end{figure}
	
	\subsubsection{Pareto Distribution}\label{pareto distribution}
	The "\NewTerm{Pareto distribution}\index{Pareto distribution}" (or "\NewTerm{Pareto law}\index{Pareto law}"), also named "\NewTerm{power law}\index{power law (statistics)}" or "\NewTerm{scale law}\index{scale law (statistics)}" is the formalization of the $80-20$ principle. This decision tool helps determine the critical factors (about $20\%$) influencing the majority ($80\%$) of the goal.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	This distribution is a fundamental and basic tool in quality management (see Industrial Engineering page \pageref{abc method} and Quantitative Management page \pageref{pareto analysis}). It is also used in reinsurance. The theory of queues had also some interest in this distribution when some research in the 11990s (holocene calendar) showed that this distribution also seems ton explain well a number of variables observed in the Internet traffic (and more generally on all high speed data networks).
	\end{tcolorbox}
	
	A random variable is said by definition follow a Pareto distribution if its cumulative distribution function is given by:
	
	with $x$ that must be greater than or equal to $x_m$.
	
	The Pareto density function (distribution function) is then given by:
	
	with $k\in \mathbb{R}_+$ and $x \geq x_m \geq 0$ (then $x>0$).
	
	The Pareto distribution is defined by two parameters, $x_m$ and $k$ (named  "\NewTerm{Pareto index}\index{Pareto index}"). This distribution is also said to be "\NewTerm{scale invariant}\index{scale invariant}" or "\NewTerm{fractal distribution}\index{fractal distribution}", because of the following property:
	
	The Pareto function is also well a distribution function as the cumulative distribution function known we have:
	
	The expected mean is given by:
	
	if $k>1$. If $k\leq 1$, the mean does not exist.
	
	To calculate the variance, using the Huygens theorem:
	
	we get:
	
	if $k>2$. If $k\leq 2$, $\text{E}(X^2)$ doesn't exist! 
	
	So if $k>2$:
	
	If $k\leq 2$, the variance doesn't exists.
	
	Here is a plot example of the Pareto distribution and cumulative distribution for the parameters $(x,x_m,k)=(x,1,2)$:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/law_pareto.jpg}
		\caption{Pareto law (mass and cumulative distribution function)}
	\end{figure}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	See that when $k\rightarrow +\infty$ the distribution approaches $\delta (x-x_m)$ where $\delta$ is the Dirac delta function. 
	\end{tcolorbox}
	There is another important way to deduce the family of Pareto distributions that allows us to understand many things about other distributions and that is often presented as follows:
	
	Let us write $x_0$ the threshold beyond which we calculate the mean of the considered quantity, and $\text{E}(Y)$ the mean beyond this threshold $x_0$ as it is proportional (linearly dependent) to the chosen threshold:
	
	This functional relation expresses the idea that the conditional mean beyond the threshold $x_0$ is a multiple of this threshold plus a constant, that is to say a linear function of the threshold.

	Thus, in project management, for example, we could say that once a certain threshold of time is exceeded, the expected duration is a multiple of this threshold plus a constant.

	If a linear relation of this type exists and is satisfied, then we talk about a probability distribution in the form of a generalized Pareto distribution.

	Consider the mean of the Bayesian conditional function given by (\SeeChapter{see section Probabilities page \pageref{conditional expectation}}):
	
	where the notation on the left hand side is somewhat shortened but the reader will implicitly understand that it is a conditional expectation.
	
	If we write $F(y)$ the cumulative distribution function $f(y)$, then we have by definition:
	
	Thus:
	
	and if we define:
	
	what we can assimilate to the "tail of the distribution".
	
	We get:
	
	and therefore we seek the very special case where:
	
	this is to say:
	
	Differentiating with respect to $x$, we find:
	
	The derivative of the integral defined above will be the derivative of a constant (valorisation of the integral in $+\infty$) minus the derivative of the analytical expression of the integral for $x_0$. So we have:
	
	Thus:
	
	and as:
	
	it comes:
	
	After simplification and rearrangement we obtain:
	
	which is a differential equation in $\bar{F}(x)$. Its Resolution provides all forms of seek Pareto distributions, according to the values taken by the parameters $a$ and $b$.
	
	To solve this differential equation, consider the special case where $a>1,b=0$. Then we have:
	
	By writing:
	
	We then get:
	
	and therefore:
	
	It comes:
	
	and therefore:
	
	So we have\label{pareto tail distribution}:
	
	Then it comes form the cumulative distribution function:
	
	If we seek for the distribution function, we derive by $x$ to get:
	
	This is the Pareto distribution we have used since the beginning and named "\NewTerm{Pareto distribution of type I}\index{Pareto distribution of type I}" (we won't see in this book those of type II).
	
	An interesting thing to observes is the case of the resolution of the following differential equation:
	
	when $a=1,b>0$. The differential equation is then reduced to:
	
	Thus:
	
	After integration:
	
	and therefore:
	
	If we make a small change in notation:
	
	and that we write the distribution function:
	
	and by derivating we get the distribution function of the exponential distribution:
	
	So the exponential distribution has a conditional mean threshold that is equal to:
	
	So the conditional mean threshold is equal to itself plus the standard deviation of the distribution.

	\pagebreak
	\subsubsection{Exponential Distribution}\label{exponential distribution}
	We define the "\NewTerm{exponential distribution}\index{exponential distribution}" (or "\NewTerm{exponential law}\index{exponential law}") by the following distribution function:	
	
	with $\lambda > 0$ that as we will immediately see is in the fact that the inverse of the mean and where $x$ is a random variable without memory. This law is also sometimes denoted $\mathcal{E}(\lambda)$.
	
	In fact the exponential distribution naturally appears from simple developments (see the Nuclear Physics chapter for example) under assumptions that impose a constance in the aging of phenomenon. In the section of Quantitative Management, we have also proved in detail in the section on the theory of queues (page \pageref{without memeory process}), that this law was without memory. That is to say, that the cumulative probability of a phenomenon occurs between the time $t$ and $t + s$, if it is not realized before, is the same as that the cumulative probability of occurring between the time $0$ and $s$.
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} This function is occurring frequently in nuclear physics (see chapter of the same name) or quantum physics (see also chapter of the same name) as well in reliability (\SeeChapter{see section Quantitative Management page \pageref{industrial engineering}}) or in the theory of queues (\SeeChapter{see section Quantitative Management page \pageref{queueing theory}}).\\

	\textbf{R2.} We can get this distribution in Microsoft Excel 11.8346 with the \texttt{EXPONDIST( )} function.
	\end{tcolorbox}	
	
	It is also really a distribution function because it verifies:
	
	The exponential distribution has for expected mean using integration by parts:
	
	and for variance using once again the Huygens relation:
	
	it remains for us to only the to calculate:
	
	A variable change $y=\lambda$ leads us to:
	
	A double integration by parts gives us:
	
	Hence:
	
	we have therefore:
	
	So the standard deviation (square root of the variance for recall) and mean have exactly the same expression!

	Here is a plot example of the exponential distribution and cumulative distribution for the parameter $\lambda=1$:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/law_exponential.jpg}
		\caption{Exponential law (mass and cumulative distribution function)}
	\end{figure}
	Now let us determine the distribution function of the exponential law:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		We will see later that the exponential distribution is a special case of a more general distribution which is the chi-square distribution, the chi-square is also a special case of a more general distribution that is the Gamma distribution. This is a very important property used in the "Poisson test" for rare events (see also below). 
	\end{tcolorbox}
	
	\subsubsection{Cauchy Distribution}
	Let $X, Y$ be two independent random variables following a Normal reduced centered distribution (with zero mean and unit variance). Thus the density function is given for each variable by:
	
	The random variable:
	
	(the absolute value will be useful in an integral during a change of variable) follows a characteristic appearance named the "\NewTerm{standard Cauchy distribution}\index{standard Cauchy distribution}" (or "\NewTerm{standard Cauchy law}\index{standard Cauchy law}") or even "\NewTerm{standard Lorentz law}\index{standard Lorentz law}" (as we took as starting point the special case of two centred reduced Normal laws!).
	
	Let us now determine its density function $f$. To do this, recall that $f$ is determined by the (general) relation:
	
	So (application of elementary differential calculus):
	
	in the case where $f$ is continuous.
	
	Since $X$ and $Y$ are independent, the density function of the random vector is given by the third axioms of probabilities (\SeeChapter{see section  Probabilities page \pageref{joint probability}}):
	
	therefore:
	
	where $D=\left\lbrace(x,y)\vert x<t\vert y\vert\right\rbrace$.
	This last integral becomes:
	
	Let us make the following change of variables $x=u\vert y \vert$ in the inner integral. We obtain:
	
	Therefore:
	
	Now the absolute value will be useful to write:
	
	For the first integral we have:
	
	It remains therefore only the second integral and making the change of variable $v=y^2$, we get:
	
	What we will denote thereafter (to respect the notations adopted so far):
	
	and that is simply the so named "standard Cauchy distribution" (as we took as starting point two centred reduced Normal laws!).
	
	It is also effectively a distribution function because it verifies (\SeeChapter{see section Differential and Integral Calculus page \pageref{usual primitives}}):
	
	It is obvious that we get therefore for the cumulative distribution function:
	
	Here is plot example of the Cauchy distribution:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/law_cauchy.jpg}
		\caption{Cauchy law (mass function) }
	\end{figure}
	The Cauchy distribution has for expected mean:
	
	
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Caution!!!! The above calculations do not give zero in facts because the subtraction of infinite is not zero but indeterminate! The Cauchy distribution therefore and strictly speaking does not admits an expected mean!
	\end{tcolorbox}
	
	Thus, even if we can build a variance:
	
	this is absurd and does not exist strictly speaking as the mean doesn't exists...!
	
	The Cauchy distribution is used a lot in financial engineering as it is heavy tailed and therefore a very good candidate to be more accurate in predicting extreme values at the opposite to the Normal distribution that has the tails decreasing to quick. Further the Cauchy distribution is a heavy tailed law with a support on $\mathbb{R}$ when the Pareto distribution (also heavy tailed) is defined only on $\mathbb{R}^+$.
	
	Software such as JASP and the \texttt{R} package BayesFactor (Morey et al., 12018) rely on the Cauchy distribution as a prior for many of the Bayesian analyses.
	
	The Cauchy distribution if one of the most famous distribution function that... we cannot found in the spreadsheet softwares like Microsoft Excel. To be able to get the closed form of the inverse Cauchy CDF we start from the CDF proven previously:
	
	and therefore if we let:
	
	We immediately get the inverse CDF:
	
	That is useful in finance as we know (\SeeChapter{see section Numerical Methods page \pageref{inverse transform sampling}}) to simulate a Cauchy variable when we use the inverse transform sampling:
	
	
	Using the random variable transformation (see page \pageref{random variable transformation}) we immediately derive the "\NewTerm{general Cauchy distribution}\index{general Cauchy distribution}" or simply named the "\NewTerm{Cauchy distribution}\index{Cauchy distribution}" and given by:
	
	In Bayesian inference the scale parameter $b$ is often denoted $\omega$ and named the "width parameter".
	
	By default, the Cauchy distribution in JASP and in the BayesFactor of the \texttt{R} software package is centred around zero. At first, this may seem counter-intuitive, because this is the prior parameter for the alternative hypothesis. The reason for the centring is that it allows us to test a non-directional hypothesis. Given that the prior distribution is symmetrical and centred around zero, effect sizes of, say, greater than $+0.5$ or smaller than $-0.5$ are equally likely. The bell-shape of the Cauchy distribution further indicates a belief that smaller effect sizes are more likely than larger effect sizes. When we increase the width parameter $\omega$, we increase the a priori probability of large effect sizes (though small effect sizes will continue to be more likely).
	
	\subsubsection{Beta Distribution}\label{beta distribution}
	Let us first recall that the Euler Gamma function is defined by the relation (\SeeChapter{see section Differential and Integral Calculus page \pageref{gamma euler function}}):
	
	We proved (\SeeChapter{see section Differential and Integral Calculus page \pageref{gamma euler function}}) that a non-trivial property of this function is:
	
	Let us now write:
	
	where:
	
	By the change of variables:
	
	we get:
	
	For the internal integral we now use the substitution $v=ut, 0\leq t\leq 1$ and therefore we find:
	
	The function $B$ that appears in the expression above is named "\NewTerm{beta function}\footnote{Historically this function comes from the calculation of $m ! n !=\lim_{a^{2} \rightarrow +\infty} \int\limits_{0}^{a^{2}} e^{-u} u^{m} \mathrm{d} u \int\limits_{0}^{a^{2}} e^{-v} v^{n}\mathrm{d}v$, using the Euler-Gamma function and that leads to $B(m+1, n+1)\equiv 2 \int_{0}^{\pi / 2} \cos ^{2 m+1} (\theta) \sin ^{2 n+1} (\theta)\mathrm{d} \theta =\frac{m ! n !}{(m+n+1)!}$.}\index{beta function}\label{beta function}" and therefore we have:
	
	Now that we have defined what we name the "beta function", consider the two parameters $a>0,b>0$ and consider also the special relation below as the "\NewTerm{beta distribution}\index{beta distribution}" or "\NewTerm{beta law}\index{beta law}" (there are several formulations of the beta distribution and a very important one is studied in detail in the section of Quantitative Management page \pageref{probabilitic pert}):
	
	where:
	
	Nous rencontrerons également cette dernière fonction dans le contexte de la Théorie Quantique des Champs !
	
	Historiquement cette fonction vient de la statistique bayésienne (notamment un cas particulier traité par Laplace) du ratio de la distribution Binomiale sur la somme de tous les issues possibles de cette même distribution Binomiale tel que :
	
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The integral rewritten as following:
	
	is named the "\NewTerm{Chebyshev integral}\index{Chebyshev integral}".\\
	
	The integral generalized as following:
	
	is named the "\NewTerm{incomplete beta function}\index{incomplete beta function}\label{incomplete beta function}". While best known for its applications in Statistics, it is also widely used in many other fields such as actuarial science, economics, finance, survival analysis, life testing and telecommunications.
	\end{tcolorbox}	
	
	We first check that $P_{a,b}(x)$ that is effectively a distribution function (without getting into too much details ...):
	
	Let us now calculate the expected mean:
	
	by using the relation:
	
	and its variance:
	
	As we know that:
	
	we find:
	
	and therefore:
	
	Examples of plots of the beta distribution function for $(a,b)=(0.1,0.5)$ in red, $(a,b)=(0.3,0.5)$ in green, $(a,b)=(0.5,0.5)$ in black, $(a,b)=(0.8,0.8)$ in blue, $(a,b)=(1,1)$ in magenta, $(a,b)=(1,1.5)$ in cyan, $(a,b)=(1,2)$ in gray, $(a,b)=(1.5,2)$ in turquoise, $(a,b)=(2,2)$ in yellow,$(a,b)=(3,3)$ in gold color:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/law_beta_samples.jpg}
		\caption{Some Beta law mass functions}
	\end{figure}
	Here is a plot example of the beta distribution and cumulative distribution for the parameters $(a,b)=(2,3)$:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/law_beta.jpg}
		\caption{Beta law (mass and cumulative distribution function) }
	\end{figure}
	The fact that the beta law is one of the rare distribution functions whose support is between $] 0,1 [$ explains its frequent use in Bayesian statistics as an a priori law of the distribution of a proportion!
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Notice the important special case that when $a,b=1$, the beta distribution, reduce to a uniform continuous distribution! Indeed:
	
	\end{tcolorbox}
	
	In general, if you want a beta prior distribution with parameters $a$ and $b$ but you only know $\mu$ and $\sigma^2$, they can be found via the both relations:
	
	We can solve the first one as follows for $b$:
	
	Substituting that latter into the above $\sigma^2$ yields:
	
	Rearranging that latter yields:
	
	and therefore:
	
	
	\paragraph{Regularized Incomplete beta function integral}\mbox{}\\\\
	What follows should better be placed in the section of Functional Analysis however it has seem more pedagogical to us to place it just here for obvious reasons.
	
	Consider the "\NewTerm{regularized incomplete Beta function $I_x(a,b)$}\index{regularized incomplete Beta function}\label{incomplete Beta function}"  (or "\NewTerm{regularized beta function}" for short) defined in terms of the incomplete beta function and the complete beta function as (for $\alpha>0$):
	
	where for recall $a>0$ and $b>0$. This function will be useful to us in some Bayesian A/B statistics (\SeeChapter{see section Statistics page \pageref{A/B testing for binary outcomes}}) but also in String Theory (\SeeChapter{see section String Theory page \pageref{string theory}})!
	
	Notice the canonical values that are immediate:
	
	We want to prove the recursive property:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	This also means that:
	
	but for $a>1$ (and still $b>0$).
	\end{tcolorbox}
	\begin{dem}
	Using the following Beta function (see page \pageref{beta function}) property:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	From the above relation and because of symmetry of the beta function, the following three results follow almost immediately:
	
	\end{tcolorbox}
	We compute the derivative:
	
	Now let:
	
	Then $f(a,b,0)=0$ and:
	
	therefore $f\equiv 0$ and the recursive property is proven.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	So we have proved that:
	
	And notice that we have the limit case:
	
	So now recursively iterating:
	
	Until the case:
	
	
	Subsuming the zeroth term into the sum:
	
	And we will stop here for the study of this function as this is all what we need in the actual state of this book for practical applications!
	
	\subsubsection{Gamma Distribution}\label{gamma distribution}
	The Euler Gamma function being known, consider two parameters $a>0,\lambda>0$ and let us define the "\NewTerm{Gamma distribution}\index{Gamma distribution}" (or "\NewTerm{Gamma law}\index{Gamma law}") as given by the relation (density function):
	
	By the change of variables $t=\lambda x$ we obtain:
	
	and we can then write the relation in a more conventional form that we find frequently in the literature:
	
	and it is under this notation that we find this distribution function in Microsoft Excel 11.8346 under the name \texttt{GAMMADIST( )} and its inverse by \texttt{GAMMAINV( )}. Notice that the previous relation is also often denoted:
	
	Let us now see a simple property of the Gamma distribution that will be partially useful for the study of the Welch statistical test (see page \pageref{Welch test}). First recall that we have shown above that:
	
	Let us write $Y=c^{te}X$, then we have immediately:
	
	So the multiplication by a constant of random variable that follows a Gamma distribution has only for effect of dividing the parameter $\lambda$ by the same constant. This is the reason why $\lambda$ is named "\NewTerm{scale parameter}\index{scale parameter}".
	
	If $a \in \mathbb{N}$, the Gamma distribution at the denominator becomes (\SeeChapter{see section Differential and Integral Calculus page \pageref{gamma euler function}}) the factorial $(a-1)!$. The Gamma function can then be written:
	
	This particular notation of the Gamma distribution is named the "\NewTerm{Erlang distribution}\index{Erlang distribution}" that we find naturally in the theory of queues and that is very important in practice!
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	If $a=1$ then $\Gamma(a)=1$ and $x^{a-1}=1$ and we fall back on the exponential distribution.
	\end{tcolorbox}	
	
	Then we check with a similar reasoning to this of the beta distribution that $P_{a,\lambda}(x)$ is a distribution function:
	
	Examples of plots of the beta distribution function for $(a,\lambda)=(0.5,1)$ in red, $(a,\lambda)=(1,1)$ in green, $(a,\lambda)=(2,1)$ in black, $(a,\lambda)=(4,2)$ in blue, $(a,\lambda)=(16,8)$ in magenta:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/law_gamma_samples.jpg}
		\caption{Some Gamma law mass functions}
	\end{figure}
	and a plot example of the Gamma distribution and cumulative distribution for the parameters $(a,\gamma)=(4,1)$:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/law_gamma.jpg}
		\caption{Gamma law (mass and cumulative distribution function) }
	\end{figure}
	The Gamma function has also for expected mean:
	
	and for variance:
	
	Let us now prove a property of the Gamma distribution that will permit us later in this chapter, during our study of the analysis of variance and confidence intervals based on small samples, another extremely important property of the Chi-square distribution.
	
	As we know, the distribution function of a random variable following a Gamma function of parameters $a,\lambda>0$ is:
		
	with (\SeeChapter{see section Differential and Integral Calculus page \pageref{gamma euler function}}) the Euler Gamma function:
	
	Moreover, when a random variable follows a Gamma function we often notice it in the following way:
	
	Let $X, Y$ be two independent variables. We will prove that if $X=\gamma(p,\lambda)$ and $Y=\gamma(q,\lambda)$, hence with the same scale parameter, then:
	
	We write $f$ the density function of the pair $X, Y,f(x)$ the density function of $X$ and $f_Y$ the density function of $Y$. Because $X$ and $Y$ are independent, we have:
	
	for all $x,y>0$.
	
	Let $Z=X+Y$. The distribution function of $Z$ is therefore:
	
	where $D=\left\lbrace(x,y)\vert x+y\leq z \right\rbrace$.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	As we already know we name such a calculation a "\NewTerm{convolution}\index{convolution}" and statisticians often have to handle such entities because they work on many random variables that they have to sum or even to multiply.
	\end{tcolorbox}
	Simplifying:
	
	We perform the following change of variable $x=x,y=s-x$. The Jacobian is therefore (\SeeChapter{see section Differential and Integral Calculus page \pageref{jacobian}}):
	
	Therefore with the new integration limits $s=x+y=x+(z-x)=z$ we have:
	
	If we denote by $g$ the density function $Z$ we have:
	
	Then it follows:
	
	$f_X$ and $f_Y$ being null when the argument is negative, we can change the limits of integration:
	
	Let us calculate $g$:
	
	After the change of variable $x=st$ we obtain:
	
	where $B$ is the beta function we saw earlier in our study of the beta distribution. But we have also proved the relation:
	
	Therefore:
	
	More explicitly:
	
	Which finally gives us:
	
	This shows that if two random variables follow a Gamma distribution then their sum will also follow a Gamma distribution with parameters:
	
	So the Gamma distribution is stable by addition as are all distribution arising from the Gamma distribution that we will see below.
	
	\paragraph{Generalized Gamma Distribution}\mbox{}\\\\
	The generalized gamma distribution is a continuous probability distribution with three parameters. It is a generalization of the two-parameter gamma distribution. Since many distributions commonly used for parametric models in survival analysis (such as the exponential distribution, the Weibull distribution and the Gamma distribution, and log-normal) are special cases of the generalized gamma, it is sometimes used to determine which parametric model is appropriate for a given set of data.

	Therefore let us notice that if we write after trials and errors the following density function named "\NewTerm{generalized Gamma law}\index{generalized Gamma law}":
	
	with $x>0$, $\alpha>0$, $\eta>0$, $\kappa>0$.
 
	Then, for $\kappa=1$ we fall back on the density function of the Weibull distribution (\SeeChapter{see section Industrial Engineering page \pageref{weibull distribution}}) that is with our own notations of the corresponding section is given by:
	 
	For $\eta=1$, we fall back the Gamma density function just introduced before:
	
	For $\kappa=1$ and $\eta=1$, we fall back on the exponential distribution also seen previously:
	
	and finally for $\eta\rightarrow 0$, $\kappa\rightarrow +\infty$ we fall back on a log-normal distribution after developing the limits using the Stirling, Hospital and Taylor techniques (\SeeChapter{see section Numerical Methods page \pageref{stirling}, Differential and Integral Calculus page \pageref{Hospital rule}, Sequences and Series page \pageref{taylor series}}):
	
	As always, on request we can detail the developments!
	
	\subsubsection{Chi-Square (Pearson) Distribution}\label{chi-square distribution}
	The "\NewTerm{chi-square distribution}\index{chi-square distribution}" (also named "\NewTerm{chi-square law}\index{chi-square law}" or "\NewTerm{Pearson law}\index{Pearson law}") has a very important place in the industrial practice for some common hypothesis tests (see far below...) and is by definition only a particular case of the Gamma distribution in the case where $a=k/2$ and $\lambda=1/2$, when $k$ is a positive integer:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	We will also prove later during our study of the confidence interval on the variance with known mean (see page \pageref{ci on the variance with known mean}), the very important property that if $X$ follows a Normal standard distribution $\mathcal{N}(0,1)$ then $X^2=\chi^2_1$.
	\end{tcolorbox}
	This relation that connects the chi-square distribution with the Gamma distribution is important in the in Microsoft Excel 11.8346 as the function \texttt{CHIDIST( )} returns the confidence level and not the distribution function. Then you must use the function \texttt{GAMMADIST()} with the parameters given above (except that you must take the inverse of $1/2$: also $2$ as parameter) to get the distribution and cumulative functions.
	
	The reader who wishes to check that the Chi-square distribution is only a special case of the Gamma distribution can write in Microsoft Excel 14.0.6123:
	
	\begin{center}
		\texttt{=CHISQ.DIST(2*x,2*k,TRUE)}\\
		\texttt{=GAMMA.DIST(x,k,1,TRUE)}
	\end{center}
	All calculations made previously still apply and we get immediately:
		
	Examples of plots of the chi-squared distribution function for $k=1$ in red, $k=3$ in green, in black, $k=4$ in blue:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/law_chi2_samples.jpg}
		\caption{Some $\chi^2$ law mass functions}
	\end{figure}
	and a plot example of the chi-squared distribution and cumulative distribution for the parameter $k=2$ (for more plots about this distribution see further below on page \pageref{continuous distributions}):
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/law_chi2.jpg}
		\caption{$\chi^2$ law (mass and cumulative distribution function) }
	\end{figure}
	In the literature, it is traditional to write:
	
	to indicate that the distribution of the random variable $X$ is a chi-square distribution. Furthermore it is common to name the parameter $k$ "\NewTerm{degrees of freedom}\index{degrees of freedom (statistics)}" and abbreviate it "$\text{df}$".
	
	The $\chi^2$ distribution is therefore a special case of the Gamma distribution and by taking $k=2$ we also find the exponential distribution (see above) for $\lambda=1/2$:
	
	Moreover, since (\SeeChapter{see section Differential and Integral Calculus page \pageref{gamma euler function}}):
	
	the $\chi^2$ distribution with $k$ equal to unity can be written as:
	
	Finally, let us finish with a fairly large property in the field of statistical tests that we will investigate a little further and particularly for confidence intervals of rare events and the famous Fisher method for multiple $p$-value null hypothesis tests. Indeed, the reader can check in a spreadsheet software like Microsoft Excel 14.0.6123 that we have:
	\begin{tabbing}
		\= \\
		\>\texttt{=POISSON.DIST(} $x \in \mathbb{N},\mu,$\texttt{TRUE)}\\
		\>\texttt{=1-CHISQ.DIST(} $2\mu,2(x+1),$\texttt{TRUE)}\\
		\>\texttt{=1-GAMMA.DIST(} $2\mu,x+1,$\texttt{TRUE)}\\
		\>\texttt{=1-EXPON.DIST(} $x,0.5,$\texttt{TRUE)}
	\end{tabbing}
	So we need to prove this relation between law $\chi^2$ and Poisson distributions. See it starting from the Gamma distribution:
	
	If we write $\lambda=1/2$ and $a=k/2$ then we have the $\chi^2$ distribution with $k$ degrees of freedom:
	
	Now remember that we have seen in the section Sequences and Series (page \pageref{usual maclaurin developments}), the following Taylor (Maclaurin) series from order $n - 1$ around $0$ to $\lambda$ with integral rest:
	
	We multiply by $e^{-\lambda}$:
	
	And therefore:
	
	Now, let us focus on the term:
	
	and make a first change of variable:
	
	and a second change of variable (caution! the $k$ in the change of variable is not the same as this in the Poisson sum...):
	
	However, we have shown in the section of Differential and Integral Calculus (page \pageref{gamma euler function}) that if $x$ is a positive integer:
	
	Then it comes:
	
	Finally we have:
	
	where we find out the chi-squared distribution under the integral! So at the end:
	
	This explains the formulas given above for the spreadsheet software, remembering that we choose to write:
	
	
	Finally for people that don't have an access to spreadsheet or statistical software:
	\begin{center}
		\begin{tabular}
		      {r@{\ }r@{\ }r@{\ }r@{\ }r@{\ }r@{\ }r@{\ }r@{\ }r@{\ }r@{\ }r@{\ }r}
		\multicolumn{12}{c}{CHI-SQUARED PERCENTAGE POINTS}\\
		\ \\
		$k$&0.1\%&0.5\%&1.0\%&2.5\%&5.0\%&10.0\%&12.5\%&20.0\%&25.0\%&33.3\%&50.0\%\\
		\ \\
		 1&0.000&0.000&0.000&0.001&0.004&0.016&0.025&0.064&0.102&0.186&0.455\\
		 2&0.002&0.010&0.020&0.051&0.103&0.211&0.267&0.446&0.575&0.811&1.386\\
		 3&0.024&0.072&0.115&0.216&0.352&0.584&0.692&1.005&1.213&1.568&2.366\\
		 4&0.091&0.207&0.297&0.484&0.711&1.064&1.219&1.649&1.923&2.378&3.357\\
		 5&0.210&0.412&0.554&0.831&1.145&1.610&1.808&2.343&2.675&3.216&4.351\\
		 6&0.381&0.676&0.872&1.237&1.635&2.204&2.441&3.070&3.455&4.074&5.348\\
		 7&0.598&0.989&1.239&1.690&2.167&2.833&3.106&3.822&4.255&4.945&6.346\\
		 8&0.857&1.344&1.646&2.180&2.733&3.490&3.797&4.594&5.071&5.826&7.344\\
		 9&1.152&1.735&2.088&2.700&3.325&4.168&4.507&5.380&5.899&6.716&8.343\\
		10&1.479&2.156&2.558&3.247&3.940&4.865&5.234&6.179&6.737&7.612&9.342\\
		11&1.834&2.603&3.053&3.816&4.575&5.578&5.975&6.989&7.584&8.514&10.341\\
		12&2.214&3.074&3.571&4.404&5.226&6.304&6.729&7.807&8.438&9.420&11.340\\
		13&2.617&3.565&4.107&5.009&5.892&7.042&7.493&8.634&9.299&10.331&12.340\\
		14&3.041&4.075&4.660&5.629&6.571&7.790&8.266&9.467&10.165&11.245&13.339\\
		15&3.483&4.601&5.229&6.262&7.261&8.547&9.048&10.307&11.037&12.163&14.339\\
		16&3.942&5.142&5.812&6.908&7.962&9.312&9.837&11.152&11.912&13.083&15.338\\
		17&4.416&5.697&6.408&7.564&8.672&10.085&10.633&12.002&12.792&14.006&16.338\\
		18&4.905&6.265&7.015&8.231&9.390&10.865&11.435&12.857&13.675&14.931&17.338\\
		19&5.407&6.844&7.633&8.907&10.117&11.651&12.242&13.716&14.562&15.859&18.338\\
		20&5.921&7.434&8.260&9.591&10.851&12.443&13.055&14.578&15.452&16.788&19.337\\
		21&6.447&8.034&8.897&10.283&11.591&13.240&13.873&15.445&16.344&17.720&20.337\\
		22&6.983&8.643&9.542&10.982&12.338&14.041&14.695&16.314&17.240&18.653&21.337\\
		23&7.529&9.260&10.196&11.689&13.091&14.848&15.521&17.187&18.137&19.587&22.337\\
		24&8.085&9.886&10.856&12.401&13.848&15.659&16.351&18.062&19.037&20.523&23.337\\
		25&8.649&10.520&11.524&13.120&14.611&16.473&17.184&18.940&19.939&21.461&24.337\\
		26&9.222&11.160&12.198&13.844&15.379&17.292&18.021&19.820&20.843&22.399&25.336\\
		27&9.803&11.808&12.879&14.573&16.151&18.114&18.861&20.703&21.749&23.339&26.336\\
		28&10.391&12.461&13.565&15.308&16.928&18.939&19.704&21.588&22.657&24.280
		  &27.336\\
		29&10.986&13.121&14.256&16.047&17.708&19.768&20.550&22.475&23.567&25.222
		  &28.336\\
		30&11.588&13.787&14.953&16.791&18.493&20.599&21.399&23.364&24.478&26.165
		  &29.336\\
		35&14.688&17.192&18.509&20.569&22.465&24.797&25.678&27.836&29.054&30.894
		  &34.336\\
		40&17.916&20.707&22.164&24.433&26.509&29.051&30.008&32.345&33.660&35.643
		  &39.335\\
		45&21.251&24.311&25.901&28.366&30.612&33.350&34.379&36.884&38.291&40.407
		  &44.335\\
		50&24.674&27.991&29.707&32.357&34.764&37.689&38.785&41.449&42.942&45.184
		  &49.335\\
		55&28.173&31.735&33.570&36.398&38.958&42.060&43.220&46.036&47.610&49.972
		  &54.335\\
		60&31.738&35.534&37.485&40.482&43.188&46.459&47.680&50.641&52.294&54.770
		  &59.335
		\end{tabular}
		\end{center}
		
		\newpage
		
		\begin{center}
		\begin{tabular}
		      {r@{\ }r@{\ }r@{\ }r@{\ }r@{\ }r@{\ }r@{\ }r@{\ }r@{\ }r@{\ }r@{\ }r}
		\multicolumn{12}{c}{CHI-SQUARED PERCENTAGE POINTS}\\
		\ \\
		$k$&60.0\%&66.7\%&75.0\%&80.0\%&87.5\%&90.0\%&95.0\%&97.5\%&99.0\%&99.5\%
		     &99.9\%\\
		\ \\
		1&0.708&0.936&1.323&1.642&2.354&2.706&3.841&5.024&6.635&7.879&10.828\\
		2&1.833&2.197&2.773&3.219&4.159&4.605&5.991&7.378&9.210&10.597&13.816\\
		3&2.946&3.405&4.108&4.642&5.739&6.251&7.815&9.348&11.345&12.838&16.266\\
		4&4.045&4.579&5.385&5.989&7.214&7.779&9.488&11.143&13.277&14.860&18.467\\
		5&5.132&5.730&6.626&7.289&8.625&9.236&11.070&12.833&15.086&16.750&20.515\\
		6&6.211&6.867&7.841&8.558&9.992&10.645&12.592&14.449&16.812&18.548&22.458\\
		7&7.283&7.992&9.037&9.803&11.326&12.017&14.067&16.013&18.475&20.278&24.322\\
		8&8.351&9.107&10.219&11.030&12.636&13.362&15.507&17.535&20.090&21.955&26.125\\
		9&9.414&10.215&11.389&12.242&13.926&14.684&16.919&19.023&21.666&23.589
		 &27.877\\
		10&10.473&11.317&12.549&13.442&15.198&15.987&18.307&20.483&23.209&25.188
		  &29.588\\
		11&11.530&12.414&13.701&14.631&16.457&17.275&19.675&21.920&24.725&26.757
		  &31.264\\
		12&12.584&13.506&14.845&15.812&17.703&18.549&21.026&23.337&26.217&28.300
		  &32.910\\
		13&13.636&14.595&15.984&16.985&18.939&19.812&22.362&24.736&27.688&29.819
		  &34.528\\
		14&14.685&15.680&17.117&18.151&20.166&21.064&23.685&26.119&29.141&31.319
		  &36.123\\
		15&15.733&16.761&18.245&19.311&21.384&22.307&24.996&27.488&30.578&32.801
		  &37.697\\
		16&16.780&17.840&19.369&20.465&22.595&23.542&26.296&28.845&32.000&34.267
		  &39.252\\
		17&17.824&18.917&20.489&21.615&23.799&24.769&27.587&30.191&33.409&35.718
		  &40.790\\
		18&18.868&19.991&21.605&22.760&24.997&25.989&28.869&31.526&34.805&37.156
		  &42.312\\
		19&19.910&21.063&22.718&23.900&26.189&27.204&30.144&32.852&36.191&38.582
		  &43.820\\
		20&20.951&22.133&23.828&25.038&27.376&28.412&31.410&34.170&37.566&39.997
		  &45.315\\
		21&21.991&23.201&24.935&26.171&28.559&29.615&32.671&35.479&38.932&41.401
		  &46.797\\
		22&23.031&24.268&26.039&27.301&29.737&30.813&33.924&36.781&40.289&42.796
		  &48.268\\
		23&24.069&25.333&27.141&28.429&30.911&32.007&35.172&38.076&41.638&44.181
		  &49.728\\
		24&25.106&26.397&28.241&29.553&32.081&33.196&36.415&39.364&42.980&45.559
		  &51.179\\
		25&26.143&27.459&29.339&30.675&33.247&34.382&37.652&40.646&44.314&46.928
		  &52.620\\
		26&27.179&28.520&30.435&31.795&34.410&35.563&38.885&41.923&45.642&48.290
		  &54.052\\
		27&28.214&29.580&31.528&32.912&35.570&36.741&40.113&43.195&46.963&49.645
		  &55.476\\
		28&29.249&30.639&32.620&34.027&36.727&37.916&41.337&44.461&48.278&50.993
		  &56.892\\
		29&30.283&31.697&33.711&35.139&37.881&39.087&42.557&45.722&49.588&52.336
		  &58.301\\
		30&31.316&32.754&34.800&36.250&39.033&40.256&43.773&46.979&50.892&53.672
		  &59.703\\
		35&36.475&38.024&40.223&41.778&44.753&46.059&49.802&53.203&57.342&60.275
		  &66.619\\
		40&41.622&43.275&45.616&47.269&50.424&51.805&55.758&59.342&63.691&66.766
		  &73.402\\
		45&46.761&48.510&50.985&52.729&56.052&57.505&61.656&65.410&69.957&73.166
		  &80.077\\
		50&51.892&53.733&56.334&58.164&61.647&63.167&67.505&71.420&76.154&79.490
		  &86.661\\
		55&57.016&58.945&61.665&63.577&67.211&68.796&73.311&77.380&82.292&85.749
		  &93.168\\
		60&62.135&64.147&66.981&68.972&72.751&74.397&79.082&83.298&88.379&91.952
		  &99.607
		\end{tabular}
	\end{center} 
	
	\paragraph{Noncentral chi-square distribution}\label{noncentral chi-square distribution}\mbox{}\\\\
	The "\NewTerm{noncentral chi-squared distribution}\index{noncentral chi-square distribution}" can be found in two major situation in industrial statistics (and hence in business):
	\begin{itemize}
		\item In the lower and upper bound estimations of tolerance interval for an univariate Normal distribution
		
		\item As the distribution of the alternative hypothesis ($H_1$) of many NHST tests and hence used to calculate the power of the corresponding tests.
	\end{itemize}
	Let us now determine the density probability function of the noncentered chi-square distribution.
	
	We know that the density probability function of $X^2=\mathcal{N}(0,1)^2$ follows by definition a $\chi^2$ distribution of one degree of freedom. The question now is what distribution follows $\mathcal{N}(\lambda,1)^2$ ??

	First we know that we can rewrite this as:
	
	Hence $T$ has distribution $\Phi(t)$ and density function (see earlier our study of the Normal distribution):
	
	The distribution of $X^2$ is obviously:
	
	Hence the density is:
	
	And we know (\SeeChapter{see section Sequences and Series page \pageref{euler maclaurin expansion}}) the following Maclaurin series:
	
	Hence:
	
	Now let us recall that if $K$ is a random variable with a Poisson distribution and mean $\lambda^2/2$, then its distribution is equal to:
	
	with $k=0,1,2,\ldots$.
	
	Therefore:
	
	Now let us recall that the chi-square distribution is given by:
	
	Hence:
	
	So:
	
	Therefore:
	
	Now let us recall that we have proved in the section Differential and Integral Calculus (see page \pageref{gamma euler function}) that:
	
	Therefore:
	
	So finally the noncentral chi-square distribution function with one degree of freedom is given by:
	
	Or explicitly:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	It can be proved (but so far we don't need this result actually anywhere in this book), that the general case with $n$ degrees of freedom is given by:
	
	\end{tcolorbox}
	That's actually all we need for our study later of tolerance intervals! So we will not derive the mean and variance of this distribution as is it would be useless in the actual state of this book!
	
	\subsubsection{Student Distribution}\label{student distribution}
	The "\NewTerm{Student distribution}\index{Student distribution}" (or "\NewTerm{Student's law}\index{Student's law}") of parameter $k$ is defined by the relation:
	
	with $k$ being the degree of freedom of the $\chi^2$ distribution underlying the construction of the Student function as we will see.

Let us indicate that this distribution can also be obtained in Microsoft Excel 11.8346 using the \texttt{TDIST( )} function and its inverse by \texttt{TINV()}.

It is indeed a distribution function because it also satisfies (remains to be proved directly, but as we will see it is the product of two distribution functions thus indirectly...):
	
	Let us see the easiest proof to justify the provenance of the Student distribution and that will also be very useful further in statistical inference and analysis of variance.
	
	For this proof, remember first that:
	\begin{enumerate}
		\item If $X$ and $Y $ are two independent random variables with respective densities $f_X,f_Y$, the distribution of the pair $(X, Y)$ has a density $f$ satisfying (axiom of probabilities!):
		
		
		\item The distribution $\mathcal{N}(0,1)$ is given by (see above):
		
		
		\item The distribution $\chi_n^2$ is given by (see above):
		
		for $y\geq 0$ and $n \geq 1$.
		
		\item The Euler function $\Gamma$ is defined for all $\alpha>0$ by (\SeeChapter{see section Differential and Integral Calculus page \pageref{gamma euler function}}):
		
		and satisfies (\SeeChapter{see section  Differential and Integral Calculus page \pageref{gamma euler function}}):
		
		for $\alpha\geq 2$.
	\end{enumerate}
	These reminders made, now consider a random variable $X$ that follows the distribution $\mathcal{N}(0,1)$ and $Y$ a random variable following the distribution $\chi_n^2$.
	
	We assume $X$ and $Y$ being independent and we consider the random variable (this is at the origin the historical study of the Student distribution in the framework of statistical inference which led to define this variable for which we will deepen the origin later):	
	
	We will prove that $T$ follows a Student distribution of parameter $n$.
	\begin{dem}
	Let $F$ and $f$ be respectively the repartition and density functions and $T$, $f_X,f_Y$ the density functions of $X, Y$ and $(X, Y)$ respectively. Then we have for all $t \in \mathbb{R}$:
	
	where: 
	
	the imposed positive and non-zero value and $y$ being due to the fact that it is under a root and furthermore at the denominator.
	
	Thus:
	
	where because $X$ follows a Normal $\mathcal{N}(0,1)$ distribution:
	
	is the Normal centered reduced cumulative distribution.
	
	Thus, we obtain the density distribution function of $T$ by deriving $F$:
	
	because (the derivative of a function is equal to its derivatives multiplied by its inner derivative):
	
	Therefore:
	
	By making the change of variable:
	
	we get:
	
	what is the Student distribution of parameter $n$.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Let us now prove what is the mean of the Student distribution:
		
	We have:
	
	But $\text{E}\left(\dfrac{1}{\sqrt{Y}}\right)$ exists if and only if $n\geq 2$. Effectively for $n=1$:
	
	and:
	
	Whereas for $n\geq 2$ we have:
	
	Thus, for $n=1$ the mean does not exist.
	
	So for $n\geq 2$:
	
	Now let us see the value of the variance. So we have:
	
	First we will discuss the existence of $\text{E}(T^2)$. We have trivially:
	
	$X$ follows a Normal centered reduced distribution thus:
	
	With regard to $\text{E}\left(\dfrac{1}{Y}\right)$ we have:
	
	where we made the change of variable $u=y/2$.
	
	But the integral defining $\Gamma\left(\dfrac{n}{2}-1\right)$ converges only if $n\geq 3$.
	
	Therefore $\text{E}(T^2)$ exists if and only if $n\geq 3$ so it's value is according to the properties of the Euler Gamma function demonstrated in the chapter of Differential and Integral Calculus:
	
	Therefore for $n\geq 3$:
	
	It is also important to note that this law is symmetrical about $0$!

	Here is a plot example of the Student distribution and cumulative distribution for the parameter $k=3$ (for more plots about this distribution see further below on page \pageref{continuous distributions}):
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/law_student.jpg}
		\caption{Student $T$ law (mass and cumulative distribution function)}
	\end{figure}
	
	\newpage
	Finally for people that don't have an access to spreadsheet or statistical software:
	\begin{center}
		\begin{tabular}
		      {r@{\quad}r@{\,}r@{\,}r@{\,}r@{\,}r@{\,}r@{\,}r@{\,}r@{\,}r@{\,}r@{\,}r}
		\multicolumn{12}{c}{STUDENT'S $T$ PERCENTAGE POINTS} \\
		\ \\
		$k$&60.0\%&66.7\%&75.0\%&80.0\%&87.5\%&90.0\%&95.0\%&97.5\%&99.0\%&99.5\%
		     &99.9\% \\
		\ \\
		 1&0.325&0.577&1.000&1.376&2.414&3.078&6.314&12.706&31.821&63.657&318.31 \\
		 2&0.289&0.500&0.816&1.061&1.604&1.886&2.920&4.303&6.965&9.925&22.327 \\
		 3&0.277&0.476&0.765&0.978&1.423&1.638&2.353&3.182&4.541&5.841&10.215 \\
		 4&0.271&0.464&0.741&0.941&1.344&1.533&2.132&2.776&3.747&4.604&7.173 \\
		 5&0.267&0.457&0.727&0.920&1.301&1.476&2.015&2.571&3.365&4.032&5.893 \\
		 6&0.265&0.453&0.718&0.906&1.273&1.440&1.943&2.447&3.143&3.707&5.208 \\
		 7&0.263&0.449&0.711&0.896&1.254&1.415&1.895&2.365&2.998&3.499&4.785 \\
		 8&0.262&0.447&0.706&0.889&1.240&1.397&1.860&2.306&2.896&3.355&4.501 \\
		 9&0.261&0.445&0.703&0.883&1.230&1.383&1.833&2.262&2.821&3.250&4.297 \\
		10&0.260&0.444&0.700&0.879&1.221&1.372&1.812&2.228&2.764&3.169&4.144 \\
		11&0.260&0.443&0.697&0.876&1.214&1.363&1.796&2.201&2.718&3.106&4.025 \\
		12&0.259&0.442&0.695&0.873&1.209&1.356&1.782&2.179&2.681&3.055&3.930 \\
		13&0.259&0.441&0.694&0.870&1.204&1.350&1.771&2.160&2.650&3.012&3.852 \\
		14&0.258&0.440&0.692&0.868&1.200&1.345&1.761&2.145&2.624&2.977&3.787 \\
		15&0.258&0.439&0.691&0.866&1.197&1.341&1.753&2.131&2.602&2.947&3.733 \\
		16&0.258&0.439&0.690&0.865&1.194&1.337&1.746&2.120&2.583&2.921&3.686 \\
		17&0.257&0.438&0.689&0.863&1.191&1.333&1.740&2.110&2.567&2.898&3.646 \\
		18&0.257&0.438&0.688&0.862&1.189&1.330&1.734&2.101&2.552&2.878&3.610 \\
		19&0.257&0.438&0.688&0.861&1.187&1.328&1.729&2.093&2.539&2.861&3.579 \\
		20&0.257&0.437&0.687&0.860&1.185&1.325&1.725&2.086&2.528&2.845&3.552 \\
		21&0.257&0.437&0.686&0.859&1.183&1.323&1.721&2.080&2.518&2.831&3.527 \\
		22&0.256&0.437&0.686&0.858&1.182&1.321&1.717&2.074&2.508&2.819&3.505 \\
		23&0.256&0.436&0.685&0.858&1.180&1.319&1.714&2.069&2.500&2.807&3.485 \\
		24&0.256&0.436&0.685&0.857&1.179&1.318&1.711&2.064&2.492&2.797&3.467 \\
		25&0.256&0.436&0.684&0.856&1.178&1.316&1.708&2.060&2.485&2.787&3.450 \\
		26&0.256&0.436&0.684&0.856&1.177&1.315&1.706&2.056&2.479&2.779&3.435 \\
		27&0.256&0.435&0.684&0.855&1.176&1.314&1.703&2.052&2.473&2.771&3.421 \\
		28&0.256&0.435&0.683&0.855&1.175&1.313&1.701&2.048&2.467&2.763&3.408 \\
		29&0.256&0.435&0.683&0.854&1.174&1.311&1.699&2.045&2.462&2.756&3.396 \\
		30&0.256&0.435&0.683&0.854&1.173&1.310&1.697&2.042&2.457&2.750&3.385 \\
		35&0.255&0.434&0.682&0.852&1.170&1.306&1.690&2.030&2.438&2.724&3.340 \\
		40&0.255&0.434&0.681&0.851&1.167&1.303&1.684&2.021&2.423&2.704&3.307 \\
		45&0.255&0.434&0.680&0.850&1.165&1.301&1.679&2.014&2.412&2.690&3.281 \\
		50&0.255&0.433&0.679&0.849&1.164&1.299&1.676&2.009&2.403&2.678&3.261 \\
		55&0.255&0.433&0.679&0.848&1.163&1.297&1.673&2.004&2.396&2.668&3.245 \\
		60&0.254&0.433&0.679&0.848&1.162&1.296&1.671&2.000&2.390&2.660&3.232 \\
		$\infty$
		  &0.253&0.431&0.674&0.842&1.150&1.282&1.645&1.960&2.326&2.576&3.090
		\end{tabular}
	\end{center}
	
	\subsubsection{Fisher Distribution}
	The "\NewTerm{Fisher distribution}\index{Fisher distribution}" (or "\NewTerm{Fisher-Snedecor distribution}\index{Fisher-Snedecor distribution}") of parameters $k$ and $l$ is defined by the relation:
	
	if $x\geq 0$ (see the plot further below on page \pageref{continuous distributions}). The parameters $k$ and $l$ are positive integers and correspond to the two degrees of freedom of the underlying chi-square distributions. This distribution is often denoted by $F_{k,l}$ or by $F(k, l)$ and can be obtained in Microsoft Excel 11.8346 with the \texttt{FDIST( )} function.
	
	It is indeed a distribution function because it satisfies the property (remains to us to prove that directly but as we will see, it is the product of two distribution functions therefore indirectly...):
	
	Let us see the easiest proof to justify the provenance of the Fisher distribution and that we will be us also very useful further in statistical inference and analysis of variance.

	For this proof, recall that:
	\begin{enumerate}
		\item The distribution $\chi_n^2$ is given by (see above):
		
		for $y\geq 0$ and $n\geq 1$.
		\item The Euler Gamma function $\Gamma$ is defined for all $\alpha>0$ by (\SeeChapter{see section Differential and Integral Calculus page \pageref{gamma euler function}}):
		
		Let $X, Y$ be two independent random variables following respectively the distributions $\chi_n^2$ and $\chi_m^2$.
	\end{enumerate}
	We consider the random variable:
	
	We will prove that the distribution of $T$ is the Fisher-Snedecor distribution of parameters $n, m$.
	
	Let us note for this purpose $F$ and $f$ the cumulative and density function of $T$ and $f_X,f_Y,f$ density functions of $X, Y$ and respectively $(X, Y)$. We have for all $t\in \mathbb{R}$:
	
	where:
	
	where the imposed positive values comes in fact that behind them there is a chi-square for $x$ and $y$.
	Therefore:
	
	We obtain the density function of $T$ by deriving $F$. First the inner derivative:
	
	Then explicitly because:
	
	we then have:
	
	By making the change of variable:
	
	we get:
	
	
	\pagebreak
	For the people that may not have access to a spreadsheet or statistical software here are some useful tables:
	\begin{center}
		\begin{tabular}{rrr@{\,}r@{\,}r@{\,}r@{\,}r@{\,}r@{\,}r@{\,}r
		                   @{\,}r@{\,}r@{\,}r@{\,}r@{\,}r@{\,}r@{\,}r}
		&&\multicolumn{14}{c}{PERCENTAGE POINTS OF THE $F$ DISTRIBUTION}\\
		\ \\
		$\nu_2\backslash\nu_1$ & & 
		\multicolumn{1}{c}{2} &\multicolumn{1}{c}{3} &
		\multicolumn{1}{c}{4} &\multicolumn{1}{c}{5} &
		\multicolumn{1}{c}{6} &\multicolumn{1}{c}{7} &
		\multicolumn{1}{c}{8} &\multicolumn{1}{c}{10}&
		\multicolumn{1}{c}{12}&\multicolumn{1}{c}{15}&
		\multicolumn{1}{c}{20}&\multicolumn{1}{c}{30}&
		\multicolumn{1}{c}{50}&\multicolumn{1}{c}{$\infty$}\\
		& $q$ \\
		1&0.900&49.5&53.6&55.8&57.2&58.2&59.1&59.7&60.5&61.0&61.5&62.0&62.6&63.0&63.3\\
		 &0.950&199.&216.&225.&230.&234.&237.&239.&242.&244.&246.&248.&250.&252.&254.\\
		 &0.975&800.&864.&900.&922.&937.&948.&957.&969.&977.&985.&993.\\
		 &0.990\\
		 &0.999\\
		2&0.900&9.00&9.16&9.24&9.29&9.33&9.35&9.37&9.39&9.41&9.43&9.44&9.46&9.47&9.49\\ 
		 &0.950&19.0&19.2&19.2&19.3&19.3&19.4&19.4&19.4&19.4&19.4&19.4&19.5&19.5&19.5\\
		 &0.975&39.0&39.2&39.2&39.3&39.3&39.4&39.4&39.4&39.4&39.4&39.4&39.5&39.5&39.5\\
		 &0.990&99.0&99.2&99.2&99.3&99.3&99.4&100.&100.&100.&100.&100.&100.&100.&99.5\\
		 &0.999&999.&999.\\
		3&0.900&5.46&5.39&5.34&5.31&5.28&5.27&5.25&5.23&5.22&5.20&5.18&5.17&5.15&5.13\\
		 &0.950&9.55&9.28&9.12&9.01&8.94&8.89&8.85&8.79&8.74&8.70&8.66&8.62&8.58&8.53\\
		 &0.975&16.0&15.4&15.1&14.9&14.7&14.6&14.5&14.4&14.3&14.3&14.2&14.1&14.0&13.9\\
		 &0.990&30.8&29.5&28.7&28.2&27.9&27.7&27.5&27.2&27.1&26.9&26.7&26.5&26.4&26.1\\
		 &0.999&149.&141.&137.&135.&133.&132.&131.&129.&128.&127.&126.&125.&125.&123.\\
		4&0.900&4.32&4.19&4.11&4.05&4.01&3.98&3.95&3.92&3.90&3.87&3.84&3.82&3.79&3.76\\
		 &0.950&6.94&6.59&6.39&6.26&6.16&6.09&6.04&5.96&5.91&5.86&5.80&5.75&5.70&5.63\\
		 &0.975&10.6&9.98&9.60&9.36&9.20&9.07&8.98&8.84&8.75&8.66&8.56&8.46&8.38&8.26\\
		 &0.990&18.0&16.7&16.0&15.5&15.2&15.0&14.8&14.5&14.4&14.2&14.0&13.8&13.7&13.5\\
		 &0.999&61.2&56.2&53.4&51.7&50.5&49.7&49.0&48.0&47.4&46.8&46.1&45.4&44.9&44.1\\
		5&0.900&3.78&3.62&3.52&3.45&3.40&3.37&3.34&3.30&3.27&3.24&3.21&3.17&3.15&3.10\\
		 &0.950&5.79&5.41&5.19&5.05&4.95&4.88&4.82&4.74&4.68&4.62&4.56&4.50&4.44&4.36\\
		 &0.975&8.43&7.76&7.39&7.15&6.98&6.85&6.76&6.62&6.52&6.43&6.33&6.23&6.14&6.02\\
		 &0.990&13.3&12.1&11.4&11.0&10.7&10.5&10.3&10.1&9.89&9.72&9.55&9.38&9.24&9.02\\
		 &0.999&37.1&33.2&31.1&29.8&28.8&28.2&27.6&26.9&26.4&25.9&25.4&24.9&24.4&23.8\\
		6&0.900&3.46&3.29&3.18&3.11&3.05&3.01&2.98&2.94&2.90&2.87&2.84&2.80&2.77&2.72\\
		 &0.950&5.14&4.76&4.53&4.39&4.28&4.21&4.15&4.06&4.00&3.94&3.87&3.81&3.75&3.67\\
		 &0.975&7.26&6.60&6.23&5.99&5.82&5.70&5.60&5.46&5.37&5.27&5.17&5.07&4.98&4.85\\
		 &0.990&10.9&9.78&9.15&8.75&8.47&8.26&8.10&7.87&7.72&7.56&7.40&7.23&7.09&6.88\\
		 &0.999&27.0&23.7&21.9&20.8&20.0&19.5&19.0&18.4&18.0&17.6&17.1&16.7&16.3&15.7\\
		7&0.900&3.26&3.07&2.96&2.88&2.83&2.78&2.75&2.70&2.67&2.63&2.59&2.56&2.52&2.47\\
		 &0.950&4.74&4.35&4.12&3.97&3.87&3.79&3.73&3.64&3.57&3.51&3.44&3.38&3.32&3.23\\
		 &0.975&6.54&5.89&5.52&5.29&5.12&4.99&4.90&4.76&4.67&4.57&4.47&4.36&4.28&4.14\\
		 &0.990&9.55&8.45&7.85&7.46&7.19&6.99&6.84&6.62&6.47&6.31&6.16&5.99&5.86&5.65\\
		 &0.999&21.7&18.8&17.2&16.2&15.5&15.0&14.6&14.1&13.7&13.3&12.9&12.5&12.2&11.7\\
		8&0.900&3.11&2.92&2.81&2.73&2.67&2.62&2.59&2.54&2.50&2.46&2.42&2.38&2.35&2.29\\
		 &0.950&4.46&4.07&3.84&3.69&3.58&3.50&3.44&3.35&3.28&3.22&3.15&3.08&3.02&2.93\\
		 &0.975&6.06&5.42&5.05&4.82&4.65&4.53&4.43&4.29&4.20&4.10&4.00&3.89&3.81&3.67\\
		 &0.990&8.65&7.59&7.01&6.63&6.37&6.18&6.03&5.81&5.67&5.52&5.36&5.20&5.07&4.86\\
		 &0.999&18.5&15.8&14.4&13.5&12.9&12.4&12.0&11.5&11.2&10.8&10.5&10.1&9.80&9.33
		\end{tabular}
		\end{center}
		
		\newpage
		
		\begin{center}
		\begin{tabular}{rrr@{\,}r@{\,}r@{\,}r@{\,}r@{\,}r@{\,}r@{\,}r
		                   @{\,}r@{\,}r@{\,}r@{\,}r@{\,}r@{\,}r@{\,}r}
		&&\multicolumn{14}{c}{PERCENTAGE POINTS OF THE $F$ DISTRIBUTION}\\
		\ \\
		$\nu_2\backslash\nu_1$ & & 
		\multicolumn{1}{c}{2} &\multicolumn{1}{c}{3} &
		\multicolumn{1}{c}{4} &\multicolumn{1}{c}{5} &
		\multicolumn{1}{c}{6} &\multicolumn{1}{c}{7} &
		\multicolumn{1}{c}{8} &\multicolumn{1}{c}{10}&
		\multicolumn{1}{c}{12}&\multicolumn{1}{c}{15}&
		\multicolumn{1}{c}{20}&\multicolumn{1}{c}{30}&
		\multicolumn{1}{c}{50}&\multicolumn{1}{c}{$\infty$}\\
		& $q$ \\
		 9&0.900&3.01&2.81&2.69&2.61&2.55&2.51&2.47&2.42&2.38&2.34&2.30&2.25&2.22&2.16\\
		  &0.950&4.26&3.86&3.63&3.48&3.37&3.29&3.23&3.14&3.07&3.01&2.94&2.86&2.80&2.71\\
		  &0.975&5.71&5.08&4.72&4.48&4.32&4.20&4.10&3.96&3.87&3.77&3.67&3.56&3.47&3.33\\
		  &0.990&8.02&6.99&6.42&6.06&5.80&5.61&5.47&5.26&5.11&4.96&4.81&4.65&4.52&4.31\\
		  &0.999&16.4&13.9&12.6&11.7&11.1&10.7&10.4&9.89&9.57&9.24&8.90&8.55&8.26&7.81\\
		10&0.900&2.92&2.73&2.61&2.52&2.46&2.41&2.38&2.32&2.28&2.24&2.20&2.16&2.12&2.06\\
		  &0.950&4.10&3.71&3.48&3.33&3.22&3.14&3.07&2.98&2.91&2.84&2.77&2.70&2.64&2.54\\
		  &0.975&5.46&4.83&4.47&4.24&4.07&3.95&3.85&3.72&3.62&3.52&3.42&3.31&3.22&3.08\\
		  &0.990&7.56&6.55&5.99&5.64&5.39&5.20&5.06&4.85&4.71&4.56&4.41&4.25&4.11&3.91\\
		  &0.999&14.9&12.6&11.3&10.5&9.93&9.52&9.20&8.75&8.45&8.13&7.80&7.47&7.19&6.76\\
		11&0.900&2.86&2.66&2.54&2.45&2.39&2.34&2.30&2.25&2.21&2.17&2.12&2.08&2.04&1.97\\
		  &0.950&3.98&3.59&3.36&3.20&3.09&3.01&2.95&2.85&2.79&2.72&2.65&2.57&2.51&2.40\\
		  &0.975&5.26&4.63&4.28&4.04&3.88&3.76&3.66&3.53&3.43&3.33&3.23&3.12&3.03&2.88\\
		  &0.990&7.21&6.22&5.67&5.32&5.07&4.89&4.74&4.54&4.40&4.25&4.10&3.94&3.81&3.60\\
		  &0.999&13.8&11.6&10.3&9.58&9.05&8.66&8.35&7.92&7.63&7.32&7.01&6.68&6.42&6.00\\
		12&0.900&2.81&2.61&2.48&2.39&2.33&2.28&2.24&2.19&2.15&2.10&2.06&2.01&1.97&1.90\\
		  &0.950&3.89&3.49&3.26&3.11&3.00&2.91&2.85&2.75&2.69&2.62&2.54&2.47&2.40&2.30\\
		  &0.975&5.10&4.47&4.12&3.89&3.73&3.61&3.51&3.37&3.28&3.18&3.07&2.96&2.87&2.72\\
		  &0.990&6.93&5.95&5.41&5.06&4.82&4.64&4.50&4.30&4.16&4.01&3.86&3.70&3.57&3.36\\
		  &0.999&13.0&10.8&9.63&8.89&8.38&8.00&7.71&7.29&7.00&6.71&6.40&6.09&5.83&5.42\\
		13&0.900&2.76&2.56&2.43&2.35&2.28&2.23&2.20&2.14&2.10&2.05&2.01&1.96&1.92&1.85\\
		  &0.950&3.81&3.41&3.18&3.03&2.92&2.83&2.77&2.67&2.60&2.53&2.46&2.38&2.31&2.21\\
		  &0.975&4.97&4.35&4.00&3.77&3.60&3.48&3.39&3.25&3.15&3.05&2.95&2.84&2.74&2.60\\
		  &0.990&6.70&5.74&5.21&4.86&4.62&4.44&4.30&4.10&3.96&3.82&3.66&3.51&3.37&3.17\\
		  &0.999&12.3&10.2&9.07&8.35&7.86&7.49&7.21&6.80&6.52&6.23&5.93&5.63&5.37&4.97\\
		14&0.900&2.73&2.52&2.39&2.31&2.24&2.19&2.15&2.10&2.05&2.01&1.96&1.91&1.87&1.80\\
		  &0.950&3.74&3.34&3.11&2.96&2.85&2.76&2.70&2.60&2.53&2.46&2.39&2.31&2.24&2.13\\
		  &0.975&4.86&4.24&3.89&3.66&3.50&3.38&3.29&3.15&3.05&2.95&2.84&2.73&2.64&2.49\\
		  &0.990&6.51&5.56&5.04&4.69&4.46&4.28&4.14&3.94&3.80&3.66&3.51&3.35&3.22&3.00\\
		  &0.999&11.8&9.73&8.62&7.92&7.44&7.08&6.80&6.40&6.13&5.85&5.56&5.25&5.00&4.60\\
		15&0.900&2.70&2.49&2.36&2.27&2.21&2.16&2.12&2.06&2.02&1.97&1.92&1.87&1.83&1.76\\
		  &0.950&3.68&3.29&3.06&2.90&2.79&2.71&2.64&2.54&2.48&2.40&2.33&2.25&2.18&2.07\\
		  &0.975&4.77&4.15&3.80&3.58&3.41&3.29&3.20&3.06&2.96&2.86&2.76&2.64&2.55&2.40\\
		  &0.990&6.36&5.42&4.89&4.56&4.32&4.14&4.00&3.80&3.67&3.52&3.37&3.21&3.08&2.87\\
		  &0.999&11.3&9.34&8.25&7.57&7.09&6.74&6.47&6.08&5.81&5.53&5.25&4.95&4.70&4.31\\
		16&0.900&2.67&2.46&2.33&2.24&2.18&2.13&2.09&2.03&1.99&1.94&1.89&1.84&1.79&1.72\\
		  &0.950&3.63&3.24&3.01&2.85&2.74&2.66&2.59&2.49&2.42&2.35&2.28&2.19&2.12&2.01\\
		  &0.975&4.69&4.08&3.73&3.50&3.34&3.22&3.12&2.99&2.89&2.79&2.68&2.57&2.47&2.32\\
		  &0.990&6.23&5.29&4.77&4.44&4.20&4.03&3.89&3.69&3.55&3.41&3.26&3.10&2.97&2.75\\
		  &0.999&11.0&9.01&7.94&7.27&6.80&6.46&6.19&5.81&5.55&5.27&4.99&4.70&4.45&4.06\\
		17&0.900&2.64&2.44&2.31&2.22&2.15&2.10&2.06&2.00&1.96&1.91&1.86&1.81&1.76&1.69\\
		  &0.950&3.59&3.20&2.96&2.81&2.70&2.61&2.55&2.45&2.38&2.31&2.23&2.15&2.08&1.96\\
		  &0.975&4.62&4.01&3.66&3.44&3.28&3.16&3.06&2.92&2.82&2.72&2.62&2.50&2.41&2.25\\
		  &0.990&6.11&5.18&4.67&4.34&4.10&3.93&3.79&3.59&3.46&3.31&3.16&3.00&2.87&2.65\\
		  &0.999&10.7&8.73&7.68&7.02&6.56&6.22&5.96&5.58&5.32&5.05&4.77&4.48&4.24&3.85
		\end{tabular}
	\end{center}
	
	\subsubsection{General Folded Normal Distribution}
	The "\NewTerm{folded Normal distribution}\index{folded Normal distribution}" is the distribution of the absolute value of a random variable with a Normal distribution\footnote{The majority of the text below comes from \url{http://www.math.uah.edu/stat/}}. As we have mentioned before, the Normal distribution is perhaps the most important in probability and is used to model an incredible variety of random phenomena. Since one may only be interested in the magnitude of a Normally distributed variable, the folded Normal arises in a very natural way especially in Finance and Industrial Engineering for Design of Experiments (\SeeChapter{see section Industrial Engineering page \pageref{doe}}). The name stems from the fact that the probability measure of the Normal distribution on $]-\infty, 0]$  is folded over to $[0, +\infty[$.
	
	\textbf{Definition (\#\thesection.\mydef):} Suppose that $X$ has a Normal distribution with mean $\mu\in\mathbb{R}$ and standard deviation $\sigma\in [0,+\infty[$. Then $Y=|X|$ has the fold normal distribution with parameters $\mu$ and $\sigma$.
	
	Suppose that $Z$ follows the standard Normal distribution. Let us recall that then $Z$ has probability density function $\phi$ and distribution function $\Phi$ given by:
	
	with $z \in \mathbb{R}$.
	
	If $\mu\in\mathbb{R}$ and $\sigma\in [0,+\infty[$, then $X=\mu +\sigma Z$ has the Normal distribution with mean $\mu$ and standard deviation $\sigma$, and therefore it is obvious at this level that:
	 
	has the folded Normal distribution with parameters $\mu$ and $\sigma$.

	Now let us determine the cumulative probability CDF function of such a variable! For $y\in[0,+\infty[$:
	 
	Since $\Phi(-Z)=1-\Phi(Z)$ we have:
	
	We cannot compute the quantile function $F^{-1}$ in closed form, but values of this function can be approximated. 
	
	It comes therefore immediately that $Y$ has probability density function  $f$ given by:
	 
	This follow from differentiating the CDF with respect to $y$ as we know!
	
	Now as always in this book we will focus only in what we need for the applications in the other chapters! So as we don't need the moments of the folder Normal distribution we will not calculate them. The only purpose of the above development were to build the tools to be able to introduce a special case of the folder Normal distribution.
	
	\paragraph{Half-Normal distribution}\label{half normal distribution}\mbox{}\\\\
	In probability theory and statistics, the "\NewTerm{half-Normal distribution}\index{half-Normal distribution}" is a special case of the folded Normal distribution.

	Let $X$ follow an ordinary Normal distribution, $\mathcal{N}(0,\sigma ^{2})$, then $Y=|X|$ follows a half-Normal distribution. Thus, the half-Normal distribution is a fold at the mean of an ordinary Normal distribution with mean $\mu=0$.
	
	Thus, let $Y=|\sigma Z|=\sigma |Z|$ where $Z$ has a standard Normal distribution and $\sigma\in [0,+\infty[$. Clearly $\sigma$ is a scale parameter, unlike the case for the general folded Normal distribution. The distribution of $Y$ when $\sigma=1$, $Y=|Z|$ has the "\NewTerm{standard half-Normal distribution}\index{standard half-Normal distribution}".
	
	As the half-Normal distribution is just a special case of the folded Normal distribution with $\mu=0$ it comes immediately:
	 
	with $y\in\mathbb{R}^+$ and sometimes denoted $\mathcal{HN}(0,\sigma^2)$.
	
	Now what interest us for the others chapters of this book (especially Design of Experiments) are the moments of that latter!
	
	To calculate them, first remember that:
	
	with $z\in\mathbb{R}$.	Therefore it is immediate that:
	
	Also remember that we have already proved that:
	
	
	We first need to determiner the moments for the Normal distribution! So for $n\in\mathbb{N}^+$:
	
	Now we integrate by parts (\SeeChapter{see section Differential and Integral Calculus page \pageref{integration by parts}}), with $u=z^n$ and $\mathrm{d}v=\phi'(z)\mathrm{d}z$ to get:
	
	Therefore for $n\in\mathbb{N}$, with $n> 1$:
	
	The moments of the standard Normal distribution are now easy to compute. First we know that:
	\begin{itemize}
		\item $\text{E}(Z)=0$
		\item $\text{E}(Z^2)=1$
	\end{itemize}
	Therefore:
	\begin{itemize}
		\item $\text{E}(Z)=0$
		\item $\text{E}(Z^2)=1$
		\item $\text{E}(Z^3)=\text{E}(Z^{2+1})=2\cdot\text{E}(Z^{2-1})=2\cdot\text{E}(Z)=0$
		\item $\text{E}(Z^4)=\text{E}(Z^{3+1})=3\cdot\text{E}(Z^{3-1})=3\cdot\text{E}(Z^2)=3\cdot 1=1\cdot 3=3$
		\item $\text{E}(Z^5)=\text{E}(Z^{4+1})=4\cdot\text{E}(Z^{4-1})=4\cdot\text{E}(Z^3)=4\cdot 0=0$
		\item $\text{E}(Z^6)=\text{E}(Z^{5+1})=5\cdot\text{E}(Z^{5-1})=5\cdot\text{E}(Z^4)=5\cdot 3=1\cdot 3\cdot 5=15$
		\item $\text{E}(Z^7)=\text{E}(Z^{6+1})=6\cdot\text{E}(Z^{6-1})=6\cdot\text{E}(Z^5)=6\cdot 0=0$
		\item $\text{E}(Z^8)=\text{E}(Z^{7+1})=7\cdot\text{E}(Z^{7-1})=7\cdot\text{E}(Z^6)=7\cdot 15=1\cdot 3\cdot 5\cdot 7$
		\item $\ldots$
	\end{itemize}
	Therefore we see that for the odd powers, that is to say $Z^{2n+1}$ with $n\in\mathbb{N}$ then:
	
	and for even powers:
	
	It follows for $X=\mathcal{N}(0,\sigma)$ (just check with the special case of $n=0$ and $n=1$):
	
	The moments of the half-Normal distribution can now be computed explicitly. 
	
	First it should be quite obvious by construction that the even order moments of $Y$ are the same as the even order moments of $Z$ (in both case the values are all positive and therefore equal). Hence:
	
	For the odd order moments we must use (see above):
	 
	with $x\in\mathbb{R}^+$. Therefore by definition:
	
	hence:
	
	Now we make the change of variable $u=y^2/(2\sigma^2)$, therefore first we have (this is obvious):
	
	and:
	
	Therefore we have so far:
	
	So finally:
	
	We recognize in this expression the Gamma Euler function integral (\SeeChapter{see section Differential and Integral Calculus page \pageref{gamma euler function}})! Therefore it is immediate that:
	
	So as summary:
	
	So finally we get the result we need for some properties of the Brownian motion in finance:
	
	But still one property is missing and now for our needs in the section of Industrial Engineering (page \pageref{doe}): the value of the Median!
	
	So let $M_e$ denote the median of the half-Normal distribution. Then by definition it follows that:
	
	Substituting $y/(\sqrt{2}\sigma)=u$ we have:
	
	We recognize here the Error function (\SeeChapter{see section Thermodynamics page \pageref{error function}}). Therefore:
	
	Therefore:
	
	A spreadsheet software like Microsoft Excel gives us for the complementary Error function\index{error function}\index{complementary error function}:
	\begin{center}
	\texttt{=ERFC(0.5)=0.479500122}
	\end{center}
	Therefore:
	
	The technique that we will see in the section Industrial Engineering (page \pageref{pareto margin error}) makes the approximation that therefore:
	
	indeed... it's engineering...
	
	\subsubsection{Laplace Distribution}
	In probability theory and statistics, the "\NewTerm{Laplace distribution}\index{Laplace distribution}\label{Laplace distribution}" is a continuous probability distribution named also sometimes named the "\NewTerm{double exponential distribution}\index{double exponential distribution}", because it can be thought of as two exponential distributions (with an additional location parameter) spliced together back-to-back (although the term is also sometimes used to refer to the Gumbel distribution). This distribution has many applications but the one that interest us in this books is the LASSO regression (\SeeChapter{see section Numerical Methods page \pageref{LASSO regularization}}).
	
	The difference between two independent identically distributed exponential random variables is governed by a Laplace distribution.
	
	\begin{dem}
	Let $X_1$ and $X_2$ be independent exponential random variable with population means $\alpha_1$ and $\alpha_2$ respectively. Define $Y=X_1-X_2$. The goal is to find the distribution of $Y$ by the cumulative distribution function technique. First we know that:
	
	and:
	
	By independence, it follows that the joint probability density function of $X_1$ and $X_2$ is:
	
	We want $F_Y(y)=P(Y\leq y)$ where $-\infty<y<+\infty$. Looking at the support of $f_{X_1,X_2}(x_1,x_2)$ given above, we see that the resulting cumulative distribution function for $Y$ will (must) be piecewise, where the pieces are separated at $y=0$. So we first consider the case where $y\leq 0$. The double integral giving us the cumulative distribution function $F_Y(y)$ of $y\leq 0$ is:
	
	For $y > 0$, the cumulative distribution function of $Y$ is:
	
	Differentiating with respect to $y$ gives:
	
	which is the probability density function of a Laplace random variable with parameters $\alpha_1$
and $\alpha_2$.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	A random variable has a $\mathcal{L}(\mu,b)$ distribution if its probability density function is:
	
	Here, $\mu$  is a location parameter and $b > 0$, which is sometimes referred to as the diversity, is a scale parameter. If $\mu =0$ and $b=1$, the positive half-line is exactly an exponential distribution scaled by $1/2$.
	
	Notice that the Laplace distribution is often denoted:
	
	
	The probability density function of the Laplace distribution is also reminiscent of the normal distribution; however, whereas the normal distribution is expressed in terms of the squared difference from the mean $\mu$, the Laplace density is expressed in terms of the absolute difference from the mean. Consequently, the Laplace distribution has fatter tails than the normal distribution.
	
	The Laplace distribution is easy to integrate (if one distinguishes two symmetric cases) due to the use of the absolute value function. Its cumulative distribution function is as follows:
	
	
	Let us now derive the mean and variance of the Laplace distribution adopting the following notation:
	
	Let $u=x-\mu$, then we have $x=u+\mu$ and $\mathrm{d}x=\mathrm{d}u$. By linearity we have $\text{E}(X)=\text{E}(U+\mu)=\text{E}(U)+\mu$, hence:
	
	and:
	
	where:
	
	Thus:
	
	
	\pagebreak
	\subsubsection{Benford Distribution}
	This distribution was discovered first in 11881 (holocene calendar) by Simon Newcomb, an American astronomer, after he saw that the wear (and so the use) of the preferred first pages of logarithms tables (at this time there we compiled into books). Frank Benford, around 11938 (holocene calendar) remarked at his turn this unequal wear, believing he was the first to formulate this law that unduly bears his name today and arrived at the same results after having listed tens of thousands of data (lengths of rivers , stock quotes, etc.).

	There is also one possible explanation: we need more often to extract the logarithm of numbers starting with $1$ that numbers starting with $9$, implying that the first are in "bigger quantity" than the second one.

	Although this idea may seem to him quite implausible, Benford began to test his hypothesis. Nothing more simple: he studies tables of numerical values and calculates the percentage of occurrence of the left-most digit (first decimal). The results obtained confirm his intuition:
	\begin{table}[H]\centering\small
	\begin{tabular}{cccccccccc}\hline
	First position number &
	1 &  2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
	Apparition probability (\%)& 30.1 & 17.6 & 12.5 & 9.7 & 7.9
	& 6.7 & 5.8 & 5.1 & 4.6 \\ \hline
	\end{tabular}
	\caption{Occurrence of a digit following the Benford distribution}
	\end{table}
	From these data, Benford found experimentally that the cumulative probability of a number beginning with the digit $n$ (except $0$) is (we will prove this later) is given by the relation:
	
	named "\NewTerm{Benford distribution}\index{Benford distribution}" (or "\NewTerm{Benford law}\index{Benford law}").
	
	Here is a Maple plot of the previous function:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/benford.jpg}
		\caption{Plot of the Benford function (cumulative distribution function)}
	\end{figure}
	It should be noted that this distribution applies only to lists of values that are "natural", that is to say numbers with physical meaning. It obviously does not work on a list of numbers randomly drawn.

	The Benford distribution has been tested on all kinds of tables: length of the rivers of the world, country area, election results, price list of grocery store ... It is true almost every time.

	The distribution is said to be independent of the selected unit. If we take for example a supermarket price list , it also works well with the costs expressed in dollars as with the same costs converted into Euros.

	This strange phenomenon remained unexplained and little studied until quite recently. Then a general proof was given in 11996 (holocene calendar), which uses the central limit theorem.

	As surprising as it may seem, this distribution has found application: it is said that the IRS use it to detect false statements. The principle is based on the restriction seen above: Benford's distributions applies only to values with physical meaning.

	Thus, if there is a universal probability distribution $P(n)$ on such numbers, they should be invariant under scaling such that:
	
	If:
	
	Then:
	
	and the normalization of the distribution gives:
	
	If we derive $P(kn)=f(k)P(n)$ with respect to $k$ we obtain:
	
	choosing $k = 1$ we have:
	
	This differential equation has for solution:
	
	This function is not strictly speaking a distribution function (it diverges) and secondly, the physics and human laws impose limits.

	So we have to compare this distribution with respect to an arbitrary reference. Thus, if the decimal number studied contains power of $10$ ($10$ in total: $0,1,2,3,4,5,6,7,9$) the probability that the first non-zero digit (decimal) is $D$ is also given by:
	
	The limits of the integral are from $1$ to $10$ because the null value is prohibited.

	The integral in the denominator gives:
	
	The integral in the numerator gives:
	
	Finally:
	
	By the properties of logarithms (\SeeChapter{see section Functional Analysis page \pageref{logarithms}}) we have:
	
	However, the Benford's distribution applies not only to non-scaling data but also to numbers from any sources. Explain this case involves a more rigorous investigation using the central limit theorem. This demonstration was conducted only in 11996 (holocene calendar) by T. Hill by an approach using the distribution of distributions.

	To summarize an important part of everything we've seen so far, the picture below is very useful because it summarizes the relation between 76 most common univariate distributions 76 (57 continuous and 19 discrete):
	\begin{figure}[H]
		\centering
		\includegraphics[width=\textwidth]{img/arithmetics/distributions.pdf}
		\caption[Relations between distributions]{Relations between distributions (authors: AMS Lawrence M. Leemis and Jacquelyn T. McQueston)}
	\end{figure}
	And we shouldn't forget the traditional summary tables that every good textbook provides (they should obviously add the median, the skewness and kurtosis but there is sadhl most of time no room for it):
	\begin{table}[H]
		\centering
		\resizebox{\textwidth}{!}{\begin{tabular}{@{}l*6{>{\begin{math}\displaystyle}c<{\end{math}}}@{}}
		  \toprule &&&&&& \\[-2ex]
		  & \text{Notation}
		  & F_X(x) & f_X(x) & \text{E}{(X)} & \text{V}{(X)} & M_X(s) \\[1ex]
		
		  \midrule
		
		  Uniform & \text { Unif }\{a, \ldots, b\} & \begin{cases}0 & x<a \\ \frac{\lfloor x\rfloor-a+1}{b-a} & a \leq x \leq b \\ 1 & x>b\end{cases} & \frac{I(a \leq x \leq b)}{b-a+1} &
		  \frac{a+b}{2} & \frac{(b-a+1)^2-1}{12} &
		  \frac{e^{as}-e^{-(b+1)s}}{s(b-a)} \\[3ex]
		
		  Bernoulli & \operatorname{Bern}(p) & (1-p)^{1-x} & p^x(1-p)^{1-x} &
		  p & p(1-p) &
		  1-p+pe^s \\[3ex]
		
		  Binomial & \operatorname{Bin}(n, p) & I_{1-p}(n-x,x+1) & \left(\begin{array}{l}
		n \\ x \end{array}\right) p^x(1-p)^{n-x} &
		  np & np(1-p) &
		  (1-p+pe^s)^n \\[3ex]
		
		  Multinomial & \text { Mult }(n, p) & & \frac{n !}{x_{1} ! \ldots x_{k} !} p_1^{x_1} \ldots p_k^{x_k} \quad \sum_{i=1}^k x_i=n &
		  \left( {\begin{array}{*{20}{c}}
		    {n{p_1}}\\
		    \vdots \\
		    {n{p_k}}
		  \end{array}} \right) & \left( {\begin{array}{*{20}{c}}
		    {n{p_1}(1 - {p_1})}&{ - n{p_1}{p_2}}\\
		    { - n{p_2}{p_1}}& \ddots 
		    \end{array}} \right) &
		  \left( \sum_{i=0}^k p_i e^{s_i} \right)^n \\[3ex]
		
		  Hypergeometric & \operatorname{Hyp}(N, m, n) &
		  \approx \Phi\left(\displaystyle\frac{x-np}{\sqrt{np(1-p)}}\right) &
		  \frac{\left(\begin{array}{c}m \\ x \end{array}\right)\left(\begin{array}{c}
		N-m \\ n-x \end{array}\right)}{\left(\begin{array}{l} N \\ n\end{array}\right)} &
		  \frac{nm}{N} & \frac{nm(N-n)(N-m)}{N^2(N-1)} & \\[3ex]
		
		  Negative Binomial & \operatorname{NBin}(r, p) & I_p(r, x+1) & \left(\begin{array}{c}
		x+r-1 \\ r-1 \end{array}\right) p^r(1-p)^x &
		  r\frac{1-p}{p} & r\frac{1-p}{p^2} &
		  \left(\frac{pe^s}{1-(1-p)e^s}\right)^r \\[3ex]
		
		  Geometric & \operatorname{Geo}(p) &
		  1-(1-p)^x \quad x \in \mathbb{N}^{+} &
		  p(1-p)^{x-1} \quad x \in \mathbb{N}^{+} &
		  \frac{1}{p} & \frac{1-p}{p^2} &
		  \frac{pe^s}{1-(1-p)e^s} \\[3ex]
		
		  Poisson & \operatorname{Po}(\lambda) & e^{-\lambda} \sum_{i=0}^x \frac{\lambda^i}{i !} & \frac{\lambda^x e^{-\lambda}}{x !} &
		  \lambda & \lambda &
		  e^{\lambda(e^s-1)}\\[3ex]
		
		  \bottomrule
		\end{tabular}}
		\caption{Summary of most common discrete distributions}
	\end{table}
	
	\begin{table}[H]
		\centering
		\resizebox{\textwidth}{!}{\newcolumntype{M}{>{\begin{math}\displaystyle}c<{\end{math}}}
		\begin{tabular}{@{}l*6{M}@{}}
		  \toprule &&&&&& \\[-2ex]
		  & \text{Notation}
		  & F_X(x) & f_X(x) & \text{E}{(X)} & \text{V}{(X)} & M_X(s) \\[1ex]
		
		  \midrule
		
		  Uniform & \operatorname{Unif}(a, b) & \begin{cases}0 & x<a \\ \frac{x-a}{b-a} & a<x<b \\ 1 & x>b\end{cases} & \frac{I(a<x<b)}{b-a} &
		  \frac{a+b}{2} & \frac{(b-a)^2}{12} &
		  \frac{e^{sb}-e^{sa}}{s(b-a)} \\[3ex]
		
		  Normal & \mathcal{N}\left(\mu, \sigma^2\right) &
		  \Phi(x)=\displaystyle\int_{-\infty}^x \phi(t)\,dt &
		  \phi(x)=\frac{1}{\sigma \sqrt{2 \pi}} \exp \left\{-\frac{(x-\mu)^2}{2 \sigma^2}\right\} &
		  \mu & \sigma^2 &
		  e^{\mu s + \frac{\sigma^2s^2}{2}}\\[3ex]
		
		  Log-Normal & \ln \mathcal{N}\left(\mu, \sigma^2\right) &
		  \frac{1}{2}+\frac{1}{2} \text{erf}\left[\frac{\ln x-\mu}{\sqrt{2\sigma^2}}\right] &
		  \frac{1}{x\sqrt{2\pi\sigma^2}} e^{-\frac{(\ln x - \mu)^2}{2\sigma^2}} &
		  e^{\mu+\sigma^2/2} &
		  (e^{\sigma^2}-1) e^{2\mu+\sigma^2} &
		  \\[3ex]
		
		  Multivariate Normal & \operatorname{MVN}(\mu, \Sigma) & &
		  (2\pi)^{-k/2} |\Sigma|^{-1/2} e^{-\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu)} &
		  \mu & \Sigma &
		  e^{\mu^T s + \frac{1}{2} s^T \Sigma s}\\[3ex]
		
		  Student's $t$ & \text{Student}(\nu)
		  & I_x\left( \frac{\nu}{2},\frac{\nu}{2} \right)
		  & \frac{\Gamma\left(\frac{\nu+1}{2}\right)}
		    {\sqrt{\nu\pi}\Gamma\left(\frac{\nu}{2}\right)}
		    \left(1+\frac{x^2}{\nu}\right)^{-(\nu+1)/2}
		  & 0 \quad \nu  > 1
		  & \begin{cases}
		      \displaystyle\frac{\nu}{\nu-2} & \nu > 2 \\
		      \infty & 1 < \nu \le 2
		    \end{cases}
		  & \\[3ex]
		
		  Chi-square & \chi_k^2 &
		  \frac{1}{\Gamma(k/2)} \gamma\left(\frac{k}{2}, \frac{x}{2}\right) &
		  \frac{1}{2^{k/2} \Gamma(k/2)} x^{k/2-1} e^{-x/2}&
		  k & 2k &
		  (1-2s)^{-k/2} \; s<1/2\\[3ex]
		
		  F & \text{F}(d_1,d_2) &
		  I_\frac{d_1x}{d_1x+d_2}\left(\frac{d_1}{2},\frac{d_2}{2}\right) &
		  \frac{\sqrt{\frac{(d_1x)^{d_1} d_2^{d_2}}{(d_1x+d_2)^{d_1+d_2}}}}
		    {x\mathrm{B}\left(\frac{d_1}{2},\frac{d_1}{2}\right)} &
		  \frac{d_2}{d_2-2} %\; d_2 > 2
		  & \frac{2d_2^2(d_1+d_2-2)}{d_1(d_2-2)^2(d_2-4)} %\; d_2 > 4
		  & \\[3ex]
		
		  Exponential & \operatorname{Exp}(\beta) & 1-e^{-x / \beta} & \frac{1}{\beta} e^{-x / \beta} &
		  \beta & \beta^2 &
		  \frac{1}{1-\frac{s}{\beta}} \left(s<\beta\right) \\[3ex]
		
		  Gamma & \operatorname{Gamma}(\alpha, \beta) &
		  \frac{\gamma(\alpha,\beta x)}{\Gamma(\alpha)} & \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-1} e^{-\beta x} &
		  \frac{\alpha}{\beta} & \frac{\alpha}{\beta^2} &
		  \left(\frac{1}{1-\frac{s}{\beta}} \right)^\alpha \left(s<\beta\right)\\[3ex]
		
		  Inverse Gamma & \operatorname{InvGamma}(\alpha, \beta) & \frac{\Gamma\left(\alpha, \frac{\beta}{x}\right)}{\Gamma(\alpha)} & \frac{\beta^\alpha}{\Gamma(\alpha)} x^{-\alpha-1} e^{-\beta / x} &
		  \frac{\beta}{\alpha-1} \; \alpha>1 &
		  \frac{\beta^2}{(\alpha-1)^2(\alpha-2)} \; \alpha > 2 &
		  \frac{2(-\beta s)^{\alpha/2}}{\Gamma(\alpha)}K_\alpha
		  \left( \sqrt{-4\beta s} \right)\\[3ex]
		
		  Dirichlet & \operatorname{Dir}(\alpha) & & \frac{\Gamma\left(\sum_{i=1}^k \alpha_i\right)}{\prod_{i=1}^k \Gamma\left(\alpha_i\right)} \prod_{i=1}^k x_i^{\alpha_i-1} &  \frac{\alpha_i}{\sum_{i=1}^k \alpha_i} &
		  \frac{\text{E}{X_i}(1-\text{E}{X_i})}{\sum_{i=1}^k\alpha_i + 1} & \\[3ex]
		
		  Beta & \operatorname{Beta}(\alpha, \beta) & I_x(\alpha,\beta)& \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \Gamma(\beta)} x^{\alpha-1}(1-x)^{\beta-1}&
		  \frac{\alpha}{\alpha+\beta} &
		  \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)} &
		  1+\sum_{k=1}^{\infty} \left( \prod_{r=0}^{k-1}
		    \frac{\alpha+r}{\alpha+\beta+r} \right) \frac{s^k}{k!} \\[3ex]
		
		  Weibull & \text{Weibull}(\lambda, k) & 1 - e^{-(x/\lambda)^k} & \frac{k}{\lambda}\left(\frac{x}{\lambda}\right)^{k-1} e^{-(x / \lambda)^k} &
		  \lambda \Gamma\left(1 + \frac{1}{k} \right) &
		  \lambda^2 \Gamma\left(1 + \frac{2}{k}\right) - \mu^2 &
		  \sum_{n=0}^\infty \frac{s^n \lambda^n}{n!} \Gamma\left(1+\frac{n}{k}\right)
		  \\[3ex]
		
		  Pareto & \mathrm{Pareto}(x_m, \alpha) &
		  1 - \left(\frac{x_m}{x} \right)^\alpha \; x\ge x_m &
		  \alpha\frac{x_m^\alpha}{x^{\alpha+1}} \quad x\ge x_m&
		  \frac{\alpha x_m}{\alpha-1} \; \alpha>1 &
		  \frac{x_m^2\alpha}{(\alpha-1)^2(\alpha-2)} \; \alpha>2 &
		  \alpha(-x_m s)^\alpha \Gamma(-\alpha,-x_m s) \; s<0\\[3ex]
		
		  \bottomrule
		\end{tabular}}
		\caption{Summary of most common continuous distributions}
	\end{table}
	And here a first high quality vectorial visual summary of some discrete and continuous probability density functions :
	\begin{figure}[H]
		\centering
		\includegraphics[width=\textwidth]{img/arithmetics/distributions_tikz.pdf}
		\caption[Visual summary of some important continuous and discrete statistical distributions]{Visual summary of some important continuous and discrete statistical distributions (authors: Adrian Tam,  V. Isoz)}
	\end{figure}
	and another visual summary of some important continuous distribution with their PDF and CDF:
	\begin{figure}[H]
		\centering
		\includegraphics[width=\textwidth]{img/arithmetics/continuous_distributions.jpg}
		\vspace*{1mm}
		\caption[Visual summary of some important continuous statistical distributions (PDF and CDF)]{Visual summary of some important continuous statistical distributions (source: ?)}
	\end{figure}
	
	\pagebreak
	\subsection{Likelihood Estimators}\label{likelihood estimators}
	What follows is of extreme importance in the field of statistics and is used widely in practice. It is important therefore to pay attention! Besides the fact that we will use massively likelihood estimators in this chapter, we shall find it in the chapter of Numerical Methods for advanced and generalized linear regression (including their generalization that are $M$-estimators or even $W$-estimators!) and also in the chapter of Industrial Engineering in the context of parametric estimation of reliability.
	
	We assume that we have observations $x_1,x_2,x_3,\ldots ,x_n$ which are realizations of unbiased independent random variables (in the sense that they are randomly selected from a batch) $X_1,X_2,X_3,\ldots ,X_n$ of a unknown probability distribution but having the same one.
	
	Suppose we proceed by trial and error to estimate the unknown probability distribution $P$. One way to proceed is to ask if the observations $x_1,x_2,x_3,\ldots,x_n$ had a high probability to get out or not with this arbitrary probability distribution $P$.

	We need for this to calculate the joint probability that the observations $x_1,x_2,x_3,\ldots,x_n$ had to get out with the probabilities $p_1,p_2,p_3,\ldots,p_n$. This joint probability is equal to (\SeeChapter{see section Probabilities page \pageref{joint probability}}):
	
	noting by the letter $P$ the assumed probability distribution associated to $p_1,p_2,p_3,\ldots,p_n$. You must admit that it would be particularly awkward, at the intuition level of risk, to choose a probability distribution (with its parameters!) that minimizes this quantity...
	
	Instead, we will seek the probabilities $p_1,p_2,p_3,\ldots,p_n$ (or the associated parameters of the probability distribution) that maximizes $\prod_{i=1}^n P(X_i=x_i)$, that is to say, that makes the observations $x_1,x_2,x_3,\ldots,x_n$ the most likely possible.

	This leads us to seek the parameter(s) $\theta$ that maximizes the quantity:
	 
	and where the parameter $\theta$ is often in undergraduate school level problems a first order moment (mean) or second order moment (variance).

	The quantity $L$ is named "likelihood". It is a function of the parameter(s) $\theta$ and observations $\theta$.

	The value(s) of the parameter(s) $\theta$ that maximize the likelihood $L_n(\theta)$ are named "\NewTerm{maximum likelihood estimators}\index{maximum likelihood estimators}\label{maximum likelihood estimators}" (MLE estimators).

	In the very special case but useful of the Normal distribution, one of the parameters $\theta$ will be the variance (see a little further concrete example) and can be considered intuitive to the physicist that to maximize the probability, the standard deviation should be as small as possible (so that the maximum numbers of events are in the same interval). Thus, when we calculate a MLE which is the smallest among several possible, then we are talking about a UMV estimator for "\NewTerm{Uniform Minimum Variance Unbiased}\index{uniform minimum variance unbiased}" because their own variance should be as small as possible. This can be demonstrated (but the proof is not very elegant) using the definition of the Fisher Information and the Fréchet theorem (or Rao-Cramér) that makes use of the Cauchy-Schwartz inequality (\SeeChapter{see section Vector Calculus page \pageref{cauchy-schwarz inequality}}) and the analogy between mean and scalar product ... This demonstration will not be in this book.

	Let us still do seven small examples (very classic, useful and important in the industry) with in order of importance (i.e. not necessarily in order of ease...) the distribution function of Gauss-Laplace (Normal distribution), the Poisson distribution, the binomial distribution, the Geometric distribution, the Weibull distribution, the Gamma distribution and finally the Pareto distribution.
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} These seven examples are important as used in SPC (statistical process control) in various international companies around the world (\SeeChapter{see section Industrial Engineering page \pageref{industrial engineering}}).\\
	
	\textbf{R2.} A frequentist maximum likelihood estimate is like to the "mode" of the likelihood function, so in a Bayesian context with uniform prior, the MLE is like a mode of the posterior.\\
	
	\textbf{R3.} In Bayesian statistics (see further below) we have the "\NewTerm{maximum a posteriori}\index{maximum a posteriori}" (MAP) which finds a maximum of the posterior distribution for a uniform prior probability. The MAP estimator coincides with the maximum-likelihood estimator.
	\end{tcolorbox}
	
	\subsubsection{Normal Distribution MLE}\label{normal distribution mle}
	Let be $x_1,x_2,\ldots ,x_n$ an $n$-sample of identically distributed random variables assumed to follow a Gaussian-Laplace (Normal) distribution of parameters $\mu$ and $\sigma^2$.

	We are looking what are the values of the maximum likelihood estimators $\vec{\theta}$ that maximize the likelihood $L_n(\theta)$ of the Normal distribution?

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	It is trivial that the maximum likelihood estimators vector is here: 
	
	\end{tcolorbox}
	We have proved earlier above that the density of a Gaussian random variable was given by: 
	
	The likelihood is then given by:
	
	Maximize a function or maximize its logarithm is equivalent, therefore the "\NewTerm{log-likelihood}\index{log-likelihood}\label{log-likelihood}" will be:
	
	To determine the two estimators of the Normal distribution, first let us fix the standard deviation. To do this, we derive $\ln(L(\mu,\sigma))$ over $\mu$ and look for what the average value of the function is equal to zero.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The partial derivative that we seek to cancel is often named the "score function" (we will often find this concept during our study of optimization techniques and machine learning in the Chapter of Theoretical Computing):
	
	\end{tcolorbox}
	
	It remains after simplification the following term that is equal to zero:
	
	Thus, the maximum likelihood estimator of the expected mean of the Normal distribution is after rearrangement:
	
	and we see that it is simply the arithmetic mean (or also named "\NewTerm{sample mean}\index{sample mean}").
	
	Let us now fix the mean. The cancellation of the derivative of $\ln(L(\mu,\sigma))$ over $\sigma$ leads us to:
	
	This allows us to write the maximum likelihood estimator for the standard deviation (the variance when the mean is known under the an assumed distribution also supposed known!):
	
	that some people also name "\NewTerm{Pearson standard deviation}\index{Pearson standard deviation}"...
	
	Even if it is a little bit redundant some people asked us to show the proof of the estimator of the covariance matrix (and therefore the correlation matrix).
	
	Remember that we have proved earlier that for the bivariate case we have:
	
	In fact the relation is the same for the multivariate case with $T$!
	
	The log-likelihood is therefore immediate by analogy with the univariate case:
	
	That we can also write (as $\Sigma$ is diagonal and $\Sigma^{-1}$ also):
	
	Where by definition and using the estimator of the mean:
	
	Then we deduce that:
	
	and we get finally:
	
	However, we have not yet defined what is a good estimator! What we mean here is:
	\begin{itemize}
	\item If the mean of an estimator is equal to itself, we say that this estimator is "unbiased\label{unbiased estimator}" and that's obviously what we want!
	
	\item If the mean of an estimator is not equal to itself, then we say that this estimator is "biased" and is necessarily less good...
	\end{itemize}
	In the previous example, the average is unbiased (this is trivial as the average of the arithmetic mean is equal to itself). But what about the variance (verbatim the standard deviation)?

	A simple little calculation by linearity of the mean (since the random variables are identically distributed) will give us the answer in the case where the theoretical average (mean) is approximated as in practice (industry) by the estimator of the mean (most common case).

	So we have for the calculation of the mean of the "\NewTerm{sample variance}\index{sample variance}":
	
	However, as the variables are supposed to be identically distributed:
	
	And as we have (Huygens theorem):
	
	wherein the second relation can be written only because we use the maximum likelihood estimator of the average (empirical average). 
	Therefore combining the two above relations with the prior-previous 
 one we get:
	
	and as:
	
	Finally we have\label{mean of unbiased standard deviation}:
	
	so we have a bias of at least one standard error:
	
	then we say that this estimator has a negative bias (it underestimates the true value!).
	
	We also note that the estimator tends towards to an unbiased estimator of the variance (USV) when the number of items tends to infinity $n\rightarrow +\infty$. We say that we have a "\NewTerm{asymptotically unbiased}\index{asymptotically unbiased estimator}" or "\NewTerm{asymptotically unbiased estimator}".
	
	Then, the "\NewTerm{bias of an estimator}\index{bias of an estimator}\label{bias of an estimator}" is defined, according to the  special case above, in general as:
	

	It is important to note that we have yet proved that the empirical variance tends towards the theoretical variance when $n$ tends to infinity and ... that the data follows or not a Normal distribution!
	
	By the properties of the mean, we get now:
	
	We have then:
	
	simply named the "\NewTerm{standard deviation}" ... (that must not be confused with the "standard error" as we shall see later).
	
	An estimator is named "\NewTerm{consistent estimator}\index{consistent estimator}" if it converges \underline{in probability} (as it can be considered itself as random variable following it own distribution), when $n\rightarrow +\infty$, towards the true parameter value. Technically, this condition will be written:
	
	
	Maybe, the following example may help. Let's suppose the samples are taking from a Normal distribution. Then using the fact that $\frac{(n-1)S^2}{\sigma^2}$ (see page \pageref{Cochran theorem}) is a chi-squared random variable with $n-1$ degrees of freedom, we get:
	
	where we have used that fact that (see page \pageref{chi-square distribution}):
	
	So written differently (according to above notation):
	
	From these facts we can informally see that the distribution of $\text{V}\left(\hat{\sigma}^2\right)$ is becoming more and more concentrated at $\sigma^2$ as the sample size increases since the mean $\text{E}\left(\hat{\sigma}^2\right)$ is converging to $\sigma^2$ (i.e. $\hat{\sigma}$ is unbiased) and the variance $\text{V}\left(\hat{\sigma}^2\right)$ is converging to $0$ (i.e. $\hat{\sigma}$ is consistent)!

	So we finally summarize as following the two important previous results:

	\begin{enumerate}
		\item The "\NewTerm{biased maximum likelihood estimator}\index{biased maximum likelihood}" or also named "\NewTerm{empirical standard deviation}\index{empirical standard deviation}\label{empirical standard deviation}" or "\NewTerm{sample standard deviation}\index{sample standard deviation}" or "\NewTerm{Pearson standard deviation}\index{Pearson standard deviation}" ... is therefore given by:
		
		when $n\rightarrow +\infty$. We find this standard deviation depending on the context (by tradition) noted in five other ways that are:
		
		and sometimes (but this is very awkward because it often generates confusion with the unbiased estimator) $\sigma$ or $S$.
		
		\item The "\NewTerm{unbiased maximum likelihood estimator}\index{unbiased likelihood estimator}" or simply named "\NewTerm{standard deviation}\index{standard deviation}" with the "\NewTerm{Bessel correction}\index{Bessel correction}" (the $-1$ at the denominator is the correction\footnote{Some textbooks also define the "\NewTerm{Bessel factor}" as $n/(n-1)$ like if we multiply the uncorrected sample variance by it we get the unbiased estimator of the population variance.}!):
		
		which as we can see is a consistent estimator (when $n$ tends to infinity it tends to the biased maximum likelihood estimator).
		
		We find this standard deviation depending on the context (by tradition) noted in three other ways that are:
		
	\end{enumerate}

	We find these last two notations often in tables and in many softwares and we will use them later in the development of confidence intervals and hypothesis testing!

	For example, in the Microsoft Excel 11.8346 the unbiased estimator is given by the \texttt{STDEV( )} function and the non-biased by \texttt{STDEVP()}.

In total, this makes us is three estimators for the same indicator! As in the overwhelming majority of cases of the industry the mean is not known, we usually use the last two relations bordered above. Now this is where comes the vicious part: when we calculate the bias of this two estimators, the first is biased, the second is not. So we tend to use only the latter. Nay! Because we could also talk about the variance and precision of an estimator, which are also important criteria for judging the quality of an estimator relative to another. If we were to calculate the variance of the two estimators, then the first, which is biased, is smaller than the second which is unbiased variance! All that to say that the criteria of bias is not (by far) the only one to be study to judge the quality of an estimator.

Finally, it is important to remember that the factor $-1$ in the denominator of the unbiased maximum likelihood estimator stems from the need to correct the mean of the biased estimator initially subtracted by one time the standard error!

	\subsubsection{Poisson Distribution MLE}\label{poisson distribution mle}

	Using the same method as for the Normal (Gauss-Laplace) distribution, we will seek the maximum likelihood estimators of the Poisson distribution which for recall is given by:
	
	Thus, the likelihood is given by:
	
	Maximize a function or maximize its logarithm is equivalent therefore:
	
	We are now looking to maximize it:
	
	and thus we obtain the only maximum likelihood estimator that will be:
	
	It is quite normal to find in this example the sample mean because it is the best possible estimator for the parameter of the Poisson distribution (which also represents the mean of a Poisson distribution).

	Knowing that the standard deviation of this particular distribution (see above during the development of the Poisson distribution) is the square root of the mean, then we have for the standard deviation maximum likelihood:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	We show in the same way identical results for the exponential distribution that is widely used in preventive maintenance and reliability!
	\end{tcolorbox}
	
	\subsubsection{Binomial Distribution MLE}
	Suppose we have a Bernoulli model in which each observation has a constant and equal chance of success denoted $p$. A Bernoulli variable takes one of two values, conventionally $1$ or $0$, which indicate a 'success' or 'failure'. The probability distribution for this variable is:
	
	If $S$ stands for the number of successes and $F$ for the number of failures, then the likelihood is:
	
	and the log-likelihood would then be:
	
	We then take the derivative of this with respect of $p$:
	
	We now set this equal to zero and solve for $p$ and then we get immediately:
	
	
	\subsubsection{Geometric Distribution MLE}
	
	Using the same method as for the Normal distribution (Gauss-Laplace) and the Poisson distribution, we will seek the maximum likelihood estimator of the Binomial which we recall, is given by:
	
	Accordingly, the likelihood is given by:
	
	It should be remembered that the factor following the combinatorial term already expressed the successive variables according to what we saw during our study of the Bernoulli and Binomial distribution functions. Hence the disappearance of the product in the preceding equality.

	Maximize a function or maximize its logarithm is equivalent therefore:
	
	We are now looking to maximize it:
	
	The reader may have perhaps noticed that the binomial coefficient has disappeared. Therefore, we immediately deduce that the estimator of the binomial distribution is the same as the geometric distribution.

	Which gives:
	
	from which we derive the maximum likelihood estimator:
	
	This result is quite intuitive if we consider the classic example of a coin that has a chance on two of dropping on one of its faces. The probability $p$ being the number of times $k$ a given face where was observed in the total number of tests (all sides combined).
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In practice, it is not as easy to apply these estimators! We must carefully consider which are most suitable for a given experiment and ideally also calculate the mean squared error (standard error) of each of the estimators of the mean (as we have already done for the empirical mean earlier). In short... it is a long process of reflection.
	\end{tcolorbox}
	
	\subsubsection{Weibull Distribution MLE}\label{weibull distribution mle}
	
	We saw in the section of Industrial Engineering a very detailed study of the three-parameter Weibull distribution with its standard deviation and mean because as we mentioned it is quite used in the field of reliability engineering (\SeeChapter{see section Industrial Engineering page \pageref{weibull distribution}}).

	Unfortunately, the three parameters of this distribution are unknown in practice. Using estimators however we can determine the expression of two of the three assuming $\gamma$ as zero. This gives us the following Weibull distribution named  "Weibull distribution with two parameters":
	
	and for recall with $\beta>0$ and $\eta>0$.
	
	Therefore the likelihood is given by:	
		
	Maximize a function or maximize its logarithm is equivalent therefore:	
	
	Now we seek to maximize this by remembering that (\SeeChapter{see section Differential and Integral Calculus page \pageref{usual derivatives}}):
	
	then:
	
	And we get for the second parameter:
		
	then:
	
	Finally to summarize with the correct notations (and in the resolution order in practice):
	
	Solving these equations involves heavy computations and we can a priori do nothing with that in conventional spreadsheets softwares such as Microsoft Excel or Open Office Calc without programming (at least as far as we know...).

	We then take a different approach by writing our Weibull distribution with two parameters as follows:	
	
	with for recall $\beta>0$ and $\theta>0$.

	Therefore the likelihood is given by:
	
	Maximize a function or maximize its logarithm is equivalent therefore:
	
	Now we seek to maximize this by remembering that (\SeeChapter{see section Differential and Integral Calculus page \pageref{usual derivatives}}):
	
	then:
	
	And we have for the second parameter:
	
	It is then immediate that:
	
	injected into the equation:
	
	We get:
	
	simplifying:
	
	The resolution of the two equations (in order from top to bottom):
	\begin{equation}
	  \addtolength{\fboxsep}{5pt}
	   \boxed{
	   \begin{gathered}
	   	\begin{aligned}
	     &\dfrac{\displaystyle \sum_{i=1}^n x_i^\beta \ln(x_i)}{\dfrac{1}{n}\displaystyle \sum_{i=1}^n x_i^\beta}-\dfrac{1}{\beta}-\dfrac{1}{n}\sum_{i=1}^n \ln(x_i)=0\\
		&\dfrac{1}{n}\sum_{i=1}^n x_i^\beta-\bar{\theta}
		\end{aligned}
	   \end{gathered}
	   }
	\end{equation}
	
	can easily be calculated with the Target Tool of Microsoft Excel or Open Office Calc.
	
	\subsubsection{Gamma Distribution MLE}
	Here we will use a technique named "\NewTerm{method of moments}\index{method of moments}" to determine the estimators of the parameters of the Gamma distribution.

	Suppose that $X_1, \ldots , X_n$ are independent and identically distributed random variables according to the Gamma distribution with density:
	
	We seek to estimate $a,\lambda$. For this, we first determine some theoretical moments.
	The first moment is the expected mean that as we have proved before is given by:
	
	and the second moment, the mean of the square of the random variable, is as we have implicitly proved in the proof of the variance of the Gamma distribution given by:
	
	We then express the relation between the parameters and the theoretical moments:
	
	The resolution of this simple system gives:
	
	Once this system established, the method of moments consist to use the empirical moments, i.e. for our example the first two, $\hat{m}_1,\hat{m}_2$:
	
	that we define as equal to the true theoretical moments ... Therefore, it comes:
	\begin{equation}
	  \addtolength{\fboxsep}{5pt}
	   \boxed{
	   \begin{gathered}
	   	\begin{aligned}
	    	a&=\dfrac{\hat{m}_1^2}{\hat{m}_2-\hat{m}_1^2}\\
			\lambda&=\dfrac{\hat{m}_2-\hat{m}_1^2}{\hat{m}_1^2}
		\end{aligned}
	   \end{gathered}
	   }
	\end{equation}
	
	\subsubsection{Pareto Distribution MLE}
	Let us recall the Pareto density given by:
	
	Let us choose a more convenient and traditional notation:
	
	The likelihood function of the Pareto distribution given a sample $x=(x_1,\ldots, x_n)$ is given by:
	
	Taking the log of the likelihood, we obtain:
	
	Since a higher $\beta$ will always result in a higher likelihood - $\ln (\beta)$ is monotonically increasing - we maximize the likelihood by setting $\hat{\beta}$ as high as possible (we can't use differentiation to maximize the likelihood in this case). Since $\beta \leq x_{i}$ for all $i,$ we maximize the likelihood by setting $\hat{\beta}=\underset{i}{\min}\; x_{i},$ the smallest $x_{i}$ in the sample.
	
	For $\alpha$, we set the partial derivative of $\ell$ with respect to $\alpha$ equal to $0$:
	
	Therefore:
	
	
	\subsubsection{Censored data MLE}
	The "\NewTerm{likelihood of censored data}\index{likelihood of censored data}" is of crucial importance in the field of industrial reliability and also in pharmacies and clinical studies, knowing that for reasons of cost and unforeseen circumstances, there is often a limit to the time of observation or events which make that the subjects of study leave the experiment. Knowing that there are four types of situations (see figure below if needed):
	\begin{enumerate}
		\item "\NewTerm{Uncensored data}\index{uncensored data}" (un), the event of interest is observed between the start time of measurement and end of measurement.
	
		\item "\NewTerm{Right-censored}\index{right-censored}" (rc), the event did not occur during the limited observation time, that is, between the measurement start and end time.
	
		\item "\NewTerm{Left-censored}\index{left-censored}" (lc), the object of study left the experiment for whatever reasons observed between the time of beginning of measurement and end of measurement, or we do not know with precision at which what should have been measured started.
	
		\item "\NewTerm{Censorship interval}\index{censorship interval}" (ci), an event of interest took place between two instants, but we do not know exactly when in this interval (before, between or after the measurement periods).
	\end{enumerate}
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,670); %set diagram left start at 0, and has height of 670
		
		%Shape: Axis 2D [id:dp17380544111619645] 
		\draw  (70,355.3) -- (579.3,355.3)(120.93,25) -- (120.93,392) (572.3,350.3) -- (579.3,355.3) -- (572.3,360.3) (115.93,32) -- (120.93,25) -- (125.93,32)  ;
		%Straight Lines [id:da3973665805285225] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (197.3,76) -- (197.3,359) ;
		%Straight Lines [id:da6618445417853447] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (441.3,76) -- (441.3,359) ;
		%Straight Lines [id:da9184755249649512] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (198.3,103) -- (299.3,103) ;
		\draw [shift={(303.3,103)}, rotate = 180] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (17.49,-5.26) .. controls (11.12,-2.23) and (5.29,-0.48) .. (0,0) .. controls (5.29,0.48) and (11.12,2.23) .. (17.49,5.26)   ;
		%Straight Lines [id:da24921233968823597] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (197.3,131.5) -- (357.3,131.5) ;
		\draw [shift={(361.3,131.5)}, rotate = 180] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (17.49,-5.26) .. controls (11.12,-2.23) and (5.29,-0.48) .. (0,0) .. controls (5.29,0.48) and (11.12,2.23) .. (17.49,5.26)   ;
		%Straight Lines [id:da11607907995459144] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (197.3,160) -- (423.3,160) ;
		\draw [shift={(427.3,160)}, rotate = 180] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (17.49,-5.26) .. controls (11.12,-2.23) and (5.29,-0.48) .. (0,0) .. controls (5.29,0.48) and (11.12,2.23) .. (17.49,5.26)   ;
		%Straight Lines [id:da8462412240109836] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]  [dash pattern={on 2.53pt off 3.02pt}]  (199.3,206) -- (300.3,206) ;
		\draw [shift={(304.3,206)}, rotate = 180] [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (17.49,-5.26) .. controls (11.12,-2.23) and (5.29,-0.48) .. (0,0) .. controls (5.29,0.48) and (11.12,2.23) .. (17.49,5.26)   ;
		%Straight Lines [id:da6757745879913091] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]  [dash pattern={on 2.53pt off 3.02pt}]  (197.3,235) -- (499.3,235) ;
		\draw [shift={(503.3,235)}, rotate = 180] [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (17.49,-5.26) .. controls (11.12,-2.23) and (5.29,-0.48) .. (0,0) .. controls (5.29,0.48) and (11.12,2.23) .. (17.49,5.26)   ;
		%Straight Lines [id:da5569507679769929] 
		\draw [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][line width=2.25]  [dash pattern={on 6.75pt off 4.5pt}]  (122.3,288) -- (395.3,288) ;
		\draw [shift={(399.3,288)}, rotate = 180] [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][line width=2.25]    (17.49,-5.26) .. controls (11.12,-2.23) and (5.29,-0.48) .. (0,0) .. controls (5.29,0.48) and (11.12,2.23) .. (17.49,5.26)   ;
		%Straight Lines [id:da625418206789448] 
		\draw [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][line width=2.25]  [dash pattern={on 6.75pt off 4.5pt}]  (120.3,322) -- (423.3,322) ;
		\draw [shift={(427.3,322)}, rotate = 180] [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][line width=2.25]    (17.49,-5.26) .. controls (11.12,-2.23) and (5.29,-0.48) .. (0,0) .. controls (5.29,0.48) and (11.12,2.23) .. (17.49,5.26)   ;
		%Straight Lines [id:da2774962167983923] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (473.3,40) -- (516.3,40) ;
		%Straight Lines [id:da24743206074332025] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]  [dash pattern={on 2.53pt off 3.02pt}]  (474.3,61) -- (517.3,61) ;
		%Straight Lines [id:da28674982479519295] 
		\draw [color={rgb, 255:red, 139; green, 87; blue, 42 }  ,draw opacity=1 ][line width=2.25]  [dash pattern={on 6.75pt off 4.5pt}]  (475.3,82) -- (516.3,82) ;
		
		% Text Node
		\draw (100,359) node [anchor=north west][inner sep=0.75pt]   [align=left] {O};
		% Text Node
		\draw (153,360) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{65.08pt}\setlength\topsep{0pt}
		\begin{center}
		Measurement\\start
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (395,360) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{65.08pt}\setlength\topsep{0pt}
		\begin{center}
		Measurement\\end
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (316,95) node [anchor=north west][inner sep=0.75pt]   [align=left] {Event};
		% Text Node
		\draw (375,123) node [anchor=north west][inner sep=0.75pt]   [align=left] {Event};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (430,147) -- (476,147) -- (476,172) -- (430,172) -- cycle  ;
		\draw (443,153) node [anchor=north west][inner sep=0.75pt]   [align=left] {Event};
		% Text Node
		\draw (318,198) node [anchor=north west][inner sep=0.75pt]   [align=left] {?};
		% Text Node
		\draw (517,226) node [anchor=north west][inner sep=0.75pt]   [align=left] {?};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (402,275) -- (448,275) -- (448,300) -- (402,300) -- cycle  ;
		\draw (410,279) node [anchor=north west][inner sep=0.75pt]   [align=left] {Event};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (430,310) -- (476,310) -- (476,335) -- (430,335) -- cycle  ;
		\draw (438,314) node [anchor=north west][inner sep=0.75pt]   [align=left] {Event};
		% Text Node
		\draw (520,31) node [anchor=north west][inner sep=0.75pt]   [align=left] {Uncensored};
		% Text Node
		\draw (520,51.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {Right-censored};
		% Text Node
		\draw (521,72) node [anchor=north west][inner sep=0.75pt]   [align=left] {Left-censored};
		% Text Node
		\draw (583,346.4) node [anchor=north west][inner sep=0.75pt]    {$t$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Type of censored data}
	\end{figure}
	
	It is then relatively intuitive that if all events are independent then the likelihood function will be given by:
	
	where the index $l$ is for "left" and the index $r$ is for "right". 
	
	We have obviously (think of the discrete case before thinking of the continuous case):
	
	In the field of reliability engineering that interest us here, we often reduce ourselves to the following cases:
	
	but with the following notation (...):
	
	In many experiments, the number of faulty elements $f$ to be observed on $n$ is fixed in advance (we often speak of "\NewTerm{type II censorship}\index{type II censorship}") or simply known afterwards. Therefore, we have:
	
	Let us explicit this probability with Weibull's law. Let us recall that a possible form of writing of the density function is written for the latter (\SeeChapter{see section Industrial Engineering page \pageref{weibull distribution}}):
	
	and that we have proved that its survival function was given by:
	
	Therefore:
	
	Maximizing the likelihood is equivalent to maximizing the logarithm. Since then:
	
	We calculate the partial derivative with respect to $\alpha$, $\beta$ (\SeeChapter{see section of Differential and Integral Calculus  page \pageref{usual derivatives}}):
	
	The second relation can easily be explicited with respect to $\beta$ such that:
	
	In any case, it seems that it will be necessary to use numerical optimization techniques to determine all parameters.
	
	\subsubsection{Fisher Information Matrix}
	We will introduce now a matrix that will be useful to us much more later for some advanced statistical tests but also to determine (among others!) non-obvious statistical law parameters and also an important information criterion named the "Akaike Information Criterion" (\SeeChapter{see section Numerical Methods page \pageref{Akaike information criterion}}).
	
	We have seen earlier that the partial derivative of the log-likelihood  (that we will denote $\mathcal{L}$ for the following developments) relatively to a given parameter $\theta_i$ of a probability function (depending of a set of parameters $\vec{\theta}$) will give us an estimator of this parameter if put it as equal to zero. 
	
	Also respectively we then have that the partial derivative valuated at this correspond estimator then gives:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The expression:
		
	is known as the "\NewTerm{score function}\index{score function}".
	\end{tcolorbox}
	
	Let us recall that during our study of bivariate Taylor series (\SeeChapter{see section Sequences and Series page \pageref{multivariate taylor series}}), we have proved that:
	
	where:
	
	Let us do the following replacement:
	
	We then have:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In the univariate case this is also written:
	 
	or in some textbooks:
	
	\end{tcolorbox}
	In vector form this is often written:
	
	Now remember that $f(\vec{v}_0)$ and $\vec{\nabla}f(\vec{v}_0)$ does not contain anymore any term in $x$ or $y$ as the function is 'valuated'. Hence if we take the gradient of the whole previous expression, we get:
	
	If you don't understand that result, do it again with the univariate case may help. Indeed, starting from:
	
	We then have in a very very detailed way:
	
	That latter equality can indeed be written:
	
	Now let us come back to the more general equality:
	
	We can solve this expression for the optimal point $\vec{\nabla}f(\vec{v}) = \vec{0}$, giving:
	
	Hence:
	
	This is the Newton-Raphson method for multidimensional optimization\index{multidimensional Newton-Raphson method} by the way... 
	
	Now let us focus on the following equality from the previous one:
	
	And let us change the notation and replace $f$ by $\mathcal{L}$:
	
	where $\vec{\nabla}\mathcal{L}(\vec{\theta})$ (the gradient of the likelihood at the point $\vec{\theta}$) is named the "\NewTerm{score vector}\index{score vector}".
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In the univariate case this is will also be reduced to:
	
	\end{tcolorbox}
	Now let us have a bit 'fun'... Let us take the outer product (\SeeChapter{see section Linear Algebra page \pageref{outer product}}) of $\vec{\theta}$, ie:
	
	Ok so far nothing interesting. But let us take the expected mean:
	
	Here we recognize something! The elements of the diagonal look similar to a variance... The variance of $\theta_1$ relatively to $\theta_{1,0}$. It is common to assume that $\theta_1$ is the estimator and $\theta_{1,0}$ is the true value, then we write (keep in mind that this can be generalized to more than $2$ parameters!):
	
	We recognize here the variance-covariance matrix\footnote{Sadly denoted in some textbooks $\text{V}(\vec{\theta})$...} (\SeeChapter{see above page \pageref{variance covariance matrix}}). Ok that's nice but so what??? What does this bring us? In fact using previous equalities we have:
	
	Using the properties of transposition as proved in the section of Linear Algebra and the property of matrix multiplication association property, that latter relation can be written:
	
	Using another property of the transposed, this can be also written:
	
	Assuming the Hessian is symmetric such that $H^T=H$ (if Schwarz's theorem\footnote{See section Differential and Integral Calculus page \pageref{Schwarz theorem}} applies on the function considered inside the Hessian!), we then have:
	
	Using the linearity property of the mean:
	
	Now let us focus on:
	
	But we need an important result before! As the likelihood $L(\vec{x}),\vec{\theta})$ is a density, we know that:
	
	Now by differentiating both sides with respect to one of the parameter $\theta_i$ we get:
	
	By assuming (this assumption must be checked on every case!) that we have:
	
	Then:
	
	But this is also equal to:
	
	Differentiating again with respect to $\theta_i$ and taking the derivative inside gives:
	
	Hence:
	
	Written in a more common way by statisticians as following:
	
	And using the notation defined earlier:
	
	If we do the same development again but when taking the partial derivative a second time but relatively to $\theta_j$ instead of $\theta_i$ we get immediately:
	
	This can be put obviously in matrix form for each component $i,j$. Then it is usual to define the  mean of matrix made of the components $\frac{\partial \mathcal{L}(\vec{x},\vec{\theta})}{\partial \theta_i} \frac{\partial \mathcal{L}(\vec{x},\vec{\theta})}{\partial \theta_j}$ as the "\NewTerm{Fisher information matrix}\index{Fisher information matrix}\label{Fisher information matrix}" denoted $\mathcal{I}(\vec{\theta})$ and we recognize that a matrix made of the components $\frac{\partial^2 \mathcal{L}(\vec{x},\vec{\theta})}{\partial \theta_i \partial \theta_j}$ is the "Hessian" (\SeeChapter{see section Sequences and Series page \pageref{hessian matrix}}) denoted for recall $H(\vec{\theta})$, then we have:
	
	And this is the relation we were expecting to find!
	
	So now let us come back to:
	
	That we can now rewrite as:
	
	So finally we get the important final result we were seeking from the start named the "\NewTerm{information matrix equality}\index{information matrix equality}":
	
	That is that the covariance-variance matrix (itself equal to the outer product of $\vec{\theta}$ with itself!), is equal to the negative on the inverse Hessian! Or in other words:  the hessian of a likelihood function equals the information matrix, or the variance-covariance matrix of the score functions!
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Our sample is made up of the first  $n$ terms of an independent and identically distributed sequence $\{X_n\}$ of normal random variables having mean $\mu$ and variance  $\sigma^2$. The probability density function of a generic term of the sequence is:
	
	The mean $\mu$ and the variance  $\sigma^2$ are the two parameters that need to be estimated.\\
	
	We can read off the standard error of $\vec{\theta}$ from the square roots of the diagonal elements of the matrix $\Sigma$. Note that these are only correct asymptotically and are hard to calculate in finite samples.\\
	
	We know that the likelihood function is:
	
	and hence (for recall):
	
	The likelihood and their solution are for recall:
	
	Now let us calculate the Hessian of $\hat{\theta}=(\hat{\mu},\hat{\sigma}^2$, that is the Hessian of $\mathcal{L}(\vec{\theta})$. We have to compute $4$ components of the matrix as the likelihood has $2$ parameters ($2\times 2=4$) and the out-diagonal components are equal assuming the Schwarz theorem applies. Then:
	
	When evaluated at $\hat{\vec{\theta}}$:
	
	Hence:
	
	We have also:
	
	We have:
	
	Hence, the Fisher information matrix is given by:
	
	Matrix inversion gives us also (\SeeChapter{see section Linear Algebra page \pageref{determinant matrix inverse}}):
	
	Hence:
	
	\end{tcolorbox}	
	Now let us recall that during our proof of the central limit theorem, we have proved that (page \pageref{central limit theorem}):
	
	Therefore:
	
	Now instead of the estimator $\hat{\mu}$, let us consider the estimate of the score function (the $\sigma$ is obviously the same as the previous relation):
	
	Now we consider that $\hat{\mathcal{S}}$ at the true value of $\theta$ (i.e. $\theta_0$) is not zero as it is an estimator (but it's however probable near zero)! But $\mathcal{S}$ is however zero at $\theta_0$. Then we have:
	
	Hence:
	
	Or in vector form:
	
	With explicitly, by construction of the variance-covariance matrix (for the special case of $2\times 2$ to simplify the example):
	
	However, estimated at $\vec{\theta}_0$ the true values $\mathcal{S}_1$ and $\mathcal{S}_2$ are equal to zero. Then the variance-covariance matrix reduces to:
	
	Or by expliciting the score functions:
	
	We recognize here the Fisher information matrix! Therefore:
	
	And so finally:
	
	
	\subsection{Finite Population Correction Factor}
	Now we prove another result which we will be required in some statistical tests that we will see later.

Suppose we have a population of $N$ individuals that we represent by the set $\left\lbrace1,2,\ldots ,N\right\rbrace$ and a random variable $X$ which is an application of $\left\lbrace1,2,\ldots ,N\right\rbrace$ in $\mathbb{R}$. We denote by $x_i=X(i)$. The mean of $X$ is thus given by:

	
	Remember the variance of $X$ is by definition:
	
	Now we consider the set $E$ of samples of size $n$ taken in $\left\lbrace1,2,\ldots ,N\right\rbrace$ with $0<n<N$. Each individual has a probability of being drawn equal to:
		
	We are interested in the random variable $\bar{X}$ defined on $E$ and that equal to the sample mean. More specifically:
	
	To calculate the variance $\text{V}(\bar{X})$, we will $\bar{X}$ express as a sum of random variables. Indeed, if we define the variables $X_k$ with $k=1\ldots N$ by:
	
	We have naturally by the previous definition (see with caution the sum limits!):
	
	and thus we get:
	
	The random variables $X_k$ are not independent in pairs, in fact as we shall see, their covariances are not zero if $N$ is finite. Otherwise (zero covariance), we find a result already proved earlier:
	
	So we need to calculate the variances $\text{V}(X)$ and covariances $\text{cov}(X_i,X_j)$.
	
	For this purpose we will use the Huygens relation and we will start by calculating the mean $\text{E}(X_k)$:
	
	But $P(X_k=x_k)$ is the probability that a sample contains $k$. This probability is obviously equal to $n/N$ and therefore:
	
	Similarly we obtain:
	
	We can therefore calculate the variance $\text{V}(X_k)$:
	
	To calculate the covariances we need now to calculate the means $\text{E}(X_iX_j)$:
	
	But $P(X_i=x_i,X_j=x_j)$ is the probability that a sample contains $i$ and $j$. This probability is obviously given by:
	
	and therefore:
	
	We can now compute the covariance:
	
	We are now able to calculate $\text{V}(\bar{X})$:
	
	Using Huygens theorem we get:
	
	Using the result proved above and previous relation:
	
	Therefore:
	
	For the double sum $\displaystyle \sum_{i\neq j}^N x_ix_j$, we have:
	
	Therefore:
	
	Thus:
	
	The famous factor:
	
	that we have already encountered during our study the hypergeometric distribution is named "\NewTerm{finite correction factor (on finite population)}\index{finite correction factor}" and has the effect of reducing the standard error especially as $n$ is large. This factor is less than $1$, so the variance of the hypergeometric is always less than that of the corresponding binomial. This makes sense because the draws are made without replacement in the hypergeometric distribution.
	
	If the sample is relatively large (more than $ 5 \% $ of the population), it is sometimes customary to use this correction factor.
	
	In the science of survey, we can conclude that for the "random sample without replacement (RSWOR)" and for "random sample with replacement (RSWR)" the average is most of times given by:
	
	but for the standard deviation:
	
	where the correction factor is sometimes approximated by:
	
	
	\pagebreak
	\subsection{Confidence Intervals (inference)}\label{confidence interval}
	Until now we have always determined the likelihood estimators or simple estimators (variance, standard deviation) from theoretical statistics distributions or measured on an entire population of data. But now that we have covered descriptive statistics, probability and sampling, we are ready to make "\NewTerm{frequentist inference}\index{frequentist inference}" and "\NewTerm{bayesian inference}\index{Bayesian inference}" from our sample data to our population of interest.  
	
	In statistical frequentist inference, we take what we know from the sample, apply the underlying theory of sampling (central limit theorem) to make statements about our population of interest.  We make estimates about the population through the use of the sample data.  Estimates can either be point estimates or interval estimates. In statistical Bayesian inference, we take the likelihood knowledge we have on the data and the prior on some parameters to infer a posterior distribution having new "hyperparameters".
	\begin{fquote}[?]Being a statistician means never having to say you are certain!
 	\end{fquote}
	
	\subsubsection{Frequentist Inference}\label{frequentist inference}
	\textbf{Definition (\#\thesection.\mydef):} A "\NewTerm{confidence interval}\index{confidence interval}"  (C.I.) in frequentist inference is a pair of numbers (in the univariate case) that defines (a priori) the range of possible values with a certain cumulative probability of the distribution of a given estimator from a sample of an experience (the range of the statistical indicator being usually calculated using real measured parameters). In other words, the C.I. is a collection of intervals with a given percentage (probability) of them containing the true parameter!
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The confidence interval is often confused with the "\NewTerm{Bayesian credibility interval}\index{Bayesian credibility interval}" that is an interval within which an unobserved parameter value falls with a particular probability. It is an interval in the domain of a posterior probability distribution or a predictive distribution (the generalisation to multivariate problems is the "credible region").
	\end{tcolorbox}
	
	
	Textbook authors and proponents of confidence intervals bridge the gap seamlessly by claiming that confidence intervals have three desirable properties: first, that the confidence coefficient can be read as a measure of the uncertainty one should have that the interval contains the parameter; second, that the CI width is a measure of estimation uncertainty; and third, that the interval contains the "likely" or "reasonable" values for the parameter. These all involve reasoning about the parameter from the observed data: that is, they are "post-data" inferences.
	
	These interpretations of confidence intervals are not correct. We name the mistake these authors have made the "\NewTerm{Fundamental Confidence Fallacy}" (FCF) because it seems to flow naturally from the definition of the confidence interval (see \cite{perezgonzalez2017fallacy}).
	
	The reasoning behind the Fundamental Confidence Fallacy seems plausible: on a given sample, we could get any one of the possible confidence intervals. If $95\%$ of the possible confidence intervals contain the true value, without any other information it seems reasonable to say that we have $95\%$ certainty that we obtained one of the confidence intervals that contain the true value. This interpretation is suggested by the name "confidence interval" itself: the word "confident", in lay use, is closely related to concepts of plausibility and belief. The name "confidence interval" — rather than, for instance, the more accurate "\NewTerm{coverage procedure}" — encourages the Fundamental Confidence Fallacy.
	
	So if the confidence intervals of two parameters overlap you can't conclude that these parameters are not significantly different from each other! And that's because if you construct a hundred $95\%$ confidence intervals, then about $95$ of these intervals will contain the population value (you must always keep in mind that you work with statistical objects!!!!). So the specific values of a calculated confidence intervals should rigorously not be directly interpreted!
	
	In short, this is what you need to remember when comparing two confidence intervals:
	\begin{enumerate}
		\item When two confidence intervals of a given parameter do not overlap, the difference between the two parameters will be significant.
		\item When two confidence intervals of a given parameter do overlap, the difference between the two parameters can be significant or non-significant.
	\end{enumerate}
	Keep in mind for example when the parameter is the mean, that standard errors of means are not the same as the standard errors of differences of means!
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.76]{img/arithmetics/confidence_interval_weather_forecasting.jpg}
		\caption[Example of confidence interval in weather-forecasting]{Example of confidence intervals that have interested million of peoples not so far ago in the past...}
	\end{figure}
	We now turn to the task that consists naturally to ask ourselves what must be the sample sizes of our measured data to have some validity for our estimators or even to which confidence interval correspond a given standard deviation or quantile in a Normal centered reduced distribution (for large samples), in a chi-square distribution, Student distribution or Fisher distribution (we will see the last two cases of small sample sizes further below during our study of ANalysis Of VAriance, ie ANOVA) when the mean or variance are known or unknown respectively on all or part of the given population.

	It is important to know that these confidence intervals often use the central limit theorem that will be proved further below (to avoid any possible frustration) and the developments that we will do now are also useful in the field of (a posteriori) Hypotheses Tests that have a major role in statistics and therefore indirectly in all fields of science!!!

	Finally, it could be useful to indicate that a large numbers of organizations (private or institutional) make false statistics because the assumptions and conditions of use of these confidence intervals (verbatim hypotheses tests) are not rigorously verified or simply omitted or worse, the whole base (measurements) is not collected in the rules of art (reliability of the data collection and reproducibility protocols not validated by scientific peer).
	
	The reader must also know that we have put many other confidence interval techniques detailed proofs related for example for regression techniques in the section of Theoretical Computing page \pageref{regression techniques}.

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The practitioner should be very careful about the calculation of confidence intervals and the use of hypothesis testing in practice. This is why, to avoid trivial usage error or interpretation error, it is important to refer to the following international standards eg: ISO 2602:1980 \textit{(Statistical interpretation of test results - Estimation of the mean - Confidence interval)}, ISO 2854:1976 (\textit{Statistical interpretation of data - Techniques of estimation and tests relating to means and variances}), ISO 3301:1975 (\textit{Statistical interpretation of data - Comparison of two means in the case of paired observations}), ISO 3494:1976 (\textit{Statistical interpretation of data - Effectiveness of tests relating to means and variances}), ISO 5479:1997 (\textit{Statistical interpretation of data - Tests for departure from the normal distribution}), ISO 10725:2000 + ISO 11648-1:2003 + ISO 11648-2:2001 (\textit{Sampling plans and procedures for acceptance for control of bulk materials}), ISO 11453:1996 (\textit{Statistical interpretation of data - Tests and confidence intervals relating to proportions}), ISO 16269-4:2010 (\textit{Statistical interpretation of data - Detection and treatment of outliers}), ISO 16269-6:2005 (\textit{Statistical interpretation of data - Determination of statistical tolerance intervals}), ISO 16269-8:2004 (\textit{Statistical Interpretation of data - Determination of prediction intervals}), ISO / TR 18532:2009 (\textit{Guidelines for the application of statistical quality and industrial standards}).
	\end{tcolorbox}
	
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Caution! The reader must not expect to find in this book a detailed derivation of the almost 500 parametric, non-parametric and bayesian hypothetical tests and their respective confidence intervals! It would take tenth of thousand of pages to do such a work. However we provide in the section of Numerical Methods at  page \pageref{list of statistical tests}, an exhaustive list of all these kind of tests and even an illustration (see page \pageref{mindmap of data science}) categorizing them by family (there may be some categorization errors!).
	\end{tcolorbox}
	
	\paragraph{C.I. on the Mean with known theoretical Variance}\label{ci on the mean with know variance}\index{statistical tests!$Z$-test}\mbox{}\\\\
	Let's start with the simplest and most common case that is the determination of the number of individuals to have some confidence in the average of the measurements of a random variable assumed to follow a Normal distribution.

	First let us recall that we showed at the beginning of this chapter that the standard error (standard deviation of the mean) was under the assumptions of independent and identically distributed variables (i.i.d.):
		
	Now, before we go any further, consider $X$ as a random variable following a Normal distribution with mean $\mu$ and standard deviation $\sigma$. We would like that the random variable has for example $95\%$ cumulative probability of being in a given bounded symmetric interval. Which is therefore expressed as follows:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Therefore with a confidence interval of $95\%$ you will be right a posteriori $19$ times out of $20$, or any other level of confidence or risk level $\alpha$ ($1$-confidence level, $5\%$) that you will be set up in advance (many practitioners recommend in this beginning of the 121st century (holocene calendar) to take a $\alpha=1\%$). On average, your conclusions will therefore be good, but we can never know whether a particular decision is good! If the risk level is very low but the event still occurs, specialists then speak about a "\NewTerm{large deviation}\index{large deviation}" or a "\NewTerm{black swan}\index{black swan}". Management of outliers is addressed in ISO 16269-4:2010 \textit{Detection and treatment of outliers} that any engineer doing business statistics has to follow. 
	\end{tcolorbox}
	By centering and reducing the random variable:
	
	Let us now write $Y$ the reduced centered variable:
	
	Since the Normal centered reduced distribution is symmetric:
	
	Therefore:
	
	From there reading statistical tables of the standard Normal distribution (or by using a simple spreadsheet software), we have to satisfy the equality that:
	
	Which can easily be obtained with Microsoft Excel 11.8346 by using the function: 
	\begin{center}
	\texttt{=-NORMSINV((1-0.95)/2)}
	\end{center}
	As noted in the traditional way in the general case other than the $95\%$ one ($Z$ is the random variable corresponding to the half quantile of the chosen threshold of the standard Normal distribution):
	
	Now, consider that the variable $X$ on which we wish to make statistical inference is the average (and we show later that it follows a Normal distribution centered reduced distribution). Therefore:
	
	Then we get:
	
	from which we obviously take (normally...) the upper integer value...
	
	The latter notation is usually written in the following way highlighting better the width of the confidence interval of an underlying threshold level\footnote{Don't forget that in this book we write $Z_{\alpha/2}$ where most authors write $Z_{1-\alpha/2}$} $\alpha$:
	
	Relation named "\NewTerm{sample size estimation by Normal distribution}\index{sample size estimation by Normal distribution}".

	Thus, we now know the number of individuals we must have to ensure to get a given precision interval $\delta$ (margin of error) around the mean and that for a given percentage measures are in this range and assuming the theoretical standard deviation $\sigma$ is known (or imposed) in advance (typically used in quality engineering or surveys).
	
	In the case of polls / surveys where the sample population is large enough, then we can introduce the population correction factor fpc that we proved earlier above (we speak then of SPWR for "survey plan without replacement"). Then it comes by taking the approximation of the fpc:
	
	In other words, we can calculate the number $n$ of individuals to measure to ensure a given confidence interval (associated to the quantile $Z$) of the measured average assuming the theoretical standard deviation known (or imposed) and wishing a precision $\delta$ in absolute value of the mean.

	However ... in reality, the variable $Z$ comes from the central limit theorem (see below) that gives for a large sample size (approximately):
	
	Rearranging we get then:
	
	and as $Z$ can be negative or positive then it is more logic to write this as:
	
	Thus:
	
	That engineers sometimes write:
	
	where LCL is the lower confidence limit and UCL the upper confidence limit. This is the Six Sigma terminology (\SeeChapter{see section Industrial Engineering \pageref{six sigma}}).
	
	And we have seen earlier that for a confidence interval of $95\%$ we have $Z=1.96$. And since the Normal distribution is symmetric:
	
	Thus we finally write\footnote{Don't forget that in this book we write $Z_{\alpha/2}$ where most authors write $Z_{1-\alpha/2}$} the "\NewTerm{one sample $Z$ test}\index{one sample $Z$-test}\label{one sample z test}":
	
	where we define for all tests having the same structure, the "\NewTerm{margin error}\index{margin error}" by:
	
	If the elements sampled are not independent then we have to write obviously:
	
	where for recall as proven earlier:
	
	is the finite population correction factor.
	
	As we have already mentioned, and we will prove a little further, the arithmetic reduced centered mean of a series of independent and identically distributed random variables with finite variance asymptotically follows a standard Normal distribution, this is why the confidence interval above is very general! This is why we sometimes speak of "\NewTerm{asymptotic confidence interval of the mean}".

	These intervals obviously have for origin the fact that we work very often in statistics with samples and not the entire available population. The selected sampling thus affects the value of the punctual estimator. We then speak of "\NewTerm{sampling fluctuation}".
	
	In the particular case of an IC (confidence interval) at $95\%$, the last relation will be written:
	
	Sometimes we find the prior-previous inequality in the following equivalent notation:
	
	or more rarely with the following general notation (for all intervals):
	
	where ME stands for "\NewTerm{margin of error}\index{margin error}".
	
	We are thus now able to estimate population sizes needed to obtain a certain level of confidence $\alpha$ in an outcome or to estimate the confidence interval in which is the theoretical mean knowing the experimental (empirical) average and the estimator maximum likelihood of the standard deviation. We can of course therefore also determine the a posteriori probability that the mean is outside a given range ... (one as the other being widely used in the industry).

Finally, note that from the previous result, we deduce immediately the stability property of the Normal distribution (shown above) the following test that we find in many statistical softwares:
	
	named "\NewTerm{bilateral $Z$ test on the difference of two means}\index{bilateral $Z$ test on the difference of means}" or also sometimes named "\NewTerm{two sample $Z$ test}\index{two sample $Z$ test}" and with the corresponding confidence interval:
	
	And this is not because two means are significantly different that their confidence intervals do not overlap!!!! As shows the graph below obtained with Minitab 16 software where the test-$Z$ of the difference is significant at $95\%$:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/arithmetics/confidence_interval_line_plot_overlap.jpg}
		\caption[Line plot illustration of the overlay of two $95\%$ confidence intervals]{Line plot illustration of the overlay of two $95\%$ confidence intervals (source: Minitab 15)}
	\end{figure}
	while their mean is significantly different to a confidence level of $95\%$.
	
	Also notice that many people (especially non-scientists) don't understand why a "practical mean difference" is not the same as "statistical mean difference". But the reason is obvious! The first doesn't involve the variance and the second one do! Hence the fact that the second one is more rigorous!!! 
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The size of the parent population for the relations developed above does not come into consideration in the calculations of confidence intervals or even not in the sample size, and because it is considered as infinite. So be careful not sometimes not to have sample sizes that are larger than the actual parent population... 
	\end{tcolorbox}
	
	\paragraph{C.I. on the Variance with known theoretical Mean}\label{ci on the variance with known mean}\index{statistical tests!$\chi^2$ test for variance}\mbox{}\\\\
	Let's start by demonstrating a fundamental property of the Chi-square distribution:

	If a random variable $X$ follows a Normal centered reduced distribution $X=\mathcal{N}(0,1)$ then its square follows a chi-square distribution of $1$ degree of freedom:	
	
	This result is sometimes named a "\NewTerm{Wald statistics}\index{Wald's statistics}" and any statistical test using it directly (we should better speak about a "test family") can be designated under the name "\NewTerm{Wald's test}\index{Wald's test}" (for a concrete example see further below the Cochran-Mantel-Haenszel test page \pageref{cochran mantel test}).
	
	\begin{dem}
	To prove this property, it suffices to calculate the density of the random $X^2$ variable with $X=\mathcal{N}(0,1)$. However, if $X=\mathcal{N}(0,1)$ and if we set $Y=X^2$, then for all $y \geq 0$ we get:
	
	Since the standard Normal distribution is symmetric about $0$ for the random variable $X$, we can write:
	
	Denoting by $\Phi$ the cumulative distribution function of the standard Normal distribution, we have:
		
	and as:
	
	therefore:
	
	The cumulative distribution function of the random variable $Y=X^2$ is thus given by:
	
	if $y$ is greater than or equal to zero, null if $y$ is less than zero. We will denote this cumulative distribution $f_Y(y)$ for the further calculations.

	Since the density distribution function is the derivative of the cumulative distribution function and $X$ follows a Normal centered reduced distribution so we reduced for the random variable $X$:
	
	and then it follows for the probability distribution of $Y$ (which is the square of $X$ for reminder!):
	
	this last expression corresponds is exactly the relation we obtained during our study of the chi-square distribution imposing a degree of freedom equal to the unit.

	The theorem is therefore proved, that is if $X$ follows a Normal centered reduced distribution while its square follows a Chi-square distribution of $1$ degree of freedom as:	
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}	
	\end{dem}
	
	This type of relation is used mainly in industrial processes and their control (\SeeChapter{see section Industrial Engineering page \pageref{quality control charts}}).
	
	Now let us open a parenthesis that is quite important in some linear regression software reports and especially in the curvature test for design of experiment (\SeeChapter{see section Industrial Engineering page \pageref{doe}}). Let us recall that we have:
	
	And we have just prove above that:
	
	Therefore:
	
	And as we have also seen that:
	
	it follows that:
	
	or more commonly in practice\label{relation Student and Fisher degrees and freedom}:
	

	We will now use a result proved during our study of the Gamma distribution. We have effectively seen that the sum of two random variables following a Gamma distribution also follows a Gamma distribution where the two parameters are added:
	
	As the Chi-square distribution is a special case of the Gamma distribution, the same result applies.

	To be more precise, this is equivalent to say: If $X_1,\ldots ,X_k$ are random independent and identically distributed (i.i.d.) variables $\mathcal{N}(0,1)$ then by extension of the above proof where we have shown that:	
	
	and by the property of linearity of the Gamma distribution, then sum of their squares follows a chi-square distribution of degree $k$ such that:
	
	Thus, the distribution of $\chi^2$ of $k$ degrees freedom is the probability distribution of the sum of squares of $k$ Normal centered reduced variables linearly independent of each other. It is in fact the linearity property of the chi-square distribution (implicitly the linearity of the Gamma distribution)!

	Now see another significant property of the chi-square distribution: If $X_1,\ldots ,X_n$ are independent and identically distributed $\mathcal{N}(\mu,\sigma)$ (thus the same mean and the same standard and following a Normal distribution) random variables and if we write the maximum likelihood estimator variance by:
	
	then, the ratio of the random variable $S_*^2$ on the standard deviation assumed to be known for the entire population ("the true standard deviation" or "theoretical standard deviation"!) multiplied by the number of individuals $n$ population follows a chi-square distribution of degree $n$ such that:
	
	This result is named the "\NewTerm{Cochran theorem}\index{Cochran theorem}\label{Cochran theorem}" or "\NewTerm{Fisher-Cochran theorem}\index{Fisher-Cochran theorem}" (in the particular case of Gaussian samples) and thus gives us a distribution for the empirical standard deviations (whose parent law is a Normal distribution!).

	Using the value of the standard deviation proved during our study of chi-square distribution we have:
	
	But $n$ and $\sigma$ are imposed and are therefore considered as constants. We have therefore:
	
	And therefore we have an expression of the standard deviation of the empirical standard deviation if we know the standard deviation of the population:
	
	But we have proved during our study of estimators that:
	
	It follows:
	
	It follows therefore the sometimes important relation in the practice of the estimator of the standard deviation of ... the standard deviation:
	
	Recall that the parent population is said to be "infinite" if the sample selection with replacement or if the size $N$ of the parent population is much higher than this of the sample of size $n$.
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} In laboratories the $X_1,\ldots ,X_n$ can be seen as a class of individuals of the same product identically studied by different research teams with instruments of the same precision (standard deviation of the measure identically equal).\\
	
	\textbf{R2.} $S_{*}^2$ is the "\NewTerm{inter-class variance}\index{inter-class variance}" also named "\NewTerm{explained variance}\index{explained variance}". So it gives a measure of the variance occurring in different laboratories.
	\end{tcolorbox}
	
	What is interesting here is that from the calculation of the chi-square distribution and by knowing $n$ and the standard deviation $\sigma^2$ it is possible to estimate the interclass variance (and also interclass standard deviation).

	To see that this latter property is a generalization of the basic relation:
	
	it suffices to see that the random variable $nS_*^2/\sigma^2$ is a sum of $n$ squares of $\mathcal{N}(0,1)$ independent of each other. Indeed, recall that a centered reduced random variable (see our study of the Normal distribution) is given by:
	
	Therefore:
	
	However, since the random variables $X_1,\ldots ,X_n$ are independent and identically distributed according to a Normal distribution, then the random variables:
	
	are also independent and identically distributed according to a Normal distribution but a centered reduced one.
	
	Since:
	
	rearranging we get:
	
	So on the population of measurements, the true standard deviation follows the relation given above. It is therefore feasible to make statistical inference on the standard deviation When the theoretical mean is known (...).
	
	Since the chi-square distributions is not symmetric, the only way to make this inference is to use numerical calculations and then we denote the confidence interval at the level of $95\%$ (for example ...) as follows:
	
	Either by writing $95\%=1-\alpha$:
	
	the denominator being obviously the quantile of the chi-squared distribution. This relation is rarely used in practice as the theoretical average (mean) is not known. In order to avoid confusion, the latter relation is often denoted as follows:
	
	Let's see the most common case:
	
	\paragraph{C.I. on the Variance with known empirical Mean}\label{ci on the variance with empirical mean}\mbox{}\\\\
	Let us now make statistical inference when the theoretical average $\mu$ of the population (i.e. the mean) is not known. To do this, consider now the sum of:	
	
	where for recall is $\bar{X}$ the empirical average (arithmetic mean) of the sample:
	
	Continuing the development we have:
	
	However, we have proved earlier in this chapter that the sum of the deviations from the mean was zero. So:
	
	and by taking back the unbiased estimator of the Normal distribution (we change notation to respect the traditions and differentiate the empirical average of the theoretical mean):
	
	Thus:
	
	or with another common notation:
	
	Since the second term (squared) follows a Normal centered reduced distribution too, then if we remove it we get by the proof made earlier above about the chi-square distribution property, the following result\label{chi-squared sample variance proof}:
	
	These developments allow us this time to also make inferences about the variance of a $\mathcal{N}(0,1)$ distribution when the parameters $\mu$ and $\sigma$ of the parent population are both unknown. It is this result that gives us, for example, the confidence interval:
	
	when the theoretical average (mean) $\mu$ is unknown. And also to avoid any confusion, it is more usual to write:
	
	In the same way as above, we can calculate the standard deviation of the standard deviation that has a great importance in the practice of finance:
	
	
	\pagebreak
	\paragraph{C.I. on the Mean with known empirical Variance}\index{statistical tests!Student $T$-test}\mbox{}\\\\ 
	We have proved much higher that the Student distribution came from the following relation:
	
	if $Z$ and $U$ are independent random variables and if $Z$ follows a Normal centered reduced distribution $\mathcal{N}(0,1)$ and $U$ a chi-square distribution $\chi^2(k)$ as:
	
	and remember that its density function is symmetrical!
	
	Here is a very important application of the above result:
	
	Suppose that $X_1,\ldots ,X_n$ is a random sample of size $n$ from a distribution $\mathcal{N}(\mu,\sigma)$. So we can already write that following developments made above:
	
	And for $U$ that follows a $\chi^2(k)$ distribution, then if we ask that $k=n-1$ then according to the results above:
	
	We then get after some trivial simplifications:
	
	So since:
	
	follows a Student distribution with parameter $k$ then we get the "\NewTerm{independent one-sample $T$-test}\index{independent one-sample $T$-test}" or simply named "\NewTerm{one-sample $T$-test}\index{one-sample $T$-test}":
	
	which also follows a Student distribution of parameter $n-1$ and is widely used in laboratories for calibration testing\footnote{The engineer must not forget that a calibrated instrument, does not mean that is measures correctly! However it means that it has known values of incertitude on its measurements relatively to a reference or a specification! }.

	This gives us also after rearrangement:
	
	This allows us to make inference about the mean $\mu$ of a Normal distribution with the theoretical standard deviation being unknown (meaning that there is not enough experimental values) but where the unbiased estimator of the standard deviation is known. It is this result that gives us the confidence interval\footnote{Don't forget that in this book we write $T_{\alpha/2}$ where most authors write $T_{1-\alpha/2}$}\label{student confidence interval of the mean}:
	
	where we see the same factors as for the statistical inference on the average (mean) of a (theoretical) random with know standard deviation as the Student distribution is asymptotically equal to the Normal distribution for large values of $n$. Thus, the previous interval and the following interval:
	
	gives very similar values (to three decimal places) for values of $n$ at around $10,000$ (in practice we consider that for $100$ this is the same...).
	
	We immediately deduce by the stability property of the chi-square distribution (proved above in that this property arises from the Gamma distribution) the following test that we find in many statistical software:
	
	named "\NewTerm{bilateral $t$ (Student) test on the difference of two means}\index{bilateral $t$ (Student) test on the difference of two means}" or more simply "\NewTerm{two sample $T$-test}\index{two sample $T$-test}".

	We can of course therefore also determine the probability that the mean is inside or outside a certain range ... (the both case being widely used in industry).

	The reader can for fun control with Microsoft Excel 11.8346 that for a large number of measurements $n$, the Student distribution tends to the Normal centered reduced distribution by comparing the values of the two functions below:
	\begin{center}
	\texttt{=T.INV(5\%/2,n-1)}\\
	\texttt{=NORM.S.INV(5\%/2)}
	\end{center}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The previous result was obtained by William S. Gosset around 11910 (holocene calendar). Gosset who had studied mathematics and chemistry, worked as a statistician for the Guinness brewery in England. At that time, we knew that if $X_1,\ldots ,X_n$ are independent and identically distributed random variables then:
	
	However, in statistical applications we were rather obviously interested in the following quantity:
	
	We then merely assume that this amount followed almost a Normal centered reduced distribution, which was not a bad approximation as can show the image below ($\mathrm{d}f=n-1$):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}
		\begin{axis}[
		    %axis equal=true,
		    xlabel={$x$}, ylabel={$f(x)$},
		    legend style={nodes={scale=0.5, transform shape}},
		    legend entries={Normal law $\mathcal{N}(0.1)$, $\mathrm{df}=1$, $\mathrm{df}=2$, $\mathrm{df}=5$, $\mathrm{df}=10$},
		    ymin=0, ymax=0.4, xmin=-5, xmax=5,
		    xtick distance=1,
		    ytick distance=0.1
		]
			\addplot[red, samples=101] {1/(1*sqrt(2*pi)) * exp(-x^2/2)};
		    \addplot[blue, samples=101] {1/sqrt(pi)/1.772454 * (1+x^2)^(-1)};
		    \addplot[green, samples=101] {1/sqrt(2*pi)* 0.886226/1 * (1+x^2/2)^(-3/2)};
		    \addplot[violet, samples=101] {1/sqrt(5*pi)* 2/1.329340 * (1+x^2/5)^(-3)};
		    \addplot[brown, samples=101] {1/sqrt(10*pi)* 52.3427777/24 * (1+x^2/10)^(-5.5)};
		\end{axis};
		\end{tikzpicture}
		\caption{Comparison between the Normal and Student distribution functions}
	\end{figure}
	After numerous simulations, Gosset came to the conclusion that this approximation was valid only when $n$ is large enough (so that gave him the indication that there must be somewhere behind the central limit theorem). He decided to determine the origin of the distribution and after completing a course in statistics with Karl Pearson he obtained his famous result that he published under the pseudonym Student. Thus, is why we name "Student distribution" that law that should have been named the "Gosset distribution". 
	\end{tcolorbox}
	Finally, note that the Student's $T$-test is also used to identify whether changes (increasing or vice versa) in the average of two identical populations are statistically significant. That is to say, if the size of two dependent samples is the same then we can create the following test (we included all different types of writing that can be found in the literature and in many software implementing this test):
	
	With:
	
	And in the homoscedastic case where $\text{V}(X_1)=\text{V}(X_2)$, remembering that:
	
	Hence:
	
	The relation:
	
	 is very useful for comparing the same sample twice in different measurement situations (sales before or after a discount on an article for example). This relation is named "\NewTerm{$T$-test (Student) averages two paired samples (or dependent samples)}" or more simply "\NewTerm{paired sample $T$-test}\index{paired sample $T$-test}\label{paired t-test}\index{statistical tests!paired $T$-test}".

	\textbf{Definition (\#\thesection.\mydef):} We speak of "\NewTerm{paired samples}\index{paired samples}" if the sample values are taken 2 times on the same individuals (i.e. the values of the pairs are not independent, unlike two samples taken independently).
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	For paired tests the distribution of each sample is completely unimportant! What matters is the distribution of the "pair differences"!!
	\end{tcolorbox}
	
	\paragraph{C.I. on the Geometric Mean (for known or unknown Variance)}\label{geometric mean test}\mbox{}\\\\
	Let's just see how to compute the confidence interval for the geometric mean that is of main importance in finance! Consider we have $X_{1}, \ldots X_{n}$ from some distribution with mean $\mu$ and variance $\sigma^{2}$.

	Consider $Y_{i}=\ln(X_{i})$ (in practice some people take the $\log$ rather than the $\ln$), where the arithmetic mean of $Y$ is $\mu_{y}$ and it corresponding variance is $\sigma_{y}^{2}$. Consider the average of the $Y$s: $\bar{Y}_{n}=\sum_{i=1}^{n} Y_{i} / n$. Then due to the central limit theorem:
	
	Now consider $e^{\bar{Y}_{n}}$:
	
	Thus, $e^{\bar{Y}}$ is the geometric mean of the $X_i$! So next, we can apply the Delta method to the central limit theorem. Define $g(x)=e^{x}$, then $g^{\prime}(x)=e^{x}$. By the Delta method (\SeeChapter{see section Theoretical Computing page \pageref{delta method}}):
	
	Notice well the $e^{2 \mu_{y}}$ in front of the $\sigma_{y}^{2}$! So we you have a tool to make our confidence intervals from!
	
	$e^{\mu_{y}}$ is our true geometric mean of the $Y$s, and we need to make a confidence interval for this (this is not a confidence interval for the expected value $\mu$ ). The first step is to estimate $\sigma_{y}^{2}$. Since $\sigma_{y}^{2}$ is the variance of the $Y$s:
	
	To make our $100(1-\alpha)\%$ confidence interval for the true geometric mean of the $X$s we then have:
	
	That can also be written:
	
	This is the "\NewTerm{$Z$-test of the geometric mean}\index{statistical tests!$Z$-test of the geometric mean}" (when the variance is known).
	
	Keep in mind that the $Z$-test of the geometric mean must be used only if the $\sigma_y$ is known. Otherwise we should use the Student version of that test! That is:
	
	This is the "\NewTerm{$T$-test of the geometric mean}\index{statistical tests!$T$-test of the geometric mean}".
	
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Caution! Many people and statistical softwares calculate this interval the wrong way. For example in the Stata documentation in 12021 (holocene calendar), the method of calculation is explicitly described as: \og \textit{To compute the geometric mean, the function \texttt{ameans} creates $u_j=\ln(x_j)$ for all positive $x_j$. The arithmetic mean of the $u_j$ and its confidence interval are then computed as in the function \texttt{ci}.  Let $\bar{u}$ be the resulting mean, and  let $[L, U]$ be  the  corresponding  confidence  interval.  The  geometric  mean  is  then $\exp(\bar{u})$, and its confidence interval is $[\exp(L),\exp(U) ]$.} \fg{}. And sadly the package DescTools of the \texttt{R} statistical software and its function Gmeans() is based on Stata method...
	\end{tcolorbox}
	
	\paragraph{Binomial exact Test}\label{binomial exact test}\index{statistical tests!binomial exact test}\mbox{}\\\\
	Often when measuring we want to compare two small samples taken randomly (without replacement!) from also a small population ... to know if they are statistically significantly different or not as when we were expecting a perfect equality!

	We are looking for a suitable test for the following cases:

	\begin{itemize}
		\item To know if the sample of a population prefers to use a given technical method of work rather than another when we expect that the population does not prefer one of the other

		\item To know if the sample of a population has a predominant characteristic among two possibilities when we expect that the population is well balanced
	\end{itemize}

	Before going further into details, let us remind that we must be extremely cautious about how to get the two samples. The experience must be unbiased, this is to say for reminder, that the sampling protocol must not favour one of the both characteristics of the population (if you study the balance between man/woman in a population by attracting people for the survey with a gift in the form of jewellery or just by calling during the workdays you will have a biased sample ... because you'll probably naturally have more women than men...).

	This said, this situation match with a binomial distribution for which we proved earlier in this chapter that the probability of $k$ successes in a population of size $N$ with a probability of success is p (probability of failure $q$ being therefore $1 - p$) was given by the relation:
		
	In the case before we are interesting we have $p=q=0.5$:
	
	while remembering that the distribution will not be symmetrical and especially if the population size $N$ is small.

	If we now denote by $x$ the number of successes (considered as the size of the first sample) and $y$ is the number of failures (considered as the size of the second sample), then we have:
	
	This being done, to build the test and by the asymmetry of the distribution, we will calculate the cumulative probability that $k$ is smaller than the $x$ obtained by the experience and sum it to the cumulative probability that $k$ is greater than the $y$ obtained by the experiment (which corresponds to a cumulative probability of respectively left and right tails of the distribution). So this sum corresponds to the probability:
	
	and this last relation is named "\NewTerm{binomial exact test (two-tailed)}\index{binomial exact test (two tailed)}".

	If the probability $P$ obtained by the sum is above a certain cumulative probability fixed in advance, then we say that the difference with a random sample in a perfectly balanced population is not statistically significant (bilaterally ...) and respectively if it is below, the difference will be statistically significant and therefore we reject the assumed equilibrium.

	Therefore, if:
	
	the difference with a balanced population will be considered not statistically significant. Often we will $\alpha$ to be at the maximum equal to $5\%$ (but rarely below) which corresponds to a confidence interval of $95\%$.

	Unfortunately from a statistical software to the other the required parameters or results will not necessarily be the same (spreadsheets softwares for example do not include a specific function for the binomial test, will often have to build a table or develop yourself a function). For example, some softwares automatically calculate and impose (which is quite logical in a sense...)
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\textbf{{\Large \ding{45}}Example:}\\\\
	From a small population having two particular characteristics $x$ and $y$ that interest us and which we expect to have a perfect balance but as $x=y$ we actually got $x=5$ and $y=7$. We would like do the calculation with Microsoft Excel 11.8346 to know whether this difference is statistically significant or not at a level of $5\%$?\\

	So, to answer this question, we will calculate the cumulative probability:
	
	which gives us:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.55]{img/arithmetics/binomial_coefficient_calculated.jpg}
		\caption[]{Calculated values of the binomial coefficients\\in Microsoft Excel 11.8346}
	\end{figure}
	thus explicitly:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.55]{img/arithmetics/binomial_coefficient_calculated_explicit.jpg}
		\caption[]{Formulas for calculating binomial coefficients\\in Microsoft Excel 11.8346}
	\end{figure}
	thus the cumulative probability being $0.774$ (i.e. $77.4\%$) the difference compared with balanced population will be considered as not statistically significant.
	\end{tcolorbox}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	This test is also used by most statistical software (such as Minitab) to give a confidence interval of the conformity of opinions in relation to that of an expert. This is what we call a R\&R study (reproducibility \& repeatability)\index{reproducibility \& repeatability study} by attributes (see the companion Minitab for an example).
	\end{tcolorbox}	
	
	\paragraph{C.I. for a Proportion}\label{proportion confidence interval}\index{statistical tests!$p$-test}\mbox{}\\\\
	For information some statisticians use the fact that the Normal distribution arises from the Poisson distribution which itself derives from the binomial distribution (we have proved it when $n$ tends to infinity and $p$ and $q$ are of the same order) to build a confidence interval in the context of the analysis of proportions (widely used in the analysis of the quality in the industry).

	To see this, we note $X_i$ the random variable defined by:
	
	where the attribute $A$ can be the property "defective" or "non-defective" for example, in an analysis of pieces. We note by $k$ the number of successes of the attribute $A$.

	The random variable $X=X_1+X_2+\ldots +X_n$ we have proved it earlier in this chapter, follows a binomial distribution with parameters $n$ and $p$ with the moments:
	
	That said, we do not know the true value of $p$. We will use the estimator of the binomial distribution proved above:
	
	Based on the properties of the mean we have then:
	
	And by using the properties of the variance, we have the following relation for the variance of the sample mean of the proportion:
	
	This then brings us to:
	
	Finally, remember that we have proved that the Normal distribution resulted from the binomial distribution under certain conditions (practitioners admit that it is applicable as $n>50$ and $np \geq 5$). In other words, the random variable $X$ following a binomial distribution follows a Normal distribution under certain conditions. Obviously, if $X$ follows a Normal distribution then $X/n$ also (and so do $\hat{p}$...). Therefore we can center and reduce $\hat{p}$ so that it behaves as the reduced Normal centered random variable denoted by $Z$:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. If $5\%$ of the annual production of a business fails, what is the probability that by taking a sample of $75$ pieces of the production line only $2\%$ or less will be defective?\\

	We therefore have:
	
	The corresponding cumulative probability to that value can be easily obtained with Microsoft Excel 11.8346:

	\begin{center}
	\texttt{=NORMSDIST(-1.19)=11.66\%}
	\end{center}
	But note that we do not have $np\geq 5$ that is satisfied therefore we could exclude to use this result.\\
	
	E2. In its report from 11998 (holocene calendar), JP Morgan explained that during the year 11998 (holocene calendar) its losses went beyond the Value at Risk (\SeeChapter{see section Economy page \pageref{value at risk}}) $20$ days on $252$ working days of the year based on a $95\%$ temporal VaR (thus $5\%$ of working days considered as loss). At the threshold of $95\%$ it is just bad luck or is that the VaR model used was bad?
	
	So it was just bad luck.\\
	
	E3. Some individuals expect all people declared clinically death to experience NDE (Near Death Experience). The study \textit{Near-death experience in survivors of cardiac arrest: a prospective study in the Netherlands} on a sample of $344$ patients has $62$ patients ($18\%$) that reported a NDE (i.e. $82\%$ did not experience NDE). We want to know if $82\%$ is statistically significantly different of the expected $100\%$ at a threshold of $5\%$:
	
	The corresponding cumulative probability to that value can be easily obtained with Microsoft Excel 11.8346:

	\begin{center}
	\texttt{=NORMSDIST(-24.73)=100\%}
	\end{center}
	Therefore NDE is not a global phenomenon and we can not assign to it, beyond all reasonable doubt, an absolute existence! However other factors may explain the fact that only a few people experience NDE.
	\end{tcolorbox}
	We can now approximate the confidence interval for the proportion by using the fact that the binomial distribution has a Normal asymptotic behaviour under the conditions demonstrated during our introduction of the Normal distribution such as we get the "\NewTerm{one-proportion $Z$ test}\index{statistical tests!one-proportion $Z$ test}" or also more commonly named "\NewTerm{one-proportion $p$ test}\index{statistical tests!one-proportion $p$ test}":
	
	That (non-parametric) test is also commonly named "\NewTerm{binomial test}\index{statistical tests!binomial test}" and must not be confused with the binomial exact test derived at page \pageref{binomial exact test}.
	
	Before proceeding to an example, it may be useful to clarify to the reader that this approximation by a Normal distribution is very common and that we'll meet it again numerous times in proofs that will follow. It is even so common that this approximation method has a name..: the "\NewTerm{Wald's method}\index{Wald's method}" (well actually there are several Wald methods but we will only use the most known one).
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\textbf{{\Large \ding{45}}Example:}\\\\
	We take $\alpha=5\%$, then we have:
	
	That is to say:
	
	On a production of $300$ elements we found that $8$ of them were defect. What is the confidence interval?\\

	We check first with:
	
	that:
	
	So it is acceptable to use the confidence interval by the Normal distribution. We therefore have:
	
	That is to say:
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Caution! Because the comparison of two proportions is of main interest for the pharmaceutical industry, it is therefore a quite intensive mathematical research topic. At the point that the Wald confidence interval for the comparison of two proportions is not the only existing one. Some statistical softwares for (like \texttt{R} for example) give the options to use complementary (more robust or exact) confidence intervals like: the Wilson confidence interval, the Agresti-Coull confidence interval, the Agresti-Caffo confidence interval, the Newcombe confidence interval, the Miettinen-Nurminen confidence interval, the Skewness-corrected asymptotic score (SCAS) confidence interval, the Blaker confidence interval, the Clopper-Pearson confidence interval, the Hepworth confidence interval. As always... never forget the Dunning-Kruger effect... especially in Science!
	\end{tcolorbox}
	
	To conclude this subject, we can obviously be interested to the number of individuals (sample size) necessary to satisfy a certain (imposed) confidence interval accuracy when having an imposed standard deviation.

	We therefore have according to the above assumptions and in the acceptance of the approximation by a Normal distribution:
	
	And by proceeding in an identical manner to the developments made above with the Normal distribution, we get the "\NewTerm{sample size for proportion estimation by binomial distribution}\index{sample size!for one proportion}":
	
	obviously we normally take the integer value in practice...
	
	Applying the same reasoning for two groups of proportions we get the "\NewTerm{sample size for two proportions estimation by binomial distribution}\index{sample size!for two proportions}":
	

	A question that often comes up in practice is the fact to know whether you have to take a unilateral or bilateral test. In fact there is no precise answer, it depends on what we want to highlight.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The size of the parent population for the relations developed above does not come into consideration in the calculation of confidence intervals or in one of the sample size, and because it is considered infinite. So be careful to not have sometimes sample sizes that are larger than the possible real parent population... Otherwise you should insert the fpc (finite population factor).
	\end{tcolorbox}
	The reader can find in most marketing textbooks tabulated values (including the fpc!) in the result above that lead to the construction of the following table\label{table of sample sizes for surveys}:
	\begin{table}[H]
		\centering
		\begin{tabular}{|r|ccc|ccc|}
		\rowcolor[gray]{0.75}\hline & \multicolumn{3}{|c|} { \textbf{Confidence level} $=95 \%$} & \multicolumn{3}{c|} { \textbf{Confidence level} $=99 \%$} \\
		\rowcolor[gray]{0.75}\hline & \multicolumn{3}{|c|} { \textbf{Margin of error} } & \multicolumn{3}{|c|} { \textbf{Margin of error} } \\
		\rowcolor[gray]{0.75}\hline \textbf{Population size} & $5 \%$ & $2,5 \%$ & $1 \%$ & $5 \%$ & $2,5 \%$ & $1 \%$ \\
		\hline 100 & 80 & 94 & 99 & 87 & 96 & 99 \\
		500 & 217 & 377 & 475 & 285 & 421 & 485 \\
		1.000 & 278 & 606 & 906 & 399 & 727 & 943 \\
		10.000 & 370 & 1.332 & 4.899 & 622 & 2.098 & 6.239 \\
		100.000 & 383 & 1.513 & 8.762 & 659 & 2.585 & 14.227 \\
		500.000 & 384 & 1.532 & 9.423 & 663 & 2.640 & 16.055 \\
		1.000 .000 & 384 & 1.534 & 9.512 & 663 & 2.647 & 16.317 \\
		\hline
		\end{tabular}
		\caption{One question survey sample size with fpc factor}
	\end{table}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\textbf{{\Large \ding{45}}Example:}\\\\
	We would like to know the number of individuals (sample size) to take in a production lot knowing that the proportion of defective units is imposed at $30\%$ with a tolerated error of about $5\%$ between the actual and empirical proportion and to obtain a confidence interval at a level of $95\%$ of the result:
	
	\end{tcolorbox}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The last relation is very often used in sampling theory (analysis for referendum with responses of type: Yes/No) where sometimes the sample size $n$ is imposed for costs reasons of the survey and for which we seek to calculate the uncertainty $\delta$ and sometimes the reciprocal (the uncertainty is imposed and therefore we seek to know the sample size). 
	\end{tcolorbox}
	
	\pagebreak
	\subparagraph{Variance stabilizing transformation}\label{variance stabilizing transformation}\mbox{}\\\\
	Often, the variance of an estimator depends on the value of the parameter being estimated. For example, if $X_{n} \sim$ Binomial $(n, p)$, then with $\hat{p}_{n}=X_{n} / n$:
	
	In regression situations, for instance, one usually desires the dependent $Y_{i}^{\prime}$ s to have the same variance for each $i$, so that if these $Y_{i}^{\prime}$ s are binomial, or Poisson, the variance will not be constant. However, taking a function of the variables may achieve approximately equal variances. Such a function is named a "\NewTerm{variance stabilizing transformation}\index{variance stabilizing transformation}". Formally, if $\widehat{\theta}_{n}$ is an estimator of $\theta$, then we wish to find a $g$ such that:
	
	The "$1$" for the variance is arbitrary. The important thing is that it does not depend on $\theta$. In the binomial example, the variance stabilizing $g$ would satisfy:
	
	We know that:
	
	and by the univariate $\Delta$-method (\SeeChapter{see section Theoretical Computing page \pageref{delta method}}):
	
	What should $g$ be so that the variance is equal to $1$?  We need to solve to answer that question:
	
	so that:
	
	First, let $u=\sqrt{y}$, so that $y=u^{2}$ and $\mathrm{d} y=2 u \mathrm{d} u$, and:
	
	The integral is $\arcsin (u)$ (\SeeChapter{see section Differential and Integral Calculus page \pageref{usual primitives}}), which means the variance stabilizing transformation is:
	
	Note that adding a constant to $g$ won't change the derivative. The approximation suggested by:
	
	is then after rearranging the previous relation:
	
	An approximate $95\%$ confidence interval for $2 \arcsin (\sqrt{p})$ is:
	
	That interval can be inverted to obtain the interval for $p$, that is, apply $g^{-1}(u)=\sin (u / 2)^{2}$ to both ends:
	
	This interval may be slightly better than the usual approximate interval:
	
	
	\pagebreak
	\subparagraph{Test of equality (or difference) of two Proportions}\mbox{}\\\\
	Always in the same context as the previous approximation of the binomial distribution by a Normal distribution, the industry (especially biostatistics) likes to compare two proportions of two different populations to see if they are statistically equal or not (i.e. said statistically significantly different or not).

	Therefore, let us recall that we have proved the stability of the Normal distribution if two random variables are independent and identically distributed (according to a Normal distribution!):
	
	Under the above assumptions it is then approximately the same for the difference of two proportions:
	
	Therefore we know that this new reduced centered variable follows a Normal distribution as:
	
	and as we seek to know the cumulative probability that the mean of the difference is zero, the latter relation is reduced in this case to "\NewTerm{$Z$-test for the equality of two proportions unpooled}\index{statistical tests!equality two proportions test unpooled}":
	
	Obviously we can build (as always...) a confidence interval:
	
	We refer to this interval as the "\NewTerm{Wald interval}\index{Wald interval}\label{Wald interval}". Wald intervals for a single proportion or for the difference of two proportions are known to perform poorly, and are usually well below the nominal coverage), although they are still widely used in practice because of their simplicity. The Wald interval also suffers from overshoot,that is, bounds of the calculated interval can be outside the range of possible values!
	
	The above test in the field of sensitivity analysis (especially when applied to the design of websites), is sadly named because of a lack of precision the "\NewTerm{proportions  A/B test}\index{statistical tests!proportions A/B test}" (not to be confuse with the other kind of A/B tests that we will see further below). Indeed we will see later that a confusion can arise because a pooled version of that test also exist, but also an unpooled $\chi^2$ version and pooled $\chi^2$ version...
	
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Warning! The test of the difference of two proportions of two different samples is obviously not the same as the test of the difference of two proportions in the same sample (covariance oblige since the two proportions are therefore no longer independent)! In the latter case we use McNemar's test (\SeeChapter{see section Numerical Methods page \pageref{mcnemar test}}).
	\end{tcolorbox}

	However, it seems that the following approximate relation, named "\NewTerm{$Z$-test for the equality of two proportions pooled}\index{statistical tests!equality two proportions test pooled}\label{pooled z-test proportions}", following the return on experience is more correct when taking a modified denominator:
	
	where $\hat{p}$ will be taken as the mixture of the two populations. That is to say:
	
	thus (by changing the notations of the indices of the experimental proportions):
	
	This test is named "\NewTerm{two proportions $Z$ test}\index{two proportions $Z$ test}" or more simply "\NewTerm{two proportions $p$ test}\index{two proportions $p$ test}". In medicine, it is named the "\NewTerm{test of differences in risk}\index{test of differences in risk}" (meaning implicitly that each proportion is a segment of the studied population in relation to an undesired event).
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Numerical simulations seems to show that the unpooled variance is slightly more biased than the pooled one. We will also see during our study of the $\chi^2$ test of independance (see page \pageref{chi-square test of independence}) why softwares like \texttt{R} use the $\chi^2$ test instead of the $Z$-test to compare two proportions.
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	In the context of a sampling plans (\SeeChapter{see section Industrial Engineering page \pageref{sampling plans}}) we have taken on a first batch of $50$ individuals, $48$ that are in perfect conditions. In a second batch of $30$ individuals, $26$ were in perfect condition.\\

	Thus we have:
	
	We would like to know if the difference is statistically significant with a $95\%$ confidence interval or simply due to chance. We then use:
	
	and:
	
	This corresponds to a cumulative probability using Microsoft Excel 11.8346:
	\begin{center}
		\texttt{=NORMSDIST(1.535)=93.77\%}
	\end{center}
	Therefore the difference is due to chance (that said it is almost in extremis...). In other words, it is not statistically significant under the set constraints.
	\end{tcolorbox}
	
	\pagebreak
	\paragraph{Sign Test}\index{statistical tests!sign test}\mbox{}\\\\
	We measure something on a sample and later, we measure the same thing on this same sample but with a different method (so it is therefore paired samples!). Both ordered rankings of measures are compared and too each case is assigned a sign ("$+$" for an increase in the rankings, "$-$" in case of descent). Those who remain at the same level are eliminated.

	According to the hypothesis to be tested, there are so many "$+$" as "$-$", that is to say, the median of the distribution has not changed (this statement may not seem obvious at first reading so be aware to take time to think about it).

	The idea is that for each pair of values, there are only two possible signs of change, we have a chance on two ($50\%$ probability) that the difference is positive or negative. This test is based only on the study of signs of the differences between the pairs of individuals, regardless of the values o these differences.

	We can wish to control two assumptions:
	\begin{itemize}

		\item The inequality of proportions of signs must be statistically significant. So one of the two signs must be in a small number compared to the other, which corresponds to a left-sided test (the cumulative probability of the small number of characters must be below a certain level $\alpha$).
		
		\item The proportion of the two signs must be low unbalanced $(P(+)=P(-)=0.5)$. It is therefore in this case a bilateral test (the most common case) with a given level $\alpha$.
	\end{itemize}

	To create such a test, we consider the appearance of the "$+$" and "$-$" as a binary random sampling system where the order of success is not taken into account (it is therefore based on a binomial or hypergeometric distribution) and with replacement (which immediately eliminates the hypergeometric distribution that is not symmetric and problematic to use in practice...). To consider a random sampling with replacement (with the fact that we do not reinject each individual in reality), it is necessary that the population $N$ is large. This is why the sign test considers that the paired values should be continuous (which allows verbatim to approach the hypergeometric distribution by the binomial distribution). However, some statistical software use the hypergeometric distribution for the sake of precision.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	You should know that most statistical software, do implicitly the assumption in this test that the data are continuous and use therefore the binomial distribution.
	\end{tcolorbox}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
\textbf{{\Large \ding{45}}Example:}\\\\
	Consider two sets of measurements with two different methods. We would like to test the hypothesis with a confidence level $\alpha$ of $5\%$ if the difference between the two methods is statistically significant (thus we expect a balance of signs). This is therefore two samples sign test (knowing that it is also possible to do the same by comparing the values of a single sample to its median):
	
	Therefore we have the differences:
	
	With the signs:
	
	Well it is already clear that the result will be the rejection of the hypothesis as there is no difference. But we will still do the calculations. As the test is a two-sided at the level of $5\%$ , the cumulative probability of obtaining at least two signs "+" must not be less than $2.5\%$ and not more than $97.5\%$ if we want to accept (not reject) the assumption as that the difference is not statistically significant.\\
	
	We then have:
	
	Either with Microsoft Excel 14.0.6123:
	\begin{center}
		\texttt{=BINOMDIST(2,12,0.5,1)=1.928\%}
	\end{center}	
	or if we don't do the approximation by being more accurate with the hypergeometric distribution:		
	\begin{center}
		\texttt{=HYPGEOM.DIST(2,24/2,12,24,TRUE)=0.17\%}
	\end{center}	
	which is not really better...!\\
	
	So the cumulative probability is less than $2.5\%$ and is by far not more than $97.5\%$, therefore we reject the hypothesis as that the difference is not statistically significant.\\
	
	We could accept the hypothesis if we take for $\alpha$ the value:
	
	but this is not the case!
	\end{tcolorbox}
	To conclude on this sign test (median test), we have for information some statistical softwares that propose a confidence interval of the median based on the calculation method described previously (confidence interval of the binomial distribution). However, we think that it would better to use bootstrapping techniques as we have seen in the section on Numerical Methods (page \pageref{bootstrap}), so won't introduce this technique here. In addition it may be useful to know that some make an approximation using the Normal distribution (as with most tests but we won't study this approximation in this context).

	\paragraph{Mood's Median Test}\index{statistical tests!Moods's median test}\mbox{}\\\\
	Here we will introduce a test that has many names: "\NewTerm{median test}\index{median test}", "\NewTerm{Mood's median test}\index{Mood's median test}" or "\NewTerm{Westenberg-Mood's median test}\index{Westenberg-Mood's median test}" or "\NewTerm{Brown-Mood's median test}\index{Brown-Mood's median test}" ...

	We consider two independent samples $(X_1,\ldots ,X_{n_1})$ and $(Y_1,\ldots ,Y_{n_2})$. We assume that $(X_1,\ldots ,X_{n1})$ is an independent and identically distributed sample from a continuous distribution $F$ and $(Y_1,\ldots ,Y_{n_2})$ is an independent and identically distributed sample from a continuous distribution $G$.

	After the grouping of the $n_1+n_2$ values of the two samples, $k=n_1M_n$ is the number of observations $X_i$ of the first sample that are greater than the median $N=n_1+n_2$ of the observations (the notation is not great because it can give the impression that it is a multiplication...).

	Under the null hypothesis that $X$ and $Y$ variables follow the same continuous distribution (that is to say, $G = F$) hypothesis, the variable $k=n_1M_n$ can take the values $0,1,\ldots ,n_1$ according to the hypergeometric distribution:
	
	Therefore, we can calculate the unilateral cumulative probability of having $k$. Mood's test is also a purely unilateral test.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
\textbf{{\Large \ding{45}}Example:}\\\\
	Consider the two samples:
	
	The overall median calculated with Microsoft Excel 14.0.6123 is $26.10$. We have a total of:
	
	Then it comes with Microsoft Excel 14.0.6123:
	\begin{center}
		\texttt{=HYPGEOM.DIST(8,26/2,13,26,TRUE)=94.24\%}
	\end{center}
	So at a threshold of $5\%$, we do not reject the null hypothesis (but... being close to the limit this is a bit tedious to conclude that...). If we do the same calculation using the binomial distribution we obtain:
	\begin{center}
		\texttt{=BINOM.DIST(8,26/2,0.5,1)=86.65\%}
	\end{center}
	But obviously here the approximation does not apply since a binomial approximation is acceptable in practice when the sample is about $10$ times smaller than the population.
	\end{tcolorbox}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Unfortunately, there are several versions of the Mood test. For example, a software such as Minitab compares thanks to a contingency table... the values above or below the median and made a simple chi-square test of independence (Pearson's test) as seen in the section of Numerical Methods. 
	\end{tcolorbox}
	
	\paragraph{Poisson Test (1 sample)}\index{statistical tests!Poisson test}\mbox{}\\\\
	We know that a number of rare events follow a Poisson distribution. We can then allow us as for any other distribution to calculate the cumulative probability in a given interval (bilateral or unilateral).

	So if we have a discrete random variable following a Poisson distribution:
	
	We then have to a certain right sided level of confidence $\alpha$, the closest value $n$ of $k$ satisfying the condition:
	
	So to find the value $n$ (strictly positive integer or null value) we should reverse the sum, which is not... something funny to do (this is why most spreadsheet softwares do not offer at this day the inverse of the Poisson distribution).

	Now recall that we have seen in the section on Sequences and Series, the following Taylor (Maclaurin) series with full integral rest to order $n - 1$ around $0$ to $\lambda$:
	
	
	Result we had also given in the form of functions for Microsoft Excel 14.0.6123 so that the reader can verify this equivalence:
	\begin{center}
		\texttt{=POISSON(}$x \in \mathbb{N},\mu,$\texttt{TRUE)}\\
		\texttt{=1-CHIDIST(}$2\mu,2(x+1),$\texttt{TRUE)}
	\end{center}
	Then it follows that in spreadsheets softwares, we can use the inverse chi-square distribution to calculate the inverse of the Poisson distribution with this time however a small nuance: the result will not necessarily be an integer.

	If for example we take (always with Microsoft Excel 14.0.6123)
	\begin{center}
		\texttt{=1-CHI.DIST(2*20,2*(15+1),TRUE)=15.6513135\%}
	\end{center}
	The question is then to find the notation for the opposite ... This is then given by (we divide by $2$ to fall back on the mean that is the value of interest):
	\begin{center}
		\texttt{=CHIINV(1-15.6513135\%,2*(15+1))/2=15.53194258}
	\end{center}
	Finally, the notation of the inverse is relatively natural. Thus, the "\NewTerm{$1$ sample Poissons' test}\index{Poissons' test with $1$ sample}" at a given right sided $\alpha$ level can be written:
	\begin{center}
		$k\leq $\texttt{=CHIINV(1-alpha,2*(number of measures+1))/2}
	\end{center}
	Formally:
	
	Note however one thing! It seems that some statistical software approximate sometimes with abuse the Poisson distribution by a Normal distribution. Therefore, the unilateral interval is calculated with:
	
	But with the Poisson distribution, remember that we have:
	
	Therefore:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	 A company manufactures televisions in a constant quantity and has measured the number of defective product produced each quarter for the past ten years (so $4$ times $10$ measures). The stakeholders determines the maximum acceptable number of defective units is $20$ per quarter and wants to determine if the production satisfies these requirements (under the assumption that the distribution of defective follow a Poisson distribution) at a confidence level of $5\%$.\\
	
	The $40$ measures give us an average of:
	
	Then we have with the rough approximation:
	
	Either in a spreadsheet software like Microsoft Excel 14.0.6123:
	\begin{center}
		\texttt{=NORM.S.INV(1-5\%)*SQRT(20/40)+17.825=18.988}
	\end{center}
	or:
	
	Either in a spreadsheet software like Microsoft Excel 11.8346:
	\begin{center}
		\texttt{=CHIINV(1-5\%,2*(20+1))/2=14.072 }
	\end{center}
	In both cases, we are below the imposed average of $2 $ (so we reject the null hypothesis that the number of defects is greater than or equal to $20$). Obviously, it is possible for each of the methods to determine what should be the cumulative probability (confidence level) which brings us to the $20$ limit (therefore the $p$value in other words on which we will come back further below). With the first method (Normal approximation), the $p$-value is $0.104\%$.
	\end{tcolorbox}
	
	Obviously, in the bilateral case, we have:
	
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	 An airline company had $2$ crash on $1,000,000$ flights (very rare event). What is two-sided the confidence at the level of $95\%$ knowing that globally the number of accidents per million is $0.4$.\\

	We have therefore:
	
	Either the upper bound with a spreadsheet software like Microsoft Excel 11.8346 is given by:
	\begin{center}
		\texttt{=CHIINV(1-5\%/2,2*(2+1))/2=7.224}
	\end{center}
	and for the lower bound:
	\begin{center}
		\texttt{=CHIINV(1-5\%/2,2*(2+1))/2=0.618 }
	\end{center}
	So statistically, the company is less secure than all companies.
	\end{tcolorbox}
	
	\paragraph{Poisson Test (2 samples)}\mbox{}\\\\
	We have just seen that:
	
	However, following the same reasoning that led us to construct the following test of average comparison:
	
	or its equivalent with the Student distribution when the true standard deviation is not known and using the fact that we have shown that the Poisson distribution is stable by the addition (and hence by subtraction), that Gamma distribution was also stable by the addition (and therefore by subtraction) and also the chi-square distribution since it is only a special case of the gamma distribution, we tend to write perhaps a little bit to fast the extension of what we have seen before:
	
	And in the facts this is a trap as say some practitioners ... because the chi-square distribution has a support which is defined as being strictly positive and the confidence interval could naturally have a negative left terminal (... O\_o). One solution could be to use the test of the difference of two proportions that we have already discussed earlier:
	
	Of course, only in the case that the conditions for approaching the test with a Normal distribution are met (proportions have to be typically less than $0.1$ and no greater than $50$).
	
	Most softwares seem to have implemented this latter method (with which I do not necessarily agree).
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\textbf{{\Large \ding{45}}Example:}\\\\
	An airline company had $2$ airplanes crash was in $1,000,000$ flights (very rare event). Another company had $3$ crashes in $1,200,000$ flights. What is the two sided confidence interval at the level of $95\%$ assuming that the difference is zero.\\

	Therefore, the proportions are:
	
	We write:
	
	
	Then we have:
	
	which gives a confidence interval for the expected theoretical proportion difference:
	
	and therefore as $-0.0000005$ is in this interval, we accept the hypothesis as the difference of proportions are not statistically significant at the threshold of $5\%$.\\

	Or taking the non approximated expression, we have (with the same conclusion):
	\begin{gather*}
		\dfrac{\chi_{5\%/2}^2(2(3+1)-2(2+1))}{2}\leq \lambda_2-\lambda_1\leq \dfrac{\chi_{1-5\%/2}^2(2(3+1)-2(2+1))}{2}\\
		\Downarrow\\
		0.0253 \leq \mu \leq 3.6889
	\end{gather*}
	\end{tcolorbox}
	
	So to summarize some convergence of distributions in all these different tests and intervals that we have seen so far, we offer the reader the following diagram that we hope... will clarify perhaps more or less things:
	\begin{figure}[H]
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,387); %set diagram left start at 0, and has height of 387
		
		%Straight Lines [id:da10258366129304974] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (287.5,45.67) -- (287.5,122.67) ;
		\draw [shift={(287.5,125.67)}, rotate = 270] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da2348794039880886] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (209.5,152.67) -- (142.15,188.26) ;
		\draw [shift={(139.5,189.67)}, rotate = 332.14] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da049238626211485226] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (387.5,152.67) -- (441.08,191.89) ;
		\draw [shift={(443.5,193.67)}, rotate = 216.21] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da12762127621247643] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (189.5,307.67) -- (298.67,269.65) ;
		\draw [shift={(301.5,268.67)}, rotate = 160.8] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da1558630596598467] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (451.5,305.67) -- (472,270.26) ;
		\draw [shift={(473.5,267.67)}, rotate = 120.07] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		%Straight Lines [id:da44302340665924933] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (441.5,33.67) -- (589.5,33.67) ;
		%Straight Lines [id:da3832118439067649] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (589.5,33.67) -- (589.5,228.67) ;
		%Straight Lines [id:da047194566376032254] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (535.5,228.67) -- (589.5,228.67) ;
		%Straight Lines [id:da7108982707489218] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (214.5,227.67) -- (272.5,227.67) ;
		\draw [shift={(275.5,227.67)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
		
		% Text Node
		\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]   (173,20) -- (441,20) -- (441,46) -- (173,46) -- cycle  ;
		\draw (176,24) node [anchor=north west][inner sep=0.75pt]   [align=left] {Hypergeometric distribution $\displaystyle \mathcal{H}( N,n,p)$};
		% Text Node
		\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]   (202,127) -- (398,127) -- (398,153) -- (202,153) -- cycle  ;
		\draw (205,131) node [anchor=north west][inner sep=0.75pt]   [align=left] {Binomial distribution $\displaystyle \mathcal{B}( n,p)$};
		% Text Node
		\draw  [color={rgb, 255:red, 144; green, 19; blue, 254 }  ,draw opacity=1 ][line width=1.5]   (37,213) -- (213,213) -- (213,239) -- (37,239) -- cycle  ;
		\draw (40,217) node [anchor=north west][inner sep=0.75pt]   [align=left] {Poisson distribution $\displaystyle \mathcal{P}( \lambda )$};
		% Text Node
		\draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ][line width=1.5]   (333,213) -- (535,213) -- (535,245) -- (333,245) -- cycle  ;
		\draw (336,217) node [anchor=north west][inner sep=0.75pt]   [align=left] {Normal distribution $\displaystyle \mathcal{N}\left( \mu ,\sigma ^{2}\right)$};
		% Text Node
		\draw  [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ][line width=1.5]   (107,308) -- (277,308) -- (277,334) -- (107,334) -- cycle  ;
		\draw (110,312) node [anchor=north west][inner sep=0.75pt]   [align=left] {Student distribution $\displaystyle T( k)$};
		% Text Node
		\draw  [color={rgb, 255:red, 65; green, 117; blue, 5 }  ,draw opacity=1 ][line width=1.5]   (362,307) -- (524,307) -- (524,335) -- (362,335) -- cycle  ;
		\draw (365,311) node [anchor=north west][inner sep=0.75pt]   [align=left] {Chi-squared distribution $\displaystyle \chi ^{2}( k)$};
		% Text Node
		\draw (290,74) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle n\ll N$};
		% Text Node
		\draw (446,14) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle n\ll N,np( 1-p)  >18$};
		% Text Node
		\draw (537,36) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \mu \simeq np$};
		% Text Node
		\draw (415,74) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \sigma ^{2} \simeq \dfrac{( N-pN)}{( N-1)} np( 1-p)$};
		% Text Node
		\draw (416,155) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle np( 1-p)  >18$};
		% Text Node
		\draw (378,191) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \mu \simeq np,\sigma ^{2} \simeq np( 1-p)$};
		% Text Node
		\draw (126,135) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle  \begin{array}{{>{\displaystyle}l}}
		p< 0.1\\
		n >50
		\end{array}$};
		% Text Node
		\draw (90,191) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \lambda \simeq np$};
		% Text Node
		\draw (215,205) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \lambda  >18$};
		% Text Node
		\draw (275,207) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle  \begin{array}{{>{\displaystyle}l}}
		\mu \simeq \lambda \\
		\sigma ^{2} \simeq \lambda 
		\end{array}$};
		% Text Node
		\draw (192,270) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle k >100$};
		% Text Node
		\draw (309,249) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \mu \simeq 0,\sigma ^{2} =1$};
		% Text Node
		\draw (431,249) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \mu \simeq k,\sigma ^{2} =2k$};
		% Text Node
		\draw (465,283) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle k >30$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Convergence of different customary distributions in elementary statistical inference}
	\end{figure}
	And also this table where all relations have been demonstrated in detail above, and some already used (others will be used later):
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|p{4cm}|c|c|}
					\hline
	\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Statistical sampling}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Statistic Average}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Statistic Standard-Deviation}} \\ \hline
			\pbox{20cm}{Mean \\ (infinite population)} & $\mu$ & $\dfrac{\sigma}{\sqrt{n}}$\\ \hline
			\pbox{20cm}{Mean \\ (finite population)} & $\mu$ & $\dfrac{\sigma}{\sqrt{n}}\sqrt{\dfrac{N-n}{N-1}}$\\ \hline
			\pbox{20cm}{Proportion \\ (finite population)} & $p$ & $\sqrt{\dfrac{p(1-p)}{n}}$\\ \hline
			\pbox{20cm}{Proportion \\ (infinite population)} & $p$ & $\sqrt{\dfrac{p(1-p)}{n}}\sqrt{\dfrac{N-n}{N-1}}$ \\ \hline
			\pbox{20cm}{$\hat{\sigma}^2$ \\ (infinite population)} & $\dfrac{n-1}{n}\sigma^2$ & $\sqrt{\dfrac{2(n-1)}{n^2}}\sigma^2$\\ \hline
		\end{tabular}
		\end{center}
		\caption[]{Table of statistical sampling proved earlier and used in part until now}
	\end{table}
	
	\pagebreak
	\paragraph{Confidence/Tolerance/Prediction/Credibility Intervals}\mbox{}\\\\
	Here we go and in order to avoid frequent confusion and before moving on to more complex subjects, we will compare the confidence interval, the credibility interval, the tolerance interval (often named "fluctuation interval" in some school programs) and finally the prediction interval.

	\textbf{Definitions (\#\thesection.\mydef):}
	
	\begin{enumerate}
		\item[D1.] The "\NewTerm{tolerance interval}\index{tolerance interval}" (or "fluctuation interval"), from the statistician point of view\footnote{At the opposite of that of the mechanical engineer where the tolerance interval is obviously something the measurement interval in which a piece will not be rejected from the production.}, is an interval containing a certain percentage (usually $68.26, 95.44 \text{ or } 99.73\%$ in the case of a Normal distribution) of individuals in a population of measures. 
		
		\item[D2.] The "\NewTerm{confidence interval}\index{confidence interval}" for a sample mean (or a sample proportion) must be read as the percentage of similarly constructed confidence intervals (usually $90, 95$ or $99\%$ in the two sided case) that will contain the true expected mean (or true expected proportion) knowing the measurement data a posteriori.
		
		\item[D3.] The "\NewTerm{credibility interval}\index{credibility interval}" for a sample mean (or proportion $p$) is the interval value to a given probability level (usually $90, 95$ or $99\%$ in the two sided case) for the expected mean (true average) or the proportion of the population where we have special prior for the mean/standard deviation or for the proportion (see further below page \pageref{credibility interval}).
		
		\item[D4.] The "\NewTerm{prediction interval}\index{prediction interval}" is used to determine an interval for a single value based on the knowledge of the sample mean and the standard deviation of the population.
	\end{enumerate}
	
	An example being more often better than a thousand words, consider the case where the mean and the standard deviation of prices are $49$ DVD are given by:
	
	Therefore we have:
	
	corresponding to the tolerance intervals according to a Normal distribution of $68.26, 95.44$ and $99.73\%$.

	But a confidence interval of $95\%$ based on the relation proved above:
	
	gives:
	
	So $95\%$ cumulative probability that the true mean is between $31.32$ and $31.78$.
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
		
		%Shape: Axis 2D [id:dp5240413197936336] 
		\draw  (133,232.2) -- (406.5,232.2)(139.5,95) -- (139.5,247.2) (399.5,227.2) -- (406.5,232.2) -- (399.5,237.2) (134.5,102) -- (139.5,95) -- (144.5,102) (163.5,227.2) -- (163.5,237.2)(187.5,227.2) -- (187.5,237.2)(211.5,227.2) -- (211.5,237.2)(235.5,227.2) -- (235.5,237.2)(259.5,227.2) -- (259.5,237.2)(283.5,227.2) -- (283.5,237.2)(307.5,227.2) -- (307.5,237.2)(331.5,227.2) -- (331.5,237.2)(355.5,227.2) -- (355.5,237.2)(379.5,227.2) -- (379.5,237.2)(134.5,208.2) -- (144.5,208.2)(134.5,184.2) -- (144.5,184.2)(134.5,160.2) -- (144.5,160.2)(134.5,136.2) -- (144.5,136.2)(134.5,112.2) -- (144.5,112.2) ;
		\draw   ;
		%Shape: Rectangle [id:dp8715810860197248] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (139.5,218.8) -- (163.5,218.8) -- (163.5,232.2) -- (139.5,232.2) -- cycle ;
		%Shape: Rectangle [id:dp01713691493415559] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (163.5,188.8) -- (187.5,188.8) -- (187.5,232.2) -- (163.5,232.2) -- cycle ;
		%Shape: Rectangle [id:dp06265320734171653] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (187.5,164.8) -- (211.5,164.8) -- (211.5,232.2) -- (187.5,232.2) -- cycle ;
		%Shape: Rectangle [id:dp6047158524231784] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (211.5,146.8) -- (235.5,146.8) -- (235.5,232.2) -- (211.5,232.2) -- cycle ;
		%Shape: Rectangle [id:dp231296516718545] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (235.5,132.8) -- (259.5,132.8) -- (259.5,232.2) -- (235.5,232.2) -- cycle ;
		%Shape: Rectangle [id:dp7687365563160633] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (259.5,136.8) -- (283.5,136.8) -- (283.5,232.2) -- (259.5,232.2) -- cycle ;
		%Shape: Rectangle [id:dp6342731506032719] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (283.5,178.8) -- (307.5,178.8) -- (307.5,232.2) -- (283.5,232.2) -- cycle ;
		%Shape: Rectangle [id:dp5834969415034525] 
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (307.5,222.8) -- (331.5,222.8) -- (331.5,232.2) -- (307.5,232.2) -- cycle ;
		%Straight Lines [id:da43499871025426806] 
		\draw    (204.5,309.6) -- (278.37,309.6) ;
		%Straight Lines [id:da8104641717347447] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (204.5,309.6) -- (204.5,301.2) ;
		%Straight Lines [id:da17387591338046926] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (204.5,318) -- (204.5,309.6) ;
		%Straight Lines [id:da39684039653833203] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (208.27,318) -- (204.5,318) ;
		%Straight Lines [id:da7410204357726378] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (208.27,301.2) -- (204.5,301.2) ;
		
		%Straight Lines [id:da4809731395915655] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (278.48,309.94) -- (278.46,318.34) ;
		%Straight Lines [id:da8274341106433054] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (278.5,301.54) -- (278.48,309.94) ;
		%Straight Lines [id:da08836838529655977] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (274.73,301.52) -- (278.5,301.54) ;
		%Straight Lines [id:da7299224732299188] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (274.69,318.32) -- (278.46,318.34) ;
		
		
		%Straight Lines [id:da5111130495541267] 
		\draw    (164.5,366.6) -- (318.24,366.6) ;
		%Straight Lines [id:da9075889413987934] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (164.5,366.6) -- (164.5,358.2) ;
		%Straight Lines [id:da5340212363218471] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (164.5,375) -- (164.5,366.6) ;
		%Straight Lines [id:da10386031607871393] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (172.35,375) -- (164.5,375) ;
		%Straight Lines [id:da22255822031514372] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (172.35,358.2) -- (164.5,358.2) ;
		
		%Straight Lines [id:da1289330188311717] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (318.46,366.94) -- (318.42,375.34) ;
		%Straight Lines [id:da8825618423117907] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (318.5,358.54) -- (318.46,366.94) ;
		%Straight Lines [id:da17113040905860655] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (310.65,358.52) -- (318.5,358.54) ;
		%Straight Lines [id:da35362858355736115] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (310.57,375.32) -- (318.42,375.34) ;
		
		
		%Straight Lines [id:da8612388312991877] 
		\draw    (127.5,427.6) -- (357.11,427.6) ;
		%Straight Lines [id:da35414488961007207] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (127.5,427.6) -- (127.5,419.2) ;
		%Straight Lines [id:da36104061053369674] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (127.5,436) -- (127.5,427.6) ;
		%Straight Lines [id:da4395123078865495] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (139.22,436) -- (127.5,436) ;
		%Straight Lines [id:da4898903749556951] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (139.22,419.2) -- (127.5,419.2) ;
		
		%Straight Lines [id:da7428504368192455] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (357.44,427.94) -- (357.37,436.34) ;
		%Straight Lines [id:da1735552907877418] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (357.5,419.54) -- (357.44,427.94) ;
		%Straight Lines [id:da05862517528142552] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (345.78,419.52) -- (357.5,419.54) ;
		%Straight Lines [id:da8939015603295644] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (345.65,436.32) -- (357.37,436.34) ;
		
		
		%Straight Lines [id:da6998122105551068] 
		\draw    (225.5,478.6) -- (249.46,478.6) ;
		%Straight Lines [id:da8887286326089592] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (225.5,478.6) -- (225.5,470.2) ;
		%Straight Lines [id:da2966863991986117] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (225.5,487) -- (225.5,478.6) ;
		%Straight Lines [id:da5150472129069892] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (226.72,487) -- (225.5,487) ;
		%Straight Lines [id:da2951360361911146] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (226.72,470.2) -- (225.5,470.2) ;
		
		%Straight Lines [id:da2672344942387914] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (249.49,478.94) -- (249.49,487.34) ;
		%Straight Lines [id:da21547676164529994] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (249.5,470.54) -- (249.49,478.94) ;
		%Straight Lines [id:da8782991858170734] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (248.28,470.52) -- (249.5,470.54) ;
		%Straight Lines [id:da09452245373494828] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (248.26,487.32) -- (249.49,487.34) ;
		
		
		%Right Arrow [id:dp2598624424909135] 
		\draw   (305,307.19) -- (389.5,307.19) -- (389.5,300.4) -- (405.5,309.7) -- (389.5,319) -- (389.5,312.21) -- (305,312.21) -- cycle ;
		%Right Arrow [id:dp2826943669027888] 
		\draw   (326.5,364.06) -- (392.92,364.06) -- (392.92,357.42) -- (405.5,366.51) -- (392.92,375.6) -- (392.92,368.97) -- (326.5,368.97) -- cycle ;
		%Right Arrow [id:dp6494335378272231] 
		\draw   (365.44,426.28) -- (399.12,426.28) -- (399.12,418.86) -- (405.5,429.03) -- (399.12,439.2) -- (399.12,431.77) -- (365.44,431.77) -- cycle ;
		%Right Arrow [id:dp28823914891633007] 
		\draw   (258,476.26) -- (382.02,476.26) -- (382.02,469.4) -- (405.5,478.8) -- (382.02,488.2) -- (382.02,481.34) -- (258,481.34) -- cycle ;
		
		% Text Node
		\draw (84.4,175) node [anchor=north west][inner sep=0.75pt]  [rotate=-270]  {$\%$};
		% Text Node
		\draw (259,274) node [anchor=north west][inner sep=0.75pt]   [align=left] {Price};
		% Text Node
		\draw (116,222.2) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (116,201.2) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$5$};
		% Text Node
		\draw (113,178.2) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$10$};
		% Text Node
		\draw (113,154.2) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$15$};
		% Text Node
		\draw (113,131.2) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$20$};
		% Text Node
		\draw (113,104.2) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$25$};
		% Text Node
		\draw (124,250) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$29.5$};
		% Text Node
		\draw (155.4,250) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$30$};
		% Text Node
		\draw (174.8,250) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$30.5$};
		% Text Node
		\draw (204.2,250) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$31$};
		% Text Node
		\draw (224.6,250) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$31.5$};
		% Text Node
		\draw (252,250) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$32$};
		% Text Node
		\draw (271.4,250) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$32.5$};
		% Text Node
		\draw (299.8,250) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$33$};
		% Text Node
		\draw (319.2,250) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$33.5$};
		% Text Node
		\draw (348.6,250) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$34$};
		% Text Node
		\draw (368,250) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$34.5$};
		% Text Node
		\draw (193.4,324) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$30.8$};
		% Text Node
		\draw (262.4,324) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$32.4$};
		% Text Node
		\draw (155.4,385) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$30.0$};
		% Text Node
		\draw (303.4,385) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$33.2$};
		% Text Node
		\draw (118.4,444) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$29.2$};
		% Text Node
		\draw (341.4,444) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$34.0$};
		% Text Node
		\draw (200.4,496) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$31.32$};
		% Text Node
		\draw (243.4,496) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$31.78$};
		% Text Node
		\draw (412,285.4) node [anchor=north west][inner sep=0.75pt]   [align=left] {Estimated tolerance\\interval for the prices at\\$\displaystyle 68.26\%$ of all DVDs};
		% Text Node
		\draw (412,345.4) node [anchor=north west][inner sep=0.75pt]   [align=left] {Estimated tolerance\\interval for the prices at\\$\displaystyle 95.44\%$ of all DVDs};
		% Text Node
		\draw (412,404.4) node [anchor=north west][inner sep=0.75pt]   [align=left] {Estimated tolerance\\interval for the prices at\\$\displaystyle 99.73\%$ of all DVDs};
		% Text Node
		\draw (412,460.4) node [anchor=north west][inner sep=0.75pt]   [align=left] {Estimated confidence\\interval (at $\displaystyle 95\%$ ) for the\\population of DVDs};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[]{Histogram of the prices of a sample of $49$ DVDs}
	\end{figure}
	Now lets us introduce a new concept that is rarely addressed in the statistics literature. The idea of the prediction interval is rather than look at the confidence interval of the mean based on an experimental average, to use this experimental average (sample mean) as a basis for predicting the interval of a single value (and not of the average!).

	We'll look at the difference between the mean and a punctual value:
	
	that we will assume close to zero (it is better to have a reliable product and pass the tests to obtain the authorization of sales...). About the variance, what interests us is not just the standard deviation of the mean anymore, but the standard deviation of the difference... and as the sample is independent of the unique value we have:
	
	So we can write as a first approximation:
	
	And of course after what we saw:
	
	So we can build verbatim the prediction interval:
	
	
	\subsubsection{Bayesian Inference}
	To understand the idea of "\NewTerm{Bayesian inference}\index{Bayesian inference}" (to not be confuse with "Bayesian Hypothesis Testing") remember that in the non-Bayesian (frequentist) approach we measure for example a parameter $\theta$. Afterwards, we calculate the standard error or a confidence interval on which we perform a hypothesis test.

	In the Bayesian approach we assume that the parameter $\theta$ follows a non-trivial prior distribution.  We then calculate its posterior probability distribution knowing the measurements made and we calculate afterwards a confidence interval on that latter posterior.
	
	According to some specialists there is now a plenty of theorem and masses of worked-out numerical examples that shows evidence of the superiority of the Bayesian method in a hundred of different areas! The traditional frequentist methods which use only sampling distributions are usable and useful in many particularly simple, idealized problems; however, they represent the most proscribed special cases of probability theory, because they presuppose conditions (independent repetitions of a random experiment but no relevant information!) that are hardly ever met in real problems.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The frequentist approach is the case of what we also name "\NewTerm{uninformative prior}". But to reader must not think that a uniform law prior is an example of uninformative prior (there is a detailed mathematical example further below just after our study of beta-binomial posterior)!!! Indeed, the pure frequentists doesn't make ANY prior (i.e. the probability is completely indeterminate $0/0$)! The pure frequentist always assumes that the sample data is exactly representative of the population because that's all it can do.
	\end{tcolorbox}

	Let us assume that we have an univariate bayesian model with a likelihood $P(y|\vec{\theta})$ (also sometimes denoted $f(y|\vec{\theta})$) and an univariate prior distribution $P(\vec{\theta})$ (also sometimes denoted $f(\vec{\theta})$).

	Let us recall that (according to what we have seen during our study of Bayesian probabilities page \pageref{compound probabilities}) if we multiply the univariate likelihood $P(y|\vec{\theta})$ and the univariate  prior probability $P(\vec{\theta})$ we get the univariate posterior distribution $P(\vec{\theta}|y)$ (also denoted $\pi(\vec{\theta}|y)$). Indeed, using what we have seen earlier above we get (the denominator is for recall the "marginal probability" or also sometimes named the "evidence"):
	
	where $P(y|\vec{\theta})P(\vec{\theta})$ is commonly named the "\NewTerm{unnormalized posterior density}\index{unnormalized posterior density}".
	
	Also often denoted (still in the univariate case and using the definition of marginal distribution):
	
	Where the ratio:
	
	is sometimes named the "\NewTerm{standardized likelihood}\index{standardized likelihood}".
	
	With the exception of the marginal probability, posterior, prior and likelihood are not a point estimate. They are curves as illustrated below:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1467); %set diagram left start at 0, and has height of 1467
		
		%Curve Lines [id:da24724121015178424] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=0.1 ][line width=1.5]    (492.5,284) .. controls (502.07,283.79) and (510.2,283.87) .. (515.17,278.67) .. controls (520.13,273.47) and (526.67,244.07) .. (528.5,234.67) .. controls (530.33,225.26) and (538.5,168) .. (543.83,168) .. controls (549.17,168) and (558.5,243) .. (565.83,264) .. controls (573.17,285) and (578.01,283.16) .. (582.5,283.33) .. controls (586.99,283.5) and (622.33,283.67) .. (640,284) ;
		%Curve Lines [id:da931776023890732] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=0.1 ][line width=1.5]    (155.5,382) .. controls (165.07,381.79) and (173.2,381.87) .. (178.17,376.67) .. controls (183.13,371.47) and (189.67,342.07) .. (191.5,332.67) .. controls (193.33,323.26) and (201.5,266) .. (206.83,266) .. controls (212.17,266) and (221.5,341) .. (228.83,362) .. controls (236.17,383) and (241.01,381.16) .. (245.5,381.33) .. controls (249.99,381.5) and (285.33,381.67) .. (303,382) ;
		%Curve Lines [id:da5252872073006505] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=0.1 ][line width=1.5]    (155.5,382) .. controls (169.67,382) and (191.92,383.52) .. (208.33,372.67) .. controls (224.74,361.81) and (243,322) .. (258.67,323) .. controls (274.33,324) and (275.67,334.33) .. (285.33,355.67) .. controls (295,377) and (298.51,381.83) .. (303,382) ;
		%Straight Lines [id:da7404825287211709] 
		\draw [line width=3.75]    (60,216) -- (387.5,216) ;
		%Curve Lines [id:da17613562673973204] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (262.5,183) .. controls (272.07,182.79) and (280.2,182.87) .. (285.17,177.67) .. controls (290.13,172.47) and (296.67,143.07) .. (298.5,133.67) .. controls (300.33,124.26) and (308.5,67) .. (313.83,67) .. controls (319.17,67) and (328.5,142) .. (335.83,163) .. controls (343.17,184) and (348.01,182.16) .. (352.5,182.33) .. controls (356.99,182.5) and (392.33,182.67) .. (410,183) ;
		%Curve Lines [id:da2738415164463477] 
		\draw [color={rgb, 255:red, 189; green, 16; blue, 224 }  ,draw opacity=1 ][fill={rgb, 255:red, 189; green, 16; blue, 224 }  ,fill opacity=0.5 ][line width=1.5]    (155.5,382) .. controls (165.07,381.79) and (184.5,383) .. (191.17,379) .. controls (197.83,375) and (203.17,353.67) .. (205.17,347) .. controls (207.17,340.33) and (213.5,322) .. (216.5,322) .. controls (219.5,322) and (233.83,366.33) .. (237.5,372.33) .. controls (241.17,378.33) and (243.17,381.33) .. (245.17,381.33) .. controls (247.17,381.33) and (284.67,382) .. (303,382) ;
		%Curve Lines [id:da9591316119794553] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (45,183) .. controls (59.17,183) and (81.42,184.52) .. (97.83,173.67) .. controls (114.24,162.81) and (132.5,123) .. (148.17,124) .. controls (163.83,125) and (165.17,135.33) .. (174.83,156.67) .. controls (184.5,178) and (188.01,182.83) .. (192.5,183) ;
		%Curve Lines [id:da3413637252760118] 
		\draw [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ][line width=1.5]    (492.5,284) .. controls (502.07,283.79) and (520.87,285.87) .. (525.83,280.67) .. controls (530.8,275.47) and (537.67,228.4) .. (539.5,219) .. controls (541.33,209.6) and (547.17,154.67) .. (552.5,154.67) .. controls (557.83,154.67) and (565.83,231.33) .. (570.17,255) .. controls (574.5,278.67) and (583.01,283.5) .. (587.5,283.67) .. controls (591.99,283.84) and (622.33,283.67) .. (640,284) ;
		%Shape: Rectangle [id:dp6125575880261553] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] (262.5,49) -- (410,49) -- (410,183) -- (262.5,183) -- cycle ;
		%Shape: Rectangle [id:dp1373779859846025] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] (45,49) -- (192.5,49) -- (192.5,183) -- (45,183) -- cycle ;
		%Shape: Rectangle [id:dp3514831702157626] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] (155.5,248) -- (303,248) -- (303,382) -- (155.5,382) -- cycle ;
		%Shape: Rectangle [id:dp2018306714217344] 
		\draw  [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ] (492.5,150) -- (640,150) -- (640,284) -- (492.5,284) -- cycle ;
		%Curve Lines [id:da7232223566490841] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=0.1 ][line width=1.5]    (492.5,284) .. controls (506.67,284) and (528.92,285.52) .. (545.33,274.67) .. controls (561.74,263.81) and (580,224) .. (595.67,225) .. controls (611.33,226) and (612.67,236.33) .. (622.33,257.67) .. controls (632,279) and (635.51,283.83) .. (640,284) ;
		%Curve Lines [id:da9124918739529908] 
		\draw [color={rgb, 255:red, 189; green, 16; blue, 224 }  ,draw opacity=0.1 ][line width=1.5]    (492.83,284) .. controls (502.4,283.79) and (521.83,285) .. (528.5,281) .. controls (535.17,277) and (540.5,255.67) .. (542.5,249) .. controls (544.5,242.33) and (550.83,224) .. (553.83,224) .. controls (556.83,224) and (571.17,268.33) .. (574.83,274.33) .. controls (578.5,280.33) and (580.5,283.33) .. (582.5,283.33) .. controls (584.5,283.33) and (622,284) .. (640.33,284) ;
		
		% Text Node
		\draw (98,29) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{0.29,0.56,0.89}{Prior}}};
		% Text Node
		\draw (22.5,163.5) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {{\footnotesize probability density}};
		% Text Node
		\draw (70.5,186) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\footnotesize model parameter}};
		% Text Node
		\draw (298.5,30) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{0.82,0.01,0.11}{Likelihood}}};
		% Text Node
		\draw (240,163.5) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {{\footnotesize probability density}};
		% Text Node
		\draw (288,186) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\footnotesize model parameter}};
		% Text Node
		\draw (198.5,229) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{0.74,0.06,0.88}{Evidence}}};
		% Text Node
		\draw (133,362.5) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {{\footnotesize probability density}};
		% Text Node
		\draw (181,385) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\footnotesize model parameter}};
		% Text Node
		\draw (532.5,130) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{Posterior}};
		% Text Node
		\draw (470,264.5) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {{\footnotesize probability density}};
		% Text Node
		\draw (518,287) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\footnotesize model parameter}};
		% Text Node
		\draw (420,210) node [anchor=north west][inner sep=0.75pt]  [font=\Huge]  {$=$};
		% Text Node
		\draw (205,100) node [anchor=north west][inner sep=0.75pt]  [font=\Huge]  {$\times $};
		% Text Node
		\draw (137,105) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize,color={rgb, 255:red, 74; green, 144; blue, 226 }  ,opacity=1 ]  {$\pi ( \theta )$};
		% Text Node
		\draw (275,71.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize,color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ]  {$\mathcal{L}( \theta )$};
		% Text Node
		\draw (563,158.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize,color={rgb, 255:red, 0; green, 0; blue, 0 }  ,opacity=1 ]  {$\mathcal{P}( \theta )$};
		% Text Node
		\draw (180,294.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize,color={rgb, 255:red, 189; green, 16; blue, 224 }  ,opacity=1 ]  {$\int \pi ( \theta )\mathcal{L}( \theta )\mathrm{d} \theta $};
		
		\end{tikzpicture}
	\end{figure}
	Or, in the example below, we show the probability density function (PDF) on different car locations:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.3]{img/arithmetics/bayesian_inference_illustrated.jpg}
	\end{figure}
	We strongly recommend the reader to see the examples below for a better understanding of the denominator.
	
	\textbf{Definition (\#\thesection.\mydef):} A prior $P(\vec{\theta})$ for a sampling model is named a "\NewTerm{conjugate prior}\index{conjugate prior}\label{conjugate prior}" if the resulting posterior denoted traditionally $\pi(\vec{\theta}|\vec{y})$ (for multivariate case) is in the same distributional family as the prior.
	
	In other words: if our posterior distribution is of the same family as the prior distribution, we speak of "\NewTerm{conjugation of distributions}". Then we also say that the prior function is the "\NewTerm{conjugation of the likelihood}" (in practice it is however the exception rather than the rule ...).
	
	Here is for example a table summarizing the most classic cases:
	\begin{table}[H]
		\begin{tabular}{|l|l|l|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|c|}{\cellcolor[gray]{0.75}\textbf{Likelihood}} & \multicolumn{1}{c|}{\cellcolor[gray]{0.75}\textbf{Prior}} & \multicolumn{1}{c|}{\cellcolor[gray]{0.75}\textbf{Posterior}} \\ \hline
		Binomial & Beta & (Beta-)Binomial distribution \\ \hline
		Negative Binomial & Beta & (Beta-)Negative Binomial distribution \\ \hline
		Poisson & Gamma & (Poisson-)Gamma distribution \\ \hline
		Geometric & Beta & (Beta-)Geometric distribution \\ \hline
		Normal (unknown mean) & Normal & Normal distribution \\ \hline
		Normal (unknown variance) & Inverse Gamma & (Normal-)Gamma inverse distribution \\ \hline
		Normal (all moments known) & Normal/Gamma & Normal/Gamma distribution \\ \hline
		Multinomial & Dirichlet & (Multinomial-)Dirichlet distribution \\ \hline
		\end{tabular}
	\end{table}
	However (and as always!) in this book we will detail only the conjugations that have practical applications in other chapters! 
	
	A last reminder before continuing. If we know for a given likelihood distribution the expression $P(y_i/\vec{\theta})$, then for a vector of independent realizations of a random variable, the total likelihood is equal to the product of the probabilities:
	
	Be careful with Bayesian inference! Don't trust absolutely intuition, for even simple prior plus likelihood scenarios defy it! The quartet below shows that the choice of prior and likelihood distribution matters a lot. More specifically the "tailed-ness" of the distribution matters more!!

	The author of this quartet depicts four cases:
	\begin{itemize}
		\item Normal prior, Normal likelihood (all is fine here)
	
		\item $T$-distribution prior, $T$-distribution likelihood (things start to go awry, we now have a bimodal posterior)
	
		\item $T$-distribution prior, Normal likelihood (the posterior mimics the likelihood and in a way, prior had absolutely no sway)
	
		\item Normal prior, $T$-distribution likelihood (the posterior mimics the prior and in a way likelihood had absolutely no sway)
	\end{itemize}
	Therefore the Bayesian inference requires a lot of hardwork and knowledge to get it right.
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,812); %set diagram left start at 0, and has height of 812
		
		%Shape: Axis 2D [id:dp45148665831523416] 
		\draw  (164,226.28) -- (324.11,226.28)(171.56,80) -- (171.56,234.28) (317.11,221.28) -- (324.11,226.28) -- (317.11,231.28) (166.56,87) -- (171.56,80) -- (176.56,87) (197.56,221.28) -- (197.56,231.28)(223.56,221.28) -- (223.56,231.28)(249.56,221.28) -- (249.56,231.28)(275.56,221.28) -- (275.56,231.28)(301.56,221.28) -- (301.56,231.28)(166.56,200.28) -- (176.56,200.28)(166.56,174.28) -- (176.56,174.28)(166.56,148.28) -- (176.56,148.28)(166.56,122.28) -- (176.56,122.28)(166.56,96.28) -- (176.56,96.28) ;
		\draw   ;
		%Curve Lines [id:da2554496581223711] 
		\draw [line width=1.5]    (171.56,226.28) .. controls (179.38,226.19) and (205.31,225.79) .. (210.11,222.19) .. controls (214.91,218.59) and (220.51,121.39) .. (223.31,121.39) .. controls (226.11,121.39) and (230.91,217.79) .. (236.51,222.19) .. controls (242.11,226.59) and (309.13,226.09) .. (321.31,226.19) ;
		%Curve Lines [id:da9913903412731817] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (171.56,226.28) .. controls (179.38,226.19) and (234.11,227.39) .. (240.11,222.59) .. controls (246.11,217.79) and (246.91,87.79) .. (249.71,87.79) .. controls (252.51,87.79) and (257.31,217.79) .. (262.91,222.19) .. controls (268.51,226.59) and (309.13,226.09) .. (321.31,226.19) ;
		%Curve Lines [id:da10376922147032386] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (171.56,226.28) .. controls (179.38,226.19) and (258.11,225.79) .. (262.91,222.19) .. controls (267.71,218.59) and (273.31,121.39) .. (276.11,121.39) .. controls (278.91,121.39) and (283.71,217.79) .. (289.31,222.19) .. controls (294.91,226.59) and (311.59,226.2) .. (323.77,226.3) ;
		%Shape: Axis 2D [id:dp9280988893359083] 
		\draw  (365,226.28) -- (525.11,226.28)(372.56,80) -- (372.56,234.28) (518.11,221.28) -- (525.11,226.28) -- (518.11,231.28) (367.56,87) -- (372.56,80) -- (377.56,87) (398.56,221.28) -- (398.56,231.28)(424.56,221.28) -- (424.56,231.28)(450.56,221.28) -- (450.56,231.28)(476.56,221.28) -- (476.56,231.28)(502.56,221.28) -- (502.56,231.28)(367.56,200.28) -- (377.56,200.28)(367.56,174.28) -- (377.56,174.28)(367.56,148.28) -- (377.56,148.28)(367.56,122.28) -- (377.56,122.28)(367.56,96.28) -- (377.56,96.28) ;
		\draw   ;
		%Curve Lines [id:da1705420194233449] 
		\draw [line width=1.5]    (372.56,226.28) .. controls (380.38,226.19) and (395.96,222.46) .. (402.96,214.96) .. controls (409.96,207.46) and (419.31,136.89) .. (424.46,137.1) .. controls (429.61,137.31) and (437.46,209.96) .. (446.46,217.46) .. controls (455.46,224.96) and (510.13,226.09) .. (522.31,226.19) ;
		%Curve Lines [id:da8235093452652149] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (372.56,226.28) .. controls (380.38,226.19) and (400.46,226.81) .. (406.46,222.01) .. controls (412.46,217.21) and (418.96,196.51) .. (425.96,196.51) .. controls (432.96,196.51) and (434.96,214.28) .. (451.96,215.51) .. controls (468.96,216.73) and (471.96,186.23) .. (478.46,186.23) .. controls (484.96,186.23) and (492.96,218.23) .. (500.46,221.23) .. controls (507.96,224.23) and (519.84,226.17) .. (522.31,226.19) ;
		%Curve Lines [id:da3731871986085784] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (372.56,226.28) .. controls (380.38,226.19) and (449.96,227.01) .. (456.96,219.51) .. controls (463.96,212.01) and (471.46,138.19) .. (477.46,138.1) .. controls (483.46,138.01) and (492.46,210.51) .. (501.46,218.01) .. controls (510.46,225.51) and (508.96,224.51) .. (522.31,226.19) ;
		%Shape: Axis 2D [id:dp8259365050064147] 
		\draw  (164,423.28) -- (324.11,423.28)(171.56,277) -- (171.56,431.28) (317.11,418.28) -- (324.11,423.28) -- (317.11,428.28) (166.56,284) -- (171.56,277) -- (176.56,284) (197.56,418.28) -- (197.56,428.28)(223.56,418.28) -- (223.56,428.28)(249.56,418.28) -- (249.56,428.28)(275.56,418.28) -- (275.56,428.28)(301.56,418.28) -- (301.56,428.28)(166.56,397.28) -- (176.56,397.28)(166.56,371.28) -- (176.56,371.28)(166.56,345.28) -- (176.56,345.28)(166.56,319.28) -- (176.56,319.28)(166.56,293.28) -- (176.56,293.28) ;
		\draw   ;
		%Curve Lines [id:da6352093331012711] 
		\draw [line width=1.5]    (171.56,423.28) .. controls (179.38,423.19) and (205.31,422.79) .. (210.11,419.19) .. controls (214.91,415.59) and (220.51,318.39) .. (223.31,318.39) .. controls (226.11,318.39) and (230.91,414.79) .. (236.51,419.19) .. controls (242.11,423.59) and (309.13,423.09) .. (321.31,423.19) ;
		%Curve Lines [id:da9731885831849691] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (171.56,423.28) .. controls (179.38,423.19) and (209.14,422.84) .. (213.6,419.28) .. controls (218.05,415.71) and (221.11,338.92) .. (222.85,338.35) .. controls (224.6,337.78) and (223.6,323.28) .. (225.6,323.28) .. controls (227.6,323.28) and (228.6,345.28) .. (230.1,346.28) .. controls (231.6,347.28) and (232.61,416.12) .. (236.51,419.19) .. controls (240.42,422.25) and (240.1,417.28) .. (242.6,417.78) .. controls (245.1,418.28) and (246.6,421.28) .. (250.6,421.78) .. controls (254.6,422.28) and (315.94,423.14) .. (321.31,423.19) ;
		%Curve Lines [id:da13877608073833825] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (171.56,423.28) .. controls (177.7,423.21) and (221.92,423.7) .. (244.02,420.28) .. controls (250.06,419.34) and (254.46,418.12) .. (255.96,416.51) .. controls (262.96,409.01) and (270.46,335.19) .. (276.46,335.1) .. controls (282.46,335.01) and (291.46,407.51) .. (300.46,415.01) .. controls (309.46,422.51) and (307.96,421.51) .. (321.31,423.19) ;
		%Shape: Axis 2D [id:dp04953501712201924] 
		\draw  (366,423.28) -- (526.11,423.28)(373.56,277) -- (373.56,431.28) (519.11,418.28) -- (526.11,423.28) -- (519.11,428.28) (368.56,284) -- (373.56,277) -- (378.56,284) (399.56,418.28) -- (399.56,428.28)(425.56,418.28) -- (425.56,428.28)(451.56,418.28) -- (451.56,428.28)(477.56,418.28) -- (477.56,428.28)(503.56,418.28) -- (503.56,428.28)(368.56,397.28) -- (378.56,397.28)(368.56,371.28) -- (378.56,371.28)(368.56,345.28) -- (378.56,345.28)(368.56,319.28) -- (378.56,319.28)(368.56,293.28) -- (378.56,293.28) ;
		\draw   ;
		%Curve Lines [id:da1263850749765585] 
		\draw [line width=1.5]    (373.56,423.28) .. controls (381.38,423.19) and (396.96,419.46) .. (403.96,411.96) .. controls (410.96,404.46) and (420.31,333.89) .. (425.46,334.1) .. controls (430.61,334.31) and (438.46,406.96) .. (447.46,414.46) .. controls (456.46,421.96) and (511.13,423.09) .. (523.31,423.19) ;
		%Curve Lines [id:da9281894111983364] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (371.1,423.16) .. controls (378.92,423.08) and (457.65,422.67) .. (462.45,419.07) .. controls (467.25,415.47) and (472.85,318.27) .. (475.65,318.27) .. controls (478.45,318.27) and (483.25,414.67) .. (488.85,419.07) .. controls (494.45,423.47) and (511.13,423.09) .. (523.31,423.19) ;
		%Curve Lines [id:da9459991908593672] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (371.1,423.16) .. controls (378.92,423.08) and (453.41,422.4) .. (459.41,417.6) .. controls (465.41,412.8) and (467.41,349.1) .. (468.91,346.1) .. controls (470.41,343.1) and (471.77,324.1) .. (472.41,324.1) .. controls (473.06,324.1) and (472.55,329.64) .. (475.91,333.6) .. controls (479.27,337.56) and (480.31,415.2) .. (485.91,419.6) .. controls (491.51,424) and (508.66,422.97) .. (520.85,423.07) ;
		
		% Text Node
		\draw (218,28) node [anchor=north west][inner sep=0.75pt]  [font=\large] [align=left] {\textbf{Richard McElreath's Quartet}};
		% Text Node
		\draw (129.5,175.5) node [anchor=north west][inner sep=0.75pt]  [font=\small,rotate=-270] [align=left] {Density};
		% Text Node
		\draw (162,229) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle 0$};
		% Text Node
		\draw (151.5,182.5) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize,rotate=-270] [align=left] {$\displaystyle 0.2$};
		% Text Node
		\draw (151.5,130.5) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize,rotate=-270] [align=left] {$\displaystyle 0.4$};
		% Text Node
		\draw (188,235) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle -5$};
		% Text Node
		\draw (221,235) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle 0$};
		% Text Node
		\draw (246,235) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle 5$};
		% Text Node
		\draw (270,235) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle 10$};
		% Text Node
		\draw (296,235) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle 15$};
		% Text Node
		\draw (187.02,86.02) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {\begin{minipage}[lt]{29.92pt}\setlength\topsep{0pt}
		\begin{center}
		{\footnotesize Normal}\\{\footnotesize likelihood}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (228.82,70.62) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize,color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ] [align=left] {{\footnotesize posterior}};
		% Text Node
		\draw (259.77,86.02) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize,color={rgb, 255:red, 74; green, 144; blue, 226 }  ,opacity=1 ] [align=left] {\begin{minipage}[lt]{23.74pt}\setlength\topsep{0pt}
		\begin{center}
		{\footnotesize Normal}\\{\footnotesize prior}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (330.5,175.5) node [anchor=north west][inner sep=0.75pt]  [font=\small,rotate=-270] [align=left] {Density};
		% Text Node
		\draw (363,229) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle 0$};
		% Text Node
		\draw (352.5,182.5) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize,rotate=-270] [align=left] {$\displaystyle 0.2$};
		% Text Node
		\draw (352.5,130.5) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize,rotate=-270] [align=left] {$\displaystyle 0.4$};
		% Text Node
		\draw (389,235) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle -5$};
		% Text Node
		\draw (422,235) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle 0$};
		% Text Node
		\draw (447,235) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle 5$};
		% Text Node
		\draw (471,235) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle 10$};
		% Text Node
		\draw (497,235) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle 15$};
		% Text Node
		\draw (402.52,99.52) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {\begin{minipage}[lt]{29.92pt}\setlength\topsep{0pt}
		\begin{center}
		{\footnotesize Student}\\{\footnotesize likelihood}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (497.82,195.62) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize,color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ] [align=left] {{\footnotesize posterior}};
		% Text Node
		\draw (460.77,99.52) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize,color={rgb, 255:red, 74; green, 144; blue, 226 }  ,opacity=1 ] [align=left] {\begin{minipage}[lt]{25.21pt}\setlength\topsep{0pt}
		\begin{center}
		{\footnotesize Student}\\{\footnotesize prior}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (129.5,372.5) node [anchor=north west][inner sep=0.75pt]  [font=\small,rotate=-270] [align=left] {Density};
		% Text Node
		\draw (162,426) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle 0$};
		% Text Node
		\draw (151.5,379.5) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize,rotate=-270] [align=left] {$\displaystyle 0.2$};
		% Text Node
		\draw (151.5,327.5) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize,rotate=-270] [align=left] {$\displaystyle 0.4$};
		% Text Node
		\draw (188,432) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle -5$};
		% Text Node
		\draw (221,432) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle 0$};
		% Text Node
		\draw (246,432) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle 5$};
		% Text Node
		\draw (270,432) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle 10$};
		% Text Node
		\draw (296,432) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle 15$};
		% Text Node
		\draw (200.02,280.02) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {\begin{minipage}[lt]{29.92pt}\setlength\topsep{0pt}
		\begin{center}
		{\footnotesize Normal}\\{\footnotesize likelihood}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (260.77,298.02) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize,color={rgb, 255:red, 74; green, 144; blue, 226 }  ,opacity=1 ] [align=left] {\begin{minipage}[lt]{25.21pt}\setlength\topsep{0pt}
		\begin{center}
		{\footnotesize Student}\\{\footnotesize prior}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (229.82,333.12) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize,color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ] [align=left] {{\footnotesize posterior}};
		% Text Node
		\draw (331.5,372.5) node [anchor=north west][inner sep=0.75pt]  [font=\small,rotate=-270] [align=left] {Density};
		% Text Node
		\draw (364,426) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle 0$};
		% Text Node
		\draw (353.5,379.5) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize,rotate=-270] [align=left] {$\displaystyle 0.2$};
		% Text Node
		\draw (353.5,327.5) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize,rotate=-270] [align=left] {$\displaystyle 0.4$};
		% Text Node
		\draw (390,432) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle -5$};
		% Text Node
		\draw (423,432) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle 0$};
		% Text Node
		\draw (448,432) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle 5$};
		% Text Node
		\draw (472,432) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle 10$};
		% Text Node
		\draw (498,432) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {$\displaystyle 15$};
		% Text Node
		\draw (484.32,348.62) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize,color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ] [align=left] {{\footnotesize posterior}};
		% Text Node
		\draw (403.52,296.52) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {\begin{minipage}[lt]{29.92pt}\setlength\topsep{0pt}
		\begin{center}
		{\footnotesize Student}\\{\footnotesize likelihood}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (457.77,280.02) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize,color={rgb, 255:red, 74; green, 144; blue, 226 }  ,opacity=1 ] [align=left] {\begin{minipage}[lt]{23.74pt}\setlength\topsep{0pt}
		\begin{center}
		{\footnotesize Normal}\\{\footnotesize prior}
		\end{center}
		
		\end{minipage}};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Richard McElreath's Quartet}
	\end{figure}

	\paragraph{Beta prior conjugate to Binomial likelihood (beta-binomial distribution)}\mbox{}\\\\
	Let us now introduce the beta-binomial distribution that is used a lot in Engineering but also in Bayesian statistics (especially known for the famous A/B test for example!). 
	
	We decide to start with this distribution have a binomial likelihood because it is a law with only one parameter and pedagogically it seems more judicious to start with that latter rather that with a Normal likelihood which has two parameters (furthermore there are three possible conjugates for the Normal likelihood as shows in the above table).

	\textbf{Definition (\#\thesection.\mydef):} The "\NewTerm{beta-binomial distribution}\index{beta-binomial distribution}\label{beta-binomial distribution}" is the binomial distribution in which the probability $p$ of success at each trial is fixed but randomly drawn from a beta distribution prior to $n$ Bernoulli trials. 
	
	Let us now prove that the Beta distribution is a conjugate distribution of the binomial distribution, but before a definition!
	
	Suppose we observe $n$ independent Bernoulli random variable $x_1,\ldots,x_n$. We wish to estimate the success probability $p$ with the Bayesian approach. We will use a beta $P_{a,b}$ priori for $p$ (as $P_{a,b}\in ]0,1[$ it may be a good choice...) and prove this is a conjugate prior.
	
	First using the relationship between the Euler Gamma function and the factorial (\SeeChapter{see section Differential and Integral Calculus page \pageref{gamma euler function}}) we have using the conventional notation (since the probability of the prior is assumed independent of the trial itself we then multiply both probabilities):
	
	Notice that we also have the following equivalent result (just the notation differs!):
	
	Although it is not really necessary, let us derive the marginal density of $y$ (in purpose to normalize the posterior density further below!):
	
	That latter marginal density is also sometimes written:
	
	So a common (wrong) definition of the beta-binomial distribution is given by these two equivalent relations of the marginal density:
	
	
	Now let us focus on the posterior $\pi(\vec{\theta},y)$ (i.e. the "real" expression of the beta-binomial distribution), then given by:
	
	That latter equality is sometimes denoted:
	
	Clearly this posterior is a beta distribution $P_{a,b}(y+a,n-y+b)$. So we see that only the parameters of the beta prior change into now "hyper-parameters" of a beta posterior! In other words, this means that if the likelihood function is binomial, then a beta prior gives a beta posterior!
	
	Notice that we then quickly get by extension (when drawing $N$ items among the $n$):
	
	That latter relation is sometimes denoted $\text{Bbin}(a,b,n)$.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The worst is that some authors in the field of reliability engineering denote the cumulated probability that $y<n-F$ where $F$ is the number of failures as following (\SeeChapter{see section Industrial Engineering page \pageref{Beta-binomial sampling size}}):
	
	Then it does not become obvious at all out of context to know if we are dealing with a mass density or distribution function.
	\end{tcolorbox}
	An interesting case to consider is when we have no particular premonition about prior distribution. Therefore, we can take a function that has the same probability for all values in the $ [0,1] $ interval of the $ p $ parameter of the binomial distribution.

	Therefore, the beta function can always be kept as a distribution a priori since it is reduced to the uniform case:
	
	which is therefore a continuous uniform distribution in the interval $ [0,1] $. Hence the posterior function of the parameter $p$ is given by:
	
	Let us calculate now the expected mean and variance of the beta binomial distribution using\footnote{There are other (complicated) ways to derive expectation and variance, but if you remember these relations, then you have as far as we know the simplest derivation!} the law of total expectation (see page \pageref{iterated conditional mean}) and the law of total variance (page \pageref{iterated condititional variance}):
	
	Expectation and variance for the binomial distribution are assumed to be so well known that we will not mention them here (see page \pageref{binomial distribution}), and for the beta distribution we have (see page  \pageref{beta distribution}):
	
	Using the law of total expectation , we have:
	
	and using the law of total variance, we have (in the second line we used the Huygens relation):
	
	So to summarize we have for posterior expectation and posterior variance:
	
	If we reintroduce $p$, this can we be written:
	
	We notice that the variance is higher than for the corresponding binomial distribution. We say then that there is "over-dispersion". 
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In case of $a=b=0.5$ for the beta distribution, it is quite common to speak about the "Jeffreys prior".
	\end{tcolorbox}
	
	\subparagraph{Uniform vs Uninformative prior}\mbox{}\\\\
	When prior distributions have no population basis, they can be difficult to construct, and there has long been a desire for prior distributions that can be guaranteed to play a minimal role in the posterior distribution. Such distributions are sometimes named "\NewTerm{reference prior distributions}\index{reference prior distributions}", and the prior density is described as "\NewTerm{uninformative prior}\index{uninformatived prior}" or "\NewTerm{noninformative prior}\index{noninformative prior}". The rationale for using noninformative prior distributions is often said \textit{to be to let the data speak for themselves}, so that inferences are unaffected by information external to the current data.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1.2]{img/arithmetics/uninformative_prior_joke.jpg}
		\caption[]{Depending on the choice of the uninformative prior and likelihood sometimes things can go very wrong}
	\end{figure}
	Some people would expect a uniform prior to be a good example of an uninformative prior and get the same result as the frequentist approach. However, this is not the case\footnote{For reasons unknown to us yet, even String theory physicists seems to use the uniform distribution as "natural choice" (naturalness criteria) for uninformative prior. A first attempt to justify the uniform probability distribution for the naturalness criterion might be to say that it doesn't introduce additional parameters into the String theory. But of course it does: it introduces the number $1$ as a typical width! Some physicists, however argue without evidence yet, that $1$ satisfy naturalness criterion...}!

	As an example, let's look the classical Bernoulli problem - you flip coins and $s$ are heads, $f$ are tails.

	We want to predict $p$, the probability of heads on any given coin flip.
	
	The frequentist approach is to say that $p$ is simply $s/(s+f)$. In this approach, we assume nothing about the distribution of $p$ and we're simply finding the maximum likelihood estimate. This makes sense!
	
	Now, let's adopt the Bayesian approach and assume we say:
	
	As we have proved it earlier above at page \pageref{beta-binomial distribution} if we make our prior the beta distribution, we get:
	
	Now let's say we want to find the expected value of $p$:
	
	We fall back obviously on the same expression of the expected mean as the beta distribution (as it is a conjugate prior!). Hence we can do the same simplification:
	
	Makes sense so far! Now, let's say we want to choose a really "objective" prior to emulate the frequentist approach, like a uniform distribution. This is equivalent to $B(1,1)$ as we have proved it during our study of the Beta distribution. In this case, we would get:
	
	This may hurts the brain of some people! Even though we're being as objective as we can ("$p$ is equally likely to be anything"), we're getting a different result than the frequentist approach.
	
	On the flip side, if we choose what we may consider as an improper prior like $B(0,0)$, we get the same result as the frequentist approach:
	
	What makes $B(0,0)$ more similar to the frequentist approach than $B(1,1)$? What assumption are we making in $B(1,1)$? How is $B(0,0)$ "less informed"? In fact you can't ask this question because it is simply inconsistent by definition!
	
	\paragraph{Normal prior conjugate to Normal likelihood}\mbox{}\\\\
	We will focus here only on the case of the unknown mean because in practice it is the most used one (as far as we know) and it is also according to our personal point of view the only  case that is intuitive to explain pedagogically speaking...

	Let us consider that we want to make inference on the mean. We have the presumption that the measured random variable $y$ probably follows a Normal distribution of mean and standard deviation $\mathcal{N}(y|\mu,\sigma)$ (i.e. the likelihood of $y$) and with the prior that $\mu$ follows a Normal distribution $\mathcal{N}(\mu_0,\sigma_0)$,  which can be considered as a good choice as long as the possible values of $\mu$ correspond to the definition domain of the Normal distribution.

	We have then (remember that in the prior distribution it is the parameter of interest which is the random variable!):
	
	We group and rearrange some terms:	
	
	 We now multiply the inside part of the square brackets by:
	
	in the purpose to eliminate the $\mu^2$:
	
	where $k$ does not depend on $\mu$. Let us continue...:
	
	Assuming that it exist a $k$ such that (...):
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In other words we have to check that when using this tool that:
	
	\end{tcolorbox}
	The posterior distribution thus looks indeed like the appearance of a Normal law:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	At the opposite of our study of the beta-binomial posterior distribution, we will not here divide by the marginal distribution that is anyway just a normalization constant and doesn't really interest us for now for the practical application we will have much later in this book.
	\end{tcolorbox}
	The mean and the variance of the posterior distribution are then, by analogy, equal to:
	
	Let us analyse a little more closely how the average prior mean $\mu_0$ and the posterior mean $\mu_p$ are related to each other:
	
	We can observe that when $n$ increases, the empirical mean $\bar{y}$ (second term) dominates the prior mean $\mu_0$. When the prior variance $\sigma_0^2$ decreases, the average a priori (first term) takes a predominant importance.
	
	Recall that in practice the specialist will, based on his return of experience (REX),  choose the prior $\{\mu_0,\sigma_0\}$ about $\mu_p$ (since that latter depends on them) and that the standard deviation $\sigma$ is assumed to be known (it is the set of all these parameters that make a lot of practitioners prefer the frequentist approach...).
	
	\pagebreak
	\paragraph{Credibility Intervals}\mbox{}\\\\
	In Bayesian inference, a "\NewTerm{credibility interval}\index{credibility interval}\label{credibility interval}" also named "\NewTerm{credible interval}" is an interval within which an unobserved parameter value falls with a particular probability. It is an interval in the domain of a posterior probability distribution or a predictive distribution. The generalisation to multivariate problems is the credible region. Credible intervals are analogous to confidence intervals in frequentist statistics, although they differ on a philosophical basis: Bayesian intervals treat their bounds as fixed and the estimated parameter as a random variable, whereas frequentist confidence intervals treat their bounds as random variables and the parameter as a fixed value. Also, Bayesian credible intervals use (and indeed, require) knowledge of the situation-specific prior distribution, while the frequentist confidence intervals do not.
	
	A Bayesian statistician would say that given his observed data, there is a $95\%$ probability that the true value of $\theta$ falls within the credible region while the frequentist statistician would say there is a $95\%$ cumulated probability that when he computes a confidence interval from data of this sort, the true value of $\theta$ will fall within it.
	
	Technically the bayesian "credibility interval" of size $1-\alpha$ is an interval $[x_1,x_2]$, where:
	
	Note that it is the parameter and not the interval limits, conditioned on the data $D$, that is random in this framework. This makes a credibility interval more intuitive because now we can interpret $1-\alpha$ as a (posterior) probability given an observed data set. In the frequentist setting, a given confidence interval is either right or wrong since $\theta$ is fixed. The confidence level only tells us what proportion of confidence intervals would be correct if we would repeat our calculations using new independent samples from the same underlying distribution, which many non-statisticians find confusing.
	
	Let us see a companion example well illustrated (by the way...) in our \texttt{R} companion book to get more intuition about different approach.
	
	Consider a simple problem with Bernoulli trials in the field of insurance claims. We want to derive some confidence interval for the probability to claim a loss. There were $n=1047$ policies and in $y=159$ claims.

	Consider the standard (frequentist) confidence interval of the proportion. What does that mean that:
	
	is the (asymptotic) $95\%$ confidence interval? The way we may see it is very simple. Let us generate some samples, of size $n$, with the same probability as the empirical one, i.e. $\hat{p}$ (which is the meaning of \textit{from data of this sort}). For each sample, we compute the confidence interval with the relationship above. It is a $95\%$ confidence interval because in $95\%$ of the scenarios, the empirical value lies in the confidence interval. 
	
	If we write a \texttt{R} code that computes 100 of such intervals we get typically:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/arithmetics/credibility_interval_r_example_100_ci.jpg}
	\end{figure}
	Now, what about the Bayesian credibility interval ? Assume that the prior distribution for the probability (denoted $\hat{p}$ in the frequentist framework) to claim a loss has a $\mathcal{B}(a,b)$ distribution. We have seen in the earlier above that, since the Beta distribution is the conjugate of the Bernoulli one, the posterior distribution will also be Beta. More precisely:
	
	Let us choose a uniform prior:
	
	and let us consider we do the experiment only once ($N=1$):
	
	Based on that property, the credibility interval at $95\%$ is based on quantiles of that (posterior) distribution such as:
		
	 and has the following shape if plotted with \texttt{R}:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/arithmetics/credibility_interval_r_example_ci_posterior.jpg}
	\end{figure}
	It is straightforward with a statistical software to get the lower bound and upper bound quantile of the credibility interval.
	
	We can now plot again $100$ simulations but not generated using all the same estimated proportion as before, but using some possible probabilities (in green, below, we can visualize the histogram of those randomly chosen values), based on that posterior distribution (given the observations):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/arithmetics/credibility_interval_r_example_100_ci_posterior.jpg}
	\end{figure}
	Here above there is $95\%$ of probability that those empirical proportions lie in the credibility interval, defined using quantiles of the posterior distribution. We can actually visualize all those proportions: in black the proportion used to generate the sample, and then, in blue or red, the averages obtained on those simulated samples.
	
	\paragraph{Bayes Factor (BF)}\mbox{}\\\\
	In statistics, the use of "\NewTerm{Bayes Factor}\index{Bayes Factor}\label{Bayes Factor}" (i.e. "evidence" in experimental science) is a Bayesian alternative to classical $p$-value hypothesis testing. Bayesian model comparison is a method of model selection based on Bayes factors. The models under consideration are statistical models. The aim of the Bayes factor is to quantify the support for a model over another, regardless of whether these models are correct. The technical definition of "support" in the context of Bayesian inference is described below.
	
	Let us recall that according to Bayes’ theorem, we can update prior probabilities of some model $\mathcal{M}$ ($P(\mathcal{M})$) to posterior probabilities ($P(\mathcal{M}|D)$) after observing some datum $D$ by accounting for the probability of observing that datum given the model ($P(D|\mathcal{M})$, also known as the "likelihood"):
	
	Using this equation, We can compare the probability-odds of two models:
	
	That latter is also often denoted:
	
	Where the left-most term are the posterior odds, the right-most term are the prior odds, and the middle term is the Bayes factor:
	
	That latter is also often denoted:
	
	Thus, Bayes factors can be seen either as a ratio quantifying the relative probability of some observed data by two models as they can be computed by comparing the marginal likelihoods of the two models, or as the degree by which some prior beliefs about the relative credibility of two models are to be updated as they can be computed by dividing posterior odds by prior odds, as we will soon show it.
	
	Obviously when the two models are equally probable a priori, so that $P(\mathcal{M}_1)=P(\mathcal{M}_2)$, the Bayes factor is equal to the ratio of the posterior probabilities of $\mathcal{M}_1$ and $\mathcal{M}_2$.
	
	\begin{table}[H]
		\centering	
		\begin{tabular}{|c|l|}
		\hline
		\rowcolor[gray]{0.75}\textbf{Bayes factor} $\text{BF}_{10}$ & \textbf{Label} \\ \hline 
		$>100$ & Extreme evidence for $H_1$ \\ \hline
		$30-100$ & Very strong evidence for $H_1$ \\ \hline 
		$10-30$ & Strong evidence for $H_1$ \\ \hline 
		$3-10$ & Moderate evidence for $H_1$ \\ \hline 
		$1-3$ & Anecdotal evidence for $H_1$ \\ \hline 
		$1$ & No evidence \\ \hline 
		$1 / 3-1$ & Anecdotal evidence for $H_0$ \\ \hline 
		$1 / 3-1 / 10$ & Moderate evidence for $H_0$ \\ \hline 
		$1 / 10-1 / 30$ & Strong evidence for $H_0$ \\ \hline 
		$1 / 30-1 / 100$ & Very strong evidence for $H_0$ \\ \hline 
		$<1/100$ & Extreme evidence for $H_0$ \\ \hline
		\end{tabular}
		\caption{Interpretations of the Bayes Factor}
	\end{table}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Suppose we have a random variable that produces either a success or a failure. We want to compare a model $\mathcal{M}_{1}$ where the probability of success is $q=1/2$, and another model $\mathcal{M}_{2}$ where $\theta$ is unknown and we take a prior distribution for $\theta$ that is uniform on $[0,1]$. We take a sample of $200,$ and find $115$ successes and $85$ failures. The likelihood can be calculated according to the binomial distribution:
	
	Thus we have for $\mathcal{M}_{1}$:
	
	whereas for $\mathcal{M}_{2}$ we have (the development involves the beta distribution):
	
	The ratio is then:
	
	which is "barely worth mentioning" even if it points very slightly towards $\mathcal{M}_{1}$.\\

	A frequentist hypothesis test of $\mathcal{M}_{1}$ (here considered as a null hypothesis) would have produced a very different result. Such a test says that $\mathcal{M}_{1}$ should be rejected at the $5\%$ significance level, since the probability of getting $115$ or more successes from a sample of $200$ if $\theta = 1/2$ is $0.0200$, and as a two-tailed test of getting a figure as extreme as or more extreme than $115$ is $0.0400$. Note that $115$ is more than two standard deviations away from $100$. Thus, whereas a frequentist hypothesis test would yield significant results at the $5\%$ significance level, the Bayes factor hardly considers this to be an extreme result. Note, however, that a non-uniform prior (for example one that reflects the fact that you expect the number of success and failures to be of the same order of magnitude) could result in a Bayes factor that is more in agreement with the frequentist hypothesis test.
	\end{tcolorbox}

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	We can see in this example that the Bayes factor (evidence) of $H_{0}$ to $H_{1}$ is then the ratio of the likelihood under $H_{0}$ to the \underline{average likelihood} under $H_{1}$ (or "odds" of $H_{0}$ to $H_{1}$)!\\
	
	A classical likelihood-ratio test would have found the maximum likelihood estimate for $\theta$, namely $115/200 = 0.575$, whence:
	
	(rather than averaging over all possible $\theta$). That gives a likelihood ratio of $0.1045$ and points towards $\mathcal{M}_{2}$.\\
		
	$\mathcal{M}_{2}$ is a more complex model than $\mathcal{M}_{1}$ because it has a free parameter which allows it to model the data more closely. The ability of Bayes factors to take this into account is a reason why Bayesian inference has been put forward as a theoretical justification for and generalisation of Occam's razor, reducing Type I errors.\\
		
	On the other hand, the modern method of relative likelihood takes into account the number of free parameters in the models, unlike the classical likelihood ratio. The relative likelihood method could be applied as follows. Model $\mathcal{M}_{1}$ has $0$ parameters, and so its AIC (\SeeChapter{see section Numerical Methods page \pageref{Akaike information criterion}}) value is $2 \cdot 0-2 \cdot \ln (0.005956)=10.2467$. The model $\mathcal{M}_{2}$ has $1$ parameter, and so its AIC  value is $2 \cdot 1-2 \cdot \ln (0.056991)=7.7297$. Hence $\mathcal{M}_{1}$ is about exp $((7.7297-10.2467) / 2)=0.284$ times as probable as $\mathcal{M}_{2}$ to minimize the information loss. Thus $\mathcal{M}_{2}$ is slightly preferred, but $\mathcal{M}_{1}$ cannot be excluded!	
	\end{tcolorbox}
	
	Notice from the above example we can deduce even a more general definition of the Bayes factor:
	
	Or written in the form of the null and alternative hypothesis:
	
	The Bayes factor (evidence) of $H_{0}$ to $H_{1}$ is then ratio of the average likelihood under $H_{0}$ to the average likelihood under $H_{1}$ (or "odds" of $H_{0}$ to $H_{1}$)!
	
	In most cases this factor has however to be calculated using Monte Carlo Markov Chains (\SeeChapter{see section Numerical Methods page \pageref{MCMC}}).
	
	As already mentioned during our study of the Cauchy distribution, when submitting a paper with Bayes Factors for publication, it is likely that the reviewers will ask the authors to justify their priors and to explicit it in their paper (otherwise a Bayes Factor may have been hacked!). Software such as JASP and the \texttt{R} package BayesFactor rely on the Cauchy distribution (with two parameters that are most of time very difficult to justify in practice) as a prior for many of the analyses.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	To avoid hidden Bayes Factor hacking, softwares like JASP automatically provides a plot of the evidence for $H_1$ or $H_0$ for different width parameters of the Cauchy prior (named sometimes "sensitivity analysis"). In addition to the plot, JASP also provides a value denoted $\max \text{BF}10$: this gives the Bayes Factor (for the alternative hypothesis) and the width parameter value of the Cauchy distribution (denoted $r$ in JASP) where the largest Bayes Factor is found. Thus, conducting numerous analyses and reporting only the “best” one is a problem both for Frequentist and Bayesian inference. Bayes Factors do not provide a solution to this problem and also doesn't provide a solution to the false positive rate!!! So $p$-values and Bayes Factors share some of their pitfalls. Neither should be applied without some understanding of the underlying principles! Therefore a shift from Frequentist to Bayesian statistics is unlikely to be sufficient to alleviate the symptoms of the replication crisis...
	\end{tcolorbox}
	
	Then in practice we just replace both numerator and denominator with Monte Carlo estimates. For the numerator we get:
	
	where the subscript $\mathcal{M}_1$ at the expectation indicates that $\theta$ in the expectation is distributed according to (the prior of) model $1$ and, correspondingly, the $\theta_1^{(j)}$ on the right-hand side are independent and identically distributed samples from the prior of model $1$. Using the same approximation for the denominator, we get:
	
	The law of large numbers guarantees that this estimate converges to the correct value $\widehat{\text{BF}}_{12}$ as the sample size $N$ increases.
	
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Caution! Their is some confusion between the frequentist and Bayesian paradigms because they use many of the same words to mean very different things. Name, "probability". Beyond Kolmogorov's axioms i would be a good idea to eliminate the word "probability" from the field of statistics. Under the frequentist paradigm, « \textit{the probability...}» should only ever be referred to as, « \textit{the long-run sampling proportion...}». Likewise under the Bayesian paradigm, « \textit{the probability...}» should only ever be referred as to as « \textit{the unfalsifiable belief of the experimenter...}». If the field of statistics makes this one simple change it can avoid a lot of nonsensical claims.
	\end{tcolorbox}
	
	 \begin{figure}[H]
		\centering
		\includegraphics[width=1\textwidth]{img/arithmetics/bayesian_vs_frequentist.jpg}
		\caption[What it means to be "revolutionary" in statistics]{What it means to be "revolutionary" in statistics (author: Geoffrey Johnson)}
	\end{figure}
	
	\pagebreak
	\subsection{Weak Law of Large Numbers (Khinchin's law)}\label{weak law of large numbers}
	We will now focus on a very interesting relation in statistics that can tell a lot of things while having little information and whatever the distribution (which is not bad!). This is a widely used property for example in statistical simulation in the context of the use of Monte Carlo techniques.
	
	Given a random variable with values in $\mathbb{R}^+$. Then we will show the following relation named "\NewTerm{Markov inequality}\index{Markov inequality}":
	
	with in the particular context of probabilities $\text{E}(X)\leq \lambda$.

	In other words, we propose to prove that the probability that a random variable is greater than or equal to $\lambda$ is less then or equal to its mean divided by the value $\lambda$ considerated and regardless of the probability distribution of the random variable $X$!
	
	\begin{dem}
	Let us write the values of $X$ by $(x_1,\ldots ,x_n)$, where $0\leq x_1 < x_2 < \ldots  <x_n$ (that is to say sorted in ascending order) and let us also write $x_0=0$. We note first that the inequality is trivial in case where $\lambda \geq x_n \geq 0$. Indeed, as $X$ can be included only between $0$ and $x_n$ by definition then the probability that it is greater to $x_n$ is equal to zero. In other words:
	
	and $X$ being positive, then $\text{E}(X)$ is also positive, thus the inequality in this special case in a first time. 
	
	Otherwise, we have $0<\lambda\leq x_n$ and then there exist one $k\in (1,\ldots ,n)$ such that $x_{k-1}<\lambda \leq x_k$. Thus:
	
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\textbf{{\Large \ding{45}}Example:}\\\\
	We assume that the number of outgoing parts of a given factory during one week is a random variable of mean $50$. If we wish to estimate the cumulative probability that production exceeds $75$ parts we will simply apply:
	
	\end{tcolorbox}
	
	Consider now a kind of generalization of this inequality named "\NewTerm{Bienayme-Chebyshev inequality}\index{Bienayme-Chebyshev inequality}\label{Bienayme-Chebyshev inequality}" (abbreviated "\NewTerm{BC inequality}") that will allow us to get a very very interesting and important result a little bit later.

Consider a real random variable $X$ (so we do not limit ourselves to the only cases where it is in $\mathbb{R}^+$). Then we will prove the following Bienayme-Chebyshev inequality:
	
	which expresses the fact that the smaller the standard deviation is, more the probability that the random variable $X$ moves away from its expectation is low.
	
	\begin{dem}
	We obtain this inequality by first writing:
	
	where the choice of the square will serve us for a future simplification.

	Then by applying Markov's inequality (as you see is something that can be useful ...) to the random variable $Y=\left[X-\text{E}(X)\right]^2$ with $\lambda=\varepsilon^2$ it comes automatically:
	
	Then, using the definition of the variance:
	
	We get:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}

	If we put:
	
	The equality will be written:
	
	and expresses the cumulative probability in order that $X$ moves away from the mean of more than $t$ times its standard deviation, is below $1/t^2$. But for $t = 2$, $3$ and $4$, we can deduce that there is, respectively, $75\%$, $89\%$ and $94\%$ of the data between the interval defined by $2$, $3$ and $4$ standard deviations around the mean. There is, in particular, less than $1$ chance on $9$ that $X$ moves away from its mean by more than three times the standard deviation. This is also this theorem that is used by the Basel Committee to define the Value At Risk correction factor used in finance (\SeeChapter{see section Economy page \pageref{value at risk}}).
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\textbf{{\Large \ding{45}}Example:}\\\\
	We take the example where the number of outgoing parts of a given factory during one week is a random variable of mean $50$. We assume in addition that the variance of the weekly production is $25$. We seek to calculate the probability that the production of next week is between $40$ and $60$ pieces.\\
	
	To calculate this we must first remember that the BT inequality is based partially on the term $|X-\text{E}(X)|$ so we have:
	
	Therefore the BT inequality allows us to work on equal intervals in absolute value which is also written:
	
	Then we just have to apply numerical values to the inequality:
	
	\end{tcolorbox}
	
	The last two inequalities obtained before the example will allow us to obtain a very important and powerful relation that we call "\NewTerm{weak law of large numbers}\index{weak law of large numbers}" (WLLN) or "\NewTerm{Khinchin theorem}\index{Khinchin theorem}".

	Consider a random variable $X$ having a variance and $(X_n)_{n \in \mathbb{N}^*}$ a sequence of independent random variables (i.e. uncorrelated by pairs) of the same distribution as this of $X$ and all having the same mean $\mu$ and the same standard deviations $\sigma$.

	What we will show is that if we measure the same random quantity $X_n$ of the same distribution in the process of a series of independent experiments (so in this case, we say technically that the sequence $(X_n)_{n \in \mathbb{N}^*}$ of random variables is defined on the same probability space) , then the arithmetic average of the observed values will stabilize on the mean of $X$ when the number of measurements tends to infinity\footnote{This explains for example why births, weddings, deaths display a fascinating regularity from one year to the other on huge independent - uncorrelated - population of individuals}:
	
	when $n\rightarrow +\infty$ this is the very important result which we did mention above! The empirical estimator of the mean tends for any distribution to the true expected mean if $n$ is large! So by this rule we assure that the sample average is a consistent estimator of mean! This result (quite intuitive) is sometimes named the "\NewTerm{fundamental theorem of Monte Carlo}\index{fundamental theorem of Monte Carlo}\label{fundamental theorem of Monte Carlo}" because it is central to the simulations principle of the same name (\SeeChapter{see section Numerical Methods page \pageref{monte carlo simulations}}), which are crucial in the study of advanced statistics.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	This convergence property is also named "\NewTerm{regression toward the mean}\index{regression toward the mean}" or simply "\NewTerm{reversion to the mean}\index{reversion to the mean}". Implicitly meaning that if a variable is extreme on its first measurement, it will tend to be closer to the average on its second measurement - and if it is extreme on its second measurement, it will tend to have been closer to the average on its first. That's why measuring and individual performance (whatever what kind of performance), only once doesn't make sense scientifically speaking as that performance fluctuates more or less around its mean!!!\\

	One of Galton's first examples was the average height of parents and their children. He found that tall parents had (on average) children who were smaller than them, and that short parents had (on average) children who were taller than them. In both cases the children with parents at the extreme ends of the distribution had heights closer to the population mean height.
	\end{tcolorbox}
	So in other words, the cumulative probability that the difference between the arithmetic average and the expected of the observed random variables to be in a given range around the average tends to zero as the number of measured random variables tends to infinity (that which is ultimately intuitive).
	
	This result allows us to estimate the expected mean value using the empirical mean (arithmetic average) calculated on a very large number of experiments.
	\begin{dem}
	We use the Chebyshev-Bienaymé inequality for the random variable (this relation is difficult to interpret but allows us to get the desired result):
	
	And we first calculate using the mathematical properties of the mean that we proved earlier:
	
	and in a second time using the mathematical properties of the variance as already proved above:
	
	and since we assumed uncorrelated variables then the covariance between them is zero therefore:
	
	So by injecting it into the BT inequality:
	
	that becomes:
	
	and the inequality tends effectively to zero as well $n$ at the denominator tends to infinity.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	Note that the latter relation is often denoted in some works, and according to what we saw at the beginning of this section:
	
	or:
	
	Therefore for $\forall \varepsilon >0$:
	
	
	\subsection{Characteristic Function}
	In probability theory and statistics, the "\NewTerm{characteristic function}\index{characteristic function (probability theory)}\label{charactertistic function}" of any real-valued random variable completely defines its probability distribution. If a random variable admits a probability density function, we will prove further below that then the characteristic function is the inverse Fourier transform of the probability density function. Thus it provides the basis of an alternative route to analytical results compared with working directly with probability density functions or cumulative distribution functions. 
	
	Before giving an engineer proof of the famous central limit theorem, first let us introduce the concept of "characteristic function", which is central in statistics.
	
	First, remember that the Fourier transform is given in its physicist version (\SeeChapter{see section Sequences and Series page \pageref{fourier transform}}) by the relation:
	
	Let us recall that the Fourier transform is an analogue of the Fourier series theory for non-periodic functions, and allows to associate them a frequencies spectrum. To a given factor, it is a "\NewTerm{bilateral Laplace transform}\index{bilateral Laplace transform}" given by (\SeeChapter{see section Analysis page \pageref{bilateral Laplace transform}}):
	
	where $p$ is the complex variable  given in the present case by (the real part is zero, because the Fourier transform is the particular case of a Laplace transform whose real part of the variable is zero: then make Fourier transform is like make a Laplace transform on the axis of imaginary numbers only):
	
	Now we want to prove that if:
	
	In other words, we are looking for a simplified expression of the Fourier transform of the derivative of $f(x)$.
	
	\begin{dem}
	We start from:
	
	An integration by parts gives (\SeeChapter{see section Differential and Integral Calculus page \pageref{integration by parts}}):
	
	By imposing that $f$ tends to zero at infinity, then we have:
	
	and:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	This is the first result we needed.
	\end{dem}
	
	Now let us prove that if:
	
	
	\begin{dem}
	So we start from:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	This is the second result we needed.
	\end{dem}
	
	Now let us perform the calculation of the Fourier transform of the Normal centered-reduced distribution $\mathcal{N}(0,1)$ (this choice is not innocent...):
	
	We know that this latter relation is trivially solution of the following differential equation (or it satisfies it in other words...):
	
	taking the Fourier transform of both sides of the equality, we use the previous two results:
	
	We have:
	
	Or:
	
	Then after integration:
	
	As:
		
	We therefore have:
	
	We proved during our study of the Normal distribution that:
	
	Therefore:
	
	We then have (very important result!):
	
	Let us now introduce the characteristic function as defined by statisticians:
	
	which is an important and powerful analytical tool for analysing a sum of independent random variables. In addition, this function contains all the information characteristic of the random variable $X$ (all moments) as its Taylor development will show it to us just further below!
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The notation is not innocent since the $\text{E}[\ldots ]$ represents an expected mean of the density function with respect to the complex exponential.
	\end{tcolorbox}
	Therefore the characteristic function of the Normal reduced centered random variable of distribution law:
	
	becomes easy to determine because:
	
	This is why the characteristic function of the reduced centered Normal distribution is often assimilated to a simple Fourier transform (\SeeChapter{see section Sequences and Series page \pageref{fourier transform}}).
	
	And thanks to the previous result:
	
	Therefore:
	
	which is the result we will need for the central limit theorem that we will study just after. This characteristic function is equal, to a given constant, to the probability density of the law. Then we say that the characteristic function of a Gaussian function is Gaussian function....
	
	But before that, let us look a little closer this characteristic function:
	
	Using a Maclaurin development (\SeeChapter{see section Sequences and Series page \pageref{taylor series}}) and changing some notations we get:
	
	and by inverting the sum and the integral, we have:
	
	This characteristic function contains therefore all the moments (general term used for the mean, variance, skewness and kurtosis) of $X$.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	Let us see another famous example with the Poisson distribution (interesting case as it is a discrete distribution function!)!\\
	
	Proof. Using the definition of characteristic function:
	$$
	\begin{aligned}
	\varphi_{X}(t) &=\text{E}\left[\exp (\mathrm{i} t X)\right] \\
	&=\sum_{x \in \mathbb{R}_{X}} \exp (\mathrm{i} t x) P_{X}(x) \\
	&=\sum_{x \in \mathbb{R}_{X}}[\exp (\mathrm{i} t)]^{x} \exp (-\lambda) \frac{1}{x !} \lambda^{x} \\
	&=\exp (-\lambda) \sum_{x=0}^{+\infty} \frac{(\lambda \exp (\mathrm{i} t))^{x}}{x !} \\
	&=\exp (-\lambda) \exp (\lambda \exp (\mathrm{i} t)) \\
	&=\exp (\lambda[\exp (\mathrm{i} t)-1])
	\end{aligned}
	$$
	where:
	$$
	\exp (\lambda \exp (\mathrm{i} t))=\sum_{x=0}^{+\infty} \frac{(\lambda \exp (\mathrm{i} t))^{x}}{x !}
	$$
	is the usual Taylor series expansion of the exponential function as proved in the section Sequences and Series page \pageref{usual maclaurin developments} (note that the series converges for any value of $t$).
	\end{tcolorbox}
	
	\subsubsection{Moment generating function and cumulants}\label{moment generating function and cumulants}
	In probability theory and statistics, the moment-generating function of a real-valued random variable is an alternative specification of its probability distribution. Thus, it provides the basis of an alternative route to analytical results compared with working directly with probability density functions or cumulative distribution functions. There are particularly simple results for the moment-generating functions of distributions defined by the weighted sums of random variables. However, not all random variables have moment-generating functions.

	As its name implies, the moment generating function can be used to compute a distribution's moments: the $n$th moment about $0$ is the $n$th derivative of the moment-generating function, evaluated at $0$.

	In addition to real-valued distributions (univariate distributions), moment-generating functions can be defined for vector- or matrix-valued random variables, and can even be extended to more general cases.

	The moment-generating function of a real-valued distribution does not always exist, unlike the characteristic function. There are relations between the behaviour of the moment-generating function of a distribution and properties of the distribution, such as the existence of moments.
	
	The moment-generating function of a random variable $X$ is:
	
	wherever this expectation exists. In other words, the moment-generating function of $X$ is the expectation of the random variable $e^{t X}$.
	
	$M_{X}(0)$ always exists and is equal to $1$ (see proof just below). However, a key problem with moment-generating functions is that moments and the moment generating function may not exist, as the integrals need not converge absolutely. By contrast, the characteristic function or Fourier transform always exists (because it is the integral of a bounded function on a space of finite measure), and for some purposes may be used instead.
	
	The moment-generating function is so named because it can be used to find the moments of distributions. The series expansion of $e^{t X}$ is:
	
	Hence:
	
	where $m_{n}$ is the $n$ th moment. Differentiating $M_{X}(t)$ a number $i$ times with respect to $t$ and setting $t=0$, we obtain the $i$th moment about the origin, $m_{i}$.
	
	If it exists for $t$ in a neighbourhood of $0$, then all "\NewTerm{moments}\index{moments}" of $X$ exist, and:
	
	The "\NewTerm{cumulant generating function}\index{cumulant generating function}\label{cumulant generating function}" is the log of the moment generating function:
	
	It generates the "\NewTerm{cumulants}\index{cumulants}\label{cumulants}", which are defined by what the cumulant generating function generates, i.e., for a random variable, the $k^{t h}$ cumulant is:
	
	Cumulants are slightly easier to work with than moments. The first four are:
	
	The skewness and kurtosis (see page \pageref{skewness and kurtosis}) are then simple functions of the cumulants:
	
	If $X$ is a continuous random variable, the following relation between its moment-generating function $M_{X}(t)$ and the two-sided Laplace transform of its probability density function $f_{X}(x)$ holds:
	
	since the PDF's two-sided Laplace transform (\SeeChapter{see section Analysis page \pageref{bilateral Laplace transform}}) is given as:
	
	and the "\NewTerm{moment-generating function's}\index{moment-generating function}\label{moment-generating function}" (MGF) definition expands (by the law of the unconscious statistician) to:
	
	
	The moment-generating function is the expectation of a function of the random variable, it can be written as:
	\begin{itemize}
		\item For a discrete probability mass function, $M_{X}(t)=\sum_{i=1}^{+\infty} e^{t x_{i}} p_{i}$ 
		
		\item For a continuous probability density function, $M_{X}(t)=\int_{-\infty}^{+\infty} e^{t x} f(x) \mathrm{d} x$
	\end{itemize}

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. Let us take the case of the Normal distribution.\\
	
	Suppose $X = \mathcal{N}\left(\mu, \sigma^{2}\right)$ (with $\sigma^{2}>0$). Then its MGF is:
	$$
	\begin{aligned}
	M_{X}(t) &=\text{E}\left(e^{t X}\right)=\frac{1}{\sqrt{2 \pi} \sigma} \int\limits_{-\infty}^{+\infty} e^{t x} e^{-\frac{1}{2} \frac{(x-\mu)^{2}}{\sigma^{2}}} \mathrm{d} x =\frac{1}{\sqrt{2 \pi} \sigma} \int\limits_{-\infty}^{+\infty} e^{t x-\frac{1}{2} \frac{1}{\sigma^{2}}\left(x^{2}-2 \mu x+\mu^{2}\right)}\mathrm{d} x  \\
	&=\frac{1}{\sqrt{2 \pi} \sigma} \int\limits_{-\infty}^{+\infty} e^{-\frac{1}{2} \frac{1}{\sigma^{2}}\left(x^{2}-2\left(\mu+t \sigma^{2}\right) x+\mu^{2}\right)}\mathrm{d} x 
	\end{aligned}
	$$
	In the exponent, complete the square with respect to the $x$:
	$$
	x^{2}-2\left(\mu+t \sigma^{2}\right) x=\left(x-\left(\mu+t \sigma^{2}\right)\right)^{2}-\left(\mu+t \sigma^{2}\right)^{2}
	$$
	Then:
	$$
	M_{X}(t)=e^{\frac{1}{2}\frac{1}{\sigma^{2}}\left(\left(\mu+t \sigma^{2}\right)^{2}-\mu^{2}\right)} \int\limits_{-\infty}^{+\infty} \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{1}{2} \frac{1}{\sigma^{2}}\left(x-\left(\mu+t \sigma^{2}\right)\right)^{2}} \mathrm{d}x
	$$
	Notice that the integrand is the probability density function of a $\mathcal{N}\left(\mu+t \sigma^{2}, \sigma^{2}\right)$, which means the integral is $1$. Expanding the $\left(\mu+t \sigma^{2}\right)^{2}$ and simplifying a bit yields:
	$$
	M_{X}(t)=e^{t \mu+\frac{1}{2} \sigma^{2} t^{2}}, t \in \mathbb{R}
	$$
	The cumulant generating function is then a simple quadratic:
	$$
	c_{X}(t)=t \mu+\frac{1}{2} \sigma^{2} t^{2}
	$$
	and it is easy to see that:
	$$
	c_{X}^{\prime}(0)=\mu, \quad c^{\prime \prime}(0)=\sigma^{2}, \quad c^{\prime \prime \prime}(t)=0
	$$
	Thus the mean is $\mu$ and variance is $\sigma^{2}$ (not surprisingly), and all other cumulants are $0$. In particular, the skewness and kurtosis are both $0$ .\\
	
	E2. Let us see now an example with the Gamma distribution.\\
	
	The Gamma distribution has as we know two parameters: $\alpha>0$ is the shape parameter, and $\lambda>0$ is the scale parameter. Its space is $\mathcal{X}=[0, +\infty[$, and its probability density function is $c x^{\alpha-1} \exp (-\lambda x)$, where:
	$$
	\frac{1}{c}=\int\limits_{0}^{+\infty} x^{\alpha-1} e^{-\lambda x} \mathrm{d} x =\int\limits_{0}^{+\infty}(u / \lambda)^{\alpha-1} e^{-u}  \mathrm{d} u / \lambda=\frac{1}{\lambda^{\alpha}} \int\limits_{0}^{+\infty} u^{\alpha-1} e^{-u} \mathrm{d} u=\frac{1}{\lambda^{\alpha}} \Gamma(\alpha)
	$$
	The gamma function is defined to be that very integral, i.e., for $\alpha>0$ (\SeeChapter{see section Differential and Integral Calculus page \pageref{gamma euler function}}):
	$$
	\Gamma(\alpha)=\int\limits_{0}^{+\infty} u^{\alpha-1} e^{-u} \mathrm{d} u
	$$
	It generally has no closed-form value, but there are some convenient facts that we have already proven during it's study:
	$$
	\begin{aligned}
	\Gamma(\alpha+1) &=\alpha \Gamma(\alpha) \text { for } \alpha>0 \\
	\Gamma(n) &=(n-1) ! \text { for } n=1,2, \ldots ; \\
	\Gamma\left(\frac{1}{2}\right) &=\sqrt{\pi}
	\end{aligned}
	$$
	The probability density function for a $\Gamma(\alpha, \lambda)$ is then:
	$$
	f(x \mid \alpha, \lambda)=\frac{\lambda^{\alpha}}{\Gamma(\alpha)} x^{\alpha-1} e^{-\lambda x}, x \in(0, \infty)
	$$
	If $\alpha=1$, then we fall back on the "exponential distribution" $(\lambda)$, with probability density function:
	$$
	g(x \mid \lambda)=\lambda e^{-\lambda x}, x \in \mathcal{X}=[0, +\infty[
	$$
	The MGF is:
	$$
	M_{X}(t)=\text{E}\left(e^{t X}\right) =\frac{\lambda^{\alpha}}{\Gamma(\alpha)} \int\limits_{0}^{+\infty} e^{t x} x^{\alpha-1} e^{-\lambda x}\mathrm{d}x =\dfrac{\lambda^{\alpha}}{\Gamma(\alpha)} \int\limits_{0}^{+\infty} x^{\alpha-1} e^{-(\lambda-t) x}\mathrm{d}x
	$$
	That integral needs $(\lambda-t)>0$ to be finite, so we need $t<\lambda$, which means the MGF is finite for a neighbourhood of zero, since $\lambda>0$. Now the integral at the end of the previous relation is the same as that the one just earlier above:
	$$\int\limits_{0}^{+\infty} x^{\alpha-1} \exp (-\lambda x) \mathrm{d} x$$
	except that $\lambda-t$ is in place of $\lambda .$ Thus we have:
	$$
	\text{E}\left(e^{t X}\right) =\frac{\lambda^{\alpha}}{\Gamma(\alpha)} \frac{\Gamma(\alpha)}{(\lambda-t)^{\alpha}} =\left(\frac{\lambda}{\lambda-t}\right)^{\alpha}, t<\lambda
	$$
	We will use the cumulant generating function $c_{X}(t)=\log \left(M_{X}(t)\right)$ to obtain the mean and variance, because it is slightly easier. Thus:
	$$
	c_{X}^{\prime}(t)=\frac{\partial}{\partial t} \alpha(\log (\lambda)-\log (\lambda-t))=\frac{\alpha}{\lambda-t} \Longrightarrow \text{E}(X)=c_{X}^{\prime}(0)=\frac{\alpha}{\lambda}
	$$
	and:
	$$
	c_{X}^{\prime \prime}(t)=\frac{\partial^{2}}{\partial t^{2}} \alpha(\log (\lambda)-\log (\lambda-t))=\frac{\alpha}{(\lambda-t)^{2}} \Longrightarrow \text{V}(X)=c_{X}^{\prime \prime}(0)=\frac{\alpha}{\lambda^{2}}
	$$
	In general, the $k^{\text {th }}$ cumulant is:
	$$
	\gamma_{k}=\left.\frac{\partial^{k}}{\partial t^{k}} c_{X}(t)\right|_{t=0}=(k-1) ! \frac{\alpha}{\lambda^{k}}
	$$
	and in particular:
	$$
	\begin{aligned}
	\text { Skewness }(X)&=\frac{2 \alpha / \lambda^{3}}{\alpha^{3 / 2} / \lambda^{3}}=\frac{2}{\sqrt{\alpha}} \\
	\text { Kurtosis }(X)&=\frac{6 \alpha / \lambda^{4}}{\alpha^{2} / \lambda^{4}} =\frac{6}{\alpha}
	\end{aligned}
	$$
	Thus the skewness and kurtosis depends on just the shape parameter $\alpha$. Also, they are positive, but tend to $0$ as $\alpha$ increases.\\
	
	E3. Let us see now an example with the binomial distribution.\\
	
	The Binomial is as we already know a model for counting the number of successes in $n$ trials, e.g., the number of heads in ten flips of a coin, where the trials are independent and have the same probability $p$ of success:
	$$
	X = \mathcal{B}(n, p) \Longrightarrow f_{X}(x)=\left(\begin{array}{c}
	n \\
	x
	\end{array}\right) p^{x}(1-p)^{n-x}, \quad x \in \mathcal{X}=\{0,1, \ldots, n\}
	$$
	The fact that the probability density function sums to $1$ relies on the binomial theorem (\SeeChapter{see section Calculus page \pageref{binomial theorem}}):
	$$
	(a+b)^{n}=\sum_{x=0}^{n} a^{x} b^{n-x}
	$$
	with $a=p$ and $b=1-p$. This theorem also helps in finding the MGF:
	$$
	\begin{aligned}
	M_{X}(t)&=\text{E}\left(e^{t X}\right)=\sum_{x=0}^{n} e^{t x} f_{X}(x)=\sum_{x=0}^{n} e^{t x}\left(\begin{array}{c}n \\x\end{array}\right) p^{x}(1-p)^{n-x} \\
	&=\sum_{x=0}^{n}\left(\begin{array}{c}n \\x	\end{array}\right)\left(p e^{t}\right)^{x}(1-p)^{n-x}=\left(p e^{t}+1-p\right)^{n}	
	\end{aligned}
	$$
	It is finite for all $t \in \mathbb{R}$, as is the case for any bounded random variable. The first two moments are:
	$$
	\text{E}(X)=M_{X}^{\prime}(0)=\left.n\left(p e^{t}+1-p\right)^{n-1} p e^{t}\right|_{t=0}=n p
	$$
	and:
	$$
	\begin{aligned}
	\text{E}\left(X^{2}\right) &=M_{X}^{\prime \prime}(0)=\left.\left[n(n-1)\left(p e^{t}+1-p\right)^{n-2}\left(p e^{t}\right)^{2}+n\left(p e^{t}+1-p\right)^{n-1} p e^{t}\right]\right|_{t=0} \\
	&=n(n-1) p^{2}+n p
	\end{aligned}
	$$
	Thus:
	$$
	\begin{aligned}
	\text{V}(X)&=\text{E}\left(X^{2}\right)-\text{E}(X)^{2} =n(n-1) p^{2}+n p-(n p)^{2}=n p-n p^{2}=n p(1-p)\end{aligned}
	$$
	E4. Let us see now an example with the Poisson distribution.\\
	$$
	\begin{aligned}
	M_{X}(t)&=\text{E}\left(e^{t X}\right)=\sum_{x=0}^{n} e^{t x} f_{X}(x)=\sum_{x=0}^{+\infty} e^{t x}e^{-\lambda}\dfrac{1}{x!}\lambda^x\\
	&=e^{-\lambda}\sum_{x=0}^{+\infty}\dfrac{\left(\lambda e^{t}\right)^x}{x!}\\
	&=e^{-\lambda}e^{\lambda e^{t}}=e^{\lambda(e^{t}-1)}
	\end{aligned}
	$$
	where:
	$$
	e^{\lambda e^{t}}=\sum_{x=0}^{+\infty}\dfrac{\left(\lambda e^{t}\right)^x}{x!}
	$$
	is the usual Taylor series expansion of the exponential function (\SeeChapter{see section Sequences and Series page \pageref{usual maclaurin developments}}). Furthermore, since the series converges for any value of $t$, the moment generating function of a Poisson random variable exists for any $t\in\mathbb{R}$
	\end{tcolorbox}
	\begin{theorem}
	\label{approximation Poisson as Normal}The limiting distribution of a Poisson $(\lambda)$ distribution as $\lambda \rightarrow +\infty$ is Normal (this will be useful to us in the section of Quality Engineering).
	\end{theorem} 
	\begin{dem}
	Let $X_n \sim \operatorname{Poisson}(n)$, for $n=1,2, \ldots$ The probability mass function of $X_n$ is:
	
	The moment generating function of $X_n$ is as proved in the example just above:
	
	for $-\infty<t<+\infty$. Taking the limit gets us nowhere because:
	
	So now consider a "standardized" Poisson random variable:
	
	which has limiting moment generating function:
	
	by using the moment generating function of a Poisson random variable and expanding the exponential function as a series. This can be recognized as the moment generating function of a standard Normal random variable! This implies that the associated unstandardized random variable $X_n$ has a limiting distribution that is Normal with mean $n$ and variance $n$. This result is the basis for the "normal approximation to the Poisson distribution"! In practice most people consider the approximation valid enough if $n>20$ (i.e. $\lambda>20$).

	\end{dem}
	
	\pagebreak
	\subsection{Central Limit Theorem}\label{central limit theorem}
	The central limit theorem is a set of results from the early 120th century (holocene calendar) on the weak convergence in probability of a sequence of random variables. Intuitively, from these results, any sum (implicitly: the average of these variable) of independent random variables identically distributed tends to some given random variable. The best known and most important result is simply named "\NewTerm{central limit theorem}\index{central limit theorem}" concerning a sum of independent random variables with existing variance whose number tends to infinity and it is this that we will prove heuristically here.
	
	In the simplest case, considered below for the proof of the theorem, these variables are continuous, independent and have the same mean and the same variance. To try to get a finite result, we should center this sum by subtracting its average and reduce it by dividing it by its standard deviation. Under fairly broad conditions, the probability distribution (of the average) converges to a Normal centered reduced distribution. The ubiquity of the Normal distribution is explained by the fact that many considered random phenomena are due to the superposition of an infinity of many small causes.
	
	This probability theorem has an interpretation in mathematical statistics. The latter associates a probability distribution to a population. Each element extracted from the population is considered as a random variable and by bringing together a number $n $ of these supposed independent variables, we get a sample. The sum of these random variables divided by $n$ gives a new variable named the "empirical mean". This, once reduced, tends to a Normal reduced variable as $n$ approaches infinity as we know.
	
	The central limit theorem tells us what we should expect in terms of sums of independent random variables. But what about the products? Well, the logarithm of a product (strictly positive factors) is the sum of logarithms of factors, so that the logarithm of a product of random variables (with strictly positive values) tends to a Normal distribution, resulting a log-normal distribution for the product itself (see our study of the Log-Normal distribution earlier above).
	
	In itself, the convergence to the Normal distribution ("\NewTerm{asymptotic normality}\index{asymptotic normality}") of many random variables when their number tends to infinity only interests the mathematician. For the practitioner, it is worth stopping shortly before the limit: the sum of a large number of these variables is nearly Gaussian, which often provides a more usable approximation than the exact law.
	
	Moving further away from theory, we can say that a good number of natural phenomena are due to the superposition of numerous causes, more or less independent. As a result, the Normal law represents them reasonably effectively.
	
	Conversely, we can say that no concrete phenomenon is really Gaussian because it can not exceed certain limits, especially if its values are all positive (or all negative).
	
	\begin{dem}
	Given $\left\lbrace X_i \right\rbrace_{i=1\ldots +\infty}$ a suite (sample) of continuous random variables (in our simplified proof...), independent (independent measures of physical or mechanical phenomena for example) and identically distributed, for which the average  $\mu_X$ and standard deviation $\sigma_X$ exist (this means that the central limit theorem works for finite variance phenomena only as far as we know!!!).
	
	We saw earlier in this section that:
	
	are the same expressions of centered reduced variable generated using a sequence of $n$ identically distributed random variables that by construction therefore has a zero mean and unit variance:
	
	Let us develop the first form of the prior-previous  equality (the both are anyway equal!):
	
	Now using the characteristic function of the Normal centered-reduced distribution  (we simplify at the same time the notations of the estimators of the average and standard deviation):
	
	As the random variables $X_i$ are independent and identically distributed, we get:
	
	A Taylor expansion (\SeeChapter{see section Sequences and Series page \pageref{taylor series}}) of the term between braces gives at the third order (Maclaurin series expansion of the exponential):
	
	Finally:
	
	Let us put:
	
	Then we have:
	
	So as $x$ approaches infinity (\SeeChapter{see section Functional Analysis page \pageref{limits}}):
	
	We find back the characteristic function of the reduced centered Normal distribution!
	
	In two words, the Central Limit Theorem (CLT) said that for large samples, the centered and reduced sum of $n$ random variables independent ad identically distributed follow a Normal centered and reduced distribution. And so we have verbatim to the empirical mean:
	
	Or more elegantly the CLT will be written:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Caution! In facts there are three central limits theorem (CLT) nowadays... The "Lindeberg-Levy CLT" proven just above that isn't very useful because of the need for identically distributed variables. The "Lyapunov's and Lindeberg CLT" which is much more flexible relaxing the requirement of identically distributed variables at the cost of the necessity of higher moments like skewness and kurtosis to disappear sufficiently quickly (being just finite and quickly go towards zero). The "Gnedenko-Kolmogorov CLT" that is even better as it handles also heavy-tails (but there are also traps as many things in life are not as easy as they seem to...!).
	\end{tcolorbox}
	
	Now we will illustrate the central limit theorem in the case of a sequence $\left\lbrace X_i \right\rbrace$ of discrete random variables following a Bernoulli distribution with parameter equal to $1/2$.
	
	We can imagine that $XN$ represents the result obtained in the $n$-th launch of a coin (assigning the number $1$ for head and $0$ for tail). Let us write:
	
	the average. We obviously have for all $n$ in this special case:
	
	and therefore:
	
	After having centered and reduced $\bar{X}_n$ we get:
	
	Let us note $\Phi$ as always the cumulative Normal reduced centered distribution.
	
	The central limit theorem says that for any $t \in \mathbb{R}$:
	
	Using Maple 4.00b we have plot in blue some graphs of the function:
	
	for different values of $n$. We have Represented in red the function $\Phi$.
	
	For $n=1$:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/clt_bernoulli_n_1.jpg}
		\caption[Approaching the Bernoulli distribution by the\\ Normal distribution according to the CLT]{First approach of the Bernoulli distribution by the Normal distribution according to the CLT}
	\end{figure}
	For $n=2$:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/clt_bernoulli_n_2.jpg}
		\caption[]{Second approach of the Bernoulli distribution by the\\ Normal distribution according to the CLT}
	\end{figure}
	For $n=5$:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/clt_bernoulli_n_5.jpg}
		\caption[]{Fifth approach of the Bernoulli distribution by the\\ Normal distribution according to the CLT}
	\end{figure}
	For $n=30$:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/clt_bernoulli_n_30.jpg}
		\caption[]{Thirteenth approach of the Bernoulli distribution by the\\ Normal distribution according to the CLT}
	\end{figure}
	These graphs were obtained with Maple 4.00b using the following commands:
	
	\texttt{>with(stats):\\
	>with(plots):\\
	>e1:=plot(Heaviside(t+1)*statevalf[dcdf,binomiald[1,0.5]](trunc((t+1)/2))\\
	,t=-2..2,y=0..1,color=blue):\\
	>e2:=plot(Heaviside(t+sqrt(2))*statevalf[dcdf,binomiald[2,0.5]]\\
	(trunc((t*sqrt(2)+2)/2)),t=-sqrt(2)-1..sqrt(2)+1,y=0..1,color=blue):\\
	>e3:=plot(Heaviside(t+sqrt(5))*statevalf[dcdf,binomiald[5,0.5]]\\
	(trunc((t*sqrt(5)+5)/2)),t=-sqrt(5)-1..sqrt(5)+1,y=0..1,color=blue):\\
	>e4:=plot(statevalf[cdf,normald](t),t=-5..5):\\
	>e5:=plot(Heaviside(t+sqrt(30))*statevalf[dcdf,binomiald[30,0.5]]\\
	(trunc((t*sqrt(30)+30)/2)),t=-sqrt(30)-1..sqrt(30)+1,y=0..1,color=blue):\\
	>display({e1,e4});\\
	>display({e2,e4});\\
	>display({e4,e3});\\
	>display({e5,e4});}
	
	clearly show the convergence of $F_n$ to $\Phi$.
	
	In fact we see that convergence is downright uniform which is confirmed by the "\NewTerm{Moivre-Laplace central limit theorem}\index{Moivre-Laplace central limit theorem}":
	
	Given $X_n$ a sequence of independent random variables with the same Bernoulli parameter $p$, $0<p<1$. Therefore:
	
	tends uniformly to $\Phi(t)$ on $\mathbb{R}$ when $n \rightarrow +\infty$.
	
	\subsection{Univariate Hypothesis and Adequation tests (NHST)}\label{nhst}
	
	\textbf{Definition (\#\thesection.\mydef):} A "\NewTerm{statistical model}\index{statistical model}" is a mathematical model that embodies a set of statistical assumptions concerning the generation of sample data (and similar data from a larger population), often in considerably idealized form. It is usually specified as a mathematical relationship between one or more random variables and other non-random variables named "parameters". All statistical hypothesis tests and all statistical estimators are derived via statistical models!
	
	During our study of confidence intervals, remember that we get the below few relations (it is only a sample of the most important one proved above!):
	
	and:
	
	and:
	
	and finally:
	
	that allowed to do statistical inference based on the knowledge or not of the true mean or variance of the whole or on a sample of the population. In other words, under in what range stood a given moment (mean or variance) as a function of the chosen confidence level $\alpha$.
	
	We will also prove in details later the following two intervals:
	
	and:
	
	These are only a few of the more than $500$ parametric and non-parametric confidence intervals - and their respective tests - that exist at the beginning of the 121st century (for more see the Data Science mind map at page \pageref{mindmap of data science})!!!
	
	\textbf{Definition (\#\thesection.\mydef):} When we want to know if we can trust the value of a statistic (mean, median, variance, correlation coefficient, etc.) with some certainty, we speak of "\NewTerm{hypothesis testing}\index{hypothesis testing}" and especially of "\NewTerm{conformity test}\index{conformity test}" (we speak of "\NewTerm{adequation test}\index{adequation test}" when the purpose is to check that measures will follow a particular distribution law).
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The reader must also remember, as already said earlier in this section, that we have put many other confidence interval techniques detailed proofs related for example for regression techniques in the section of Theoretical Computing.
	\end{tcolorbox}	
	
	Hypothesis testing are intended to check whether the sample can be considered as extracted from a given population or be representative of this population vis-a-vis of parameter such as a mean, variance or the observed frequency. This implies that the theoretical distribution of the parameter is known at the population level. The hypothesis tests are not made to prove the null hypothesis (usually expressing equality or uniformity between different populations), but to eventually reject it (to be exact the rejection of the null hypothesis is more robust). In terms of the communication of statistical tests a number of experts recommend:
	
	\begin{enumerate}
		\item To always communicate the $p$-value with four decimal places (we will come back later on this concept).
		
		\item Never say that a low $p$-value shows a significant magnitude of the effect studied because it is not necessary true (to check you just take a phenomenon of very small amplitude on a large sample and then the $p$-value will become very small by construction). Once again will discuss this more deeply later.
		
		\item To always give the confidence interval of the test whether it is unilateral or bilateral.
		
		\item To be careful to not to set a rejection threshold to the test except if a standard (norm) or legislation requires it (in which case we will specify which one).
		
		\item To never say that the test is "proved" or "significant" or even "statistically significant". Just say that the result is "statistical" or that we have the "likelihood of the data knowing the null hypothesis" and that's it (in bayesian terms: «\textit{the likelihood of the data is not the credence of the model}»)!
		
		 \item If the interest is to show the null hypothesis and that it is not rejected, since often its statistical power is low, you will need to repeat the experience to reinforce the conclusion.
		 
		 \item If the interest is to reject the null hypothesis and that this is true,  a good scientific practice is to look for additional studies that would faulty conclusion.
		 
		 \item If there is for example no statistical difference between two values, therefore this does not mean that there is presence of statistical equivalence. It is then necessary to proceed to "equivalence tests".
		 
		 \item The rejection of the null hypothesis does not mean that the mechanism of the phenomenon has been highlighted but just indicates, for refresh, an information about the size of the dataset a posteriori.
		 
		 \item We communicate the  posteriori power of the test.
	\end{enumerate}
	To summarize, the studies must be communicated respecting the principle of truthfulness, after having been the subject of appropriate controls, and must be exposed, described and presented with impartially (some people say "\NewTerm{a-theoric}\index{a-theoric}"). We must not confuse between objectives and speculatives results! The conclusions should be the most faithful expression of the content of facts and datas.
	
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe The reader has to know that Null Hypothesis Significance Testing (NHST) is the result of the evolution of two hypothesis testing theories, ie the Fisher's approach to data testing and the Neyman-Pearson's approach tests of acceptance. The final procedure is the incongruent combination of the previous two theories into the current approach: NSHT. We may also consider that Bayesian Hypothesis Testing (BHT) is itself a kind (...) of evolution of NHST. All these four hypothesis testing theories (Fisher, Neyman-Pearson, NHST, BHT) are complementary and the scientist should avoid to use only one of them when doing analyzing an experiment outcome. As already mentioned at the beginning of the book, different analytical approach are or first importance to compare the results by minimizing the practitioner bias. For more details see \cite{perezgonzalez2015fisher}.
	\end{tcolorbox}
	
	For example, if we want to know with some confidence whether a given mean of a population sample is realistic about the true unknown theoretical mean we will use the "\NewTerm{$Z$-test}\index{$Z$-test}" which is simply:
	
	Now remember that we have proved that if we have two random variables of law:
	
	then subtracting (differentiate) the averages gives:
	
	So for the difference of two average of random variables from two independent population samples we obtain directly:
	
	We can then adapt the $Z$-test as:
	
	The relation that is very useful when for two samples of two populations, we want check if there is a statistically significant difference of the mean to a given fixed level of confidence level $\alpha$ and the associated probability for this difference:
	
	Therefore:
	
	We then speak of "\NewTerm{two-sample $Z$-test}\index{two-sample $Z$-test}" and it is used lost in the industry to ensure the equality of two averages of two measurements of populations when standard deviation is known.
	
	And if the theoretical standard deviation is not known, we use the "\NewTerm{Student $T$-test}\index{Student $T$-test}" (used a lot in pharmacoeconomics) proved above:
	
	In the same idea for standard deviation, we use the "\NewTerm{Chi-square variance test}\index{chi-square variance test}" as already proved above:
	
	And when we want to test for the equality of variance of two populations we use "\NewTerm{Fisher $F$-test}\index{Fisher $F$-test}" (that will be proved below during our study of the analysis of variance):
	
	In practice we must be aware that the purpose of a test is very often to show that the effect is significant. It is then customary to say that the test is successful if the null hypothesis is rejected in favour of the alternative hypothesis! When the practitioner knows that the effect is significant and his test fails to reject the null hypothesis $H_0$ this is sometimes named the "\NewTerm{dilemma of not rejecting the null hypothesis}\index{dilemma of not rejecting the null hypothesis}". As we will see a little further, the idea is then to calculate an "\NewTerm{a posteriori test power}\index{a posteriori test power}" (it is then named by some softwares like SPSS, "\NewTerm{observed power}\index{observed power (statistics)}") and adapt the sample size accordingly to have an acceptable power according to tradition.
	
	\subsubsection{Direction of hypothesis test and $p$-values}\label{p value}\index{$p$-value}
	The fact that we get all values satisfying a right bounded test AND (!) left bounded is what we name in the general case a "\NewTerm{two-tailed test}\index{two-tailed test}" as it includes the left sided and right sided unilateral tests. Thus, all the above tests are in a bilateral form, but we could make a unilateral use too! We use one-tailed test when the expected difference (or difference to highlight) can only go in one direction (typically in the case of clinical trials or during a corrective quality action in the industry for which we expect a improvement going in one single direction). Unilateral tests are sometimes named "\NewTerm{non-inferiority tests}\index{non-inferiority tests}" (left-sided) or "\NewTerm{non-superiority test}\index{non-superiority test}" (right-sided).
	
	Below we presented for example a right unilateral test (since the rejection region is on the right and therefore the cumulative probability is left-sided) and a two-sided test of $5\%$ (since the beginning of the 121st century (holocene calendar), more and more practitioners recommend to take a value of $1\%$):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[>=stealth,
		  every node/.style={rounded corners},
		  declare function={
		    normalpdf(\x,\mu,\sigma)=
		    (2*3.1415*\sigma^2)^(-0.5)*exp(-(\x-\mu)^2/(2*\sigma^2));
		  }]
		  
		  \pgfplotsset{
		  myplot/.style = {
		    width = 10cm, height = 5cm,
		    samples = 75,
		    ticks = none,
		  }
		  } %
		
		  \begin{scope}
		    \begin{axis}[myplot, smooth]
		      \def\zValue{1.645}
		      \addplot[domain = -4:-\zValue, draw = none, fill = cyan, opacity = 0.75] {normalpdf(x, 0, 1)} \closedcycle;
		      \addplot[domain = \zValue:4, draw = none, fill = orange, opacity = 0.75] {normalpdf(x, 0, 1)} \closedcycle;
		      \addplot[smooth, thick, domain = -4:4, color = gray] {normalpdf(x,0,1)};
		      \path[draw, o->] (axis cs: 2, 0.02) to[out = 90, in = 180] (axis cs: 2, 0.2) node[right] {$\alpha/2=2.5\%$};
		      \path[draw, o->] (axis cs: -2, 0.02) to[out = 90, in = 0] (axis cs: -2, 0.2) node[left] {$\alpha/2=2.5\%$};
		      \node at (axis cs: 0, 0.1) {$1 - \alpha$};
		      \node[below right, align = left] at (axis description cs: 0.01, 0.97) {Bilateral\\ $H_0: \theta \neq \theta_0$};
		    \end{axis}
		  \end{scope}
		
		  \begin{scope}[yshift = -3.75cm]
		    \begin{axis}[myplot, smooth]
		      \def\zValue{1.282}
		      \addplot[domain = -4:-\zValue, draw = none, fill = cyan, opacity = 0.75] {normalpdf(x, 0, 1)} \closedcycle;
		      \addplot[smooth, thick, domain = -4:4, color = gray] {normalpdf(x,0,1)};
		      \path[draw, o->] (axis cs: -1.7, 0.03) to[out = 90, in = 0] (axis cs: -2, 0.2) node[left] {$\alpha=2.5\%$};
		      \node at (axis cs: 0.25, 0.1) {$1 - \alpha$};
		      \node[below right, align = left] at (axis description cs: 0.01, 0.97) {Unilateral left\\ $H_0: \theta < \theta_0$};
		    \end{axis}
		  \end{scope}
		
		  \begin{scope}[yshift = -7.5cm]
		    \begin{axis}[myplot, smooth]
		      \def\zValue{1.282}
		      \addplot[domain = \zValue:4, draw = none, fill = orange, opacity = 0.75] {normalpdf(x, 0, 1)} \closedcycle;
		      \addplot[smooth, thick, domain = -4:4, color = gray] {normalpdf(x,0,1)};
		      \path[draw, o->] (axis cs: 1.7, 0.03) to[out = 90, in = 180] (axis cs: 2, 0.2) node[right] {$\alpha=2.5\%$};
		      \node at (axis cs: -0.25, 0.1) {$1 - \alpha$};
		      \node[below right, align = left] at (axis description cs: 0.01, 0.97) {Unilateral right\\ $H_0: \theta > \theta_0$};
		    \end{axis}
		  \end{scope}
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[Illustration of a test (or confidence level) unilateral and bilateral]{Illustration of a test (or confidence level) unilateral and bilateral\\(author: Walmes Zeviani)}
	\end{figure}
	We can also summarize how to determine the $p$-value (which will be discussed more in detail further below) with the following diagram:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/p_values_construction.jpg}
		\caption{Resume figure to determine the $p$-value of parametrical test with symmetrical distributions.}
	\end{figure}
	
	\textbf{Definition (\#\thesection.\mydef):} The "\NewTerm{$p$-value}\index{$p$-value}" is the likelihood of obtaining an effect at least as extreme as the one in your sample data, assuming the truth of the null hypothesis. In other words, if the null hypothesis is true, the $p$-value is the probability of obtaining your sample data. It answers the question, are your sample data unusual if the null hypothesis is true? If you are thinking that the $p$-value is the probability that the null hypothesis is true, the probability that you are making a mistake if you reject the null, or anything else along these lines, this is the most common misunderstanding of practitioners.
	
	As already mentioned, the $p$-value is often considered as significant if it is less than $5\%$ or even better when... below $1\%$. In particle physics, according to the Gauss distribution (for which it is easy to convert percentages into standard deviation as we already know!), the $p$-value is often communicated in $\sigma$ as shown in the random example below of pentaquark discovery studies in 12003 (holocene calendar):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/arithmetics/sigma_pvalues_pentaquark.jpg}
	\end{figure}
	or another random (well-known random...) example of the Higgs Boson confirmation discovery at the LHC in 12012\footnote{The standard of certainty that particle physicists require at the beginning of the 121st century (holocene calendar) for a discovery is roughly $1$ in $3.5$ million, ie $5\sigma$.} (holocene calendar):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{img/arithmetics/sigma_pvalues_higgs_boson.jpg}
	\end{figure}
	The common misconception is what we'd really like to know. We would loooove to know the probability that a hypothesis is correct, or the probability that we're making a mistake. What we get instead with these frequentist methods is the probability of our observation given the null hypothesis, which just isn't as useful.
	
	It would be great if we could take evidence solely from a sample and determine the probability that the sample is wrong. Unfortunately, that seems not possible with the frequentist methods and this for logical reasons when you think about it. Without outside information, a sample can't tell you whether it's representative of the population.
	
	The $p$-values are based exclusively on information contained within a sample. Consequently, $p$-values can't answer the question that we most want answered, but there seems to be an irresistible temptation towards interpreting it that way.
	
	Let us also note that the hypothesis tests on the standard deviation (variance), the mean or correlation are named "\NewTerm{parametric tests}\index{parametric tests}" in reverse of the nonparametric tests that we will see much further below.
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} There is also another definition of the concept of parametric and nonparametric tests that we will see further below (a bit different because more precise).\\
	
	\textbf{R2.} Warning! Some authors or teachers sometimes talk about "left-sided" for a "right-sided test"... In fact it is simply a choice of vocabulary. If the reference of teaching is not the rejection area but the acceptance area, then it is clear that the right and left are concepts that are reversed...
	\end{tcolorbox}	
	Finally, many software calculate what we name therefore the "\NewTerm{$p$-value}" which is the limit calculated risk (probability threshold) $\alpha$ that might have set the statistician to be at the border between acceptance of the null hypothesis and its rejection (remember that a successful test does not prove anything and a rejection is better but still do not prove anything!). So the $p$-value is a fundamental value in statistics because it gives the possibility to quantify the likelihood of the null hypothesis $H_0$ (acceptance or rejection).
	
	\textbf{Definition (\#\thesection.\mydef):} But strictly speaking the "\NewTerm{$p$-value}\index{$p$-value}" is the conditional (Bayesian) probability, that our data satisfy the null hypothesis (that $H_0$ is true\footnote{Equivalently we can say that it is the conditional probability of obtaining data as extreme as the one that was actually observed, assuming that the null hypothesis is true! Another nice way to describe, it is to say that the $p$-value is the proportion of times that you would see evidence stronger than what was observed, against the null hypothesis, if the null hypothesis were true and you hypothetically repeated the experiment (sampling of individuals from a population) a large number of times!}), ie the likelihood of results:
	
	 and (sadly...) not the probability of the null hypothesis knowing the data (plausibility of $H_0$, ie $P(H_0|\text{data})$)!:
		 
	 While the difference may be small as we saw in the section Probabilities, it is not zero! So in reality the $p$-value says nothing about the hypothesis itself, but it provides information on the experimental data if the experiment is conducted infinite number of times and all relevant assumptions hold! So as a $p$-value simply gives information only in the case we would repeat the experiment a large number of times we understand better why $p$-values are often decried in single experiment studies...
	 
	 As Ronald Fisher famously said: «\textit{A scientific fact should be regarded as experimentally established only if a properly designed experiment rarely fails to give this level of significance}». What this means is $p$-value less than $0.05$ (or as applicable) only tells you that one should repeat the experiment!
	 
	 It is true that the smaller the $p$-value, the more unusual the data would be if every single assumption were correct; but a very small $p$-value does not tell us which assumption is incorrect. For example, the $p$-value may be very small because the targeted hypothesis is false; but it may instead (or in addition) be very small because the study protocols were violated, or because it was selected for presentation based on its small size. Conversely, a large $p$-value indicates only that the data are not unusual under the model, but does not imply that the model or any aspect of it (such as the targeted hypothesis) is correct; it may instead (or in addition) be large because (again) the study protocols were violated, or because it was selected for presentation based on its large size!
	 
	 The general definition of a $p$-value may help one to understand why statistical tests tell us much less than what many think they do: Not only does a $p$-value not tell us whether the hypothesis targeted for testing is true or not; it says nothing specifically related to that hypothesis unless we can be completely assured that every other assumption used for its computation is correct, an assurance that is lacking in far too many studies.
	 
	 Especially keep in mind (because we have a natural cognitive bias about this...) that in general:
	
	But these two probabilities (likelihood on the left and plausibility on the right) are however linked by the Bayes theorem (\SeeChapter{see section Probabilities page \pageref{bayes formula}}):
	
	But the both terms $P(H_0)$ (prior probability of the observed phenomenon) and $P(\text{data})$ are in practice difficult or even impossible to know.
	
	For hypothesis testing, for example, the $5\%$ risk threshold $\alpha$ is the risk to reject the null hypothesis $H_0$ even though it is true. If the risk imposed/chosen is $5\%$ and the calculated $p$-value is less (in most tests but be careful because this is not a generality!!!), the test fails (rejection of the null hypotheses) in favour of an alternative hypothesis denoted by $H_1$ or $H_a$. Never forget that reject the test is always better in term of power than accepting it. In other words we can say that the $p$-value is the smallest $\alpha$ at which we would reject $H_0$.
	
	In a paper published the March 7, 12016 (holocene calendar) by the American Statistical Association we have a list of principles to improve the conduct and interpretation of quantitative science relatively to the usage of the $p$-value! The statement’s six principles, many of which address misconceptions and misuse of the $p$-
value, are the following:
	\begin{enumerate}
		\item $p$-values can indicate how incompatible the data are with a specified statistical model.
		
		\item $p$-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.
		
		\item Scientific conclusions and business or policy decisions should not be based only on whether a $p$-value passes a specific threshold.
		
		\item Proper inference requires full reporting and transparency.
		
		\item A $p$-value, or statistical significance, does not measure the size of an effect or the importance of a result.
		
		\item By itself, a $p$-value does not provide a good measure of evidence regarding a model or hypothesis.
	\end{enumerate}
	
	The alternative hypothesis $H_1$ has of course itself its own risk that we denote by $\beta$ and its own $p$-value. So when the null hypothesis is not rejected, the risk associated with this decision is a "\NewTerm{risk of the second kind}\index{risk of the second kind}". To assess this, we should calculate the power of the test considered (see proofs further below).
	
	Perhaps, to better understand, here is an illustration of a particular case of an  bilateral hypothesis test of the average for a typical random variable following a Normal distribution (basically it's almost the same principle for all tests...):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,578); %set diagram left start at 0, and has height of 578
		
		%Shape: Polygon Curved [id:ds4342003864561583] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][fill={rgb, 255:red, 254; green, 243; blue, 21 }  ,fill opacity=1 ] (285.6,24.46) .. controls (302.34,24.64) and (326.1,88.96) .. (334.85,108.71) .. controls (343.6,128.46) and (358.15,151.66) .. (373.35,154.94) .. controls (388.56,158.21) and (407.2,159.46) .. (407.06,159.46) .. controls (406.91,159.46) and (166.06,159.46) .. (165.86,159.28) .. controls (165.67,159.1) and (187.31,159.21) .. (200.31,153.21) .. controls (213.31,147.21) and (226.69,127.9) .. (235.06,109.46) .. controls (243.43,91.02) and (268.86,24.28) .. (285.6,24.46) -- cycle ;
		%Shape: Polygon Curved [id:ds5469066387497896] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][fill={rgb, 255:red, 248; green, 38; blue, 38 }  ,fill opacity=1 ] (225.85,159.28) .. controls (226.1,159.03) and (166.06,159.46) .. (165.86,159.28) .. controls (165.67,159.1) and (185.2,159.37) .. (200.31,153.21) .. controls (215.42,147.05) and (225.35,127.78) .. (225.6,127.53) .. controls (225.85,127.28) and (225.6,159.53) .. (225.85,159.28) -- cycle ;
		%Shape: Polygon Curved [id:ds02559745982606354] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][fill={rgb, 255:red, 248; green, 38; blue, 38 }  ,fill opacity=1 ] (345.56,129.16) .. controls (345.35,127.19) and (356.17,147.51) .. (368.35,152.44) .. controls (380.53,157.37) and (407.2,159.46) .. (407.06,159.46) .. controls (406.91,159.46) and (345.6,159.44) .. (345.6,158.94) .. controls (345.6,158.44) and (345.76,131.14) .. (345.56,129.16) -- cycle ;
		%Straight Lines [id:da722416651982738] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (285.76,19.16) -- (285.76,349.28) ;
		%Shape: Polygon Curved [id:ds9540489908010028] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ] (285.6,24.46) .. controls (302.34,24.64) and (326.1,88.96) .. (334.85,108.71) .. controls (343.6,128.46) and (358.66,151.1) .. (373.86,154.37) .. controls (389.07,157.64) and (407.2,159.46) .. (407.06,159.46) .. controls (406.91,159.46) and (166.06,159.46) .. (165.86,159.28) .. controls (165.67,159.1) and (187.31,159.21) .. (200.31,153.21) .. controls (213.31,147.21) and (226.69,127.9) .. (235.06,109.46) .. controls (243.43,91.02) and (268.86,24.28) .. (285.6,24.46) -- cycle ;
		%Shape: Axis 2D [id:dp15147243193453197] 
		\draw  (154.86,159.28) -- (407.86,159.28)(165.86,16) -- (165.86,167.28) (400.86,154.28) -- (407.86,159.28) -- (400.86,164.28) (160.86,23) -- (165.86,16) -- (170.86,23) (173.86,154.28) -- (173.86,164.28)(181.86,154.28) -- (181.86,164.28)(189.86,154.28) -- (189.86,164.28)(197.86,154.28) -- (197.86,164.28)(205.86,154.28) -- (205.86,164.28)(213.86,154.28) -- (213.86,164.28)(221.86,154.28) -- (221.86,164.28)(229.86,154.28) -- (229.86,164.28)(237.86,154.28) -- (237.86,164.28)(245.86,154.28) -- (245.86,164.28)(253.86,154.28) -- (253.86,164.28)(261.86,154.28) -- (261.86,164.28)(269.86,154.28) -- (269.86,164.28)(277.86,154.28) -- (277.86,164.28)(285.86,154.28) -- (285.86,164.28)(293.86,154.28) -- (293.86,164.28)(301.86,154.28) -- (301.86,164.28)(309.86,154.28) -- (309.86,164.28)(317.86,154.28) -- (317.86,164.28)(325.86,154.28) -- (325.86,164.28)(333.86,154.28) -- (333.86,164.28)(341.86,154.28) -- (341.86,164.28)(349.86,154.28) -- (349.86,164.28)(357.86,154.28) -- (357.86,164.28)(365.86,154.28) -- (365.86,164.28)(373.86,154.28) -- (373.86,164.28)(381.86,154.28) -- (381.86,164.28)(389.86,154.28) -- (389.86,164.28)(160.86,151.28) -- (170.86,151.28)(160.86,143.28) -- (170.86,143.28)(160.86,135.28) -- (170.86,135.28)(160.86,127.28) -- (170.86,127.28)(160.86,119.28) -- (170.86,119.28)(160.86,111.28) -- (170.86,111.28)(160.86,103.28) -- (170.86,103.28)(160.86,95.28) -- (170.86,95.28)(160.86,87.28) -- (170.86,87.28)(160.86,79.28) -- (170.86,79.28)(160.86,71.28) -- (170.86,71.28)(160.86,63.28) -- (170.86,63.28)(160.86,55.28) -- (170.86,55.28)(160.86,47.28) -- (170.86,47.28)(160.86,39.28) -- (170.86,39.28)(160.86,31.28) -- (170.86,31.28) ;
		\draw [color={rgb, 255:red, 74; green, 74; blue, 74 }  ,opacity=1 ]  ;
		%Straight Lines [id:da9473092815158279] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (225.6,127.53) -- (225.6,352.28) ;
		%Shape: Polygon Curved [id:ds6552965895141998] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][fill={rgb, 255:red, 254; green, 243; blue, 21 }  ,fill opacity=1 ] (369.6,211.46) .. controls (386.34,211.64) and (410.1,275.96) .. (418.85,295.71) .. controls (427.6,315.46) and (442.15,338.66) .. (457.35,341.94) .. controls (472.56,345.21) and (491.2,346.46) .. (491.06,346.46) .. controls (490.91,346.46) and (250.06,346.46) .. (249.86,346.28) .. controls (249.67,346.1) and (271.31,346.21) .. (284.31,340.21) .. controls (297.31,334.21) and (310.69,314.9) .. (319.06,296.46) .. controls (327.43,278.02) and (352.86,211.28) .. (369.6,211.46) -- cycle ;
		%Shape: Polygon Curved [id:ds9367090381465286] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][fill={rgb, 255:red, 248; green, 38; blue, 38 }  ,fill opacity=1 ] (345.94,346.28) .. controls (346.44,346.28) and (250.06,346.46) .. (249.86,346.28) .. controls (249.67,346.1) and (269.2,346.37) .. (284.31,340.21) .. controls (299.42,334.05) and (316.19,305.28) .. (323.19,287.78) .. controls (330.19,270.28) and (345.6,237.87) .. (345.69,237.78) .. controls (345.79,237.69) and (345.44,346.28) .. (345.94,346.28) -- cycle ;
		%Shape: Axis 2D [id:dp22758917727795813] 
		\draw  (154.86,346.28) -- (504.86,346.28)(165.8,203) -- (165.8,354.28) (497.86,341.28) -- (504.86,346.28) -- (497.86,351.28) (160.8,210) -- (165.8,203) -- (170.8,210) (173.8,341.28) -- (173.8,351.28)(181.8,341.28) -- (181.8,351.28)(189.8,341.28) -- (189.8,351.28)(197.8,341.28) -- (197.8,351.28)(205.8,341.28) -- (205.8,351.28)(213.8,341.28) -- (213.8,351.28)(221.8,341.28) -- (221.8,351.28)(229.8,341.28) -- (229.8,351.28)(237.8,341.28) -- (237.8,351.28)(245.8,341.28) -- (245.8,351.28)(253.8,341.28) -- (253.8,351.28)(261.8,341.28) -- (261.8,351.28)(269.8,341.28) -- (269.8,351.28)(277.8,341.28) -- (277.8,351.28)(285.8,341.28) -- (285.8,351.28)(293.8,341.28) -- (293.8,351.28)(301.8,341.28) -- (301.8,351.28)(309.8,341.28) -- (309.8,351.28)(317.8,341.28) -- (317.8,351.28)(325.8,341.28) -- (325.8,351.28)(333.8,341.28) -- (333.8,351.28)(341.8,341.28) -- (341.8,351.28)(349.8,341.28) -- (349.8,351.28)(357.8,341.28) -- (357.8,351.28)(365.8,341.28) -- (365.8,351.28)(373.8,341.28) -- (373.8,351.28)(381.8,341.28) -- (381.8,351.28)(389.8,341.28) -- (389.8,351.28)(397.8,341.28) -- (397.8,351.28)(405.8,341.28) -- (405.8,351.28)(413.8,341.28) -- (413.8,351.28)(421.8,341.28) -- (421.8,351.28)(429.8,341.28) -- (429.8,351.28)(437.8,341.28) -- (437.8,351.28)(445.8,341.28) -- (445.8,351.28)(453.8,341.28) -- (453.8,351.28)(461.8,341.28) -- (461.8,351.28)(469.8,341.28) -- (469.8,351.28)(477.8,341.28) -- (477.8,351.28)(485.8,341.28) -- (485.8,351.28)(160.8,338.28) -- (170.8,338.28)(160.8,330.28) -- (170.8,330.28)(160.8,322.28) -- (170.8,322.28)(160.8,314.28) -- (170.8,314.28)(160.8,306.28) -- (170.8,306.28)(160.8,298.28) -- (170.8,298.28)(160.8,290.28) -- (170.8,290.28)(160.8,282.28) -- (170.8,282.28)(160.8,274.28) -- (170.8,274.28)(160.8,266.28) -- (170.8,266.28)(160.8,258.28) -- (170.8,258.28)(160.8,250.28) -- (170.8,250.28)(160.8,242.28) -- (170.8,242.28)(160.8,234.28) -- (170.8,234.28)(160.8,226.28) -- (170.8,226.28)(160.8,218.28) -- (170.8,218.28) ;
		\draw [color={rgb, 255:red, 74; green, 74; blue, 74 }  ,opacity=1 ]  ;
		%Straight Lines [id:da7187920863329391] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (345.56,129.16) -- (345.56,351.28) ;
		%Straight Lines [id:da6964578965222128] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (369.56,147.16) -- (369.56,350.28) ;
		
		% Text Node
		\draw (142,123.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0.1$};
		% Text Node
		\draw (142,90.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0.2$};
		% Text Node
		\draw (143,51.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0.3$};
		% Text Node
		\draw (144,21.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0.4$};
		% Text Node
		\draw (179,165.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$-3$};
		% Text Node
		\draw (211,165.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$-2$};
		% Text Node
		\draw (244,165.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$-1$};
		% Text Node
		\draw (282,165.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0$};
		% Text Node
		\draw (314,165.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$1$};
		% Text Node
		\draw (346,165.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$2$};
		% Text Node
		\draw (378,165.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$3$};
		% Text Node
		\draw (182,126.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$\alpha /2$};
		% Text Node
		\draw (357,126.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$\alpha /2$};
		% Text Node
		\draw (330,48.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$ \begin{array}{l}
		H_{0} :\mu =0\\
		\alpha =\text{Type I Error}
		\end{array}$};
		% Text Node
		\draw (142,310.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0.1$};
		% Text Node
		\draw (142,277.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0.2$};
		% Text Node
		\draw (143,238.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0.3$};
		% Text Node
		\draw (144,208.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0.4$};
		% Text Node
		\draw (179,352.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$-3$};
		% Text Node
		\draw (211,352.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$-2$};
		% Text Node
		\draw (244,352.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$-1$};
		% Text Node
		\draw (282,352.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0$};
		% Text Node
		\draw (314,352.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$1$};
		% Text Node
		\draw (346,352.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$2$};
		% Text Node
		\draw (378,352.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$3$};
		% Text Node
		\draw (307,280.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$\beta $};
		% Text Node
		\draw (413,235.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$ \begin{array}{l}
		H_{1} :\mu =2.5\\
		\beta =\text{Type II Error}
		\end{array}$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Null and alternative hypothesis of a special case of two-sided test}
	\end{figure}
	Thus, in the case presented above, we see better why the null hypothesis can be accepted or rejected in favour of the alternative hypothesis (which is of the same law that the null hypothesis but just shifted) depending on the reference value measured that will be used for the test (in the special case of the above figure it is the arithmetic mean of measurements).
	
	We also note that the red area of the alternative hypothesis, corresponding to the cumulated probability $\beta$, is partially merged with the yellow part of the null hypothesis. This is why we can sometimes accept the null hypothesis wrongly. However, we see that smaller is $\beta$, more the alternative hypothesis would be far from the red boundary zone of the null hypothesis (this would correspond to a translation to the right in this case) and less the likelihood of a false conclusion is big. This is why we talk about "\NewTerm{risk $\beta$}\index{risk $\beta$}\index{$\beta$ risk}" because smaller it is, the better. Verbatim more $1-\beta$ is big, then smaller is the risk of confusing the null and alternative hypothesis. This is why $1-\beta$ is named "\NewTerm{power of the test}\index{power of the test}" (see below the subsection that is devoted to this concept).
	
	We (sadly) do not reject the null hypothesis if the $p$-value is by tradition greater than $5\%$ ($0.05$). In fact, bigger is the $p$-value, the better it is because for the people interested in the null hypothesis because the confidence interval is becoming smaller. If the confidence interval is becoming huge (very close to $100\%$) because the $p$-value is very small then the analysis does not really make sense physically speaking anymore in the point of view of the null hypothesis!
	
	Thus, if the $p$-value is small, that mean we should take a low risk $\alpha$ of error, therefore accept (i.e. do not reject) in almost all cases the tested hypothesis ($H_0$)...
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} We should never say that we "accept" a hypothesis or that it is "true" or "false" as these terms are too strong and might suggest a scientific proof. We should say if we "reject" or "does not reject" the null hypothesis and it may be possibly "correct" or "incorrect".\\
	
	\textbf{R2.} For bilateral test hypotheses, we can for example say that we have (or not have) a significant difference between the measured reference value and the expected value. For one-sided tailed, we can say that the measured reference value is significantly bigger or smaller than the expected value.\\
	
	\textbf{R3.} Moreover if the reader has well understand the construction of hypothesis testing, the fact of wrongly reject a hypothesis ("\NewTerm{Type I error}\index{type I error}" or "\NewTerm{Error of the first kind}\index{error of the first kind}") is more robust than accept it wrongly ("\NewTerm{Type II error}\index{type II error}" or "\NewTerm{Error of the second kind}\index{error of the second kind}").\\
	
	\textbf{R4.} The reader will also have noticed with the help of the previous figure that a one sided test has a higher power than a one-sided test (for a same risk threshold of course!). Thus, a statistically insignificant difference in bilateral test, can be statistically significant in a one sided test.\\
	
	\textbf{R5.} If the $p$-value is close to the limit threshold (rejection) value we say that the "\NewTerm{effect is marginally significant}\index{marginally significant effect}".
	\end{tcolorbox}	
	
	A big issue with hypothesis testing\footnote{A second one is the "significance filter curse" quite well documented in \cite{van2021significance}.} is what we name "\NewTerm{$p$-hacking}\index{$p$-hacking}": the underlying idea consists of replicating a test dozens of times until it provides the conclusion that the experimenter wants... or try all possible combinations of variables until we found something significant or even just take big samples (as we know that NHST tests fail systemically when samples are big). 
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/arithmetics/p_hacking.jpg}
		\caption[$p$-hacking]{$p$-hacking Open Science Collaboration, \textit{Estimating the reproducibility of physiological science}, Science, 349 (12015)}
	\end{figure}
	$p$-hacking is generally thought of as cheating, but what if we made it compulsory instead? If the purpose of studies is to push the frontiers of knowledge, then perhaps playing around with different methods shouldn't be thought of as a dirty trick, but encouraged as a way of exploring boundaries. A recent project spearheaded by Brian Nosek, a founder of the non-profit Center for Open Science, offered a clever way to do this.

	Nosek's team invited researchers to take part in a crowd-sourcing data analysis project. The setup was simple. Participants were all given the same data set and prompt: Do soccer referees give more red cards to dark-skinned players than light-skinned ones? They were then asked to submit their analytical approach for feedback from other teams before diving into the analysis.

	Twenty-nine teams with a total of $61$ analysts took part. The researchers used a wide variety of methods, ranging - for those of you interested in the methodological gore - from simple linear regression techniques to complex multilevel regressions and Bayesian approaches. They also made different decisions about which secondary variables to use in their analyses.

	Despite analysing the same data, the researchers got a variety of results. Twenty teams concluded that soccer referees gave more red cards to dark-skinned players, and nine teams found no significant relationship between skin color and red cards.
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/arithmetics/repetability.jpg}
	\end{figure}
	The variability in results wasn't due to fraud or sloppy work. These people are assumed to be highly competent analysts who were motivated to find the evidence, said Eric Luis Uhlmann, a psychologist at the Insead business school in Singapore and one of the project leaders. Even the most skilled researchers must make subjective choices that have a huge impact on the result they find.

	But these disparate results don't mean that studies can't inch us toward strong evidence. The important lesson here is that a single analysis is not sufficient to find a definitive answer. 
	
	However generally the reader must keep in mind that if there are multiple statistical tests that apply for exactly the same thing and work differently (quite common in statistics like the multiple homogeneity of variance tests, the multiple normality tests, the multiple unit root tests, the multiple stationarity tests, etc.), then a single significant result leads to rejection!
	
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Caution! Furthermore statistical significance has literally \underline{zero} connection with "\NewTerm{practical significance}\index{practical significance}" also named "\NewTerm{scientific importance}\index{scientific importance}" or "\NewTerm{relevance}". It will never tell you if something is important or not! Indeed, it's not an indicator knowing the context! That a null hypothesis is reject or not only means that the hypothesis test has not met the pre-set significance level $\alpha$ and not that the results are significant or not significant for practical applications (they very likely are to the scientist spending nights in the lab collecting the data)! The test statistics typically just looks at the effect (difference, ratio, covariance, etc) and the variance. In other words, it just looks if the signal is visible enough on the background of the noise. It takes the current condition - sample size - into consideration. More data means that smaller effects will be indicated as statistically significant.
	\end{tcolorbox}
	
	Keep in mind that as we said in the introduction: What makes science so powerful is that it's self-correcting. Sure, false findings get published, but eventually new studies come along to overturn them, and new evidence is revealed. At least, that's how it's supposed to work. But scientific publishing doesn't have a great track record when it comes to self-correction, but this is changing with new technologies that facilitates such tracking with computer databases.
	
	So the scientific method is still a posteriori far away the most reliable method (less worst) that any other way of knowing that we have TODAY (but it's not an argument however to not try to find a better one)!
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	By the way... the reader can try to remember an adage named after economist Charles Goodhart, which has been phrased by Marilyn Strathern as: «\textit{When a measure becomes a target, it ceases to be a good measure.}» But the original formulation was: «\textit{Any observed statistical regularity will tend to collapse once pressure is placed upon it for control purposes.}»
	\end{tcolorbox}	

	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] The "\NewTerm{probability $\alpha$ of Type I error}" (error of the first kind / false negative\footnote{Notice that depending on the point of view of the scientist, a "false negative" can be named "false positive", this is why many textbooks avoid using this specific vocabulary!}) is the probability of rejecting the null hypothesis when it is likely to be true\footnote{Please keep in mind that as already mentioned in the introduction of this book, we are speaking here not about the "philosophical truth", but the "truth" as the best explanation at our actual level of knowledge!!!}.
		
		\item[D2.] The "\NewTerm{probability $\beta$ of Type II error}" (error of the second kind / false positive) is the probability of maintaining the null hypothesis when it is likely to be false. Since, by definition, power is equal to one $1-\beta$, the power of a test will get smaller as beta gets bigger.
	\end{enumerate}
	\begin{center}
	  \begin{tabular}{|l|c|c|c|}
	  \hline
	    \cellcolor[gray]{0.75}   & \multicolumn{2}{|c|}{\cellcolor[gray]{0.75}\textbf{State of Nature}} \\ \hline
	\cellcolor[gray]{0.75}\textbf{Test outcome} & $H_0$ \textbf{false} & $H_0$ \textbf{true} \\ \hline
	\textbf{Reject }$H_0$ & \cellcolor{green!30}\parbox{5.5cm}{Correctly rejected null hypothesis\\ \centering($1-\beta$: power of the test)} & \cellcolor{red!30}\parbox{5cm}{Type I Error (false negative)\\ \centering(Risk $\alpha$)} \\[3ex] \hline
	\textbf{Fail to reject }$H_0$ & \cellcolor{red!30}\parbox{5cm}{Type II Error (false positive)\\ \centering(Risk $\beta$)} & \cellcolor{green!30}\parbox{5.5cm}{\centering Correctly accepted null hypothesis\\ \centering ($1-\alpha$: confidence level)}  \\[3ex] \hline
	  \end{tabular}
	\end{center}
	Or in a more mathematical form:
	\begin{center}
	  \begin{tabular}{|l|c|c|c|}
	  \hline
	    \cellcolor[gray]{0.75}   & \multicolumn{2}{|c|}{\cellcolor[gray]{0.75}\textbf{State of Nature}} \\ \hline
	\cellcolor[gray]{0.75}\textbf{Test outcome} & $H_0$ \textbf{false} & $H_0$ \textbf{true} \\ \hline
	\textbf{Reject }$H_0$ & \cellcolor{green!30}\parbox{5.5cm}{Correctly rejected null hypothesis\\ \centering $P(\text{reject } H_0|H_0 \text{ is false})=1-\beta$} & \cellcolor{red!30}\parbox{5cm}{Type I Error (false negative)\\ \centering $P(\text{reject } H_0|H_0 \text{ is true})=\alpha$} \\[3ex] \hline
	\textbf{Fail to reject }$H_0$ & \cellcolor{red!30}\parbox{5cm}{Type II Error (false positive)\\ \centering $P(\text{fail reject } H_0|H_0 \text{ is false})=\beta$} & \cellcolor{green!30}\parbox{5.5cm}{\centering Correctly accepted null hypothesis\\ \centering $P(\text{fail reject } H_0|H_0 \text{ is true})=1-\alpha$}  \\[3ex] \hline
	  \end{tabular}
	\end{center}
	Thus a traditional criteria selection is to use the following principle: among all the tests that have the same size of Type I error, choose one that has the smallest size of the value of the Type II error.
	
	Thus, it can be said that a hypothesis test simply has for purpose to minimize $\alpha$ the error of Type I and, on the contrary, to maximize $1-\beta$, the power of the test.
	
	In general, the magnitude of the Type II error increases when that of the Type I error decreases. We can not, as far as we know, minimize the two errors at the same time. For this reason, we often take a fixed value of $\alpha$, size of the type I error, and we minimize $\beta$, magnitude of the Type II error (i.e. we increase the power $1-\beta$).
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In 12017 (holocene calendar) the false negative rate in the general U.S.A. population for HIV test is around $0.003\%$, or three times out of every $100,000$ tests.\\

False positive rates are far lower, between $0.0004\%$ and $0.0007\%$ due to the practice by which the initial positive result is confirmed with a secondary test.\\ 

Other example, false negatives occur in $1$ of every $2,500$ women screened (NHS Breast Screening Programme – \textit{Helping you decide} leaflet of July 12013 according to holocene calendar) and occur more often in younger women than in older women because the breast tissue of younger women is denser.
	\end{tcolorbox}
	To close this subject, here are the three types of situations testing hypotheses on the statistics that is the average in the framework of an underlying Normal distribution and whose mean is in this particular case zero and of unit variance (because we can often come back to this particular case by centering and reducing the underlying random variable or by doing special transformations):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1138); %set diagram left start at 0, and has height of 1138
		
		%Shape: Polygon Curved [id:ds4342003864561583] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][fill={rgb, 255:red, 254; green, 243; blue, 21 }  ,fill opacity=1 ] (287.6,518.46) .. controls (304.34,518.64) and (328.1,582.96) .. (336.85,602.71) .. controls (345.6,622.46) and (360.15,645.66) .. (375.35,648.94) .. controls (390.56,652.21) and (409.2,653.46) .. (409.06,653.46) .. controls (408.91,653.46) and (168.06,653.46) .. (167.86,653.28) .. controls (167.67,653.1) and (189.31,653.21) .. (202.31,647.21) .. controls (215.31,641.21) and (228.69,621.9) .. (237.06,603.46) .. controls (245.43,585.02) and (270.86,518.28) .. (287.6,518.46) -- cycle ;
		%Shape: Polygon Curved [id:ds5469066387497896] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][fill={rgb, 255:red, 248; green, 38; blue, 38 }  ,fill opacity=1 ] (227.85,653.28) .. controls (228.1,653.03) and (168.06,653.46) .. (167.86,653.28) .. controls (167.67,653.1) and (187.2,653.37) .. (202.31,647.21) .. controls (217.42,641.05) and (227.35,621.78) .. (227.6,621.53) .. controls (227.85,621.28) and (227.6,653.53) .. (227.85,653.28) -- cycle ;
		%Shape: Polygon Curved [id:ds02559745982606354] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][fill={rgb, 255:red, 248; green, 38; blue, 38 }  ,fill opacity=1 ] (347.56,623.16) .. controls (347.35,621.19) and (358.17,641.51) .. (370.35,646.44) .. controls (382.53,651.37) and (409.2,653.46) .. (409.06,653.46) .. controls (408.91,653.46) and (347.6,653.44) .. (347.6,652.94) .. controls (347.6,652.44) and (347.76,625.14) .. (347.56,623.16) -- cycle ;
		%Shape: Polygon Curved [id:ds9540489908010028] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ] (287.6,518.46) .. controls (304.34,518.64) and (328.1,582.96) .. (336.85,602.71) .. controls (345.6,622.46) and (360.66,645.1) .. (375.86,648.37) .. controls (391.07,651.64) and (409.2,653.46) .. (409.06,653.46) .. controls (408.91,653.46) and (168.06,653.46) .. (167.86,653.28) .. controls (167.67,653.1) and (189.31,653.21) .. (202.31,647.21) .. controls (215.31,641.21) and (228.69,621.9) .. (237.06,603.46) .. controls (245.43,585.02) and (270.86,518.28) .. (287.6,518.46) -- cycle ;
		%Shape: Axis 2D [id:dp15147243193453197] 
		\draw  (156.86,653.28) -- (409.86,653.28)(167.86,510) -- (167.86,661.28) (402.86,648.28) -- (409.86,653.28) -- (402.86,658.28) (162.86,517) -- (167.86,510) -- (172.86,517) (175.86,648.28) -- (175.86,658.28)(183.86,648.28) -- (183.86,658.28)(191.86,648.28) -- (191.86,658.28)(199.86,648.28) -- (199.86,658.28)(207.86,648.28) -- (207.86,658.28)(215.86,648.28) -- (215.86,658.28)(223.86,648.28) -- (223.86,658.28)(231.86,648.28) -- (231.86,658.28)(239.86,648.28) -- (239.86,658.28)(247.86,648.28) -- (247.86,658.28)(255.86,648.28) -- (255.86,658.28)(263.86,648.28) -- (263.86,658.28)(271.86,648.28) -- (271.86,658.28)(279.86,648.28) -- (279.86,658.28)(287.86,648.28) -- (287.86,658.28)(295.86,648.28) -- (295.86,658.28)(303.86,648.28) -- (303.86,658.28)(311.86,648.28) -- (311.86,658.28)(319.86,648.28) -- (319.86,658.28)(327.86,648.28) -- (327.86,658.28)(335.86,648.28) -- (335.86,658.28)(343.86,648.28) -- (343.86,658.28)(351.86,648.28) -- (351.86,658.28)(359.86,648.28) -- (359.86,658.28)(367.86,648.28) -- (367.86,658.28)(375.86,648.28) -- (375.86,658.28)(383.86,648.28) -- (383.86,658.28)(391.86,648.28) -- (391.86,658.28)(162.86,645.28) -- (172.86,645.28)(162.86,637.28) -- (172.86,637.28)(162.86,629.28) -- (172.86,629.28)(162.86,621.28) -- (172.86,621.28)(162.86,613.28) -- (172.86,613.28)(162.86,605.28) -- (172.86,605.28)(162.86,597.28) -- (172.86,597.28)(162.86,589.28) -- (172.86,589.28)(162.86,581.28) -- (172.86,581.28)(162.86,573.28) -- (172.86,573.28)(162.86,565.28) -- (172.86,565.28)(162.86,557.28) -- (172.86,557.28)(162.86,549.28) -- (172.86,549.28)(162.86,541.28) -- (172.86,541.28)(162.86,533.28) -- (172.86,533.28)(162.86,525.28) -- (172.86,525.28) ;
		\draw [color={rgb, 255:red, 74; green, 74; blue, 74 }  ,opacity=1 ]  ;
		%Shape: Polygon Curved [id:ds541660649658191] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][fill={rgb, 255:red, 254; green, 243; blue, 21 }  ,fill opacity=1 ] (287.6,33.46) .. controls (304.34,33.64) and (328.1,97.96) .. (336.85,117.71) .. controls (345.6,137.46) and (360.15,160.66) .. (375.35,163.94) .. controls (390.56,167.21) and (409.2,168.46) .. (409.06,168.46) .. controls (408.91,168.46) and (168.06,168.46) .. (167.86,168.28) .. controls (167.67,168.1) and (189.31,168.21) .. (202.31,162.21) .. controls (215.31,156.21) and (228.69,136.9) .. (237.06,118.46) .. controls (245.43,100.02) and (270.86,33.28) .. (287.6,33.46) -- cycle ;
		%Shape: Polygon Curved [id:ds9406432447681619] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][fill={rgb, 255:red, 248; green, 38; blue, 38 }  ,fill opacity=1 ] (347.56,138.16) .. controls (347.35,136.19) and (358.17,156.51) .. (370.35,161.44) .. controls (382.53,166.37) and (409.2,168.46) .. (409.06,168.46) .. controls (408.91,168.46) and (347.6,168.44) .. (347.6,167.94) .. controls (347.6,167.44) and (347.76,140.14) .. (347.56,138.16) -- cycle ;
		%Straight Lines [id:da2882749670459288] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (287.76,16.16) -- (287.76,199.19) ;
		%Shape: Polygon Curved [id:ds38376734929501755] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ] (287.6,33.46) .. controls (304.34,33.64) and (328.1,97.96) .. (336.85,117.71) .. controls (345.6,137.46) and (360.66,160.1) .. (375.86,163.37) .. controls (391.07,166.64) and (409.2,168.46) .. (409.06,168.46) .. controls (408.91,168.46) and (168.06,168.46) .. (167.86,168.28) .. controls (167.67,168.1) and (189.31,168.21) .. (202.31,162.21) .. controls (215.31,156.21) and (228.69,136.9) .. (237.06,118.46) .. controls (245.43,100.02) and (270.86,33.28) .. (287.6,33.46) -- cycle ;
		%Shape: Axis 2D [id:dp12828767818714315] 
		\draw  (156.86,168.28) -- (409.86,168.28)(167.86,25) -- (167.86,176.28) (402.86,163.28) -- (409.86,168.28) -- (402.86,173.28) (162.86,32) -- (167.86,25) -- (172.86,32) (175.86,163.28) -- (175.86,173.28)(183.86,163.28) -- (183.86,173.28)(191.86,163.28) -- (191.86,173.28)(199.86,163.28) -- (199.86,173.28)(207.86,163.28) -- (207.86,173.28)(215.86,163.28) -- (215.86,173.28)(223.86,163.28) -- (223.86,173.28)(231.86,163.28) -- (231.86,173.28)(239.86,163.28) -- (239.86,173.28)(247.86,163.28) -- (247.86,173.28)(255.86,163.28) -- (255.86,173.28)(263.86,163.28) -- (263.86,173.28)(271.86,163.28) -- (271.86,173.28)(279.86,163.28) -- (279.86,173.28)(287.86,163.28) -- (287.86,173.28)(295.86,163.28) -- (295.86,173.28)(303.86,163.28) -- (303.86,173.28)(311.86,163.28) -- (311.86,173.28)(319.86,163.28) -- (319.86,173.28)(327.86,163.28) -- (327.86,173.28)(335.86,163.28) -- (335.86,173.28)(343.86,163.28) -- (343.86,173.28)(351.86,163.28) -- (351.86,173.28)(359.86,163.28) -- (359.86,173.28)(367.86,163.28) -- (367.86,173.28)(375.86,163.28) -- (375.86,173.28)(383.86,163.28) -- (383.86,173.28)(391.86,163.28) -- (391.86,173.28)(162.86,160.28) -- (172.86,160.28)(162.86,152.28) -- (172.86,152.28)(162.86,144.28) -- (172.86,144.28)(162.86,136.28) -- (172.86,136.28)(162.86,128.28) -- (172.86,128.28)(162.86,120.28) -- (172.86,120.28)(162.86,112.28) -- (172.86,112.28)(162.86,104.28) -- (172.86,104.28)(162.86,96.28) -- (172.86,96.28)(162.86,88.28) -- (172.86,88.28)(162.86,80.28) -- (172.86,80.28)(162.86,72.28) -- (172.86,72.28)(162.86,64.28) -- (172.86,64.28)(162.86,56.28) -- (172.86,56.28)(162.86,48.28) -- (172.86,48.28)(162.86,40.28) -- (172.86,40.28) ;
		\draw [color={rgb, 255:red, 74; green, 74; blue, 74 }  ,opacity=1 ]  ;
		%Straight Lines [id:da7131931819186095] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (347.56,70.16) -- (347.56,200.19) ;
		%Straight Lines [id:da7697920142413504] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (171,73) -- (344.86,73) ;
		\draw [shift={(346.86,73)}, rotate = 180] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(169,73)}, rotate = 0] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da8394972399150666] 
		\draw [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ][fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ]   (348.86,73) -- (417.86,73) ;
		\draw [shift={(419.86,73)}, rotate = 180] [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(346.86,73)}, rotate = 0] [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Polygon Curved [id:ds9473191747474492] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][fill={rgb, 255:red, 254; green, 243; blue, 21 }  ,fill opacity=1 ] (287.6,284.55) .. controls (304.34,284.73) and (328.1,349.05) .. (336.85,368.8) .. controls (345.6,388.55) and (360.15,411.75) .. (375.35,415.03) .. controls (390.56,418.3) and (409.2,419.55) .. (409.06,419.55) .. controls (408.91,419.55) and (168.06,419.55) .. (167.86,419.37) .. controls (167.67,419.18) and (189.31,419.3) .. (202.31,413.3) .. controls (215.31,407.3) and (228.69,387.99) .. (237.06,369.55) .. controls (245.43,351.11) and (270.86,284.37) .. (287.6,284.55) -- cycle ;
		%Shape: Polygon Curved [id:ds48745215102958706] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][fill={rgb, 255:red, 248; green, 38; blue, 38 }  ,fill opacity=1 ] (227.85,419.37) .. controls (228.1,419.12) and (168.06,419.55) .. (167.86,419.37) .. controls (167.67,419.18) and (187.2,419.46) .. (202.31,413.3) .. controls (217.42,407.14) and (227.35,387.87) .. (227.6,387.62) .. controls (227.85,387.37) and (227.6,419.62) .. (227.85,419.37) -- cycle ;
		%Shape: Polygon Curved [id:ds8269344218916304] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ] (287.6,284.55) .. controls (304.34,284.73) and (328.1,349.05) .. (336.85,368.8) .. controls (345.6,388.55) and (360.66,411.18) .. (375.86,414.46) .. controls (391.07,417.73) and (409.2,419.55) .. (409.06,419.55) .. controls (408.91,419.55) and (168.06,419.55) .. (167.86,419.37) .. controls (167.67,419.18) and (189.31,419.3) .. (202.31,413.3) .. controls (215.31,407.3) and (228.69,387.99) .. (237.06,369.55) .. controls (245.43,351.11) and (270.86,284.37) .. (287.6,284.55) -- cycle ;
		%Shape: Axis 2D [id:dp7896593885202665] 
		\draw  (156.86,419.37) -- (409.86,419.37)(167.86,276.09) -- (167.86,427.37) (402.86,414.37) -- (409.86,419.37) -- (402.86,424.37) (162.86,283.09) -- (167.86,276.09) -- (172.86,283.09) (175.86,414.37) -- (175.86,424.37)(183.86,414.37) -- (183.86,424.37)(191.86,414.37) -- (191.86,424.37)(199.86,414.37) -- (199.86,424.37)(207.86,414.37) -- (207.86,424.37)(215.86,414.37) -- (215.86,424.37)(223.86,414.37) -- (223.86,424.37)(231.86,414.37) -- (231.86,424.37)(239.86,414.37) -- (239.86,424.37)(247.86,414.37) -- (247.86,424.37)(255.86,414.37) -- (255.86,424.37)(263.86,414.37) -- (263.86,424.37)(271.86,414.37) -- (271.86,424.37)(279.86,414.37) -- (279.86,424.37)(287.86,414.37) -- (287.86,424.37)(295.86,414.37) -- (295.86,424.37)(303.86,414.37) -- (303.86,424.37)(311.86,414.37) -- (311.86,424.37)(319.86,414.37) -- (319.86,424.37)(327.86,414.37) -- (327.86,424.37)(335.86,414.37) -- (335.86,424.37)(343.86,414.37) -- (343.86,424.37)(351.86,414.37) -- (351.86,424.37)(359.86,414.37) -- (359.86,424.37)(367.86,414.37) -- (367.86,424.37)(375.86,414.37) -- (375.86,424.37)(383.86,414.37) -- (383.86,424.37)(391.86,414.37) -- (391.86,424.37)(162.86,411.37) -- (172.86,411.37)(162.86,403.37) -- (172.86,403.37)(162.86,395.37) -- (172.86,395.37)(162.86,387.37) -- (172.86,387.37)(162.86,379.37) -- (172.86,379.37)(162.86,371.37) -- (172.86,371.37)(162.86,363.37) -- (172.86,363.37)(162.86,355.37) -- (172.86,355.37)(162.86,347.37) -- (172.86,347.37)(162.86,339.37) -- (172.86,339.37)(162.86,331.37) -- (172.86,331.37)(162.86,323.37) -- (172.86,323.37)(162.86,315.37) -- (172.86,315.37)(162.86,307.37) -- (172.86,307.37)(162.86,299.37) -- (172.86,299.37)(162.86,291.37) -- (172.86,291.37) ;
		\draw [color={rgb, 255:red, 74; green, 74; blue, 74 }  ,opacity=1 ]  ;
		%Straight Lines [id:da6600624691340156] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (287.76,258.16) -- (287.76,441.19) ;
		%Straight Lines [id:da8613140939322748] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (227.56,306.16) -- (227.56,442.16) ;
		%Straight Lines [id:da575707655887354] 
		\draw [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ]   (170,324) -- (224.86,324) ;
		\draw [shift={(226.86,324)}, rotate = 180] [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(168,324)}, rotate = 0] [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da23158833432681347] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (228.86,324) -- (402.73,324) ;
		\draw [shift={(404.73,324)}, rotate = 180] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(226.86,324)}, rotate = 0] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da910289075422966] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (287.76,501.16) -- (287.76,684.19) ;
		%Straight Lines [id:da4411828504409889] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (227.56,543.16) -- (227.56,679.16) ;
		%Straight Lines [id:da8863115134824593] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (348.56,543.16) -- (348.56,679.16) ;
		%Straight Lines [id:da23865649814741108] 
		\draw [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ]   (171,558) -- (225.86,558) ;
		\draw [shift={(227.86,558)}, rotate = 180] [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(169,558)}, rotate = 0] [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da6731970295482401] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (229.86,558) -- (344.86,558) ;
		\draw [shift={(346.86,558)}, rotate = 180] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(227.86,558)}, rotate = 0] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da027212055596960827] 
		\draw [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ]   (348.86,558) -- (403.73,558) ;
		\draw [shift={(405.73,558)}, rotate = 180] [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(346.86,558)}, rotate = 0] [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		
		% Text Node
		\draw (144,617.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0.1$};
		% Text Node
		\draw (144,584.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0.2$};
		% Text Node
		\draw (145,545.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0.3$};
		% Text Node
		\draw (146,515.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0.4$};
		% Text Node
		\draw (181,659.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$-3$};
		% Text Node
		\draw (213,659.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$-2$};
		% Text Node
		\draw (246,659.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$-1$};
		% Text Node
		\draw (284,659.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0$};
		% Text Node
		\draw (316,659.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$1$};
		% Text Node
		\draw (348,659.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$2$};
		% Text Node
		\draw (380,659.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$3$};
		% Text Node
		\draw (184,621.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$\alpha /2$};
		% Text Node
		\draw (359,621.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$\alpha /2$};
		% Text Node
		\draw (144,132.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0.1$};
		% Text Node
		\draw (144,99.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0.2$};
		% Text Node
		\draw (145,60.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0.3$};
		% Text Node
		\draw (146,30.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0.4$};
		% Text Node
		\draw (181,174.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$-3$};
		% Text Node
		\draw (213,174.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$-2$};
		% Text Node
		\draw (246,174.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$-1$};
		% Text Node
		\draw (284,174.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0$};
		% Text Node
		\draw (316,174.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$1$};
		% Text Node
		\draw (348,174.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$2$};
		% Text Node
		\draw (380,174.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$3$};
		% Text Node
		\draw (359,136.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$\alpha $};
		% Text Node
		\draw (185,43.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize,color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ]  {$ \begin{array}{l}
		H_{0} :\mu =0\\
		\text{"Acceptation" area}
		\end{array}$};
		% Text Node
		\draw (145,8.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$f( x)$};
		% Text Node
		\draw (411.06,171.86) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$Z$};
		% Text Node
		\draw (328,202.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$|Z_{\text{critical}} |$};
		% Text Node
		\draw (255,5.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$F( x) =50\%$};
		% Text Node
		\draw (351,39.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize,color={rgb, 255:red, 126; green, 211; blue, 33 }  ,opacity=1 ]  {$ \begin{array}{l}
		H_{1} :\mu  >0\\
		\text{Rejection area}
		\end{array}$};
		% Text Node
		\draw (144,383.49) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0.1$};
		% Text Node
		\draw (144,350.49) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0.2$};
		% Text Node
		\draw (145,311.49) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0.3$};
		% Text Node
		\draw (146,281.49) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0.4$};
		% Text Node
		\draw (184,387.49) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$\alpha $};
		% Text Node
		\draw (254,244.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$F( x) =50\%$};
		% Text Node
		\draw (145,258.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$f( x)$};
		% Text Node
		\draw (411.06,422.95) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$Z$};
		% Text Node
		\draw (199,444.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$-|Z_{\text{critical}} |$};
		% Text Node
		\draw (174,289.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize,color={rgb, 255:red, 126; green, 211; blue, 33 }  ,opacity=1 ]  {$ \begin{array}{l}
		H_{1} :\mu < 0\\
		\text{Rejection area}
		\end{array}$};
		% Text Node
		\draw (312,289.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize,color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ]  {$ \begin{array}{l}
		H_{0} :\mu =0\\
		\text{"Acceptation" area}
		\end{array}$};
		% Text Node
		\draw (254,487.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$F( x) =50\%$};
		% Text Node
		\draw (145,492.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$f( x)$};
		% Text Node
		\draw (411.06,656.86) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$Z$};
		% Text Node
		\draw (199,682.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$-|Z_{\text{critical}} |$};
		% Text Node
		\draw (377,243.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$|Z_{\text{critical}} |$};
		% Text Node
		\draw (328,682.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$|Z_{\text{critical}} |$};
		% Text Node
		\draw (174,521.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize,color={rgb, 255:red, 126; green, 211; blue, 33 }  ,opacity=1 ]  {$ \begin{array}{l}
		H_{1} :\mu \neq 0\\
		\text{Rejection area}
		\end{array}$};
		% Text Node
		\draw (352,521.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize,color={rgb, 255:red, 126; green, 211; blue, 33 }  ,opacity=1 ]  {$ \begin{array}{l}
		H_{1} :\mu \neq 0\\
		\text{Rejection area}
		\end{array}$};
		% Text Node
		\draw (244,521.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize,color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ]  {$ \begin{array}{l}
		H_{0} :\mu =0\\
		\text{"Acceptation" area}
		\end{array}$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{The three possible scenarios for a hypothesis test on the average}
	\end{figure}
	Let us indicate that it makes no sense (as opposed to what we can sometimes read in some paper or electronic media) to have the following null hypotheses in the special case shown above:
	
	with the alternative hypothesis that automatically follows (we did not write it because it is useless and obvious). The reason is simple: how could you position your reduced centered Normal distribution if the mean is not fixed ... ??? This is why the null hypothesis in the context of the tests on the mean (and some other tests) are always written with an equality!
	
	Obviously the reader has very likely noticed that:
	\begin{itemize}
		\item If the confidence interval contains the parameter of interest relatively to $H_0$ the test is not significant
	
		\item If the confidence interval excluded the parameter of interest relatively to $H_0$ the test is significant
	\end{itemize}
	
	To summarize, we can say that if we make a decision, we can make an error and it is best not to make mistakes often. Clearly, the probability of saying something stupid must be known and preferably small.
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/arithmetics/null_hypothesis.jpg}
		\caption[Comics illustrating null and alternative hypothesis]{Comics illustrating Null and Alternative hypothesis (author: ?)}
	\end{figure}
	But keep in mind that a better understanding of the $p$-value will not take away the human impulse (cognitive bias) to use statistics to create an impossible level of confidence. Most people want something that they can't really get, and that they don't really understand because of a lack of education and critical thinking. They want certainty! And even if you try to explain them you will have a backfire effect as they suffer from the anti-intellectualism syndrome.
	
	\subsubsection{Fisher's method for multiple $p$-values}
	In statistics, "\NewTerm{Fisher's method}\index{Fisher's method}, also known as "\NewTerm{Fisher's combined probability test}\index{Fisher's combined probability test}", is a technique for data fusion or "meta-analysis" (analysis of analyses). It was developed by and named for Ronald Fisher. In its basic form, it is used to combine the results from several independent tests bearing upon the same overall hypothesis ($H_0$).

	Consider a set of $k$ independent tests, each of these to test a certain null hypothesis $H_{0|i}, i=\{1, 2, \ldots, k\}$. For each test, a significance level $p_{i}$, i.e., a $p$-value, is obtained. All these $p$-values can be combined into a joint test whether there is a global effect, i.e., if a global null hypothesis $H_0$ can be rejected.

	There are a number of ways to combine these independent, partial tests. The Fisher method is one of these, and is perhaps the most famous and most widely used. The test was presented in Fisher's now classical book, \textit{Statistical Methods for Research Workers} (11932 according to holocene calendar).

	The test is based on the fact that the probability of rejecting the global null hypothesis is related to intersection of the probabilities of each individual test, $\prod_i p_i$. However, $\prod_i p_i$ is not uniformly distributed (but each $p_i$ is assumed to be uniformly distributed!), even if the null is true for all partial tests, and cannot be used itself as the joint significance level for the global test. To remediate this fact, some interesting properties and relationships among distributions of random variables were exploited by Fisher and embodied in the succinct excerpt above. The proof is given below:
	\begin{dem}
	So we are looking for the distribution law of:
	
	but this seems quite difficult to do as we are dealing with a product. Then we take first logarithm (the natural one as we know in statistics that most of time in is the $\ln$ that appears:
	
	Therefore we have now a sum of the natural logarithm of uniform distribution $\mathcal{U}_{0,1}$. It's a bit better but now we have the problem of the $\ln\left(\mathcal{U}_{0,1}\right)$.

	But now let us see something nice! Remember that the cumulative distribution function of an exponential distribution is:
	
	The inverse is then given by:
	
	If $P$ is a random variable uniformly distributed in the interval $[0,1]$, so is $1-P$. As consequence the previous relation can be equivalently written as:
	
	where once again: $P=\mathcal{U}_{0,1}$. Therefore we have just found that the natural logarithm of a uniform random variable in the interval $[0,1]$ follows an exponential distribution with parameter $\lambda=1$!

	But now we have a small a problem. As we have a sum of exponential laws we have never proved if the exponential law is stable by addition...
	
	Let us now recall that the chi-squared distribution with $k$ degrees of freedom is given by:
	
	If $k=2$ we get:
	
	In other words, chi-squared distribution with two degrees of freedom is equal to a exponential distribution with $\lambda=1/2$!
	
	And remember now during our study of the Gamma distribution we have proved that it was stable by addition. And as we have proved that the chi-square distribution follows from a special case of the Gamma distribution, and just now that the exponential distribution follows from a special case of the chi-squared distribution, then that latter is also stable by addition with degrees of freedom!!!!
	
	Therefore multiplying the sum of logarithms by $-2$ to get rid of the $-$ coming from the exponential law and the $1/2$ coming from what bounds the chi-squared law to the exponential law, we get:
	
	from which a $p$-value for the global hypothesis can be easily obtained.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Under Fisher's method, two small $p$-values $p_1$ and $p_2$ combine to form a smaller $p$-value. The yellow-green boundary in the figure below defines the region where the meta-analysis $p$-value is below 0.05. For example, if both $p$-values are around $0.10$, or if one is around $0.04$ and one is around $0.25$, the meta-analysis $p$-value is around $0.05$:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/arithmetics/fisher_multiple_pvalues_test.jpg}
		\caption[Fisher's method for multiple $p$-values]{Fisher's method for multiple $p$-values (source: Wikipedia)}
	\end{figure}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In the famous judgement case of Lucia de Berk the court made heavy use of statistical calculations to achieve its conviction. In a 12003 (holocene calendar) TV special of NOVA, Dutch professor of Criminal Law Theo de Roos stated: «\textit{In the Lucia de Berk case statistical evidence has been of enormous importance. I do not see how one could have come to a conviction without it}». The law psychologist Henk Elffers, who was used by the courts as expert witness on statistics both in the original case and on appeal, was also interviewed on the programme and stated that the chance of a nurse working at the three hospitals being present at the scene of so many unexplained deaths and resuscitation is one in $342$ million. But sadly, this value was wrongly calculated (this is why any important statistical calculations should be peer-reviewed always at least by three other independent people) by making a simple calculations of the multiplication of the $p$-values... Furthermore, always in the context of Lucai de Berk the statisticians Richard D. Gill and Piet Groeneboom calculated a chance of one in twenty-five that a nurse could experience a sequence of events of the same type as Lucia de Berk.\\

	The use of probability arguments in the De Berk case was discussed in a 12007 (holocene calendar) \textit{Nature} article by Mark Buchanan. He wrote: « \textit{The court needs to weigh up two different explanations: murder or coincidence. The argument that the deaths were unlikely to have occurred by chance (whether $1$ in $48$ or $1$ in $342$ million) is not that meaningful on its own - for instance, the probability that ten murders would occur in the same hospital might be even more unlikely. What matters is the relative likelihood of the two explanations! However, the court was given an estimate for only the first scenario.} »\\
	
	So we see here that using simple multiplication of Fisher multiplication inference is not the point in such a situation!
	\end{tcolorbox}
	
	Notice that the use of Fisher's method works only if the studies included in a meta-analysis are comparable (sample size). If they aren't we have to use other techniques permitting the usage of weights like illustrated in the figure below that we have already encountered in the Introduction of this book:
	\begin{figure}[H]
		\centering
		\includegraphics[width=1\textwidth]{img/intro/meta_analysis.jpg}
		\caption[Special example of a meta-analysis]{Special example of a meta-analysis in the field of clinical trials with concepts that we will detail without exception in this entire book (source: \cite{legg2007occupational})}
	\end{figure}
	The risk difference\index{risk difference} (R.D.) being simply the difference of two proportions, we use the famous following Wald interval (see page \pageref{Wald interval}) for the calculation of the confidence interval:
	
	where by definition $\text{R.D.}:=p_X-p_Y$.
	
	We see above that the width of the $95\%$ confidence interval of the common risk difference is smaller than the width of the $95\%$ confidence interval of the estimated effect in each of the trials. That's obviously because the pooled effect is more precise than the effect estimated in the trials.

	It is also very important to notice that in a meta-analysis (like the example above), even if no trial detects an effect of the experimental intervention (as you can see above in every trial zero is in the confidence interval), the synthesis of the results of the trials (Risk Difference as above) can make it possible to detect an overall effect (the reader can see that in the total confidence interval, the zero is outside the confidence interval). We gain in power of the test!
	
	The reader may ask where does the $-0.09$ comes from in the "Total (95\% CI)" line? It comes from the simple following weighted average of all the risk differences:
	$$\widehat{\text{R.D.}}_{\text{weighted}}=0.128\cdot(-0.08)+0.087\cdot(0.01)+0.153\cdot(-0.13)+\ldots=-0.09$$
	
	\paragraph{Simpson's Paradox (sophism)}\label{Simpson paradox}\mbox{}\\\\
	The "\NewTerm{Simpson's paradox}\index{Simpson's paradox}" (in fact it's a sophism and not a paradox because it is just a special case of omitted variable bias), also named "\NewTerm{Yule-Simpson effect}\index{Simpson's paradox}", is a paradox in probability and statistics, in which a trend appears in different groups of data but disappears or reverses when these groups are combined\footnote{Edward H. Simpson first described this phenomenon in a technical paper in 11951 (holocene calendar), but the statisticians Karl Pearson et al., in 11899 (holocene calendar), and Udny Yule, in 11903 (holocene calendar), had mentioned similar effects earlier. The name Simpson's paradox was introduced by Colin R. Blyth in 11972 (holocene calendar).}.
	
	This result is often encountered especially in social-science (Human Resources, Marketing, Psychology), and medical-science statistics and is particularly confounding when frequency data are unduly given causal interpretations. Simpson's paradox disappears when causal relations are brought into consideration. 
	
	Let us see a famous real example for a real medical study (there is also another famous case with the \textit{Nation at Risk} report, if the reader is interested, just search for it on Internet!):
	
	The table below shows the success rates and numbers of treatments for treatments involving both small and large kidney stones, where Treatment $A$ includes all open surgical procedures and Treatment $B$ is percutaneous nephrolithotomy (which involves only a small puncture). The numbers in parentheses indicate the number of success cases over the total size of the group (for example, $93\%$ equals $81$ divided by $87$):
	
	\begin{center}
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|p{3cm}|p{3cm}|p{3cm}|}
				\hline
				 & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Treatment $A$}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Treatment $B$}} \\ \hline
				\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Small stones}}  & Group 1\newline $93\%\; (81/87)$  & Group 2\newline $87\%\; (234/270)$ \\ \hline
				\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Large stones}}   & Group 3\newline $73\%\; (192/263)$ & Group 4\newline $69\%\; (55/88)$  \\ \hline
				\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Both}} & \textbf{$78\%\; (273/350)$} & \textbf{$83\%\; (289/350)$} \\ \hline
		\end{tabular}
	\end{center}
	
	The paradoxical conclusion is that treatment $A$ is more effective when used on small stones, and also when used on large stones, yet treatment $B$ is more effective when considering both sizes at the same time. In this example the "lurking" variable (or "\NewTerm{confounding variable}"\index{confounding variable}) of the stone size was not previously known to be important until its effects were included.
	
	Formally, if we denote (we change the notations!!!!) $A$ the result, $B$ the treatment, and $C$ the stones, in the general case, with the Simpson paradox it is possible to have using Bayesian probabilities:
	

	Which treatment is considered better is determined by an inequality between two ratios (successes/total). The reversal of the inequality between the ratios, which creates Simpson's paradox, happens because two effects occur together:
	
	\begin{enumerate}
		\item The sizes of the groups, which are combined when the lurking variable is ignored, are very different. Doctors tend to give the severe cases (large stones) the better treatment $A$, and the milder cases (small stones) the inferior treatment $B$. Therefore, the totals are dominated by groups $3$ and $2$, and not by the two much smaller groups $1$ and $4$.
		
		\item The lurking variable has a large effect on the ratios, i.e. the success rate is more strongly influenced by the severity of the case than by the choice of treatment. Therefore, the group of patients with large stones using treatment $A$ (group $3$) does worse than the group with small stones, even if the latter used the inferior treatment $B$ (group $2$).
	\end{enumerate}

	Based on these effects, the paradoxical result is seen to arise by suppression of the causal effect of stone size on successful treatment. The paradoxical result can be rephrased more accurately as follows: When the less effective treatment ($B$) is applied more frequently to easier cases, it can appear to be a more effective treatment.
	
	Simpson's paradox usually fools us on tests of performance. In a famous example, researchers concluded that a newer treatment for kidney stones was more effective than traditional surgery, but it was later revealed that the newer treatment was more often being used on small kidney stones. More recently, on elementary school tests, minority students in Texas outperform their peers in Wisconsin, but Texas has so many minority students that Wisconsin beats it in state rankings. It would be a shame if Simpson's paradox led doctors to prescribe ineffective treatments or Texas schools to waste money copying Wisconsin.
	
	Consider another funny illustrated example in Marketing! Consider we have the following information:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/simpson_paradox_normal.jpg}
		\caption[]{Marketing Campaign performance}
	\end{figure}
	Obviously the campaign $B$ perform the best into converting an e-mail to click ("conversion rate")!
	
	But now splitting in clicking high valuable links (costly products: high ticket conversion) or low valuable links (low ticket conversion) we get:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/simpson_paradox_ouch.jpg}
		\caption[]{Marketing Campaign performance in ticket conversion covariate}
	\end{figure}
	Or for people that are more visual (with plots) here is an example from our \texttt{R} companion book. This is a simple regression of the IRIS dataset. The linear regression trend seem negative:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.9]{img/arithmetics/simpson_paradox_plot_before_clustering.jpg}
		\caption{IRIS dataset non-clustered regression in \texttt{R} to illustrate Simpson paradox}
	\end{figure}
	Now if we cluster the regression.... all the trends are positive...:
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.9\textwidth]{img/arithmetics/simpson_paradox_plot_after_clustering.jpg}
		\caption{IRIS dataset clustered regression in \texttt{R} to illustrate Simpson paradox}
	\end{figure}
	Some people may argue that this is not business oriented... So here is an example of Simpson’s Paradox typical in business intelligence dashboards:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/simpson_paradox_business_intelligence.jpg}
	\end{figure}
	We see above that aggregated regional Sales look like they are on plan. But at the next level of detail (at the Territory level), we can see that Territory A is doing very poorly ($25\%$ of plan) and Territory B is doing exceptionally well ($400\%$ of plan). This is a major challenge in the world of BI. It's easy to feel content with what the BI reports and dashboards are telling us without having an easy, automated way to delve into the minutiae to find those areas of the business that are significantly over- or under-performing. 
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Another kind of quite famous similar case in statistics is named the "\NewTerm{Lord's paradox}\index{Lord's paradox}" (that is also only a sophism in reality!). It just highlights the obvious fact that under some conditions an ANOVA (ANalysis Of VARiance) won't give the same result as an ANCOVA (ANalysis of CoVAriance). These are two different kind of tools that we will see further below.
	\end{tcolorbox}
	
	\pagebreak
	\subsubsection{Power of a test}\label{power of a text}
	When the effect is actually large, we can expect that we need less observations to demonstrate it that when the effect is small ... but how much exactly? Did we have the possibility to do this, in terms of number of measures, to "prove" what we seek? Should we go about it differently and change the device to do the  observation/experiment?
	
	We have seen earlier that we can never "prove" the null hypothesis\footnote{Remember that with NHST, if the $p$-value is greater than a given threshold, we do should not say that we can accept (or "prove") the null hypothesis; rather, we use the convoluted locution that « \textit{We have failed to disprove the null}».}; all we can do is reject or fail to reject it. However, there are times when it is necessary to try to "prove" (i.e. get evidence) the non-existence of a difference between groups. This most often happens within the context of comparing a new treatment against an established one and showing that the new intervention is not inferior to the standard (in some circumstances, we need many more subjects to get evidence of non-inferiority than we need to show a difference). Can we do something like this?
	
	To answer all questions above, we need now to study in more detail the concept of "\NewTerm{power of a test}\index{power of a test}" that we have so far only mentioned, remember the following figure that we have already seen just a little bit earlier:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,578); %set diagram left start at 0, and has height of 578
		
		%Shape: Polygon Curved [id:ds4342003864561583] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][fill={rgb, 255:red, 254; green, 243; blue, 21 }  ,fill opacity=1 ] (285.6,24.46) .. controls (302.34,24.64) and (326.1,88.96) .. (334.85,108.71) .. controls (343.6,128.46) and (358.15,151.66) .. (373.35,154.94) .. controls (388.56,158.21) and (407.2,159.46) .. (407.06,159.46) .. controls (406.91,159.46) and (166.06,159.46) .. (165.86,159.28) .. controls (165.67,159.1) and (187.31,159.21) .. (200.31,153.21) .. controls (213.31,147.21) and (226.69,127.9) .. (235.06,109.46) .. controls (243.43,91.02) and (268.86,24.28) .. (285.6,24.46) -- cycle ;
		%Shape: Polygon Curved [id:ds5469066387497896] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][fill={rgb, 255:red, 248; green, 38; blue, 38 }  ,fill opacity=1 ] (225.85,159.28) .. controls (226.1,159.03) and (166.06,159.46) .. (165.86,159.28) .. controls (165.67,159.1) and (185.2,159.37) .. (200.31,153.21) .. controls (215.42,147.05) and (225.35,127.78) .. (225.6,127.53) .. controls (225.85,127.28) and (225.6,159.53) .. (225.85,159.28) -- cycle ;
		%Shape: Polygon Curved [id:ds02559745982606354] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][fill={rgb, 255:red, 248; green, 38; blue, 38 }  ,fill opacity=1 ] (345.56,129.16) .. controls (345.35,127.19) and (356.17,147.51) .. (368.35,152.44) .. controls (380.53,157.37) and (407.2,159.46) .. (407.06,159.46) .. controls (406.91,159.46) and (345.6,159.44) .. (345.6,158.94) .. controls (345.6,158.44) and (345.76,131.14) .. (345.56,129.16) -- cycle ;
		%Straight Lines [id:da722416651982738] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (285.76,19.16) -- (285.76,349.28) ;
		%Shape: Polygon Curved [id:ds9540489908010028] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ] (285.6,24.46) .. controls (302.34,24.64) and (326.1,88.96) .. (334.85,108.71) .. controls (343.6,128.46) and (358.66,151.1) .. (373.86,154.37) .. controls (389.07,157.64) and (407.2,159.46) .. (407.06,159.46) .. controls (406.91,159.46) and (166.06,159.46) .. (165.86,159.28) .. controls (165.67,159.1) and (187.31,159.21) .. (200.31,153.21) .. controls (213.31,147.21) and (226.69,127.9) .. (235.06,109.46) .. controls (243.43,91.02) and (268.86,24.28) .. (285.6,24.46) -- cycle ;
		%Shape: Axis 2D [id:dp15147243193453197] 
		\draw  (154.86,159.28) -- (407.86,159.28)(165.86,16) -- (165.86,167.28) (400.86,154.28) -- (407.86,159.28) -- (400.86,164.28) (160.86,23) -- (165.86,16) -- (170.86,23) (173.86,154.28) -- (173.86,164.28)(181.86,154.28) -- (181.86,164.28)(189.86,154.28) -- (189.86,164.28)(197.86,154.28) -- (197.86,164.28)(205.86,154.28) -- (205.86,164.28)(213.86,154.28) -- (213.86,164.28)(221.86,154.28) -- (221.86,164.28)(229.86,154.28) -- (229.86,164.28)(237.86,154.28) -- (237.86,164.28)(245.86,154.28) -- (245.86,164.28)(253.86,154.28) -- (253.86,164.28)(261.86,154.28) -- (261.86,164.28)(269.86,154.28) -- (269.86,164.28)(277.86,154.28) -- (277.86,164.28)(285.86,154.28) -- (285.86,164.28)(293.86,154.28) -- (293.86,164.28)(301.86,154.28) -- (301.86,164.28)(309.86,154.28) -- (309.86,164.28)(317.86,154.28) -- (317.86,164.28)(325.86,154.28) -- (325.86,164.28)(333.86,154.28) -- (333.86,164.28)(341.86,154.28) -- (341.86,164.28)(349.86,154.28) -- (349.86,164.28)(357.86,154.28) -- (357.86,164.28)(365.86,154.28) -- (365.86,164.28)(373.86,154.28) -- (373.86,164.28)(381.86,154.28) -- (381.86,164.28)(389.86,154.28) -- (389.86,164.28)(160.86,151.28) -- (170.86,151.28)(160.86,143.28) -- (170.86,143.28)(160.86,135.28) -- (170.86,135.28)(160.86,127.28) -- (170.86,127.28)(160.86,119.28) -- (170.86,119.28)(160.86,111.28) -- (170.86,111.28)(160.86,103.28) -- (170.86,103.28)(160.86,95.28) -- (170.86,95.28)(160.86,87.28) -- (170.86,87.28)(160.86,79.28) -- (170.86,79.28)(160.86,71.28) -- (170.86,71.28)(160.86,63.28) -- (170.86,63.28)(160.86,55.28) -- (170.86,55.28)(160.86,47.28) -- (170.86,47.28)(160.86,39.28) -- (170.86,39.28)(160.86,31.28) -- (170.86,31.28) ;
		\draw [color={rgb, 255:red, 74; green, 74; blue, 74 }  ,opacity=1 ]  ;
		%Straight Lines [id:da9473092815158279] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (225.6,127.53) -- (225.6,352.28) ;
		%Shape: Polygon Curved [id:ds6552965895141998] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][fill={rgb, 255:red, 254; green, 243; blue, 21 }  ,fill opacity=1 ] (369.6,211.46) .. controls (386.34,211.64) and (410.1,275.96) .. (418.85,295.71) .. controls (427.6,315.46) and (442.15,338.66) .. (457.35,341.94) .. controls (472.56,345.21) and (491.2,346.46) .. (491.06,346.46) .. controls (490.91,346.46) and (250.06,346.46) .. (249.86,346.28) .. controls (249.67,346.1) and (271.31,346.21) .. (284.31,340.21) .. controls (297.31,334.21) and (310.69,314.9) .. (319.06,296.46) .. controls (327.43,278.02) and (352.86,211.28) .. (369.6,211.46) -- cycle ;
		%Shape: Polygon Curved [id:ds9367090381465286] 
		\draw  [color={rgb, 255:red, 128; green, 128; blue, 128 }  ,draw opacity=1 ][fill={rgb, 255:red, 248; green, 38; blue, 38 }  ,fill opacity=1 ] (345.94,346.28) .. controls (346.44,346.28) and (250.06,346.46) .. (249.86,346.28) .. controls (249.67,346.1) and (269.2,346.37) .. (284.31,340.21) .. controls (299.42,334.05) and (316.19,305.28) .. (323.19,287.78) .. controls (330.19,270.28) and (345.6,237.87) .. (345.69,237.78) .. controls (345.79,237.69) and (345.44,346.28) .. (345.94,346.28) -- cycle ;
		%Shape: Axis 2D [id:dp22758917727795813] 
		\draw  (154.86,346.28) -- (504.86,346.28)(165.8,203) -- (165.8,354.28) (497.86,341.28) -- (504.86,346.28) -- (497.86,351.28) (160.8,210) -- (165.8,203) -- (170.8,210) (173.8,341.28) -- (173.8,351.28)(181.8,341.28) -- (181.8,351.28)(189.8,341.28) -- (189.8,351.28)(197.8,341.28) -- (197.8,351.28)(205.8,341.28) -- (205.8,351.28)(213.8,341.28) -- (213.8,351.28)(221.8,341.28) -- (221.8,351.28)(229.8,341.28) -- (229.8,351.28)(237.8,341.28) -- (237.8,351.28)(245.8,341.28) -- (245.8,351.28)(253.8,341.28) -- (253.8,351.28)(261.8,341.28) -- (261.8,351.28)(269.8,341.28) -- (269.8,351.28)(277.8,341.28) -- (277.8,351.28)(285.8,341.28) -- (285.8,351.28)(293.8,341.28) -- (293.8,351.28)(301.8,341.28) -- (301.8,351.28)(309.8,341.28) -- (309.8,351.28)(317.8,341.28) -- (317.8,351.28)(325.8,341.28) -- (325.8,351.28)(333.8,341.28) -- (333.8,351.28)(341.8,341.28) -- (341.8,351.28)(349.8,341.28) -- (349.8,351.28)(357.8,341.28) -- (357.8,351.28)(365.8,341.28) -- (365.8,351.28)(373.8,341.28) -- (373.8,351.28)(381.8,341.28) -- (381.8,351.28)(389.8,341.28) -- (389.8,351.28)(397.8,341.28) -- (397.8,351.28)(405.8,341.28) -- (405.8,351.28)(413.8,341.28) -- (413.8,351.28)(421.8,341.28) -- (421.8,351.28)(429.8,341.28) -- (429.8,351.28)(437.8,341.28) -- (437.8,351.28)(445.8,341.28) -- (445.8,351.28)(453.8,341.28) -- (453.8,351.28)(461.8,341.28) -- (461.8,351.28)(469.8,341.28) -- (469.8,351.28)(477.8,341.28) -- (477.8,351.28)(485.8,341.28) -- (485.8,351.28)(160.8,338.28) -- (170.8,338.28)(160.8,330.28) -- (170.8,330.28)(160.8,322.28) -- (170.8,322.28)(160.8,314.28) -- (170.8,314.28)(160.8,306.28) -- (170.8,306.28)(160.8,298.28) -- (170.8,298.28)(160.8,290.28) -- (170.8,290.28)(160.8,282.28) -- (170.8,282.28)(160.8,274.28) -- (170.8,274.28)(160.8,266.28) -- (170.8,266.28)(160.8,258.28) -- (170.8,258.28)(160.8,250.28) -- (170.8,250.28)(160.8,242.28) -- (170.8,242.28)(160.8,234.28) -- (170.8,234.28)(160.8,226.28) -- (170.8,226.28)(160.8,218.28) -- (170.8,218.28) ;
		\draw [color={rgb, 255:red, 74; green, 74; blue, 74 }  ,opacity=1 ]  ;
		%Straight Lines [id:da7187920863329391] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (345.56,129.16) -- (345.56,351.28) ;
		%Straight Lines [id:da6964578965222128] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (369.56,147.16) -- (369.56,350.28) ;
		
		% Text Node
		\draw (142,123.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0.1$};
		% Text Node
		\draw (142,90.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0.2$};
		% Text Node
		\draw (143,51.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0.3$};
		% Text Node
		\draw (144,21.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0.4$};
		% Text Node
		\draw (179,165.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$-3$};
		% Text Node
		\draw (211,165.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$-2$};
		% Text Node
		\draw (244,165.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$-1$};
		% Text Node
		\draw (282,165.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0$};
		% Text Node
		\draw (314,165.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$1$};
		% Text Node
		\draw (346,165.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$2$};
		% Text Node
		\draw (378,165.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$3$};
		% Text Node
		\draw (182,126.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$\alpha /2$};
		% Text Node
		\draw (357,126.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$\alpha /2$};
		% Text Node
		\draw (330,48.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$ \begin{array}{l}
		H_{0} :\mu =0\\
		\alpha =\text{Type I Error}
		\end{array}$};
		% Text Node
		\draw (142,310.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0.1$};
		% Text Node
		\draw (142,277.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0.2$};
		% Text Node
		\draw (143,238.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0.3$};
		% Text Node
		\draw (144,208.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0.4$};
		% Text Node
		\draw (179,352.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$-3$};
		% Text Node
		\draw (211,352.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$-2$};
		% Text Node
		\draw (244,352.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$-1$};
		% Text Node
		\draw (282,352.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$0$};
		% Text Node
		\draw (314,352.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$1$};
		% Text Node
		\draw (346,352.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$2$};
		% Text Node
		\draw (378,352.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$3$};
		% Text Node
		\draw (307,280.4) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$\beta $};
		% Text Node
		\draw (413,235.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$ \begin{array}{l}
		H_{1} :\mu =2.5\\
		\beta =\text{Type II Error}
		\end{array}$};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption[]{Null and alternative hypothesis of a special case of two-sided test}
	\end{figure}
	In the particular example above, we will obviously reject the null hypothesis $H_0$ if  $\bar{X}>1.96$ or $\bar{X}<-1.96$. Imagine that under the alternative hypothesis $H_1$, if we measured 2.5 for $\bar{X}$, we will have for power of the test:
	
	So the test is relatively strong/powerful (in practice, we consider a test to be powerful if its value is above of $80\%$ - that is to say the $\beta\leq 20\%$). Thus, we see that the (a posteriori!) power $1-\beta$ is even larger than the $p$-value is small (and respectively the power will be a posteriori even smaller than the $p$-value is great). Therefore the a posteriori power is in decreasing correspondence with the $p$-value (in practice it is however a bit absurd to make these calculations a posteriori but some scientific paper require this information).
	
	So in general:
	
	
	A great way to play with these concepts and visualize them is to take a look at the Wolfram widget available on \url{http://demonstrations.wolfram.com/StatisticalPower/}:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.9]{img/arithmetics/wolfram_widget_power_errors_test.jpg}
	\end{figure}
	Before continuing, let us inform you that many don't understand why the probability of getting a type I error when performing a hypothesis test, isn't affected by the sample size?!
	
	If we were using standard hypothesis testing, we may set the confidence level $\alpha$ and the compare the test $p$-value to it. In this case the sample size will not impact the probability of Type I error because your confidence level $\alpha$ is the probability of Type I error, pretty much by definition. In other words, you set the probability of Type I error by choosing the confidence level! The probability of Type I error is only impacted by your choice of the confidence level and nothing else!
	
	But yes, we could also fix Type II error rate (but this is not the purpose of science as we know to "prove" the null hypothesis as it is practically quite hard most of times!), the Type I error rate usually becomes the unknown parameter and it is dependent on the sample size, the variance and the effect size (delta).
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Many people say that if a Person $A$ posits the existence of some phenomenon, it is incumbent on Person $A$ to prove its existence rather than it being Person $B$'s job to disprove it. For example, if we assert that there are pink unicorns with purple polka dots hiding deep in the forest, I cannot simply say, «\textit{Well, prove that they don't exist}». For the scientific world to accept my claim, I have to bring one back, dead or alive. Is it ever legitimate to violate this injunction and try to prove the non-existence of something? In the text above we then have taken a look at trying to "prove" (i.e. get "enough evidence") that something does not exist\footnote{Obviously if we are sure that we have investigated ALL possible combinations, places and that the huge majority properties of the unicorn are proven wrong or impossible, then it's very highly likely that the unicorn itself doesn't exist}.
	\end{tcolorbox}
	
	\paragraph{Evidence ("proof") of a negative}\mbox{}\\\\
	Georgia minister Nelson L. Price asserts on his website (and many other science popularizer on YouTube and scientifically illiterate social networks users) that «\textit{one of the laws of logic is that you can't prove a negative}» (also named "argumentum ad ignorantium"). The fact is, however, that this supposed "law of logic" is no such thing!
	
	Notice, for a start, that «You cannot prove a negative» is itself a negative. So, if it were true, it would itself be unprovable. Notice that any claim can be transformed into a negative by a little rephrasing—most obviously, by negating the claim and then negating it again. "I exist" is logically equivalent to "I do not not exist," which is a negative. Yet here is a negative it seems we might perhaps be able to prove (in the style of Descartes: «\textit{I think, therefore I do not not exist!}»).
	
	Of course, those who say «You cannot prove a negative», or who say we do an «argumentum ad ignorantium»,  will insist that we have misunderstood their point. As Steven D. Hales notes, when people say, "You can't prove a negative," what they really mean is that you cannot prove that something does not exist. If this point were correct, it would apply not just to supernatural beings lying beyond the cosmic veil but also to things that might be supposed to exist on this side of the veil, such as unicorns, Martians, rabbits with 20 heads, and so on. We would not be able to prove the non-existence of any of these things either.
	
	But is the point correct? Is it true that we can never prove that something does not exist? Again, it depends \textbf{IF} you have sufficiently defined the scope of what you are looking at and the level of evidence\footnote{For example the huge majority of properties and acts that define some divinities have been showed with strong evidence as wrong. This lead us to the very likely conclusion that there are no divinities - especially divinities that have become mythologies give us a strong evidence for this.}. If someone claims there's a unicorn in the tool shed (see \cite{streiner2003unicorns} for a deep treatment of this subject), we can quickly establish this person is mistaken by going and taking a look. We could similarly establish there's no Loch Ness monster by draining the loch. But what of the claim that unicorns once existed? We can't travel back in time and directly observe all of the past as we can every corner of the tool shed or Loch Ness. Does it follow that we can't prove unicorns never existed?
	
	It depends in part, as we already know it, on what we mean by "prove". The word has a variety of meanings. By saying something is "prove", we might mean that it is established beyond all possible doubt. Or we might mean it has been established beyond reasonable doubt (this is the kind of proof in experimental science). Can we establish beyond reasonable doubt that unicorns have never inhabited the Earth? True, the history of our planet has been and gone, so we can no longer directly inspect it. But surely, if unicorns did roam the Earth, we would expect to find some evidence of their presence, such as fossils of unicorns or at least of closely related animals from which unicorns might plausibly have evolved. There is none. We also have plenty of evidence that unicorns are a fictional creation, in which case, it's surely reasonable for us to conclude that there never were any unicorns. Indeed, we would suggest we can give evidence of this beyond reasonable doubt (especially if all inherent properties of unicorns have already been broken down).
	
	In response, it might be said, «But you can't prove conclusively, beyond all possible doubt, that unicorns never roamed the Earth\footnote{A "conclusive evidence" is evidence that cannot be contradicted by any other evidence. It is so strong as to overbear any other evidence to the contrary. The evidence is of such a nature that it compels a fact-finder to come to a certain conclusion.}.» This is undeniably true! However, this point is not peculiar to negatives. It can be made about any claim about the unobserved, and thus any scientific theory at all, including scientific theories about what does exist. We can prove beyond reasonable doubt that dinosaurs existed, but not beyond all possible doubt!
	
	Despite the mountain of evidence that dinosaurs roamed the Earth, it's still possible that, say, all those dinosaur fossils are fakes placed there by alien pranksters long ago.
	
	Let's sum up. If "you can't prove a negative" means you can't prove beyond reasonable doubt that certain things don't exist, then the claim is just false. We prove the non-existence of things on a regular basis. If, on the other hand, "you can't prove a negative" means you cannot prove beyond all possible doubt that something does not exist, well, that may, arguably, be true. But so what? That point is irrelevant so far as defending beliefs in supernatural entities against the charge that science and/or reason have established beyond reasonable doubt that they don't exist.
	
	Let us now consider something has a finite number of observable (i.e. refutable) properties/characteristics in a finite volume. So say we have a bowl. We can prove (i.e. get evidence) the non-existence of milk in our bowl with a photo (or more complex apparatus) of an empty bowl. But this photo does not depict negative milk. It depicts a positive bottom of the bowl, and from that positive, we can deduce the impossibility of milk being there, knowing, from other evidence, what milk looks like. So we are using positive evidence to prove impossibility. Impossibility is the exhaustion of exclusive possibilities. Possibilities are zero or positive. In this case, milk and empty are mutually exclusive, where the likelihood of milk is $0$, and the likeliness of empty is $1$. This is different from proving a negative. Now as most religions have holy books that depicts quite precisely some properties and actions of their gods on Earth (finite volume), then if we found evidence in that closed volume (Earth) that disprove at least one of the properties (i.e. characteristic) of their god, then that latter very likely doesn't exist beyond any reasonable doubt (B.A.R.D.)... Technically this is denoted:
	
	and named the "\NewTerm{Stenger-Neyman-Pearson supernatural falsification method}". So applying this relation to religious books (vedas, bible, torah, quran, etc.) knowing that the bible has (without counting the contradictions and historical errors) at least 21 scientific errors, the quran has (without counting the contradictions and historical errors) at least 73 scientific errors, the torah has (without counting the contradictions and historical errors) at least 20 scientific errors, the vedas have (without counting the contradictions and historical errors) at least 26 scientific errors bring us to quite an obvious conclusion about the perfection - and hence existence - of their supposed respective deities...
	
	\begin{fquote}[Etienne Klein]Science doesn't tell us what to think but sometimes it tells us what we can no longer believe.
 	\end{fquote}
	
	The old adage «You cannot prove a negative» (the correct formulation should be «You cannot get evidence of a negative») is a typical evidence level 02 (see page \pageref{evidence levels chart}) argument from scientifically illiterate people. 
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{img/arithmetics/universal_negative.jpg}
	\end{figure} 
	So proving (i.e. get evidence) the non-existence depends how tightly defined the thing are checking for does not exist. If we can define some attribute the thing absolutely must have in a finite volume or time range, then we may be able to have strong evidence that the thing with that attribute does not exist. But as soon as there is any vagueness in the description, we cannot do so. So, again in order to prove (i.e. get evidence) something does not exist we must establish the properties of that thing in a finite volume to the point of being able to test its negation.
	
	\subparagraph{Two-One-Sided $T$-test procedure (TOST)}\index{statistical tests!TOST test}\mbox{}\\\\
	If we want to test the equality of two values $\hat{\theta}_1=\hat{\theta}_2$ (mean, proportion or other), meaning the non-existence of an effect (or a "negative"), as alternative hypothesis $H_A$ we can't do that with classical hypothesis and alternative test (as it is quite hard to build the statistics of the null hypothesis $H_0$, ie the statistical distribution of $\hat{\theta}_1\neq\hat{\theta}_2$)!

	A well known approach we then use in practice (considering the case with means!) is the "\NewTerm{Two One-Sided Tests (TOST) procedure}\index{two one-sided tests (TOST) procedure}". This involves looking at the difference between the mean values for the two populations then testing a joint null hypothesis: that this difference is less than or equal to $+\delta$ AND that this difference is greater than or equal to $-\delta$  (for some sufficiently small and positive $\delta$). If we can reject this joint null hypothesis, then you can conclude that the absolute value of the difference between the means is less than $\delta$ (see \cite{juzek2019set} for an analysis on how to choose $\delta$).

	In other words, the Two-One-Sided $T$-test procedure (TOST) is used to show that two samples are similar, in contrast to classical statistical tests which check for dissimilarity. The TOST relies on a parameter called delta, which has to be set by the researcher using their intuition. Doing so can be difficult, because of complex interactions of relevant parameters.

	In some fields there may be industry standards to use as a guide for $\delta$. If not, the equivalence acceptance criterion is best developed from a consensus among stakeholders such as decision makers, system engineers, operators, and subject matter experts (SMEs). Deciding what difference in SUT performance is "good enough" and determining $\delta$ for the equivalence test is not a trivial task and adequate time should be allotted for this in the planning phase. The value of $\delta$ should be based on how big a difference is practically important to consider the means not equivalent. $\Delta$ is similar to the difference to detect when we size a traditional designed experiment. Once $\delta$ is derived, the hypothesis can be stated as:
	
	A one-sided test similarity $T$-test (OST) is appropriate if the test team only cares if the response differs in one direction. For example, if a new rubber compound is used to make a tire with a mileage guarantee the producers may only care if the tire gets fewer miles of use. This is often named a "\NewTerm{test for non-inferiority}\index{test for non inferiority}". In these situations, the null hypothesis is often termed the "inferiority" hypothesis and the alternate is the "non inferiority" hypothesis. The following equation is used to compare the $t$-ratio to the critical value.

	As always we no longer use $\mu$, because it represents the true population parameter, which we will likely never know. Instead we use $\hat{\mu}$, which denotes the sample mean. The critical value $T_{\alpha, \text{df}}$ can be calculated with software or looked up in statistics tables.

	We reject $H_0$ if: 
	
	In similarity testing, there is no two-sided test. Instead, we conduct two non inferiority tests from both sides:
	
	When the hypotheses are expanded to create the test statistics we get the following criterion. We reject $H_{0}$ if: 
	
	Note that the two means are considered equivalent if, and only if, both null hypotheses are rejected! We must show that difference in the means is less than $+\delta$ and greater than $-\delta$ to show that the means are equivalent; it cannot be true in just one direction. It is also prudent to point out that this is different than a traditional two-sided hypothesis test. When conducting a two-sided test with a Type I probability of $\alpha$, we use $\alpha / 2$ to compare to the $p$-value on each side. In this situation we use the full value of $\alpha$ on both tests because they are both one-sided.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners, breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Suppose we are analysing the effectiveness of the door gunner in a helicopter. It is normal for the door gunner to use the M-249 but a unit wants to instead arm them with the M-4. The M-249 is a machine gun capable of a higher volume of fire, but it is heavier, harder to move, and more prone to jam. Can the M-4 provide equivalent combat power? We have the following data:
	$$\begin{array}{|l|r|}
	\rowcolor[gray]{0.75}\hline \textbf{ Weapon Type } & \begin{array}{c}
	\textbf{ Target Damage } \\
	\textbf{ Score }
	\end{array} \\
	\hline \text { M-249 } & 180 \\
	\hline \text { M-249 } & 143 \\
	\hline \text { M-4 } & 65 \\
	\hline \text { M-4 } & 112 \\
	\hline \text { M-4 } & 139 \\
	\hline \text { M-4 } & 112 \\
	\hline \text { M-4 } & 125 \\
	\hline \text { M-4 } & 78 \\
	\hline \text { M-4 } & 138 \\
	\hline \text { M-4 } & 84 \\
	\hline \text { M-249 } & 117 \\
	\hline \text { M-249 } & 169 \\
	\hline \text { M-249 } & 111 \\
	\hline \text { M-249 } & 114 \\
	\hline \text { M-249 } & 166 \\
	\hline \text { M-4 } & 134 \\
	\hline \text { M-249 } & 131 \\
	\hline \text { M-4 } & 69 \\
	\hline \text { M-249 } & 93 \\
	\hline \text { M-249 } & 177 \\
	\hline
	\end{array}$$
	The reader can see the weapon used and the damage score for each of the $20$ trials. This example is small enough that we can go through the calculations manually. Three basic statistics are calculated and shown in the table below:
	$$\begin{array}{|c|c|c|c|}
	\rowcolor[gray]{0.75}\hline \textbf { Weapon } & \textbf { Sample size } & \textbf { Sample mean } & \textbf { Sample variance } \\
	\hline \text { M-249 } & n_{1}=10 & \hat{\mu}_{1}=140.1 & \hat{\sigma}_{1}^{2}=981.21 \\
	\hline \text { M-4 } & n_{2}=10 & \hat{\mu}_{2}=105.6 & \hat{\sigma}_{2}^{2}=849.6 \\
	\hline
	\end{array}$$
	Using the values calculated in the table above we can calculate the test statistic and critical value. First for the upper end of the curve:
	$$
	\text{SE}=\sqrt{\frac{\hat{\sigma}_{1}^{2}}{n_{1}}+\frac{\hat{\sigma}_{2}^{2}}{n_{2}}}=\sqrt{\frac{981.21}{10}+\frac{849.6}{10}}=\sqrt{183.0811} \cong 13.53
	$$
	We reject $H_0$ if:
	$$\frac{\left|\hat{\mu}_{1}-\hat{\mu}_{2}\right|+\delta}{\text{SE}}>T_{\alpha, n_{1}+n_{2}-2}$$
	Hence:
	$$
	\frac{|140.1-105.6|+20}{13.53}>T_{0.05,18}
	$$
	That is (right TOST):
	$$4.03>1.734 \stackrel{\text { yields }}{\longrightarrow} \text{True}$$
	The statement is true so we reject $H_0$ in the first one-sided test. Next,  we conduct the second test (left TOST):
	$$\frac{\left|\hat{\mu}_{1}-\hat{\mu}_{2}\right|-\delta}{\text{SE}}<-T_{\alpha, n_{1}+n_{2}-2}$$
	Hence:
	$$\frac{|140.1-105.6|-20}{13.53}<-T_{0.05,18}$$
	That is:
	$$1.07<-1.734 \stackrel{\text { yields }}{\longrightarrow} \text{False}$$
	The statement is false and we fail to reject $H_{0}$ in the second one-side test. Therefore, we also fail to reject the overall null hypothesis that the M-4 is not equivalent to the M-249. This is not a surprising outcome because our initial difference in means was $34.5$.
	\end{tcolorbox}
	
	The same reasoning can be applied for other tests. This is why in the \texttt{R} software you can find also a function to compute the TOST for proportions!
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Depending on the statistical software packages, the degrees of freedom may be calculated using the Welch correction. Some software packages return only one $p$-value and only one confidence interval (most of the time for reasons we can't explain they return that of the left TOST only). Some even return a completely wrong calculation of the $p$-value. In other words it is better to run a TOST on more than three statistical packages to be almost sure to get a converging conclusion...
	\end{tcolorbox}
	
	\paragraph{Power of the one sample $Z$-test and $T$-test}\mbox{}\\\\
	Very generally, in the case of a bilateral test, the above relation will be written:
	
	If the standard deviation of the mean (e.g. variance) is not unitary, we have:
	
	Therefore we get:
	
	written in another way:
	
	It is in this form that we find the power of a bilateral test of the average (power of a $1$ sample $Z$-test):
	
	where $d_\sigma$ is sometimes named the "\NewTerm{size effect}\index{size effect}" and therefore defined by:
	
	and $\delta$ is named the "\NewTerm{difference}"!
	
	It goes without saying that if the true variance is not known, we must be replace the Normal law by the Student law as:
	
	with:
	
	
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Caution with a small recurring trap! The development above corresponds to a $\delta$ which is negative with respect to the first example! The relation is a little bit different in the case where $\delta$ is positive but it does not matter because the power of the test is the same in absolute value!
	\end{tcolorbox}
	
	For the sample size it quite simple. We have by equating the quantiles:
	
	And therefore in bilateral:
	
	where we see that if the power of the test is imposed as being equal to $50\%$, with having therefore $Z_{1-\beta}$ that is $0$ then we fall back (!) on the relation of the sample size for the Normal distribution proved earlier above:
	
	Note also that we sometimes find in the literature the prior-previous relation (the sample size $n$ such that power of the test is at least $1-\beta$) as follows:
	
	Obviously we can set other parameters to determine the value of the remaining variable. We could also, for example, look for the value of the power of the test by fixing the standard deviation, the sample size and the risk threshold, etc.
	
	One reader offered us a very elegant way of finding the same result with much less developments... Indeed, you need only to see in the previous figure that we have:
	
	equality from which we draw immediately an equivalent relation to the two previous one (which obviously gives the same numerical result):
	
	In practice, the sample standard deviation will usually be used in the analysis (thus a $T$-test or interval will be calculated instead of a $Z$-test or interval). In that case, there is greater uncertainty in the estimate, which must be incorporated into the sample size estimate. For the case of testing the mean of a normal population, the relation will be:
	

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The attentive reader will have perhaps noticed that we assumed in the previous developments that the standard deviation of the mean of the null hypothesis and that of the alternative one is implicitly assumed to be the same... In practice this is almost all the time so, this is why almost all statistical software require only one standard deviation to calculate the power of the $1$-Sample $Z$ Test. However, for some rare academic softwares, they ask the standard deviation of the two means. But then the above mathematical developments are obviously different.
	\end{tcolorbox}
	
	A test power analysis may have several facets:
	\begin{enumerate}
		\item We know the level of the test, the sample size and effect size (implicitly the difference) and we seek to calculate its power. This allows us to see if our experimental device is properly calibrated.
		
		\item We know the desired test power, the risk threshold level and effect size to detect. We seek then to calculate the sample size needed to mount an effective experimental design.
		
		\item We know the desired test power, the risk threshold level and sample size and we seek to know the effect size we can hope to highlight.
	\end{enumerate}
	Without exception, we consider as unnecessary to show a test if the expected power is less than $80\%$. This power corresponds to a $80\%$ probability of not rejecting the null hypothesis wrongly or, what remains to the same: a $20\%$ of Type II error.
	
	Obviously, it is possible to make the same reasoning (analytically when possible, otherwise numerically) with absolutely ALL the hypothesis tests that we have seen until now. So as there is a little more than a hundred hypothesis testing in the field of statistics as we have already mentioned... it is obvious that we're not going to have.... fun ... to make the same developments to determine the sample size, effect size and power for all these tests but only for the classics one. As long as we have computers at our disposal with the algorithms integrated by computer scientists, we do not need to redo any developments that would not bring much. Moreover, the majority of statistical softwares can calculate usually the power of only 5 to 10 common tests.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	We will not deal with parametric statistical tests for the detection of outliers in this book as the Q-Dixon's test or Grubb's test and this just because they have too empirical origin and that they have no interest analytically speaking. By cons, if some readers insist, and that we have the time, we can put the details on these tests with detailed algorithms for calculating critical values using a simple spreadsheet and Monte Carlo technique with any distribution of their choice (but not only for data following a Normal law contrary to what is written in most books).
	\end{tcolorbox}
	
	\paragraph{Power of the two sample $Z$-test and $T$-test}\mbox{}\\\\
	This figure below (taken from van Belle) illustrates some important basic concepts that will follow:
	\begin{figure}[H]
		\centering
		\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        
		
		% Pattern Info
		\tikzset{
		pattern size/.store in=\mcSize, 
		pattern size = 5pt,
		pattern thickness/.store in=\mcThickness, 
		pattern thickness = 0.3pt,
		pattern radius/.store in=\mcRadius, 
		pattern radius = 1pt}
		\makeatletter
		\pgfutil@ifundefined{pgf@pattern@name@_n68at9tqf}{
		\pgfdeclarepatternformonly[\mcThickness,\mcSize]{_n68at9tqf}
		{\pgfqpoint{-\mcThickness}{-\mcThickness}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{\pgfpoint{\mcSize}{\mcSize}}
		{
		\pgfsetcolor{\tikz@pattern@color}
		\pgfsetlinewidth{\mcThickness}
		\pgfpathmoveto{\pgfpointorigin}
		\pgfpathlineto{\pgfpoint{0}{\mcSize}}
		\pgfusepath{stroke}
		}}
		
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1467); %set diagram left start at 0, and has height of 1467
		
		%Shape: Polygon Curved [id:ds7167131390068235] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (382.5,350.67) .. controls (382.5,350.67) and (398.99,366.92) .. (442.17,375.33) .. controls (485.34,383.74) and (560.4,385.33) .. (558.17,385.33) .. controls (555.94,385.33) and (382.5,386.4) .. (382,386.2) .. controls (381.5,386) and (382.5,350.67) .. (382.5,350.67) -- cycle ;
		%Shape: Polygon Curved [id:ds9851846559124309] 
		\draw  [pattern=_n68at9tqf,pattern size=2.7750000000000004pt,pattern thickness=0.75pt,pattern radius=0pt, pattern color={rgb, 255:red, 0; green, 0; blue, 0}] (196.83,385) .. controls (199.6,384.6) and (234.8,383.8) .. (250.8,379.8) .. controls (266.8,375.8) and (272,373.8) .. (284,367.8) .. controls (296,361.8) and (314.4,337.8) .. (319.2,329) .. controls (324,320.2) and (345.2,283.4) .. (351.6,268.2) .. controls (358,253) and (382,201) .. (381.6,201) .. controls (381.2,201) and (381.6,387) .. (382,386.2) .. controls (382.4,385.4) and (194.07,385.4) .. (196.83,385) -- cycle ;
		%Shape: Polygon Curved [id:ds4079027563189672] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (41.83,386) .. controls (44.79,384.25) and (55.42,389.33) .. (86.63,379) .. controls (117.83,368.67) and (132.17,349.33) .. (131.83,349.67) .. controls (131.5,350) and (131.83,385) .. (131.83,384.67) .. controls (131.83,384.33) and (38.88,387.75) .. (41.83,386) -- cycle ;
		%Straight Lines [id:da44620879898910815] 
		\draw    (256.5,394) -- (256.5,57) ;
		\draw [shift={(256.5,55)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da764877861086436] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (430.5,55) -- (430.5,394) ;
		%Straight Lines [id:da09901215346642478] 
		\draw  [dash pattern={on 0.84pt off 2.51pt}]  (381.5,130) -- (381.5,420) ;
		%Shape: Brace [id:dp10597228245366419] 
		\draw   (267,420) .. controls (267,424.67) and (269.33,427) .. (274,427) -- (307,427) .. controls (313.67,427) and (317,429.33) .. (317,434) .. controls (317,429.33) and (320.33,427) .. (327,427)(324,427) -- (360,427) .. controls (364.67,427) and (367,424.67) .. (367,420) ;
		%Shape: Brace [id:dp056923655457077116] 
		\draw   (386,421) .. controls (386,425.67) and (388.33,428) .. (393,428) -- (395.75,428) .. controls (402.42,428) and (405.75,430.33) .. (405.75,435) .. controls (405.75,430.33) and (409.08,428) .. (415.75,428)(412.75,428) -- (418.5,428) .. controls (423.17,428) and (425.5,425.67) .. (425.5,421) ;
		%Straight Lines [id:da2898856276857429] 
		\draw    (140,256) -- (181.92,288.77) ;
		\draw [shift={(183.5,290)}, rotate = 218.01] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da14893102276184234] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (257,295) -- (166.5,295) ;
		%Straight Lines [id:da5149800885079385] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (520,295) -- (431.5,295) ;
		%Straight Lines [id:da17474306522905292] 
		\draw    (546,256) -- (508.96,290.63) ;
		\draw [shift={(507.5,292)}, rotate = 316.92] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da5804547076906807] 
		\draw [color={rgb, 255:red, 155; green, 155; blue, 155 }  ,draw opacity=1 ]   (520,295) -- (431.5,295) ;
		%Curve Lines [id:da2947282628658974] 
		\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]    (30.5,386) .. controls (259.13,386.25) and (280.5,388) .. (316.5,333) .. controls (352.5,278) and (362.35,236.35) .. (385.88,190.75) .. controls (409.4,145.15) and (420,138.5) .. (431.38,138) .. controls (442.75,137.5) and (458.75,160.5) .. (464.75,173.5) .. controls (470.75,186.5) and (491.06,231.79) .. (504.88,261.25) .. controls (518.69,290.71) and (538.58,335.08) .. (551.5,348) .. controls (564.42,360.92) and (578.63,374.75) .. (610.13,381.5) ;
		%Curve Lines [id:da9853346503500506] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (30.5,386) .. controls (80.88,385.75) and (117.13,374.5) .. (143.5,333) .. controls (169.88,291.5) and (170.5,285) .. (183.38,255.25) .. controls (196.25,225.5) and (230.13,137.75) .. (256.5,138) .. controls (282.88,138.25) and (298.38,189.25) .. (318.13,228.75) .. controls (337.88,268.25) and (356.36,320.45) .. (378.5,346) .. controls (400.64,371.55) and (454.5,386) .. (613.5,385) ;
		%Straight Lines [id:da7603647894440799] 
		\draw    (25.83,385.67) -- (629.5,386) ;
		\draw [shift={(631.5,386)}, rotate = 180.03] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		
		% Text Node
		\draw (251,398.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (424,398.4) node [anchor=north west][inner sep=0.75pt]    {$\delta $};
		% Text Node
		\draw (568,392.4) node [anchor=north west][inner sep=0.75pt]    {$\overline{y}_{0} -\overline{y}_{1}$};
		% Text Node
		\draw (260,432.4) node [anchor=north west][inner sep=0.75pt]    {$0+Z_{1-\alpha /2} \sigma \sqrt{\dfrac{2}{n}}$};
		% Text Node
		\draw (393,432.4) node [anchor=north west][inner sep=0.75pt]    {$\delta -Z_{1-\beta } \sigma \sqrt{\dfrac{2}{n}}$};
		% Text Node
		\draw (71,342.4) node [anchor=north west][inner sep=0.75pt]    {$\alpha /2$};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (410,338) -- (448,338) -- (448,362) -- (410,362) -- cycle  ;
		\draw (413,342.4) node [anchor=north west][inner sep=0.75pt]    {$\alpha /2$};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (344,338) -- (362,338) -- (362,362) -- (344,362) -- cycle  ;
		\draw (347,342.4) node [anchor=north west][inner sep=0.75pt]    {$\beta $};
		% Text Node
		\draw (191,23.9) node [anchor=north west][inner sep=0.75pt]    {$H_{0} :\mu _{0} -\mu _{1} =0$};
		% Text Node
		\draw (378,23.9) node [anchor=north west][inner sep=0.75pt]    {$H_{1} :\mu _{0} -\mu _{1} =\delta $};
		% Text Node
		\draw (358,94) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{33.89pt}\setlength\topsep{0pt}
		\begin{center}
		Critical\\value
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (97,215.4) node [anchor=north west][inner sep=0.75pt]    {$\text{SE} =\sigma \sqrt{\dfrac{2}{n}}$};
		% Text Node
		\draw (517,215.4) node [anchor=north west][inner sep=0.75pt]    {$\text{SE} =\sigma \sqrt{\dfrac{2}{n}}$};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (393,261) -- (499,261) -- (499,287) -- (393,287) -- cycle  ;
		\draw (396,265) node [anchor=north west][inner sep=0.75pt]   [align=left] {Power$\displaystyle =1-\beta $};
		
		\end{tikzpicture}
	\end{figure} 
	Each curve represents the difference between two groups on some continuous measurement $\left(\Delta=y_{0}-y_{1}\right)$. The null hypothesis, that there is no difference between the two groups, is represented by the curve on the left $\left(\mu_{\Delta}=0\right)$. The alternative hypothesis, that there is some difference between the two groups, is represented by the curve on the right $\left(\mu_{\Delta}=\delta\right)$. Each distribution is also characterized by its variance $\left(\sigma^{2}\right)$ which is usually assumed to be the same (or similar enough...) for both distributions. The relationship between the standard error of the differences in means and the standard deviation\footnote{This is based on: $\sqrt{\frac{\sigma^{2}}{n_{1}}+\frac{\sigma^{2}}{n_{2}}}=\sigma \sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}\cong\sigma \sqrt{\frac{2}{n}}$}:
	
	 allows us to set up calculations for the sample size, $n$. The term $\sigma \sqrt{\frac{2}{n}}$ derives from the standard error for the difference between two means with equal variances.
	 
	The reader can also see the (conditional) probabilities about test results and true underlying differences that make up the classical components of sample size calculations:
	\begin{itemize}
		\item type I error or $\alpha=P(+$ result $\mid$ - difference), ie. false positive
			\begin{itemize}
				\item the traditional black-shaded $0.05 / 2$ tails on the null hypothesis
			\end{itemize}
		\item type II error or $\beta=P($ result $\mid$ + difference), i.e. false negative
			\begin{itemize}
				\item the traditional hatched $0.2$ left-handed tail on the alternative hypothesis
			\end{itemize}
		\item power or $1-\beta=P(+$ result $\mid+$ difference), i.e. true positive
			\begin{itemize}
				\item the area to the right of $\beta$ on the alternative hypothesis
			\end{itemize}
	\end{itemize}
	Finally, you can also see that you can increase the power of a study without increasing the sample size by:
	\begin{enumerate}
		\item Measuring your outcome more precisely, which decreases the variance or "shrinks" the two curves towards their respective means and increases the area under the curve to the right of the critical value, or
		
		\item Studying a greater effect size, in this case the difference between $\mu_{\Delta}=0$ and $\mu_{\Delta}=\delta$, which separates the curves from each other and similarly increases the area under the curve to the right of the critical value
	\end{enumerate}
	Returning to the figure above, the critical value is the point where the upper value of $\alpha$ on the null curve and the value for $\beta$ on the alternative curve meet. This defines the point at which you accept or reject your null hypothesis, and sets up the sample size equation as:
	
	Solving for $n$ allows us to calculate sample sizes "\NewTerm{sample size for difference of homoscedastic means}\index{sample size!for difference of homoscedastic means}":
	
	Substituting the usual values of $1.96$ for $\alpha$ and $0.84$ for $\beta$, and defining the denominator as the square of the standardized difference $\left(\Delta^{2}\right)$, yields the rule of thumb called "\NewTerm{Lehr's equation}\index{Lehr's equation}" or the "\NewTerm{Rule of $16$}\index{rule of $16$}" :				
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Say, for example, you want to demonstrate a 10 point difference in IQ between two groups, one of which is exposed to a potential toxin, the other of which is not. Using a mean population IQ of 100, and a standard deviation of 20 :
	$$n=\frac{16}{(100-90 / 20)^{2}}=\frac{16}{(0.5)^{2}}=64$$
	or a total sample size of $128$.
	\end{tcolorbox}
	We can re-arrange Lehr's equation to solve for the detectable standardized difference between two populations:
	$$n=\frac{16}{(\Delta)^{2}} \Leftrightarrow \Delta=\frac{4}{\sqrt{n}}$$
	For the unstandardised difference:
	$$
	\delta=\frac{4 \sigma}{\sqrt{n}}
	$$
	So, for example, we were interested in knowing the difference in mean IQ we could detect with a sample size of $50$. Applying the above relation we get about $12$ IQ points.
	
	In practice, the sample standard deviation will usually be used in the analysis (thus a $T$-test or interval will be calculated instead of a $Z$-test or interval). In that case, there is greater uncertainty in the estimate, which must be incorporated into the sample size estimate. For the case of testing the mean of a normal population, the formula is now:
	
	
	\paragraph{Power of the one and two samples $P$-test}\mbox{}\\\\
	Same as with the confidence interval of the Normal distribution with known theoretical standard deviation (that is to say the entire population), we can determine the number of individuals (sample size) for a fixed test power for the 1 sample proportion test studied in detail earlier. For this, we use the same technique as for the power of the one sample $Z$-test. We write so at first:
	
	Hence we deduce:
	
	So if the power is $50\%$ we fall back on:
	
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{img/arithmetics/one_proportion_output.png}
		\caption{Power analysis table of one proportion test from PSS software}
	\end{figure}
	
	For the power of the test of the difference of two proportions (two sample proportion test), with the objective of determining the sample size, we have to put $n=n_1=n_2$. Therefore the developments we get during our study of the test of the difference of two proportions can be written:
	
	with therefore:
	
	In the same way that we did for the $Z$-test and the one sample $p$-test, we have:
	
	Therefore:
	
	So what it takes us to assume that the real difference between the two proportions is the average (which is debatable...).
	
	But we also have (as the samples are independent and using the property of the variance):
	
	Therefore:
	
	That gives obviously:
	
	We have then after rearrangement:
	
	The \texttt{R} software has a function named \texttt{power.prop.test( )} that helps us play with the above relation. 
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	You might find other relations in the literature for the sample size of the one sample or two sample proportions tests. But however be careful...!!! Sadly some textbooks calculates the $n$ based on how to get a given confidence interval, and not on how to get a given power (hence the absence of $Z_\beta$ the in the corresponding relations). So this has nothing to do with the sample size to get a given power but instead what is the sample size to do $p$-hacking.......
	\end{tcolorbox}
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.15]{img/arithmetics/how_to_run_a_good_study.jpg}
	\end{figure}
	
	Now consider that the power is not given to find the sample size, but that we have the sample size and we want to computer the power!
	
	As companion example consider that we are trying to test whether the sex ratio of some sampled individuals significantly differs from the expected sex ratio of $1$ assuming we have $n= 64$, of which female=$34$ and male=$30$.
	
	In order to find the power, we need to have a specific alternative in mind. Suppose that null hypothesis is $H_{0}: p=0.5$ vs. $H_{1}: p>0.5$, where $p=P(\text{Female})$. Also suppose we have $n=64$ and you want the power of a test at level $\alpha=0.05$ against the specific alternative $p=0.6$.
	
	For an exact binomial test, we need to first to find the critical value $c$ such that $P(X \leq c \mid n=64, p=0.5)$ is maximized, but still below $0.95$. With any statistical software or even spreadsheet software we can find that this value $c=40$ (cautious! many statistical softwares do a Normal approximation of the binomial law in such a power calculation...). Then the power of this test against the alternative value $p=0.6$ is given by $P(X \leq 40\,|\,n=64, p=0.6) = 0.3927$.

	
	\paragraph{Power calculation based on simulations}\mbox{}\\\\
	In more complicated models it is not possible to determine an explicit expression for the power of a test and calculation of power based on simulations can be an attractive approach. Sometimes it may also just be easier to let the computer do the job by running simulations rather than to get bugged down in derivations of explicit expressions for power even though they may in fact be possible to derive.

	Recall that power is the probability of getting a significant result when there is in fact a difference, thus in the long run it is the proportion of significant results to the total number of tests:
	
	We can let the computer generate random data from the model under the alternative hypothesis and then perform the significance test. We can even do that many many times and record the $p$-values allowing us to calculate the power via the relation above.
	
	\pagebreak
	\subsubsection{$A$/$B$ tests}
	"\NewTerm{$A$/$B$ testing}\index{$A$/$B$ test}" (also named "\NewTerm{bucket tests}" or "\NewTerm{split-run testing}") is a randomized experiment with two variants, $A$ and $B$. It includes application of statistical hypothesis testing or "two-sample hypothesis testing" as used in the field of statistics. $A$/$B$ testing is a way to compare two versions of a single variable, typically by testing a subject's response to variant $A$ against variant $B$, and determining which of the two variants is more effective.
	
	Booming in business and a staple analysis in medical trials, the $A$/$B$ tests assesses the effect of an intervention or treatment by comparing its success rate with that of a control condition. As the name implies, two versions ($A$ and $B$) are compared, which are identical except for one variation that might affect a user's behaviour. Version $A$ might be the currently used version (control), while version $B$ is modified in some respect (treatment). For instance, on an e-commerce website the purchase funnel is typically a good candidate for $A$/$B$ testing, as even marginal improvements in drop-off rates can represent a significant gain in sales. Significant improvements can sometimes be seen through testing elements like copy text, layouts, images and colors, but not always.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	When running an $A$/$B$ tests on something and regularly check ongoing experiments for significant results, you must be careful to not fall into the trap that statisticians call "\NewTerm{repeated significance testing errors}\index{repeated significance testing errors}". Indeed, the significance calculation makes always critical assumption that we have that the sample size fixed in \underline{advance}! If we wait and the sample size is too big, we fall again in the $p$-hacking case where the sample size is too big and then creates a false significant test.
	\end{tcolorbox}
	Let us see three typical $A$/$B$ tests by ascending order of mathematical complexity (the first one may be considered as a "frequentist test" and the two others as "bayesian tests").
	
	\paragraph{Naive proportion approach}\mbox{}\\\\
	The most naive $A/B$ test approach is completely based on the one-proportion $p$ test" that is given for reminder (see page \pageref{proportion confidence interval})  by:
	
	but where the notation is traditionally changed as following:
	
	where CR is the abbreviation of "conversion rate"\index{conversion rate}.
	
	Practitioners of that naive test also define the "\NewTerm{uplift}\index{uplift}" as being simply the relative increase in conversion rate between Group $A$ and Group $B$. It is possible to have negative uplift if our Control group is more effective than the new Test group. Here is the expression:
	
	Obviously using the standard error measure of the one proportion $p$ test and $Z$-value we can also compute the minimum and maximum expected uplift within a specific level of confidence if needed!
	
	\paragraph{Fieller's test (ratio of two means)}\index{statistical tests!Fieller's A/B test}\mbox{}\\\\
	The purpose of the "\NewTerm{Fieller's test}" is the calculation of a confidence interval for the ratio of two means. Its is an especially trendy test in the field of webdesign when two design or user interface of a same website (or the number of clicks) have to be compared. The Fieller's test is also commonly known under the name "\NewTerm{A/B test for means}\index{A/B test for means}".
	
	Let us consider $X$ and $Y$ two random variables Normally distributed representing averages of samples of size $n$ and $m$ respectively and having same variance, such that:
	
 	So this is a homoscedastic case and then in practice we take the global standard deviation.

	In reality, however, we will have access only to the estimators:
	
 	What interests us is to build a confidence interval for the ratio:
	
 	And this is the purpose of Fieller's test obviously!

The idea is then to compute and to group the two random variables into one:
	
 	which by the null hypothesis will therefore follow a random variable of the type:
	
 	We then have in extenso:
	
 	If the standard deviation can only be estimated, we know that we then have:
	
 	It follows, of course, that:
	
	By symmetry of the Student's law we can therefore write that:
	
	After rearrangement, we get:
	
	There are now two interesting situations to consider depending on the sign of the coefficient $\theta^2$ but let's put it aside for now concentrating only on the roots of the polynomial in the parenthesis. We then have (\SeeChapter{see section Calculus page \pageref{second order polynomial roots}}):
	
	This is simplified immediately into:
	
 	The development in the root gives:
	 
	What can be rewritten in a form that will be useful to us:
	 
	Let us put:
	
 	Our roots can then be written:
	
 	or:
	
	or otherwise:
	
	We do a last simplification:
	
	Who are therefore for recall the roots of:
	
 	The coefficient of $\theta^2$ in the polynomial is $Y^2-bS^2T_{\alpha/2}^2(k)$. Therefore:
	\begin{itemize}
		\item If the latter is greater than zero, then the parabola is concave (seen from above) and the confidence interval is then between the two roots of the parabola (because the values there are negative according to the imposed inequality).

		\item If the coefficient of $\theta^2$ in the polynomial is negative then the parabola is convex (seen from above) and the confidence interval is then outside the two roots of the parabola because the values there are negative according to the imposed inequality).

		\item If the coefficient of $\theta^2$ has a near zero probability of being equal to zero we can ignore this scenario.
	\end{itemize}
	Obviously only the first case interests us because the second is difficult to interpret (the intervals would have the infinite bounds which is unrealistic and especially useless). So we must have:
	
	What can be written:
	
	Therefore:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us consider the case where we have:
	
	Therefore:
	
	We have:
	
	and:
	
	As well as (we give this calculation because some statistical software - \texttt{R} in particular - given them in the output while normally it is only useful for Student's t test of two independent homoscedastic samples):
	
	We then have the following interval between the two roots of the parabola:
	
	\end{tcolorbox}
	We can go a little further by considering the case where:
	
	Either within the estimated framework:
	
 	This gives us the following distribution (assuming Normality) for the random variable grouping the first two according to $X-\theta Y$:
	
	We then  have:
	
 	And therefore, as before, we can write:
	
	What we can rewrite in the following form:
	
 	What developed gives:
	
	We group together the terms as earlier above:
	
 	We then have for roots after simplification of the numerical factors:
	
 	After the simplification method seen above (pfff...) we finally get:
	
	
	\paragraph{Bayesian $A$/$B$ testing for binary outcomes}\label{A/B testing for binary outcomes}\index{statistical tests!Bayesian A/B test}\mbox{}\\\\
	For a binary-outcome test (e.g. a test of conversion rates), the probability that $B$ will beat $A$ in the long run is given by\footnote{The derivation and text mainly comes from \url{https://www.evanmiller.org/bayesian-ab-testing.html}.}:
	
	where:
	\begin{itemize}
		\item $\alpha_A$ is one plus the number of successes for $A$
		\item $\beta_A$ is one plus the number of failures for $A$
		\item $\alpha_B$ is one plus the number of successes for $B$
		\item $\beta_B$ is one plus the number of failures for $B$
		\item $B$ is the beta function as seen on page \pageref{beta function} (keep in mind we must have $a>0,b>0$):
		
	\end{itemize}
	\begin{dem}
	The beta distribution is a convenient prior distribution for modelling a binomial parameter $p$ (as we know that this is one of the rare distribution that has a domain of definition in $[0,1]$). Starting from a non-informative prior, the distribution of $p$ after $S$ successes and $F$ failures is given by $B(S+1,F+1)$. For the remainder of the discussion let $\alpha=S+1$ and $\beta=F+1$, which are just the two parameters of the beta distribution for the belief.
	
	Now suppose we have two experimental branches ($A$ and $B$) and have a Bayesian belief for each one:
	
	Using the probability density function of the beta distribution as prior for $p$, we can get the total probability that $p_B$ is greater than $p_A$ by integrating the joint distribution over all values for which $p_B$>$p_A$:
	
	sometimes named the "\NewTerm{Euler's hypergeometric integral}\index{Euler's hypergeometric integral}".
	
	Evaluating the inner integral:
	
	 the relation becomes:
	
	Where $I_x$ is the incomplete beta function (\SeeChapter{see section Statistics page \pageref{incomplete Beta function}}). Using the identity $I_x(1,b)=1-(1-x)b$, the recursive relationship:
	
	and the fact that $\alpha$ and $\beta$ are integers, we can express $I_x$ as:
	
	Or equivalently:
	
	Our probability integral can therefore be written:
	
	Finally:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	The reader must know that even if there exist a closed formula, a the day we write these lines, most statistical software implement numerical methods (simulations) to calculate this probability!
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} If we want to assume that each value of $p$ has the same probability, i.e. we have no out-of-data (prior) information that we would like to include in our model, then we can set $\alpha_A=\beta_A=\alpha_B=\beta_B$, that means a uniform distribution for all the values of $p$ in $[0,1]$ interval. \\
	
	\textbf{R2.} The reader can refer to our \texttt{R} companion free e-book to see how to apply these equations easily on practical cases with a free open-source software and code!
	\end{tcolorbox}
	
	\paragraph{Bayesian $A$/$B$ testing for count datas}\label{A/B testing for count datas}\mbox{}\\\\
	Analysing count data (for example, if you're comparing the number of sales per salesman, or number of sales week over week) requires a different relation. The probability that group $1$ has a higher arrival rate than group $2$ is given by:
	
	A derivation and implementation follow; they both closely mirror the binary-outcome case.
	
	\begin{dem}
	Let $\lambda_1$ and $\lambda_2$ be the Poisson parameter for each group. With a gamma-distributed prior belief (\SeeChapter{see section Statistics page \pageref{gamma distribution}}), the posterior beliefs are given by:
	
	Using the probability density function of the gamma distribution, we can get the total probability that $\lambda_1$ is greater than $\lambda_2$  by integrating the joint distribution over all values for which $\lambda_1>\lambda_2$:
	
	
	Evaluating the inner integral, the equation becomes:
	
	Where $Q$ is the upper incomplete regularized gamma function (\SeeChapter{see section Differential and Integral Calculus page \pageref{incomplete regularized Gamma function}}). Using the identity $Q(1,z)=e^{-z}$ and the recursive relationship:
	
	we can express $Q$ as (\SeeChapter{see section Differential and Integral Calculus page \pageref{incomplete regularized Gamma function}}):
	
	The probability integral can therefore be written (we used the integral already seen during our study of the Gamma function at page \pageref{gamma euler function}):
	
	Using $\Gamma(z+1)=z\Gamma(z)$, we have:
	
	Replacing the gamma functions with a beta function (see page \pageref{beta function}), we have:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Again, the reader can refer to our \texttt{R} companion free e-book to see how to apply these equations easily on practical cases with a free open-source software and code!
	\end{tcolorbox}
	
	\pagebreak
	\subsubsection{Analysis Of VAriance (ANOVA)}\index{statistical tests!ANOVA tests}\label{anova}
	"\NewTerm{Analysis Of VAriance}\index{analysis of variance}" is a collection of statistical models used to analyse the differences among group means and their associated procedures (such as "variation" among and between groups), developed by statistician and evolutionary biologist Ronald Fisher. In the ANOVA setting, the observed variance in a particular variable is partitioned into components attributable to different sources of variation.
	
The main techniques of analysis of variances are the following one (some of them are detailed further below) in the order of most to less used one (and in increasing order of complexity):
	\begin{multicols}{2}
		\begin{itemize}
		\item Two variable one-way fixed factor ANOVA (Student $T$-test)\footnote{There is a nonparametric rank-transformed equivalent named Friedman test that we will see further below.}
		
		\item One-way fixed factor ANOVA ($n$ independent fixed variables)\footnote{There is a nonparametric rank-transformed equivalent named Kruskal-Wallis where the post-hoc will be typically the Mann-Whitney $U$ test that we will all see further below.}
		
		\item Two-way fixed factor ANOVA with/without repetitions\footnote{In facts for all ANOVA there exist a mathematical variant for repeated measurements.}
		
		\item Multifactor ANOVA with/without repetition 
		
		\item Repeated Measures ANOVA (RM-ANOVA)\footnote{Repeated measures ANOVA is the equivalent of the one-way ANOVA, but for related, not independent groups, and is the extension of the dependent $T$-test. A repeated measures ANOVA is also referred to as a within-subjects ANOVA or ANOVA for correlated samples.}
		
		\item Block\footnote{There is a variant where blocked are considered as fixed or random... In facts the block is considered as an another factor. Then it's just a two-way ANOVA} (randomized) design (ANOVA in blocks)
		
		\item Latin square ANOVA
		
		\item Greaco-Latin square ANOVA
		
		\item Youden square ANOVA\footnote{These are incomplete Latin squares, in which the number of columns is not equal to the number of rows.}
		
		\item Split-Plot ANOVA
		
		\item Split-Block ANOVA
		
		\item Strip-Plot ANOVA
		
		\item Split-Split-Plot ANOVA
		
		\item Analysis of Covariance (ANCOVA)
		
		\item Nested (Hierarchical) analysis of variance (HANOVA)
		
		\item Partially Nested ANOVA (PANOVA)
		
		\item Multivariate analysis of variance (MANOVA)
		
		\item Random effects ANOVA
		
		\item Mixed effects (with fixed and random factors) ANOVA
		
		\item Cross-Over ANOVA
		
		\item Robusts ANOVA (by permutations, M-estimators based)
			\item ...
		\end{itemize}
	\end{multicols}
	and some of the ANOVA above are said "balanced" or "unbalanced", depending if the number of measurement are equal or not equal for each variable and therefore can be calculated with different types of sum of square errors in the unbalance case, named "Type I sum of squares", "Type II  sum of squares" or "Type III sum of squares". So we can say that there is at least between $20$ and $60$ types of different ANOVA\footnote{For more details see the Data Science mind map at page \pageref{mindmap of data science}.} (including the special variations but without including the randomized/bootstrapped versions...)!

	It must also be known that the terminology of ANOVA is largely from the statistical design of experiments (\SeeChapter{see section Industrial Engineering page \pageref{doe}}). The experimenter adjusts factors and measures responses in an attempt to determine an effect. Factors are assigned to experimental units by a combination of randomization\footnote{In the pharmaceutical field (mainly!), we use randomization to make the treated groups (if they are sufficiently large and comparable) and untreated groups comparable in terms of potential severity factors, which makes it possible to neutralize the differences due to the factors of severity, in order to isolate the effect of the treatment. It should be noted that randomization does not only neutralize known factors but also \underline{unknown} severity factors!} and blocking to ensure the validity of the results.

	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	For the reader interested to have a deeper overview, however without too much mathematical details, at the opposite of the present book, we recommend the following actual worldwide reference for experts in ANOVA and Design of Experiments: \textit{Design and Analysis of Experiments}\footnote{Three volume set!!!} (authors: Hinkelmann, K. and Kempthorne, O.), approximately $730.-$ USD and $1,800$ pages (see \cite{hinkelmann2007design},\cite{hinkelmann2005design} and \cite{hinkelmann2012design}) and \textit{Applied Linear Statistical Models} (see \cite{kutner2005applied}).
	\end{tcolorbox}
	
	The purpose of this section will mainly have for purpose to avoid the following type of telephone conversation between an ANOVA specialist (S) and a research scientist or engineer (R). R: «Hello, Mr. Stat, I wonder whether you have just a minute for a quick statistical question.» S: «Usually I do not do statistical consulting over the phone, but let me see if I can help you. What is the problem?» R: «We are developing new growth media for industrial producers for growing flower plants. We have three such media and we use them with four flower varieties. We have five replications for each combination of medium and flower. We have analysed the data as a $3\times 4$ two-way classification with five observations per cell. But my graduate assistant has talked to one of your students and he is now confused about the validity of this analysis. I just want you to confirm that we have done the right thing.» S: «Well, I do not know.» R: «What do you mean, you do not know? You are the expert!?» S: «I really need to know more about how you performed the experiment. For example, how did you prepare the media that you used in the individual pots? I assume that you grow the flowers in pots in the greenhouse.» R: «Yes, that is right. My graduate assistant simply mixed each medium in a big container, which we then put in the individual pots.» S: «That may be a problem, because now you may not have any replication.» R: «What do you mean, we have no replication? I just told you that we have five replications.» S: «Yes, but ... I think it would be best if you would come to my office for me to explain this to you and to take a closer look at your experiment.» R: «But we have already submitted the paper for publication.» S: «Then why don’t you come when you get the reviews back.» Silence. R: «Yes, thank you. I’ll do that...»

This is, of course, just a fictitious conversation\footnote{Another typical real example can be found on the following YouTube video: \url{https://vimeo.com/815286997}}. But many consulting statisticians have had similar conversations (most of the time the paper was already rejected). The aim of this section is to help engineers not only to better understand each other but also to obtain a better understanding of the intricacies of designing and analysing experiments.
	
	\pagebreak
	\paragraph{ANOVA with one fixed factor}\label{anova one way fixed factor}\mbox{}\\\\
	The objective of the analysis of variance (at the contrary to what its name might suggest...) is a statistical technique for comparing the means of two or more populations and that is very widely used in the pharmaceutical field or in the R\&D labs or benches test\footnote{In the case of "random factor" (concept that we will see later!) however we instead compare variances....!}. This method, however, takes its name from that it use measures of variance to determine the statistical significance, or not, of the differences of measured averages on populations or samples.
	
	More precisely, the real meaning is to know if the fact that the sample averages are (slightly) different can be assigned at random sampling or due to the fact that a variability factor actually generates significantly different samples (if we have the population size, we have obviously not such question!). 
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	For more information about the vocabulary and application, the engineer and the researcher should refer to standard ISO 3534-3:1999.
	\end{tcolorbox}
	For the analysis of variance named also "\NewTerm{one factor ANOVA}\index{ANOVA!one factor ANOVA}" (Analysis Of VAriance\index{ANOVA!analysis of variance}) or "\NewTerm{one factor ANAVAR}\index{ANOVA!one factor ANAVAR}" (ANAlysis of VARiance), or "\NewTerm{one-way ANOVA}\index{ANOVA!one-way ANOVA}" or more rigorously "\NewTerm{one fixed factor ANOVA with repetitions}\index{ANOVA!one fixed factor ANOVA with repetitions}" or "\NewTerm{ANOVA with one fixed categorical variable with repetition}\index{ANOVA!one fixed categorical variable with repetition}" or "\NewTerm{complete random design}\index{ANOVA!complete random design}" (CRD), we first recall, as we proved, that the Fisher-Snedecor law is given by the ratio of two independent random variables that follow a Chi-square law and divided by the degree of freedom such as:
	
	and we will now see its importance.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	When a factor can have a very large number of levels we consider having chosen some given levels of the factor among a multitude of possible as a random selection. This is why we speak then in this latter case of "random factor" which is the subject of specific ANOVA models that we will study once those on fixed factors mastered (e.g. ANOVA mixing fixed and random factors are named "\NewTerm{mixed ANOVA}\index{ANOVA!mixed ANOVA}").
	\end{tcolorbox}
	Let us consider a random sample of size $n$, say $X_1,X_2, \ldots ,X_n$ sampled from the law $\mathcal{N}(\mu_X,\sigma_X)$ and a random sample of size $m$, say $Y_1,Y_2, \ldots ,Y_m$ sampled from the law $\mathcal{N}(\mu_Y,\sigma_Y)$.
	
	Consider now the maximum likelihood estimators of the standard deviation of the Normal law traditionally noted in the field of analysis of variance:
	
	The above statistics are those that we could use to estimate the variances if the average theoretical $\mu_X,\mu_Y$ were known. So we can use a result proved above in our study of confidence intervals:
	
	As the $X_i$ are independent of the $Y_j$ (hypothesis that suppose the covariance to be zero, the converse being for reminder not always!), the random variables:
	
	are independent from each other.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	There is an existing type of ANOVA for the case where the variables are not independent (we then speak about "\NewTerm{covariate}\index{covariate}"). This is the "\NewTerm{ANCOVA}\index{ANCOVA}" which means "\NewTerm{ANalysis of COvariance and VAriance}\index{analysis of covariance and variance}" that uses a mix of linear regression (\SeeChapter{see section Numerical Methods page \pageref{univariate regression variance analysis}}) and ANOVA. The purpose of the ANCOVA is to statistically eliminate the indirect effect of the covariate. We will see much later how it works in details.
	\end{tcolorbox}
	We can therefore apply the Fisher-Snedecor law with:
	
	Therefore we get:
	
	Finally:
	
	This theorem allows us to deduce the confidence interval of the ratio of two variances when the theoretical mean is known. Since Fisher function is not symmetrical, the only possibility to make the inference is to use numerical calculation and then we will denote for a given confidence interval the test as follows:
	
	In the case where the averages $\mu_X,\mu_Y$ are unknown, we use the unbiased estimators of the variances noted traditionally in the field of analysis of variance:
	
	To estimate the theoretical variances, we use the result proved above:
	
	As the $X_i$ are independent of the $Y_j$ (assumption!), the variables:
	
	are independent from each other. We can therefore apply the Fisher-Snedecor distribution:
	
	Therefore we get:
	
	Finally:
	
	This theorem allows us to deduce the confidence interval of the ratio of two variances when the sample mean is known. Since Fisher function is not symmetrical, the only possibility to make the inference is to use numerical calculation and then we will denote for a given confidence interval the "\NewTerm{Fisher $F$-test}\index{Fisher $F$-test}" as follows:
	
	keeping in mind that its use implicitly requires constraints of Normality of the studied variables.
	
	Rondal A. Fisher (11890-11962 according to holocene calendar) is, as Karl Pearson (11857-11936 according to holocene calendar), one of the main founders of the modern theory of statistics. Fisher studied at Cambridge, where he obtained in 11912 (holocene calendar) a degree in astronomy. It is by studying the theory of error in astronomical observations that Fisher became interested in statistics. Fisher is the inventor of the branch of statistics named "analysis of variance".
	
	In the early 120th century (holocene calendar), Rondald A. Fisher therefore develops the very important methodology of experimental design (\SeeChapter{see section Industrial Engineering  page \pageref{doe}}). To confirm the usefulness of a factor, he developed a test to ensure that the different samples are of different natures. This test is based on the analysis of variance (of the samples), and named also ANOVA but for "\NewTerm{normalized analysis of variance}\index{normalized analysis of variance}".
	
	Let us take $k$ samples of $n$ random values each. Each of the values being considered as an observation or measurement of something or on the basis of something (a different place or a different object... in short: a single factor of variability between samples!). We will have a total of $N$ observations (measurements) given by:
	
	If each sample has an equal number of values $n$ (sample size) such that $n_1=n_2=\ldots =n_k$ we then speak of "\NewTerm{balanced plan}\index{balanced plan}" or "\NewTerm{balanced design}\index{balanced design}" with $k$ levels (or $k$ modalities).
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	If we have more factors of variability (e.g. each place compares himself to different laboratories), then we will talk about "\NewTerm{multifactorial ANOVA}\index{multifactorial ANOVA}". Therefore, if there are only two sources of variability, we talking about "\NewTerm{two factor ANOVA}\index{two factor ANOVA}" (see below for details on the various of two-factor ANOVA).
	\end{tcolorbox}
	We will consider that each of the $k$ samples is sampled (follows) a random variable following Normally distributed:
	\begin{table}[H]
		\centering
		\begin{tabular}{ |l|l|l|l| }
			\hline
			\rowcolor[gray]{0.75}\multicolumn{4}{ |c| }{Factor $1$} \\
			\hline
			\rowcolor[gray]{0.75}Sample $1$ & Sample $2$ & Sample ...$i$ & Sample $k$ \\ \hline
			 $x_{11}$ & $x_{12}$ & $\ldots$ & $x_{1k}$ \\
			 $x_{21}$ & $x_{22}$ & $\ldots$ & $x_{2k}$ \\
			 $\ldots$ & $\ldots$ & $x_{ij}$ & $\ldots$ \\
			 $x_{n1}$ & $x_{n2}$ & $\ldots$ & $x_{nk}$ \\ \hline
			 Average: $\bar{x}_1$ &  Average: $\bar{x}_2$  &  Average: $\bar{x}_i$ & Average: $\bar{x}_k$ \\
			\hline
		\end{tabular}
		\caption{Typical "cross" table of a one factor ANOVA}
	\end{table}
	Therefore in a one-way analysis of variance (one-way ANOVA), a single factor (treatment) with several levels is considered.
	
	In terms of testing, we test whether the mean of the $k$ samples of size $n$ are equal under the assumption that their variances are equal. What we write as under hypothesis notation as follows:
	
	In other words, the samples are representative of the same population (i.e. of a same statistical law). That is to say, the variations between the values of different samples are essentially due to probabilities. For this we study the variability of results in the samples and between samples. 
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The statistical model of the observations can also be written in the following very important form, named a "\NewTerm{factor effects model}\index{factor effects model}" or even sometimes a "\NewTerm{classificatory linear model}":
	
	The parameter $\mu$ is the "\NewTerm{overall mean}", $\tau_i$ is the $i$th "\NewTerm{treatment effect}", and $\varepsilon_{ij}$ is a "\NewTerm{random error}". It is assumed that $\varepsilon_{ij}$'s are i.i.d $\mathcal{N}(0,\sigma^2)$. This is a completely additive model (there is no interaction!). Under this form the hypotheses are then:
	
	Notice that we have therefore:
	
	\end{tcolorbox}		
	It is exactly the same as we will see later as writing (formulation found in some publications or books):
	
	So we will for the rest denote by $i$ the index number of the sample ($1$ to $k$) and $j$ the index of the observation (from $1$ to $n$). Therefore $x_{ij}$ will be the value of the $j$-th observation of the data sample number $i$ (we chose to reverse the usual notation so be careful not to mislead afterwards ... we're sorry ... it was bad choice me made when we started to write this book!).
	
	According to the above hypothesis, we have:	
	
	We will denote by $\bar{x}_i$ the empirical/estimated (arithmetic) average of the sample $i$ (often named "\NewTerm{marginal average}\index{marginal average}" and sometimes denoted also $\bar{x}_{i.}$):
	
	and $\bar{\bar{x}}$ (also sometimes denoted $\bar{x}_{..}$) the empirical/estimated (arithmetic) average of the $N$ values (therefore the average of the $\bar{x}_i$) is then given by:
	
	Using the properties of the mean and variance already proved above we know that:
	
	with $\mu$ which is the average of the true averages (expected means) $\mu_i$:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Writing the model as factor effect model:
	
	we see that the maximum likelihood estimators of the model parameters are $\hat{\mu}=\bar{\bar{x}}$ and $\hat{\tau}_i=\bar{x}_i-\bar{\bar{x}}$. Hence:
	
	\end{tcolorbox}
	Let us now introduce three important  variances:
	\begin{enumerate}
		\item The "\NewTerm{total variance}\index{total variance}" as being intuitively the estimated unbiased variance unbiased considering the set of $N$ observations as a single sample:
			
			where the numerator is named "\NewTerm{sum of the squares of total differences}\index{sum of the squares of total differences}".
			
			\item The "\NewTerm{variance between samples}\index{variance between sample}" (that is to say between the averages of the samples) is also intuitively the variance estimator of the averages of samples:
			
			where the numerator term is named  "\NewTerm{sum of the squared differences between samples}\index{sum of the squared differences between samples}".
			
			As we have proved that if all variables are identically distributed (same variance and same mean) and independent the variance of individuals is $n$ times that of the average:
			
			then the "\NewTerm{variance of observations}\index{variance of observations}" (random variables in a sample) is given by:
			
			We then have therefore above the hypothesis of equality of variances that is expressed in mathematical form for the developments that will follow.
			
			\item The "\NewTerm{residual variance}\index{residual variance}" is the effect of the "uncontrolled factors". It is by definition the average of the sample variances (its like: standard error):
			
			where the numerator term is named "\NewTerm{sum of squared residuals errors}\index{sum of squared residual errors}" or even more often "\NewTerm{residual errors}\index{residual errors}".
	\end{enumerate}
	Finally, these indicators are sometimes summarized as follows:
	
	Note that if the samples do not have the same size (which is rare in practice), then we have:
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} The term $Q_T$ is often indicated in the industry by the acronym SST meaning in "\NewTerm{Sum of Squares Total}\index{total sum of squares}" or more rarely TSS for "\NewTerm{Total Sum of Squares}".\\
	
	\textbf{R2.} The term $Q_A$ is often indicated in the industry by the acronym SSB meaning in "\NewTerm{Sum of Squares Between (samples)}\index{sum of squares between samples}" or more rarely SSk for "\NewTerm{Sum of Squares Between treatments}\index{sum of squares between treatments}" (hence sometimes denoted also: $\text{SS}_\text{tr}$).\\
	
	\textbf{R3.} The term $Q_R$ is often indicated in the industry by the acronym SSW meaning in  "\NewTerm{Sum of Squares Within (samples)}\index{sum of squares within samples}" or more rarely SSE for "\NewTerm{Sum of Squares due to Errors}\index{sum of squares due to errors}" (hence sometimes denoted also: $\text{SS}_E$).
	\end{tcolorbox}	
	Let us indicate that we often see in the literature (we will use a little further below this notation):
	
	so with the unbiased estimator of the variance of the observations:
	
	Before going further, let us stop on the residual variance. We then have for samples that are not of the same size:
	
	This writing is often named "\NewTerm{pooled variance}\index{pooled variance}\index{combined variance}\index{composite variance}\index{overall variance}"\label{pooled variance}. The square root of a pooled variance estimator is knows obviously as a "\NewTerm{pooled standard deviation}\index{pooled standard deviation}\index{combined standard deviation}\index{composite standard deviation}\index{overall standard deviation}\label{pooled standard deviation}". We also see that if we assume all the mean $\bar{x}_i$ to be equal and also all samples sizes $n_i$ we fall back on the standard error.
	
	Let us now open a small important parenthesis... Let's take the case of two samples only:
	
	Therefore by introducing the maximum likelihood estimator of the variance:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	It is then quite easy to understand where the following relation comes from:
	
	also named the "\NewTerm{variance of repeatability of $p$ data series}".\\
	
	In laboratories experiments this also leads to the definition of "\NewTerm{inter-series variance}\index{inter-series variance}" (difference between mean values variance of the data series and the variance of repeatability divided by the number of measurements per series), where given $p$ data series ($i$ varies from $1$ to $p$), each data series containing $j$ values ($j$ varies from $1$ to $n_i$):
	
	In practice, if the variance of the means is smaller than the contribution of the variance of repeatability, $S_R^2$, the inter-series variance is declared equal to zero.
	\end{tcolorbox}
	We can also observe that in the specific case where $n_1=n_2=n$:
	
	So:
	
	Now suppose we want to compare with a confidence interval the mean of two populations with different variance to know whether they are different or not.

	We currently know  two tests to check the averages. The $Z$-test and $T$-test. As in the industry (practice) it is rare that we have time to take large samples, let's focus on the second we had proved above:
	
	And also recall that:
	
	Now let us recall that we have proved that if we have two random variables of distribution:
	
	then the subtraction of the averages gives (property of stability of the average):
	
	So for the difference average of two random variables coming from two population samples we obtain directly:
	
	And now the idea is to take the approximation (under the assumption that the variances are equal):
	
	This approximation is named "\NewTerm{homoscedastic hypothesis}\index{homoscedastic hypothesis}".
	
	We then have the following confidence interval (assuming that we know only an estimation of the variance) remembering that the subtraction or the sum of two independent random variables implies that their variances always add (so it is the same for degrees of freedom of the Student law as we have proved above due to the direct connection with the law of the chi-squared):
	
	with:
	
	As the idea in practice is often to test the equality of expected means (and therefore that their difference is zero) from the known estimators then:
	
	In most softwares on the market, the result is given only from the fact that quantile $T$ that we get is included in the $T_{\alpha/2}$ corresponding to the given confidence interval given by (for reminder):
	
	in the case of the homoscedastic hypothesis (equality of variances homogeneity of variances).
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The latter relation is named "\NewTerm{independent two-sample $T$-test}\index{independent two-sample $T$-test}", or "\NewTerm{homoscedastic $T$-test}\index{homoscedastic $T$-test}\label{homoscedastic t test}" or "\NewTerm{$T$-test of equality of $2$ observations expectations with equal variances}" or more simply but somewhat abusively "\NewTerm{$2$-sample $t$-Test}" with samples of different sizes and equal variances. Often in the literature, both theoretical means are equal when comparing. It follows that we then have:
	
	\end{tcolorbox}
	Otherwise, in the most general case of the assumption of heteroscedasticity (not equality of variances), we explicitly write (we'll come back on this later during our study of the Welch test...):
	
	Therefore:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The ante-previous relation is named "\NewTerm{independent two-sample $T$-test}", or "\NewTerm{heteroscedasticity $T$-test}\index{statistical tests!heteroscedasticity $T$-test}" or even "\NewTerm{test of equality of expected means: two observations with different variances}". If the sizes of the samples are equal and that the variances are also equal and that we assume the two expected mean equal during the comparison, it follows that we then have:
	
	\end{tcolorbox}
	If we need to compare two means differentiated by the index $i,j$ among a number $k$ of multiple means of an ANOVA (as post-hoc test to identify which means are different as the ANOVA omnibus test doesn't tell us anything about this!), the previous relation is then generalized to:
	
	and rewritten traditionally as following:
	
	and is named the "\NewTerm{least significant difference Fisher test}\index{statistical tests!least significant difference Fisher test}\index{LSD test}\index{Fisher's LSD test}" or simply abbreviated LSD\footnote{Post-hoc tests don't need ANOVA to be run first, except the LSD test (ANOVA is so-named "protector test" to it). Post-hoc by no means mean « post-ANOVA » (those who claim it, even authors of textbooks, have clearly no idea what the tests actually do!). Post hoc (or post factum) means « after the data is collected ».}. Obviously the test consist to verify that $\bar{X}_i-\bar{X}_j>\text{LSD}$ at a given $\alpha$ threshold and repeat this procedure for all $k(k-1)/2$ comparisons.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Remember that we have derived the following two-sample Student $T$-test:
	
	with the following pooled variance:
	
	In practice it may happens that practitioners use the trimmed mean $\mu_{T_\alpha}$ of each sample as the means and the Winsorized sample (unbiased) variances $S_{n[\alpha]}$ as the variances (\SeeChapter{see section of Numerical Methods page \pageref{trimmed mean}}). The test is then named the "\NewTerm{trimmed means $T$-test}\index{statistical tests!trimmed means $T$-test}" (yes the name doesn't highlight the fact that the variances are Winsorized... who the hell knows why...????) and written as:
	
	with the following pooled variance:
	
	This is typically a test from the family of  "Robust Null Hypothesis Significance Testing" (RNHST).
	\end{tcolorbox}
	
	This finished, we close now this parenthesis and return to our main topic... We were therefore at the following table if you remember:
	
	Where we have in the case of samples of the same size:
	
	and also the total error that is the sum of the errors of the averages (between classes) and of the residual error (intra-class) and this that the samples are of the same size or not:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Remember that this sum of square total decomposition is also sometime written:
	
	where SSTR stands for "Sum of Square Treatment Residuals" (the equivalent of $Q_A$) and SSE for "Sum of Square Errors" (the equivalent of $Q_R$).
	\end{tcolorbox}
	Indeed:
	
	But we have:
	
	Because:
	
	Therefore:
	
	Now, under the strong assumption (which we will be absolutely required a little further below) that the true variances are linked such as:
	
	and therefore that respective estimators are asymptotically equal ... which in practice is approximately true only when certain conditions are met (which is why it is absolutely necessary before making an ANOVA to run an a priori calculation of the power and the sample size of the ANOVA test) we have:		
	
	that follows immediately from the proof that we made during our study of statistical inference with the Chi-square law where we got (for refresh!):
	
	To determine the number of degrees of freedom of the Chi-square law of:
	
	we will use the fact that (by the same reasoning as for the ante-previous relation):
	
	and since that $Q_T=Q_A+Q_R$, then we must have:
	
	It follows by the linearity property of the Chi-Square law:
	
	So to summarize we have:
	
	Now comes the Fisher law in the assumption that the variances are equal (and measurement Normally distributed)! Because:	
	
	What we want to do is to see if there is a difference between the variance of the averages (between classes) and the residual variance (intra-class). To compare two variances when the true averages are unknown we saw that the best was to use the Fisher test. But, we proved in our study of the Fisher law a little above that:
	
	where in our case study:
	
	Since there are dozens of different types of ANOVA we have to understand very good this choice of the simplest ANOVA we are studying right now! Thus, if the averages are the same, the null hypothesis is then that variance ratio is equal to unity (under the above assumptions already mentioned far above). If $F$ is too large at a given threshold, then we reject the null hypothesis of equality of means (because verbatim variances will be strongly different). So it seems here logic to compare the variances between groups (numerator) with that in the groups (denominator) but as we shall see this is not always the choice to be made (especially in hierarchical ANOVA)!
	
	Given the assumption of the first equality of the above relation (the one that precedes the implication), we understand at the same time much better the great sensitivity of the ANOVA results to the non equality of true variances!
	
	Let us also indicate that the previous relation\label{ANOVA Fisher test}:
	
	is often given in the literature as follows:
	
	where MSk is named "\NewTerm{Mean Square for treatments}\index{mean square for treatments}" and MSE "\NewTerm{Mean Square for Error}\index{mean square for error}". This ratio will therefore give us the value of the random variable $F$ (whose support is for reminder bounded at zero). Regarding the choice of the test (right/left one-sided or bilateral), note that if the averages are really equal, then for all $i$:
	
	So in this case:
	
	Which brings us of course to immediately adopt a right-sided test!
	
	Otherwise, in general, the interpretation of this fraction is basically as following: This is the ratio (normalized to the number of degrees of freedom) of the sum of the error of the averages (between classes) and that of the residual error (intra-class) or in other words the ratio of the interclass variance by the residual variance. The ratio thus follows a Fisher distribution with two parameters given by the degrees of freedom of the respective classes.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	If there are only two populations (samples), we must understand that then that use of the Student $T$-test is more than enough and is perfectly equivalent! In fact, the ANOVA is an indirect comparison of means, the Student $T$-test a direct comparison... so it is obvious to guess which one is better in this particular situation!
	\end{tcolorbox}	
	
	All calculations we have made until now are very often represented in softwares in a standard table form as represented below (this is for example how Microsoft Excel 11.8346 or Minitab 15.1.1 give the results):
	\begin{table}[H]\small
	\centering
		\resizebox{\textwidth}{!}{\begin{tabular}{llcccc}\hline
		\rowcolor[gray]{0.75}\textbf{Source} & \textbf{Sum of squares (SSE)} & $\chi^2$ \textbf{df} & \textbf{Mean squares} & $F$ & \textbf{Critical} $F$\\ \hline
		Inter-Class & $Q_A=\displaystyle\sum_{i}n_i\left(\bar{x}_{i}-\bar{\bar{x}}\right)^2$ & $k-1$ & $\text{MSk}=\displaystyle\dfrac{Q_A}{k-1}$ &
		$\displaystyle\dfrac{\text{MSk}}{\text{MSE}}$ & $P(F> F_{k-1,N-k})$ \\
		Intra-Class & $Q_R=\displaystyle\sum_{ij}\left(x_{ij}-\bar{x}_i\right)^2$ & $N-k$ & $ \text{MSE}=\displaystyle\dfrac{Q_R}{N-k}$  & & \\
		Total & $Q_T=\displaystyle\sum_{ij}\left(x_{ij}-\bar{\bar{x}}\right)^2$ & $N-1$ & & &\\ \hline
		\end{tabular}}
		\caption{One-way fixed factor ANOVA table}
	\end{table}
	The null hypothesis will not be rejected if the value of:
	
	is smaller or equal to the quantile of the distribution $F$ that corresponds to the cumulative probability to $1$ subtracted from confidence level $\alpha$ and denoted by $F_c$.
	
	The $p$-value in the ANOVA table and the multiple comparison results like Dunnett's test (see further below) and Tukey's test (see also further below) are based on different methodologies and can occasionally produce contradictory results. For example, it is possible that the ANOVA $p$-value can indicate that there are no differences between the means while the multiple comparisons output indicates that some means that are different. In this case, you can generally trust the multiple comparisons output.
	
	It must be remembered that to use ANOVA, we must therefore assume that the samples come from different populations (unpaired data) and follow a Normal law (even if the ANOVA is relatively robust to non-normality). It is therefore necessary to check the normality of the distributions and the homoscedasticity (Levene test for example). Otherwise, nonparametric variants of the analysis of variance must be used (Kruskal-Wallis ANOVA or Friedman's ANOVA, for example among others that we will see later). 
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} Notice that in practice, inter-class variance is often named "\NewTerm{inter-laboratory variance}\index{inter-laboratory variance}" and the intra-class variance is verbatim often named "\NewTerm{intra-laboratory variance}\index{intra-laboratory}".\\

	\textbf{R2.} There are, in this early 121st century (holocene calendar) more than $50$ existing tests or procedures comparison for the variance. The opinion varies among practitioners relatively to their relevance and the effectiveness of homogeneity of variance tests (\NewTerm{HVT}). Some argue that they are absolutely necessary to be applied before doing any ANOVA, others say that these tests are anyway of poor performance, the ANOVA being anyway more robust to homoscedasticity than the differences than can be detected by HVT, particularly in case of non-Normality. In fact, all these issues are related to the "\NewTerm{Behrens-Fisher problem}\index{Behrens-Fisher problem}", which is that the comparison of means without assuming the equivariance. However among the fifty existing tests, several comparative studies have highlighted the following efficient tests that we will present further below: Bartlett's test, Levene and Brown-Forsythe.\\

	\textbf{R3.} When certain levels of a factor are combined into one to be compared to a reference level statisticians then talk about creating "\NewTerm{contrasts}\index{contrast}" (see just below).
	\end{tcolorbox}
	The distribution of $\bar{x}_i$ is $\mathcal{N}(\mu_i,\sigma^2_\varepsilon/n)$. Therefore as we have already seen before we can write:
	
	Hence:
	
	A confidence interval for the difference of two means is then as usual:
	
	
	\subparagraph{Welch Test}\label{Welch test}\index{statistical tests!Welch test}\mbox{}\\\\
	During our one-way fixed factor ANOVA study above, we had introduced the fact that it was possible to compare the average of two populations whose data followed a Normal distribution but of unequal variances using the heteroscedastic $T$-test Student:
	
	with:
	
	But in reality we had a little passed into silence ... (some will have perhaps noticed it) that we are not authorized to make this last addition of the degrees of freedom as soon as the variances are not equal! Indeed, let us go back to the source of the Student distribution:
	
	where for recall, the variable $Z$ follows a reduced Normal centered law and the random variable $U$ follows a $\chi^2$ law with $n-1$ degrees of freedom. And remember that we have proved that:
	
	But now, let us consider that this same random variable $U$ is given by (we work as a reminder with the standard deviation of the mean!):
	
	where $X$ and $Y$ are two independent random variables following $\chi^2$ laws with respectively $f_1$ and $f_2$ degrees of freedom such that:
	
	where we impose that $a$ and $b$ are positive because otherwise, as proved earlier above, $aX$ and $bY$  would not follow gamma laws.
	
	Let us recall then that (see page \pageref{chi-square distribution}):
	
	We then have:
	
	However $U=aX+bY$ does not follow a gamma law if $a\neq b$ (as we demonstrated during our study of the Gamma law at page \pageref{gamma distribution}). Assuming, however, that they are close enough, numerical simulations have shown that we can approximate the law of $U$ by the following gamma law:
	
	which has therefore for expected mean and variance (see page \pageref{gamma distribution}):
	
	and we will determine the positive numbers $f$ and $g$ so that the expected means and the variances coincide with those of $U$.

	We therefore have by the independence of the two random variables:
	
	So that the expectation and the variance of our approximation correspond to that of $U$, we must have:
	
	By dividing the second equation by the first the $f$ disappears and we find:
	
	Replacing in the first we find what it is customary to call the "\NewTerm{Welch-Satterthwaite equation}\index{Welch-Satterthwaite equation}":
	
	Thus explicitly:
	
	As we do not know in practice the variances (and therefore the true standard deviations), we actually take:
	
	So the expression above rounded to the nearest integer is the number of degrees of freedom that must actually be taken for the Student heteroscedastic test:
	
	Finally, let us indicate that in the general case with $n$ terms, the Welch-Satterthwaite equation is written (without proof):
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	For the special case $n=2$ the latter relation can often be found in the literature in the following form (...):
	
	\end{tcolorbox}
	
	In the field of finance when using the CAPM model (\SeeChapter{see section Economy page \pageref{capital asset pricing model}}) or when using the ANCOVA (\SeeChapter{see section Statistics page \pageref{ancova}}) and not only that ... it is useful and important to compare the regression line slopes to find out if they are significantly different or not!

	We have proved in the section of Numerical Methods (page \pageref{univariate linear regression gaussian model}) that for a linear function whose parameters were estimated and which was expressed in the form:
	
	and which exact unknown expression is:
	
	we had:
	
	and that if the real variance is unknown:
	
	or sometimes also written:
	
	That has the same shape as the Student confidence interval test for the expected mean!
	
	This being recalled, the objective now is to test if two slopes of two different regressions are equal (very important test for the study of the ANCOVA!), and then we write that the null hypothesis in the context of a hypothesis test obviously as following:
	
	Since the confidence interval of a single slope of a regression is similar in all aspects to a confidence interval of the Student's mean, the difference of two slopes is then similar in all aspects to the Welch hypothesis test. We then have since we make the null hypothesis that the two true slopes have a null difference (i.e. $a_1=a_2$):
	
	with:
	
	Which is often written by tradition\index{statistical tests! $T$-test for the difference in two slopes}:
	
	Do not forget (very important case in the application of the ANCOVA) that if we work with the estimators, then $n_i\text{V}(x_i)$ become $(n_i-1)\hat{\text{V}}(x_i)$.
	
	\subparagraph{Contrasts}\mbox{}\\\\
	Many multiple comparison methods use the idea of what we name a "\NewTerm{contrast}". What is this? To introduce this concept let us consider a one-way fixed factor ANOVA where the null hypothesis $H_0$ was rejected but where actually we don't know what level of the variable cause the difference that brig us to reject the null hypothesis.
	
	Consider for example that among the levels $i$, $j$, $k$, ... we suspect the levels $i$ and $k$ to differ. Therefore we would like to test the hypothesis:
	
	or equivalently:
	
	If we had suspected at the start of the experiment that the average of the two first level (considering an imaginary experiment with a factor having $5$ fixed levels) did not differ from the average of the two highest levels, then the hypothesis would have been:
	
	or:
	
	In general a "\NewTerm{contrast}\index{contrast}" is a linear combination of parameters of the form:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Two contrasts $C_1,C_2$ with coefficients $\{c_i\}$ and $\{d_i\}$, ie:
	
	are say to be "\NewTerm{orthogonal contrasts}" if and only if $\sum c_id_i=0$.\\
	
	For example, $C_1=\mu-2\mu_2+\mu_3$ and $C_2=\mu_1-\mu_2$ are orthogonal contrast since $(1)(1)+(-2)(0)+(1)(-1)=0$.
	\end{tcolorbox}
	Both of the hypothesis above can then be expressed in terms of contrasts:
	
	Therefore in the case of our previous example:
	
	we have $c_1=+1$, $c_2=+1$, $c_3=-1$, $c_4=-1$, $c_5=0$.
	
	Testing hypotheses involving contrasts can be done in three ways. We will introduce here only the one that gives the practitioner the possibility to build a confidence interval. This method uses a Student $T$-test. Indeed, let us write the contrast in terms of estimates:
	
	as under the null hypothesis we should have (don't forget it!):
	
	The variance of the sum can be written:
	
	If the equality of variance assumption is satisfied and the design is equilibrated we have:
	
	But with estimated we know that this is written:
	
	and therefore:
	
	Hence:
	
	should follow a centered Normal law under the above assumptions:
	
	But as we work with estimators we know that we have instead:
	
	That is sometimes denoted in the field of analysis of variance:
	
	Therefore the $100\cdot (1-\alpha)$ percent confidence interval on the contrast $\sum_{i=1}^k c_i\mu_i$ is:
	
	Clearly if this confidence interval includes zero, we would be unable to reject the null hypothesis.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Keep in mind that if you want to test for example in the framework of an ANOVA that $H_0:\;\mu_2=(\mu_1+\mu_3)/2$, it is therefore equivalent to write and test the following contrast $\sum c_i\mu_i=\mu_1-2\mu_2+\mu_3=\bar{y}_1-2\bar{y}_2+\bar{y}_3$.
	\end{tcolorbox}

	\pagebreak
	\paragraph{ANOVA with two fixed factors without repetitions}\label{ANOVA with two fixed factors without repetitions}\mbox{}\\\\
	We will now see the concept of interaction that is essential to understand what is behind the fixed two-factor ANOVA, also named  "\NewTerm{ANOVA with two fixed categorical variables}\index{ANOVA!two fixed categorical variables}" or "\NewTerm{two-way ANOVA}\index{ANOVA!two-way ANOVA}" or "\NewTerm{randomized complete block design}\index{ANOVA!randomized complete block design}\footnote{As already mentioned, a "block" can be considered as a factor and hence the mathematical treatment is exactly the same!}", without and especially later with (!) repetitions. Indeed, it is only with two-factor ANOVA with repetitions - by mathematical construction - that can be statistically (under given assumptions) consider objectively if two or more factors interact significantly together.
	
	We must therefore, before moving to the pure mathematical part, introduce some concepts!
	
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] We say that there is "\NewTerm{no interaction}\index{ANOVA!non-interacting factors}" when the average of responses by a factor based on its level varies by the same amount and with the same sign as the average of the responses of another factor depending on its levels. Then we say that: the response interaction curves in the diagram are parallel.
		
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		The parallelism of response curves is normal in the situation  of no interaction because it means that whatever the level of one or the other factors, the variation (if there is any) of the response will always be the same amplitude. What is characteristic of independence (at least locally).
		\end{tcolorbox}	
		
		\item[D2.] We say that two factors are "\NewTerm{in interaction}\index{ANOVA!interacting factors}" when the average responses by a factor depending on its levels do not change by the same magnitude and / or not with the same sign as the average of the responses of another factor according to their levels. Then we say that: the response curves in the interactions diagram are not parallel.
		
		\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
		The absence of interaction is a very strong assumption and a rarely observed. Often we have interactions or strong interactions.
		\end{tcolorbox}
	\end{enumerate}
	To understand the concept, we will use small examples have no repeated measures to such we will have a simple qualitative idea of the phenomenon, but by no means a scientific approach of the concept of interaction (that will be study later in details).
	
	For every small example below we visualize the situations by means of two types of representations: a graph illustrating the main effects on the one hand and a pattern of the interactions on the other hand.
	
	\pagebreak
	Consider the following small table with two factors at two levels ("explanatory variables") including four cells ("variable of interest"):
	\begin{table}[H]
	\begin{center}
		\begin{tabular}{|c|c|c|}
			\hline
			{} & \multicolumn{2}{|c|}{\cellcolor[gray]{0.75}\textbf{Factor $2$}} \\
			\hline
			\cellcolor[gray]{0.75}\textbf{Factor $1$} & \cellcolor[gray]{0.75}Level $1$ & \cellcolor[gray]{0.75}Level $2$ \\
			\hline
			\cellcolor[gray]{0.75}Level $1$ & $3$ & $3$ \\
			\hline
			\cellcolor[gray]{0.75}Level $2$ & $3$ & $3$ \\
			\hline
		\end{tabular}
		\caption[]{First example of 2-factor ANOVA without repetition}
	\end{center}
	\end{table}
	We will get as result with software such as Minitab:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/anova_principal_effects_no_interactions.jpg}
		\caption{Main effects plot without interactions in Minitab 15}
	\end{figure}
	We see well that no factor has a major effect on anything. What is relatively obvious given the content of the previous table...
	
	The diagram of interactions (often named "\NewTerm{profiler}\index{profiler diagram}" in the industry) gives:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/anova_interactions_effects_no_interactions.jpg}
		\caption{Interactions plot without interactions in Minitab 15}
	\end{figure}
	where we can see that the factors do not interact between them (or neutralize themselves it depends...). Then we say that there is "\NewTerm{(a priori) no effect or no interaction (locally)}\index{ANOVA!non-locally interaction}". In fact in some experiments the absence of interaction is a very strong assumption and therefore very rare. This is why we must pay attention to the chosen words when interpreting interaction plots (because do not to go through pure calculations is dangerous for this step or simply not scientific!).
	
	Now consider the following table:
	\begin{table}[H]
	\begin{center}
		\begin{tabular}{|c|c|c|}
			\hline
			{} & \multicolumn{2}{|c|}{\cellcolor[gray]{0.75}\textbf{Factor $2$}} \\
			\hline
			\cellcolor[gray]{0.75}\textbf{Factor $1$} & \cellcolor[gray]{0.75}Level $1$ & \cellcolor[gray]{0.75}Level $2$ \\
			\hline
			\cellcolor[gray]{0.75}Level $1$ & $2$ & $2$ \\
			\hline
			\cellcolor[gray]{0.75}Level $2$ & $4$ & $4$ \\
			\hline
		\end{tabular}
		\caption[]{Second example of 2-factor ANOVA without repetition}
	\end{center}
	\end{table}
	It seems obvious that the factor one through considered with its levels seems to have an influence on the answer. But let us see the different representations:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/anova_principal_effects_with_interactions.jpg}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/anova_interactions_effects_with_interactions.jpg}
		\caption{Main effects and interactions plot with interactions in Minitab 15}
	\end{figure}
	Let us comment the first graph as proposed by a reader:
	
	This includes two parts: the left one analyses the effects of Factor $1$ through its two levels; that of the right do the same for the Factor $2$.
	
	Have a closer look to the left part:
	
	We see two points connected by a line segment. Here the first point, the one for the level $1$1, is located in the ordinate $2$ while the second point, that for level $2$, is located in the ordinate $4$. Now remember that each point represents an average. And the ordinate of the first point is located at the average of $(2 + 2) / 2 = 2$.
	
	Having said that and hoping this helped to a better understanding, let us continue...
	
	It is quite clear in the chart above that only the level of Factor $1$ influences the answer, while Factor $2$ has no effect on the answer. Then we say that: there is a main effect (locally) of the Factor $1$.

	
	On the diagram of interactions, we have the same information but in a different form. We see that regardless of the level of Factor $2$, the answers are horizontal and thus that it does not influence the results. We are then in a situation where "\NewTerm{(a priori) the main effect is (locally) Factor $1$ and there is an absence of interactions between factors}".
	
	Now consider the following table:
	\begin{table}[H]
	\begin{center}
		\begin{tabular}{|c|c|c|}
			\hline
			{} & \multicolumn{2}{|c|}{\cellcolor[gray]{0.75}\textbf{Factor $2$}} \\
			\hline
			\cellcolor[gray]{0.75}\textbf{Factor $1$} & \cellcolor[gray]{0.75}Level $1$ & \cellcolor[gray]{0.75}Level $2$ \\
			\hline
			\cellcolor[gray]{0.75}Level $1$ & $4$ & $2$ \\
			\hline
			\cellcolor[gray]{0.75}Level $2$ & $4$ & $2$ \\
			\hline
		\end{tabular}
		\caption[]{Third example of 2-factor ANOVA without repetition}
	\end{center}
	\end{table}
	This time we can observe that the Factor $2$ has an influence but not the Factor $1$. But let us also see this with our two types of representations:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/anova_principal_effects_with_interactions_second_example.jpg}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/anova_interactions_effects_with_interactions_second_example.jpg}
		\caption{Main effects and interactions plot with interactions in Minitab 15}
	\end{figure}
	We see well on the top diagram that the Factor $1$ has no influence. On the bottom diagram it is less obvious but the superposition of the two lines indicates that the Factor $1$ has no influence. Then we say that there is: "\NewTerm{(a priori) main effect (locally) of Factor $2$ and absence of interactions between factors}".
	
	Now consider the following table:
	\begin{table}[H]
	\begin{center}
		\begin{tabular}{|c|c|c|}
			\hline
			{} & \multicolumn{2}{|c|}{\cellcolor[gray]{0.75}\textbf{Factor $2$}} \\
			\hline
			\cellcolor[gray]{0.75}\textbf{Factor $1$} & \cellcolor[gray]{0.75}Level $1$ & \cellcolor[gray]{0.75}Level $2$ \\
			\hline
			\cellcolor[gray]{0.75}Level $1$ & $3$ & $1$ \\
			\hline
			\cellcolor[gray]{0.75}Level $2$ & $5$ & $3$ \\
			\hline
		\end{tabular}
		\caption[]{Fourth example of 2-factor ANOVA without repetition}
	\end{center}
	\end{table}
	We see that both factors have an influence on the answer. We can clearly see it on the both diagrams below:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/anova_principal_effects_with_interactions_third_example.jpg}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/anova_interactions_effects_with_interactions_third_example.jpg}
		\caption[]{Main effects and interactions plot with interactions in Minitab 15}
	\end{figure}
	We can see very well on the top diagram that the Factor $1$ has an influence on the answer and that we have the same conclusion for the Factor $2$ (and furthermore to in the same magnitude regardless of the direction!). On the bottom diagram it is less obvious, but the same conclusion is valid. We then say that: "\NewTerm{(a priori) both factors are (locally) significant and without interactions}\index{ANOVA!both factors locally significant}".
	
	\pagebreak
	Now consider the following table:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|}
			\hline
			{} & \multicolumn{2}{|c|}{\cellcolor[gray]{0.75}\textbf{Factor $2$}} \\
			\hline
			\cellcolor[gray]{0.75}\textbf{Factor $1$} & \cellcolor[gray]{0.75}Level $1$ & \cellcolor[gray]{0.75}Level $2$ \\
			\hline
			\cellcolor[gray]{0.75}Level $1$ & $2$ & $4$ \\
			\hline
			\cellcolor[gray]{0.75}Level $2$ & $4$ & $2$ \\
			\hline
		\end{tabular}
		\caption[]{Fifth example of 2-factor ANOVA without repetition}
	\end{table}
	which in this form is not always obvious to interpret. But with the diagrams we immediately have a more relevant information:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/anova_principal_effects_with_interactions_fourth_example.jpg}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/anova_interactions_effects_with_interactions_fourth_example.jpg}
		\caption[]{Main effects and interactions plot with interactions in Minitab 15}
	\end{figure}
	We see well on the top diagram that none of the factors has an effect in average on the answer a priori (same diagram as at the very beginning of this series of examples with the same mean!). The bottom diagram gives us additional information by cons (!!!): The factors have a cross-influence and as this cross-influence is of the same magnitude, the effects cancel out. Then we say that: "\NewTerm{(a priori) the two factors are (locally) in interaction in F$1$ * F$2$}\index{ANOVA!both factors locally in interaction}".
	
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Caution! It may happen that a "statistical interaction" like the one above may not mean what you thing! In regression, when you predict $Y$ from two variables (say, $X$ and $Z$), they are therefore said to "interact" when the predictiveness of one variable depends on the other. For example, perhaps $X$ predicts $Y$ strongly when $Z$ is high but terribly otherwise. In this case, you might say that $X$ and $Z$ interact to predict $Y$. We could think of one predictor (e.g., $Z$) like a light switch that turns the other variable on or off. For $X$ to predict $Y$, you also need the right combination of $X$ and $Z$. Very important! And for this reason, analysts and data scientists often explore this by including product terms in their regressions:
	$$Y=X+Z+X*Z$$
	If $X*Z$ explains unique variance in $Y$, this is a sign that $Z$ and $X$ interact. We can re-arrange terms to tell the story of this interaction, but this is how we commonly test it. So.... $X*Z$ is strong and significant. However... (!!!) you can also get the same effect in other cases. Eek! Why? Imagine that $X$ has a quadratic relationship to $Y$, say:
	$$Y=X+X^2$$
	This is just a special case of interaction. So, in this case, $X$ "moderates itself" and that makes sense: if the relationship between $X$ and $Y$ is non-linear, it's stronger at some levels of $X$ but weaker at others. The relationship between $X$ and $Y$ depends.... on $X$! Well, if other variables correlate with $X$ (e.g., $Z$), guess what? They may act the same way because they are picking up on that quadratic relationship. So, if you run the usual "test for interaction":
	$$Y=X+Z+X*Z$$
	Your term may come up significant because it is tapping into the quadratic relationship $X$ has with itself, which you haven't controlled for! So be sceptical of interactions, particularly when there is non-linearity present! Examine the linearity of relationships between your predictors (single one at a time) and your outcome if you do plan to test for interactions!
	\end{tcolorbox}
	
	Now consider the following table:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|}
			\hline
			{} & \multicolumn{2}{|c|}{\cellcolor[gray]{0.75}\textbf{Factor $2$}} \\
			\hline
			\cellcolor[gray]{0.75}\textbf{Factor $1$} & \cellcolor[gray]{0.75}Level $1$ & \cellcolor[gray]{0.75}Level $2$ \\
			\hline
			\cellcolor[gray]{0.75}Level $1$ & $1$ & $3$ \\
			\hline
			\cellcolor[gray]{0.75}Level $2$ & $5$ & $3$ \\
			\hline
		\end{tabular}
		\caption[]{Sixth example of 2-factor ANOVA without repetition}
	\end{table}
	which in this form is not also always obvious to interpret. But with the diagrams we immediately have a more relevant information:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/anova_principal_effects_with_interactions_fifth_example.jpg}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/anova_interactions_effects_with_interactions_fifth_example.jpg}
		\caption[]{Main effects and interactions plot with interactions in Minitab 15}
	\end{figure}
	We observe well on the top diagram that the Factor $1$ appears to have an influence and the factor $2$ no (in average!). The bottom interactions diagram gives us, too, once again, additional information (!!!): It is that the factors are interacting. We say then that we have "\NewTerm{(a priori) two factors (locally) in interaction F$1$ * F$2$ where the influence of Factor $1$ is significant}".
	
	Now consider the following table:
	\begin{table}[H]
	\begin{center}
		\begin{tabular}{|c|c|c|}
			\hline
			{} & \multicolumn{2}{|c|}{\cellcolor[gray]{0.75}\textbf{Factor $2$}} \\
			\hline
			\cellcolor[gray]{0.75}\textbf{Factor $1$} & \cellcolor[gray]{0.75}Level $1$ & \cellcolor[gray]{0.75}Level $2$ \\
			\hline
			\cellcolor[gray]{0.75}Level $1$ & $3$ & $3$ \\
			\hline
			\cellcolor[gray]{0.75}Level $2$ & $5$ & $1$ \\
			\hline
		\end{tabular}
		\caption[]{Seventh example of 2-factor ANOVA without repetition}
	\end{center}
	\end{table}
	We see that both factors influence the answer. We see that well on the both diagrams below:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/anova_principal_effects_with_interactions_sixth_example.jpg}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/anova_interactions_effects_with_interactions_sixth_example.jpg}
		\caption[]{Main effects and interactions plot with interactions in Minitab 15}
	\end{figure}
	Here we say we have: "\NewTerm{(a priori) the two factors (locally) interacting F$1$ * F$2$ where the influence of Factor $2$ is significant}".
	
	And finally a last example:
	\begin{table}[H]
	\begin{center}
		\begin{tabular}{|c|c|c|}
			\hline
			{} & \multicolumn{2}{|c|}{\cellcolor[gray]{0.75}\textbf{Factor $2$}} \\
			\hline
			\cellcolor[gray]{0.75}\textbf{Factor $1$} & \cellcolor[gray]{0.75}Level $1$ & \cellcolor[gray]{0.75}Level $2$ \\
			\hline
			\cellcolor[gray]{0.75}Level $1$ & $1$ & $5$ \\
			\hline
			\cellcolor[gray]{0.75}Level $2$ & $5$ & $1$ \\
			\hline
		\end{tabular}
		\caption[]{Eighth and last example of 2-factor ANOVA without repetition}
	\end{center}
	\end{table}
	Which gives us the two diagrams:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/anova_principal_effects_with_interactions_seventh_example.jpg}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/anova_interactions_effects_with_interactions_seventh_example.jpg}
		\caption[]{Main effects and interactions plot with interactions in Minitab 15}
	\end{figure}
	Here we say we have: "\NewTerm{(a priori) the two factors (locally) interacting F$1$ * F$2$ where the influence of both factors is significant}".
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} A belief (commonly spread) of people who have a lack experience in laboratories is to think that for an interaction to be significant it is necessary that the both factors that compose it are also significant.\\
	
	\textbf{R2.} If an interaction is significant, the main effects are objectively rarely interpretable!
	\end{tcolorbox}
	After all these diagrams and tables, let us turn back to the mathematical part:
	
	We saw earlier how to perform a one-way fixed factor analysis of variance. To recap, this consists to a test for equality of means for $k$ independent samples of $n$ random variables each (in the case where all samples have the same number of measures: a balanced ANOVA). Each sample is regarded as an experiment on a different subject then considered therefore as an independent variable factor!!!
	
	However it happens in the reality that we do for each sample vary a second parameter, then considered a second variable factor but independent of the first one. We speak then of course analysis of variance with two factors. In addition, we will consider in the first instance to simplify the calculations that the random variables are independent! Therefore a factor has no influence over the other has we already mention it !!! In other words there is no interaction between factors. We speak then of a "\NewTerm{two-factor ANOVA without interactions}\index{two-factor ANOVA without interaction}".
	
	To determine the formulation of the test to be performed, remember that for the analysis of variance with factor, we decomposed the total variance in the sum of the variance of average (inter-class) and of the residual variance (intraclass) such that:
	
	explicitly we compare the samples $i=1\ldots k$:
	
	which had given us in the end:
	
	For the ANOVA with two fixed factors we will start form the table below:
	\begin{table}[H]
		\centering
		\begin{tabular}{ |c|c|c|c|c|c| }
			\hline
			\rowcolor[gray]{0.75}\multicolumn{1}{|c|}{\phantom} & \multicolumn{4}{ |c| }{Factor $A$} & \\
			\hline
			\rowcolor[gray]{0.75}Factor $B$ & Sample $1$ & Sample $2$ & Sample ...$j$ & Sample $r$ & \\ \hline
			\cellcolor[gray]{0.75}Sample $1$ &  $x_{11}$ & $x_{12}$ & $\ldots$ & $x_{1r}$ & Average $\bar{x}_1$ \\
			\cellcolor[gray]{0.75}Sample $2$ &  $x_{21}$ & $x_{22}$ & $\ldots$ & $x_{2r}$ & Average $\bar{x}_2$\\
			 \cellcolor[gray]{0.75}Sample $i$ & $\ldots$ & $\ldots$ & $x_{ij}$ & $\ldots$ & Average $\bar{x}_i$ \\
			 \cellcolor[gray]{0.75}Sample $k$ & $x_{n1}$ & $x_{n2}$ & $\ldots$ & $x_{kr}$ & Average $\bar{x}_k$\\ \hhline{|=|=|=|=|=|=|}
			 \phantom & Average: $\bar{x}_1$ &  Average: $\bar{x}_2$  &  Average: $\bar{x}_j$ & Average: $\bar{x}_r$ & $\bar{\bar{x}}$ \\
			\hline
		\end{tabular}
		\caption{Typical cross structure for the analysis of variance of 2 fixed factors without repetitions}
	\end{table}
	for which in a laboratory the factor kept fixed while the other is varied will be named the "\NewTerm{block factor}\index{ANOVA!block factor}" and the other will be named the "\NewTerm{treatment factor}\index{ANOVA!treatment factor}" and in practice it will be ensured that the latter does not always change out in the same order in the purpose to eliminate possible inertia effects when switching from one treatment to another (the Americans refer to two-factor controlled ANOVAs without interactions under the terms: "\NewTerm{randomized block design}\index{ANOVA!randomized block design}" (GRBD)).
	
	To continue the whole trick is to break down the total variance by comparing the average of the rows (observations) indexed this time with $i=1\ldots k$ and of columns (samples) indexed with $j=1\ldots r$ relatively to the total average such as:
	
	But we have in a first time:
	
	So it remains:
	
	But we also have:
	
	For the rest, let us indicate first that relatively to our above table, we have:
	
	It follows then that:
	
	and then  it comes immediately that we have likewise:
	
	So it remains in the end:
	
	what we will denote in this book in the following condensed form:
	
	where $Q_A,Q_B$ are obviously associated with the main effects (comparison of marginal means with the total average).
	
	Therefore compared to the one-way fixed factor ANOVA we have an additional term for the total variance!
	
	In order it is clear that the first sum of the differences from the first column factor:
	
	will have just as for the one-way fixed factor ANOVA $k-1$ degrees of freedom. That is to say, under the same assumptions as the one-way fixed factor ANOVA:
	
	
	The second sum of the differences from the second factor (line factor):
	
	is new but we prove in a perfectly identical way to the first one it will have $k-1$ degrees of freedom. That is to say, under the same assumptions as the one-way fixed factor ANOVA:
	
	For the third sum that follows mandatory also a chi-square law (since the total variance follows a chi-square law and that the first two terms of the sum also!):
	
	it's a little bit trickier ... but there's a trick in the physicist way of life...! We know from our study of the one-way fixed factor ANOVA that the sum of the degrees of freedom of each term must equal the total number of degrees of freedom. In other words, we must have for the two-way fixed factor ANOVA:
	
	So obviously what is missing is equal to:
	
	Therefore:
	
	We then have the following table:
	
	Finally, the rest is exactly the same as for the one-way fixed factor ANOVA just that we have two tests to do this time that are:
	
	The choice above seems intuitively practical...
	
	All calculations we have made above are often represented in software in a standard form of a table whose form and content is presented below (this is how Minitab 15.1.1 and Microsoft Excel 11.8346 present it for example):
	\begin{table}[H]
		\resizebox{\textwidth}{!}{\begin{tabular}{lcccc}\hline
		\rowcolor[gray]{0.75}\textbf{Sum of squares (SSE)} & $\chi^2$ \textbf{df} & \textbf{Mean squares} & $F$ & \textbf{Critical} $F$\\ \hline
		$Q_A=k\displaystyle\sum_{j}\left(\bar{x}_{j}-\bar{\bar{x}}\right)^2$ & $k-1$ & $\text{MSk}A=\dfrac{Q_A}{k-1}$ &
		$\dfrac{\text{MSk}A}{\text{MSE}}$ & $P(F> F_{k-1,(k-1)(r-1)})$ \\
		$Q_B=r\displaystyle\sum_{i}\left(\bar{x}_{i}-\bar{\bar{x}}\right)^2$ & $r-1$ & $\text{MSk}B=\dfrac{Q_B}{r-1}$ &
		$\dfrac{\text{MSk}B}{\text{MSE}}$ & $P(F> F_{r-1,(k-1)(r-1)})$ \\
		$Q_R=\sum_{ij}\left(x_{ij}-\bar{x}_i-\bar{x}_j+\bar{\bar{x}}\right)^2$ & $(k-1)(r-1)$ & $ \text{MSE}=\dfrac{Q_R}{(k-1)(n-1)}$  & & \\
		$Q_T=\displaystyle\sum_{ij}\left(x_{ij}-\bar{\bar{x}}\right)^2$ & $N-1$ & $\text{MST}=\dfrac{Q_T}{N-1}$ & &\\ \hline
		\end{tabular}}
		\caption{Two-way fixed factor ANOVA table without repetitions}
	\end{table}
	and the condition of acceptance of the null hypothesis of equality of means for each factor is the same as for the one-way fixed factor ANOVA (see the server of exercises for a detailed practical example with Microsoft Excel 11.8346).
	
	So we have two Fisher tests to know whether if factor $A$ (respectively $B$) have a significant influence on the measures or not.
	
	Obviously, in the above developments, factors $A$ and $B$ are interchangeable in the developments by symmetry!
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The corresponding factor effect model is (notice that the model is assumed to be purely additive, that is without interactions!):
	
	with $i=1\ldots a$, $j=1\ldots b$ where $\mu$ is the overall mean, $\tau_i$ is the effect of the $i$th of treatment $A$, $\beta_j$ is the effect of the $j$th level of treatment $B$ (main effects) and $\varepsilon_{ij}$ is the habitual random error following $\mathcal{N}(0,\sigma_\varepsilon^2)$.\\
	
	The assumptions on the effect terms are $\sum_{i=1}^a\tau_i=0$, $\sum_{j=1}^b\beta_j=0$.\\
	
	The hypothesis of interest are $H_0:\;  \tau_i=0$ and $H_0:\; \beta_j=0$.\\
	
	We see that the maximum likelihood estimators of the model parameters are $\hat{\mu}=\bar{\bar{x}}$ and $\hat{\tau}_i=\bar{x}_i-\bar{\bar{x}}$ and $\hat{\beta}_j=\bar{x}_j-\bar{\bar{x}}$. Hence:
	
	\end{tcolorbox}
	
	\pagebreak
	\paragraph{ANOVA with two fixed factors with repetitions}\mbox{}\\\\
	So far we have examined ANOVA on experiments with one or two fixed factors (one or two categorical variables not sampled from a populations). In the case of two factors, we considered that for each combination of factors we only had one measure (cell). But it can happen (and this is better!) that have several measures for one combination of factors!  We name this type of study of "\NewTerm{experimental design with repeated measures}\index{experimental design with repeated measures}" and the results will be processed with an analysis of variance for repeated measures (RM-ANOVA) with two fixed factors and interactions! This is an extremely important tool as it allows validation studies by several independent laboratories (or employees) and is also associated with many other statistical tools such as the study of reproducibility and repeatability (R\&R Study) to name only the best known cases in the industrial field.
	
	You must  understand that it is mandatory in the field of statistics to involve interactions between factors systematically when we are dealing with an experience having repeated measures. This for the simple reason that the mathematical interaction term only appears in this situation!
	
	Thus, it may be intuitive (even before the proof) that a two fixed factor ANOVA with repeated measures contains one double interaction, and two main effects. A three fixed factor ANOVA with repeated measures will have one triple interaction, three double interactions and 3 main effects. And so on...
	
	Before we start, we will consider the following measurement table (this kind of table typically belongs to the family of "multi-dimensional crossed-table with subtotals\label{multi-dimensional crossed-table with subtotals}"):
	\begin{table}[H]
		\centering
		\begin{tabular}{ |l|c|c|c|c|c| }
			\hline
			\multicolumn{1}{|c|}{\phantom} & \multicolumn{4}{ |c| }{Factor $A$} & \\
			\hline
			Factor $B$ & Sample $1$ & Sample $2$ & Sample ...$j$ & Sample $r$ & AVERAGE \\ \hline
			Sample $1$ &  $x_{111}$ & $x_{112}$ & $\ldots$ & $x_{11r}$ & \\
			$\;$ Replication $2$ &  $x_{211}$ & $x_{212}$ & $\ldots$ & $x_{21r}$ & \\			
			$\;$ Replication $m$ &  $\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ & \\			
			$\;$ Replication $n$ &  $x_{n11}$ & $x_{n12}$ & $\ldots$ & $x_{n1r}$ & \\ \hline
			Average Sample $1$ &  $\bar{x}_{11}$ & $\bar{x}_{12}$ & $\ldots$ & $\bar{x}_{13}$ & $\bar{x}_{1.}$ \\ \hline		
			Sample $2$ &  $x_{121}$ & $x_{122}$ & $\ldots$ & $x_{12r}$ & \\
			$\;$ Replication $2$ & $x_{221}$ & $x_{222}$ & $\ldots$ & $x_{22r}$ & \\			
			$\;$ Replication $m$ & $\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ & \\			
			$\;$ Replication $n$ & $x_{n21}$ & $x_{n22}$ & $\ldots$ & $x_{n2r}$ & \\ \hline
			Average Sample $2$ & $\bar{x}_{21}$ & $\bar{x}_{22}$ & $\ldots$ & $\bar{x}_{2r}$ & $\bar{x}_{2.}$ \\ \hline			
			Sample $i$ &  $\ldots$ & $\ldots$ & $x_{1ij}$ & $\ldots$ & \\
			$\;$ Replication $2$ & $\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ & \\			
			$\;$ Replication $m$ & $\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ & \\			
			$\;$ Replication $n$ & $\ldots$ & $\ldots$ & $x_{nij}$ & $x_{nir}$ & \\ \hline	
			Average Sample $i$ & $\ldots$ & $\ldots$ & $\bar{x}_{ij}$ & $\ldots$ & $\bar{x}_{i.}$ \\ \hline		
			Sample $k$ & $x_{2k1}$ & $x_{2k2}$ & $\ldots$ & $x_{2kr}$ & \\
			$\;$ Replication $2$ &  $x_{21}$ & $x_{22}$ & $\ldots$ & $x_{2r}$ & \\			
			$\;$ Replication $m$ &  $\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ & \\			
			$\;$ Replication $n$ &  $x_{nk1}$ & $x_{nk2}$ & $\ldots$ & $x_{nkr}$ & \\	\hline
			Average Sample $k$ &  $\bar{x}_{k1}$ & $\bar{x}_{k2}$ & $\ldots$ & $\bar{x}_{kr}$ & $\bar{x}_{k.}$ \\ \hhline{|=|=|=|=|=|=|}
			 AVERAGE & $\bar{x}_{.1}$ & $\bar{x}_{.2}$ & $\bar{x}_{.j}$ & $\bar{x}_{.r}$ & $\bar{\bar{x}}_{..}$ \\
			\hline
		\end{tabular} 
		\caption{Typical cross structure for the analysis of variance of 2 fixed factors with repetitions}
	\end{table}
	with the usual properties of the mean (for reminder):
	
	And remember that the two fixed factor ANOVA without replications (and therefore without possibility to analyse mathematically the interactions), all the trick was to break down the total variance by comparing the average of the lines indexed  with $i=1\ldots k$ and of columns indexed with $j=1\ldots r$ compared with the total average.
	
	The idea will now be just about the same except that we will compare the mean of the lines indexed with $i=1\ldots k$ and columns indexed with $j=1\ldots r$ not only in comparison to the total average but also that of each row and of each column.
	
	For this purpose with start from what we got for the two fixed factor ANOVA without replication:
	
	but whose notation will just be adapted to the context:
	
	Obviously, with this notation the two fixed factor ANOVA without replication becomes:
	
	But in the present case, we must add a term for the replication and adapt the notation for the measurements. So, without repeating all the developments (it is a bit cheeky but...) we get directly:
	
	where in order, $m$ is the replication of the sample $i$ of the factor $A$ and of sample $j$ of factor $B$.
	
	It comes then obviously the interclass variances for factors $A$ and $B$ that are immediate:
	
	where $Q_A,Q_B$ are obviously again associated with main effects (comparisons of marginal averages with the total average).
	
	Now let's play a little by introducing in the sum, in positive and negative in the last term:
	
	the average of replications:
	
	that we will find in fine in the total sum of squares:
	
	Of course, we recognize rather quickly the intra-classes variance (often also named "\NewTerm{residual error}\index{residual error}" or simply in the particular case of the two fixed factor ANOVA with repetitions "\NewTerm{repeatability error}\index{repeatability error}"):
	
	and the term we can interpret (by comparison with the two factor ANOVA without repetitions) as the variance of interaction:
	
	If the ANOVA is balanced the term:
	
	must vanish. Let us check this:
	
	and therefore for $i$ and $j$ fixed it comes:
	
	And therefore the summation over all $i$ and $j$ will be zero also by extension. Those who have a doubt about the cancellation of the two terms of the development above, may be able to reassure themselves by making a numerical application (enjoy!...).
	
	Then finally:
	
	where for reminder, $n$ is therefore the number of replications, $r$ the number of samples of factor $A$ and $k$ and the number of samples of factor B (the latter two parameters are often mixed by those who make calculations by hand). Result is sometimes denoted as following in the literature:
	
	So in comparison to the two fixed factor ANOVA without replication, we have an additional term for the total variance.
	
	In the order it is clear that the first sum of the differences from the first factor column:
	
	will have just like for one-way fixed factor ANOVA and two-way fixed factor ANOVA without repetition $k-1$ degrees of freedom. That is to say, under the same assumptions of these latter ANOVA, we have:
	
	The second sum of the differences relatively to the second factor line:
	
	will have under the same assumptions the property:
	
	Thanks to the reasoning performed with two fixed actor ANOVA without repetition, we know that for the interaction term:
	
	We have:
	
	It remains to determine the number of degrees of freedom of the last term:
	
	To do this, we proceed in the same way as for the two fixed factor ANOVA without repetitions. We know from our study of the one-way fixed factor ANOVA that the sum of the degrees of freedom of each term must be equal tot the total number of degrees of freedom. In other words, we must have for the two fixed factors ANOVA:
	
	So what is missing is obvious:
	
	Therefore:
	
	We have then the following table:
	
	Finally, the rest is exactly the same as for the two-way fixed factor ANOVA without replication with the difference that now we just have three tests this time:
	
	Again the choice of the ratios is relatively intuitive!
	
	All calculations we have made above are often represented in software in a standard form of a table whose form and content is presented below (this is how Minitab 15.1.1 and Microsoft Excel 11.8346 present it for example):
	\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}{\begin{tabular}{lcccc}\hline
	\rowcolor[gray]{0.75}\textbf{Sum of squares (SSE)} & $\chi^2$ \textbf{df} & \textbf{Mean squares} & $F$ & \textbf{Critical} $F$\\ \hline
	$Q_A=nk\displaystyle\sum_{j}\left(\bar{x}_{.j}-\bar{\bar{x}}_{..}\right)^2$ & $k-1$ & $\text{MSk}A=\dfrac{Q_A}{k-1}$ &
	$\dfrac{\text{MSk}A}{\text{MSE}}$ & $P(F> F_{k-1,Nk-r})$ \\
	$Q_B=nr\displaystyle\sum_{i}\left(\bar{x}_{i.}-\bar{\bar{x}}_{..}\right)^2$ & $r-1$ & $\text{MSk}B=\dfrac{Q_B}{r-1}$ &
	$\dfrac{\text{MSk}B}{\text{MSE}}$ & $P(F> F_{r-1,N-kr})$ \\
	$Q_{A\times B}=\displaystyle\sum_{mij}\left(\bar{x}_{mij}-\bar{x}_{i.}-\bar{x}_{.j}+\bar{\bar{x}}_{..}\right)^2$ & $(k-1)(r-1)$ & $ \text{MSk}AB=\dfrac{Q_{A\times B}}{(k-1)(n-1)}$  & $\dfrac{\text{Msk}AB}{\text{MSE}}$ & $P(F> F_{(k-1)(n-1),N-kr})$\\
	$Q_R=\displaystyle\sum_{mij}\left(\bar{x}_{mij}-\bar{x}_{ij}\right)^2$ & $N-kr$ & $ \text{MSE}=\dfrac{Q_R}{N-kr}$  &  & \\
	$Q_T=\displaystyle\sum_{mij}\left(x_{mij}-\bar{\bar{x}}_{..}\right)^2$ & $N-1$ &  $\text{MST}=\dfrac{Q_T}{N-1}$  & &\\ \hline
	\end{tabular}}
	\caption{Two-way fixed factor ANOVA table with repetitions}
	\end{table}
	and the null hypothesis of equality of means for each factor is the same as for one-way fixed factor ANOVA (see the exercise server for a detailed practical example with Microsoft Excel 11.8346).
	
	So we have three Fisher tests to know whether for each if factor $A$, respectively $B$ or interaction $AB$ have a significant influence on the measures or not.
	
	Obviously, in the above developments, factors $A$ and $B$ are interchangeable by symmetry!
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The corresponding factor effect model is (notice that the model is not purely additive!):
	
	with $i=1\ldots a$, $j=1\ldots b$, $k=1\ldots n$ where $\mu$ is the overall mean, $\tau_i$ is the effect of the $i$th of treatment $A$, $\beta_j$ is the effect of the $j$th level of treatment $B$ (main effects), $(\tau\beta)_{ij}$ is the effect of the interaction between $\tau_i$ and $\beta_j$, and $\varepsilon_{ijk}$ is the habitual random error following $\mathcal{N}(0,\sigma_\varepsilon^2)$.\\
	
	The assumptions on the effect terms are $\sum_{i=1}^a\tau_i=0$, $\sum_{j=1}^b\beta_j=0$, $\sum_{i=1}^a (\tau\beta)_{ij}=0$ for $j=1\ldots b$ and $\sum_{j=1}^b (\tau\beta)_{ij}=0$ for $i=1\ldots a$.\\
	
	The hypothesis of interest are $H_0:\;  \tau_i=0$, $H_0:\; \beta_j=0$ or $H_0:\; (\tau\beta)_{ij}=0$.
	\end{tcolorbox}
	
	\paragraph{Multifactor ANOVA with Repeated Measures}\mbox{}\\\\
	The "\NewTerm{multifactor categorical variables ANOVA with repeated measures}\index{multifactor ANOVA with repeated measures}" is simply the name by which specialists refer to the following fixed-factors ANOVA:
	\begin{itemize}
		\item three factors (fixed) ANOVA with or without repetition
		\item four factors (fixed) ANOVA with or without repetition
		\item five factors (fixed) ANOVA with or without repetition
		\item etc.
	\end{itemize}
	Obviously, the ANOVA with one and two fixed factors are also part of the family of multifactor ANOVA. Also be aware that most statistical software manages up to ANOVA with $15$ fixed factors fixed factors (categorical variables) on the assumption condition that the plan is balanced (i.e. for each level of each factor, there is the same number of measurements). A spreadsheet software (like Microsoft Excel for example) by default is able to deal with ANOVA having a maximum of two fixed factors.
	
	Okay now the reader may be disappointed (well I am also disappointed to have only one life...) because honestly I do not want to redo all the developments seen above for the one-way and two-way fixed factors ANOVA for $3$, $4$ and up to $15$ factors because it would take more than 100 A4 pages to do in a pedagogical and clear form plus it is always based on the same mechanical development. Sadly the generalized theory of ANOVA well being much shorter, it is indigestible for my taste.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The practitioner (and the reader) must avoid at any cost to do the confusion between "Multifactor" / factorial ANOVA that refer to cases where we have more than one factor (i.e., categorical explanatory variable) and "Multivariate" ANOVA that is also named MANOVA (note the initial M) that refers to cases where we have multiple dependent / response variables (we will study that latter further below page \pageref{MANOVA}). 
	\end{tcolorbox}
	
	\pagebreak
	\subparagraph{Interlude on CRD, RBD, RCBD, IBD, GRBD...}\mbox{}\\\\
	So far we have seen some very common ANOVA. But it is time to discuss the way to do the experiments before going further. But before it would be useful to first make a recall between two main type of experiments in science!
	
	\begin{fquote}[Ronald Fisher]To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of...
 	\end{fquote}
	
	\textbf{Definition (\#\thesection.\mydef):} The "\NewTerm{observational study}\index{observational study}" is defined as follows:
	\begin{itemize}
		\item Collect data in a way that does not directly interfere with how the data arise, i.e. merely "observe"
		
		\item Based on an observational study, we can only establish an \textbf{association}, in other words "\textbf{correlation}", between the explanatory and response variables
		
		\item If an observational study uses data from the past, it is named "\NewTerm{retrospective study}\index{retrospective study}", whereas if data are collected throughout the study, it is named a "\NewTerm{prospective study}\index{prospective study}". 
	\end{itemize}
	\textbf{Definition (\#\thesection.\mydef):} The "\NewTerm{experiment study}\index{experiment study}" is defined as follows:
	\begin{itemize}
		\item Randomly assign subjects to various treatments
		
		\item Can establish \textbf{causal} connections between the explanatory and response variables!
	\end{itemize}
	As companion example, consider we want to evaluate the relationship between regularly working out and energy level. We can design the study as an observational study or an experiment.
	\begin{itemize}
		\item In an observational study, we sample two types of people from the population, those who choose to work out regularly and those who don't. Then, we find the average energy level for the two groups of people and compare.

		\item In an experiment, we sample a group of people from the population, and we randomly assign these people into two groups: those who will regularly work out throughout the course of study, and those who will not. The difference is that the decision of whether to work out or not is not left up to the subjects as in the observational study, but is instead imposed by researcher. At the end, we compare the energy level of the two groups.
	\end{itemize}
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1467); %set diagram left start at 0, and has height of 1467
		
		%Rounded Rect [id:dp7531447764479342] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (74.93,319.63) .. controls (75.49,319.64) and (75.95,320.1) .. (75.94,320.67) -- (75.6,341.3) .. controls (75.59,341.87) and (75.12,342.32) .. (74.55,342.32) -- (71.47,342.28) .. controls (70.91,342.27) and (70.45,341.81) .. (70.46,341.24) -- (70.8,320.61) .. controls (70.81,320.04) and (71.28,319.59) .. (71.85,319.59) -- cycle ;
		%Rounded Rect [id:dp5879222231151007] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (81.47,319.7) .. controls (82.03,319.7) and (82.49,320.17) .. (82.48,320.73) -- (82.14,341.37) .. controls (82.13,341.93) and (81.66,342.39) .. (81.09,342.38) -- (78.01,342.34) .. controls (77.45,342.33) and (77,341.87) .. (77,341.3) -- (77.34,320.67) .. controls (77.35,320.1) and (77.82,319.65) .. (78.39,319.66) -- cycle ;
		%Flowchart: Stored Data [id:dp6054432153222404] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (82.66,299.28) -- (82.46,325.58) .. controls (82.48,322.81) and (79.87,320.55) .. (76.64,320.53) .. controls (73.41,320.52) and (70.77,322.74) .. (70.75,325.51) -- (70.96,299.21) .. controls (70.98,296.44) and (73.62,294.21) .. (76.85,294.23) .. controls (80.08,294.25) and (82.68,296.51) .. (82.66,299.28) -- cycle ;
		%Rounded Rect [id:dp8278415052066925] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (73.72,295.82) .. controls (74.97,296.39) and (75.42,297.81) .. (74.71,298.98) -- (65.14,314.83) .. controls (64.43,316.01) and (62.84,316.49) .. (61.58,315.92) -- (61.58,315.92) .. controls (60.33,315.35) and (59.89,313.94) .. (60.59,312.77) -- (70.17,296.91) .. controls (70.87,295.74) and (72.46,295.25) .. (73.72,295.82) -- cycle ;
		%Rounded Rect [id:dp7537389443715876] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (79.9,295.82) .. controls (78.65,296.4) and (78.2,297.81) .. (78.91,298.98) -- (88.48,314.83) .. controls (89.19,316.01) and (90.78,316.49) .. (92.04,315.92) -- (92.04,315.92) .. controls (93.29,315.35) and (93.74,313.94) .. (93.03,312.77) -- (83.46,296.92) .. controls (82.75,295.74) and (81.16,295.25) .. (79.9,295.82) -- cycle ;
		%Shape: Ellipse [id:dp8671450386270616] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (70.44,287.08) .. controls (70.44,283.95) and (73.37,281.4) .. (76.99,281.4) .. controls (80.61,281.4) and (83.54,283.95) .. (83.54,287.08) .. controls (83.54,290.22) and (80.61,292.76) .. (76.99,292.76) .. controls (73.37,292.76) and (70.44,290.22) .. (70.44,287.08) -- cycle ;
		
		%Rounded Rect [id:dp1750570388200119] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (93.24,331.24) .. controls (93.81,331.25) and (94.26,331.71) .. (94.25,332.28) -- (93.91,352.91) .. controls (93.9,353.48) and (93.44,353.93) .. (92.87,353.92) -- (89.79,353.89) .. controls (89.22,353.88) and (88.77,353.41) .. (88.78,352.85) -- (89.12,332.22) .. controls (89.13,331.65) and (89.59,331.19) .. (90.16,331.2) -- cycle ;
		%Rounded Rect [id:dp6384496458428623] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (99.78,331.3) .. controls (100.35,331.31) and (100.8,331.78) .. (100.79,332.34) -- (100.45,352.97) .. controls (100.44,353.54) and (99.98,353.99) .. (99.41,353.99) -- (96.33,353.95) .. controls (95.76,353.94) and (95.31,353.48) .. (95.32,352.91) -- (95.66,332.28) .. controls (95.67,331.71) and (96.13,331.26) .. (96.7,331.26) -- cycle ;
		%Flowchart: Stored Data [id:dp323634012354973] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (100.98,310.88) -- (100.77,337.19) .. controls (100.79,334.42) and (98.19,332.16) .. (94.96,332.14) .. controls (91.73,332.12) and (89.09,334.35) .. (89.07,337.12) -- (89.27,310.81) .. controls (89.29,308.05) and (91.93,305.82) .. (95.16,305.84) .. controls (98.4,305.86) and (101,308.12) .. (100.98,310.88) -- cycle ;
		%Rounded Rect [id:dp15514967875669528] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (92.03,307.43) .. controls (93.29,308) and (93.73,309.41) .. (93.02,310.59) -- (83.45,326.44) .. controls (82.74,327.61) and (81.15,328.1) .. (79.9,327.53) -- (79.9,327.53) .. controls (78.64,326.96) and (78.2,325.55) .. (78.91,324.38) -- (88.48,308.52) .. controls (89.19,307.35) and (90.78,306.86) .. (92.03,307.43) -- cycle ;
		%Rounded Rect [id:dp6981234288946547] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (98.22,307.43) .. controls (96.96,308) and (96.52,309.42) .. (97.22,310.59) -- (106.79,326.44) .. controls (107.5,327.61) and (109.09,328.1) .. (110.35,327.53) -- (110.35,327.53) .. controls (111.61,326.96) and (112.05,325.55) .. (111.34,324.37) -- (101.77,308.52) .. controls (101.07,307.35) and (99.47,306.86) .. (98.22,307.43) -- cycle ;
		%Shape: Ellipse [id:dp5065860844338106] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (88.76,298.69) .. controls (88.76,295.55) and (91.69,293.01) .. (95.31,293.01) .. controls (98.92,293.01) and (101.86,295.55) .. (101.86,298.69) .. controls (101.86,301.83) and (98.92,304.37) .. (95.31,304.37) .. controls (91.69,304.37) and (88.76,301.83) .. (88.76,298.69) -- cycle ;
		
		%Rounded Rect [id:dp3459010209016491] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (114.02,319.63) .. controls (114.59,319.64) and (115.04,320.1) .. (115.03,320.67) -- (114.69,341.3) .. controls (114.68,341.87) and (114.22,342.32) .. (113.65,342.32) -- (110.57,342.28) .. controls (110,342.27) and (109.55,341.81) .. (109.56,341.24) -- (109.9,320.61) .. controls (109.91,320.04) and (110.37,319.59) .. (110.94,319.59) -- cycle ;
		%Rounded Rect [id:dp2737674477827441] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (120.56,319.7) .. controls (121.13,319.7) and (121.58,320.17) .. (121.57,320.73) -- (121.23,341.37) .. controls (121.22,341.93) and (120.76,342.39) .. (120.19,342.38) -- (117.11,342.34) .. controls (116.54,342.33) and (116.09,341.87) .. (116.1,341.3) -- (116.44,320.67) .. controls (116.45,320.1) and (116.92,319.65) .. (117.48,319.66) -- cycle ;
		%Flowchart: Stored Data [id:dp9912232027157097] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (121.76,299.28) -- (121.55,325.58) .. controls (121.57,322.81) and (118.97,320.55) .. (115.74,320.53) .. controls (112.51,320.52) and (109.87,322.74) .. (109.85,325.51) -- (110.05,299.21) .. controls (110.08,296.44) and (112.71,294.21) .. (115.95,294.23) .. controls (119.18,294.25) and (121.78,296.51) .. (121.76,299.28) -- cycle ;
		%Rounded Rect [id:dp9340404666878124] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (112.81,295.82) .. controls (114.07,296.39) and (114.51,297.81) .. (113.81,298.98) -- (104.23,314.83) .. controls (103.53,316.01) and (101.93,316.49) .. (100.68,315.92) -- (100.68,315.92) .. controls (99.43,315.35) and (98.98,313.94) .. (99.69,312.77) -- (109.26,296.91) .. controls (109.97,295.74) and (111.56,295.25) .. (112.81,295.82) -- cycle ;
		%Rounded Rect [id:dp1385731928240428] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (119,295.82) .. controls (117.74,296.4) and (117.3,297.81) .. (118.01,298.98) -- (127.57,314.83) .. controls (128.28,316.01) and (129.88,316.49) .. (131.13,315.92) -- (131.13,315.92) .. controls (132.39,315.35) and (132.83,313.94) .. (132.12,312.77) -- (122.55,296.92) .. controls (121.85,295.74) and (120.25,295.25) .. (119,295.82) -- cycle ;
		%Shape: Ellipse [id:dp517607126802079] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (109.54,287.08) .. controls (109.54,283.95) and (112.47,281.4) .. (116.09,281.4) .. controls (119.7,281.4) and (122.64,283.95) .. (122.64,287.08) .. controls (122.64,290.22) and (119.7,292.76) .. (116.09,292.76) .. controls (112.47,292.76) and (109.54,290.22) .. (109.54,287.08) -- cycle ;
		
		%Rounded Rect [id:dp8442358067284719] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (131.98,331.24) .. controls (132.55,331.25) and (133,331.71) .. (132.99,332.28) -- (132.66,352.91) .. controls (132.65,353.48) and (132.18,353.93) .. (131.61,353.92) -- (128.53,353.89) .. controls (127.96,353.88) and (127.51,353.41) .. (127.52,352.85) -- (127.86,332.22) .. controls (127.87,331.65) and (128.34,331.19) .. (128.9,331.2) -- cycle ;
		%Rounded Rect [id:dp45703973202175807] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (138.52,331.3) .. controls (139.09,331.31) and (139.54,331.78) .. (139.53,332.34) -- (139.2,352.97) .. controls (139.19,353.54) and (138.72,353.99) .. (138.15,353.99) -- (135.07,353.95) .. controls (134.51,353.94) and (134.05,353.48) .. (134.06,352.91) -- (134.4,332.28) .. controls (134.41,331.71) and (134.88,331.26) .. (135.44,331.26) -- cycle ;
		%Flowchart: Stored Data [id:dp5906271013306594] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (139.72,310.88) -- (139.51,337.19) .. controls (139.54,334.42) and (136.93,332.16) .. (133.7,332.14) .. controls (130.47,332.12) and (127.83,334.35) .. (127.81,337.12) -- (128.02,310.81) .. controls (128.04,308.05) and (130.68,305.82) .. (133.91,305.84) .. controls (137.14,305.86) and (139.74,308.12) .. (139.72,310.88) -- cycle ;
		%Rounded Rect [id:dp5638139780598257] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (130.78,307.43) .. controls (132.03,308) and (132.47,309.41) .. (131.77,310.59) -- (122.2,326.44) .. controls (121.49,327.61) and (119.9,328.1) .. (118.64,327.53) -- (118.64,327.53) .. controls (117.39,326.96) and (116.94,325.55) .. (117.65,324.38) -- (127.22,308.52) .. controls (127.93,307.35) and (129.52,306.86) .. (130.78,307.43) -- cycle ;
		%Rounded Rect [id:dp08362043744534109] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (136.96,307.43) .. controls (135.7,308) and (135.26,309.42) .. (135.97,310.59) -- (145.54,326.44) .. controls (146.25,327.61) and (147.84,328.1) .. (149.09,327.53) -- (149.09,327.53) .. controls (150.35,326.96) and (150.79,325.55) .. (150.09,324.37) -- (140.52,308.52) .. controls (139.81,307.35) and (138.22,306.86) .. (136.96,307.43) -- cycle ;
		%Shape: Ellipse [id:dp9865599245263799] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (127.5,298.69) .. controls (127.5,295.55) and (130.43,293.01) .. (134.05,293.01) .. controls (137.67,293.01) and (140.6,295.55) .. (140.6,298.69) .. controls (140.6,301.83) and (137.67,304.37) .. (134.05,304.37) .. controls (130.43,304.37) and (127.5,301.83) .. (127.5,298.69) -- cycle ;
		
		%Rounded Rect [id:dp9245732098222528] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (153.12,319.63) .. controls (153.68,319.64) and (154.14,320.1) .. (154.13,320.67) -- (153.79,341.3) .. controls (153.78,341.87) and (153.31,342.32) .. (152.74,342.32) -- (149.66,342.28) .. controls (149.1,342.27) and (148.65,341.81) .. (148.65,341.24) -- (148.99,320.61) .. controls (149,320.04) and (149.47,319.59) .. (150.04,319.59) -- cycle ;
		%Rounded Rect [id:dp11726189998990777] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (159.66,319.7) .. controls (160.22,319.7) and (160.68,320.17) .. (160.67,320.73) -- (160.33,341.37) .. controls (160.32,341.93) and (159.85,342.39) .. (159.29,342.38) -- (156.2,342.34) .. controls (155.64,342.33) and (155.19,341.87) .. (155.2,341.3) -- (155.53,320.67) .. controls (155.54,320.1) and (156.01,319.65) .. (156.58,319.66) -- cycle ;
		%Flowchart: Stored Data [id:dp7576490879310387] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (160.85,299.28) -- (160.65,325.58) .. controls (160.67,322.81) and (158.07,320.55) .. (154.83,320.53) .. controls (151.6,320.52) and (148.96,322.74) .. (148.94,325.51) -- (149.15,299.21) .. controls (149.17,296.44) and (151.81,294.21) .. (155.04,294.23) .. controls (158.27,294.25) and (160.87,296.51) .. (160.85,299.28) -- cycle ;
		%Rounded Rect [id:dp3633126630225729] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (151.91,295.82) .. controls (153.16,296.39) and (153.61,297.81) .. (152.9,298.98) -- (143.33,314.83) .. controls (142.62,316.01) and (141.03,316.49) .. (139.77,315.92) -- (139.77,315.92) .. controls (138.52,315.35) and (138.08,313.94) .. (138.78,312.77) -- (148.36,296.91) .. controls (149.06,295.74) and (150.65,295.25) .. (151.91,295.82) -- cycle ;
		%Rounded Rect [id:dp931606584481073] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (158.09,295.82) .. controls (156.84,296.4) and (156.39,297.81) .. (157.1,298.98) -- (166.67,314.83) .. controls (167.38,316.01) and (168.97,316.49) .. (170.23,315.92) -- (170.23,315.92) .. controls (171.48,315.35) and (171.93,313.94) .. (171.22,312.77) -- (161.65,296.92) .. controls (160.94,295.74) and (159.35,295.25) .. (158.09,295.82) -- cycle ;
		%Shape: Ellipse [id:dp9045633473122392] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (148.63,287.08) .. controls (148.63,283.95) and (151.56,281.4) .. (155.18,281.4) .. controls (158.8,281.4) and (161.73,283.95) .. (161.73,287.08) .. controls (161.73,290.22) and (158.8,292.76) .. (155.18,292.76) .. controls (151.56,292.76) and (148.63,290.22) .. (148.63,287.08) -- cycle ;
		
		%Rounded Rect [id:dp2660881269427746] 
		\draw   (47.5,269.4) .. controls (47.5,257.03) and (57.53,247) .. (69.9,247) -- (165.1,247) .. controls (177.47,247) and (187.5,257.03) .. (187.5,269.4) -- (187.5,336.6) .. controls (187.5,348.97) and (177.47,359) .. (165.1,359) -- (69.9,359) .. controls (57.53,359) and (47.5,348.97) .. (47.5,336.6) -- cycle ;
		%Rounded Rect [id:dp35832837013652474] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (74.93,158.63) .. controls (75.49,158.64) and (75.95,159.1) .. (75.94,159.67) -- (75.6,180.3) .. controls (75.59,180.87) and (75.12,181.32) .. (74.55,181.32) -- (71.47,181.28) .. controls (70.91,181.27) and (70.45,180.81) .. (70.46,180.24) -- (70.8,159.61) .. controls (70.81,159.04) and (71.28,158.59) .. (71.85,158.59) -- cycle ;
		%Rounded Rect [id:dp7927185279630877] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (81.47,158.7) .. controls (82.03,158.7) and (82.49,159.17) .. (82.48,159.73) -- (82.14,180.37) .. controls (82.13,180.93) and (81.66,181.39) .. (81.09,181.38) -- (78.01,181.34) .. controls (77.45,181.33) and (77,180.87) .. (77,180.3) -- (77.34,159.67) .. controls (77.35,159.1) and (77.82,158.65) .. (78.39,158.66) -- cycle ;
		%Flowchart: Stored Data [id:dp8295544707381708] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (82.66,138.28) -- (82.46,164.58) .. controls (82.48,161.81) and (79.87,159.55) .. (76.64,159.53) .. controls (73.41,159.52) and (70.77,161.74) .. (70.75,164.51) -- (70.96,138.21) .. controls (70.98,135.44) and (73.62,133.21) .. (76.85,133.23) .. controls (80.08,133.25) and (82.68,135.51) .. (82.66,138.28) -- cycle ;
		%Rounded Rect [id:dp0478999702919809] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (73.72,134.82) .. controls (74.97,135.39) and (75.42,136.81) .. (74.71,137.98) -- (65.14,153.83) .. controls (64.43,155.01) and (62.84,155.49) .. (61.58,154.92) -- (61.58,154.92) .. controls (60.33,154.35) and (59.89,152.94) .. (60.59,151.77) -- (70.17,135.91) .. controls (70.87,134.74) and (72.46,134.25) .. (73.72,134.82) -- cycle ;
		%Rounded Rect [id:dp9160785910695406] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (79.9,134.82) .. controls (78.65,135.4) and (78.2,136.81) .. (78.91,137.98) -- (88.48,153.83) .. controls (89.19,155.01) and (90.78,155.49) .. (92.04,154.92) -- (92.04,154.92) .. controls (93.29,154.35) and (93.74,152.94) .. (93.03,151.77) -- (83.46,135.92) .. controls (82.75,134.74) and (81.16,134.25) .. (79.9,134.82) -- cycle ;
		%Shape: Ellipse [id:dp1708740952764778] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (70.44,126.08) .. controls (70.44,122.95) and (73.37,120.4) .. (76.99,120.4) .. controls (80.61,120.4) and (83.54,122.95) .. (83.54,126.08) .. controls (83.54,129.22) and (80.61,131.76) .. (76.99,131.76) .. controls (73.37,131.76) and (70.44,129.22) .. (70.44,126.08) -- cycle ;
		
		%Rounded Rect [id:dp33082703791130297] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (93.24,170.24) .. controls (93.81,170.25) and (94.26,170.71) .. (94.25,171.28) -- (93.91,191.91) .. controls (93.9,192.48) and (93.44,192.93) .. (92.87,192.92) -- (89.79,192.89) .. controls (89.22,192.88) and (88.77,192.41) .. (88.78,191.85) -- (89.12,171.22) .. controls (89.13,170.65) and (89.59,170.19) .. (90.16,170.2) -- cycle ;
		%Rounded Rect [id:dp08710198653191603] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (99.78,170.3) .. controls (100.35,170.31) and (100.8,170.78) .. (100.79,171.34) -- (100.45,191.97) .. controls (100.44,192.54) and (99.98,192.99) .. (99.41,192.99) -- (96.33,192.95) .. controls (95.76,192.94) and (95.31,192.48) .. (95.32,191.91) -- (95.66,171.28) .. controls (95.67,170.71) and (96.13,170.26) .. (96.7,170.26) -- cycle ;
		%Flowchart: Stored Data [id:dp7403206822796375] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (100.98,149.88) -- (100.77,176.19) .. controls (100.79,173.42) and (98.19,171.16) .. (94.96,171.14) .. controls (91.73,171.12) and (89.09,173.35) .. (89.07,176.12) -- (89.27,149.81) .. controls (89.29,147.05) and (91.93,144.82) .. (95.16,144.84) .. controls (98.4,144.86) and (101,147.12) .. (100.98,149.88) -- cycle ;
		%Rounded Rect [id:dp24256427111882162] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (92.03,146.43) .. controls (93.29,147) and (93.73,148.41) .. (93.02,149.59) -- (83.45,165.44) .. controls (82.74,166.61) and (81.15,167.1) .. (79.9,166.53) -- (79.9,166.53) .. controls (78.64,165.96) and (78.2,164.55) .. (78.91,163.38) -- (88.48,147.52) .. controls (89.19,146.35) and (90.78,145.86) .. (92.03,146.43) -- cycle ;
		%Rounded Rect [id:dp06045657997175646] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (98.22,146.43) .. controls (96.96,147) and (96.52,148.42) .. (97.22,149.59) -- (106.79,165.44) .. controls (107.5,166.61) and (109.09,167.1) .. (110.35,166.53) -- (110.35,166.53) .. controls (111.61,165.96) and (112.05,164.55) .. (111.34,163.37) -- (101.77,147.52) .. controls (101.07,146.35) and (99.47,145.86) .. (98.22,146.43) -- cycle ;
		%Shape: Ellipse [id:dp8726205742108515] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (88.76,137.69) .. controls (88.76,134.55) and (91.69,132.01) .. (95.31,132.01) .. controls (98.92,132.01) and (101.86,134.55) .. (101.86,137.69) .. controls (101.86,140.83) and (98.92,143.37) .. (95.31,143.37) .. controls (91.69,143.37) and (88.76,140.83) .. (88.76,137.69) -- cycle ;
		
		%Rounded Rect [id:dp7685343345194164] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (114.02,158.63) .. controls (114.59,158.64) and (115.04,159.1) .. (115.03,159.67) -- (114.69,180.3) .. controls (114.68,180.87) and (114.22,181.32) .. (113.65,181.32) -- (110.57,181.28) .. controls (110,181.27) and (109.55,180.81) .. (109.56,180.24) -- (109.9,159.61) .. controls (109.91,159.04) and (110.37,158.59) .. (110.94,158.59) -- cycle ;
		%Rounded Rect [id:dp7031681757222146] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (120.56,158.7) .. controls (121.13,158.7) and (121.58,159.17) .. (121.57,159.73) -- (121.23,180.37) .. controls (121.22,180.93) and (120.76,181.39) .. (120.19,181.38) -- (117.11,181.34) .. controls (116.54,181.33) and (116.09,180.87) .. (116.1,180.3) -- (116.44,159.67) .. controls (116.45,159.1) and (116.92,158.65) .. (117.48,158.66) -- cycle ;
		%Flowchart: Stored Data [id:dp994241899917865] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (121.76,138.28) -- (121.55,164.58) .. controls (121.57,161.81) and (118.97,159.55) .. (115.74,159.53) .. controls (112.51,159.52) and (109.87,161.74) .. (109.85,164.51) -- (110.05,138.21) .. controls (110.08,135.44) and (112.71,133.21) .. (115.95,133.23) .. controls (119.18,133.25) and (121.78,135.51) .. (121.76,138.28) -- cycle ;
		%Rounded Rect [id:dp33183153426443335] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (112.81,134.82) .. controls (114.07,135.39) and (114.51,136.81) .. (113.81,137.98) -- (104.23,153.83) .. controls (103.53,155.01) and (101.93,155.49) .. (100.68,154.92) -- (100.68,154.92) .. controls (99.43,154.35) and (98.98,152.94) .. (99.69,151.77) -- (109.26,135.91) .. controls (109.97,134.74) and (111.56,134.25) .. (112.81,134.82) -- cycle ;
		%Rounded Rect [id:dp06599109896366362] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (119,134.82) .. controls (117.74,135.4) and (117.3,136.81) .. (118.01,137.98) -- (127.57,153.83) .. controls (128.28,155.01) and (129.88,155.49) .. (131.13,154.92) -- (131.13,154.92) .. controls (132.39,154.35) and (132.83,152.94) .. (132.12,151.77) -- (122.55,135.92) .. controls (121.85,134.74) and (120.25,134.25) .. (119,134.82) -- cycle ;
		%Shape: Ellipse [id:dp604391797764791] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (109.54,126.08) .. controls (109.54,122.95) and (112.47,120.4) .. (116.09,120.4) .. controls (119.7,120.4) and (122.64,122.95) .. (122.64,126.08) .. controls (122.64,129.22) and (119.7,131.76) .. (116.09,131.76) .. controls (112.47,131.76) and (109.54,129.22) .. (109.54,126.08) -- cycle ;
		
		%Rounded Rect [id:dp9861228873708652] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (131.98,170.24) .. controls (132.55,170.25) and (133,170.71) .. (132.99,171.28) -- (132.66,191.91) .. controls (132.65,192.48) and (132.18,192.93) .. (131.61,192.92) -- (128.53,192.89) .. controls (127.96,192.88) and (127.51,192.41) .. (127.52,191.85) -- (127.86,171.22) .. controls (127.87,170.65) and (128.34,170.19) .. (128.9,170.2) -- cycle ;
		%Rounded Rect [id:dp4673638020473734] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (138.52,170.3) .. controls (139.09,170.31) and (139.54,170.78) .. (139.53,171.34) -- (139.2,191.97) .. controls (139.19,192.54) and (138.72,192.99) .. (138.15,192.99) -- (135.07,192.95) .. controls (134.51,192.94) and (134.05,192.48) .. (134.06,191.91) -- (134.4,171.28) .. controls (134.41,170.71) and (134.88,170.26) .. (135.44,170.26) -- cycle ;
		%Flowchart: Stored Data [id:dp6296118697970348] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (139.72,149.88) -- (139.51,176.19) .. controls (139.54,173.42) and (136.93,171.16) .. (133.7,171.14) .. controls (130.47,171.12) and (127.83,173.35) .. (127.81,176.12) -- (128.02,149.81) .. controls (128.04,147.05) and (130.68,144.82) .. (133.91,144.84) .. controls (137.14,144.86) and (139.74,147.12) .. (139.72,149.88) -- cycle ;
		%Rounded Rect [id:dp536356851352799] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (130.78,146.43) .. controls (132.03,147) and (132.47,148.41) .. (131.77,149.59) -- (122.2,165.44) .. controls (121.49,166.61) and (119.9,167.1) .. (118.64,166.53) -- (118.64,166.53) .. controls (117.39,165.96) and (116.94,164.55) .. (117.65,163.38) -- (127.22,147.52) .. controls (127.93,146.35) and (129.52,145.86) .. (130.78,146.43) -- cycle ;
		%Rounded Rect [id:dp9654943790704298] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (136.96,146.43) .. controls (135.7,147) and (135.26,148.42) .. (135.97,149.59) -- (145.54,165.44) .. controls (146.25,166.61) and (147.84,167.1) .. (149.09,166.53) -- (149.09,166.53) .. controls (150.35,165.96) and (150.79,164.55) .. (150.09,163.37) -- (140.52,147.52) .. controls (139.81,146.35) and (138.22,145.86) .. (136.96,146.43) -- cycle ;
		%Shape: Ellipse [id:dp25506010613694086] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (127.5,137.69) .. controls (127.5,134.55) and (130.43,132.01) .. (134.05,132.01) .. controls (137.67,132.01) and (140.6,134.55) .. (140.6,137.69) .. controls (140.6,140.83) and (137.67,143.37) .. (134.05,143.37) .. controls (130.43,143.37) and (127.5,140.83) .. (127.5,137.69) -- cycle ;
		
		%Rounded Rect [id:dp8341708384423276] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (153.12,158.63) .. controls (153.68,158.64) and (154.14,159.1) .. (154.13,159.67) -- (153.79,180.3) .. controls (153.78,180.87) and (153.31,181.32) .. (152.74,181.32) -- (149.66,181.28) .. controls (149.1,181.27) and (148.65,180.81) .. (148.65,180.24) -- (148.99,159.61) .. controls (149,159.04) and (149.47,158.59) .. (150.04,158.59) -- cycle ;
		%Rounded Rect [id:dp2285004034741569] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (159.66,158.7) .. controls (160.22,158.7) and (160.68,159.17) .. (160.67,159.73) -- (160.33,180.37) .. controls (160.32,180.93) and (159.85,181.39) .. (159.29,181.38) -- (156.2,181.34) .. controls (155.64,181.33) and (155.19,180.87) .. (155.2,180.3) -- (155.53,159.67) .. controls (155.54,159.1) and (156.01,158.65) .. (156.58,158.66) -- cycle ;
		%Flowchart: Stored Data [id:dp11316656463024] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (160.85,138.28) -- (160.65,164.58) .. controls (160.67,161.81) and (158.07,159.55) .. (154.83,159.53) .. controls (151.6,159.52) and (148.96,161.74) .. (148.94,164.51) -- (149.15,138.21) .. controls (149.17,135.44) and (151.81,133.21) .. (155.04,133.23) .. controls (158.27,133.25) and (160.87,135.51) .. (160.85,138.28) -- cycle ;
		%Rounded Rect [id:dp30594210738002525] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (151.91,134.82) .. controls (153.16,135.39) and (153.61,136.81) .. (152.9,137.98) -- (143.33,153.83) .. controls (142.62,155.01) and (141.03,155.49) .. (139.77,154.92) -- (139.77,154.92) .. controls (138.52,154.35) and (138.08,152.94) .. (138.78,151.77) -- (148.36,135.91) .. controls (149.06,134.74) and (150.65,134.25) .. (151.91,134.82) -- cycle ;
		%Rounded Rect [id:dp8698208984114857] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (158.09,134.82) .. controls (156.84,135.4) and (156.39,136.81) .. (157.1,137.98) -- (166.67,153.83) .. controls (167.38,155.01) and (168.97,155.49) .. (170.23,154.92) -- (170.23,154.92) .. controls (171.48,154.35) and (171.93,152.94) .. (171.22,151.77) -- (161.65,135.92) .. controls (160.94,134.74) and (159.35,134.25) .. (158.09,134.82) -- cycle ;
		%Shape: Ellipse [id:dp949229763817792] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (148.63,126.08) .. controls (148.63,122.95) and (151.56,120.4) .. (155.18,120.4) .. controls (158.8,120.4) and (161.73,122.95) .. (161.73,126.08) .. controls (161.73,129.22) and (158.8,131.76) .. (155.18,131.76) .. controls (151.56,131.76) and (148.63,129.22) .. (148.63,126.08) -- cycle ;
		
		%Rounded Rect [id:dp535442548220036] 
		\draw   (47.5,108.4) .. controls (47.5,96.03) and (57.53,86) .. (69.9,86) -- (165.1,86) .. controls (177.47,86) and (187.5,96.03) .. (187.5,108.4) -- (187.5,175.6) .. controls (187.5,187.97) and (177.47,198) .. (165.1,198) -- (69.9,198) .. controls (57.53,198) and (47.5,187.97) .. (47.5,175.6) -- cycle ;
		%Straight Lines [id:da44461721139621857] 
		\draw    (188,303) -- (252.5,303) ;
		\draw [shift={(254.5,303)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da38105235647821334] 
		\draw    (187,141) -- (251.5,141) ;
		\draw [shift={(253.5,141)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Rectangle [id:dp3878096498470096] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}] (35.5,66.5) -- (330.5,66.5) -- (330.5,389.5) -- (35.5,389.5) -- cycle ;
		%Shape: Rectangle [id:dp613780229596826] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}] (337.5,66.5) -- (632.5,66.5) -- (632.5,389.5) -- (337.5,389.5) -- cycle ;
		%Rounded Rect [id:dp19783679095694406] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (369.93,234.63) .. controls (370.49,234.64) and (370.95,235.1) .. (370.94,235.67) -- (370.6,256.3) .. controls (370.59,256.87) and (370.12,257.32) .. (369.55,257.32) -- (366.47,257.28) .. controls (365.91,257.27) and (365.45,256.81) .. (365.46,256.24) -- (365.8,235.61) .. controls (365.81,235.04) and (366.28,234.59) .. (366.85,234.59) -- cycle ;
		%Rounded Rect [id:dp8192872306849086] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (376.47,234.7) .. controls (377.03,234.7) and (377.49,235.17) .. (377.48,235.73) -- (377.14,256.37) .. controls (377.13,256.93) and (376.66,257.39) .. (376.09,257.38) -- (373.01,257.34) .. controls (372.45,257.33) and (372,256.87) .. (372,256.3) -- (372.34,235.67) .. controls (372.35,235.1) and (372.82,234.65) .. (373.39,234.66) -- cycle ;
		%Flowchart: Stored Data [id:dp16754982340599445] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (377.66,214.28) -- (377.46,240.58) .. controls (377.48,237.81) and (374.87,235.55) .. (371.64,235.53) .. controls (368.41,235.52) and (365.77,237.74) .. (365.75,240.51) -- (365.96,214.21) .. controls (365.98,211.44) and (368.62,209.21) .. (371.85,209.23) .. controls (375.08,209.25) and (377.68,211.51) .. (377.66,214.28) -- cycle ;
		%Rounded Rect [id:dp21413335834456482] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (368.72,210.82) .. controls (369.97,211.39) and (370.42,212.81) .. (369.71,213.98) -- (360.14,229.83) .. controls (359.43,231.01) and (357.84,231.49) .. (356.58,230.92) -- (356.58,230.92) .. controls (355.33,230.35) and (354.89,228.94) .. (355.59,227.77) -- (365.17,211.91) .. controls (365.87,210.74) and (367.46,210.25) .. (368.72,210.82) -- cycle ;
		%Rounded Rect [id:dp4591798082569396] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (374.9,210.82) .. controls (373.65,211.4) and (373.2,212.81) .. (373.91,213.98) -- (383.48,229.83) .. controls (384.19,231.01) and (385.78,231.49) .. (387.04,230.92) -- (387.04,230.92) .. controls (388.29,230.35) and (388.74,228.94) .. (388.03,227.77) -- (378.46,211.92) .. controls (377.75,210.74) and (376.16,210.25) .. (374.9,210.82) -- cycle ;
		%Shape: Ellipse [id:dp44427933678131093] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (365.44,202.08) .. controls (365.44,198.95) and (368.37,196.4) .. (371.99,196.4) .. controls (375.61,196.4) and (378.54,198.95) .. (378.54,202.08) .. controls (378.54,205.22) and (375.61,207.76) .. (371.99,207.76) .. controls (368.37,207.76) and (365.44,205.22) .. (365.44,202.08) -- cycle ;
		
		%Rounded Rect [id:dp8248459265599106] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (388.24,246.24) .. controls (388.81,246.25) and (389.26,246.71) .. (389.25,247.28) -- (388.91,267.91) .. controls (388.9,268.48) and (388.44,268.93) .. (387.87,268.92) -- (384.79,268.89) .. controls (384.22,268.88) and (383.77,268.41) .. (383.78,267.85) -- (384.12,247.22) .. controls (384.13,246.65) and (384.59,246.19) .. (385.16,246.2) -- cycle ;
		%Rounded Rect [id:dp7656740652583467] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (394.78,246.3) .. controls (395.35,246.31) and (395.8,246.78) .. (395.79,247.34) -- (395.45,267.97) .. controls (395.44,268.54) and (394.98,268.99) .. (394.41,268.99) -- (391.33,268.95) .. controls (390.76,268.94) and (390.31,268.48) .. (390.32,267.91) -- (390.66,247.28) .. controls (390.67,246.71) and (391.13,246.26) .. (391.7,246.26) -- cycle ;
		%Flowchart: Stored Data [id:dp8938655797364359] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (395.98,225.88) -- (395.77,252.19) .. controls (395.79,249.42) and (393.19,247.16) .. (389.96,247.14) .. controls (386.73,247.12) and (384.09,249.35) .. (384.07,252.12) -- (384.27,225.81) .. controls (384.29,223.05) and (386.93,220.82) .. (390.16,220.84) .. controls (393.4,220.86) and (396,223.12) .. (395.98,225.88) -- cycle ;
		%Rounded Rect [id:dp5112708370249794] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (387.03,222.43) .. controls (388.29,223) and (388.73,224.41) .. (388.02,225.59) -- (378.45,241.44) .. controls (377.74,242.61) and (376.15,243.1) .. (374.9,242.53) -- (374.9,242.53) .. controls (373.64,241.96) and (373.2,240.55) .. (373.91,239.38) -- (383.48,223.52) .. controls (384.19,222.35) and (385.78,221.86) .. (387.03,222.43) -- cycle ;
		%Rounded Rect [id:dp5512828729517187] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (393.22,222.43) .. controls (391.96,223) and (391.52,224.42) .. (392.22,225.59) -- (401.79,241.44) .. controls (402.5,242.61) and (404.09,243.1) .. (405.35,242.53) -- (405.35,242.53) .. controls (406.61,241.96) and (407.05,240.55) .. (406.34,239.37) -- (396.77,223.52) .. controls (396.07,222.35) and (394.47,221.86) .. (393.22,222.43) -- cycle ;
		%Shape: Ellipse [id:dp5857349160083087] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (383.76,213.69) .. controls (383.76,210.55) and (386.69,208.01) .. (390.31,208.01) .. controls (393.92,208.01) and (396.86,210.55) .. (396.86,213.69) .. controls (396.86,216.83) and (393.92,219.37) .. (390.31,219.37) .. controls (386.69,219.37) and (383.76,216.83) .. (383.76,213.69) -- cycle ;
		
		%Rounded Rect [id:dp890205561678965] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (409.02,234.63) .. controls (409.59,234.64) and (410.04,235.1) .. (410.03,235.67) -- (409.69,256.3) .. controls (409.68,256.87) and (409.22,257.32) .. (408.65,257.32) -- (405.57,257.28) .. controls (405,257.27) and (404.55,256.81) .. (404.56,256.24) -- (404.9,235.61) .. controls (404.91,235.04) and (405.37,234.59) .. (405.94,234.59) -- cycle ;
		%Rounded Rect [id:dp0004974504743859853] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (415.56,234.7) .. controls (416.13,234.7) and (416.58,235.17) .. (416.57,235.73) -- (416.23,256.37) .. controls (416.22,256.93) and (415.76,257.39) .. (415.19,257.38) -- (412.11,257.34) .. controls (411.54,257.33) and (411.09,256.87) .. (411.1,256.3) -- (411.44,235.67) .. controls (411.45,235.1) and (411.92,234.65) .. (412.48,234.66) -- cycle ;
		%Flowchart: Stored Data [id:dp7551393669941329] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (416.76,214.28) -- (416.55,240.58) .. controls (416.57,237.81) and (413.97,235.55) .. (410.74,235.53) .. controls (407.51,235.52) and (404.87,237.74) .. (404.85,240.51) -- (405.05,214.21) .. controls (405.08,211.44) and (407.71,209.21) .. (410.95,209.23) .. controls (414.18,209.25) and (416.78,211.51) .. (416.76,214.28) -- cycle ;
		%Rounded Rect [id:dp1969169318200341] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (407.81,210.82) .. controls (409.07,211.39) and (409.51,212.81) .. (408.81,213.98) -- (399.23,229.83) .. controls (398.53,231.01) and (396.93,231.49) .. (395.68,230.92) -- (395.68,230.92) .. controls (394.43,230.35) and (393.98,228.94) .. (394.69,227.77) -- (404.26,211.91) .. controls (404.97,210.74) and (406.56,210.25) .. (407.81,210.82) -- cycle ;
		%Rounded Rect [id:dp8951579435099439] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (414,210.82) .. controls (412.74,211.4) and (412.3,212.81) .. (413.01,213.98) -- (422.57,229.83) .. controls (423.28,231.01) and (424.88,231.49) .. (426.13,230.92) -- (426.13,230.92) .. controls (427.39,230.35) and (427.83,228.94) .. (427.12,227.77) -- (417.55,211.92) .. controls (416.85,210.74) and (415.25,210.25) .. (414,210.82) -- cycle ;
		%Shape: Ellipse [id:dp7580027679435124] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (404.54,202.08) .. controls (404.54,198.95) and (407.47,196.4) .. (411.09,196.4) .. controls (414.7,196.4) and (417.64,198.95) .. (417.64,202.08) .. controls (417.64,205.22) and (414.7,207.76) .. (411.09,207.76) .. controls (407.47,207.76) and (404.54,205.22) .. (404.54,202.08) -- cycle ;
		
		%Rounded Rect [id:dp09197414007900884] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (426.98,246.24) .. controls (427.55,246.25) and (428,246.71) .. (427.99,247.28) -- (427.66,267.91) .. controls (427.65,268.48) and (427.18,268.93) .. (426.61,268.92) -- (423.53,268.89) .. controls (422.96,268.88) and (422.51,268.41) .. (422.52,267.85) -- (422.86,247.22) .. controls (422.87,246.65) and (423.34,246.19) .. (423.9,246.2) -- cycle ;
		%Rounded Rect [id:dp028720828031321588] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (433.52,246.3) .. controls (434.09,246.31) and (434.54,246.78) .. (434.53,247.34) -- (434.2,267.97) .. controls (434.19,268.54) and (433.72,268.99) .. (433.15,268.99) -- (430.07,268.95) .. controls (429.51,268.94) and (429.05,268.48) .. (429.06,267.91) -- (429.4,247.28) .. controls (429.41,246.71) and (429.88,246.26) .. (430.44,246.26) -- cycle ;
		%Flowchart: Stored Data [id:dp7658040322604782] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (434.72,225.88) -- (434.51,252.19) .. controls (434.54,249.42) and (431.93,247.16) .. (428.7,247.14) .. controls (425.47,247.12) and (422.83,249.35) .. (422.81,252.12) -- (423.02,225.81) .. controls (423.04,223.05) and (425.68,220.82) .. (428.91,220.84) .. controls (432.14,220.86) and (434.74,223.12) .. (434.72,225.88) -- cycle ;
		%Rounded Rect [id:dp41692038342204474] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (425.78,222.43) .. controls (427.03,223) and (427.47,224.41) .. (426.77,225.59) -- (417.2,241.44) .. controls (416.49,242.61) and (414.9,243.1) .. (413.64,242.53) -- (413.64,242.53) .. controls (412.39,241.96) and (411.94,240.55) .. (412.65,239.38) -- (422.22,223.52) .. controls (422.93,222.35) and (424.52,221.86) .. (425.78,222.43) -- cycle ;
		%Rounded Rect [id:dp5157067974614329] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (431.96,222.43) .. controls (430.7,223) and (430.26,224.42) .. (430.97,225.59) -- (440.54,241.44) .. controls (441.25,242.61) and (442.84,243.1) .. (444.09,242.53) -- (444.09,242.53) .. controls (445.35,241.96) and (445.79,240.55) .. (445.09,239.37) -- (435.52,223.52) .. controls (434.81,222.35) and (433.22,221.86) .. (431.96,222.43) -- cycle ;
		%Shape: Ellipse [id:dp5230942272316998] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (422.5,213.69) .. controls (422.5,210.55) and (425.43,208.01) .. (429.05,208.01) .. controls (432.67,208.01) and (435.6,210.55) .. (435.6,213.69) .. controls (435.6,216.83) and (432.67,219.37) .. (429.05,219.37) .. controls (425.43,219.37) and (422.5,216.83) .. (422.5,213.69) -- cycle ;
		
		%Rounded Rect [id:dp6600243077063295] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (448.12,234.63) .. controls (448.68,234.64) and (449.14,235.1) .. (449.13,235.67) -- (448.79,256.3) .. controls (448.78,256.87) and (448.31,257.32) .. (447.74,257.32) -- (444.66,257.28) .. controls (444.1,257.27) and (443.65,256.81) .. (443.65,256.24) -- (443.99,235.61) .. controls (444,235.04) and (444.47,234.59) .. (445.04,234.59) -- cycle ;
		%Rounded Rect [id:dp6959629095358828] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (454.66,234.7) .. controls (455.22,234.7) and (455.68,235.17) .. (455.67,235.73) -- (455.33,256.37) .. controls (455.32,256.93) and (454.85,257.39) .. (454.29,257.38) -- (451.2,257.34) .. controls (450.64,257.33) and (450.19,256.87) .. (450.2,256.3) -- (450.53,235.67) .. controls (450.54,235.1) and (451.01,234.65) .. (451.58,234.66) -- cycle ;
		%Flowchart: Stored Data [id:dp061437497569677646] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (455.85,214.28) -- (455.65,240.58) .. controls (455.67,237.81) and (453.07,235.55) .. (449.83,235.53) .. controls (446.6,235.52) and (443.96,237.74) .. (443.94,240.51) -- (444.15,214.21) .. controls (444.17,211.44) and (446.81,209.21) .. (450.04,209.23) .. controls (453.27,209.25) and (455.87,211.51) .. (455.85,214.28) -- cycle ;
		%Rounded Rect [id:dp19303101905617792] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (446.91,210.82) .. controls (448.16,211.39) and (448.61,212.81) .. (447.9,213.98) -- (438.33,229.83) .. controls (437.62,231.01) and (436.03,231.49) .. (434.77,230.92) -- (434.77,230.92) .. controls (433.52,230.35) and (433.08,228.94) .. (433.78,227.77) -- (443.36,211.91) .. controls (444.06,210.74) and (445.65,210.25) .. (446.91,210.82) -- cycle ;
		%Rounded Rect [id:dp20047104987708253] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (453.09,210.82) .. controls (451.84,211.4) and (451.39,212.81) .. (452.1,213.98) -- (461.67,229.83) .. controls (462.38,231.01) and (463.97,231.49) .. (465.23,230.92) -- (465.23,230.92) .. controls (466.48,230.35) and (466.93,228.94) .. (466.22,227.77) -- (456.65,211.92) .. controls (455.94,210.74) and (454.35,210.25) .. (453.09,210.82) -- cycle ;
		%Shape: Ellipse [id:dp7071016688077341] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (443.63,202.08) .. controls (443.63,198.95) and (446.56,196.4) .. (450.18,196.4) .. controls (453.8,196.4) and (456.73,198.95) .. (456.73,202.08) .. controls (456.73,205.22) and (453.8,207.76) .. (450.18,207.76) .. controls (446.56,207.76) and (443.63,205.22) .. (443.63,202.08) -- cycle ;
		
		%Rounded Rect [id:dp6025446784161821] 
		\draw   (342.5,212.4) .. controls (342.5,200.03) and (352.53,190) .. (364.9,190) -- (460.1,190) .. controls (472.47,190) and (482.5,200.03) .. (482.5,212.4) -- (482.5,279.6) .. controls (482.5,291.97) and (472.47,302) .. (460.1,302) -- (364.9,302) .. controls (352.53,302) and (342.5,291.97) .. (342.5,279.6) -- cycle ;
		%Rounded Rect [id:dp8172940972820371] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (515.93,234.63) .. controls (516.49,234.64) and (516.95,235.1) .. (516.94,235.67) -- (516.6,256.3) .. controls (516.59,256.87) and (516.12,257.32) .. (515.55,257.32) -- (512.47,257.28) .. controls (511.91,257.27) and (511.45,256.81) .. (511.46,256.24) -- (511.8,235.61) .. controls (511.81,235.04) and (512.28,234.59) .. (512.85,234.59) -- cycle ;
		%Rounded Rect [id:dp5258391592387985] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (522.47,234.7) .. controls (523.03,234.7) and (523.49,235.17) .. (523.48,235.73) -- (523.14,256.37) .. controls (523.13,256.93) and (522.66,257.39) .. (522.09,257.38) -- (519.01,257.34) .. controls (518.45,257.33) and (518,256.87) .. (518,256.3) -- (518.34,235.67) .. controls (518.35,235.1) and (518.82,234.65) .. (519.39,234.66) -- cycle ;
		%Flowchart: Stored Data [id:dp061305683147764745] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (523.66,214.28) -- (523.46,240.58) .. controls (523.48,237.81) and (520.87,235.55) .. (517.64,235.53) .. controls (514.41,235.52) and (511.77,237.74) .. (511.75,240.51) -- (511.96,214.21) .. controls (511.98,211.44) and (514.62,209.21) .. (517.85,209.23) .. controls (521.08,209.25) and (523.68,211.51) .. (523.66,214.28) -- cycle ;
		%Rounded Rect [id:dp8886255520927606] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (514.72,210.82) .. controls (515.97,211.39) and (516.42,212.81) .. (515.71,213.98) -- (506.14,229.83) .. controls (505.43,231.01) and (503.84,231.49) .. (502.58,230.92) -- (502.58,230.92) .. controls (501.33,230.35) and (500.89,228.94) .. (501.59,227.77) -- (511.17,211.91) .. controls (511.87,210.74) and (513.46,210.25) .. (514.72,210.82) -- cycle ;
		%Rounded Rect [id:dp8165185739242578] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (520.9,210.82) .. controls (519.65,211.4) and (519.2,212.81) .. (519.91,213.98) -- (529.48,229.83) .. controls (530.19,231.01) and (531.78,231.49) .. (533.04,230.92) -- (533.04,230.92) .. controls (534.29,230.35) and (534.74,228.94) .. (534.03,227.77) -- (524.46,211.92) .. controls (523.75,210.74) and (522.16,210.25) .. (520.9,210.82) -- cycle ;
		%Shape: Ellipse [id:dp22211802501813116] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (511.44,202.08) .. controls (511.44,198.95) and (514.37,196.4) .. (517.99,196.4) .. controls (521.61,196.4) and (524.54,198.95) .. (524.54,202.08) .. controls (524.54,205.22) and (521.61,207.76) .. (517.99,207.76) .. controls (514.37,207.76) and (511.44,205.22) .. (511.44,202.08) -- cycle ;
		
		%Rounded Rect [id:dp6763730971966841] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (534.24,246.24) .. controls (534.81,246.25) and (535.26,246.71) .. (535.25,247.28) -- (534.91,267.91) .. controls (534.9,268.48) and (534.44,268.93) .. (533.87,268.92) -- (530.79,268.89) .. controls (530.22,268.88) and (529.77,268.41) .. (529.78,267.85) -- (530.12,247.22) .. controls (530.13,246.65) and (530.59,246.19) .. (531.16,246.2) -- cycle ;
		%Rounded Rect [id:dp5464530536372028] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (540.78,246.3) .. controls (541.35,246.31) and (541.8,246.78) .. (541.79,247.34) -- (541.45,267.97) .. controls (541.44,268.54) and (540.98,268.99) .. (540.41,268.99) -- (537.33,268.95) .. controls (536.76,268.94) and (536.31,268.48) .. (536.32,267.91) -- (536.66,247.28) .. controls (536.67,246.71) and (537.13,246.26) .. (537.7,246.26) -- cycle ;
		%Flowchart: Stored Data [id:dp11445095928168536] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (541.98,225.88) -- (541.77,252.19) .. controls (541.79,249.42) and (539.19,247.16) .. (535.96,247.14) .. controls (532.73,247.12) and (530.09,249.35) .. (530.07,252.12) -- (530.27,225.81) .. controls (530.29,223.05) and (532.93,220.82) .. (536.16,220.84) .. controls (539.4,220.86) and (542,223.12) .. (541.98,225.88) -- cycle ;
		%Rounded Rect [id:dp7206201771371892] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (533.03,222.43) .. controls (534.29,223) and (534.73,224.41) .. (534.02,225.59) -- (524.45,241.44) .. controls (523.74,242.61) and (522.15,243.1) .. (520.9,242.53) -- (520.9,242.53) .. controls (519.64,241.96) and (519.2,240.55) .. (519.91,239.38) -- (529.48,223.52) .. controls (530.19,222.35) and (531.78,221.86) .. (533.03,222.43) -- cycle ;
		%Rounded Rect [id:dp9921397384217994] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (539.22,222.43) .. controls (537.96,223) and (537.52,224.42) .. (538.22,225.59) -- (547.79,241.44) .. controls (548.5,242.61) and (550.09,243.1) .. (551.35,242.53) -- (551.35,242.53) .. controls (552.61,241.96) and (553.05,240.55) .. (552.34,239.37) -- (542.77,223.52) .. controls (542.07,222.35) and (540.47,221.86) .. (539.22,222.43) -- cycle ;
		%Shape: Ellipse [id:dp4665363071990245] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (529.76,213.69) .. controls (529.76,210.55) and (532.69,208.01) .. (536.31,208.01) .. controls (539.92,208.01) and (542.86,210.55) .. (542.86,213.69) .. controls (542.86,216.83) and (539.92,219.37) .. (536.31,219.37) .. controls (532.69,219.37) and (529.76,216.83) .. (529.76,213.69) -- cycle ;
		
		%Rounded Rect [id:dp9685857544305849] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (555.02,234.63) .. controls (555.59,234.64) and (556.04,235.1) .. (556.03,235.67) -- (555.69,256.3) .. controls (555.68,256.87) and (555.22,257.32) .. (554.65,257.32) -- (551.57,257.28) .. controls (551,257.27) and (550.55,256.81) .. (550.56,256.24) -- (550.9,235.61) .. controls (550.91,235.04) and (551.37,234.59) .. (551.94,234.59) -- cycle ;
		%Rounded Rect [id:dp5165872303822192] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (561.56,234.7) .. controls (562.13,234.7) and (562.58,235.17) .. (562.57,235.73) -- (562.23,256.37) .. controls (562.22,256.93) and (561.76,257.39) .. (561.19,257.38) -- (558.11,257.34) .. controls (557.54,257.33) and (557.09,256.87) .. (557.1,256.3) -- (557.44,235.67) .. controls (557.45,235.1) and (557.92,234.65) .. (558.48,234.66) -- cycle ;
		%Flowchart: Stored Data [id:dp08260731722074088] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (562.76,214.28) -- (562.55,240.58) .. controls (562.57,237.81) and (559.97,235.55) .. (556.74,235.53) .. controls (553.51,235.52) and (550.87,237.74) .. (550.85,240.51) -- (551.05,214.21) .. controls (551.08,211.44) and (553.71,209.21) .. (556.95,209.23) .. controls (560.18,209.25) and (562.78,211.51) .. (562.76,214.28) -- cycle ;
		%Rounded Rect [id:dp4101932011998046] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (553.81,210.82) .. controls (555.07,211.39) and (555.51,212.81) .. (554.81,213.98) -- (545.23,229.83) .. controls (544.53,231.01) and (542.93,231.49) .. (541.68,230.92) -- (541.68,230.92) .. controls (540.43,230.35) and (539.98,228.94) .. (540.69,227.77) -- (550.26,211.91) .. controls (550.97,210.74) and (552.56,210.25) .. (553.81,210.82) -- cycle ;
		%Rounded Rect [id:dp7454330636885991] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (560,210.82) .. controls (558.74,211.4) and (558.3,212.81) .. (559.01,213.98) -- (568.57,229.83) .. controls (569.28,231.01) and (570.88,231.49) .. (572.13,230.92) -- (572.13,230.92) .. controls (573.39,230.35) and (573.83,228.94) .. (573.12,227.77) -- (563.55,211.92) .. controls (562.85,210.74) and (561.25,210.25) .. (560,210.82) -- cycle ;
		%Shape: Ellipse [id:dp3292352574034161] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (550.54,202.08) .. controls (550.54,198.95) and (553.47,196.4) .. (557.09,196.4) .. controls (560.7,196.4) and (563.64,198.95) .. (563.64,202.08) .. controls (563.64,205.22) and (560.7,207.76) .. (557.09,207.76) .. controls (553.47,207.76) and (550.54,205.22) .. (550.54,202.08) -- cycle ;
		
		%Rounded Rect [id:dp4154123303426671] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (572.98,246.24) .. controls (573.55,246.25) and (574,246.71) .. (573.99,247.28) -- (573.66,267.91) .. controls (573.65,268.48) and (573.18,268.93) .. (572.61,268.92) -- (569.53,268.89) .. controls (568.96,268.88) and (568.51,268.41) .. (568.52,267.85) -- (568.86,247.22) .. controls (568.87,246.65) and (569.34,246.19) .. (569.9,246.2) -- cycle ;
		%Rounded Rect [id:dp5916422538787545] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (579.52,246.3) .. controls (580.09,246.31) and (580.54,246.78) .. (580.53,247.34) -- (580.2,267.97) .. controls (580.19,268.54) and (579.72,268.99) .. (579.15,268.99) -- (576.07,268.95) .. controls (575.51,268.94) and (575.05,268.48) .. (575.06,267.91) -- (575.4,247.28) .. controls (575.41,246.71) and (575.88,246.26) .. (576.44,246.26) -- cycle ;
		%Flowchart: Stored Data [id:dp9171010367189687] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (580.72,225.88) -- (580.51,252.19) .. controls (580.54,249.42) and (577.93,247.16) .. (574.7,247.14) .. controls (571.47,247.12) and (568.83,249.35) .. (568.81,252.12) -- (569.02,225.81) .. controls (569.04,223.05) and (571.68,220.82) .. (574.91,220.84) .. controls (578.14,220.86) and (580.74,223.12) .. (580.72,225.88) -- cycle ;
		%Rounded Rect [id:dp18809870518087513] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (571.78,222.43) .. controls (573.03,223) and (573.47,224.41) .. (572.77,225.59) -- (563.2,241.44) .. controls (562.49,242.61) and (560.9,243.1) .. (559.64,242.53) -- (559.64,242.53) .. controls (558.39,241.96) and (557.94,240.55) .. (558.65,239.38) -- (568.22,223.52) .. controls (568.93,222.35) and (570.52,221.86) .. (571.78,222.43) -- cycle ;
		%Rounded Rect [id:dp7246425614618988] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (577.96,222.43) .. controls (576.7,223) and (576.26,224.42) .. (576.97,225.59) -- (586.54,241.44) .. controls (587.25,242.61) and (588.84,243.1) .. (590.09,242.53) -- (590.09,242.53) .. controls (591.35,241.96) and (591.79,240.55) .. (591.09,239.37) -- (581.52,223.52) .. controls (580.81,222.35) and (579.22,221.86) .. (577.96,222.43) -- cycle ;
		%Shape: Ellipse [id:dp4250695581796393] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (568.5,213.69) .. controls (568.5,210.55) and (571.43,208.01) .. (575.05,208.01) .. controls (578.67,208.01) and (581.6,210.55) .. (581.6,213.69) .. controls (581.6,216.83) and (578.67,219.37) .. (575.05,219.37) .. controls (571.43,219.37) and (568.5,216.83) .. (568.5,213.69) -- cycle ;
		
		%Rounded Rect [id:dp9018594240203761] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (594.12,234.63) .. controls (594.68,234.64) and (595.14,235.1) .. (595.13,235.67) -- (594.79,256.3) .. controls (594.78,256.87) and (594.31,257.32) .. (593.74,257.32) -- (590.66,257.28) .. controls (590.1,257.27) and (589.65,256.81) .. (589.65,256.24) -- (589.99,235.61) .. controls (590,235.04) and (590.47,234.59) .. (591.04,234.59) -- cycle ;
		%Rounded Rect [id:dp5360977976421777] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (600.66,234.7) .. controls (601.22,234.7) and (601.68,235.17) .. (601.67,235.73) -- (601.33,256.37) .. controls (601.32,256.93) and (600.85,257.39) .. (600.29,257.38) -- (597.2,257.34) .. controls (596.64,257.33) and (596.19,256.87) .. (596.2,256.3) -- (596.53,235.67) .. controls (596.54,235.1) and (597.01,234.65) .. (597.58,234.66) -- cycle ;
		%Flowchart: Stored Data [id:dp9376567154386359] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (601.85,214.28) -- (601.65,240.58) .. controls (601.67,237.81) and (599.07,235.55) .. (595.83,235.53) .. controls (592.6,235.52) and (589.96,237.74) .. (589.94,240.51) -- (590.15,214.21) .. controls (590.17,211.44) and (592.81,209.21) .. (596.04,209.23) .. controls (599.27,209.25) and (601.87,211.51) .. (601.85,214.28) -- cycle ;
		%Rounded Rect [id:dp30762591583947674] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (592.91,210.82) .. controls (594.16,211.39) and (594.61,212.81) .. (593.9,213.98) -- (584.33,229.83) .. controls (583.62,231.01) and (582.03,231.49) .. (580.77,230.92) -- (580.77,230.92) .. controls (579.52,230.35) and (579.08,228.94) .. (579.78,227.77) -- (589.36,211.91) .. controls (590.06,210.74) and (591.65,210.25) .. (592.91,210.82) -- cycle ;
		%Rounded Rect [id:dp946972405840375] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (599.09,210.82) .. controls (597.84,211.4) and (597.39,212.81) .. (598.1,213.98) -- (607.67,229.83) .. controls (608.38,231.01) and (609.97,231.49) .. (611.23,230.92) -- (611.23,230.92) .. controls (612.48,230.35) and (612.93,228.94) .. (612.22,227.77) -- (602.65,211.92) .. controls (601.94,210.74) and (600.35,210.25) .. (599.09,210.82) -- cycle ;
		%Shape: Ellipse [id:dp118303613222692] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (589.63,202.08) .. controls (589.63,198.95) and (592.56,196.4) .. (596.18,196.4) .. controls (599.8,196.4) and (602.73,198.95) .. (602.73,202.08) .. controls (602.73,205.22) and (599.8,207.76) .. (596.18,207.76) .. controls (592.56,207.76) and (589.63,205.22) .. (589.63,202.08) -- cycle ;
		
		%Rounded Rect [id:dp8310001160024834] 
		\draw   (488.5,212.4) .. controls (488.5,200.03) and (498.53,190) .. (510.9,190) -- (606.1,190) .. controls (618.47,190) and (628.5,200.03) .. (628.5,212.4) -- (628.5,279.6) .. controls (628.5,291.97) and (618.47,302) .. (606.1,302) -- (510.9,302) .. controls (498.53,302) and (488.5,291.97) .. (488.5,279.6) -- cycle ;
		%Straight Lines [id:da8590203357411688] 
		\draw    (410,302) -- (410,343) ;
		\draw [shift={(410,345)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da39461717957936737] 
		\draw    (563,302) -- (563,343) ;
		\draw [shift={(563,345)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Rounded Rect [id:dp827787328601681] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (354.93,113.63) .. controls (355.49,113.64) and (355.95,114.1) .. (355.94,114.67) -- (355.6,135.3) .. controls (355.59,135.87) and (355.12,136.32) .. (354.55,136.32) -- (351.47,136.28) .. controls (350.91,136.27) and (350.45,135.81) .. (350.46,135.24) -- (350.8,114.61) .. controls (350.81,114.04) and (351.28,113.59) .. (351.85,113.59) -- cycle ;
		%Rounded Rect [id:dp2168240117998319] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (361.47,113.7) .. controls (362.03,113.7) and (362.49,114.17) .. (362.48,114.73) -- (362.14,135.37) .. controls (362.13,135.93) and (361.66,136.39) .. (361.09,136.38) -- (358.01,136.34) .. controls (357.45,136.33) and (357,135.87) .. (357,135.3) -- (357.34,114.67) .. controls (357.35,114.1) and (357.82,113.65) .. (358.39,113.66) -- cycle ;
		%Flowchart: Stored Data [id:dp24255881944598978] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (362.66,93.28) -- (362.46,119.58) .. controls (362.48,116.81) and (359.87,114.55) .. (356.64,114.53) .. controls (353.41,114.52) and (350.77,116.74) .. (350.75,119.51) -- (350.96,93.21) .. controls (350.98,90.44) and (353.62,88.21) .. (356.85,88.23) .. controls (360.08,88.25) and (362.68,90.51) .. (362.66,93.28) -- cycle ;
		%Rounded Rect [id:dp4073536851533599] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (353.72,89.82) .. controls (354.97,90.39) and (355.42,91.81) .. (354.71,92.98) -- (345.14,108.83) .. controls (344.43,110.01) and (342.84,110.49) .. (341.58,109.92) -- (341.58,109.92) .. controls (340.33,109.35) and (339.89,107.94) .. (340.59,106.77) -- (350.17,90.91) .. controls (350.87,89.74) and (352.46,89.25) .. (353.72,89.82) -- cycle ;
		%Rounded Rect [id:dp8944939014866629] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (359.9,89.82) .. controls (358.65,90.4) and (358.2,91.81) .. (358.91,92.98) -- (368.48,108.83) .. controls (369.19,110.01) and (370.78,110.49) .. (372.04,109.92) -- (372.04,109.92) .. controls (373.29,109.35) and (373.74,107.94) .. (373.03,106.77) -- (363.46,90.92) .. controls (362.75,89.74) and (361.16,89.25) .. (359.9,89.82) -- cycle ;
		%Shape: Ellipse [id:dp9401966910902873] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (350.44,81.08) .. controls (350.44,77.95) and (353.37,75.4) .. (356.99,75.4) .. controls (360.61,75.4) and (363.54,77.95) .. (363.54,81.08) .. controls (363.54,84.22) and (360.61,86.76) .. (356.99,86.76) .. controls (353.37,86.76) and (350.44,84.22) .. (350.44,81.08) -- cycle ;
		
		%Rounded Rect [id:dp5051266893946491] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (383.48,113.63) .. controls (384.05,113.64) and (384.5,114.1) .. (384.49,114.67) -- (384.16,135.3) .. controls (384.15,135.87) and (383.68,136.32) .. (383.11,136.32) -- (380.03,136.28) .. controls (379.47,136.27) and (379.01,135.81) .. (379.02,135.24) -- (379.36,114.61) .. controls (379.37,114.04) and (379.84,113.59) .. (380.4,113.59) -- cycle ;
		%Rounded Rect [id:dp22935750797962973] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (390.02,113.7) .. controls (390.59,113.7) and (391.04,114.17) .. (391.03,114.73) -- (390.7,135.37) .. controls (390.69,135.93) and (390.22,136.39) .. (389.65,136.38) -- (386.57,136.34) .. controls (386.01,136.33) and (385.55,135.87) .. (385.56,135.3) -- (385.9,114.67) .. controls (385.91,114.1) and (386.38,113.65) .. (386.94,113.66) -- cycle ;
		%Flowchart: Stored Data [id:dp2905648879732321] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (391.22,93.28) -- (391.01,119.58) .. controls (391.04,116.81) and (388.43,114.55) .. (385.2,114.53) .. controls (381.97,114.52) and (379.33,116.74) .. (379.31,119.51) -- (379.52,93.21) .. controls (379.54,90.44) and (382.18,88.21) .. (385.41,88.23) .. controls (388.64,88.25) and (391.24,90.51) .. (391.22,93.28) -- cycle ;
		%Rounded Rect [id:dp13757328126917856] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (382.28,89.82) .. controls (383.53,90.39) and (383.98,91.81) .. (383.27,92.98) -- (373.7,108.83) .. controls (372.99,110.01) and (371.4,110.49) .. (370.14,109.92) -- (370.14,109.92) .. controls (368.89,109.35) and (368.44,107.94) .. (369.15,106.77) -- (378.72,90.91) .. controls (379.43,89.74) and (381.02,89.25) .. (382.28,89.82) -- cycle ;
		%Rounded Rect [id:dp9246696857397219] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (388.46,89.82) .. controls (387.2,90.4) and (386.76,91.81) .. (387.47,92.98) -- (397.04,108.83) .. controls (397.75,110.01) and (399.34,110.49) .. (400.59,109.92) -- (400.59,109.92) .. controls (401.85,109.35) and (402.29,107.94) .. (401.59,106.77) -- (392.02,90.92) .. controls (391.31,89.74) and (389.72,89.25) .. (388.46,89.82) -- cycle ;
		%Shape: Ellipse [id:dp32875771617949123] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (379,81.08) .. controls (379,77.95) and (381.93,75.4) .. (385.55,75.4) .. controls (389.17,75.4) and (392.1,77.95) .. (392.1,81.08) .. controls (392.1,84.22) and (389.17,86.76) .. (385.55,86.76) .. controls (381.93,86.76) and (379,84.22) .. (379,81.08) -- cycle ;
		
		%Rounded Rect [id:dp27100355374432117] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (412.04,113.63) .. controls (412.61,113.64) and (413.06,114.1) .. (413.05,114.67) -- (412.71,135.3) .. controls (412.7,135.87) and (412.24,136.32) .. (411.67,136.32) -- (408.59,136.28) .. controls (408.02,136.27) and (407.57,135.81) .. (407.58,135.24) -- (407.92,114.61) .. controls (407.93,114.04) and (408.4,113.59) .. (408.96,113.59) -- cycle ;
		%Rounded Rect [id:dp6030851303166405] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (418.58,113.7) .. controls (419.15,113.7) and (419.6,114.17) .. (419.59,114.73) -- (419.25,135.37) .. controls (419.24,135.93) and (418.78,136.39) .. (418.21,136.38) -- (415.13,136.34) .. controls (414.56,136.33) and (414.11,135.87) .. (414.12,135.3) -- (414.46,114.67) .. controls (414.47,114.1) and (414.94,113.65) .. (415.5,113.66) -- cycle ;
		%Flowchart: Stored Data [id:dp6160903287741593] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (419.78,93.28) -- (419.57,119.58) .. controls (419.59,116.81) and (416.99,114.55) .. (413.76,114.53) .. controls (410.53,114.52) and (407.89,116.74) .. (407.87,119.51) -- (408.07,93.21) .. controls (408.1,90.44) and (410.73,88.21) .. (413.97,88.23) .. controls (417.2,88.25) and (419.8,90.51) .. (419.78,93.28) -- cycle ;
		%Rounded Rect [id:dp919668665484479] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (410.83,89.82) .. controls (412.09,90.39) and (412.53,91.81) .. (411.83,92.98) -- (402.25,108.83) .. controls (401.55,110.01) and (399.96,110.49) .. (398.7,109.92) -- (398.7,109.92) .. controls (397.45,109.35) and (397,107.94) .. (397.71,106.77) -- (407.28,90.91) .. controls (407.99,89.74) and (409.58,89.25) .. (410.83,89.82) -- cycle ;
		%Rounded Rect [id:dp9352815867056934] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (417.02,89.82) .. controls (415.76,90.4) and (415.32,91.81) .. (416.03,92.98) -- (425.6,108.83) .. controls (426.3,110.01) and (427.9,110.49) .. (429.15,109.92) -- (429.15,109.92) .. controls (430.41,109.35) and (430.85,107.94) .. (430.14,106.77) -- (420.57,90.92) .. controls (419.87,89.74) and (418.27,89.25) .. (417.02,89.82) -- cycle ;
		%Shape: Ellipse [id:dp9530174694828817] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (407.56,81.08) .. controls (407.56,77.95) and (410.49,75.4) .. (414.11,75.4) .. controls (417.72,75.4) and (420.66,77.95) .. (420.66,81.08) .. controls (420.66,84.22) and (417.72,86.76) .. (414.11,86.76) .. controls (410.49,86.76) and (407.56,84.22) .. (407.56,81.08) -- cycle ;
		
		%Rounded Rect [id:dp11509605973808101] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (440.6,113.63) .. controls (441.17,113.64) and (441.62,114.1) .. (441.61,114.67) -- (441.27,135.3) .. controls (441.26,135.87) and (440.8,136.32) .. (440.23,136.32) -- (437.15,136.28) .. controls (436.58,136.27) and (436.13,135.81) .. (436.14,135.24) -- (436.48,114.61) .. controls (436.49,114.04) and (436.95,113.59) .. (437.52,113.59) -- cycle ;
		%Rounded Rect [id:dp7698890894018406] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (447.14,113.7) .. controls (447.71,113.7) and (448.16,114.17) .. (448.15,114.73) -- (447.81,135.37) .. controls (447.8,135.93) and (447.34,136.39) .. (446.77,136.38) -- (443.69,136.34) .. controls (443.12,136.33) and (442.67,135.87) .. (442.68,135.3) -- (443.02,114.67) .. controls (443.03,114.1) and (443.49,113.65) .. (444.06,113.66) -- cycle ;
		%Flowchart: Stored Data [id:dp2972586846314429] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (448.34,93.28) -- (448.13,119.58) .. controls (448.15,116.81) and (445.55,114.55) .. (442.32,114.53) .. controls (439.09,114.52) and (436.45,116.74) .. (436.43,119.51) -- (436.63,93.21) .. controls (436.65,90.44) and (439.29,88.21) .. (442.52,88.23) .. controls (445.76,88.25) and (448.36,90.51) .. (448.34,93.28) -- cycle ;
		%Rounded Rect [id:dp9539903985399063] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (439.39,89.82) .. controls (440.65,90.39) and (441.09,91.81) .. (440.38,92.98) -- (430.81,108.83) .. controls (430.1,110.01) and (428.51,110.49) .. (427.26,109.92) -- (427.26,109.92) .. controls (426,109.35) and (425.56,107.94) .. (426.27,106.77) -- (435.84,90.91) .. controls (436.55,89.74) and (438.14,89.25) .. (439.39,89.82) -- cycle ;
		%Rounded Rect [id:dp29408025155333983] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (445.58,89.82) .. controls (444.32,90.4) and (443.88,91.81) .. (444.58,92.98) -- (454.15,108.83) .. controls (454.86,110.01) and (456.45,110.49) .. (457.71,109.92) -- (457.71,109.92) .. controls (458.97,109.35) and (459.41,107.94) .. (458.7,106.77) -- (449.13,90.92) .. controls (448.42,89.74) and (446.83,89.25) .. (445.58,89.82) -- cycle ;
		%Shape: Ellipse [id:dp24119883906504702] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (436.12,81.08) .. controls (436.12,77.95) and (439.05,75.4) .. (442.67,75.4) .. controls (446.28,75.4) and (449.22,77.95) .. (449.22,81.08) .. controls (449.22,84.22) and (446.28,86.76) .. (442.67,86.76) .. controls (439.05,86.76) and (436.12,84.22) .. (436.12,81.08) -- cycle ;
		
		%Rounded Rect [id:dp1091812004992283] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (469.16,113.63) .. controls (469.73,113.64) and (470.18,114.1) .. (470.17,114.67) -- (469.83,135.3) .. controls (469.82,135.87) and (469.35,136.32) .. (468.79,136.32) -- (465.71,136.28) .. controls (465.14,136.27) and (464.69,135.81) .. (464.7,135.24) -- (465.03,114.61) .. controls (465.04,114.04) and (465.51,113.59) .. (466.08,113.59) -- cycle ;
		%Rounded Rect [id:dp38681546130075106] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (475.7,113.7) .. controls (476.27,113.7) and (476.72,114.17) .. (476.71,114.73) -- (476.37,135.37) .. controls (476.36,135.93) and (475.89,136.39) .. (475.33,136.38) -- (472.25,136.34) .. controls (471.68,136.33) and (471.23,135.87) .. (471.24,135.3) -- (471.58,114.67) .. controls (471.58,114.1) and (472.05,113.65) .. (472.62,113.66) -- cycle ;
		%Flowchart: Stored Data [id:dp4056139137309618] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (476.89,93.28) -- (476.69,119.58) .. controls (476.71,116.81) and (474.11,114.55) .. (470.87,114.53) .. controls (467.64,114.52) and (465.01,116.74) .. (464.98,119.51) -- (465.19,93.21) .. controls (465.21,90.44) and (467.85,88.21) .. (471.08,88.23) .. controls (474.31,88.25) and (476.92,90.51) .. (476.89,93.28) -- cycle ;
		%Rounded Rect [id:dp43124501905929025] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (467.95,89.82) .. controls (469.21,90.39) and (469.65,91.81) .. (468.94,92.98) -- (459.37,108.83) .. controls (458.66,110.01) and (457.07,110.49) .. (455.82,109.92) -- (455.82,109.92) .. controls (454.56,109.35) and (454.12,107.94) .. (454.83,106.77) -- (464.4,90.91) .. controls (465.1,89.74) and (466.7,89.25) .. (467.95,89.82) -- cycle ;
		%Rounded Rect [id:dp04488647114686839] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (474.13,89.82) .. controls (472.88,90.4) and (472.43,91.81) .. (473.14,92.98) -- (482.71,108.83) .. controls (483.42,110.01) and (485.01,110.49) .. (486.27,109.92) -- (486.27,109.92) .. controls (487.52,109.35) and (487.97,107.94) .. (487.26,106.77) -- (477.69,90.92) .. controls (476.98,89.74) and (475.39,89.25) .. (474.13,89.82) -- cycle ;
		%Shape: Ellipse [id:dp28622927347507243] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (464.67,81.08) .. controls (464.67,77.95) and (467.61,75.4) .. (471.22,75.4) .. controls (474.84,75.4) and (477.77,77.95) .. (477.77,81.08) .. controls (477.77,84.22) and (474.84,86.76) .. (471.22,86.76) .. controls (467.61,86.76) and (464.67,84.22) .. (464.67,81.08) -- cycle ;
		
		%Rounded Rect [id:dp18005400739012067] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (497.72,113.63) .. controls (498.28,113.64) and (498.74,114.1) .. (498.73,114.67) -- (498.39,135.3) .. controls (498.38,135.87) and (497.91,136.32) .. (497.34,136.32) -- (494.26,136.28) .. controls (493.7,136.27) and (493.24,135.81) .. (493.25,135.24) -- (493.59,114.61) .. controls (493.6,114.04) and (494.07,113.59) .. (494.64,113.59) -- cycle ;
		%Rounded Rect [id:dp9644805231560121] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (504.26,113.7) .. controls (504.82,113.7) and (505.28,114.17) .. (505.27,114.73) -- (504.93,135.37) .. controls (504.92,135.93) and (504.45,136.39) .. (503.88,136.38) -- (500.8,136.34) .. controls (500.24,136.33) and (499.79,135.87) .. (499.79,135.3) -- (500.13,114.67) .. controls (500.14,114.1) and (500.61,113.65) .. (501.18,113.66) -- cycle ;
		%Flowchart: Stored Data [id:dp4378430437757428] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (505.45,93.28) -- (505.25,119.58) .. controls (505.27,116.81) and (502.66,114.55) .. (499.43,114.53) .. controls (496.2,114.52) and (493.56,116.74) .. (493.54,119.51) -- (493.75,93.21) .. controls (493.77,90.44) and (496.41,88.21) .. (499.64,88.23) .. controls (502.87,88.25) and (505.47,90.51) .. (505.45,93.28) -- cycle ;
		%Rounded Rect [id:dp37940107652394706] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (496.51,89.82) .. controls (497.76,90.39) and (498.21,91.81) .. (497.5,92.98) -- (487.93,108.83) .. controls (487.22,110.01) and (485.63,110.49) .. (484.37,109.92) -- (484.37,109.92) .. controls (483.12,109.35) and (482.68,107.94) .. (483.38,106.77) -- (492.96,90.91) .. controls (493.66,89.74) and (495.25,89.25) .. (496.51,89.82) -- cycle ;
		%Rounded Rect [id:dp4279796465526493] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (502.69,89.82) .. controls (501.44,90.4) and (500.99,91.81) .. (501.7,92.98) -- (511.27,108.83) .. controls (511.98,110.01) and (513.57,110.49) .. (514.83,109.92) -- (514.83,109.92) .. controls (516.08,109.35) and (516.53,107.94) .. (515.82,106.77) -- (506.25,90.92) .. controls (505.54,89.74) and (503.95,89.25) .. (502.69,89.82) -- cycle ;
		%Shape: Ellipse [id:dp6570213607100486] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (493.23,81.08) .. controls (493.23,77.95) and (496.16,75.4) .. (499.78,75.4) .. controls (503.4,75.4) and (506.33,77.95) .. (506.33,81.08) .. controls (506.33,84.22) and (503.4,86.76) .. (499.78,86.76) .. controls (496.16,86.76) and (493.23,84.22) .. (493.23,81.08) -- cycle ;
		
		%Rounded Rect [id:dp632230583038583] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (526.27,113.63) .. controls (526.84,113.64) and (527.29,114.1) .. (527.28,114.67) -- (526.95,135.3) .. controls (526.94,135.87) and (526.47,136.32) .. (525.9,136.32) -- (522.82,136.28) .. controls (522.25,136.27) and (521.8,135.81) .. (521.81,135.24) -- (522.15,114.61) .. controls (522.16,114.04) and (522.63,113.59) .. (523.19,113.59) -- cycle ;
		%Rounded Rect [id:dp7335302573382141] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (532.81,113.7) .. controls (533.38,113.7) and (533.83,114.17) .. (533.82,114.73) -- (533.49,135.37) .. controls (533.48,135.93) and (533.01,136.39) .. (532.44,136.38) -- (529.36,136.34) .. controls (528.8,136.33) and (528.34,135.87) .. (528.35,135.3) -- (528.69,114.67) .. controls (528.7,114.1) and (529.17,113.65) .. (529.73,113.66) -- cycle ;
		%Flowchart: Stored Data [id:dp007125509792853757] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (534.01,93.28) -- (533.8,119.58) .. controls (533.83,116.81) and (531.22,114.55) .. (527.99,114.53) .. controls (524.76,114.52) and (522.12,116.74) .. (522.1,119.51) -- (522.31,93.21) .. controls (522.33,90.44) and (524.97,88.21) .. (528.2,88.23) .. controls (531.43,88.25) and (534.03,90.51) .. (534.01,93.28) -- cycle ;
		%Rounded Rect [id:dp23753410895924976] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (525.07,89.82) .. controls (526.32,90.39) and (526.76,91.81) .. (526.06,92.98) -- (516.49,108.83) .. controls (515.78,110.01) and (514.19,110.49) .. (512.93,109.92) -- (512.93,109.92) .. controls (511.68,109.35) and (511.23,107.94) .. (511.94,106.77) -- (521.51,90.91) .. controls (522.22,89.74) and (523.81,89.25) .. (525.07,89.82) -- cycle ;
		%Rounded Rect [id:dp09639586773218367] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (531.25,89.82) .. controls (529.99,90.4) and (529.55,91.81) .. (530.26,92.98) -- (539.83,108.83) .. controls (540.54,110.01) and (542.13,110.49) .. (543.38,109.92) -- (543.38,109.92) .. controls (544.64,109.35) and (545.08,107.94) .. (544.38,106.77) -- (534.81,90.92) .. controls (534.1,89.74) and (532.51,89.25) .. (531.25,89.82) -- cycle ;
		%Shape: Ellipse [id:dp9101187943596678] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (521.79,81.08) .. controls (521.79,77.95) and (524.72,75.4) .. (528.34,75.4) .. controls (531.96,75.4) and (534.89,77.95) .. (534.89,81.08) .. controls (534.89,84.22) and (531.96,86.76) .. (528.34,86.76) .. controls (524.72,86.76) and (521.79,84.22) .. (521.79,81.08) -- cycle ;
		
		%Rounded Rect [id:dp7122565471203888] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (554.83,113.63) .. controls (555.4,113.64) and (555.85,114.1) .. (555.84,114.67) -- (555.5,135.3) .. controls (555.49,135.87) and (555.03,136.32) .. (554.46,136.32) -- (551.38,136.28) .. controls (550.81,136.27) and (550.36,135.81) .. (550.37,135.24) -- (550.71,114.61) .. controls (550.72,114.04) and (551.19,113.59) .. (551.75,113.59) -- cycle ;
		%Rounded Rect [id:dp5978279233114465] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (561.37,113.7) .. controls (561.94,113.7) and (562.39,114.17) .. (562.38,114.73) -- (562.04,135.37) .. controls (562.03,135.93) and (561.57,136.39) .. (561,136.38) -- (557.92,136.34) .. controls (557.35,136.33) and (556.9,135.87) .. (556.91,135.3) -- (557.25,114.67) .. controls (557.26,114.1) and (557.73,113.65) .. (558.29,113.66) -- cycle ;
		%Flowchart: Stored Data [id:dp7570178427682603] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (562.57,93.28) -- (562.36,119.58) .. controls (562.38,116.81) and (559.78,114.55) .. (556.55,114.53) .. controls (553.32,114.52) and (550.68,116.74) .. (550.66,119.51) -- (550.86,93.21) .. controls (550.89,90.44) and (553.52,88.21) .. (556.76,88.23) .. controls (559.99,88.25) and (562.59,90.51) .. (562.57,93.28) -- cycle ;
		%Rounded Rect [id:dp4198354723536244] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (553.62,89.82) .. controls (554.88,90.39) and (555.32,91.81) .. (554.62,92.98) -- (545.04,108.83) .. controls (544.34,110.01) and (542.75,110.49) .. (541.49,109.92) -- (541.49,109.92) .. controls (540.24,109.35) and (539.79,107.94) .. (540.5,106.77) -- (550.07,90.91) .. controls (550.78,89.74) and (552.37,89.25) .. (553.62,89.82) -- cycle ;
		%Rounded Rect [id:dp5993690731732542] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (559.81,89.82) .. controls (558.55,90.4) and (558.11,91.81) .. (558.82,92.98) -- (568.38,108.83) .. controls (569.09,110.01) and (570.69,110.49) .. (571.94,109.92) -- (571.94,109.92) .. controls (573.2,109.35) and (573.64,107.94) .. (572.93,106.77) -- (563.36,90.92) .. controls (562.66,89.74) and (561.06,89.25) .. (559.81,89.82) -- cycle ;
		%Shape: Ellipse [id:dp7089819048810118] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (550.35,81.08) .. controls (550.35,77.95) and (553.28,75.4) .. (556.9,75.4) .. controls (560.51,75.4) and (563.45,77.95) .. (563.45,81.08) .. controls (563.45,84.22) and (560.51,86.76) .. (556.9,86.76) .. controls (553.28,86.76) and (550.35,84.22) .. (550.35,81.08) -- cycle ;
		
		%Rounded Rect [id:dp6949312419660958] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (583.39,113.63) .. controls (583.96,113.64) and (584.41,114.1) .. (584.4,114.67) -- (584.06,135.3) .. controls (584.05,135.87) and (583.59,136.32) .. (583.02,136.32) -- (579.94,136.28) .. controls (579.37,136.27) and (578.92,135.81) .. (578.93,135.24) -- (579.27,114.61) .. controls (579.28,114.04) and (579.74,113.59) .. (580.31,113.59) -- cycle ;
		%Rounded Rect [id:dp34201331258214984] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (589.93,113.7) .. controls (590.5,113.7) and (590.95,114.17) .. (590.94,114.73) -- (590.6,135.37) .. controls (590.59,135.93) and (590.13,136.39) .. (589.56,136.38) -- (586.48,136.34) .. controls (585.91,136.33) and (585.46,135.87) .. (585.47,135.3) -- (585.81,114.67) .. controls (585.82,114.1) and (586.28,113.65) .. (586.85,113.66) -- cycle ;
		%Flowchart: Stored Data [id:dp3770920666288802] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (591.13,93.28) -- (590.92,119.58) .. controls (590.94,116.81) and (588.34,114.55) .. (585.11,114.53) .. controls (581.87,114.52) and (579.24,116.74) .. (579.22,119.51) -- (579.42,93.21) .. controls (579.44,90.44) and (582.08,88.21) .. (585.31,88.23) .. controls (588.55,88.25) and (591.15,90.51) .. (591.13,93.28) -- cycle ;
		%Rounded Rect [id:dp39703384909658035] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (582.18,89.82) .. controls (583.44,90.39) and (583.88,91.81) .. (583.17,92.98) -- (573.6,108.83) .. controls (572.89,110.01) and (571.3,110.49) .. (570.05,109.92) -- (570.05,109.92) .. controls (568.79,109.35) and (568.35,107.94) .. (569.06,106.77) -- (578.63,90.91) .. controls (579.34,89.74) and (580.93,89.25) .. (582.18,89.82) -- cycle ;
		%Rounded Rect [id:dp6006619468860215] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (588.37,89.82) .. controls (587.11,90.4) and (586.67,91.81) .. (587.37,92.98) -- (596.94,108.83) .. controls (597.65,110.01) and (599.24,110.49) .. (600.5,109.92) -- (600.5,109.92) .. controls (601.76,109.35) and (602.2,107.94) .. (601.49,106.77) -- (591.92,90.92) .. controls (591.21,89.74) and (589.62,89.25) .. (588.37,89.82) -- cycle ;
		%Shape: Ellipse [id:dp33069558850718894] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (578.9,81.08) .. controls (578.9,77.95) and (581.84,75.4) .. (585.46,75.4) .. controls (589.07,75.4) and (592.01,77.95) .. (592.01,81.08) .. controls (592.01,84.22) and (589.07,86.76) .. (585.46,86.76) .. controls (581.84,86.76) and (578.9,84.22) .. (578.9,81.08) -- cycle ;
		
		%Rounded Rect [id:dp6889638202323063] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (611.93,113.63) .. controls (612.49,113.64) and (612.95,114.1) .. (612.94,114.67) -- (612.6,135.3) .. controls (612.59,135.87) and (612.12,136.32) .. (611.55,136.32) -- (608.47,136.28) .. controls (607.91,136.27) and (607.45,135.81) .. (607.46,135.24) -- (607.8,114.61) .. controls (607.81,114.04) and (608.28,113.59) .. (608.85,113.59) -- cycle ;
		%Rounded Rect [id:dp37800170707039316] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (618.47,113.7) .. controls (619.03,113.7) and (619.49,114.17) .. (619.48,114.73) -- (619.14,135.37) .. controls (619.13,135.93) and (618.66,136.39) .. (618.09,136.38) -- (615.01,136.34) .. controls (614.45,136.33) and (614,135.87) .. (614,135.3) -- (614.34,114.67) .. controls (614.35,114.1) and (614.82,113.65) .. (615.39,113.66) -- cycle ;
		%Flowchart: Stored Data [id:dp721067477041047] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (619.66,93.28) -- (619.46,119.58) .. controls (619.48,116.81) and (616.87,114.55) .. (613.64,114.53) .. controls (610.41,114.52) and (607.77,116.74) .. (607.75,119.51) -- (607.96,93.21) .. controls (607.98,90.44) and (610.62,88.21) .. (613.85,88.23) .. controls (617.08,88.25) and (619.68,90.51) .. (619.66,93.28) -- cycle ;
		%Rounded Rect [id:dp6247973775635676] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (610.72,89.82) .. controls (611.97,90.39) and (612.42,91.81) .. (611.71,92.98) -- (602.14,108.83) .. controls (601.43,110.01) and (599.84,110.49) .. (598.58,109.92) -- (598.58,109.92) .. controls (597.33,109.35) and (596.89,107.94) .. (597.59,106.77) -- (607.17,90.91) .. controls (607.87,89.74) and (609.46,89.25) .. (610.72,89.82) -- cycle ;
		%Rounded Rect [id:dp05577446945693221] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (616.9,89.82) .. controls (615.65,90.4) and (615.2,91.81) .. (615.91,92.98) -- (625.48,108.83) .. controls (626.19,110.01) and (627.78,110.49) .. (629.04,109.92) -- (629.04,109.92) .. controls (630.29,109.35) and (630.74,107.94) .. (630.03,106.77) -- (620.46,90.92) .. controls (619.75,89.74) and (618.16,89.25) .. (616.9,89.82) -- cycle ;
		%Shape: Ellipse [id:dp1099596682827062] 
		\draw  [draw opacity=0][fill={rgb, 255:red, 214; green, 214; blue, 214 }  ,fill opacity=1 ] (607.44,81.08) .. controls (607.44,77.95) and (610.37,75.4) .. (613.99,75.4) .. controls (617.61,75.4) and (620.54,77.95) .. (620.54,81.08) .. controls (620.54,84.22) and (617.61,86.76) .. (613.99,86.76) .. controls (610.37,86.76) and (607.44,84.22) .. (607.44,81.08) -- cycle ;
		
		%Straight Lines [id:da9356531088904974] 
		\draw    (485,139) -- (412.15,188.87) ;
		\draw [shift={(410.5,190)}, rotate = 325.61] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da9990603952095034] 
		\draw    (485,139) -- (557.86,189.86) ;
		\draw [shift={(559.5,191)}, rotate = 214.91] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		
		% Text Node
		\draw (69,250) node [anchor=north west][inner sep=0.75pt]   [align=left] {Don't work out};
		% Text Node
		\draw (88.9,89) node [anchor=north west][inner sep=0.75pt]   [align=left] {Work out};
		% Text Node
		\draw (254,119) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{40.53pt}\setlength\topsep{0pt}
		\begin{center}
		Average\\energy\\level
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (256,282) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{40.53pt}\setlength\topsep{0pt}
		\begin{center}
		Average\\energy\\level
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (118,39) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{Observational Study}};
		% Text Node
		\draw (393,39) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{Experimental Study}};
		% Text Node
		\draw (380.9,277) node [anchor=north west][inner sep=0.75pt]   [align=left] {Work out};
		% Text Node
		\draw (509.9,277) node [anchor=north west][inner sep=0.75pt]   [align=left] {Don't Work out};
		% Text Node
		\draw (369,352.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{67.73pt}\setlength\topsep{0pt}
		\begin{center}
		Average\\energy level
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (522,352.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{67.73pt}\setlength\topsep{0pt}
		\begin{center}
		Average\\energy level
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (419,154) node [anchor=north west][inner sep=0.75pt]   [align=left] {Random assignment};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Experimental vs Observational studies}
	\end{figure}
	Based on the observational study, even if we find a difference between the energy levels of these two groups of people, we really can't attribute this difference solely to working out because there may be other variables that we didn't control for in this study that contribute to the observed difference. For example, people who are in better shape might be more likely to regularly work out and also have higher energy levels. However, in the experiment such variables that might also contribute to the outcome are likely equally represented in the two groups due to random assignment. Therefore, if we find a difference between the two averages, we can indeed make (likely) a causal statement attributing this difference to working out.
	
	In practice, starting from the following experimental setup (one experimental unit, 24 observational units):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1467); %set diagram left start at 0, and has height of 1467
		
		%Shape: Rectangle [id:dp3323956907711456] 
		\draw  [fill={rgb, 255:red, 255; green, 247; blue, 217 }  ,fill opacity=1 ] (228.5,31) -- (407.5,31) -- (407.5,309) -- (228.5,309) -- cycle ;
		%Shape: Circle [id:dp7082271644208993] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (237,57) .. controls (237,47.61) and (244.61,40) .. (254,40) .. controls (263.39,40) and (271,47.61) .. (271,57) .. controls (271,66.39) and (263.39,74) .. (254,74) .. controls (244.61,74) and (237,66.39) .. (237,57) -- cycle ;
		%Shape: Circle [id:dp7909932311172885] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (279.67,57) .. controls (279.67,47.61) and (287.28,40) .. (296.67,40) .. controls (306.06,40) and (313.67,47.61) .. (313.67,57) .. controls (313.67,66.39) and (306.06,74) .. (296.67,74) .. controls (287.28,74) and (279.67,66.39) .. (279.67,57) -- cycle ;
		%Shape: Circle [id:dp3770787863630556] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (322.34,57) .. controls (322.34,47.61) and (329.95,40) .. (339.34,40) .. controls (348.73,40) and (356.34,47.61) .. (356.34,57) .. controls (356.34,66.39) and (348.73,74) .. (339.34,74) .. controls (329.95,74) and (322.34,66.39) .. (322.34,57) -- cycle ;
		%Shape: Circle [id:dp6952057010783017] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (365,57) .. controls (365,47.61) and (372.61,40) .. (382,40) .. controls (391.39,40) and (399,47.61) .. (399,57) .. controls (399,66.39) and (391.39,74) .. (382,74) .. controls (372.61,74) and (365,66.39) .. (365,57) -- cycle ;
		
		%Shape: Circle [id:dp6977713949533912] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (237,101) .. controls (237,91.61) and (244.61,84) .. (254,84) .. controls (263.39,84) and (271,91.61) .. (271,101) .. controls (271,110.39) and (263.39,118) .. (254,118) .. controls (244.61,118) and (237,110.39) .. (237,101) -- cycle ;
		%Shape: Circle [id:dp6113794155768664] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (279.67,101) .. controls (279.67,91.61) and (287.28,84) .. (296.67,84) .. controls (306.06,84) and (313.67,91.61) .. (313.67,101) .. controls (313.67,110.39) and (306.06,118) .. (296.67,118) .. controls (287.28,118) and (279.67,110.39) .. (279.67,101) -- cycle ;
		%Shape: Circle [id:dp7625844390619605] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (322.34,101) .. controls (322.34,91.61) and (329.95,84) .. (339.34,84) .. controls (348.73,84) and (356.34,91.61) .. (356.34,101) .. controls (356.34,110.39) and (348.73,118) .. (339.34,118) .. controls (329.95,118) and (322.34,110.39) .. (322.34,101) -- cycle ;
		%Shape: Circle [id:dp5472214015562362] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (365,101) .. controls (365,91.61) and (372.61,84) .. (382,84) .. controls (391.39,84) and (399,91.61) .. (399,101) .. controls (399,110.39) and (391.39,118) .. (382,118) .. controls (372.61,118) and (365,110.39) .. (365,101) -- cycle ;
		
		%Shape: Circle [id:dp586983738411216] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (237,145) .. controls (237,135.61) and (244.61,128) .. (254,128) .. controls (263.39,128) and (271,135.61) .. (271,145) .. controls (271,154.39) and (263.39,162) .. (254,162) .. controls (244.61,162) and (237,154.39) .. (237,145) -- cycle ;
		%Shape: Circle [id:dp22305640430700002] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (279.67,145) .. controls (279.67,135.61) and (287.28,128) .. (296.67,128) .. controls (306.06,128) and (313.67,135.61) .. (313.67,145) .. controls (313.67,154.39) and (306.06,162) .. (296.67,162) .. controls (287.28,162) and (279.67,154.39) .. (279.67,145) -- cycle ;
		%Shape: Circle [id:dp5665023617197158] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (322.34,145) .. controls (322.34,135.61) and (329.95,128) .. (339.34,128) .. controls (348.73,128) and (356.34,135.61) .. (356.34,145) .. controls (356.34,154.39) and (348.73,162) .. (339.34,162) .. controls (329.95,162) and (322.34,154.39) .. (322.34,145) -- cycle ;
		%Shape: Circle [id:dp8580232205482174] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (365,145) .. controls (365,135.61) and (372.61,128) .. (382,128) .. controls (391.39,128) and (399,135.61) .. (399,145) .. controls (399,154.39) and (391.39,162) .. (382,162) .. controls (372.61,162) and (365,154.39) .. (365,145) -- cycle ;
		
		%Shape: Circle [id:dp8435754810066951] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (237,189) .. controls (237,179.61) and (244.61,172) .. (254,172) .. controls (263.39,172) and (271,179.61) .. (271,189) .. controls (271,198.39) and (263.39,206) .. (254,206) .. controls (244.61,206) and (237,198.39) .. (237,189) -- cycle ;
		%Shape: Circle [id:dp763953657236085] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (279.67,189) .. controls (279.67,179.61) and (287.28,172) .. (296.67,172) .. controls (306.06,172) and (313.67,179.61) .. (313.67,189) .. controls (313.67,198.39) and (306.06,206) .. (296.67,206) .. controls (287.28,206) and (279.67,198.39) .. (279.67,189) -- cycle ;
		%Shape: Circle [id:dp7433664399062454] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (322.34,189) .. controls (322.34,179.61) and (329.95,172) .. (339.34,172) .. controls (348.73,172) and (356.34,179.61) .. (356.34,189) .. controls (356.34,198.39) and (348.73,206) .. (339.34,206) .. controls (329.95,206) and (322.34,198.39) .. (322.34,189) -- cycle ;
		%Shape: Circle [id:dp909942696418591] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (365,189) .. controls (365,179.61) and (372.61,172) .. (382,172) .. controls (391.39,172) and (399,179.61) .. (399,189) .. controls (399,198.39) and (391.39,206) .. (382,206) .. controls (372.61,206) and (365,198.39) .. (365,189) -- cycle ;
		
		%Shape: Circle [id:dp5961465305296836] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (237,233) .. controls (237,223.61) and (244.61,216) .. (254,216) .. controls (263.39,216) and (271,223.61) .. (271,233) .. controls (271,242.39) and (263.39,250) .. (254,250) .. controls (244.61,250) and (237,242.39) .. (237,233) -- cycle ;
		%Shape: Circle [id:dp900038857787361] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (279.67,233) .. controls (279.67,223.61) and (287.28,216) .. (296.67,216) .. controls (306.06,216) and (313.67,223.61) .. (313.67,233) .. controls (313.67,242.39) and (306.06,250) .. (296.67,250) .. controls (287.28,250) and (279.67,242.39) .. (279.67,233) -- cycle ;
		%Shape: Circle [id:dp13540188798731867] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (322.34,233) .. controls (322.34,223.61) and (329.95,216) .. (339.34,216) .. controls (348.73,216) and (356.34,223.61) .. (356.34,233) .. controls (356.34,242.39) and (348.73,250) .. (339.34,250) .. controls (329.95,250) and (322.34,242.39) .. (322.34,233) -- cycle ;
		%Shape: Circle [id:dp20955585753166428] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (365,233) .. controls (365,223.61) and (372.61,216) .. (382,216) .. controls (391.39,216) and (399,223.61) .. (399,233) .. controls (399,242.39) and (391.39,250) .. (382,250) .. controls (372.61,250) and (365,242.39) .. (365,233) -- cycle ;
		
		%Shape: Circle [id:dp16031970656722505] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (237,277) .. controls (237,267.61) and (244.61,260) .. (254,260) .. controls (263.39,260) and (271,267.61) .. (271,277) .. controls (271,286.39) and (263.39,294) .. (254,294) .. controls (244.61,294) and (237,286.39) .. (237,277) -- cycle ;
		%Shape: Circle [id:dp49683597589722384] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (279.67,277) .. controls (279.67,267.61) and (287.28,260) .. (296.67,260) .. controls (306.06,260) and (313.67,267.61) .. (313.67,277) .. controls (313.67,286.39) and (306.06,294) .. (296.67,294) .. controls (287.28,294) and (279.67,286.39) .. (279.67,277) -- cycle ;
		%Shape: Circle [id:dp2799828134426221] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (322.34,277) .. controls (322.34,267.61) and (329.95,260) .. (339.34,260) .. controls (348.73,260) and (356.34,267.61) .. (356.34,277) .. controls (356.34,286.39) and (348.73,294) .. (339.34,294) .. controls (329.95,294) and (322.34,286.39) .. (322.34,277) -- cycle ;
		%Shape: Circle [id:dp8179384367397646] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (365,277) .. controls (365,267.61) and (372.61,260) .. (382,260) .. controls (391.39,260) and (399,267.61) .. (399,277) .. controls (399,286.39) and (391.39,294) .. (382,294) .. controls (372.61,294) and (365,286.39) .. (365,277) -- cycle ;
		\end{tikzpicture}
	\end{figure}
	that we number as following:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1467); %set diagram left start at 0, and has height of 1467
		
		%Shape: Rectangle [id:dp3323956907711456] 
		\draw  [fill={rgb, 255:red, 255; green, 247; blue, 217 }  ,fill opacity=1 ] (228.5,31) -- (407.5,31) -- (407.5,309) -- (228.5,309) -- cycle ;
		%Shape: Circle [id:dp7082271644208993] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (237,57) .. controls (237,47.61) and (244.61,40) .. (254,40) .. controls (263.39,40) and (271,47.61) .. (271,57) .. controls (271,66.39) and (263.39,74) .. (254,74) .. controls (244.61,74) and (237,66.39) .. (237,57) -- cycle ;
		%Shape: Circle [id:dp7909932311172885] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (279.67,57) .. controls (279.67,47.61) and (287.28,40) .. (296.67,40) .. controls (306.06,40) and (313.67,47.61) .. (313.67,57) .. controls (313.67,66.39) and (306.06,74) .. (296.67,74) .. controls (287.28,74) and (279.67,66.39) .. (279.67,57) -- cycle ;
		%Shape: Circle [id:dp3770787863630556] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (322.34,57) .. controls (322.34,47.61) and (329.95,40) .. (339.34,40) .. controls (348.73,40) and (356.34,47.61) .. (356.34,57) .. controls (356.34,66.39) and (348.73,74) .. (339.34,74) .. controls (329.95,74) and (322.34,66.39) .. (322.34,57) -- cycle ;
		%Shape: Circle [id:dp6952057010783017] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (365,57) .. controls (365,47.61) and (372.61,40) .. (382,40) .. controls (391.39,40) and (399,47.61) .. (399,57) .. controls (399,66.39) and (391.39,74) .. (382,74) .. controls (372.61,74) and (365,66.39) .. (365,57) -- cycle ;
		
		%Shape: Circle [id:dp6977713949533912] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (237,101) .. controls (237,91.61) and (244.61,84) .. (254,84) .. controls (263.39,84) and (271,91.61) .. (271,101) .. controls (271,110.39) and (263.39,118) .. (254,118) .. controls (244.61,118) and (237,110.39) .. (237,101) -- cycle ;
		%Shape: Circle [id:dp6113794155768664] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (279.67,101) .. controls (279.67,91.61) and (287.28,84) .. (296.67,84) .. controls (306.06,84) and (313.67,91.61) .. (313.67,101) .. controls (313.67,110.39) and (306.06,118) .. (296.67,118) .. controls (287.28,118) and (279.67,110.39) .. (279.67,101) -- cycle ;
		%Shape: Circle [id:dp7625844390619605] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (322.34,101) .. controls (322.34,91.61) and (329.95,84) .. (339.34,84) .. controls (348.73,84) and (356.34,91.61) .. (356.34,101) .. controls (356.34,110.39) and (348.73,118) .. (339.34,118) .. controls (329.95,118) and (322.34,110.39) .. (322.34,101) -- cycle ;
		%Shape: Circle [id:dp5472214015562362] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (365,101) .. controls (365,91.61) and (372.61,84) .. (382,84) .. controls (391.39,84) and (399,91.61) .. (399,101) .. controls (399,110.39) and (391.39,118) .. (382,118) .. controls (372.61,118) and (365,110.39) .. (365,101) -- cycle ;
		
		%Shape: Circle [id:dp586983738411216] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (237,145) .. controls (237,135.61) and (244.61,128) .. (254,128) .. controls (263.39,128) and (271,135.61) .. (271,145) .. controls (271,154.39) and (263.39,162) .. (254,162) .. controls (244.61,162) and (237,154.39) .. (237,145) -- cycle ;
		%Shape: Circle [id:dp22305640430700002] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (279.67,145) .. controls (279.67,135.61) and (287.28,128) .. (296.67,128) .. controls (306.06,128) and (313.67,135.61) .. (313.67,145) .. controls (313.67,154.39) and (306.06,162) .. (296.67,162) .. controls (287.28,162) and (279.67,154.39) .. (279.67,145) -- cycle ;
		%Shape: Circle [id:dp5665023617197158] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (322.34,145) .. controls (322.34,135.61) and (329.95,128) .. (339.34,128) .. controls (348.73,128) and (356.34,135.61) .. (356.34,145) .. controls (356.34,154.39) and (348.73,162) .. (339.34,162) .. controls (329.95,162) and (322.34,154.39) .. (322.34,145) -- cycle ;
		%Shape: Circle [id:dp8580232205482174] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (365,145) .. controls (365,135.61) and (372.61,128) .. (382,128) .. controls (391.39,128) and (399,135.61) .. (399,145) .. controls (399,154.39) and (391.39,162) .. (382,162) .. controls (372.61,162) and (365,154.39) .. (365,145) -- cycle ;
		
		%Shape: Circle [id:dp8435754810066951] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (237,189) .. controls (237,179.61) and (244.61,172) .. (254,172) .. controls (263.39,172) and (271,179.61) .. (271,189) .. controls (271,198.39) and (263.39,206) .. (254,206) .. controls (244.61,206) and (237,198.39) .. (237,189) -- cycle ;
		%Shape: Circle [id:dp763953657236085] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (279.67,189) .. controls (279.67,179.61) and (287.28,172) .. (296.67,172) .. controls (306.06,172) and (313.67,179.61) .. (313.67,189) .. controls (313.67,198.39) and (306.06,206) .. (296.67,206) .. controls (287.28,206) and (279.67,198.39) .. (279.67,189) -- cycle ;
		%Shape: Circle [id:dp7433664399062454] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (322.34,189) .. controls (322.34,179.61) and (329.95,172) .. (339.34,172) .. controls (348.73,172) and (356.34,179.61) .. (356.34,189) .. controls (356.34,198.39) and (348.73,206) .. (339.34,206) .. controls (329.95,206) and (322.34,198.39) .. (322.34,189) -- cycle ;
		%Shape: Circle [id:dp909942696418591] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (365,189) .. controls (365,179.61) and (372.61,172) .. (382,172) .. controls (391.39,172) and (399,179.61) .. (399,189) .. controls (399,198.39) and (391.39,206) .. (382,206) .. controls (372.61,206) and (365,198.39) .. (365,189) -- cycle ;
		
		%Shape: Circle [id:dp5961465305296836] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (237,233) .. controls (237,223.61) and (244.61,216) .. (254,216) .. controls (263.39,216) and (271,223.61) .. (271,233) .. controls (271,242.39) and (263.39,250) .. (254,250) .. controls (244.61,250) and (237,242.39) .. (237,233) -- cycle ;
		%Shape: Circle [id:dp900038857787361] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (279.67,233) .. controls (279.67,223.61) and (287.28,216) .. (296.67,216) .. controls (306.06,216) and (313.67,223.61) .. (313.67,233) .. controls (313.67,242.39) and (306.06,250) .. (296.67,250) .. controls (287.28,250) and (279.67,242.39) .. (279.67,233) -- cycle ;
		%Shape: Circle [id:dp13540188798731867] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (322.34,233) .. controls (322.34,223.61) and (329.95,216) .. (339.34,216) .. controls (348.73,216) and (356.34,223.61) .. (356.34,233) .. controls (356.34,242.39) and (348.73,250) .. (339.34,250) .. controls (329.95,250) and (322.34,242.39) .. (322.34,233) -- cycle ;
		%Shape: Circle [id:dp20955585753166428] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (365,233) .. controls (365,223.61) and (372.61,216) .. (382,216) .. controls (391.39,216) and (399,223.61) .. (399,233) .. controls (399,242.39) and (391.39,250) .. (382,250) .. controls (372.61,250) and (365,242.39) .. (365,233) -- cycle ;
		
		%Shape: Circle [id:dp16031970656722505] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (237,277) .. controls (237,267.61) and (244.61,260) .. (254,260) .. controls (263.39,260) and (271,267.61) .. (271,277) .. controls (271,286.39) and (263.39,294) .. (254,294) .. controls (244.61,294) and (237,286.39) .. (237,277) -- cycle ;
		%Shape: Circle [id:dp49683597589722384] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (279.67,277) .. controls (279.67,267.61) and (287.28,260) .. (296.67,260) .. controls (306.06,260) and (313.67,267.61) .. (313.67,277) .. controls (313.67,286.39) and (306.06,294) .. (296.67,294) .. controls (287.28,294) and (279.67,286.39) .. (279.67,277) -- cycle ;
		%Shape: Circle [id:dp2799828134426221] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (322.34,277) .. controls (322.34,267.61) and (329.95,260) .. (339.34,260) .. controls (348.73,260) and (356.34,267.61) .. (356.34,277) .. controls (356.34,286.39) and (348.73,294) .. (339.34,294) .. controls (329.95,294) and (322.34,286.39) .. (322.34,277) -- cycle ;
		%Shape: Circle [id:dp8179384367397646] 
		\draw  [fill={rgb, 255:red, 243; green, 214; blue, 252 }  ,fill opacity=1 ] (365,277) .. controls (365,267.61) and (372.61,260) .. (382,260) .. controls (391.39,260) and (399,267.61) .. (399,277) .. controls (399,286.39) and (391.39,294) .. (382,294) .. controls (372.61,294) and (365,286.39) .. (365,277) -- cycle ;
		
		
		% Text Node
		\draw (376.72,49.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (334.08,49.9) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (292.43,49.9) node [anchor=north west][inner sep=0.75pt]    {$3$};
		% Text Node
		\draw (247.78,49.9) node [anchor=north west][inner sep=0.75pt]    {$4$};
		% Text Node
		\draw (376.72,93.9) node [anchor=north west][inner sep=0.75pt]    {$5$};
		% Text Node
		\draw (334.08,93.9) node [anchor=north west][inner sep=0.75pt]    {$6$};
		% Text Node
		\draw (292.43,93.9) node [anchor=north west][inner sep=0.75pt]    {$7$};
		% Text Node
		\draw (247.78,93.9) node [anchor=north west][inner sep=0.75pt]    {$8$};
		% Text Node
		\draw (376.72,137.9) node [anchor=north west][inner sep=0.75pt]    {$9$};
		% Text Node
		\draw (329.08,137.9) node [anchor=north west][inner sep=0.75pt]    {$10$};
		% Text Node
		\draw (287.43,137.9) node [anchor=north west][inner sep=0.75pt]    {$11$};
		% Text Node
		\draw (243.78,137.9) node [anchor=north west][inner sep=0.75pt]    {$12$};
		% Text Node
		\draw (372.72,181.9) node [anchor=north west][inner sep=0.75pt]    {$13$};
		% Text Node
		\draw (330.08,181.9) node [anchor=north west][inner sep=0.75pt]    {$14$};
		% Text Node
		\draw (287.43,181.9) node [anchor=north west][inner sep=0.75pt]    {$15$};
		% Text Node
		\draw (243.78,181.9) node [anchor=north west][inner sep=0.75pt]    {$16$};
		% Text Node
		\draw (372.72,225.9) node [anchor=north west][inner sep=0.75pt]    {$17$};
		% Text Node
		\draw (330.08,225.9) node [anchor=north west][inner sep=0.75pt]    {$18$};
		% Text Node
		\draw (287.43,225.9) node [anchor=north west][inner sep=0.75pt]    {$19$};
		% Text Node
		\draw (243.78,225.9) node [anchor=north west][inner sep=0.75pt]    {$20$};
		% Text Node
		\draw (372.72,269.9) node [anchor=north west][inner sep=0.75pt]    {$21$};
		% Text Node
		\draw (330.08,269.9) node [anchor=north west][inner sep=0.75pt]    {$22$};
		% Text Node
		\draw (287.43,269.9) node [anchor=north west][inner sep=0.75pt]    {$23$};
		% Text Node
		\draw (243.78,269.9) node [anchor=north west][inner sep=0.75pt]    {$24$};
		
		\end{tikzpicture}
	\end{figure}
	we differentiate the following configurations\footnote{Without taking into account that for each, there is a version for balanced and unbalanced designs and also a version for missing values} from the more common to the most rare or advanced type of design (also named "random control trials" (RCT) or "random control designs" (RCD)):
	\begin{itemize}
		\item "\NewTerm{Complete randomized design}\index{ANOVA!complete randomized design}" (CRD):
		
		One-Way ANOVA (but not only) are typically associated to CRD as the idea behind is to make measurement on experimental units that are randomized for each level of the factor, to avoid a possible bias (confounding). A typical corresponding configuration will be:
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
			%uncomment if require: \path (0,1467); %set diagram left start at 0, and has height of 1467
			
			%Shape: Rectangle [id:dp3323956907711456] 
			\draw  [fill={rgb, 255:red, 255; green, 247; blue, 217 }  ,fill opacity=1 ] (289.5,35) -- (468.5,35) -- (468.5,313) -- (289.5,313) -- cycle ;
			%Shape: Circle [id:dp8956193049184411] 
			\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (298,61) .. controls (298,51.61) and (305.61,44) .. (315,44) .. controls (324.39,44) and (332,51.61) .. (332,61) .. controls (332,70.39) and (324.39,78) .. (315,78) .. controls (305.61,78) and (298,70.39) .. (298,61) -- cycle ;
			%Shape: Circle [id:dp35263048255777596] 
			\draw  [fill={rgb, 255:red, 255; green, 146; blue, 158 }  ,fill opacity=1 ] (340.67,61) .. controls (340.67,51.61) and (348.28,44) .. (357.67,44) .. controls (367.06,44) and (374.67,51.61) .. (374.67,61) .. controls (374.67,70.39) and (367.06,78) .. (357.67,78) .. controls (348.28,78) and (340.67,70.39) .. (340.67,61) -- cycle ;
			%Shape: Circle [id:dp4302119990474258] 
			\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (383.34,61) .. controls (383.34,51.61) and (390.95,44) .. (400.34,44) .. controls (409.73,44) and (417.34,51.61) .. (417.34,61) .. controls (417.34,70.39) and (409.73,78) .. (400.34,78) .. controls (390.95,78) and (383.34,70.39) .. (383.34,61) -- cycle ;
			%Shape: Circle [id:dp5336355850957886] 
			\draw  [fill={rgb, 255:red, 255; green, 146; blue, 158 }  ,fill opacity=1 ] (426,61) .. controls (426,51.61) and (433.61,44) .. (443,44) .. controls (452.39,44) and (460,51.61) .. (460,61) .. controls (460,70.39) and (452.39,78) .. (443,78) .. controls (433.61,78) and (426,70.39) .. (426,61) -- cycle ;
			%Shape: Circle [id:dp11817830977182942] 
			\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (298,105) .. controls (298,95.61) and (305.61,88) .. (315,88) .. controls (324.39,88) and (332,95.61) .. (332,105) .. controls (332,114.39) and (324.39,122) .. (315,122) .. controls (305.61,122) and (298,114.39) .. (298,105) -- cycle ;
			%Shape: Circle [id:dp9337555336603114] 
			\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (340.67,105) .. controls (340.67,95.61) and (348.28,88) .. (357.67,88) .. controls (367.06,88) and (374.67,95.61) .. (374.67,105) .. controls (374.67,114.39) and (367.06,122) .. (357.67,122) .. controls (348.28,122) and (340.67,114.39) .. (340.67,105) -- cycle ;
			%Shape: Circle [id:dp42727978348597784] 
			\draw  [fill={rgb, 255:red, 255; green, 146; blue, 158 }  ,fill opacity=1 ] (383.34,105) .. controls (383.34,95.61) and (390.95,88) .. (400.34,88) .. controls (409.73,88) and (417.34,95.61) .. (417.34,105) .. controls (417.34,114.39) and (409.73,122) .. (400.34,122) .. controls (390.95,122) and (383.34,114.39) .. (383.34,105) -- cycle ;
			%Shape: Circle [id:dp005098165549407785] 
			\draw  [fill={rgb, 255:red, 255; green, 146; blue, 158 }  ,fill opacity=1 ] (426,105) .. controls (426,95.61) and (433.61,88) .. (443,88) .. controls (452.39,88) and (460,95.61) .. (460,105) .. controls (460,114.39) and (452.39,122) .. (443,122) .. controls (433.61,122) and (426,114.39) .. (426,105) -- cycle ;
			%Shape: Circle [id:dp31607205356315293] 
			\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (298,149) .. controls (298,139.61) and (305.61,132) .. (315,132) .. controls (324.39,132) and (332,139.61) .. (332,149) .. controls (332,158.39) and (324.39,166) .. (315,166) .. controls (305.61,166) and (298,158.39) .. (298,149) -- cycle ;
			%Shape: Circle [id:dp4833111989186021] 
			\draw  [fill={rgb, 255:red, 189; green, 218; blue, 255 }  ,fill opacity=1 ] (340.67,149) .. controls (340.67,139.61) and (348.28,132) .. (357.67,132) .. controls (367.06,132) and (374.67,139.61) .. (374.67,149) .. controls (374.67,158.39) and (367.06,166) .. (357.67,166) .. controls (348.28,166) and (340.67,158.39) .. (340.67,149) -- cycle ;
			%Shape: Circle [id:dp8103569616283224] 
			\draw  [fill={rgb, 255:red, 189; green, 218; blue, 255 }  ,fill opacity=1 ] (383.34,149) .. controls (383.34,139.61) and (390.95,132) .. (400.34,132) .. controls (409.73,132) and (417.34,139.61) .. (417.34,149) .. controls (417.34,158.39) and (409.73,166) .. (400.34,166) .. controls (390.95,166) and (383.34,158.39) .. (383.34,149) -- cycle ;
			%Shape: Circle [id:dp9106783112141077] 
			\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (426,149) .. controls (426,139.61) and (433.61,132) .. (443,132) .. controls (452.39,132) and (460,139.61) .. (460,149) .. controls (460,158.39) and (452.39,166) .. (443,166) .. controls (433.61,166) and (426,158.39) .. (426,149) -- cycle ;
			%Shape: Circle [id:dp8048113504426111] 
			\draw  [fill={rgb, 255:red, 255; green, 146; blue, 158 }  ,fill opacity=1 ] (298,193) .. controls (298,183.61) and (305.61,176) .. (315,176) .. controls (324.39,176) and (332,183.61) .. (332,193) .. controls (332,202.39) and (324.39,210) .. (315,210) .. controls (305.61,210) and (298,202.39) .. (298,193) -- cycle ;
			%Shape: Circle [id:dp2042809257939613] 
			\draw  [fill={rgb, 255:red, 189; green, 218; blue, 255 }  ,fill opacity=1 ] (340.67,193) .. controls (340.67,183.61) and (348.28,176) .. (357.67,176) .. controls (367.06,176) and (374.67,183.61) .. (374.67,193) .. controls (374.67,202.39) and (367.06,210) .. (357.67,210) .. controls (348.28,210) and (340.67,202.39) .. (340.67,193) -- cycle ;
			%Shape: Circle [id:dp03417872776536601] 
			\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (383.34,193) .. controls (383.34,183.61) and (390.95,176) .. (400.34,176) .. controls (409.73,176) and (417.34,183.61) .. (417.34,193) .. controls (417.34,202.39) and (409.73,210) .. (400.34,210) .. controls (390.95,210) and (383.34,202.39) .. (383.34,193) -- cycle ;
			%Shape: Circle [id:dp44437114019545865] 
			\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (426,193) .. controls (426,183.61) and (433.61,176) .. (443,176) .. controls (452.39,176) and (460,183.61) .. (460,193) .. controls (460,202.39) and (452.39,210) .. (443,210) .. controls (433.61,210) and (426,202.39) .. (426,193) -- cycle ;
			%Shape: Circle [id:dp9380695872309981] 
			\draw  [fill={rgb, 255:red, 189; green, 218; blue, 255 }  ,fill opacity=1 ] (298,237) .. controls (298,227.61) and (305.61,220) .. (315,220) .. controls (324.39,220) and (332,227.61) .. (332,237) .. controls (332,246.39) and (324.39,254) .. (315,254) .. controls (305.61,254) and (298,246.39) .. (298,237) -- cycle ;
			%Shape: Circle [id:dp5519960749143016] 
			\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (340.67,237) .. controls (340.67,227.61) and (348.28,220) .. (357.67,220) .. controls (367.06,220) and (374.67,227.61) .. (374.67,237) .. controls (374.67,246.39) and (367.06,254) .. (357.67,254) .. controls (348.28,254) and (340.67,246.39) .. (340.67,237) -- cycle ;
			%Shape: Circle [id:dp7278139326826107] 
			\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (383.34,237) .. controls (383.34,227.61) and (390.95,220) .. (400.34,220) .. controls (409.73,220) and (417.34,227.61) .. (417.34,237) .. controls (417.34,246.39) and (409.73,254) .. (400.34,254) .. controls (390.95,254) and (383.34,246.39) .. (383.34,237) -- cycle ;
			%Shape: Circle [id:dp332109258554919] 
			\draw  [fill={rgb, 255:red, 255; green, 146; blue, 158 }  ,fill opacity=1 ] (426,237) .. controls (426,227.61) and (433.61,220) .. (443,220) .. controls (452.39,220) and (460,227.61) .. (460,237) .. controls (460,246.39) and (452.39,254) .. (443,254) .. controls (433.61,254) and (426,246.39) .. (426,237) -- cycle ;
			%Shape: Circle [id:dp4006033681370744] 
			\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (298,281) .. controls (298,271.61) and (305.61,264) .. (315,264) .. controls (324.39,264) and (332,271.61) .. (332,281) .. controls (332,290.39) and (324.39,298) .. (315,298) .. controls (305.61,298) and (298,290.39) .. (298,281) -- cycle ;
			%Shape: Circle [id:dp8426456774715223] 
			\draw  [fill={rgb, 255:red, 189; green, 218; blue, 255 }  ,fill opacity=1 ] (340.67,281) .. controls (340.67,271.61) and (348.28,264) .. (357.67,264) .. controls (367.06,264) and (374.67,271.61) .. (374.67,281) .. controls (374.67,290.39) and (367.06,298) .. (357.67,298) .. controls (348.28,298) and (340.67,290.39) .. (340.67,281) -- cycle ;
			%Shape: Circle [id:dp37275015485378127] 
			\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (383.34,281) .. controls (383.34,271.61) and (390.95,264) .. (400.34,264) .. controls (409.73,264) and (417.34,271.61) .. (417.34,281) .. controls (417.34,290.39) and (409.73,298) .. (400.34,298) .. controls (390.95,298) and (383.34,290.39) .. (383.34,281) -- cycle ;
			%Shape: Circle [id:dp8005137761157473] 
			\draw  [fill={rgb, 255:red, 189; green, 218; blue, 255 }  ,fill opacity=1 ] (426,281) .. controls (426,271.61) and (433.61,264) .. (443,264) .. controls (452.39,264) and (460,271.61) .. (460,281) .. controls (460,290.39) and (452.39,298) .. (443,298) .. controls (433.61,298) and (426,290.39) .. (426,281) -- cycle ;
			
			% Text Node
			\draw (437.72,53.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
			% Text Node
			\draw (395.08,53.9) node [anchor=north west][inner sep=0.75pt]    {$2$};
			% Text Node
			\draw (353.43,53.9) node [anchor=north west][inner sep=0.75pt]    {$3$};
			% Text Node
			\draw (308.78,53.9) node [anchor=north west][inner sep=0.75pt]    {$4$};
			% Text Node
			\draw (437.72,97.9) node [anchor=north west][inner sep=0.75pt]    {$5$};
			% Text Node
			\draw (395.08,97.9) node [anchor=north west][inner sep=0.75pt]    {$6$};
			% Text Node
			\draw (353.43,97.9) node [anchor=north west][inner sep=0.75pt]    {$7$};
			% Text Node
			\draw (308.78,97.9) node [anchor=north west][inner sep=0.75pt]    {$8$};
			% Text Node
			\draw (437.72,141.9) node [anchor=north west][inner sep=0.75pt]    {$9$};
			% Text Node
			\draw (390.08,141.9) node [anchor=north west][inner sep=0.75pt]    {$10$};
			% Text Node
			\draw (348.43,141.9) node [anchor=north west][inner sep=0.75pt]    {$11$};
			% Text Node
			\draw (304.78,141.9) node [anchor=north west][inner sep=0.75pt]    {$12$};
			% Text Node
			\draw (433.72,185.9) node [anchor=north west][inner sep=0.75pt]    {$13$};
			% Text Node
			\draw (391.08,185.9) node [anchor=north west][inner sep=0.75pt]    {$14$};
			% Text Node
			\draw (348.43,185.9) node [anchor=north west][inner sep=0.75pt]    {$15$};
			% Text Node
			\draw (304.78,185.9) node [anchor=north west][inner sep=0.75pt]    {$16$};
			% Text Node
			\draw (433.72,229.9) node [anchor=north west][inner sep=0.75pt]    {$17$};
			% Text Node
			\draw (391.08,229.9) node [anchor=north west][inner sep=0.75pt]    {$18$};
			% Text Node
			\draw (348.43,229.9) node [anchor=north west][inner sep=0.75pt]    {$19$};
			% Text Node
			\draw (304.78,229.9) node [anchor=north west][inner sep=0.75pt]    {$20$};
			% Text Node
			\draw (433.72,273.9) node [anchor=north west][inner sep=0.75pt]    {$21$};
			% Text Node
			\draw (391.08,273.9) node [anchor=north west][inner sep=0.75pt]    {$22$};
			% Text Node
			\draw (348.43,273.9) node [anchor=north west][inner sep=0.75pt]    {$23$};
			% Text Node
			\draw (304.78,273.9) node [anchor=north west][inner sep=0.75pt]    {$24$};
			% Text Node
			\draw (86,36) node [anchor=north west][inner sep=0.75pt]   [align=left] {Fertilizer $1$ : Blue\\Fertilizer $2$ : Red\\Fertilizer $3$ : Black\\No Fertilizer: White (control)};
			\end{tikzpicture}
		\end{figure}
		The corresponding linear model is simple, beginning with a grand mean $\mu$, and an "offset" term $\alpha_i$ for each of the $a$ treatment groups:
		
		
		\item "\NewTerm{Randomized block design}\index{ANOVA!randomized block design}\footnote{That design and all others that follow belong to a family of designs named sometimes "crossover designs" or "crossover control trials"}" (RBD):
		
		When a factor is kept fixed, while we will vary the others, it will be named the "\NewTerm{blocking factor}\footnote{More formally a block is a matched group of individual who are similar or identical on a nuisance variable.}\index{ANOVA!blocking factor}" and the others will be named "\NewTerm{treatment factors}\index{treatment factor}" (some authors designate the two fixed factor ANOVA without interactions by the name "\NewTerm{randomized block design}"). 
		
		In the figure below, we see some blocks (corresponding typically to hard-to-change factors as days, observers, lands, etc.):
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
			%uncomment if require: \path (0,1467); %set diagram left start at 0, and has height of 1467
			
			%Shape: Rectangle [id:dp3323956907711456] 
			\draw  [fill={rgb, 255:red, 255; green, 247; blue, 217 }  ,fill opacity=1 ] (289.5,35) -- (468.5,35) -- (468.5,308) -- (289.5,308) -- cycle ;
			%Shape: Rectangle [id:dp5555128169009411] 
			\draw  [fill={rgb, 255:red, 248; green, 228; blue, 130 }  ,fill opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (294.01,261) -- (464.01,261) -- (464.01,301) -- (294.01,301) -- cycle ;
			%Shape: Rectangle [id:dp7266187583580583] 
			\draw  [fill={rgb, 255:red, 248; green, 228; blue, 130 }  ,fill opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (294.01,217) -- (464.01,217) -- (464.01,257) -- (294.01,257) -- cycle ;
			%Shape: Rectangle [id:dp5273305016629877] 
			\draw  [fill={rgb, 255:red, 248; green, 228; blue, 130 }  ,fill opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (294.01,173) -- (464.01,173) -- (464.01,213) -- (294.01,213) -- cycle ;
			%Shape: Rectangle [id:dp3869455965307047] 
			\draw  [fill={rgb, 255:red, 248; green, 228; blue, 130 }  ,fill opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (294.01,129) -- (464.01,129) -- (464.01,169) -- (294.01,169) -- cycle ;
			%Shape: Rectangle [id:dp1689464760599264] 
			\draw  [fill={rgb, 255:red, 248; green, 228; blue, 130 }  ,fill opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (294.01,85) -- (464.01,85) -- (464.01,125) -- (294.01,125) -- cycle ;
			%Shape: Rectangle [id:dp7881057123703761] 
			\draw  [fill={rgb, 255:red, 248; green, 228; blue, 130 }  ,fill opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (294.01,41) -- (464.01,41) -- (464.01,81) -- (294.01,81) -- cycle ;
			%Shape: Circle [id:dp8956193049184411] 
			\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (298,61) .. controls (298,51.61) and (305.61,44) .. (315,44) .. controls (324.39,44) and (332,51.61) .. (332,61) .. controls (332,70.39) and (324.39,78) .. (315,78) .. controls (305.61,78) and (298,70.39) .. (298,61) -- cycle ;
			%Shape: Circle [id:dp35263048255777596] 
			\draw  [fill={rgb, 255:red, 255; green, 146; blue, 158 }  ,fill opacity=1 ] (340.67,61) .. controls (340.67,51.61) and (348.28,44) .. (357.67,44) .. controls (367.06,44) and (374.67,51.61) .. (374.67,61) .. controls (374.67,70.39) and (367.06,78) .. (357.67,78) .. controls (348.28,78) and (340.67,70.39) .. (340.67,61) -- cycle ;
			%Shape: Circle [id:dp4302119990474258] 
			\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (383.34,61) .. controls (383.34,51.61) and (390.95,44) .. (400.34,44) .. controls (409.73,44) and (417.34,51.61) .. (417.34,61) .. controls (417.34,70.39) and (409.73,78) .. (400.34,78) .. controls (390.95,78) and (383.34,70.39) .. (383.34,61) -- cycle ;
			%Shape: Circle [id:dp5336355850957886] 
			\draw  [fill={rgb, 255:red, 189; green, 218; blue, 255 }  ,fill opacity=1 ] (426,61) .. controls (426,51.61) and (433.61,44) .. (443,44) .. controls (452.39,44) and (460,51.61) .. (460,61) .. controls (460,70.39) and (452.39,78) .. (443,78) .. controls (433.61,78) and (426,70.39) .. (426,61) -- cycle ;
			%Shape: Circle [id:dp11817830977182942] 
			\draw  [fill={rgb, 255:red, 189; green, 218; blue, 255 }  ,fill opacity=1 ] (298,105) .. controls (298,95.61) and (305.61,88) .. (315,88) .. controls (324.39,88) and (332,95.61) .. (332,105) .. controls (332,114.39) and (324.39,122) .. (315,122) .. controls (305.61,122) and (298,114.39) .. (298,105) -- cycle ;
			%Shape: Circle [id:dp9337555336603114] 
			\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (340.67,105) .. controls (340.67,95.61) and (348.28,88) .. (357.67,88) .. controls (367.06,88) and (374.67,95.61) .. (374.67,105) .. controls (374.67,114.39) and (367.06,122) .. (357.67,122) .. controls (348.28,122) and (340.67,114.39) .. (340.67,105) -- cycle ;
			%Shape: Circle [id:dp42727978348597784] 
			\draw  [fill={rgb, 255:red, 255; green, 146; blue, 158 }  ,fill opacity=1 ] (383.34,105) .. controls (383.34,95.61) and (390.95,88) .. (400.34,88) .. controls (409.73,88) and (417.34,95.61) .. (417.34,105) .. controls (417.34,114.39) and (409.73,122) .. (400.34,122) .. controls (390.95,122) and (383.34,114.39) .. (383.34,105) -- cycle ;
			%Shape: Circle [id:dp005098165549407785] 
			\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (426,105) .. controls (426,95.61) and (433.61,88) .. (443,88) .. controls (452.39,88) and (460,95.61) .. (460,105) .. controls (460,114.39) and (452.39,122) .. (443,122) .. controls (433.61,122) and (426,114.39) .. (426,105) -- cycle ;
			%Shape: Circle [id:dp31607205356315293] 
			\draw  [fill={rgb, 255:red, 255; green, 146; blue, 158 }  ,fill opacity=1 ] (298,149) .. controls (298,139.61) and (305.61,132) .. (315,132) .. controls (324.39,132) and (332,139.61) .. (332,149) .. controls (332,158.39) and (324.39,166) .. (315,166) .. controls (305.61,166) and (298,158.39) .. (298,149) -- cycle ;
			%Shape: Circle [id:dp4833111989186021] 
			\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (340.67,149) .. controls (340.67,139.61) and (348.28,132) .. (357.67,132) .. controls (367.06,132) and (374.67,139.61) .. (374.67,149) .. controls (374.67,158.39) and (367.06,166) .. (357.67,166) .. controls (348.28,166) and (340.67,158.39) .. (340.67,149) -- cycle ;
			%Shape: Circle [id:dp8103569616283224] 
			\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (383.34,149) .. controls (383.34,139.61) and (390.95,132) .. (400.34,132) .. controls (409.73,132) and (417.34,139.61) .. (417.34,149) .. controls (417.34,158.39) and (409.73,166) .. (400.34,166) .. controls (390.95,166) and (383.34,158.39) .. (383.34,149) -- cycle ;
			%Shape: Circle [id:dp9106783112141077] 
			\draw  [fill={rgb, 255:red, 189; green, 218; blue, 255 }  ,fill opacity=1 ] (426,149) .. controls (426,139.61) and (433.61,132) .. (443,132) .. controls (452.39,132) and (460,139.61) .. (460,149) .. controls (460,158.39) and (452.39,166) .. (443,166) .. controls (433.61,166) and (426,158.39) .. (426,149) -- cycle ;
			%Shape: Circle [id:dp8048113504426111] 
			\draw  [fill={rgb, 255:red, 255; green, 146; blue, 158 }  ,fill opacity=1 ] (298,193) .. controls (298,183.61) and (305.61,176) .. (315,176) .. controls (324.39,176) and (332,183.61) .. (332,193) .. controls (332,202.39) and (324.39,210) .. (315,210) .. controls (305.61,210) and (298,202.39) .. (298,193) -- cycle ;
			%Shape: Circle [id:dp2042809257939613] 
			\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (340.67,193) .. controls (340.67,183.61) and (348.28,176) .. (357.67,176) .. controls (367.06,176) and (374.67,183.61) .. (374.67,193) .. controls (374.67,202.39) and (367.06,210) .. (357.67,210) .. controls (348.28,210) and (340.67,202.39) .. (340.67,193) -- cycle ;
			%Shape: Circle [id:dp03417872776536601] 
			\draw  [fill={rgb, 255:red, 189; green, 218; blue, 255 }  ,fill opacity=1 ] (383.34,193) .. controls (383.34,183.61) and (390.95,176) .. (400.34,176) .. controls (409.73,176) and (417.34,183.61) .. (417.34,193) .. controls (417.34,202.39) and (409.73,210) .. (400.34,210) .. controls (390.95,210) and (383.34,202.39) .. (383.34,193) -- cycle ;
			%Shape: Circle [id:dp44437114019545865] 
			\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (426,193) .. controls (426,183.61) and (433.61,176) .. (443,176) .. controls (452.39,176) and (460,183.61) .. (460,193) .. controls (460,202.39) and (452.39,210) .. (443,210) .. controls (433.61,210) and (426,202.39) .. (426,193) -- cycle ;
			%Shape: Circle [id:dp9380695872309981] 
			\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (298,237) .. controls (298,227.61) and (305.61,220) .. (315,220) .. controls (324.39,220) and (332,227.61) .. (332,237) .. controls (332,246.39) and (324.39,254) .. (315,254) .. controls (305.61,254) and (298,246.39) .. (298,237) -- cycle ;
			%Shape: Circle [id:dp5519960749143016] 
			\draw  [fill={rgb, 255:red, 255; green, 146; blue, 158 }  ,fill opacity=1 ] (340.67,237) .. controls (340.67,227.61) and (348.28,220) .. (357.67,220) .. controls (367.06,220) and (374.67,227.61) .. (374.67,237) .. controls (374.67,246.39) and (367.06,254) .. (357.67,254) .. controls (348.28,254) and (340.67,246.39) .. (340.67,237) -- cycle ;
			%Shape: Circle [id:dp7278139326826107] 
			\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (383.34,237) .. controls (383.34,227.61) and (390.95,220) .. (400.34,220) .. controls (409.73,220) and (417.34,227.61) .. (417.34,237) .. controls (417.34,246.39) and (409.73,254) .. (400.34,254) .. controls (390.95,254) and (383.34,246.39) .. (383.34,237) -- cycle ;
			%Shape: Circle [id:dp332109258554919] 
			\draw  [fill={rgb, 255:red, 189; green, 218; blue, 255 }  ,fill opacity=1 ] (426,237) .. controls (426,227.61) and (433.61,220) .. (443,220) .. controls (452.39,220) and (460,227.61) .. (460,237) .. controls (460,246.39) and (452.39,254) .. (443,254) .. controls (433.61,254) and (426,246.39) .. (426,237) -- cycle ;
			%Shape: Circle [id:dp4006033681370744] 
			\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (298,281) .. controls (298,271.61) and (305.61,264) .. (315,264) .. controls (324.39,264) and (332,271.61) .. (332,281) .. controls (332,290.39) and (324.39,298) .. (315,298) .. controls (305.61,298) and (298,290.39) .. (298,281) -- cycle ;
			%Shape: Circle [id:dp8426456774715223] 
			\draw  [fill={rgb, 255:red, 255; green, 146; blue, 158 }  ,fill opacity=1 ] (340.67,281) .. controls (340.67,271.61) and (348.28,264) .. (357.67,264) .. controls (367.06,264) and (374.67,271.61) .. (374.67,281) .. controls (374.67,290.39) and (367.06,298) .. (357.67,298) .. controls (348.28,298) and (340.67,290.39) .. (340.67,281) -- cycle ;
			%Shape: Circle [id:dp37275015485378127] 
			\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (383.34,281) .. controls (383.34,271.61) and (390.95,264) .. (400.34,264) .. controls (409.73,264) and (417.34,271.61) .. (417.34,281) .. controls (417.34,290.39) and (409.73,298) .. (400.34,298) .. controls (390.95,298) and (383.34,290.39) .. (383.34,281) -- cycle ;
			%Shape: Circle [id:dp8005137761157473] 
			\draw  [fill={rgb, 255:red, 189; green, 218; blue, 255 }  ,fill opacity=1 ] (426,281) .. controls (426,271.61) and (433.61,264) .. (443,264) .. controls (452.39,264) and (460,271.61) .. (460,281) .. controls (460,290.39) and (452.39,298) .. (443,298) .. controls (433.61,298) and (426,290.39) .. (426,281) -- cycle ;
			
			% Text Node
			\draw (437.72,53.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
			% Text Node
			\draw (395.08,53.9) node [anchor=north west][inner sep=0.75pt]    {$2$};
			% Text Node
			\draw (353.43,53.9) node [anchor=north west][inner sep=0.75pt]    {$3$};
			% Text Node
			\draw (308.78,53.9) node [anchor=north west][inner sep=0.75pt]    {$4$};
			% Text Node
			\draw (437.72,97.9) node [anchor=north west][inner sep=0.75pt]    {$5$};
			% Text Node
			\draw (395.08,97.9) node [anchor=north west][inner sep=0.75pt]    {$6$};
			% Text Node
			\draw (353.43,97.9) node [anchor=north west][inner sep=0.75pt]    {$7$};
			% Text Node
			\draw (308.78,97.9) node [anchor=north west][inner sep=0.75pt]    {$8$};
			% Text Node
			\draw (437.72,141.9) node [anchor=north west][inner sep=0.75pt]    {$9$};
			% Text Node
			\draw (390.08,141.9) node [anchor=north west][inner sep=0.75pt]    {$10$};
			% Text Node
			\draw (348.43,141.9) node [anchor=north west][inner sep=0.75pt]    {$11$};
			% Text Node
			\draw (304.78,141.9) node [anchor=north west][inner sep=0.75pt]    {$12$};
			% Text Node
			\draw (433.72,185.9) node [anchor=north west][inner sep=0.75pt]    {$13$};
			% Text Node
			\draw (391.08,185.9) node [anchor=north west][inner sep=0.75pt]    {$14$};
			% Text Node
			\draw (348.43,185.9) node [anchor=north west][inner sep=0.75pt]    {$15$};
			% Text Node
			\draw (304.78,185.9) node [anchor=north west][inner sep=0.75pt]    {$16$};
			% Text Node
			\draw (433.72,229.9) node [anchor=north west][inner sep=0.75pt]    {$17$};
			% Text Node
			\draw (391.08,229.9) node [anchor=north west][inner sep=0.75pt]    {$18$};
			% Text Node
			\draw (348.43,229.9) node [anchor=north west][inner sep=0.75pt]    {$19$};
			% Text Node
			\draw (304.78,229.9) node [anchor=north west][inner sep=0.75pt]    {$20$};
			% Text Node
			\draw (433.72,273.9) node [anchor=north west][inner sep=0.75pt]    {$21$};
			% Text Node
			\draw (391.08,273.9) node [anchor=north west][inner sep=0.75pt]    {$22$};
			% Text Node
			\draw (348.43,273.9) node [anchor=north west][inner sep=0.75pt]    {$23$};
			% Text Node
			\draw (304.78,273.9) node [anchor=north west][inner sep=0.75pt]    {$24$};
			% Text Node
			\draw (85,36) node [anchor=north west][inner sep=0.75pt]   [align=left] {Fertilizer $\displaystyle 1$ : Blue\\Fertilizer $\displaystyle 2$ : Red\\Fertilizer $\displaystyle 3$ : Black\\No Fertilizer: White (control)};
			% Text Node
			\draw (361,13) node [anchor=north west][inner sep=0.75pt]   [align=left] {Wall};
			% Text Node
			\draw (473,52) node [anchor=north west][inner sep=0.75pt]   [align=left] {Block I};
			% Text Node
			\draw (473,98.8) node [anchor=north west][inner sep=0.75pt]   [align=left] {Block II};
			% Text Node
			\draw (473,143.6) node [anchor=north west][inner sep=0.75pt]   [align=left] {Block III};
			% Text Node
			\draw (473,186.4) node [anchor=north west][inner sep=0.75pt]   [align=left] {Block IV};
			% Text Node
			\draw (473,230.2) node [anchor=north west][inner sep=0.75pt]   [align=left] {Block V};
			% Text Node
			\draw (473,274) node [anchor=north west][inner sep=0.75pt]   [align=left] {Block VI};
			\end{tikzpicture}
		\end{figure}
		The corresponding linear model is simple as with just have a CRD applied to each block $b$:
		
		In practice we will ensure that the randomization in the blocks is not always performed in the same order to eliminate potential inertia when changing from one treatment to the other effects (the reader can see in the previous figure that two blocks \texttt{I} and \texttt{V} have the same randomization). This lead us to the next type of design.
		
		\item "\NewTerm{Randomized complete block design}\index{ANOVA!randomized complete block design}" (RCBD):
		
		The randomized complete block design (RCBD) also uses a restricted randomization scheme in the purpose to reduce the errors which account for differences among observation within each treatment. Within every block (e.g., location), the treatments are randomized to the experimental units (e.g., plots of land). This creates difference among the blocks and makes observation within a block more similar.
		
		The design is named "complete" because we see the complete set of treatments within every block and there is no identically randomized blocks (we will later also learn about incomplete block designs where this is not the case anymore). Note that blocking already exists at the time of randomization (and not only at the time of the analysis).
		
		The analysis of a randomized complete block design is straightforward. We treat the block factor as "another" factor in our model\footnote{We can also block on more than one factor. A special case is the so-named Latin Square Design} and then it is just a two-way ANOVA.
		
		Because in the previous RBD the blocks I and V had identical randomization, we have now in RCBD configuration:
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
			%uncomment if require: \path (0,1467); %set diagram left start at 0, and has height of 1467
			
			%Shape: Rectangle [id:dp3323956907711456] 
			\draw  [fill={rgb, 255:red, 255; green, 247; blue, 217 }  ,fill opacity=1 ] (289.5,35) -- (468.5,35) -- (468.5,308) -- (289.5,308) -- cycle ;
			%Shape: Rectangle [id:dp5555128169009411] 
			\draw  [fill={rgb, 255:red, 248; green, 228; blue, 130 }  ,fill opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (294.01,261) -- (464.01,261) -- (464.01,301) -- (294.01,301) -- cycle ;
			%Shape: Rectangle [id:dp7266187583580583] 
			\draw  [fill={rgb, 255:red, 248; green, 228; blue, 130 }  ,fill opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (294.01,217) -- (464.01,217) -- (464.01,257) -- (294.01,257) -- cycle ;
			%Shape: Rectangle [id:dp5273305016629877] 
			\draw  [fill={rgb, 255:red, 248; green, 228; blue, 130 }  ,fill opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (294.01,173) -- (464.01,173) -- (464.01,213) -- (294.01,213) -- cycle ;
			%Shape: Rectangle [id:dp3869455965307047] 
			\draw  [fill={rgb, 255:red, 248; green, 228; blue, 130 }  ,fill opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (294.01,129) -- (464.01,129) -- (464.01,169) -- (294.01,169) -- cycle ;
			%Shape: Rectangle [id:dp1689464760599264] 
			\draw  [fill={rgb, 255:red, 248; green, 228; blue, 130 }  ,fill opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (294.01,85) -- (464.01,85) -- (464.01,125) -- (294.01,125) -- cycle ;
			%Shape: Rectangle [id:dp7881057123703761] 
			\draw  [fill={rgb, 255:red, 248; green, 228; blue, 130 }  ,fill opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (294.01,41) -- (464.01,41) -- (464.01,81) -- (294.01,81) -- cycle ;
			%Shape: Circle [id:dp8956193049184411] 
			\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (298,61) .. controls (298,51.61) and (305.61,44) .. (315,44) .. controls (324.39,44) and (332,51.61) .. (332,61) .. controls (332,70.39) and (324.39,78) .. (315,78) .. controls (305.61,78) and (298,70.39) .. (298,61) -- cycle ;
			%Shape: Circle [id:dp35263048255777596] 
			\draw  [fill={rgb, 255:red, 255; green, 146; blue, 158 }  ,fill opacity=1 ] (340.67,61) .. controls (340.67,51.61) and (348.28,44) .. (357.67,44) .. controls (367.06,44) and (374.67,51.61) .. (374.67,61) .. controls (374.67,70.39) and (367.06,78) .. (357.67,78) .. controls (348.28,78) and (340.67,70.39) .. (340.67,61) -- cycle ;
			%Shape: Circle [id:dp4302119990474258] 
			\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (383.34,61) .. controls (383.34,51.61) and (390.95,44) .. (400.34,44) .. controls (409.73,44) and (417.34,51.61) .. (417.34,61) .. controls (417.34,70.39) and (409.73,78) .. (400.34,78) .. controls (390.95,78) and (383.34,70.39) .. (383.34,61) -- cycle ;
			%Shape: Circle [id:dp5336355850957886] 
			\draw  [fill={rgb, 255:red, 189; green, 218; blue, 255 }  ,fill opacity=1 ] (426,61) .. controls (426,51.61) and (433.61,44) .. (443,44) .. controls (452.39,44) and (460,51.61) .. (460,61) .. controls (460,70.39) and (452.39,78) .. (443,78) .. controls (433.61,78) and (426,70.39) .. (426,61) -- cycle ;
			%Shape: Circle [id:dp11817830977182942] 
			\draw  [fill={rgb, 255:red, 189; green, 218; blue, 255 }  ,fill opacity=1 ] (298,105) .. controls (298,95.61) and (305.61,88) .. (315,88) .. controls (324.39,88) and (332,95.61) .. (332,105) .. controls (332,114.39) and (324.39,122) .. (315,122) .. controls (305.61,122) and (298,114.39) .. (298,105) -- cycle ;
			%Shape: Circle [id:dp9337555336603114] 
			\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (340.67,105) .. controls (340.67,95.61) and (348.28,88) .. (357.67,88) .. controls (367.06,88) and (374.67,95.61) .. (374.67,105) .. controls (374.67,114.39) and (367.06,122) .. (357.67,122) .. controls (348.28,122) and (340.67,114.39) .. (340.67,105) -- cycle ;
			%Shape: Circle [id:dp42727978348597784] 
			\draw  [fill={rgb, 255:red, 255; green, 146; blue, 158 }  ,fill opacity=1 ] (383.34,105) .. controls (383.34,95.61) and (390.95,88) .. (400.34,88) .. controls (409.73,88) and (417.34,95.61) .. (417.34,105) .. controls (417.34,114.39) and (409.73,122) .. (400.34,122) .. controls (390.95,122) and (383.34,114.39) .. (383.34,105) -- cycle ;
			%Shape: Circle [id:dp005098165549407785] 
			\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (426,105) .. controls (426,95.61) and (433.61,88) .. (443,88) .. controls (452.39,88) and (460,95.61) .. (460,105) .. controls (460,114.39) and (452.39,122) .. (443,122) .. controls (433.61,122) and (426,114.39) .. (426,105) -- cycle ;
			%Shape: Circle [id:dp31607205356315293] 
			\draw  [fill={rgb, 255:red, 255; green, 146; blue, 158 }  ,fill opacity=1 ] (298,149) .. controls (298,139.61) and (305.61,132) .. (315,132) .. controls (324.39,132) and (332,139.61) .. (332,149) .. controls (332,158.39) and (324.39,166) .. (315,166) .. controls (305.61,166) and (298,158.39) .. (298,149) -- cycle ;
			%Shape: Circle [id:dp4833111989186021] 
			\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (340.67,149) .. controls (340.67,139.61) and (348.28,132) .. (357.67,132) .. controls (367.06,132) and (374.67,139.61) .. (374.67,149) .. controls (374.67,158.39) and (367.06,166) .. (357.67,166) .. controls (348.28,166) and (340.67,158.39) .. (340.67,149) -- cycle ;
			%Shape: Circle [id:dp8103569616283224] 
			\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (383.34,149) .. controls (383.34,139.61) and (390.95,132) .. (400.34,132) .. controls (409.73,132) and (417.34,139.61) .. (417.34,149) .. controls (417.34,158.39) and (409.73,166) .. (400.34,166) .. controls (390.95,166) and (383.34,158.39) .. (383.34,149) -- cycle ;
			%Shape: Circle [id:dp9106783112141077] 
			\draw  [fill={rgb, 255:red, 189; green, 218; blue, 255 }  ,fill opacity=1 ] (426,149) .. controls (426,139.61) and (433.61,132) .. (443,132) .. controls (452.39,132) and (460,139.61) .. (460,149) .. controls (460,158.39) and (452.39,166) .. (443,166) .. controls (433.61,166) and (426,158.39) .. (426,149) -- cycle ;
			%Shape: Circle [id:dp8048113504426111] 
			\draw  [fill={rgb, 255:red, 255; green, 146; blue, 158 }  ,fill opacity=1 ] (298,193) .. controls (298,183.61) and (305.61,176) .. (315,176) .. controls (324.39,176) and (332,183.61) .. (332,193) .. controls (332,202.39) and (324.39,210) .. (315,210) .. controls (305.61,210) and (298,202.39) .. (298,193) -- cycle ;
			%Shape: Circle [id:dp2042809257939613] 
			\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (340.67,193) .. controls (340.67,183.61) and (348.28,176) .. (357.67,176) .. controls (367.06,176) and (374.67,183.61) .. (374.67,193) .. controls (374.67,202.39) and (367.06,210) .. (357.67,210) .. controls (348.28,210) and (340.67,202.39) .. (340.67,193) -- cycle ;
			%Shape: Circle [id:dp03417872776536601] 
			\draw  [fill={rgb, 255:red, 189; green, 218; blue, 255 }  ,fill opacity=1 ] (383.34,193) .. controls (383.34,183.61) and (390.95,176) .. (400.34,176) .. controls (409.73,176) and (417.34,183.61) .. (417.34,193) .. controls (417.34,202.39) and (409.73,210) .. (400.34,210) .. controls (390.95,210) and (383.34,202.39) .. (383.34,193) -- cycle ;
			%Shape: Circle [id:dp44437114019545865] 
			\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (426,193) .. controls (426,183.61) and (433.61,176) .. (443,176) .. controls (452.39,176) and (460,183.61) .. (460,193) .. controls (460,202.39) and (452.39,210) .. (443,210) .. controls (433.61,210) and (426,202.39) .. (426,193) -- cycle ;
			%Shape: Circle [id:dp9380695872309981] 
			\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (298,237) .. controls (298,227.61) and (305.61,220) .. (315,220) .. controls (324.39,220) and (332,227.61) .. (332,237) .. controls (332,246.39) and (324.39,254) .. (315,254) .. controls (305.61,254) and (298,246.39) .. (298,237) -- cycle ;
			%Shape: Circle [id:dp5519960749143016] 
			\draw  [fill={rgb, 255:red, 189; green, 218; blue, 255 }  ,fill opacity=1 ] (340.67,237) .. controls (340.67,227.61) and (348.28,220) .. (357.67,220) .. controls (367.06,220) and (374.67,227.61) .. (374.67,237) .. controls (374.67,246.39) and (367.06,254) .. (357.67,254) .. controls (348.28,254) and (340.67,246.39) .. (340.67,237) -- cycle ;
			%Shape: Circle [id:dp7278139326826107] 
			\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (383.34,237) .. controls (383.34,227.61) and (390.95,220) .. (400.34,220) .. controls (409.73,220) and (417.34,227.61) .. (417.34,237) .. controls (417.34,246.39) and (409.73,254) .. (400.34,254) .. controls (390.95,254) and (383.34,246.39) .. (383.34,237) -- cycle ;
			%Shape: Circle [id:dp332109258554919] 
			\draw  [fill={rgb, 255:red, 255; green, 146; blue, 158 }  ,fill opacity=1 ] (426,237) .. controls (426,227.61) and (433.61,220) .. (443,220) .. controls (452.39,220) and (460,227.61) .. (460,237) .. controls (460,246.39) and (452.39,254) .. (443,254) .. controls (433.61,254) and (426,246.39) .. (426,237) -- cycle ;
			%Shape: Circle [id:dp4006033681370744] 
			\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (298,281) .. controls (298,271.61) and (305.61,264) .. (315,264) .. controls (324.39,264) and (332,271.61) .. (332,281) .. controls (332,290.39) and (324.39,298) .. (315,298) .. controls (305.61,298) and (298,290.39) .. (298,281) -- cycle ;
			%Shape: Circle [id:dp8426456774715223] 
			\draw  [fill={rgb, 255:red, 255; green, 146; blue, 158 }  ,fill opacity=1 ] (340.67,281) .. controls (340.67,271.61) and (348.28,264) .. (357.67,264) .. controls (367.06,264) and (374.67,271.61) .. (374.67,281) .. controls (374.67,290.39) and (367.06,298) .. (357.67,298) .. controls (348.28,298) and (340.67,290.39) .. (340.67,281) -- cycle ;
			%Shape: Circle [id:dp37275015485378127] 
			\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (383.34,281) .. controls (383.34,271.61) and (390.95,264) .. (400.34,264) .. controls (409.73,264) and (417.34,271.61) .. (417.34,281) .. controls (417.34,290.39) and (409.73,298) .. (400.34,298) .. controls (390.95,298) and (383.34,290.39) .. (383.34,281) -- cycle ;
			%Shape: Circle [id:dp8005137761157473] 
			\draw  [fill={rgb, 255:red, 189; green, 218; blue, 255 }  ,fill opacity=1 ] (426,281) .. controls (426,271.61) and (433.61,264) .. (443,264) .. controls (452.39,264) and (460,271.61) .. (460,281) .. controls (460,290.39) and (452.39,298) .. (443,298) .. controls (433.61,298) and (426,290.39) .. (426,281) -- cycle ;
			
			% Text Node
			\draw (437.72,53.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
			% Text Node
			\draw (395.08,53.9) node [anchor=north west][inner sep=0.75pt]    {$2$};
			% Text Node
			\draw (353.43,53.9) node [anchor=north west][inner sep=0.75pt]    {$3$};
			% Text Node
			\draw (308.78,53.9) node [anchor=north west][inner sep=0.75pt]    {$4$};
			% Text Node
			\draw (437.72,97.9) node [anchor=north west][inner sep=0.75pt]    {$5$};
			% Text Node
			\draw (395.08,97.9) node [anchor=north west][inner sep=0.75pt]    {$6$};
			% Text Node
			\draw (353.43,97.9) node [anchor=north west][inner sep=0.75pt]    {$7$};
			% Text Node
			\draw (308.78,97.9) node [anchor=north west][inner sep=0.75pt]    {$8$};
			% Text Node
			\draw (437.72,141.9) node [anchor=north west][inner sep=0.75pt]    {$9$};
			% Text Node
			\draw (390.08,141.9) node [anchor=north west][inner sep=0.75pt]    {$10$};
			% Text Node
			\draw (348.43,141.9) node [anchor=north west][inner sep=0.75pt]    {$11$};
			% Text Node
			\draw (304.78,141.9) node [anchor=north west][inner sep=0.75pt]    {$12$};
			% Text Node
			\draw (433.72,185.9) node [anchor=north west][inner sep=0.75pt]    {$13$};
			% Text Node
			\draw (391.08,185.9) node [anchor=north west][inner sep=0.75pt]    {$14$};
			% Text Node
			\draw (348.43,185.9) node [anchor=north west][inner sep=0.75pt]    {$15$};
			% Text Node
			\draw (304.78,185.9) node [anchor=north west][inner sep=0.75pt]    {$16$};
			% Text Node
			\draw (433.72,229.9) node [anchor=north west][inner sep=0.75pt]    {$17$};
			% Text Node
			\draw (391.08,229.9) node [anchor=north west][inner sep=0.75pt]    {$18$};
			% Text Node
			\draw (348.43,229.9) node [anchor=north west][inner sep=0.75pt]    {$19$};
			% Text Node
			\draw (304.78,229.9) node [anchor=north west][inner sep=0.75pt]    {$20$};
			% Text Node
			\draw (433.72,273.9) node [anchor=north west][inner sep=0.75pt]    {$21$};
			% Text Node
			\draw (391.08,273.9) node [anchor=north west][inner sep=0.75pt]    {$22$};
			% Text Node
			\draw (348.43,273.9) node [anchor=north west][inner sep=0.75pt]    {$23$};
			% Text Node
			\draw (304.78,273.9) node [anchor=north west][inner sep=0.75pt]    {$24$};
			% Text Node
			\draw (85,36) node [anchor=north west][inner sep=0.75pt]   [align=left] {Fertilizer $\displaystyle 1$ : Blue\\Fertilizer $\displaystyle 2$ : Red\\Fertilizer $\displaystyle 3$ : Black\\No Fertilizer: White (control)};
			% Text Node
			\draw (361,13) node [anchor=north west][inner sep=0.75pt]   [align=left] {Wall};
			% Text Node
			\draw (473,52) node [anchor=north west][inner sep=0.75pt]   [align=left] {Block I};
			% Text Node
			\draw (473,98.8) node [anchor=north west][inner sep=0.75pt]   [align=left] {Block II};
			% Text Node
			\draw (473,143.6) node [anchor=north west][inner sep=0.75pt]   [align=left] {Block III};
			% Text Node
			\draw (473,186.4) node [anchor=north west][inner sep=0.75pt]   [align=left] {Block IV};
			% Text Node
			\draw (473,230.2) node [anchor=north west][inner sep=0.75pt]   [align=left] {Block V};
			% Text Node
			\draw (473,274) node [anchor=north west][inner sep=0.75pt]   [align=left] {Block VI};
			
			\end{tikzpicture}
		\end{figure}
		So if you are able to maintain the condition homogeneous/constant throughout your experiment then CRD is just fine for lab experiment (lab experiments are usually conducted under the controlled conditions therefore the CRD test is usually used). But if you have resources like time, space and funds and you are confident enough to neutralize the variation (error) using suitable bollocking technique then go for the RCBD to reduce the experimental error.
		
		\item "\NewTerm{Incomplete block design}\index{ANOVA!incomplete block design}" (IBD):
		
		Some experiments may consist of a large number of treatments and it may not be feasible to run all the treatments in all the blocks. Designs where only some of the treatments appear in every block are known as incomplete block designs.
		
		Here is an example where we can see that in some blocks, some treatments never appear:
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
			%uncomment if require: \path (0,1467); %set diagram left start at 0, and has height of 1467
			
			%Shape: Rectangle [id:dp3323956907711456] 
			\draw  [fill={rgb, 255:red, 255; green, 247; blue, 217 }  ,fill opacity=1 ] (289.5,35) -- (468.5,35) -- (468.5,308) -- (289.5,308) -- cycle ;
			%Shape: Rectangle [id:dp5555128169009411] 
			\draw  [fill={rgb, 255:red, 248; green, 228; blue, 130 }  ,fill opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (294.01,261) -- (464.01,261) -- (464.01,301) -- (294.01,301) -- cycle ;
			%Shape: Rectangle [id:dp7266187583580583] 
			\draw  [fill={rgb, 255:red, 248; green, 228; blue, 130 }  ,fill opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (294.01,217) -- (464.01,217) -- (464.01,257) -- (294.01,257) -- cycle ;
			%Shape: Rectangle [id:dp5273305016629877] 
			\draw  [fill={rgb, 255:red, 248; green, 228; blue, 130 }  ,fill opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (294.01,173) -- (464.01,173) -- (464.01,213) -- (294.01,213) -- cycle ;
			%Shape: Rectangle [id:dp3869455965307047] 
			\draw  [fill={rgb, 255:red, 248; green, 228; blue, 130 }  ,fill opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (294.01,129) -- (464.01,129) -- (464.01,169) -- (294.01,169) -- cycle ;
			%Shape: Rectangle [id:dp1689464760599264] 
			\draw  [fill={rgb, 255:red, 248; green, 228; blue, 130 }  ,fill opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (294.01,85) -- (464.01,85) -- (464.01,125) -- (294.01,125) -- cycle ;
			%Shape: Rectangle [id:dp7881057123703761] 
			\draw  [fill={rgb, 255:red, 248; green, 228; blue, 130 }  ,fill opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (294.01,41) -- (464.01,41) -- (464.01,81) -- (294.01,81) -- cycle ;
			%Shape: Circle [id:dp8956193049184411] 
			\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (298,61) .. controls (298,51.61) and (305.61,44) .. (315,44) .. controls (324.39,44) and (332,51.61) .. (332,61) .. controls (332,70.39) and (324.39,78) .. (315,78) .. controls (305.61,78) and (298,70.39) .. (298,61) -- cycle ;
			%Shape: Circle [id:dp35263048255777596] 
			\draw  [fill={rgb, 255:red, 255; green, 146; blue, 158 }  ,fill opacity=1 ] (340.67,61) .. controls (340.67,51.61) and (348.28,44) .. (357.67,44) .. controls (367.06,44) and (374.67,51.61) .. (374.67,61) .. controls (374.67,70.39) and (367.06,78) .. (357.67,78) .. controls (348.28,78) and (340.67,70.39) .. (340.67,61) -- cycle ;
			%Shape: Circle [id:dp4302119990474258] 
			\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (383.34,61) .. controls (383.34,51.61) and (390.95,44) .. (400.34,44) .. controls (409.73,44) and (417.34,51.61) .. (417.34,61) .. controls (417.34,70.39) and (409.73,78) .. (400.34,78) .. controls (390.95,78) and (383.34,70.39) .. (383.34,61) -- cycle ;
			%Shape: Circle [id:dp5336355850957886] 
			\draw  [fill={rgb, 255:red, 255; green, 146; blue, 158 }  ,fill opacity=1 ] (426,61) .. controls (426,51.61) and (433.61,44) .. (443,44) .. controls (452.39,44) and (460,51.61) .. (460,61) .. controls (460,70.39) and (452.39,78) .. (443,78) .. controls (433.61,78) and (426,70.39) .. (426,61) -- cycle ;
			%Shape: Circle [id:dp11817830977182942] 
			\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (298,105) .. controls (298,95.61) and (305.61,88) .. (315,88) .. controls (324.39,88) and (332,95.61) .. (332,105) .. controls (332,114.39) and (324.39,122) .. (315,122) .. controls (305.61,122) and (298,114.39) .. (298,105) -- cycle ;
			%Shape: Circle [id:dp9337555336603114] 
			\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (340.67,105) .. controls (340.67,95.61) and (348.28,88) .. (357.67,88) .. controls (367.06,88) and (374.67,95.61) .. (374.67,105) .. controls (374.67,114.39) and (367.06,122) .. (357.67,122) .. controls (348.28,122) and (340.67,114.39) .. (340.67,105) -- cycle ;
			%Shape: Circle [id:dp42727978348597784] 
			\draw  [fill={rgb, 255:red, 255; green, 146; blue, 158 }  ,fill opacity=1 ] (383.34,105) .. controls (383.34,95.61) and (390.95,88) .. (400.34,88) .. controls (409.73,88) and (417.34,95.61) .. (417.34,105) .. controls (417.34,114.39) and (409.73,122) .. (400.34,122) .. controls (390.95,122) and (383.34,114.39) .. (383.34,105) -- cycle ;
			%Shape: Circle [id:dp005098165549407785] 
			\draw  [fill={rgb, 255:red, 255; green, 146; blue, 158 }  ,fill opacity=1 ] (426,105) .. controls (426,95.61) and (433.61,88) .. (443,88) .. controls (452.39,88) and (460,95.61) .. (460,105) .. controls (460,114.39) and (452.39,122) .. (443,122) .. controls (433.61,122) and (426,114.39) .. (426,105) -- cycle ;
			%Shape: Circle [id:dp31607205356315293] 
			\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (298,149) .. controls (298,139.61) and (305.61,132) .. (315,132) .. controls (324.39,132) and (332,139.61) .. (332,149) .. controls (332,158.39) and (324.39,166) .. (315,166) .. controls (305.61,166) and (298,158.39) .. (298,149) -- cycle ;
			%Shape: Circle [id:dp4833111989186021] 
			\draw  [fill={rgb, 255:red, 189; green, 218; blue, 255 }  ,fill opacity=1 ] (340.67,149) .. controls (340.67,139.61) and (348.28,132) .. (357.67,132) .. controls (367.06,132) and (374.67,139.61) .. (374.67,149) .. controls (374.67,158.39) and (367.06,166) .. (357.67,166) .. controls (348.28,166) and (340.67,158.39) .. (340.67,149) -- cycle ;
			%Shape: Circle [id:dp8103569616283224] 
			\draw  [fill={rgb, 255:red, 189; green, 218; blue, 255 }  ,fill opacity=1 ] (383.34,149) .. controls (383.34,139.61) and (390.95,132) .. (400.34,132) .. controls (409.73,132) and (417.34,139.61) .. (417.34,149) .. controls (417.34,158.39) and (409.73,166) .. (400.34,166) .. controls (390.95,166) and (383.34,158.39) .. (383.34,149) -- cycle ;
			%Shape: Circle [id:dp9106783112141077] 
			\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (426,149) .. controls (426,139.61) and (433.61,132) .. (443,132) .. controls (452.39,132) and (460,139.61) .. (460,149) .. controls (460,158.39) and (452.39,166) .. (443,166) .. controls (433.61,166) and (426,158.39) .. (426,149) -- cycle ;
			%Shape: Circle [id:dp8048113504426111] 
			\draw  [fill={rgb, 255:red, 255; green, 146; blue, 158 }  ,fill opacity=1 ] (298,193) .. controls (298,183.61) and (305.61,176) .. (315,176) .. controls (324.39,176) and (332,183.61) .. (332,193) .. controls (332,202.39) and (324.39,210) .. (315,210) .. controls (305.61,210) and (298,202.39) .. (298,193) -- cycle ;
			%Shape: Circle [id:dp2042809257939613] 
			\draw  [fill={rgb, 255:red, 189; green, 218; blue, 255 }  ,fill opacity=1 ] (340.67,193) .. controls (340.67,183.61) and (348.28,176) .. (357.67,176) .. controls (367.06,176) and (374.67,183.61) .. (374.67,193) .. controls (374.67,202.39) and (367.06,210) .. (357.67,210) .. controls (348.28,210) and (340.67,202.39) .. (340.67,193) -- cycle ;
			%Shape: Circle [id:dp03417872776536601] 
			\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (383.34,193) .. controls (383.34,183.61) and (390.95,176) .. (400.34,176) .. controls (409.73,176) and (417.34,183.61) .. (417.34,193) .. controls (417.34,202.39) and (409.73,210) .. (400.34,210) .. controls (390.95,210) and (383.34,202.39) .. (383.34,193) -- cycle ;
			%Shape: Circle [id:dp44437114019545865] 
			\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (426,193) .. controls (426,183.61) and (433.61,176) .. (443,176) .. controls (452.39,176) and (460,183.61) .. (460,193) .. controls (460,202.39) and (452.39,210) .. (443,210) .. controls (433.61,210) and (426,202.39) .. (426,193) -- cycle ;
			%Shape: Circle [id:dp9380695872309981] 
			\draw  [fill={rgb, 255:red, 189; green, 218; blue, 255 }  ,fill opacity=1 ] (298,237) .. controls (298,227.61) and (305.61,220) .. (315,220) .. controls (324.39,220) and (332,227.61) .. (332,237) .. controls (332,246.39) and (324.39,254) .. (315,254) .. controls (305.61,254) and (298,246.39) .. (298,237) -- cycle ;
			%Shape: Circle [id:dp5519960749143016] 
			\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (340.67,237) .. controls (340.67,227.61) and (348.28,220) .. (357.67,220) .. controls (367.06,220) and (374.67,227.61) .. (374.67,237) .. controls (374.67,246.39) and (367.06,254) .. (357.67,254) .. controls (348.28,254) and (340.67,246.39) .. (340.67,237) -- cycle ;
			%Shape: Circle [id:dp7278139326826107] 
			\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (383.34,237) .. controls (383.34,227.61) and (390.95,220) .. (400.34,220) .. controls (409.73,220) and (417.34,227.61) .. (417.34,237) .. controls (417.34,246.39) and (409.73,254) .. (400.34,254) .. controls (390.95,254) and (383.34,246.39) .. (383.34,237) -- cycle ;
			%Shape: Circle [id:dp332109258554919] 
			\draw  [fill={rgb, 255:red, 255; green, 146; blue, 158 }  ,fill opacity=1 ] (426,237) .. controls (426,227.61) and (433.61,220) .. (443,220) .. controls (452.39,220) and (460,227.61) .. (460,237) .. controls (460,246.39) and (452.39,254) .. (443,254) .. controls (433.61,254) and (426,246.39) .. (426,237) -- cycle ;
			%Shape: Circle [id:dp4006033681370744] 
			\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (298,281) .. controls (298,271.61) and (305.61,264) .. (315,264) .. controls (324.39,264) and (332,271.61) .. (332,281) .. controls (332,290.39) and (324.39,298) .. (315,298) .. controls (305.61,298) and (298,290.39) .. (298,281) -- cycle ;
			%Shape: Circle [id:dp8426456774715223] 
			\draw  [fill={rgb, 255:red, 189; green, 218; blue, 255 }  ,fill opacity=1 ] (340.67,281) .. controls (340.67,271.61) and (348.28,264) .. (357.67,264) .. controls (367.06,264) and (374.67,271.61) .. (374.67,281) .. controls (374.67,290.39) and (367.06,298) .. (357.67,298) .. controls (348.28,298) and (340.67,290.39) .. (340.67,281) -- cycle ;
			%Shape: Circle [id:dp37275015485378127] 
			\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (383.34,281) .. controls (383.34,271.61) and (390.95,264) .. (400.34,264) .. controls (409.73,264) and (417.34,271.61) .. (417.34,281) .. controls (417.34,290.39) and (409.73,298) .. (400.34,298) .. controls (390.95,298) and (383.34,290.39) .. (383.34,281) -- cycle ;
			%Shape: Circle [id:dp8005137761157473] 
			\draw  [fill={rgb, 255:red, 189; green, 218; blue, 255 }  ,fill opacity=1 ] (426,281) .. controls (426,271.61) and (433.61,264) .. (443,264) .. controls (452.39,264) and (460,271.61) .. (460,281) .. controls (460,290.39) and (452.39,298) .. (443,298) .. controls (433.61,298) and (426,290.39) .. (426,281) -- cycle ;
			
			% Text Node
			\draw (437.72,53.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
			% Text Node
			\draw (395.08,53.9) node [anchor=north west][inner sep=0.75pt]    {$2$};
			% Text Node
			\draw (353.43,53.9) node [anchor=north west][inner sep=0.75pt]    {$3$};
			% Text Node
			\draw (308.78,53.9) node [anchor=north west][inner sep=0.75pt]    {$4$};
			% Text Node
			\draw (437.72,97.9) node [anchor=north west][inner sep=0.75pt]    {$5$};
			% Text Node
			\draw (395.08,97.9) node [anchor=north west][inner sep=0.75pt]    {$6$};
			% Text Node
			\draw (353.43,97.9) node [anchor=north west][inner sep=0.75pt]    {$7$};
			% Text Node
			\draw (308.78,97.9) node [anchor=north west][inner sep=0.75pt]    {$8$};
			% Text Node
			\draw (437.72,141.9) node [anchor=north west][inner sep=0.75pt]    {$9$};
			% Text Node
			\draw (390.08,141.9) node [anchor=north west][inner sep=0.75pt]    {$10$};
			% Text Node
			\draw (348.43,141.9) node [anchor=north west][inner sep=0.75pt]    {$11$};
			% Text Node
			\draw (304.78,141.9) node [anchor=north west][inner sep=0.75pt]    {$12$};
			% Text Node
			\draw (433.72,185.9) node [anchor=north west][inner sep=0.75pt]    {$13$};
			% Text Node
			\draw (391.08,185.9) node [anchor=north west][inner sep=0.75pt]    {$14$};
			% Text Node
			\draw (348.43,185.9) node [anchor=north west][inner sep=0.75pt]    {$15$};
			% Text Node
			\draw (304.78,185.9) node [anchor=north west][inner sep=0.75pt]    {$16$};
			% Text Node
			\draw (433.72,229.9) node [anchor=north west][inner sep=0.75pt]    {$17$};
			% Text Node
			\draw (391.08,229.9) node [anchor=north west][inner sep=0.75pt]    {$18$};
			% Text Node
			\draw (348.43,229.9) node [anchor=north west][inner sep=0.75pt]    {$19$};
			% Text Node
			\draw (304.78,229.9) node [anchor=north west][inner sep=0.75pt]    {$20$};
			% Text Node
			\draw (433.72,273.9) node [anchor=north west][inner sep=0.75pt]    {$21$};
			% Text Node
			\draw (391.08,273.9) node [anchor=north west][inner sep=0.75pt]    {$22$};
			% Text Node
			\draw (348.43,273.9) node [anchor=north west][inner sep=0.75pt]    {$23$};
			% Text Node
			\draw (304.78,273.9) node [anchor=north west][inner sep=0.75pt]    {$24$};
			% Text Node
			\draw (86,36) node [anchor=north west][inner sep=0.75pt]   [align=left] {Fertilizer $1$ : Blue\\Fertilizer $2$ : Red\\Fertilizer $3$ : Black\\No Fertilizer: White (control)};
			% Text Node
			\draw (361,13) node [anchor=north west][inner sep=0.75pt]   [align=left] {Wall};
			% Text Node
			\draw (473,52) node [anchor=north west][inner sep=0.75pt]   [align=left] {Block I};
			% Text Node
			\draw (473,97.8) node [anchor=north west][inner sep=0.75pt]   [align=left] {Block II};
			% Text Node
			\draw (473,142.6) node [anchor=north west][inner sep=0.75pt]   [align=left] {Block III};
			% Text Node
			\draw (473,186.4) node [anchor=north west][inner sep=0.75pt]   [align=left] {Block IV};
			% Text Node
			\draw (473,229.2) node [anchor=north west][inner sep=0.75pt]   [align=left] {Block V};
			% Text Node
			\draw (473,274) node [anchor=north west][inner sep=0.75pt]   [align=left] {Block VI};
			\end{tikzpicture}
		\end{figure}
		
		\item "\NewTerm{Balanced incomplete block design}\index{ANOVA!balanced incomplete block design}" (BIBD):
		
		In such a configuration, each block is selected in a balanced manner so that any pair of treatments occur together the same number of times as any other pairs.
		
		Considering the four treatments of our previous example, there are $\begin{pmatrix} 4\\2 \end{pmatrix}=6$ ways of choosing three out of four:
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
			%uncomment if require: \path (0,1467); %set diagram left start at 0, and has height of 1467
			
			%Shape: Rectangle [id:dp3323956907711456] 
			\draw  [fill={rgb, 255:red, 255; green, 247; blue, 217 }  ,fill opacity=1 ] (289.5,35) -- (468.5,35) -- (468.5,308) -- (289.5,308) -- cycle ;
			%Shape: Rectangle [id:dp5555128169009411] 
			\draw  [fill={rgb, 255:red, 248; green, 228; blue, 130 }  ,fill opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (294.01,261) -- (464.01,261) -- (464.01,301) -- (294.01,301) -- cycle ;
			%Shape: Rectangle [id:dp7266187583580583] 
			\draw  [fill={rgb, 255:red, 248; green, 228; blue, 130 }  ,fill opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (294.01,217) -- (464.01,217) -- (464.01,257) -- (294.01,257) -- cycle ;
			%Shape: Rectangle [id:dp5273305016629877] 
			\draw  [fill={rgb, 255:red, 248; green, 228; blue, 130 }  ,fill opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (294.01,173) -- (464.01,173) -- (464.01,213) -- (294.01,213) -- cycle ;
			%Shape: Rectangle [id:dp3869455965307047] 
			\draw  [fill={rgb, 255:red, 248; green, 228; blue, 130 }  ,fill opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (294.01,129) -- (464.01,129) -- (464.01,169) -- (294.01,169) -- cycle ;
			%Shape: Rectangle [id:dp1689464760599264] 
			\draw  [fill={rgb, 255:red, 248; green, 228; blue, 130 }  ,fill opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (294.01,85) -- (464.01,85) -- (464.01,125) -- (294.01,125) -- cycle ;
			%Shape: Rectangle [id:dp7881057123703761] 
			\draw  [fill={rgb, 255:red, 248; green, 228; blue, 130 }  ,fill opacity=1 ][dash pattern={on 4.5pt off 4.5pt}] (294.01,41) -- (464.01,41) -- (464.01,81) -- (294.01,81) -- cycle ;
			%Shape: Circle [id:dp8956193049184411] 
			\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (298,61) .. controls (298,51.61) and (305.61,44) .. (315,44) .. controls (324.39,44) and (332,51.61) .. (332,61) .. controls (332,70.39) and (324.39,78) .. (315,78) .. controls (305.61,78) and (298,70.39) .. (298,61) -- cycle ;
			%Shape: Circle [id:dp35263048255777596] 
			\draw  [fill={rgb, 255:red, 255; green, 146; blue, 158 }  ,fill opacity=1 ] (340.67,61) .. controls (340.67,51.61) and (348.28,44) .. (357.67,44) .. controls (367.06,44) and (374.67,51.61) .. (374.67,61) .. controls (374.67,70.39) and (367.06,78) .. (357.67,78) .. controls (348.28,78) and (340.67,70.39) .. (340.67,61) -- cycle ;
			%Shape: Circle [id:dp11817830977182942] 
			\draw  [fill={rgb, 255:red, 189; green, 218; blue, 255 }  ,fill opacity=1 ] (298,105) .. controls (298,95.61) and (305.61,88) .. (315,88) .. controls (324.39,88) and (332,95.61) .. (332,105) .. controls (332,114.39) and (324.39,122) .. (315,122) .. controls (305.61,122) and (298,114.39) .. (298,105) -- cycle ;
			%Shape: Circle [id:dp005098165549407785] 
			\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (426,105) .. controls (426,95.61) and (433.61,88) .. (443,88) .. controls (452.39,88) and (460,95.61) .. (460,105) .. controls (460,114.39) and (452.39,122) .. (443,122) .. controls (433.61,122) and (426,114.39) .. (426,105) -- cycle ;
			%Shape: Circle [id:dp31607205356315293] 
			\draw  [fill={rgb, 255:red, 255; green, 146; blue, 158 }  ,fill opacity=1 ] (298,149) .. controls (298,139.61) and (305.61,132) .. (315,132) .. controls (324.39,132) and (332,139.61) .. (332,149) .. controls (332,158.39) and (324.39,166) .. (315,166) .. controls (305.61,166) and (298,158.39) .. (298,149) -- cycle ;
			%Shape: Circle [id:dp8103569616283224] 
			\draw  [fill={rgb, 255:red, 189; green, 218; blue, 255 }  ,fill opacity=1 ] (383.34,149) .. controls (383.34,139.61) and (390.95,132) .. (400.34,132) .. controls (409.73,132) and (417.34,139.61) .. (417.34,149) .. controls (417.34,158.39) and (409.73,166) .. (400.34,166) .. controls (390.95,166) and (383.34,158.39) .. (383.34,149) -- cycle ;
			%Shape: Circle [id:dp03417872776536601] 
			\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (383.34,193) .. controls (383.34,183.61) and (390.95,176) .. (400.34,176) .. controls (409.73,176) and (417.34,183.61) .. (417.34,193) .. controls (417.34,202.39) and (409.73,210) .. (400.34,210) .. controls (390.95,210) and (383.34,202.39) .. (383.34,193) -- cycle ;
			%Shape: Circle [id:dp44437114019545865] 
			\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (426,193) .. controls (426,183.61) and (433.61,176) .. (443,176) .. controls (452.39,176) and (460,183.61) .. (460,193) .. controls (460,202.39) and (452.39,210) .. (443,210) .. controls (433.61,210) and (426,202.39) .. (426,193) -- cycle ;
			%Shape: Circle [id:dp5519960749143016] 
			\draw  [fill={rgb, 255:red, 189; green, 218; blue, 255 }  ,fill opacity=1 ] (340.67,237) .. controls (340.67,227.61) and (348.28,220) .. (357.67,220) .. controls (367.06,220) and (374.67,227.61) .. (374.67,237) .. controls (374.67,246.39) and (367.06,254) .. (357.67,254) .. controls (348.28,254) and (340.67,246.39) .. (340.67,237) -- cycle ;
			%Shape: Circle [id:dp7278139326826107] 
			\draw  [fill={rgb, 255:red, 255; green, 146; blue, 158 }  ,fill opacity=1 ] (383.34,237) .. controls (383.34,227.61) and (390.95,220) .. (400.34,220) .. controls (409.73,220) and (417.34,227.61) .. (417.34,237) .. controls (417.34,246.39) and (409.73,254) .. (400.34,254) .. controls (390.95,254) and (383.34,246.39) .. (383.34,237) -- cycle ;
			%Shape: Circle [id:dp8426456774715223] 
			\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=1 ] (340.67,281) .. controls (340.67,271.61) and (348.28,264) .. (357.67,264) .. controls (367.06,264) and (374.67,271.61) .. (374.67,281) .. controls (374.67,290.39) and (367.06,298) .. (357.67,298) .. controls (348.28,298) and (340.67,290.39) .. (340.67,281) -- cycle ;
			%Shape: Circle [id:dp8005137761157473] 
			\draw  [fill={rgb, 255:red, 189; green, 218; blue, 255 }  ,fill opacity=1 ] (426,281) .. controls (426,271.61) and (433.61,264) .. (443,264) .. controls (452.39,264) and (460,271.61) .. (460,281) .. controls (460,290.39) and (452.39,298) .. (443,298) .. controls (433.61,298) and (426,290.39) .. (426,281) -- cycle ;
			
			% Text Node
			\draw (353.43,53.9) node [anchor=north west][inner sep=0.75pt]    {$3$};
			% Text Node
			\draw (308.78,53.9) node [anchor=north west][inner sep=0.75pt]    {$4$};
			% Text Node
			\draw (437.72,97.9) node [anchor=north west][inner sep=0.75pt]    {$5$};
			% Text Node
			\draw (308.78,97.9) node [anchor=north west][inner sep=0.75pt]    {$8$};
			% Text Node
			\draw (390.08,141.9) node [anchor=north west][inner sep=0.75pt]    {$10$};
			% Text Node
			\draw (304.78,141.9) node [anchor=north west][inner sep=0.75pt]    {$12$};
			% Text Node
			\draw (433.72,185.9) node [anchor=north west][inner sep=0.75pt]    {$13$};
			% Text Node
			\draw (391.08,185.9) node [anchor=north west][inner sep=0.75pt]    {$14$};
			% Text Node
			\draw (391.08,229.9) node [anchor=north west][inner sep=0.75pt]    {$18$};
			% Text Node
			\draw (348.43,229.9) node [anchor=north west][inner sep=0.75pt]    {$19$};
			% Text Node
			\draw (433.72,273.9) node [anchor=north west][inner sep=0.75pt]    {$21$};
			% Text Node
			\draw (348.43,273.9) node [anchor=north west][inner sep=0.75pt]    {$23$};
			% Text Node
			\draw (85,36) node [anchor=north west][inner sep=0.75pt]   [align=left] {Fertilizer $\displaystyle 1$ : Blue\\Fertilizer $\displaystyle 2$ : Red\\Fertilizer $\displaystyle 3$ : Black\\No Fertilizer: White (control)};
			% Text Node
			\draw (361,13) node [anchor=north west][inner sep=0.75pt]   [align=left] {Wall};
			% Text Node
			\draw (473,52) node [anchor=north west][inner sep=0.75pt]   [align=left] {Block I};
			% Text Node
			\draw (473,98.8) node [anchor=north west][inner sep=0.75pt]   [align=left] {Block II};
			% Text Node
			\draw (473,143.6) node [anchor=north west][inner sep=0.75pt]   [align=left] {Block III};
			% Text Node
			\draw (473,186.4) node [anchor=north west][inner sep=0.75pt]   [align=left] {Block IV};
			% Text Node
			\draw (473,230.2) node [anchor=north west][inner sep=0.75pt]   [align=left] {Block V};
			% Text Node
			\draw (473,274) node [anchor=north west][inner sep=0.75pt]   [align=left] {Block VI};
			\end{tikzpicture}
		\end{figure}
	
		\item "\NewTerm{General randomized block design}\index{ANOVA!general randomized block design}" (GRBD):
		
		Like a randomized complete block design (RCBD), a GRBD is randomized. Within each block, treatments are randomly assigned to experimental units: this randomization is also independent between blocks. In a (classic) RCBD, however, there is no replication of treatments within blocks. This replication allows the estimation and testing of an interaction term in the associated linear model.
		
		\item And others like PBIB (partial balanced incomplete design), CPBIB (cyclic balanced incomplete design), GCPIB (general cyclic balanced incomplete design), etc.
	\end{itemize}
	
	\subparagraph{Interlude on Type I, Type II, Type III ANOVAs}\label{types 1 2 3 anovss}\mbox{}\\\\
	The purpose of this interlude is to clarify the meaning of the different types of sums of squares (and the underlying "philosophies") found in most statistical software in the context of variance (ANOVA). For this, we will place ourselves in ANOVA with two fixed factors.
	
	Indeed, when data is unbalanced, there are different ways to calculate the sums of squares for ANOVA. There are at least 3 approaches, commonly named Type I sums of squares, Type II sums of squares and type III sums of squares (also named "\NewTerm{Yates's weighted squares of means}"\index{Yates's weighted squares of means}). These names seem to have been introduced into the statistics world from the SAS package but are since decades pretty widespread. Which type to use has led to an ongoing controversy. However, it essentially comes down to testing different hypotheses about the data.
	
	To introduce these three types, let us consider a fixed two-factor analysis model of variance in which:
	\begin{itemize}
		\item The first factor that we will denote by $F_1$, has $J$ levels ($J\geq 2$) that will be indexed with the letter $j$
	
		\item The second factor that we will denote by $F_2$, has $K$ levels ($K\geq 2$) that will be indexed with the letter $k$
	
		\item At the intersection of the level $j$ of $F_1$ and the level $k$ of $F_2$, we realize $n_{jk}$ observations ($n_{jk}\geq 1$) of a random variable $y$ (and we assume this time that the experimental design is not necessarily balanced)
	
		\item Each observation is denoted $y_{ijk}$ (with $i=1\ldots n_{jk}$ and $j=1 \ldots J$ and $k=1 \ldots K$)
	
		\item We put:
		\begin{itemize}
			\item $n_{j+}=\sum_{k=1}^K n_{jk}$ as the marginal sample size of the level $j$ of $F_1$
	
			\item $n_{+k}=\sum_{j=1}^J n_{jk}$ as the marginal sample size of the level $k$ of $F_2$
	
			\item $n=\sum_{j=1}^J\sum_{k=1}^K n_{jk}$ as the total number of measurements
		\end{itemize}
	\end{itemize}
	Let us introduce the different empirical partial averages of observations $y_{ijk}$:
	
	Now let us consider the total sum of squares of the model, quantity with as we know $n-1$ degrees of freedom:
	
	We will first explicit again the decomposition of the quantity TSS. For this purpose, let us first notice the following equality:
	
	As:
	
	where DP means "double product (terms)". So with our problem we have:
	
	By triple summation, we get:
	
	To detail the double products $\text{DP}_l$, let us first recall that if the quantities $ x_ {jk}$ are reals independent of $ i $, we can write:
	
	But we also know that:
	
	Therefore:
	
	It follows that the sums of the first three double products (those in which the quantity $ (y_ {ijk} -y_ {jk}) $ is in factor) are zero (i.e. $2ab$, $2ac$, $2ad$). By cons, the other three (i.e. $2bc$, $2bd$, $2cd$) are, in general, not zero. The fourth ($2bc$) is written:
	
	double product that we will denote $\text{SDP}_1$. The sums of the last two double products ($2bd$ and $2cd$ hence $2d(b+c)$) will be grouped in the following expression:
	
	sum of double product that we will denote $\text{SDP}_2$.
	
	Let's now explicit the square sums by introducing the following quantities (we know the degrees of freedom thanks to our study of the two way fixed factor ANOVA for recall!):
	\begin{itemize}
		\item Sum of squares due to the factor $ F_1 $ (quantity at $ J-1 $ degrees of freedom):
		
		
		\item Sum of squares due to the factor $ F_2 $ (quantity at $ K-1 $ degrees of freedom):
		
		
		\item Sum of squares due to the interactions of the factors  (quantity at $(J-1)(K-1)$ degrees of freedom):
		
		
		\item Sum of squares due to errors (or residual, quantity at $ n- JK $ degrees of freedom):
		
	\end{itemize}
	We can finally rewrite the total sum of squares in the form:
	
	But most often written:
	
	Or explicitly:
	
	We already know, from our derivation of the sum of squares of the balanced two way fixed factor ANOVA that in the balanced case:
	
	The decomposition of SST is then of obvious interpretation. On the other hand, this is not the case with unbalanced plans ANOVA for which these two quantities are in general non-zero.
	
	When the quantities $\text{SDP}_A$ and $\text{SDP}_B$ are non-zero, it is not possible to assign them to a single source of variation ($F_A$, $F_B$ or $F_A \times F_B$). This explains the difficulties encountered in specifying the sources of variation in a model relating to an unbalanced plan. For this reason, other reasoning is used to specify these sources, which explains the existence of several types of sums of squares, according to the chosen philosophy.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Remember that obviously the three type of sum of squares that we will see now are always equal, and especially equal to zero as we already know!  
	\end{tcolorbox}
	Let us now represent the full model by $\text{SS}(A, B, AB)$, ie:
	
	 Other models are represented similarly: 
	 
	 indicates the model with no interaction. The next one:
	 
	 indicates the model that does not account for main effects from factor $A$. We also have:
	 
	 and so on.
	 
	 The influence of particular factors (including interactions) can be tested by examining the differences between models. For example, to determine the presence of an interaction effect, an $F$-test of the models $\text{SS}(A, B, AB)$ and the no-interaction model $\text{SS}(A, B)$ would be carried out. It is handy to define incremental sums of squares to represent these differences. Let:
	 
	The notation shows the incremental differences in sums of squares, for example $\text{SS}(AB| A, B)$ represents the sum of squares for interaction after the main effects has been removed, and $\text{SS}(A | B)$ is the sum of squares for the $A$ main effect after the $B$ main effect have been removed and ignoring interactions. The different types of sums of squares then arise depending on the stage of model reduction at which they are carried out. In particular:
	\begin{itemize}
		\item The "\NewTerm{Type I ANOVA}\index{ANOVA!Type I}" also named "\NewTerm{sequential sum of squares}" is computed using $\text{SS}(A)$ for factor $A$. $\text{SS}(B | A)$ for factor $B$. $\text{SS}(AB | B, A)$ for interaction $AB$. This tests the main effect of factor $A$, followed by the main effect of factor $B$ after the main effect of $A$, followed by the interaction effect $AB$ after the main effects. Because of the sequential nature and the fact that the two main factors are tested in a particular order, this type of sums of squares will give different results for unbalanced data depending on which main effect is considered first\footnote{However in the case of balanced designs, the result will be the same and we fall back on the classical two-way fixed factor ANOVA}. This is why type I sums of squares, in unbalanced designs, is reserved to models in which there is a natural order between the factors. Notice that for type I, the individual effect SS sum to the total effect SS as:
		
		
		\item The "\NewTerm{Type II ANOVA}\index{ANOVA!Type II}"is calculated using $\text{SS}(A | B)$ for factor $A$. $\text{SS}(B | A)$ for factor $B$. This type tests for each main effect after the other main effect. Note that no significant interaction is assumed (in other words, we should test for interaction first ($\text{SS}(AB | A, B)$) and only if $AB$ is not significant, we continue with the analysis for main effects). If there is indeed no interaction, then type II is statistically more powerful than type III (see below). Notice the important property that  type II is order independent at the opposite of the other types. Notice that for type II, it is obvious that the individual effect SS does not sum to the total effect SS at the opposite of Type I. Indeed:
		
		
		\item The "\NewTerm{Type III ANOVA}\index{ANOVA!Type III}" also named "\NewTerm{marginal sum of squares}" is calculated using $\text{SS}(A | B, AB)$ for factor $A$. $\text{SS}(B | A, AB)$ for factor $B$. This type tests for the presence of a main effect after the other main effect and interaction. This approach is therefore valid in the presence of significant interactions. However, it is often not interesting to interpret a main effect if interactions are present (generally speaking, if a significant interaction is present, the main effects should not be further analysed). If the interactions are not significant, type II gives a more powerful test. Notice also that as for type II, it is obvious that the individual effect SS does not sum to the total effect SS:
		
	\end{itemize}
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} When data is balanced, the factors are orthogonal, and types I, II and III all give the same results!\\
	
	\textbf{R2.} In the beginning of the 121st century (holocene calendar), the softwares SAS and SPSS use SS type III as their default, while functions that ship with base \texttt{R} use type I by default. This can lead to different results when analysing the same data with different statistics packages.
	\end{tcolorbox}
	In summary, the hypothesis of interest is usually  about the significance of one factor while controlling for the level of the other factors. This equates to using type II or III SS. In general, if there is no significant interaction effect, then type II is more powerful, and follows the principle of marginality. If interaction is present, then type II is inappropriate while type III can still be used, but results need to be interpreted with caution (in the presence of interactions, main effects are rarely interpretable).
	
	The subject is quite controversial since decades. This is why, when possible, it would be better to avoid unbalanced experiments!
	
	\paragraph{Hierarchical (Nested) ANOVA (HANOVA)}\label{hierarchical anova}\mbox{}\\\\
	The "\NewTerm{hierarchical fixed factor ANOVA}\index{ANOVA!hierarchical fixed factor ANOVA}" (HANOVA) or "\NewTerm{nested ANOVA with fixed factors}\index{ANOVA!nested ANOVA with fixed factors}", more often referred to as "\NewTerm{two stage nested design}\index{two stage nested design}", consists in analysing samples where one of the control factors is not independent but dependent on another control factor (this is why often beginners sometimes hesitate somewhat between a two-factor ANOVA and a two-factor hierarchical ANOVA).

	Consider to introduce the concept the historical case of hierarchical ANOVA (agriculture!), two completely independent surfaces: Study Surface $A$ and Study Surface $B$. Each of these surfaces (named a "\NewTerm{whole plots}\index{whole plot}" or "\NewTerm{block effect}\index{block effect}") is separated into $4$ distinct study squares (named "\NewTerm{split-plots}\index{split-plots}" or "\NewTerm{subplots}\index{subplots}" or "\NewTerm{sub-factors}\index{sub-factors}") with $5$ replicas for each square. So we have a total of $2$ whole plot, with $8$ split-plots (test surfaces) for a total of $40$ individuals\footnote{In order to better understand the structure of the simple split-plot design it is advantageous to view it as superimposing one RCBD (for the split-plot treatments) on top of another RCBD (for the whole treatments). We shall refer to this as SPD (RCBD,RCBD). Variations of this form of split-plot designs are possible by using other than both RCBD.}:
	\begin{table}[H]
		\centering
		\begin{tabular}{cccclcccc}
		\multicolumn{4}{c}{\textbf{Study Surface $A$}} &  & \multicolumn{4}{c}{\textbf{Study Surface $B$}} \\ \cline{1-4} \cline{6-9} 
		\multicolumn{1}{|c|}{\cellcolor[gray]{0.75}$\pmb{S_1(A)}$} & \multicolumn{1}{c|}{\cellcolor[gray]{0.75}$\pmb{S_2(A)}$} & \multicolumn{1}{c|}{\cellcolor[gray]{0.75}$\pmb{S_3(A)}$} & \multicolumn{1}{c|}{\cellcolor[gray]{0.75}$\pmb{S_4(A)}$} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{S_1(B)}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{S_2(B)}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{S_3(B)}$} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}$\pmb{S_4(B)}$} \\ \cline{1-4} \cline{6-9} 
		\multicolumn{1}{|c|}{X} & \multicolumn{1}{c|}{X} & \multicolumn{1}{c|}{X} & \multicolumn{1}{c|}{X} & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{X} & \multicolumn{1}{c|}{X} & \multicolumn{1}{c|}{X} & \multicolumn{1}{c|}{X} \\ \cline{1-4} \cline{6-9} 
		\multicolumn{1}{|c|}{X} & \multicolumn{1}{c|}{X} & \multicolumn{1}{c|}{X} & \multicolumn{1}{c|}{X} & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{X} & \multicolumn{1}{c|}{X} & \multicolumn{1}{c|}{X} & \multicolumn{1}{c|}{X} \\ \cline{1-4} \cline{6-9} 
		\multicolumn{1}{|c|}{X} & \multicolumn{1}{c|}{X} & \multicolumn{1}{c|}{X} & \multicolumn{1}{c|}{X} & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{X} & \multicolumn{1}{c|}{X} & \multicolumn{1}{c|}{X} & \multicolumn{1}{c|}{X} \\ \cline{1-4} \cline{6-9} 
		\multicolumn{1}{|c|}{X} & \multicolumn{1}{c|}{X} & \multicolumn{1}{c|}{X} & \multicolumn{1}{c|}{X} & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{X} & \multicolumn{1}{c|}{X} & \multicolumn{1}{c|}{X} & \multicolumn{1}{c|}{X} \\ \cline{1-4} \cline{6-9} 
		\multicolumn{1}{|c|}{X} & \multicolumn{1}{c|}{X} & \multicolumn{1}{c|}{X} & \multicolumn{1}{c|}{X} & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{X} & \multicolumn{1}{c|}{X} & \multicolumn{1}{c|}{X} & \multicolumn{1}{c|}{X} \\ \cline{1-4} \cline{6-9} 
		\end{tabular}
		\caption[]{Hierarchical ANOVA illustration}
	\end{table}
	We are therefore in the particular situation where the effects of the levels of the factor of the second level (squares) have no concrete meaning. For example, these squares depend on the level of the \textit{Surface} factor considered and a study of the main effects of the \textit{Square} factor is here irrelevant!
	
	We can consider, for example, that the wholeplot factor represents suppliers, the subplot factor object lot types, and therefore for each supplier a given level of the subplot will physically represent a specific lot for each supplier and therefore not interchangeable.
	
	Or a more common example are the "\NewTerm{nested R\&R (Repeatability and Reproducibility) Gage studies}\index{nested R\&R gage studies}". Indeed, "\NewTerm{measurement systems analysis}\index{measurement systems analysis}" (MSA) are essential to the success of any data analysis. If you cannot rely on the tool you're using to take measurements, then why bother collecting data to begin with? It would be like trying to lose weight while relying on a scale that doesn't work. What's the point in weighing yourself?
	
	In MSA studies for continuous measurements (e.g. weight, length, volume) using non-destructive testing, each part can be measured repeatedly. In this case, we can use "\NewTerm{crossed Gage studies}\index{crossed Gage study}". However, sometimes we must conduct a MSA where the test required to take the measurement destroys the object or physically changes the characteristic that is being measured.
	
	Suppose again that we are conducting a Gage R\&R study for a destructive test with $3$ operators and $2$ replicates per part. But suppose it is not feasible to obtain $6$ specimens that are similar enough to be considered the same part, rather only $2$ specimens. Then in this case, you must use a "\NewTerm{nested Gage study}\index{nested gage study}".
	
	When a nested Gage R\&R study is not enough, try an "\NewTerm{expanded Gage R\&R}\index{expanded Gage R\&R}". Most Gage R\&R studies assess the effects of two factors on variation in your measurement system - typically operator and part. However, the effects of operator and part frequently are not enough to provide a complete understanding of the measurement system. Adding a third variable (typically gage) to the standard study is often required. When three or more factors are included in the analysis, we call the study an "expanded Gage R\&R". In the following situations, a third factor is crucial to understanding the system.
	
	\begin{figure}[H]
		\centering
		\resizebox{\textwidth}{!}{
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,877); %set diagram left start at 0, and has height of 877
		
		%Straight Lines [id:da020436553880937236] 
		\draw    (464,59) -- (464,81.2) ;
		%Straight Lines [id:da5450598617477063] 
		\draw    (434,81.2) -- (537.5,81.2) ;
		%Straight Lines [id:da17398008800994735] 
		\draw    (537.5,81.2) -- (537.5,103.4) ;
		%Straight Lines [id:da8282879054731622] 
		\draw    (540,156) -- (540,178.2) ;
		%Straight Lines [id:da2340460793431276] 
		\draw    (540,178.2) -- (619.5,178.2) ;
		%Straight Lines [id:da45213480124892036] 
		\draw    (619.5,178.2) -- (619.5,200.4) ;
		%Straight Lines [id:da8835002824289246] 
		\draw    (478.5,178.2) -- (540,178.2) ;
		%Straight Lines [id:da7625943251504128] 
		\draw    (478.5,178.2) -- (478.5,200.4) ;
		%Straight Lines [id:da8779255784745319] 
		\draw    (318.5,81.2) -- (434,81.2) ;
		%Straight Lines [id:da6716286276862626] 
		\draw    (318.5,81.2) -- (318.5,103.4) ;
		%Straight Lines [id:da5943533119047832] 
		\draw    (318,143) -- (318,165.2) ;
		%Straight Lines [id:da2466745546448721] 
		\draw    (318,165.2) -- (417.5,165.2) ;
		%Straight Lines [id:da9212378621304305] 
		\draw    (417.5,165.2) -- (417.5,244.4) ;
		%Straight Lines [id:da42865869090900244] 
		\draw    (197.5,165.2) -- (318,165.2) ;
		%Straight Lines [id:da858736916322872] 
		\draw    (197.5,165.2) -- (197.5,244) ;
		%Straight Lines [id:da5906592562989292] 
		\draw    (417,295) -- (417,317.2) ;
		%Straight Lines [id:da9629500954525572] 
		\draw    (417,317.2) -- (496.5,317.2) ;
		%Straight Lines [id:da3299465429768478] 
		\draw    (496.5,317.2) -- (496.5,339.4) ;
		%Straight Lines [id:da8732165682591404] 
		\draw    (337.5,317.2) -- (417,317.2) ;
		%Straight Lines [id:da5925407541453422] 
		\draw    (337.5,317.2) -- (337.5,339.4) ;
		%Straight Lines [id:da14408074141950955] 
		\draw    (197,297) -- (197,319.2) ;
		%Straight Lines [id:da7433434596787725] 
		\draw    (197,319.2) -- (276.5,319.2) ;
		%Straight Lines [id:da8715573873916433] 
		\draw    (276.5,319.2) -- (276.5,383.8) ;
		%Straight Lines [id:da71172295110394] 
		\draw    (104.5,319.2) -- (197,319.2) ;
		%Straight Lines [id:da07722633839913873] 
		\draw    (104.5,319.2) -- (104.5,384.4) ;
		%Straight Lines [id:da05347527969968313] 
		\draw    (277,436) -- (277,458.2) ;
		%Straight Lines [id:da5567947097763735] 
		\draw    (324.5,458.2) -- (324.5,480.4) ;
		%Straight Lines [id:da6498986656782308] 
		\draw    (229.5,458.2) -- (277,458.2) ;
		%Straight Lines [id:da7536048778455378] 
		\draw    (229.5,458.2) -- (229.5,480.4) ;
		%Straight Lines [id:da3172989800607169] 
		\draw    (277,458.2) -- (324.5,458.2) ;
		%Straight Lines [id:da40414044297177343] 
		\draw    (105,436) -- (105,458.2) ;
		%Straight Lines [id:da29469196214136906] 
		\draw    (152.5,458.2) -- (152.5,480.4) ;
		%Straight Lines [id:da26366548961019] 
		\draw    (57.5,458.2) -- (105,458.2) ;
		%Straight Lines [id:da0927089600385802] 
		\draw    (57.5,458.2) -- (57.5,480.4) ;
		%Straight Lines [id:da37947526102870843] 
		\draw    (105,458.2) -- (152.5,458.2) ;
		
		% Text Node
		\draw  [fill={rgb, 255:red, 174; green, 211; blue, 255 }  ,fill opacity=1 ]  (395,25) -- (532,25) -- (532,59) -- (395,59) -- cycle  ;
		\draw (398,26) node [anchor=north west][inner sep=0.6pt]   [align=left] {\begin{minipage}[lt]{97pt}\setlength\topsep{0pt}
		\begin{center}
		{\footnotesize Are the parts measured}\\{\footnotesize using a destructive test?}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (490,72) -- (513,72) -- (513,92) -- (490,92) -- cycle  ;
		\draw (493,76) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {No};
		% Text Node
		\draw  [fill={rgb, 255:red, 226; green, 255; blue, 240 }  ,fill opacity=1 ]  (464,99) -- (631,99) -- (631,163) -- (465,163) -- cycle  ;
		\draw (463,100) node [anchor=north west][inner sep=0.5pt]  [font=\footnotesize] [align=left] {\begin{minipage}[lt]{125pt}\setlength\topsep{0pt}
		\begin{center}
		{\footnotesize Do you have additional factors }\\{\footnotesize beyond just operator}\\{\footnotesize part, and one piece of}\\{\footnotesize measurement equipement?}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (571,170) -- (594,170) -- (594,189) -- (571,189) -- cycle  ;
		\draw (574,173) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {No};
		% Text Node
		\draw  [fill={rgb, 255:red, 255; green, 251; blue, 220 }  ,fill opacity=1 ]  (581,200) -- (649,200) -- (649,227) -- (581,227) -- cycle  ;
		\draw (584,201) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {\begin{minipage}[lt]{43.98pt}\setlength\topsep{0pt}
		\begin{center}
		Use Crossed\\Gage R\&R
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw  [fill={rgb, 255:red, 242; green, 182; blue, 255 }  ,fill opacity=1 ]  (440,200) -- (516,200) -- (516,227) -- (440,227) -- cycle  ;
		\draw (443,201) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {\begin{minipage}[lt]{49.55pt}\setlength\topsep{0pt}
		\begin{center}
		Use Expanded\\Gage R\&R
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (498,170) -- (525,170) -- (525,189) -- (498,189) -- cycle  ;
		\draw (501,173) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {Yes};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (351,72) -- (378,72) -- (378,92) -- (351,92) -- cycle  ;
		\draw (354,76) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {Yes};
		% Text Node
		\draw  [fill={rgb, 255:red, 226; green, 255; blue, 240 }  ,fill opacity=1 ]  (246,103) -- (396,103) -- (396,153) -- (246,153) -- cycle  ;
		\draw (233,105) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {\begin{minipage}[lt]{130pt}\setlength\topsep{0pt}
		\begin{center}
		Can you identify individual\\batches where parts are\\essentially identical?
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (358,156) -- (381,156) -- (381,176) -- (358,176) -- cycle  ;
		\draw (361,160) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {No};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (236,156) -- (263,156) -- (263,176) -- (236,176) -- cycle  ;
		\draw (239,160) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {Yes};
		% Text Node
		\draw  [fill={rgb, 255:red, 226; green, 255; blue, 240 }  ,fill opacity=1 ]  (343,234) -- (495,234) -- (495,299) -- (343,299) -- cycle  ;
		\draw (338,235) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {\begin{minipage}[lt]{120pt}\setlength\topsep{0pt}
		\begin{center}
		{\footnotesize Do you have additional}\\{\footnotesize factors beyond just operator}\\{\footnotesize part, and one piece of}\\{\footnotesize measurement equipement?}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (448,308) -- (471,308) -- (471,328) -- (448,328) -- cycle  ;
		\draw (451,312) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {No};
		% Text Node
		\draw  [fill={rgb, 255:red, 222; green, 175; blue, 99 }  ,fill opacity=1 ]  (458,339) -- (520,339) -- (520,367) -- (458,367) -- cycle  ;
		\draw (461,341) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {\begin{minipage}[lt]{40.01pt}\setlength\topsep{0pt}
		\begin{center}
		Use Nested\\Gage R\&R
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw  [fill={rgb, 255:red, 242; green, 182; blue, 255 }  ,fill opacity=1 ]  (299,339) -- (375,339) -- (375,367) -- (299,367) -- cycle  ;
		\draw (302,341) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {\begin{minipage}[lt]{49.55pt}\setlength\topsep{0pt}
		\begin{center}
		Use Expanded\\Gage R\&R
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (361,308) -- (388,308) -- (388,328) -- (361,328) -- cycle  ;
		\draw (364,312) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {Yes};
		% Text Node
		\draw  [fill={rgb, 255:red, 226; green, 255; blue, 240 }  ,fill opacity=1 ]  (133,233) -- (263,233) -- (263,299) -- (133,299) -- cycle  ;
		\draw (130,235) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {\begin{minipage}[lt]{100pt}\setlength\topsep{0pt}
		\begin{center}
		Are the batches large\\enough so that each\\operator can test 2-3\\parts from each batch?
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (227,309) -- (250,309) -- (250,328) -- (227,328) -- cycle  ;
		\draw (230,312) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {No};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (135,309) -- (162,309) -- (162,328) -- (135,328) -- cycle  ;
		\draw (138,312) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {Yes};
		% Text Node
		\draw  [fill={rgb, 255:red, 226; green, 255; blue, 240 }  ,fill opacity=1 ]  (203,384) -- (357,384) -- (357,447) -- (203,447) -- cycle  ;
		\draw (202,384) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {\begin{minipage}[lt]{115pt}\setlength\topsep{0pt}
		\begin{center}
		{\footnotesize Do you have additional}\\{\footnotesize factors beyond just operator}\\{\footnotesize part, and one piece of}\\{\footnotesize measurement equipement?}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw  [fill={rgb, 255:red, 226; green, 255; blue, 240 }  ,fill opacity=1 ]  (29,384) -- (183,384) -- (183,447) -- (29,447) -- cycle  ;
		\draw (28,384) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {\begin{minipage}[lt]{115pt}\setlength\topsep{0pt}
		\begin{center}
		{\footnotesize Do you have additional}\\{\footnotesize factors beyond just operator}\\{\footnotesize part, and one piece of}\\{\footnotesize measurement equipement?}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (294,449) -- (317,449) -- (317,469) -- (294,469) -- cycle  ;
		\draw (297,453) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {No};
		% Text Node
		\draw  [fill={rgb, 255:red, 222; green, 175; blue, 99 }  ,fill opacity=1 ]  (294,480) -- (356,480) -- (356,506) -- (294,506) -- cycle  ;
		\draw (297,481) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {\begin{minipage}[lt]{40.01pt}\setlength\topsep{0pt}
		\begin{center}
		Use Nested\\Gage R\&R
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw  [fill={rgb, 255:red, 242; green, 182; blue, 255 }  ,fill opacity=1 ]  (191,480) -- (267,480) -- (267,506) -- (191,506) -- cycle  ;
		\draw (194,481) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {\begin{minipage}[lt]{49.55pt}\setlength\topsep{0pt}
		\begin{center}
		Use Expanded\\Gage R\&R
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (238,449) -- (265,449) -- (265,469) -- (238,469) -- cycle  ;
		\draw (241,453) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {Yes};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (122,449) -- (145,449) -- (145,469) -- (122,469) -- cycle  ;
		\draw (125,453) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {No};
		% Text Node
		\draw  [fill={rgb, 255:red, 242; green, 182; blue, 255 }  ,fill opacity=1 ]  (19,480) -- (95,480) -- (95,506) -- (19,506) -- cycle  ;
		\draw (22,481) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {\begin{minipage}[lt]{49.55pt}\setlength\topsep{0pt}
		\begin{center}
		Use Expanded\\Gage R\&R
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw  [draw opacity=0][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ]  (66,449) -- (93,449) -- (93,469) -- (66,469) -- cycle  ;
		\draw (69,453) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {Yes};
		% Text Node
		\draw  [fill={rgb, 255:red, 255; green, 251; blue, 220 }  ,fill opacity=1 ]  (119,480) -- (187,480) -- (187,506) -- (119,506) -- cycle  ;
		\draw (122,481) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {\begin{minipage}[lt]{43.98pt}\setlength\topsep{0pt}
		\begin{center}
		Use Crossed\\Gage R\&R
		\end{center}
		
		\end{minipage}};
		
		\end{tikzpicture}}
		\vspace*{3mm}
		\caption{How to select a Gage R\&R design}
	\end{figure}
	
	So keep in mind:
	\begin{itemize}
		\item Crossed R\&R studies are mostly related to non-destructive measurements (as explained above)

		\item Nested R\&R studies are mostly related to destructive measurements (as explained above)
	\end{itemize}
	
	Each operator measures a different set of parts. Therefore, each part is said to be "nested" within operator rather than "crossed" since each part is unique to an operator. Let's look at a diagram for another view of crossed versus nested studies:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,387); %set diagram left start at 0, and has height of 387
		
		%Shape: Rectangle [id:dp13843717129999744] 
		\draw  [line width=1.5]  (53,33) -- (336.5,33) -- (336.5,230.67) -- (53,230.67) -- cycle ;
		%Shape: Rectangle [id:dp5375343368367567] 
		\draw  [line width=1.5]  (336.5,33) -- (620,33) -- (620,230.67) -- (336.5,230.67) -- cycle ;
		%Straight Lines [id:da28393564364317814] 
		\draw    (123.67,106.83) -- (85.67,157.83) ;
		%Straight Lines [id:da7159889915779809] 
		\draw    (123.67,106.83) -- (126.67,157.83) ;
		%Straight Lines [id:da5445957742012615] 
		\draw    (123.67,106.83) -- (166.67,157.83) ;
		%Straight Lines [id:da6682382603953108] 
		\draw    (123.67,106.83) -- (207.67,157.83) ;
		%Straight Lines [id:da23716867967961885] 
		\draw    (123.67,106.83) -- (248.67,157.83) ;
		%Straight Lines [id:da7308583193177576] 
		\draw    (123.67,106.83) -- (289.67,157.83) ;
		%Straight Lines [id:da046977388686542554] 
		\draw    (256.67,106.83) -- (85.67,157.83) ;
		%Straight Lines [id:da9782361174761496] 
		\draw    (256.67,106.83) -- (126.67,157.83) ;
		%Straight Lines [id:da5773455208301299] 
		\draw    (256.67,106.83) -- (166.67,157.83) ;
		%Straight Lines [id:da669043485672165] 
		\draw    (408.67,106.83) -- (370.67,157.83) ;
		%Straight Lines [id:da17490077589523345] 
		\draw    (408.67,106.83) -- (413.67,156.83) ;
		%Straight Lines [id:da36582774194269274] 
		\draw    (408.67,106.83) -- (453.67,158.83) ;
		%Straight Lines [id:da18073032631593677] 
		\draw    (541.67,106.83) -- (494.67,157.83) ;
		%Straight Lines [id:da7194651946961685] 
		\draw    (541.67,106.83) -- (534.67,157.83) ;
		%Straight Lines [id:da2602575515475376] 
		\draw    (541.67,106.83) -- (576.67,157.83) ;
		%Straight Lines [id:da2188314703683898] 
		\draw    (256.67,106.83) -- (207.67,157.83) ;
		%Straight Lines [id:da2254745754351164] 
		\draw    (256.67,106.83) -- (248.67,157.83) ;
		%Straight Lines [id:da6705380473184814] 
		\draw    (256.67,106.83) -- (289.67,157.83) ;
		
		% Text Node
		\draw (128,42) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{Crossed Example}};
		% Text Node
		\draw (421,41) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{Nested Example}};
		% Text Node
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ]  (98,82) -- (148,82) -- (148,107) -- (98,107) -- cycle  ;
		\draw (101,86) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,opacity=1 ] [align=left] {Oper1};
		% Text Node
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ]  (230,82) -- (280,82) -- (280,107) -- (230,107) -- cycle  ;
		\draw (233,86) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,opacity=1 ] [align=left] {Oper2};
		% Text Node
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ]  (384,82) -- (434,82) -- (434,107) -- (384,107) -- cycle  ;
		\draw (387,86) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,opacity=1 ] [align=left] {Oper1};
		% Text Node
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ]  (516,82) -- (566,82) -- (566,107) -- (516,107) -- cycle  ;
		\draw (519,86) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,opacity=1 ] [align=left] {Oper2};
		% Text Node
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ]  (85, 174.5) circle [x radius= 16.62, y radius= 16.62]   ;
		\draw (75,166) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,opacity=1 ] [align=left] {P1};
		% Text Node
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ]  (125.8, 174.5) circle [x radius= 16.62, y radius= 16.62]   ;
		\draw (115.8,166) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,opacity=1 ] [align=left] {P2};
		% Text Node
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ]  (166.6, 174.5) circle [x radius= 16.62, y radius= 16.62]   ;
		\draw (156.6,166) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,opacity=1 ] [align=left] {P3};
		% Text Node
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ]  (207.4, 174.5) circle [x radius= 16.62, y radius= 16.62]   ;
		\draw (197.4,166) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,opacity=1 ] [align=left] {P4};
		% Text Node
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ]  (248.2, 174.5) circle [x radius= 16.62, y radius= 16.62]   ;
		\draw (238.2,166) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,opacity=1 ] [align=left] {P5};
		% Text Node
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ]  (289, 174.5) circle [x radius= 16.62, y radius= 16.62]   ;
		\draw (279,166) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,opacity=1 ] [align=left] {P6};
		% Text Node
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ]  (372, 174.5) circle [x radius= 16.62, y radius= 16.62]   ;
		\draw (362,166) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,opacity=1 ] [align=left] {P1};
		% Text Node
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ]  (412.8, 174.5) circle [x radius= 16.62, y radius= 16.62]   ;
		\draw (402.8,166) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,opacity=1 ] [align=left] {P2};
		% Text Node
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ]  (453.6, 174.5) circle [x radius= 16.62, y radius= 16.62]   ;
		\draw (443.6,166) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,opacity=1 ] [align=left] {P3};
		% Text Node
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ]  (494.4, 174.5) circle [x radius= 16.62, y radius= 16.62]   ;
		\draw (484.4,166) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,opacity=1 ] [align=left] {P4};
		% Text Node
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ]  (535.2, 174.5) circle [x radius= 16.62, y radius= 16.62]   ;
		\draw (525.2,166) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,opacity=1 ] [align=left] {P5};
		% Text Node
		\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ]  (576, 174.5) circle [x radius= 16.62, y radius= 16.62]   ;
		\draw (566,166) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,opacity=1 ] [align=left] {P6};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Cross vs Nested studies (source: Minitab)}
	\end{figure}
	The crossed Gage study is associated with a normal fixed factor ANOVA, or a Latin square ANOVA if the measurements are not repeated (see further below page \pageref{latin Square ANOVA}) where the nested Gage study is associated with a nested ANOVA!
	
	It is customary to say that we have a "\NewTerm{two-factor split-plot}\index{two-factor split-plot}" or a "\NewTerm{two-way hierarchical ANOVA}\index{two-way hierarchical ANOVA}" experiment because the behaviour of the squares is not independent of that of the surfaces.
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} To easily differentiate a fixed two-factor ANOVA with a nested fixed two-factor ANOVA with, it is sufficient to note that physically a given level of the first fixed factor represents the same object for each of the levels of the second factor while for the hierarchical ANOVA it is each time a different object specific to the factor under which it is.\\
	
	\textbf{R2.} We have given above two examples with only one nested level, but it is possible to develop the theory for multiple nesting levels (a level, in a level that is itself in a level, and so on). We are talk then about "\NewTerm{multiple hierarchical ANOVA}\index{multiple hierarchical ANOVA}".
	\end{tcolorbox}
	A hierarchical experience and the corresponding analysis must first be able to tell us if there is a significant difference between the "whole plots" (controlled factors) and also if there is a significant difference between the "subplots".
	
	It is important to remember that in this type of structure as each level of a factor is present only once in the level of another factor we can not estimate the interactions between the two. Indeed, if we consider that the factor whole-plot is a country and the subplot a lake ... it is difficult then to move the lake in another country ... In taking our example above with the surfaces it is also not very trivial to move a square to another surface...
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	We can not use a model where factors are nested unless we have repetitions. In the opposite case where the tests would not be repeated, the effect due to the nested factor can not be studied and the model that we will have to use to analyse the data will be one of those exposed in our study of the analysis of the one-way variance.
	\end{tcolorbox}
	In the special case where the statistical study of the hierarchical ANOVA has failed us to highlight a significant difference between the "subplots" of the two "whole plots", but on the other hand has successfully highlight a significant difference between the two "whole plots" this should lead us to conclude to a particular environmental influence.
	
	Denoting by $i=1\ldots M$ the number of levels of the main factor $A$, $j=i\ldots m$ the number of levels of the nested factor $B$ and $k=1\ldots n$ the number of repeated measurements (thus a total of measurements $M\cdot m\cdot n$) and the averages:
	
	and under the same assumptions of use as the one-way or two-way ANOVA seen earlier above (independence of the errors $\varepsilon_{ijk}$, the errors have the same variance, the errors follow a Normal law), we write the decomposition of the variance in the form:
	
	But, we have:
	
	and:
	
	So it remains in the end:
	
	where the summation indices are not traditionally the same as the two-factor ANOVA with repetition (but this is a detail). If we compare with the variance decomposition of the two-factor ANOVA with repetition that was for recall:
	
	We notice that the main difference lies in the fact we are not interested in the variance of the interaction of the two factors (which was the goal or say rather a... constraint!) and that we then did not have to make it appear.
	
	Now come the quest of determining the degrees of freedom. For $Q_T$ it's $\text{df}_T=Mmn-1$. For the term $Q_A$ (principal factor), the number of degrees of freedom is the same as for the one-way ANOVA or the two-factor ANOVA with/without repetitions. That is $\text{df}_A=M-1$ degrees of freedom. For the residual term it also comes immediately that it is the same thing as the two-factor ANOVA with repetition and therefore that $\text{df}_R=Mm(n-1)=N-Mm$. Finally, the remaining term $Q_{(A)B}$ is new, but we can easily determine its number of degrees of freedom since we must have:
	
	Then it comes:
	
	Hence:
	
	We then have the following table for the hierarchical ANOVA (according to the most common representation in statistical softwares) that we will comment after:
	\begin{table}[H]
		\centering
		\resizebox{\textwidth}{!}{\begin{tabular}{lcccc}
		\hline 
		\rowcolor[gray]{0.75}\textbf{Sum of squares}	& \textbf{$\chi^2$ df} & \textbf{Mean squares} & $F$  & \textbf{Critical $F$} \\ 
			\hline 
		\parbox{5cm}{$Q_A=mn\displaystyle\sum_{i=1}^{M}\sum_{ijk}(\bar{x}_{i..}-\bar{x}_{\dots})^2$\\ (due to the surface/bloc)} & $M-1$ & $\text{MSk}A= \dfrac{Q_A}{M-1}$ &$\dfrac{\text{MSk}A}{\text{MSk}(A)B}$  & $P(F>F_{M-1,Mm(n-1)})$ \\
		\parbox{6cm}{$Q_{(A)B}=n\displaystyle\sum_{i=1}^{M}\displaystyle\sum_{j=1}^{m}(\bar{x}_{ij.}-\bar{x}_{i..})^2$\\ (due to the plot in the whole plot)} &$M(m-1)$  &$\text{MSk}(A)B= \dfrac{Q_{(A)B}}{M(m-1)}$  &  $\dfrac{\text{MSk}(A)B}{\text{MSE}}$ & $P(F>F_{M(m-1),Mm(n-1)})$  \\
		\parbox{6cm}{$Q_R=\displaystyle\sum_{i=1}^{M}\displaystyle\sum_{j=1}^{m}\sum_{k=1}^n(x_{ijk}-\bar{x}_{ij.})^2$\\ (residuals)}	& $Mm(n-1)$ & $\text{MSE}=\dfrac{Q_R}{Mm(n-1)}$  &  &  \\
		\parbox{5cm}{$Q_T=\displaystyle\sum_{ijk}(x_{ijk}-\bar{x}_{\dots})^2$\\ (total)}	& $Mmn-1$ & $\text{MST}=\dfrac{Q_T}{Mmn-1}$ &  &  \\ 
			\hline 
		\end{tabular}}
		\caption{Hierarchical (random) factor ANOVA table}
	\end{table}
	So we have two Fisher tests just like the two-factor canonical ANOVA which is relatively intuitive. On the other hand what often surprises the practitioners is the first Fisher test where we have:
	
	Instead of what we could have written a little bit too quick:
	
	So what must be understood here is that there is not one that is right and the other wrong!!!!!!!!! It is necessary to know what we are interested in to make the best choice. Thus, comparing the variance of the blocks with the residual variance of the treatments is not necessarily the most interesting for judging the equality of the average treatments between them. This is why it is better to compare the variance of the blocks with the variance of the treatments.
	
	Therefore in most statistical softwares the table above is say to be the "ANOVA table for random factors" and the table below is the version for fixed factors (in case you really need to compare the variance of the blocks with the residuals variance of the treatments!):
	\begin{table}[H]
		\centering
		\resizebox{\textwidth}{!}{\begin{tabular}{lcccc}
		\hline 
		\rowcolor[gray]{0.75}\textbf{Sum of squares}	& \textbf{$\chi^2$ df} & \textbf{Mean squares} & $F$  & \textbf{Critical $F$} \\ 
			\hline 
		\parbox{5cm}{$Q_A=mn\displaystyle\sum_{i=1}^{M}\sum_{ijk}(\bar{x}_{i..}-\bar{x}_{\dots})^2$\\ (due to the surface/block)} & $M-1$ & $\text{MSk}A= \dfrac{Q_A}{M-1}$ &$\dfrac{\text{MSk}A}{\text{MSE}}$  & $P(F>F_{M-1,Mm(n-1)})$ \\
		\parbox{6cm}{$Q_{(A)B}=n\displaystyle\sum_{i=1}^{M}\displaystyle\sum_{j=1}^{m}(\bar{x}_{ij.}-\bar{x}_{i..})^2$\\ (due to the plot in the whole plot)} &$M(m-1)$  &$\text{MSk}(A)B= \dfrac{Q_{(A)B}}{M(m-1)}$  &  $\dfrac{\text{MSk}(A)B}{\text{MSE}}$ & $P(F>F_{M(m-1),Mm(n-1)})$  \\
		\parbox{6cm}{$Q_R=\displaystyle\sum_{i=1}^{M}\displaystyle\sum_{j=1}^{m}\sum_{k=1}^n(\bar{x}_{ijk}-\bar{x}_{ij.})^2$\\ (residuals)}	& $Mm(n-1)$ & $\text{MSE}=\dfrac{Q_R}{Mm(n-1)}$  &  &  \\
		\parbox{5cm}{$Q_T=\displaystyle\sum_{ijk}(x_{ijk}-\bar{x}_{\dots})^2$\\ (total)}	& $Mmn-1$ & $\text{MST}=\dfrac{Q_T}{Mmn-1}$ &  &  \\ 
			\hline 
		\end{tabular}}
		\caption{Hierarchical (fixed) factor ANOVA table}
	\end{table}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Let us recall that for an ANOVA with fixed factors without replications we had:
	
	with $i=1\ldots a$, $j=1\ldots b$ where $\mu$ is the overall mean, $\tau_i$ is the effect of the $i$th of treatment $A$, $\beta_j$ is the effect of the $j$th level of treatment $B$ (main effects) and $\varepsilon_{ij}$ is the habitual random error following $\mathcal{N}(0,\sigma_\varepsilon^2)$.\\
	
	For nested ANOVA, the $\beta_j$ will be expanded $\alpha_j+\gamma_{ij}$ and the new model may contain an interaction term representing possible interaction between blocking factor and the treatments. Thus the factor effect model may be written:
	
	\end{tcolorbox}
	Now before we continue with some other major ANOVA type (especially "ANOVA with blocks" and "split-plot ANOVA") let us give in the form of a figure what we have seen so far and what we will see just after:
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/arithmetics/anova_main_types.jpg}
		\caption{Comparing different some major ANOVA structures}
	\end{figure}
	\begin{itemize}
		\item[$\pmb{(a)}$] A crossed design examines every combination of levels for each fixed factor
	
		\item[$\pmb{(b)}$] A nested design can progressively subreplicate a fixed factor with nested levels of a random factor that are unique to the level within which they are nested
	
		\item[$\pmb{(c)}$] If a random factor can be reused for different levels of the treatment, it can be crossed with the treatment and modelled as a block
	
		\item[$\pmb{(d)}$] A split plot design (also named "mixed-design") is a structure where the fixed effects (tissue, drug) are crossed (each combination of tissue and drug are tested) but themselves nested within replicates.\\
		
		Thus, in a mixed-design ANOVA model, one factor (a fixed effects factor) is a between-subjects variable and the other (a random effects factor) is a within-subjects variable. Thus, overall, the model is a type of mixed effect model.
	\end{itemize}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Let us recall that for an ANOVA with fixed factors without replications we had:
	
	with $i=1\ldots a$, $j=1\ldots b$ where $\mu$ is the overall mean, $\tau_i$ is the effect of the $i$th of treatment $A$, $\beta_j$ is the effect of the $j$th level of treatment $B$ (main effects) and $\varepsilon_{ij}$ is the habitual random error following $\mathcal{N}(0,\sigma_\varepsilon^2)$.\\
	
	For nested and crossed ANOVA, the $\beta_j$ will be expanded $\alpha_j+\gamma_{ij}+(\alpha\gamma)_{ij}$ and to inclusion of certain block-treatment interaction terms. Thus the factor effect model may be written:
	
	\end{tcolorbox}
	
	\paragraph{ANOVA with blocks}\mbox{}\\\\
	With a "\NewTerm{randomized block design}\index{randomized block design}" (RBD), we have a characteristic of the units-of-analysis that we stratify (block) and then randomize into our treatment conditions within each block. For example, we could block on sex (male and female) and then randomly assign to a treatment and control condition separately for males and females, ensuring balance across the blocks in the number assigned to each group. In this design, we have one factor (treatment/control) and one block (male/female). This design also controls for any variance associated with the block (you would only want to use a block that you have good reason to believe is associated with the dependent variable).

	The purpose of the blocking factor is to account for a nuisance factor and/or to reduce the error term used in performing the test for the significance of the treatment effect. For this reason, the significance of the block effect itself is not tested, nor are multiple comparisons done between fixed blocks. Otherwise, a one-way blocked ANOVA is analysed as a two-way ANOVA with no interactions and no replications. That means that the following ANOVA table:
	\begin{table}[H]
		\resizebox{\textwidth}{!}{\begin{tabular}{lcccc}\hline
		\rowcolor[gray]{0.75}\textbf{Sum of squares (SSE)} & $\chi^2$ \textbf{df} & \textbf{Mean squares} & $F$ & \textbf{Critical} $F$\\ \hline
		$Q_A=k\displaystyle\sum_{j}\left(\bar{x}_{j}-\bar{\bar{x}}\right)^2$ & $k-1$ & $\text{MSk}A=\dfrac{Q_A}{k-1}$ &
		$\dfrac{\text{MSk}A}{\text{MSE}}$ & $P(F> F_{k-1,(k-1)(r-1)})$ \\
		$Q_B=r\displaystyle\sum_{i}\left(\bar{x}_{i}-\bar{\bar{x}}\right)^2$ & $r-1$ & $\text{MSk}B=\dfrac{Q_B}{r-1}$ &
		$\dfrac{\text{MSk}B}{\text{MSE}}$ & $P(F> F_{r-1,(k-1)(r-1)})$ \\
		$Q_R=\displaystyle\sum_{ij}\left(x_{ij}-\bar{x}_i-\bar{x}_j+\bar{\bar{x}}\right)^2$ & $(k-1)(r-1)$ & $ \text{MSE}=\dfrac{Q_R}{(k-1)(n-1)}$  & & \\
		$Q_T=\displaystyle\sum_{ij}\left(x_{ij}-\bar{\bar{x}}\right)^2$ & $N-1$ & $\text{MST}=\dfrac{Q_T}{N-1}$ & &\\ \hline
		\end{tabular}}
		\caption[]{Two-way fixed factor ANOVA table without repetitions}
	\end{table}
	becomes:
	\begin{table}[H]
		\resizebox{\textwidth}{!}{\begin{tabular}{lcccc}\hline
		\rowcolor[gray]{0.75}\textbf{Sum of squares (SSE)} & $\chi^2$ \textbf{df} & \textbf{Mean squares} & $F$ & \textbf{Critical} $F$\\ \hline
		\parbox{5cm}{$Q_t=k\displaystyle\sum_{j}\left(\bar{x}_{j}-\bar{\bar{x}}\right)^2$\\treatments} & $k-1$ & $\text{MSk}t=\dfrac{Q_t}{k-1}$ &
		$\dfrac{\text{MSk}t}{\text{MSE}}$ & $P(F> F_{k-1,(k-1)(r-1)})$ \\
		\parbox{5cm}{$Q_b=r\displaystyle\sum_{i}\left(\bar{x}_{i}-\bar{\bar{x}}\right)^2$\\blocks}& $r-1$ & $\text{MSk}b=\dfrac{Q_b}{r-1}$ &
		$\dfrac{\text{MSk}b}{\text{MSE}}$ & $P(F> F_{r-1,(k-1)(r-1)})$ \\
		$Q_R=\displaystyle\sum_{ij}\left(x_{ij}-\bar{x}_i-\bar{x}_j+\bar{\bar{x}}\right)^2$ & $(k-1)(r-1)$ & $ \text{MSE}=\dfrac{Q_R}{(k-1)(n-1)}$  & & \\
		$Q_T=\displaystyle\sum_{ij}\left(x_{ij}-\bar{\bar{x}}\right)^2$ & $N-1$ & $\text{MST}=\dfrac{Q_T}{N-1}$ & &\\ \hline
		\end{tabular}}
		\caption{One-way blocked ANOVA table}
	\end{table}	
	In practical situations it is sometimes not easy to decide whether block effects should be treated as random effects. For example, are the blocks in a field experiment randomly selected form a larger population of blocks? Most likely they were the only blocks available for the experiment. Of, if the experiment is replicated over two years (setting up a GRBD with nested blocking factors),are those years randomly selected? Certainly not, but still the researcher may want to consider the as "random" years. But if one years turns out to be a dry year and the other to be a wet year, then clearly we have an intrinsic blocking factor with fixed effects. There are obviously many variations of this discussion and thus this question becomes rather philosophical and often controversial. In general, we prefer to make the analysis once by considering them as fixed and once as random (as it is really quick done with computer software), and see if the conclusions are the same.
	
	For two samples, one-way blocked ANOVA is equivalent to the two-sample paired $T$-test.
	
	\paragraph{Latin Square ANOVA without repetitions}\label{latin Square ANOVA}\mbox{}\\\\
	A "\NewTerm{Latin Square ANOVA}\index{ANOVA!Latin Square}" is a three factor nested ANOVA but with the subtility that the third nested one has not each of its levels repeated in the levels of the two first but only one time and in such a way that each of its levels appears only one time at each row and each column of the parent level:
	\begin{figure}[H]
		\begin{center}
		\begin{tabular}{|ccc|}
		\hline
		1&2&3\\
		3&1&2\\
		2&3&1\\
		\hline
		\end{tabular}
		\hspace{10pt}
		\begin{tabular}{|cccc|}
		\hline
		4&3&1&2\\
		3&4&2&1\\
		1&2&4&3\\
		2&1&3&4\\
		\hline
		\end{tabular}
		\hspace{10pt}
		\begin{tabular}{|ccccc|}
		\hline
		1&2&4&3&5\\
		4&5&2&1&3\\
		3&4&1&5&2\\
		2&3&5&4&1\\
		5&1&3&2&4\\
		\hline
		\end{tabular}
		\caption{Latin squares of orders 3, 4 and 5}
		\end{center}
	\end{figure}
	The number of level of each factor is imposed as being in equal quantity that is why there is the term "square" in "Latin Square" design! The term "Latin" comes from the fact that no term is repeated on the same line or on the same column! We then logically speak of "\NewTerm{balanced Latin square design}" or just simply of "\NewTerm{Latin square design}" (LSD).

	So this design (attributed to Leonard Euler in 11783 according to holocene calendar), represents, in some sense, the simplest form of a row-column design as it is used for comparing $K$ treatments in $K$ rows and $K$ columns, where rows and columns represent two blocking factors. They were proposed as experimental design by Ronald Fisher in 11925 (holocene calendar), although François Cretté De Palluel (11788 according to holocene calendar) already utilized the idea of a $4\times 4$ Latin square design for an agricultural experiment.
	
	A complete illustration of such as design can be given by the following example (typical in Gage R\&R crossed studies without replications!):
	\begin{table}[H]
	\centering
	\begin{tabular}{rlllll}
	  \hline
	    & \multicolumn{5}{c}{\textbf{Operators}} \\
	  \cline{2-6}
	  \textbf{Batch}  & 1 & 2 & 3 & 4 & 5 \\
	  \hline
	  1 & A=$24$ & B=$20$ & C=$19$ & D=$24$ & E=$24$ \\
	  2 & B=$17$ & C=$24$ & D=$30$ & E=$27$ & A=$36$ \\
	  3 & C=$18$ & D=$38$ & E=$26$ & A=$27$ & B=$21$ \\
	  4 & D=$26$ & E=$31$ & A=$26$ & B=$23$ & C=$22$ \\
	  5 & E=$22$ & A=$30$ & B=$20$ & C=$29$ & D=$31$ \\
	   \hline
	\end{tabular}
	\caption[]{Famous Latin square design for the Propellant model of Douglas C. Montgomery}
	\end{table}
	We see why the Latin square ANOVA are relatively rare in practice because they assume that for each object we have the same number of combinations of tests when in reality we could have less or more. Moreover, the Latin squares ANOVA are often designated as incomplete plans, or "\NewTerm{incomplete block designs}\footnote{So keep in mind that it is a randomized block design in which every treatment is not present in every block}\index{incomplete block designs}", and thus in extenso economic because if we really had to make all the combinations for a Latin square of dimension $n$, the number of total combinations would be $n^3$ whereas in reality the Latin square represents only $n^2$.
	
	Therefore we can already observe that:
	\begin{enumerate}
		\item The order of performing the trials is random
		
		\item We have many trials (as the example above should have $5\cdot 5 \cdot 5 =125$ trials if it were complete)
	\end{enumerate}
	Note also that for a given dimension a Latin square is not unique. Indeed the following Latin square:
	\begin{table}[H]
		\centering
		\begin{tabular}{rllll}
		  \hline
		    & \multicolumn{3}{c}{Cars} \\
		  \cline{2-5}
		  Drivers & 1 & 2 & 3 & 4 \\
		  \hline
		  1 & A & B & D & C \\
		  2 & D & C & A & B \\
		  3 & B & D & C & A \\
		  4 & C & A & B & D \\
		   \hline
		\end{tabular}
	\end{table}
	is equivalent to that one:
	\begin{table}[H]
		\centering
		\begin{tabular}{rllll}
		  \hline
		    & \multicolumn{3}{c}{Cars} \\
		  \cline{2-5}
		  Drivers & 1 & 2 & 3 & 4 \\
		  \hline
		  1 & A & B & D & C \\
		  2 & C & D & B & A \\
		  3 & B & A & C & D \\
		  4 & D & C & A & B \\
		   \hline
		\end{tabular}
	\end{table}
	The real interest of the Latin square ANOVA is that they are certainly economic since they are incomplete but mainly because they are orthogonal in the sense that they contain the best way of doing the tests by minimizing the quantity of these last ones. Indeed, we see in a Latin square that each letter appears only once in each line and each column (hence the origin of the name!) in order to minimize the error (mathematically there is therefore "orthogonality").
	
	It is customary to represent a Latin square ANOVA in the following more general way:
	\begin{table}[H]
		\centering
		\begin{tabular}{r|llll|c}
		  \hline
		    & \multicolumn{4}{c}{1st Factor} \\  \hline
		  \cline{2-5}
		  2nd Factor & 1 & 2 & 3 & 4 & Average of rows  \\
		  \hline
		  \texttt{I} & $y_{111}$ & $y_{122}$ & $y_{134}$ & $y_{143}$ & $\bar{y}_{1..}$ \\
		  \texttt{II} & $y_{214}$ & $y_{223}$ & $y_{231}$ & $y_{341}$ & $\bar{y}_{2..}$\\
		  \texttt{III} & $y_{312}$ & $y_{324}$ & $y_{333}$ & $y_{341}$ & $\bar{y}_{3..}$ \\
		  \texttt{IV} & $y_{413}$ & $y_{421}$ & $y_{432}$ & $y_{444}$ & $\bar{y}_{4..}$ \\ \hline
		  Average of columns & $\bar{y}_{.1.}$ & $\bar{y}_{.2.}$ & $\bar{y}_{.3.}$ & $\bar{y}_{.4.}$ & $\bar{y}_{...}$ \\
		  \hline
		\end{tabular}
	\end{table}
	And obviously notice that the number of rows is (obviously!) always equal to the number of columns and for such designs, we also speak of "\NewTerm{symmetric designs}".
	
	Let us prove that we have:
	
	where $R$ stands for "rows", $C$ for "columns" and $r$ for "residuals".
	
	Well let's go in joy and good mood ...! We first write:
	
	But:
	
	Indeed, let's start by noticing that:
	
	The same calculations show that:
	
	Furthermore:
	
	and again we have the same result for the indices $j$ and $k$. That is to say:
	
	Therefore:
	
	Now, note that:	
	
	as:
	
	Here again we also have with the same calculations:
	
	Furthermore:
	
	Therefore:
	
	We have:
	
	as: 
	
	Furthermore:
	
	as:
	
	The same result holds for the indexes $j$ and $k$, ie:
	
	Hence:
	
	Back to the beginning...
	
	Then it remains to us:
	
	Let's evaluate the double products in the last sum:
	
	because as seen previously, all these are worth $K^2(\bar{y})^2$.
	
	The other double products are also worth $0$ by the same arguments. So:
	
	To finish, let us note that:
	
	and the same for $j$ and $k$. Therefore:
	
	If we denote by $K$ the dimension of a Latin square, for each of the above terms we have the following degrees of freedom:
	
	We then have the following ANOVA table:
	\begin{table}[H]
		\centering
		\resizebox{\textwidth}{!}{\begin{tabular}{lcccc}
		\hline 
		\rowcolor[gray]{0.75}\textbf{Sum of squares}	& \textbf{$\chi^2$ df} & \textbf{Mean squares} & $F$  & \textbf{Critical $F$} \\ 
			\hline 
		\parbox{5cm}{$Q_R=K\displaystyle\sum_{i}{\left(\bar{y}_{i..}-\bar{y}_{\dots}\right)}^{2}$\\ (rows)} & $K-1$ & $\text{MSk}R= \dfrac{Q_R}{K-1}$ &$\dfrac{\text{MSk}R}{\text{MSE}}$  & $P(F>F_{K-1,(K-1)(K-2)})$ \\
		\parbox{6cm}{$Q_C=K\displaystyle\sum_{j}{\left(\bar{y}_{.j.}-\bar{y}_{\dots}\right)}^{2}$\\ (columns)} &$K-1$  &$\text{MSk}C= \dfrac{Q_{C}}{K-1}$  &  $\dfrac{\text{MSk}C}{\text{MSE}}$ & $P(F>F_{K-1,(K-1)(K-2)})$ \\
		\parbox{6cm}{$Q_{RC}=K\displaystyle\sum_{k}{\left(\bar{y}_{..k}-\bar{y}_{\dots}\right)}^{2}$\\ (treatments)}	& $K-1$ & $\text{MSk}RC=\dfrac{Q_{RC}}{K-1}$  & $\dfrac{\text{MSk}RC}{\text{MSE}}$ & $P(F>F_{K-1,(K-1)(K-2)})$ \\
		\parbox{7cm}{$Q_r=\displaystyle\sum_{i,j,k}\left({y}_{ijk}-\bar{y}_{i..}-\bar{y}_{.j.}-\bar{y}_{..k}+2\bar{y}_{\dots}\right)^2$\\ (residuals)}	& $(K-1)(K-2)$ & $\text{MSE}=\dfrac{Q_r}{(K-1)(K-2)}$  &  &  \\ 
		\parbox{6cm}{$Q_T=\displaystyle\sum_{i,j,k}{\left({y}_{ijk}-\bar{y}_{\dots}\right)}^{2}$\\ (total)} & $K^2-1$ & $\text{MST}=\dfrac{\mathrm{Q_T}}{K^2-1}$ &  &  \\ 
			\hline 
		\end{tabular}}
		\caption{Latin square ANOVA table}
	\end{table}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The factor effect model for this design is:
	
	where $i,j,k=1\ldots N$ and $y_{ijk}$ is the observation in the $i$th row and $k$th column for the $j$th treatment, $\mu$ is the overall mean, $\alpha_i$ is the $i$th row effect, $\tau_j$ is the $j$th treatment effect, $\beta_k$ is the $k$th column effect, and $\varepsilon_{ijk}$ is a random error.\\
	
	Notice that only two out of the three subscripts are necessary to identify an observation in the latin square design and that this model is (obviously) purely additive. The assumptions on the effect terms are $\sum_{i=1}^N \alpha_i=0$, $\sum_{j=1}^N \tau_j=0$ and $\sum_{k=1}^N \beta_k=0$.\\
	
	We are interested (mainly!) for zero treatment effect, that is, $H_0:\;\tau_1= \tau_2=\ldots=0$ versus $H_1:\; \tau_j\neq 0$.
	\end{tcolorbox}
	Keep in mind that the advantage of Latin square designs are:
	\begin{itemize}
		\item They allow experiments with a small number of runs (more economical than crossing all factors)
		\item Easy to analyse
	\end{itemize}
	The disadvantages are:
	\begin{itemize}
		\item The number of levels of each blocking variable must equal the number of levels of the treatment factor
	
		\item They assume that there are no interactions between the blocking variables or between the treatment variable and the blocking variable (may be unrealistic)
	
		\item It is say that squares smaller than $4\times 4$ or $5\times 5$ generally have too few replications for a desirable level of precision (low power) 
		
		\item It can be proved that $6\times 6$ Latin square designs are impossible (the proof is actually not in this book!)
	\end{itemize}
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} The Latin square designs are part of what we name also sometimes "\NewTerm{space filling design}" (family that also contains spherical designs, uniform designs with minimum potential designs, or maximum entropy designs or IMSE (integrated mean-square error) Gaussian processes).\\
	
	\textbf{R2.} There exist also rectangular Latin design for information...
	\end{tcolorbox}
	
	\pagebreak
	\paragraph{Graeco-Latin Square ANOVA}\mbox{}\\\\
	A "\NewTerm{Graeco-Latin Square ANOVA}\index{ANOVA!Graeco-Latin Square}" is an extension of a Latin square used when the number of block is greater than $2$. Concretely, a Graeco-Latin square ANOVA can control up to two nuisance factors and one treatment factor (three sources of extraneous variability...) as represented below:
	\begin{table}[H]
	\centering
	\begin{tabular}{rllll}
	  \hline
	    & \multicolumn{4}{c}{\textbf{Column}} \\
	  \cline{2-5}
	  \textbf{Row} & 1 & 2 & 3 & 4 \\
	  \hline
	  1 & A$\alpha$ & B$\beta$ & C$\gamma$ & D$\delta$ \\
	  2 & B$\delta$ & A$\gamma$ & D$\beta$ & C$\alpha$ \\
	  3 & C$\beta$ & D$\alpha$ & A$\delta$ & B$\gamma$ \\
	  4 & D$\gamma$ & C$\delta$ & B$\alpha$ & A$\beta$ \\
	   \hline
	\end{tabular}
	\caption{Graeco-Latin square of order $4$}
	\end{table}
	Use of these designs results in exceptional saving of time and means, as the total number of design points has been drastically reduced as if we consider $n$ level for each of the three factor we have only $n^2$ experimental points, when compared to $n^4$ (since there are four factors!!!!) design points in full factorial design. However similar to Latin squares, these designs may be used only when interactions are statistically insignificant.
	
	To figure out a Graeco-Latin square, we consider initially a Latin square $K \otimes K$ on which we superimposed another Latin square $K \otimes K$ which this time is denoted by Greek letters. Afterwards in a second time during the superposition, we must ensure that the following property is respected, "each row and each column can only contain couple of letters (Latin, Greek) that are distincts". Thus, if this property is verified (as shown in the diagram below) we say that the two orthogonal Latin squares are orthogonals:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1,scale=0.9]
		%uncomment if require: \path (0,1046); %set diagram left start at 0, and has height of 1046
		
		%Shape: Rectangle [id:dp0010546889998834796] 
		\draw   (87,105) -- (249.5,105) -- (249.5,228) -- (87,228) -- cycle ;
		%Shape: Rectangle [id:dp35385190394929955] 
		\draw   (363,105) -- (525.5,105) -- (525.5,228) -- (363,228) -- cycle ;
		%Shape: Brace [id:dp9646229351379811] 
		\draw   (90.5,241) .. controls (90.5,245.67) and (92.83,248) .. (97.5,248) -- (298,248) .. controls (304.67,248) and (308,250.33) .. (308,255) .. controls (308,250.33) and (311.33,248) .. (318,248)(315,248) -- (518.5,248) .. controls (523.17,248) and (525.5,245.67) .. (525.5,241) ;
		%Shape: Rectangle [id:dp9486865635086372] 
		\draw   (241,340) -- (403.5,340) -- (403.5,463) -- (241,463) -- cycle ;
		%Shape: Rectangle [id:dp5610305593702354] 
		\draw   (200.5,340) -- (241,340) -- (241,463) -- (200.5,463) -- cycle ;
		%Shape: Rectangle [id:dp5137830017664791] 
		\draw   (240.5,313) -- (403.5,313) -- (403.5,340) -- (240.5,340) -- cycle ;
		
		% Text Node
		\draw (101,58) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{90.08pt}\setlength\topsep{0pt}
		$\displaystyle 4\times 4$\textbf{ Latin square}
		\begin{center}
		Latin letters
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (379,58) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{90.08pt}\setlength\topsep{0pt}
		$\displaystyle 4\times 4$\textbf{ Latin square}
		\begin{center}
		Greek letters
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (95,111) node [anchor=north west][inner sep=0.75pt]   [align=left] {A};
		% Text Node
		\draw (138.33,111) node [anchor=north west][inner sep=0.75pt]   [align=left] {B};
		% Text Node
		\draw (181.66,111) node [anchor=north west][inner sep=0.75pt]   [align=left] {C};
		% Text Node
		\draw (226,111) node [anchor=north west][inner sep=0.75pt]   [align=left] {D};
		% Text Node
		\draw (95,143) node [anchor=north west][inner sep=0.75pt]   [align=left] {B};
		% Text Node
		\draw (138.33,143) node [anchor=north west][inner sep=0.75pt]   [align=left] {A};
		% Text Node
		\draw (181.66,143) node [anchor=north west][inner sep=0.75pt]   [align=left] {D};
		% Text Node
		\draw (226,143) node [anchor=north west][inner sep=0.75pt]   [align=left] {C};
		% Text Node
		\draw (95,170) node [anchor=north west][inner sep=0.75pt]   [align=left] {C};
		% Text Node
		\draw (138.33,170) node [anchor=north west][inner sep=0.75pt]   [align=left] {D};
		% Text Node
		\draw (181.66,170) node [anchor=north west][inner sep=0.75pt]   [align=left] {A};
		% Text Node
		\draw (226.5,170) node [anchor=north west][inner sep=0.75pt]   [align=left] {B};
		% Text Node
		\draw (95,202) node [anchor=north west][inner sep=0.75pt]   [align=left] {D};
		% Text Node
		\draw (138.33,202) node [anchor=north west][inner sep=0.75pt]   [align=left] {C};
		% Text Node
		\draw (181.66,202) node [anchor=north west][inner sep=0.75pt]   [align=left] {B};
		% Text Node
		\draw (226.5,202) node [anchor=north west][inner sep=0.75pt]   [align=left] {A};
		% Text Node
		\draw (371,111) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \alpha $};
		% Text Node
		\draw (414.33,111) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \beta $};
		% Text Node
		\draw (457.66,111) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \gamma $};
		% Text Node
		\draw (502,111) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \delta $};
		% Text Node
		\draw (371,143) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \delta $};
		% Text Node
		\draw (414.33,143) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \gamma $};
		% Text Node
		\draw (457.66,143) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \beta $};
		% Text Node
		\draw (502,143) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \alpha $};
		% Text Node
		\draw (371,170) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \beta $};
		% Text Node
		\draw (414.33,170) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \alpha $};
		% Text Node
		\draw (457.66,170) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \delta $};
		% Text Node
		\draw (502,170) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \gamma $};
		% Text Node
		\draw (371,202) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \gamma $};
		% Text Node
		\draw (414.33,202) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \delta $};
		% Text Node
		\draw (457.66,202) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \alpha $};
		% Text Node
		\draw (502,202) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \beta $};
		% Text Node
		\draw (216,263) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle 4\times 4$\textbf{ Graeco-Latin square}};
		% Text Node
		\draw (246,346) node [anchor=north west][inner sep=0.75pt]   [align=left] {A$\displaystyle \alpha $};
		% Text Node
		\draw (289.33,346) node [anchor=north west][inner sep=0.75pt]   [align=left] {B$\displaystyle \beta $};
		% Text Node
		\draw (332.66,346) node [anchor=north west][inner sep=0.75pt]   [align=left] {C$\displaystyle \gamma $};
		% Text Node
		\draw (377,346) node [anchor=north west][inner sep=0.75pt]   [align=left] {D$\displaystyle \delta $};
		% Text Node
		\draw (246,378) node [anchor=north west][inner sep=0.75pt]   [align=left] {B$\displaystyle \delta $};
		% Text Node
		\draw (289.33,378) node [anchor=north west][inner sep=0.75pt]   [align=left] {A$\displaystyle \gamma $};
		% Text Node
		\draw (332.66,378) node [anchor=north west][inner sep=0.75pt]   [align=left] {D$\displaystyle \beta $};
		% Text Node
		\draw (377,378) node [anchor=north west][inner sep=0.75pt]   [align=left] {C$\displaystyle \alpha $};
		% Text Node
		\draw (246,405) node [anchor=north west][inner sep=0.75pt]   [align=left] {C$\displaystyle \beta $};
		% Text Node
		\draw (289.33,405) node [anchor=north west][inner sep=0.75pt]   [align=left] {D$\displaystyle \alpha $};
		% Text Node
		\draw (332.66,405) node [anchor=north west][inner sep=0.75pt]   [align=left] {A$\displaystyle \delta $};
		% Text Node
		\draw (377,405) node [anchor=north west][inner sep=0.75pt]   [align=left] {B$\displaystyle \gamma $};
		% Text Node
		\draw (246,437) node [anchor=north west][inner sep=0.75pt]   [align=left] {D$\displaystyle \gamma $};
		% Text Node
		\draw (289.33,437) node [anchor=north west][inner sep=0.75pt]   [align=left] {C$\displaystyle \delta $};
		% Text Node
		\draw (332.66,437) node [anchor=north west][inner sep=0.75pt]   [align=left] {B$\displaystyle \alpha $};
		% Text Node
		\draw (377,437) node [anchor=north west][inner sep=0.75pt]   [align=left] {A$\displaystyle \beta $};
		% Text Node
		\draw (214.13,346.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (214.13,376.57) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (214.13,406.74) node [anchor=north west][inner sep=0.75pt]    {$3$};
		% Text Node
		\draw (214.13,436.9) node [anchor=north west][inner sep=0.75pt]    {$4$};
		% Text Node
		\draw (198,317) node [anchor=north west][inner sep=0.75pt]   [align=left] {Rows};
		% Text Node
		\draw (289,294) node [anchor=north west][inner sep=0.75pt]   [align=left] {Columns};
		% Text Node
		\draw (251.13,318.9) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (294.79,318.9) node [anchor=north west][inner sep=0.75pt]    {$2$};
		% Text Node
		\draw (338.47,318.9) node [anchor=north west][inner sep=0.75pt]    {$3$};
		% Text Node
		\draw (382.13,318.9) node [anchor=north west][inner sep=0.75pt]    {$4$};
		
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Merge of two Latin square designs}
	\end{figure}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The Graeco-Latin square designs are also part of what we name also sometimes "\NewTerm{space filling design}" as for the Latin square designs.
	\end{tcolorbox}
	As you can notice, the Graeco-Latin square design allows the analysis of $4$ factors (row, column, Latin letter, Greek letter), each factor having $K$ levels for only $ K \otimes K $ or $K^2$ possible combinations.

	For the need of the analysis of variance, we have to define certain variables:

	\begin{itemize}
		\item $K$ the number of levels for each factor
		
		\item $ N = K ^ 2 $, the total  number of observations $ y_ {i, j, k, l} $ in the Graeco-Latin square, where $ i $ is the $ i$-th level of the factor represented in row, $ j $ the $ j$-th level of the factor represented in column, $ k $ the $ k$-th level of the factor represented by Latin letters and the $ l $ the $ l$-th level of the factor represented by Greek letters.
	\end{itemize}
	
	Thus, one can formulate as follows:
	\begin{itemize}
	  \item The sum and the average of observations:
	 
	  \item The sum and average for each row:
	   
	  \item The sum and average for each columns:
	   
	  \item The sum and average for each Latin letter:
	  
	  \item The sum and average for each Greek letter:
	   
	\end{itemize}
	For the development, we will prefer this notation $\displaystyle\sum_{i,j,k,l}^{K}$ to that one $\displaystyle \sum_{i=1}^{K}\sum_{j=1}^{K}\sum_{k=1}^{K}\sum_{l=1}^{K}$ in order to simplify the notations.
	
	
	Let us deal now with the decomposition of the total variance! Let us prove that the latter:
	
	can be written:
	
	For that, let us first put:
	 
	Let's put some $\bar{y}_{....}$ in it like this:
	
	At this level, we fall back on our famous remarkable identity $ (a + b) ^ 2 = a ^ 2 + b ^ 2 + 2ab $. Thus in the framework our development, we get:
	
	Let us consider now the $3$th member of this expression $2\displaystyle\sum ab$:
	
	And let us show that the result of that latter is equal to $ 0 $. Let's start by developing it as below:
	
	Let us develop the first term above denoted by the exponent $^{(a)}$:
	
	Let us evaluate each of the six terms above separately: 
	
	So finally we get that the original term denoted by the exponent $^{(a)}$:
	
	is equal to:
	
	That gives:
	
	Thus, by the same boring mathematical process, we can prove that each of the other four remaining terms are also equal to zero. Thus:
	
	
	
	
	Now that we have proved that $2\displaystyle\sum ab  = 0$, that is to say:
	
	Let us come back to the beginning (...):
	
	Since the $3$rd term is equal to it remains obviously only:
	
	Let us take the $2$nd term of the above expression:
	
	In the above development let us consider the following expression:
	
	And let us prove that it is equal to zero:
	
	Consequently, each of the following expressions:
	
	
	
	is equal to $0$.
		
	Finally it remains:
	
	If we denote as usual SSE as the sum of the squares of the deviations then we can rewrite our result in this form:
	
	where:
	
	and:
	
	With for degrees of freedom:
	$$\text{df}_{T} = \text{df}_{R} + \text{df}_{C} + \text{df}_{l} + \text{df}_{g} +\text{df}_{r}$$
	$${K}^{2}-1 = \left(K-1 \right) + \left(K-1 \right) + \left(K-1 \right) + \left(K-1 \right) + \left(K-1 \right)\left(K-3 \right)$$
	The degree of freedom for the squares of the residual differences is obtained by posing:
	$${K}^{2}-1 -\left[\left(K-1 \right) + \left(K-1 \right) + \left(K-1 \right) + \left(K-1 \right) \right] = \left(K-1 \right)\left(K-3 \right)$$
	We then have the following ANOVA table:
	\begin{table}[H]
		\centering
		\resizebox{\textwidth}{!}{\begin{tabular}{lcccc}
		\hline 
		\rowcolor[gray]{0.75}\textbf{Sum of squares (SSE)}	& \textbf{$\chi^2$ df} & \textbf{Mean squares} & $F$  & \textbf{Critical $F$} \\ 
			\hline 
		\parbox{5cm}{$Q_R=\displaystyle K\sum_{i}^{K}\left( \bar{y}_{i...}-\bar{y}_{....}\right)^2$\\ (rows)} & $K-1$ & $\text{MSk}R= \dfrac{Q_R}{K-1}$ &$\dfrac{\text{MSk}R}{\text{MSE}}$  & $P(F>F_{K-1,(K-1)(K-3)})$ \\
		\parbox{6cm}{$Q_C=K\displaystyle\sum_{j}{\left(\bar{y}_{.j.}-\bar{y}_{\dots}\right)}^{2}$\\ (columns)} &$K-1$  &$\text{MSk}C= \dfrac{Q_{C}}{K-1}$  &  $\dfrac{\text{MSk}C}{\text{MSE}}$ & $P(F>F_{K-1,(K-1)(K-3)})$ \\
		\parbox{6cm}{$Q_l=\displaystyle K\sum_{k}^{K}\left( \bar{y}_{..k.}-\bar{y}_{....}\right)^2 $\\ (latin letter)}	& $K-1$ & $\text{MSk}l=\dfrac{\mathrm{Q_l}}{K-1}$  & $\dfrac{\text{MSk}l}{\text{MSE}}$ & $P(F>F_{K-1,(K-1)(K-3)})$ \\
		\parbox{6cm}{$Q_g=\displaystyle K\sum_{l}^{K}\left( \bar{y}_{...l}-\bar{y}_{....}\right)^2 $\\ (greek letter)}	& $K-1$ & $\text{MSk}g=\dfrac{Q_l}{K-1}$ & $\dfrac{\text{MSk}g}{\text{MSE}}$ & $P(F>F_{K-1,(K-1)(K-3)})$ \\ 
		\parbox{7.5cm}{$Q_r$\\$=\displaystyle\sum_{i,j,k,l}^{K}\left({y}_{ijkl}-\bar{y}_{i...}-\bar{y}_{.j..}-\bar{y}_{..k.}-\bar{y}_{...l}+3\bar{y}_{....}\right)^2 $\\ (residuals)}	& $(K-1)(K-3)$ & $\text{MSE}=\dfrac{Q_r}{(K-1)(K-3)}$  &  &  \\ 
		\parbox{7.5cm}{$Q_T$\\$=\displaystyle\sum_{i=1}^{K}\sum_{j=1}^{K}\sum_{k=1}^{K}\sum_{l=1}^{K}{\left({y}_{ijkl}-\bar{y}_{....} \right)}^{2}$\\ (total)} & $K^2-1$ & $\text{MSk}T=\dfrac{Q_T}{K^2-1}$ &  &  \\ 
			\hline 
		\end{tabular}}
		\caption{Greaco-Latin square ANOVA table}
	\end{table}
	Notice that in the Graeco-Latin Square Design as in the Latin Square
Design it is assumed that no interaction among the factors exist. This is
an even stronger assumption in the Graeco-Latin Square Design because
we have an additional factor and, hence, more possible interactions.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The factor effect model for this design is (purely additive!):
	
	where $i,j,k,l=1\ldots N$ and where $y_{ijkl}$ is the observation in the $i$th row, Latin letter $j$, Greek letter $k$, and $l$th column, $\mu$ is the overall mean, $\theta_i$ is the $i$th row effect, $\tau_j$ is the effect of the Latin letter $j$ treatment, $\omega_k$ is the effect of Greek letter $k$, $\phi_l$ is the $l$th column effect, and $\varepsilon_{ijkl}$ are i.i.d. $\mathcal{N}(0,\sigma_\varepsilon^2)$ errors. Notice that as for the Latin square design two subscripts suffice to identify an observation and $6\times 6$ Graeco-Latin designs are impossible.\\
	
	We are (mainly!) interested in testing for zero treatment effect, that is, $H_0:\;\tau_1=\tau_2=\ldots=\tau_N=0$ versus $H_1:\; \tau_j\neq 0$.
	\end{tcolorbox}
	Many softwares return only the first Fisher test (that for $Q_R$, ie the treatments) since the blocks of the Greaco-Latin design represent restrictions on randomization and that therefore the other tests may not be appropriate.
	
	
	\paragraph{Multivariate ANOVA (MANOVA)}\label{MANOVA}\mbox{}\\\\
	The "\NewTerm{Multivariate Analysis of Variance}\index{multivariate analysis of variance}" is an extension of ANOVA where we assume that the categorical variables are linearly dependent and we test whether there is a significant difference in the averages (variables supposed to be of the continuous type) through the different groups under the interdependence of the categorical explanatory variables.
	
	Again, as for the ANOVA, the MANOVA and Multivariate Linear Models are the two facets of a same coin are the ground foundations of a more field of statistics as depicted in the flowchart below (actually in the book, only Multivariate Generalized Linear Models are treated in details in the section of Numerical Methods!):
	
	\begin{figure}[H]
		\centering
		\resizebox{\textwidth}{!}{%
		\begin{tikzpicture}
		\newenvironment{cminipage}[1][5cm]{%
		  \begin{minipage}{#1}
		    \begin{center}
		    }{%
		    \end{center}
		  \end{minipage}
		}
		
		\tikzset{
		  every path/.style = {
		   ->,
		   > = stealth, 
		   very thick,
		   rounded corners},
		  state/.style = {
		    rectangle,
		    rounded corners,
		    draw=black,
		    fill=gray!10,
		    thick,
		    minimum height=2em,
		    inner sep=10pt,
		    text centered
		  },
		  note/.style = {
		    fill = yellow,
		    text width = 3.2cm,
		    text centered,
		  },
		  node distance=2cm and 1cm,
		}
	
		  \node[state] (manova) {
		    \begin{cminipage}[5cm]
		      Multivariate Linear Models (MANOVA)
		    \end{cminipage}
		  };
		
		  \node[state, below left=of manova] (mdmlm) {
		    \begin{cminipage}[5cm]
		      Multiple Design
		      Multivariate Linear Models
		    \end{cminipage}
		  };
		
		  \node[state, dotted, below=of manova] (mclm) {
		    \begin{cminipage}[5cm]
		      Multivariate Correlated Linear Models
		    \end{cminipage}
		  };
		
		  \node[state, below right=of manova] (mglm) {
		    \begin{cminipage}[5cm]
		      Multivariate Generalized\\ Linear Models
		    \end{cminipage}
		  };
		
		  \coordinate [below=0.5cm of mclm] (union);
		
		  \node[state, below=of union, fill = cyan] (mcglm) {
		    \begin{cminipage}[5cm]
		      Multivariate Covariance\\ Generalized Linear Models \\ (McGLM)
		    \end{cminipage}
		  };
		
		  \node[state, draw = cyan, below left=2cm and -1cm of mcglm] (mcgnlm) {
		    \begin{cminipage}[5cm]
		      Multivariate Covariance Generalized Nonlinear Models
		    \end{cminipage}
		  };
		
		  \node[state, draw = cyan, below right=2cm and -1cm of mcglm] (mcpglm) {
		    \begin{cminipage}[5cm]
		      Multivariate Parametrized Covariance Generalized Linear Models
		    \end{cminipage}
		  };
		
		  \path[draw] (manova) -| node [note, pos = 0.8] {$+$ by response predictor} (mdmlm);
		  \path[draw] (manova) -- node [note, pos = 0.5] {$+$ correlation between units} (mclm);
		  \path[draw] (manova) -| node [note, pos = 0.8] {$+$ exponential family distribution} (mglm);
		
		  \path[draw, -] (mdmlm) |- (union);
		  \path[draw, -] (mclm) |- (union);
		  \path[draw, -] (mglm) |- (union);
		  \path[draw] (union) -- node [note, fill = yellow] {$+$ dispersion matrix linear predictor} (mcglm);
		
		  \path[draw] (mcglm) -| node [note, pos = 0.8] {$+$ nonlinear predictor} (mcgnlm);
		  \path[draw] (mcglm) -| node [note, pos = 0.8] {$+$ parametrized dispersion matrix linear predictor} (mcpglm);
		
		\end{tikzpicture}
		}
		\vspace*{3mm}
		\caption{Families of Multivariate Linear Models}
	\end{figure}

	If it is not clear (...), the mathematical notation may be more explicit to the reader. Let us recall first that in the case of the canonical ANOVA with $1$ fixed factor at $k$ levels, the null hypothesis was of the form:
	
	For the one-way MANOVA, the null hypothesis is:
	
	where we have therefore "mean vectors" for a given number of dependent group variables in a quantity $k$.

	Thus explicitly for $k$ groups and $p$ categorical variables:
	
	As the reader will see in the developments below, the one-way MANOVA has nine assumptions (hypothesis):
	\begin{itemize}
		\item[H1.] Our two or more dependent variables should be measured at a continuous level (i.e., they are interval or ratio variables)
		\
		\item[H2.] Our independent variable should consist of two or more categorical, independent (unrelated) groups
		
		\item[H3.] We should have independence of observations, which means that there is no relationship between the observations in each group or between the groups themselves
		
		\item[H4.] We should have an adequate sample size. At least need to have more cases (e.g., participants) in each group of the independent variable than the number of dependent variables we are analysing
		
		\item[H5.] There are no univariate or multivariate outliers. First, there can be no (univariate) outliers in each group of the independent variable for any of the dependent variables
		
		\item[H6.] There is multivariate Normality
		
		\item[H7.] There is a linear relationship between each pair of dependent variables for each group of the independent variable
		
		\item[H8.] There is a homogeneity of variance-covariance matrices
		
		\item[H9.] There is no multicollinearity
	\end{itemize}
	Let us now recall that in the one-factor ANOVA we compute the $p$-value of the ratio of the estimated variances assuming the true variances as equals:
	
	The next step in understanding MANOVA is the recognition that mathematicians are lazy and sometimes do engineering statistics... So first we simplify this ratio by multiplying left and right by $(k-1)/(N-1)$. This gives what we name an "$A$-statistics":
	
	But this is not enough as we are still with independent variables! So the idea is to used variance-covariance
like matrix and to get a similar expression of as a ratio we take the determinant... This brings us to write a multivariate like Fisher test for variances (......):
	
	better known in the following form:
	
	where $W$ and $T$ are respectively the determinants of the square matrices of the sum of the squares of the within ($W$) and global (Total: $T$) deviations. Which means that if the part "within" (Between: $B$) is really large, then $\Lambda$ tends to zero. In contrast, if $B$ is very small, tends $\Lambda$ tends to $1$.

	To prove that this ratio does not depend on the covariance matrix of the dependent variables, let us recall that considering the structure of $W$ and $B+W$ we can make a spectral decomposition (see further below our study of principal component analysis) in the form:
	
	It can be demonstrated with some approximations that $\Lambda$ follows a distribution named "Wilk's distribution" (of "Samuel Stanley Wilks").

	It should be noted that Wilk's $\Lambda$ can be expressed as a function of the eigenvalues of $W^{-1}B$. From the definition of $\Lambda$, it follows using the properties of the determinant (\SeeChapter{see section Linear Algebra page \pageref{determinant}}) that:
	
	We recognize here the determinant of the "eigenvalue equations" (\SeeChapter{see section Linear Algebra page \pageref{eigenvalue equations}}) on $W^{-1}B$ but with $\lambda=1$. Indeed:
	
	So in our case here we have $\lambda=-1$ therefore:
	
	Therefore:
	
	and consequently:
	
	Also, it follows that:
	
	When Wilk's $\Lambda$ approaches $1$, we showed that it means that the difference in means is negligible. This is the case when $\ln(\Lambda)$ approaches $0$. However, when $\Lambda$ approaches $0$ or $\ln(\Lambda)$ approaches $1$, it means that the difference is large. Therefore, a large value of $\ln(\Lambda)$ (i.e., close to $0$) is an indication of the significance of the difference between the means.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The bad news is that there are in fact four different multivariate tests that are made from the above relation. The reason for four different statistics and four approximations is that the mathematics of MANOVA get so complicated in some cases that no one has ever been able to solve them as far as we know...
	\end{tcolorbox}
	Let us see now a companion detailed example:
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	We consider the following set of two dependent variables on three groups of independent variables:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|c|c|c|c|}
		\cline{1-2} \cline{4-5} \cline{7-8}
		\multicolumn{2}{|c|}{\cellcolor[gray]{0.75}$K_1$} & & \multicolumn{2}{c|}{\cellcolor[gray]{0.75}$K_2$} & & \multicolumn{2}{c|}{\cellcolor[gray]{0.75}$K_3$} \\ \cline{1-2} \cline{4-5} \cline{7-8} 
		\cellcolor[HTML]{EFEFEF}$x_1$ & \cellcolor[HTML]{EFEFEF}$x_2$ & & \cellcolor[HTML]{EFEFEF}$x_1$ & \cellcolor[HTML]{EFEFEF}$x_2$ & & \cellcolor[HTML]{EFEFEF}$x_1$  & \cellcolor[HTML]{EFEFEF}$x_2$  \\ \cline{1-2} \cline{4-5} \cline{7-8} 
		$2$ & $3$ & & $4$ & $8$ & & $7$ & $6$ \\ \cline{1-2} \cline{4-5} \cline{7-8} 
		$3$ & $4$ & & $5$ & $6$ & & $8$ & $7$ \\ \cline{1-2} \cline{4-5} \cline{7-8} 
		$5$ & $4$ & & $6$ & $7$ & & $10$ & $8$ \\ \cline{1-2} \cline{4-5} \cline{7-8} 
		$2$ & $5$ & & & & & $9$ & $5$ \\ \cline{1-2} \cline{4-5} \cline{7-8} 
		 & & & & & & $7$ & $6$ \\ \cline{1-2} \cline{4-5} \cline{7-8} 
		\multicolumn{1}{|l|}{$\bar{x}_{11}=3$} & \multicolumn{1}{l|}{$\bar{x}_{21}=4$} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{$\bar{x}_{12}=5$} & \multicolumn{1}{l|}{$\bar{x}_{22}=7$} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{$\bar{x}_{13}=8.2$} & \multicolumn{1}{l|}{$\bar{x}_{23}=6.4$} \\ \cline{1-2} \cline{4-5} \cline{7-8} 
		\end{tabular}
	\end{table} 
	with:
	
	Now we calculate:
	
	We then have for example for the first group:
	
	Which gives us in our case (very easy to check with any spreadsheet software):
	
	And we have (we notice that we assume that over all the $x_1$ and $x_2$ the number of measurement are $N_1=N_2=N$):
	
	and therefore:
	
	After the calculation of the $p$-value is more delicate and we will ask the reader to refer to the Minitab or \texttt{R} companion books for the computational details and the final conclusion.
	\end{tcolorbox}
	So we could use a one-way MANOVA to determine whether students’ short-term and long-term recall of facts differed based on three different lengths of lecture (i.e., the two dependent variables are "short-term memory recall" and "long-term memory recall", whilst the independent variable is "lecture duration", which has four independent groups: "$30$ minutes", "$60$ minutes", "$90$ minutes" and "$120$ minutes"). Alternately, a one-way MANOVA could be  used to determine whether there is a difference in salary and bonuses based on degree type (i.e., the two dependent variables are "salary" and "bonuses", whilst the independent variable is "degree type", which has five groups: "business studies", "psychology", "biological sciences", "engineering" and "law").

	When there is a statistically significant difference between the groups of the independent variable, it is possible to determine which specific groups were significantly different from each other using post hoc tests. You need to conduct these post hoc tests because the one-way MANOVA is an omnibus\footnote{"omnibus" derives from the Latin word "for all". In plain language, you can interpret an omnibus test or omnibus statistic like an "overall test" - it is testing a number of things at once.}\index{omnibus test} test statistic and cannot tell you which specific groups were significantly different from each other; it only tells you that at least two groups were different.
	
	\subparagraph{Hotelling's $T$-squared test}\index{statistical tests!Hotelling's $T$-squared test}\mbox{}\\\\
	The "\NewTerm{Hotelling's $T$-squared test}\index{Hotelling’s $T$-squared test}" (Hotelling, 11931 according to holocene calendar) is the multivariate counterpart of the $T$-test. For example, let's say we wanted to compare how well two different sets of students performed in school. We could compare univariate data (e.g. mean test scores) with a $T$-test. Or, we could use Hotelling's $T$-squared to compare multivariate data (e.g. the multivariate mean of test scores, GPA, and class grades).

	Hotelling’s $T$-squared test is based on Hotelling’s $T^2$ distribution and forms the basis for various multivariate control charts in Industrial Engineering. As the reader will understand it at then end of the development presented below, Hotelling's $T$-squared test is simply a special case of MANOVA, in which only two batches (groups) are compared!
	
	More technically speaking Hotelling's $T^2$ test makes it possible to analyse whether the measures of random variables supposed to jointly follow a multivariate Normal law and therefore having a possible correlation (variance-covariance matrix) deviate considerably from Normality or not.
	
	To introduce this test, let us recall that we have demonstrated in the section of Statistics that the Multivariate Normal law could be written:
	
	We see that the following term in the parenthesis:
	
	giving a scalar can be seen as the unique random variable of the bivariate distribution (as we can do with the square of the random variable in the reduced centered univariate Normal law). So for a given value of $D$, we are "at a certain height" of the multidimensional Gaussian distribution.
	
	What will interest us here is that when we work with a multivariate distribution some variables have less weight than others because of their large standard deviation and the correlated variables tend to reduce the amplitude $Y$.
	
	Therefore, a very interesting subject in industrial practice (Hotelling control card and regressions) is then to know that it is the distribution law which describes:
	
	
	So let us deal now with this topic!
	
	The "\NewTerm{Mahalanobis distance}\index{Mahalanobis distance}" between two points $\vec{x}$ and $\vec{y}$ is defined as
	
	Thus, the squared Mahalanobis distance of a random vector $\vec{x}=\mathcal{N}(\vec{\mu},\Sigma)$ and the center $\vec{\mu}$ of a multivariate Gaussian distribution is defined as:
	
	where $\Sigma$ is a $n\times n$ covariance matrix and $\vec{\mu}$ is the mean vector. In order to achieve a different representation of $D$ one can first perform an eigenvalue decomposition on $\Sigma^{-1}$ which is with (\SeeChapter{see section Linear Algebra page \pageref{inverse eigendecompsosition}}):
	
	where for recall  $\vec{u}_k$ is the $k$th eigenvector of the corresponding eigenvalue $\lambda_k$. Plugging the above relation back into:
	
	results in:
	
	Using the properties of transposed (\SeeChapter{see section Linear Algebra page \pageref{transposed matrix}}) matrices we get:
	
	where $Y_{k}$ is a new random variable based on an affine linear transform of the random vector $\vec{x}$. Indeed, we have:
	
	If we set:
	
	then we get:
	
	We can the say that $Y_k$ is the result of an affine transformation of $Z$.
	
	Note that $Y_{k}$ is now a random variable drawn from a univariate normal distribution $Y_{k}= \mathcal{N}\left(0, \sigma_{k}^{2}\right),$ where, according to:
	
	Therefore we get:
	
	If we insert into that last relation the following one:
	
	we get:
	
	since all eigenvectors $\vec{u}_{i}$ are pairwise orthonormal the dotted products $\vec{u}_{k}^T \vec{u}_{j}$ and $\vec{u}_{j}^T \vec{u}_{k}$ will be zero for $j \neq k$. Only for the case $j=k$ we get:
	
	since the norm $\left\|\vec{u}_{k}\right\|$ of a orthonormal eigenvector is equal to $1$. The squared Mahalanobis distance can then be expressed as:
		
	Now the Chi-square distribution with $n$ degrees of freedom is exactly defined as being the distribution of a variable which is the sum of the squares of  $n$ random variables being standard normally distributed. Hence, $D$ is Chi-square distributed with $n$ degrees of freedom.
	
	So if the random variables are really jointly Normally distributed according to a correlation matrix $\Sigma$ we then have:
	
	which is named the "\NewTerm{Hotelling $T^2$ statistics}\index{Hotelling $T^2$ statistics}".
	
	Then to a given confidence threshold we then have:
	
	that is the probability associated with a certain height (isocline) in the special case of the Normal bivariate distribution. So each quantile of the $\chi^2$ law corresponds for example in the bivariate special case to an elliptical isocline in the general case (when seen from above) or to a circle when the correlation is zero.
	
	So here what we are testing is:
	\[
	H_{0}:\left(\begin{array}{c}
	\mu_{11} \\	\mu_{12} \\	\vdots \\ 	\mu_{1 p}
	\end{array}\right)=\left(\begin{array}{c}
	\mu_{21} \\	\mu_{22} \\	\vdots \\ 	\mu_{2 p}
	\end{array}\right) \text { against } H_{1}:\left(\begin{array}{c}
	\mu_{11} \\ \mu_{12} \\ \vdots \\ \mu_{1 p}
	\end{array}\right) \neq\left(\begin{array}{c}
	\mu_{21} \\ \mu_{22} \\ \vdots \\ \mu_{2 p}
	\end{array}\right)
	\]
	Or, in other words:
	\begin{center}
	$H_{0}: \mu_{11}=\mu_{21}$ and $\mu_{12}=\mu_{22}$ and $\ldots$ and $\mu_{1 p}=\mu_{2 p}$
	\end{center}
	For the null hypothesis, that is fine only if the population means are identical for all of the variables. The alternative is that at least one pair of these means is different. This is expressed below:
	\begin{center}
		$H_{1}: \mu_{1 k} \neq \mu_{2 k}$ for at least one $k \in\{1,2, \cdots, p\}$
	\end{center}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Consider:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,387); %set diagram left start at 0, and has height of 387
		
		%Shape: Axis 2D [id:dp34402133625912956] 
		\draw  (188.67,211.23) -- (466.67,211.23)(216.47,34.83) -- (216.47,230.83) (459.67,206.23) -- (466.67,211.23) -- (459.67,216.23) (211.47,41.83) -- (216.47,34.83) -- (221.47,41.83) (246.47,206.23) -- (246.47,216.23)(276.47,206.23) -- (276.47,216.23)(306.47,206.23) -- (306.47,216.23)(336.47,206.23) -- (336.47,216.23)(366.47,206.23) -- (366.47,216.23)(396.47,206.23) -- (396.47,216.23)(426.47,206.23) -- (426.47,216.23)(211.47,181.23) -- (221.47,181.23)(211.47,151.23) -- (221.47,151.23)(211.47,121.23) -- (221.47,121.23)(211.47,91.23) -- (221.47,91.23)(211.47,61.23) -- (221.47,61.23) ;
		\draw   ;
		%Shape: Circle [id:dp9593199740703557] 
		\draw   (333.17,120.92) .. controls (333.17,118.75) and (334.92,117) .. (337.08,117) .. controls (339.25,117) and (341,118.75) .. (341,120.92) .. controls (341,123.08) and (339.25,124.83) .. (337.08,124.83) .. controls (334.92,124.83) and (333.17,123.08) .. (333.17,120.92) -- cycle ;
		%Shape: Circle [id:dp38613136463238074] 
		\draw   (349.17,137.92) .. controls (349.17,135.75) and (350.92,134) .. (353.08,134) .. controls (355.25,134) and (357,135.75) .. (357,137.92) .. controls (357,140.08) and (355.25,141.83) .. (353.08,141.83) .. controls (350.92,141.83) and (349.17,140.08) .. (349.17,137.92) -- cycle ;
		%Shape: Circle [id:dp7369438144980851] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (307.17,72.92) .. controls (307.17,70.75) and (308.92,69) .. (311.08,69) .. controls (313.25,69) and (315,70.75) .. (315,72.92) .. controls (315,75.08) and (313.25,76.83) .. (311.08,76.83) .. controls (308.92,76.83) and (307.17,75.08) .. (307.17,72.92) -- cycle ;
		%Shape: Circle [id:dp3352083765716847] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (333.17,161.92) .. controls (333.17,159.75) and (334.92,158) .. (337.08,158) .. controls (339.25,158) and (341,159.75) .. (341,161.92) .. controls (341,164.08) and (339.25,165.83) .. (337.08,165.83) .. controls (334.92,165.83) and (333.17,164.08) .. (333.17,161.92) -- cycle ;
		%Shape: Circle [id:dp17634068921057455] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (365.17,119.92) .. controls (365.17,117.75) and (366.92,116) .. (369.08,116) .. controls (371.25,116) and (373,117.75) .. (373,119.92) .. controls (373,122.08) and (371.25,123.83) .. (369.08,123.83) .. controls (366.92,123.83) and (365.17,122.08) .. (365.17,119.92) -- cycle ;
		%Straight Lines [id:da21920819377990775] 
		\draw    (371.67,83.83) -- (344.01,114.35) ;
		\draw [shift={(342.67,115.83)}, rotate = 312.18] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da034330815671298254] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ]   (391.67,166.83) -- (364.26,146.04) ;
		\draw [shift={(362.67,144.83)}, rotate = 37.18] [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		
		% Text Node
		\draw (373,73) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \overline{x}^{\,\prime} =( 8,6)$};
		% Text Node
		\draw (398,158) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,opacity=1 ] [align=left] {$\displaystyle H_{0} :\mu ' =( 9,5)$};
		
		\end{tikzpicture}	
	\end{figure}
	The data are given by:
	
	and the null hypothesis will be:
	
	Assuming data come from a multivariate normal distribution and independent observations (don't forget that $S$ is the traditional notation for the estimate of $\Sigma$):
	
	We have:
	
	Hence:
	
	The value we need for a threshold of $5\%$ is:
	
	We can compare our $T^2$ to $5.991$. We see already that we can't reject $H_0$ (see how close $\vec{\bar{x}}$ and $\vec{\mu}$ and in the figure).\\
	
	Alternatively we could compute the $p$-value easily with any spreadsheet software. This lead us to a $p$-value of $0.878$.
	\end{tcolorbox}

	\pagebreak
	\subsubsection{Equivalence tests}
	As the reader will probably have understood it from the preceding paragraphs, hypotheses tests (NHST) applied to the search for differences between groups do not technically allow us to conclude an equivalence simply because we do not reject the null hypothesis $H_0$. Yet we also showed that if the power of the test was typically above $80\%$ it did not make a problem to consider the null hypothesis $H_0$ as true but the problem is that for this we need most of times to consider large samples and in practice this is not always feasible and it can also lead to absurdities since the maths show that with large samples we almost systematically reject the null hypothesis $H_0$.

	The $p$-value can be used only to statistically reveal a posteriori the rejection of the null hypothesis $H_0$ in the vast majority of cases. To summarize this problem: «\textit{The absence of evidence, does not imply evidence of absence}». Expressed in other words in a very common case this gives (particular example!) that the non-rejection of the null hypothesis $H_0$ of equality of two means (for example) does not imply the equality of means!!!
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Some people quite often state wrongly that \og Absence of evidence is NOT evidence of absence. \fg{} Why do they do that? Perhaps because they do not know better? And with the proof below they get the possibility of knowing for sure that "Absence of evidence IS evidence of absence!" and nothing else....\\
	
	Definitions:
	\begin{itemize}
		\item[D1.] $A$ is evidence of $B$: $P(B \mid A)>P(B \mid \neg A)$

		\item[D2.] Absence of evidence: $a=\neg A$

		\item[D3.] Absence $b=\neg B$
	\end{itemize}
	\begin{equation}
		\begin{gathered}
		P(B \mid A)>P(B \mid \neg A) \quad \Leftrightarrow \quad 1-P(\neg B \mid A)>1-P(\neg B \mid \neg A)\\
		\Downarrow\\
		P(\neg B \mid \neg A)>P(\neg B \mid \neg \neg A) \quad \Leftrightarrow \quad P(\neg B \mid A)<P(\neg B \mid \neg A)\\
		\Downarrow\\
		P(b \mid a)>P(b \mid \neg a)
		\end{gathered}
	\end{equation}
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{tcolorbox}

	With the advent of generic medicines from the 11960s (holocene calendar) in the pharmaceutical field, the importance of highlighting equivalences (we are talking instead of "\NewTerm{bioequivalences}\index{bioequivalence}") has taken a growing place and organizations like the F.D.A. (Federal Drug Administration\index{federal drug administration} in U.S.A.), EMA (European Medicines Agency\index{european medicines agency}) or WHO (World Health Organization\index{world health organization}) have common guidelines that lead to the approval of generic only if the equivalence is shown under certain empirical conditions but at least standardized according to a consensus of experts.

	Thus, most often the following three conditions are required\footnote{Still by respecting the scientific publication rules introduced at the beginning of this book}:
	\begin{enumerate}
		\item A comparison criterion is required

		\item A confidence interval is required

		\item An a priori limit of bioequivalence is required
	\end{enumerate}
	Let us see a companion example to grasp the concept:
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us consider the Student's $T$ test of the difference of the averages of two unpaired samples $\{$Test, Reference$\}$ which we had demonstrated above:
	
	We then know the following two bounds of the confidence interval:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	We recognize here a special case of "TOST" test for "\NewTerm{Two-One Sided Test}\index{two-one sided test}" seen earlier.
	\end{tcolorbox}
	We will assume that the F.D.A. request:
	
	Consider that the measurement have given us:
	
	Therefore, we have the equivalence limits that will be:
	
	The confidence intervals are given by:
	
	Therefore:
	
	As we have:
	
	The equivalence is then considered by experts consensus as "proven".
	\end{tcolorbox}
	

	\subsubsection{Cochran C-test}\index{statistical tests!Cochran C-test}
	The Cochran C-test has for purpose to verify the homogeneity of variances for several populations. This is a preliminary or subsequent test (post hoc) helpful test to do before making a balanced ANOVA (balanced) analysis and is recommended by the standard ISO 5725 (as the Tukey's test that we will see much further) !
	
	Although the idea of the Cochran C-test is empirical, it is nevertheless intuitive as are the definitions of Dixon and Grubbs tests. Why then do we present in this book in details the proof of the Cochran C-test when we have mentioned that we would not do this for the test of Grubbs and Dixon because the approach of these latter was also empirical? The reason is simple in fact: the test of Grubbs and Dixon require, at least as far as we know, Monte Carlo simulations to determine the critical values of rejection or acceptance of the null hypothesis, while the critical value of Cochran C-test can be obtained relatively easily analytically.
	
	That said ... we define the Cochran C-test by the ratio:
	
	where the $S_i$ are the unbiased variances of the $N$ different sources of data, each composed of $n$ samples and the null hypothesis is intuitively the equality of variances against the alternative hypothesis that one of the variance is too big (that is to say: bad) and dismissed because considered as an outlier variance.
	
	The ISO 5725 recommends to repeat this test until there is no longer any aberrant variance (therefore too large AND far from other variances).
	
	To determine the critical value, let us invert the definition of Cochran C-test and do some basic algebraic manipulations:
	
	We note that pretty much the second term of the last equality looks like a Fisher law. As the Fisher law is not stable by the addition, we should find a way to turn the term:
	
	into a single variance. The idea is then relatively simple but still had someone to think about it... We know the $S_i$ are non-biased variances equation with a factor $1/(n-1)$. Therefore if the $N$ samples (levels) are independent, the overall variance is then by stability of the Normal distribution and by taking the traditional notations  of the ANOVA:
	
	Therefore:
	
	We recognize in the last equality the ratio of two squared variances. We then have identically to what we've proved in our study of the one-way fixed factor ANOVA without replication:
	
	and therefore it comes:
	
	which is therefore independent of $j$ and therefore the left-tail Cochran' C-test (since by definition the Cochran's ratio should be as small as possible) will have for critical value:
	
	\label{bonferroni-sidak multiple tests correction}If we consider a test with a significance level of $1-\alpha$ (thus corresponding to the cumulative probability of not making a Type I error) and we reiterate it independently again as second time. Then, if the tests are independent, following the axiom of probabilities, the probability of not making a Type I error will be given by the product of probabilities:
	
	and so on for $n$ tests. We notice so quickly that the cumulative probability of not making a Type I error decreases very quickly. For example, for $10$ independent repeated tests with a $5\%$ level, then we have:
	
	which is catastrophic! So if we want a level of confidence on repeated tests of a certain value which we will denote $\alpha_r$, it is clear that we must solve the following equation:
	
	Therefore (relation sometimes named "Šidàk equation\index{Sidak equation}"):
	
	and with a Taylor expansion to the second order it comes (\SeeChapter{see section Sequences and Series page \pageref{taylor series}}):
	
	that we name "\NewTerm{Bonferroni approximation}\index{Bonferroni approximation}" or sometimes also "\NewTerm{Boole approximation}\index{Boole approximation}" or "\NewTerm{Dunn approximation}\index{Dunn approximation}". 
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In statistics, "\NewTerm{family-wise error rate}\index{family-wise error rate}" (FWER) is the probability of making one or more false discoveries, or type I errors when performing multiple hypotheses tests\index{multiple hypotheses tests} (typically during meta-analysis).
	\end{tcolorbox}
	So in the end, we have for the Cochran C-test:
	
	that we can compute with the English version of Microsoft Excel 14.0.6123 using the following formula:
	
	\begin{center}
	\texttt{=1/(1+(N-1)/FINV(ALPHA/N,n-1,(N-1)*(n-1)))}
	\end{center}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	We benefit here from assuming that all tests are independent of each other. In practical applications, that is often not the case. Depending on the correlation structure of the tests, the Bonferroni correction could be extremely conservative, leading to a high rate of false negatives!!!
	\end{tcolorbox}
	
	\subsubsection{Family-wise error rate (adjusted $p$-values)}
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] The "\NewTerm{family-wise error rate}\index{family-wise error rate}\label{family-wise error rate}" (FWER) (or "\NewTerm{per-experiment error rate}\index{per-experiment error rate}") is the probability of making any Type I errors at all (typically during meta-analysis).
		
		\item[D2.] The "\NewTerm{false discovery rate}\index{false discovery rate}" (FDR) is the expected proportion of false rejections out of all rejections (typically also during meta-analysis).
	\end{enumerate}

	Before doing the maths, let us recall the famous case of Nosek's team that invited researchers to take part in a crowd-sourcing data analysis project. The setup was simple. Participants were all given the same data set and prompt: Do soccer referees give more red cards to dark-skinned players than light-skinned ones? They were then asked to submit their analytical approach for feedback from other teams before diving into the analysis.

	Twenty-nine teams with a total of $61$ analysts took part. The researchers used a wide variety of methods, ranging - for those of you interested in the methodological gore - from simple linear regression techniques to complex multilevel regressions and Bayesian approaches. They also made different decisions about which secondary variables to use in their analyses.

	Despite analysing the same data, the researchers got a variety of results. Twenty teams concluded that soccer referees gave more red cards to dark-skinned players, and nine teams found no significant relationship between skin color and red cards.
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/arithmetics/repetability.jpg}
		\caption{Same data, different conclusions (purpose of meta-analysis)}
	\end{figure}
	The variability in results wasn't due to fraud or sloppy work. These were highly competent analysts who were motivated to find the truth, said Eric Luis Uhlmann, a psychologist at the Insead business school in Singapore and one of the project leaders. Even the most skilled researchers must make subjective choices that have a huge impact on the result they find. 
	
	That's why knowing how to deal with multiple $p$-value is critical and especially when lives are concerned by the result (remember the meta-analysis for the influence of Hydroxychloroquine during the COVID-19 period in year 12020 according to holocene calendar).
	
	The most commonly used method which controls FWER at level $\alpha$ is the Bonferroni's method seen just earlier above. Let us make some recall about that latter!
	
	\begin{itemize}
		\item The probability of making a Type I error when dealing only with a specific test is denoted $\alpha[\text{PT}]$ (pronounced "\NewTerm{alpha per test}"). It is also named the "\NewTerm{testwise alpha}".
	
		\item The probability of making at least one Type I error for the whole family of tests denoted $\alpha[\text{PF}]$ (pronounced "\NewTerm{alpha per family of tests}"). It is also named the "\NewTerm{familywise alpha}" or the "\NewTerm{experimentwise alpha}".
	\end{itemize}
	During our study of the Cochran C-test earlier above (see page \pageref{bonferroni-sidak multiple tests correction}) we have seen that the Bonferroni correction for multiple tests was then written by (assuming independence!):
	
	where $n$ is  the number of tests.
	
	Also during our study of the Cochran C-test above we have derived the following first order Taylor approximation:
	 
	The Šidàk-Bonferonni equations can be used to find the value of $\alpha[\text{PT}]$ when $\alpha[\text{PF}]$ is fixed. For example, suppose that we want to perform four independent tests, and we want to limit the risk of making at least one Type I error to an overall value of $\alpha[\text{PF}]= 0.05$, we will consider a test significant if its associated probability is smaller than (in the case of four associated tests):
	
	With the Bonferonni approximation, a test reaches significance if its associated probability is smaller than:
	
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Sometimes we may have:
	 
	This is why it is not uncommon in the literature to find that latter relation under the form:
	
	or:
	
	that simply means that if $n\alpha[\text{PT}]>1$, then we replace it by $1$.
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Consider four null hypotheses $H_{1}, \ldots, H_{4}$ with $p$-values:
	\begin{gather*}
		p_{1}=0.01, p_{2}=0.04, p_{3}=0.03, p_{4}=0.005
	\end{gather*}
	to be tested at significance level $\alpha=0.05$. First we test $0.01\cdot 4=0.04<0.05$:  reject $H_{0(1)}$. After we test $0.04\cdot 4=0.16>0.05$: Don't reject $H_{0(2)}$. After we test $0.03\cdot 4=0.12>0.05$: Don't reject $H_{0(3)}$. Finally we test $0.005\cdot 4=0.02<0.05$: Reject $H_{0(4)}$. 
	\end{tcolorbox}

	The Bonferroni method is guaranteed to control FWER, but it has a big problem. It greatly reduces our power to detect real differences. For example, suppose the effect size is $2$ and we are doing a Student $T$-test, rejecting the null hypothesis for $p < 0.05$. With $10$ observations per group, the power is $99\%$. Now suppose we have $1,000$ tests, and use the Bonferroni method. That means that to reject the null hypothesis, we need $p < 0.00005$. The power is now only $29\%$. If we have $10,000$ tests (which is small for genomics studies) the power is only $10\%$.
	
	\paragraph{Holm-Bonferroni Correction}\mbox{}\\\\
	The "\NewTerm{Holm-Bonferroni correction}\index{Holm-Bonferroni correction}" method is as follows:
	\begin{itemize}
		\item Let $H_{1}, \ldots, H_{m}$ be a family of $m$ null hypotheses and $p_{1}, \ldots, p_{m}$ the corresponding $p$-values. 
		
		\item Start by ordering the $p$-values (from lowest to highest) $p_{(1)} \ldots p_{(m)}$ and let the associated hypotheses be $H_{(1)}, \ldots, H_{(m)}$.
		
		\item For a given significance level $\alpha$, let $k$ be the minimal index such that $p_{(k)}>\frac{\alpha}{m+1-k}$. 

		\item Reject the null hypotheses $H_{(1)}, \ldots, H_{(k-1)}$ and do not reject $H_{(k)}, \ldots, H_{(m)}$.
		
		\item If $k=1$ then do not reject any of the null hypotheses and if no such $k$ exist then reject all of the null hypotheses.
	\end{itemize}
	 This method ensures that FWER $\leq \alpha,$ where FWER is the family-wise error rate.
	 
	 \begin{dem}
	 Holm-Bonferroni controls the FWER as follows: Let $H_{(1)}, \ldots, H_{(m)}$ be a family of hypotheses, and $p_{(1)} \leq p_{(2)} \leq \ldots \leq p_{(m)}$ be the sorted $p$-values. Let $I_{0}$ be the set of indices corresponding to the (unknown) true null hypotheses, having $m_{0}$ members.

	Let us assume that we wrongly reject a true hypothesis. We have to prove that the probability of this event is at most $\alpha$.
	
	 Let $h$ be the first rejected true hypothesis (first in the ordering given by the Holm-Bonferroni test). Then $H_{(1)}, \ldots, H_{(h-1)}$ are all rejected false hypotheses and $h-1 \leq m-m_{0}$. From there, we get:
	 
	 as $h$ is rejected, we have:
	 
	 by definition of the test. Using the previous relation the right hand side is at most $\alpha/m_0$. Thus, if we wrongly reject a true hypothesis, there has to be a true hypothesis with $p$-value at most $\alpha/m_0$.
	 
	 So let us define the random variable $A=\left\{P_{i} \leq \frac{\alpha}{m_{0}} \text { for } i \in I_{0}\right\} .$ Whatever the (unknown) set of true hypotheses $I_{0}$ is, we have $P(A) \leq \alpha$ (by the Bonferroni inequalities). Therefore, the probability to reject a true hypothesis is at most $\alpha$.
	 \begin{flushright}
		$\blacksquare$  Q.E.D.
	 \end{flushright}
	 \end{dem}
	 
	 
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Consider four null hypotheses $H_{1}, \ldots, H_{4}$ with $p$-values:
	\begin{gather*}
		p_{1}=0.01, p_{2}=0.04, p_{3}=0.03, p_{4}=0.005
	\end{gather*}
	to be tested at significance level $\alpha=0.05$. Since the procedure is step-down, we first test $H_{4}=H_{(1)},$ which has the smallest $p$-value $p_{4}=p_{(1)}=0.005$. The $p$-value is compared to:
	\begin{gather*}
		\dfrac{\alpha}{m+1-k}=\dfrac{\alpha}{4+1-1}=\dfrac{0.05}{4}=0.0125
	\end{gather*} 
	the null hypothesis is rejected and we continue to the next one. Since $p_{1}=p_{(2)}=0.01$ and:
	\begin{gather*}
		\dfrac{\alpha}{m+1-k}=\dfrac{\alpha}{4+1-2}=\dfrac{0.05}{3}=0.0167
	\end{gather*} 	
	we reject $H_{1}=H_{(2)}$ as well as $0.01<0.167$ and continue. The next hypothesis $H_{3}$ is not rejected since $p_{3}=p_{(3)}=0.03$ and:
	\begin{gather*}
		\dfrac{\alpha}{m+1-k}=\dfrac{\alpha}{4+1-3}=\dfrac{0.05}{2}=0.025
	\end{gather*}
	then $0.03>0.025$. We have our $k$ that is equal to $k=3$. We stop testing and conclude that $H_{1}$ and $H_{4}$ are rejected and $H_{2}$ and $H_{3}$ are not rejected while controlling the family-wise error rate at level $\alpha=0.05 .$ Note that even though $p_{2}=p_{(4)}=0.04<0.05=\alpha$ applies, $H_{2}$ is not rejected. This is because the testing procedure stops once a failure to reject occurs.
	\end{tcolorbox}
	
	\paragraph{Holm–Šidák Correction}\mbox{}\\\\
	As we know the Bonferroni correction seen earlier above is only an approximation (first order Taylor series) of the Šidák correction.
	
	Therefore it is possible to replace:
	
	with: 
	
	resulting in a slightly more powerful test and named the "\NewTerm{Holm–Šidák Correction}\index{Holm–Šidák Correction}".
	
	\subsubsection{Adequation Tests (Goodness of Fit tests)}\label{goodness of fit tests}
	The goodness of fit (GoF) of a statistical model describes how well it fits a set of observations. Measures of goodness of fit typically summarize the discrepancy between observed values and the values expected under the model in question. Such measures can be used in statistical hypothesis testing, e.g. to test for normality of residuals, to test whether two samples are drawn from identical distributions (see Kolmogorov–Smirnov test further below), or whether outcome frequencies follow a specified distribution (see Pearson's chi-squared test below). 
	
	\paragraph{Pearson's Chi-squared GoF test}\index{statistical tests!Pearson's $\chi^2$ GoF test}\mbox{}\\\\
	We will study here our first GoF nonparametric test, certainly one of the most known and most simple one (which applies only to non-censored data as far as we know).
	
	To introduce this test, assume that a random variable follow a probability distribution $P$. If we draw a sample from the population corresponding to this law, the observed distribution, named "\NewTerm{sampling distribution}\index{sampling distribution}", always deviate more or less of the theoretical distribution, taking into account the sampling fluctuations.
	
	Generally, we do not know the shape of the law $P$, nor the value of its parameters. It is the nature of the studied phenomenon and the analysis of the observed distribution that allow to choose a law likely to be adequate and afterwards to estimate the parameters.
	
	The differences between the theoretical law and the observed distribution can be attributed either to sampling fluctuations, or to the fact that the phenomenon does not follow, in reality, the supposed law.
	
	Basically, if the gaps are small enough, we will assume they are due to random fluctuations and we will accept (not reject in fact!) the supposed law. On the contrary, if the gaps are too high, we conclude that they can not be explained solely by the fluctuations and the phenomenon does not follow the supposed law (reject the null hypothesis).
	
	To assess these gaps and to make a decision, we need:
	\begin{enumerate}
		\item Define the measure of the distance between the empirical distribution and the theoretical resulting from the retention law.
		
		\item Determine the probability law followed by the random variable giving the distance (in fact sadly not reject the null hypothesis).
		
		 \item State a decision rule to tell from the observed distribution, if the law chosen is acceptable or not.
	\end{enumerate}
	First, we will need for this purpose the central limit theorem and secondly recall that during the construction of the Normal distribution, we have proved that the variable:
	
	follow a Normal distribution centered reduced when $n$ approaches infinity (Laplace condition) and the probability $p$ was very small.
	
	In practice, the approximation is quite acceptable... in some companies... and when $np>5$ and $p\leq 0.5$ therefore (it was one of the terms that needed to tend to zero when we made the proof):
	
	For example in the two figures below where we represented the binomial laws approached by the Normal associated laws, we have on the left $n=60,p=1/6,np=10$ and on the right $n=40,p=0.05,np=2$:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/binomial_normal_approximation.jpg}
		\caption{Approach of binomial functions by associated Normal functions}
	\end{figure}
	Finally, remember that we have proved that the sum of squares of $n$ linearly independent reduced centered Normal random variable follows a chi-square with $n$ degrees of freedom denoted by $\chi^2(n)$.
	
	Now consider a random variable $X$ that follows a theoretical distribution function (continuous or discrete) $P$ and let us draw a sample of size $n$ in the population corresponding to this law $P$.
	
	The $n$ observations are distributed along $k$ terms (class values) $C_1, C_2, \ldots , C_k$, whose probabilities $p_1, p_2, \ldots , p_k$ are determined by the distribution function $P$ (refer to the example with the Henry straight line at page \pageref{Henri line}).
	
	For each modality $C_i$, the empirical sample size is a binomial random variable $k_i$ of parameters:
	
	This number $k_i$ corresponds indeed to the number of successful events: "result equal to the modality $C_i$" of probability $p_i$, obtained during the sampling on $n$ items of the experimental batch (and not in the population of the theoretical law as before!).
	
	We have proved earlier above during our the study of the binomial law that his expected mean was:
	
	represents the expected theoretical sample size of the modality $C_i$ and its variance is given by (when $n$ is very big and $p_i$ very small):
	
	Its standard deviation is therefore:
	
	Under these conditions, provided that the modality $C_i$ has a size $np_i$ of  at least equal to $5$, the reduced centered variable:
	
	between empirical and theoretical sample size can be approximately regarded as a reduced centered Normal variable as we have proved it earlier above. It is also named the "\NewTerm{Pearson's residual}\index{Pearson's residual}" for value $i$, and it compares the observed with the expected counts. The sign (positive or negative) indicates whether the observed frequency $k_i$ is higher or lower than the value fitted under the model, and the magnitude indicates the degree of departure. When data do not fit a model, examination of the Pearson residuals often helps to diagnose where the model has failed.
	
	We now define the variable\label{chi-square test of adjustment}:
	
	where $k_i$ is often named "\NewTerm{experimental frequency}\index{experimental frequency}" and $np_i$ "\NewTerm{theoretical frequency}\index{theoretical frequency}".
	
	If we take the square it is because that in a simple sum certain terms would cancel by opposing effects and thus would mask the differences, if we take the sum of the absolute values the statistical table of $D$ would be difficult to build and the test would be not very robust because of the small gap of distances. The square allows not only to have an easy statistical table of $D$ which is simple since it is based on a law with a single parameter, as we shall see, and the square also sufficiently increases the test's robustness.
	
	Note that this variable is also sometimes (somewhat unfortunately) denoted by:
	
	or more often (the $E_i$ below is not the same as the previous $E_i$ just above!!!):
	
	where:
	
	is commonly named, as already mentioned earlier above, the "\NewTerm{Pearson's residual}\index{Pearson's residual}".
	
	This variable $D$, sum of squared variables $E_i$, provides a measure of a "distance" between empirical and theoretical distribution and empirical distribution. Let us note however that this is not a distance in the usual mathematical sense (topological).
	
	Recall that $D$ can therefore also be written:
	
	$D$ is therefore the sum of squares of $N$ reduced centered Normal random variables linked by the single linear relation:
	
	where $n$ is the sample size. So $D$ follows a chi-square distribution, but with $N-1$ degrees of freedom, so a degree less because of the unique linear relation between them! Indeed, recall that the degree of freedom is the number of independent variables in the sum and not just the number of summed terms.
	
	Therefore:
	
	We name this test the "\NewTerm{nonparametric Chi-square test}\index{nonparametric Chi-square test}" or "\NewTerm{Pearson's Chi-square test}\index{Pearson's Chi-square test}" or "\NewTerm{Chi-square test of adjustment}\index{chi-square test of adjustment}" or "\NewTerm{Karl Pearson's test}\index{Karl Pearson's test}" or "\NewTerm{goodness of fit Chi-square test}\index{goodness of fit Chi-square test}"...
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	There are plenty of statistical tests with that contains "chi-square" in it. For example:
	\begin{itemize}
		\item Chi-square Goodness-of-fit test (allow us to determine whether a specified population distribution is valid)
		
		\item Chi-square test of association/independence (allow us to determine whether the distribution of one variable has been influenced by another variable)
		
		\item Chi-square test of homogeneity (allow us to compare two or more population proportions)
		
		\item Chi-square test for outliers (allow us to detect if there is at least one outlier in comparison of a specific distribution)
		
		\item Chi-square test for difference of two count data (special case of homogeneity test with only one category!)
	\end{itemize}
	Even if all those tests have different purposes, their mathematical framework and procedure are EXACTLY the same and therefore any of these four tests can be run to get at the same time the four conclusions with the same $p$-values and the same critical chi-square values!!!
	\end{tcolorbox}	
	
	We also see by the way that the squared Pearson residual is the individual contribution to the Pearson $\chi^2$ statistic.
	
	Then, the usage is to determine the value of the Chi-square distribution with $N-1$ degrees of freedom with a $5\%$ probability of being exceeded. Thus, in the hypothesis that the studied phenomenon, follows the theoretical distribution $P$, so there is a $95\%$ cumulative probability that the variable $D$ takes a value less than the one given by the Chi-Square distribution.
	
	If the value of the law of Chi-square obtained from the sample is smaller than that corresponding to $95\%$ of cumulative probability, we do not reject the null hypothesis that the phenomenon follows the law $P$.
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.}  The fact that the assumption of the law $P$ is accepted does not mean that this hypothesis is true, merely that the information given by the sample does not allow to reject it. Similarly, the fact that the assumption of the law $P$ is rejected does not necessarily mean that this assumption is false but that the information provided by the sample rather lead to the conclusion that the inadequacy of such a law.\\
	
	\textbf{R2.} For the variable $D$ to follow a chi-square law, it is necessary that the theoretical values $np_i$ of the different modalities $C_i$ are at least equal to $5$, that the sample was drawn randomly (no correlation) and that the probabilities $p_i$ are not  close to zero.
	\end{tcolorbox}	
	This goodness of fit test however suffers from a major issue: it requires to group the measurements in classes $C_i$ and in practice there is no absolute theorem (at least as far we know) to choose the number of classes (and in full width) excepted the Sturge Rule proved earlier. It is this reason that make the chi-squared goodness of fit test is reserved for discrete distributions where the problem of the choice of classes not arise.
	
	However, we will need to create goodness of tests that do not require the use of classes and we will see just after ad hoc tools for this purpose (Kolmogorov-Smirnov or Anderson-Darling to name only the most important one).
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Suppose that the births in a hospital for a period of time, are as follows:
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|p{2.5cm}|c|c|c|c|c|c|c|c|}
					\hline
					\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Day}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{M}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{T}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{W}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{T}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{F}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{S}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{S}}  & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Total}}\\ \hline
					\cellcolor[gray]{0.75}\textbf{Observations} & $120$ & $130$ & $125$ & $128$ & $80$ & $70$ & $75$ & $728$ \\ \hline
			\end{tabular}
		\end{center}
	\end{table}	
	We note that there were a total of $728$ births. We ask then the following question: How many should there be births, in theory, every day if there is no difference between days? This represents the null hypothesis $H_0$. In fact the null hypothesis states that the differences between the observed frequencies and conceptual frequencies are relatively small. We take for granted that if there is no difference there should be the same number of births each day. Since there are a total of $728$ births for $7$ days in theory there should be $728/7 = 104$ births every day. So now we have the following table:
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|p{2.5cm}|c|c|c|c|c|c|c|c|}
					\hline
					\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Day}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{M}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{T}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{W}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{T}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{F}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{S}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{S}}  & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Total}}\\ \hline
					\cellcolor[gray]{0.75}\textbf{Observations} & $120$ & $130$ & $125$ & $128$ & $80$ & $70$ & $75$ & $728$ \\ \hline
					\cellcolor[gray]{0.75}\textbf{Expected} & $104$ & $104$ & $104$ & $104$ & $104$ & $104$ & $104$ & $104$ \\ \hline
			\end{tabular}
		\end{center}
	\end{table}	
	The total of observed frequencies equals the total expected frequencies. The purpose is therefore to examine the difference between the observed and expected frequencies (assumed to follow a uniform law in this special case) using the Chi-square relation. In other words, we do a fit test between an empirical distribution function (observed) and the uniform distribution function. Then we have:
	
	The $\chi^2$ is therefore $43.49$. As such this number means little. This result should be interpreted with the help of the table of critical values of the $\chi_6^2$. Without using the table we understand that it is very unlikely that the observed frequency and theoretical frequency are identical. We accept that there may be some difference (we therefore reject the null hypothesis $H_0$ in favour of the alternative hypotheses $H_A$).\\
	
	So do not forget that this test only applies to uncensored data, that is to say for which the intervals are closed!
	\end{tcolorbox}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The above test is also used a feature selection technique in Machine Learning and is then named "\NewTerm{$\chi^2$ feature selection}\index{$\chi^2$ feature selection}". The idea is that if we have a dataset with too many columns (categorical or continuous), we which to eliminated some of them by investigating if they are statistically dependent (then we can eliminate one of them) or independent (then we can't eliminate any of them). Notice that to run this technique on continuous variables (columns), we can just group them into bins!
	\end{tcolorbox}
	
	\paragraph{$G$-test GoF}\index{statistical tests!$G$-test GoF}\mbox{}\\\\
	We use the "\NewTerm{$G$–test of goodness-of-fit}\index{$G$–test of goodness-of-fit}" or also named "\NewTerm{$G^2$-test}\index{$G^2$-test}" (that is a special case of the "likelihood ratio test" or "log-likelihood ratio test" as we will see further below at page \pageref{likelihood ratio tests}) when we have one nominal variable and that we want to see whether the number of observations in each category fits a theoretical expectation, and the sample size is large and the observed total of counts and the expected total of counts is equal!
	
	The statistical null hypothesis is that the number of observations in each category is equal to that predicted by a theory, and the alternative hypothesis is that the observed numbers are different from the expected. The null hypothesis is usually an extrinsic hypothesis, where we know the expected proportions before doing the experiment. 
	
	The $G$-test statistic is proportional to the Kullback–Leibler divergence (\SeeChapter{see section Statistical Mechanics page \pageref{kullback-leibler divergence}}) of the theoretical distribution from the empirical distribution:
	
	Let us put now:
	
	with the constraint that:
	
	so that the total number of counts stays the same (i.e. $\sum_i O_i=\sum_i E_i=N$). The $G$-test is then:
	
	If we Taylor expand this around:
	
	noticing that this assume $E_i\gg \delta_i$ and hence $E_i\gg 0$ (as for the chi-square test of adequation seen just earlier), then we know that for the logarithm (\SeeChapter{see section Sequences and Series page \pageref{usual maclaurin developments}}):
	
	Then we get:
	
	and so, we see that $G=\chi^2$ when:
	\begin{itemize}
		\item $O_i$ is close to $E_i$
		\item $\sum_i E_i=\sum_i O_i=N$
		\item $E_i\gg \delta_i$ ie $E_i\gg 0$
	\end{itemize}
	However, the more $O_i$ and $E_i$ are different, the less well this approximation will work, and the $\chi^2$ will tend to compute erroneous answers. The effects of a single outlier in a small sample set will be more pronounced, which explains why the $\chi^2$ often fails in situations with little data.
	
	Since the $\chi^2$ value is just an approximation to the $G$-value, the $G$-value can also be used in the chi-squared probability test. It seems that actually the $G$-test $p$-value need to be computed using Monte Carlo techniques.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	If the expected number of observations in any category is too small, the $G$–test may give inaccurate results, and we should use an exact test instead (Fisher exact test in case of independence as seen at page \pageref{exact Fisher test}, McNemar test in case of dependence as seen at page \pageref{mcnemar test}). In practice, it's a good idea to compute both $\chi^2$ and $G$ to see if they lead to similar results. If the resulting $p$-values are close, then we can be fairly confident that the large-sample approximation is working well. Obviously the $G$–test of goodness-of-fit assumes independence!
	\end{tcolorbox}

	\paragraph{Kolmogorov-Smirnov GoF test}\index{statistical tests!Kolmogorov-Smirnov GoF test}\mbox{}\\\\
	In statistics, the Kolmogorov-Smirnov test is also a goodness of fit test (especially well known as normality test) based on an empirical distance used to determine whether the distribution of a sample follows a well known law given by a continuous distribution function (or for comparing two samples and check if they are dependent or not as similar or not). This test, as well as that of chi-squared GoF test is only valid for non-censored data (at least not without correction obtained by numerical simulations).
	
	To introduce this test, we chose the Lilliefors approach that give the possibility to avoid complex calculations. Furthermore, softwares that provide the "\NewTerm{Lilliefors GoF test}\index{Lilliefors goodness of fit test}" do not offer the Kolmogorov-Smirnov test since the latter is correct only asymptotically (which is the case of the software Tanagra 4.14).
	
	Imagine we want to build a nonparametric GoF test who works for both discrete and continuous laws without suffering the same problem as the Chi-square GoF test (clustering in classes).
	
	To build this test, we start from the empirical distribution function already defined at the beginning of this section and given for reminder by:
	
	that obviously belong to the interval $[0,1]$.
	
	Let us now denote by $F(x)$, the true supposed law which analytical expression is known and with which we would like to compare $\hat{F}(x)$ and build the distance:
		
	The reference distribution may, however, also originate from another measurement sample. The idea is then simply to compare two empirical distributions. We speak then of "\NewTerm{Kolmogorov-Smirnov test for $2$ independent samples}\index{Kolmogorov-Smirnov test for $2$ independent samples}". Some softwares also manage empirically the case where the two samples do not have the same size.
	
	The problem with this choice of distance is ... what $x$ should we then choose to make a test? Well to answer it is simple to see that it would be foolish to take a $x$ for which this distance is minimal, because have a $D_n$ that can be almost zero does not add much information... Therefore, we rather postponed towards the greatest absolute deviation. Which brings us to redefine the distance $D_n$ as follows:
	
	where $D_n$ is named "\NewTerm{Kolmogorov-Smirnov empirical distribution}\index{Kolmogorov-Smirnov empirical distribution}" (for sure we should prove rigorously that it is really a distribution ... but for now it is too complex in terms of the content of this book however this can be verified by numerical simulations!). Before going further with respect to the theory, let's look at a practical example (because the example is long we will not put it into the conventional box).
	
	Suppose we measured the following five values:
	
	thus ordered:
	
	We want to test the following null hypothesis:
	
	where $\Phi(x)$ is as usual the distribution function of the Normal centered reduced distribution.
	
	The empirical distribution function will be given by:
	
	Then we traditionally build the following table:
	\begin{table}[H]
	\begin{center}
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|c|c|c|c|}
				\hline
\multicolumn{1}{c}{\cellcolor[gray]{0.75}$x$} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}$\hat{F}(x)$} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}$\Phi(x)$} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}$|\hat{F}(x)-\Phi(x)|$} \\ \hline
		$-1.2^{-}$ & $0$ & $0.115$ & $0.115$\\ \hline
		$-1.2$ & $0.2$ & $0.115$ & $0.085$\\ \hline
		$-1.0^{-}$ & $0.2$ & $0.159$ & $0.041$\\ \hline
		$-1.0$ & $0.4$ & $0.159$ & $0.241$ \\ \hline
		$-0.6^{-}$ & $0.4$ & $0.274$ & $0.126$ \\ \hline
		$-0.6$ & $0.6$ & $0.274$ & \textbf{0.326}\\ \hline
		$+0.2^{-}$ & $0.6$ & $0.580$ & $0.020$\\ \hline
		$+0.2$ & $0.8$ & $0.580$ & $0.220$\\ \hline
		$+0.8^{-}$ & $0.8$ & $0.788$ & $0.012$\\ \hline
		$+0.8$ & $1$ & $0.788$ & $0.212$\\ \hline
	\end{tabular}
	\end{center}
	\end{table}
	Often associated with the following graph comparing empirical (in red) and theoretical (in blue) distributions:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/ks_test_empirical_vs_theoretical.jpg}
		\caption{Representation of the approach of the Kolmogorov-Smirnov GoF}
	\end{figure}
	We then observe that the maximum deviation is $0.326$. We will denote that for after:
	
	that some softwares such as Minitab or \texttt{R} denote with the abbreviation: KS.
	
	The reader will have noticed that the biggest deviation above the curve is measured by:
	
	The largest deviation below the curve is measured by:
	
	The biggest deviation is then:
	
	But what can we do with this value? To what can we compare it? Well the idea is relatively simple and involves generating $n$ values (thus $5$ in our example) from the distribution law $F(x)$ of the null hypothesis $H_0$ and compare them to themselves. In other words, to make a Monte Carlo simulation (\SeeChapter{see section Numerical Methods page \pageref{monte carlo simulations}}).
	
	Thus, in our example, we generate $5$ random value values of $\mathcal{N}(0,1)$ which gives us example with Microsoft Excel 11.8346:
	
	\begin{center}
	\texttt{=NORM.S.INV(RANDBETWEEN(0,1000000)/1000000)}
	\end{center}
	
	We obtain then $5$ values of $Z$ (remember it is the usual notation of a random variable of a Normal centered reduced distribution) which ordered will be for example:
	
	and we repeat the same table as before:
	\begin{table}[H]
	\begin{center}
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|c|c|c|c|}
				\hline
\multicolumn{1}{c}{\cellcolor[gray]{0.75}$x$} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}$\hat{F}(x)$} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}$\Phi(x)$} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}$|\hat{F}(x)-\Phi(x)|$} \\ \hline
		$-1.427^{-}$ & $0$ & $0.077$ & $0.115$\\ \hline
		$-1.427$ & $0.2$ & $0.077$ & $0.085$\\ \hline
		$+0.082^{-}$ & $0.2$ & $0.533$ & $0.041$\\ \hline
		$+0.082$ & $0.4$ & $0.533$ & $0.241$ \\ \hline
		$+0.162^{-}$ & $0.4$ & $0.564$ & $0.126$ \\ \hline
		$+0.162$ & $0.6$ & $0.564$ & \textbf{0.326}\\ \hline
		$+0.294^{-}$ & $0.6$ & $0.616$ & $0.020$\\ \hline
		$+0.294$ & $0.8$ & $0.616$ & $0.220$\\ \hline
		$+1.292^{-}$ & $0.8$ & $0.902$ & $0.012$\\ \hline
		$+1.292$ & $1$ & $0.902$ & $0.212$\\ \hline
	\end{tabular}
	\end{center}
	\end{table}
	And so we have the maximum deviation that is $0.333$. Either with Microsoft Excel 14.0.6123:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/ks_ExcelValuesList.jpg}
		\caption[]{Calculations in Microsoft Excel 14.0.6123}
	\end{figure}
	with the explicit formulas:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/arithmetics/ks_ExcelValuesListFormulas.jpg}
		\caption[]{Explicit calculations in Microsoft Excel 14.0.6123}
	\end{figure}
	with the small corresponding VBA routine quickly and poorly made that will take the number of iterations required in the cell \texttt{K1} and will put the empirical distribution of Kolmogorov-Smirnov in column \texttt{G} of the active sheet:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.9]{img/arithmetics/ks_VBA.jpg}
		\caption[]{Small VBA script for K-S GoF test}
	\end{figure}
	We therefore reiterate the procedure a thousand times and we get the following distribution function (obtained simply by making a scatter chart type in Microsoft Excel 14.0.6123 with $2,000$ simulations):
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/ks_distribution_simulation.jpg}
		\caption[]{K-S GoF distribution simulation}
	\end{figure}
	and applying a one-sided test with a threshold of $\alpha=5\%$ we get for the $95$th percentile:
	
	The reader will find the same value in the Kolmogorov-Smirnov tables available in many books. A few thousand simulations are therefore sufficient to restore the values of the tables!
	
	And now, we compare:
	
	and therefore we do not reject the null hypothesis.
	
	But ... we must still take care with only five values, it is quite likely that the null hypothesis $H_0$ is not rejected for other distribution laws that the Normal distribution.
	
	Thus, as the reader will have noticed, for each null hypothesis $H_0$ associated with a particular distribution law, we must tabulate the empirical distribution of Kolmogorov-Smirnov for different values of $n$ and of $\alpha$ using numerical methods. In the majority of books there is only one table with a powerful theorem that shows that in reality, the critical values will be the same.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Kolmogorov and Smirnov have proved that when $n$ goes very large and that the law of the null hypothesis is continuous, it is no longer necessary to tabulate a Kolmogorov-Smirnov table for each law because we have then:
	
	therefore $D_n$ is independent of the distribution law of the null hypothesis $H_0$. By simulating with the Monte Carlo method, we observe an effective convergence when $n$ exceeds a hundred. But in practice, the vast majority of the time, it is unthinkable to have such a number of measures. Hence the fact that this theoretical result is little used in practice and justifies the absence of proof in this book.
	\end{tcolorbox}
	To conclude on the K-S GoF test, let us notice to the reader that he will find the mathematical proof of the Anderson-Darling GoF further below.
	
	\newpage
	\begin{center}
  		KOLMOGOROV-SMIRNOV ONE-SIDED TEST
	\end{center}

	\begin{center}
	\renewcommand{\arraystretch}{1.1}
	\begin{tabular}{|c|ccccc|} \hline
	$n$&  0.2   &  0.1  & 0.05  &  0.02  & 0.01  \\ \hline
	 1 & 0.9000 & 0.9500 & 0.9750 & 0.9900 & 0.9950 \\
	 2 & 0.6838 & 0.7764 & 0.8419 & 0.9000 & 0.9293 \\
	 3 & 0.5648 & 0.6360 & 0.7076 & 0.7846 & 0.8290 \\
	 4 & 0.4927 & 0.5652 & 0.6239 & 0.6889 & 0.7342 \\
	 5 & 0.4470 & 0.5094 & 0.5633 & 0.6272 & 0.6685 \\
	 6 & 0.4104 & 0.4680 & 0.5193 & 0.5774 & 0.6166 \\
	 7 & 0.3815 & 0.4361 & 0.4834 & 0.5384 & 0.5758 \\
	 8 & 0.3583 & 0.4096 & 0.4543 & 0.5065 & 0.5418 \\
	 9 & 0.3391 & 0.3875 & 0.4300 & 0.4796 & 0.5133 \\
	10 & 0.3226 & 0.3687 & 0.4092 & 0.4566 & 0.4889 \\
	11 & 0.3083 & 0.3524 & 0.3912 & 0.4367 & 0.4677 \\
	12 & 0.2958 & 0.3382 & 0.3754 & 0.4192 & 0.4490 \\
	13 & 0.2847 & 0.3255 & 0.3614 & 0.4036 & 0.4325 \\
	14 & 0.2748 & 0.3142 & 0.3489 & 0.3897 & 0.4176 \\
	15 & 0.2659 & 0.3040 & 0.3376 & 0.3771 & 0.4042 \\
	16 & 0.2578 & 0.2947 & 0.3273 & 0.3657 & 0.3920 \\
	17 & 0.2504 & 0.2863 & 0.3180 & 0.3553 & 0.3809 \\
	18 & 0.2436 & 0.2785 & 0.3094 & 0.3457 & 0.3706 \\
	19 & 0.2373 & 0.2714 & 0.3014 & 0.3369 & 0.3612 \\
	20 & 0.2316 & 0.2647 & 0.2941 & 0.3287 & 0.3524 \\
	21 & 0.2262 & 0.2586 & 0.2872 & 0.3210 & 0.3443 \\
	22 & 0.2212 & 0.2528 & 0.2809 & 0.3139 & 0.3367 \\
	23 & 0.2165 & 0.2475 & 0.2749 & 0.3073 & 0.3295 \\
	24 & 0.2120 & 0.2424 & 0.2693 & 0.3010 & 0.3229 \\
	25 & 0.2079 & 0.2377 & 0.2640 & 0.2952 & 0.3166 \\
	26 & 0.2040 & 0.2332 & 0.2591 & 0.2896 & 0.3106 \\
	27 & 0.2003 & 0.2290 & 0.2544 & 0.2844 & 0.3050 \\
	28 & 0.1968 & 0.2250 & 0.2499 & 0.2794 & 0.2997 \\
	29 & 0.1935 & 0.2212 & 0.2457 & 0.2747 & 0.2947 \\
	30 & 0.1903 & 0.2176 & 0.2417 & 0.2702 & 0.2899 \\
	31 & 0.1873 & 0.2141 & 0.2379 & 0.2660 & 0.2853 \\
	32 & 0.1844 & 0.2108 & 0.2342 & 0.2619 & 0.2809 \\
	33 & 0.1817 & 0.2077 & 0.2308 & 0.2580 & 0.2768 \\
	34 & 0.1791 & 0.2047 & 0.2274 & 0.2543 & 0.2728 \\
	35 & 0.1766 & 0.2018 & 0.2242 & 0.2507 & 0.2690 \\
	36 & 0.1742 & 0.1991 & 0.2212 & 0.2473 & 0.2653 \\
	37 & 0.1719 & 0.1965 & 0.2183 & 0.2440 & 0.2618 \\
	38 & 0.1697 & 0.1939 & 0.2154 & 0.2409 & 0.2584 \\
	39 & 0.1675 & 0.1915 & 0.2127 & 0.2379 & 0.2552 \\
	40 & 0.1655 & 0.1891 & 0.2101 & 0.2349 & 0.2521 \\ \hline
	$>40$&
	$1.07/\sqrt{n}$&$1.22/\sqrt{n}$&$1.36/\sqrt{n}$&
	$1.52/\sqrt{n}$&$1.63/\sqrt{n}$ \\
	\hline
	\end{tabular}
	\end{center}
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Caution! Let us remind what the normality tests answer: \textit{Is there convincing evidence of any deviation from the Gaussian ideal?} With moderately large real data sets, the answer is almost always yes.\\
	
	The question engineers and scientists often expect the normality test to answer is: \textit{Do the data deviate enough from the Gaussian ideal to "forbid" use of a test that assumes a Gaussian distribution?} Scientists and engineers often want the normality test to be the referee that decides when to abandon conventional (ANOVA, etc.) tests and instead analyze transformed data or use a rank-based nonparametric test or a resampling or bootstrap approach. For this purpose, normality tests are not very useful as many of these tests are robust to some kind of non-normality.
	\end{tcolorbox}
	
	\paragraph{Ryan-Joiner GoF test}\index{statistical tests!Ryan-Joiner GoF test}\mbox{}\\\\
	Consider a random variable $X$ which we know the sampling distribution and for which we would like to check the Normality or not and consider an ordered random variable $Y$ generated by a Normal centered reduced distribution (that's why Ryan-Joiner test is especially well known as normality test). To compare $X$ and $Y$, we will center $X$ and order its values in ascending order.
	
	For a same given sample size, if the ordered values of $X$ and $Y$ taken in pairs follow the same law, a linear regression based on the other should give a fairly close correlation coefficient of $1$. Taking the definition of the squared correlation coefficient, we have then:
	
	$Y$ is assumed to follow a centered reduced Normal distribution. It comes then:
	
	and if we take the estimator of the correlation coefficient:
	
	Therefore after simplification:
	
	This is the Ryan-Joiner approach (implemented in Minitab for example) of the Shapiro-Wilk test. The results of both tests are very similar. The coefficients $a_i$ can be easily obtained using any spreadsheet software by using a Monte Carlo simulation (\SeeChapter{see section Numerical Methods page \pageref{monte carlo simulations}}). If our reader wish it we will detail how to get the coefficients $a_i$ with Microsoft Excel for a given $n$.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Consider the $10$ measures of column \texttt{A} already sorted in ascending order:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/arithmetics/rj_gof_table_excel_values.jpg}
		\caption[]{Example of ordered measures, ranks, RJ coefficient and $Z$-score}
	\end{figure}
	The corresponding formulas are:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.62]{img/arithmetics/rj_gof_table_excel_formulas.jpg}
		\caption[]{Formulas in  Microsoft Excel 14.0.6123 of previous screen shot}
	\end{figure}
	and therefore we have in a sheet named \textit{CoeffMonteCarlo} Monte Carlo simulations to determine the $10$ coefficients $a_i$ traditionally denoted in the case of $10$ measurements in the tables as following: $\left\lbrace a_{1/10},a_{2/10},\ldots ,a_{10/10} \right\rbrace$. First we must create $10$ columns with Normal centered reduced variables on almost $10,000$ rows with the following Microsoft Excel formula:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/arithmetics/rj_normal_centered_reduced_variables_for_coeff.jpg}
		\caption[]{Centered reduced normal variables for RJ coefficients}
	\end{figure}
	and then we have to build the ranks of all these values row by row such as:
	\begin{center}
		\texttt{=NORM.S.INV(RANDBETWEEN(1;99999999)/100000000)}
	\end{center}
	and then we have to build the ranks (ascending order) of all these values row by row such as:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/rj_normal_centered_reduced_variables_ordered.jpg}
		\caption[]{Ascending row by row sorting of simulations to determine the RJ coefficients}
	\end{figure}
	with the following formulas (given only for the first four $i$ because of lack of space on the screenshot):
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/rj_normal_centered_reduced_variables_ordered_formulas.jpg}
		\caption[]{Microsoft Excel 14.0.6123 formulas of previous screenshot}
	\end{figure}
	Finally, it only remains to calculate the correlation coefficient between the columns \texttt{D} and \texttt{E} of the first screenshot:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/rj_correlation_coefficient.jpg}
		\caption[]{Final calculation of the RJ correlation coefficient with Microsoft Excel 14.0.6123}
	\end{figure}
	That gives:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/rj_correlation_coefficient_value.jpg}
	\end{figure}
	where the square of this value is very very close to the Shapiro-Wilk test.\\
	
	Then, to know if we can reject or not the null hypothesis $H_0$ of Normality assumption, we should repeat the same procedure with in place of the measurements use random values generated from a Normal distribution and then determine the critical value of acceptance/rejection (it is very simple to make but we can detail on readers request).\\
	
	After calculations in this special case we sadly do not reject the null hypothesis.
	\end{tcolorbox}
	
	\pagebreak
	\paragraph{Anderson-Darling GoF test}\label{anderson darling gof test}\index{statistical tests!Anderson-Darling GoF test}\mbox{}\\\\
	It is surprising that a test reasonably strong (robust) as the Kolmogorov-Smirnov test can be designed based on only a single observation and a single point of the function of candidate distribution!!! It would seem, with more hindsight, more efficient to measure the difference between the two distribution functions by comparing these functions on their entire domain, that is to say from $-\infty$ to $+\infty$.
	
	It exist a family of tests which the statistics are based on the integral of the square of the difference (these tests are often considered as nonparametric but in my opinion wrongly and same for the Kolmogorov-Smirnov GoF test that is also considered as nonparametric):
	
	between the empirical distribution function and the reference distribution function. The simplest of these statistics is:
	
	which is simply the area between the empirical distribution function and the reference distribution function. Either, by taking the above graph used during our study of the Kolmogorov-Smirnov GoF:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/anderson_darling_concept.jpg}
		\caption{Idea behind the Anderson-Darling GoF}
	\end{figure}
	However, arbitrarily, we can choose something other than the measurement $x$ for the integral. Thus, a conventional choice is to take the theoretical distribution function itself as a basic measure of the integral. It comes as follows:
	
	Statistic resulting therefrom is named "\NewTerm{Cramér-von Mises statistic}\index{Cram\'er-von Mises statistic}". However it suffers from a major flaw of robustness when measuring points are on the tails of the distribution.
	
	It then was proposed the following measure that is somewhat less sensitive to measurement points on the tails:
	 
	named "\NewTerm{Anderson-Darling statistic}\index{Anderson-Darling statistic}" which was the most used in the late 120th century (holocene calendar) and remains dominant in the early 121st century also (at least while the sample is a fair size!). It is more robust by construction that the Cram\'er-von Mises statistics , and Kolmogorov-Smirnov simulations, but numerical studies have shown that it is less robust than the Shapiro-Wilk test or Ryan-Joiner test.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	We can put the three statistics next to each other for comparisons purposes (with their traditional notation):
	
	\end{tcolorbox}
	
	Remembering that the definition of the empirical distribution $\hat{F}_n(x)$ in our study of the Kolmogorov-Smirnov GoF test implies that:
	
	if $x\in[-\infty,x_1]$ and:
	
	if $x\in[x_k,x_{k+1}]$ and:
	
	if $x\in[x_n,+\infty]$. We then have assuming in addition that $F$ is continuous:
	
	Then we make the change of variable $u=F(x)$ and therefore:
	
	and without forgetting that the integral bound change as:
	
	Therefore we get:
	
	where we of course have put:
	
	We must now calculate these integrals. So we look for the primitive of a function of the type:
	
	The primitive of the following terms:
	
	have been proved in their general form in the section of Differential and Integral Calculus and primitives are respectively:
	
	because in view of the possible values of $u$, then it is unnecessary to indicate the absolute values.
	
	So we have only to calculate the primitive:
	
	where an obvious change of variables (if you want details feel free to ask us) gives us the primitive without the constant:
	
	Then we have finally:
	
	Therefore we have:
	\thickmuskip=0mu
	\medmuskip=0mu
		
	\thickmuskip=3mu
	\medmuskip=3mu
	We can already notice that in the last equality:
	
	Therefore it remains:
	
	We now make some tricky algebraic manipulations (but simple) to condense the writing of the latter equality.
	
	First, notice that we can write the first sum as (the reader can check the two sums  are equal by developing for a small value of $n$ to make an example):
	
	which is equivalent to put $j=j-1$.

	We transform also the second sum:
	
	and the reader can check that the equality below for the third sum is verified (if you problems for this don't hesitate to contact us):
	
	which also is equivalent to put $j=j-1$.
	
	Finally, we transform the fourth sum (since in any case when $j$ is equal to $n$ then the term of the sum is zero ...):
	
	Therefore we have:
	
	Either by eliminating the terms that vanish:
	
	And by grouping the terms with the same form of logarithm:
	
	Therefore:
	
	This is one of the closed form of the Anderson-Darling GoF test (especially well known as normality test) and that in the context of a Normal distribution is expressed by tradition in the following form:
	
	But there is another very common simplified expression. To establish it, we start again from the prior-previous expression:
	
	By doing the change of variable $j=1+n-i$ in the last sum, the expression:
	
	becomes:
	
	and the bounds of the sum becomes:
	
	Therefore:
	
	Hence:
	
	Finally:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Suppose we have measured the following five values:
	\begin{gather*}
		-1.2,0.2,-0.6,0.8,-1.0
	\end{gather*}
	thus ordered:
	\begin{gather*}
		x_1=-1.2,x_2=-1.0,x_3=-0.6,x_4=0.2,x_5=0.8
	\end{gather*}
	We want to test the following null hypothesis:
	
	where $\Phi(x)$ is for recall the distribution function of the Normal centered reduced distribution $\mathcal{N}(0,1)$. \\

	To implement the calculation of the AD in a spreadsheet software like Microsoft Excel 14.0.6123 we first do:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/ad_gof_excel_initial_values.jpg}
		\caption[]{Initial values for AD GoF test}
	\end{figure}
	Explicitly:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{img/arithmetics/ad_gof_excel_initial_values_formulas.jpg}
		\caption[]{Formulas in previous figure values for AD GoF test}
	\end{figure}
	So we get the same value of the indicator AD that as statistical softwares that allows to choose the law to be compared (and therefore the parameters relating). However, for very small samples statistics software uses the following correction (which we were not able to re-obtain by simulation...):
	
	thus in our case AD* is about $0.789$ as $\text{AD}=0.636$.
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	Then to calculate the $p$-value we need to investigate a curiosity... Indeed if we determine it by a Monte Carlo simulation as we did during our practical application of the Kolmogorov-Smirnov test by first writing in the column \texttt{A} from a new sheet the following formulas:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/ad_gof_excel_initial_pvalue_sorting.jpg}
		\caption[]{Formulas to sort Monte Carlo simulated values for AD GoF test}
	\end{figure}
	values coming from the column \texttt{O} where we have:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/ad_gof_excel_mc_simulated_values.jpg}
		\caption[]{Monte Carlo simulated formulas values for AD GoF test}
	\end{figure}
	The reader will notice that this is same as comparing the sample with a uniform distribution!!!\\
	
	Then having prepare the following columns \texttt{H}, \texttt{I} (see the figure below) that will contain the simulated values reported by the VBA code given a little further and columns \texttt{L}, \texttt{M} that allow us after to have the distribution of AD and AD\* values thanks to the calculation of the percentile:
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/arithmetics/ad_gof_excel_reported_ad_values_formulas.jpg}
		\caption[]{Columns for writing the AD/AD* values computed by VBA and the percentiles calculations}
	\end{figure}
	with the little VBA code below quickly and dirty made:
	\begin{lstlisting}[language={[Visual]Basic}, caption={VBA code of the AD GoF test}]
	Sub SimulAndersonDarling()

		Const intSimulations As Integer = 1000
		Dim i As Integer

		For i=1 To intSimulations
			Calculate
			Cells(i+1, 8).Value = Cells(9, 2).Value
			Cells(i+1, 9).Value = Cells(10, 2).Value
			Cells(8, 17).Value = i
		Next i
	End Sub
	\end{lstlisting}
	we then have with $10,000$ simulations the following distribution of AD and AD* values:
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.9]{img/arithmetics/ad_gof_percentiles.jpg}
		\caption[]{AD GoF test percentiles}
	\end{figure}
	So whether it is for AD or AD* the $p$-value is in our particular case almost equal to $1-40\%=60\%$ which corresponds to the tabulated values by Peter A.W. Lewis at IBM (11961 according to holocene calendar).\\
	
	What is curious and that we need to investigate is that the majority of software use the following formulas (R.B. D'Agostino and M.A. Stephens, Eds., 11986 (holocene calendar), Goodness-of-Fit Technology, Marcel Dekker) to avoid Monte Carlo simulations:
	
	and in our case, the application of these formulas give a $p$-value of about $9\%$ !!! Value actually given by statistical softwares! We will try to find where this difference comes from as our calculation seem subjectively more accurate than this empirical formulas... We asked the support of an American statistical package editor to explain the reason for the difference between the values tabulated by Peter A.W. Lewis and those of R.B D'Augostino and M.A. Stephens but they were not able to answer. We also contacted M.A. Stephens himself so that he can communicate us how he had obtained these formulas but we never get any answers ...
	\end{tcolorbox}
	
	\pagebreak
	\paragraph{Cramér–von Mises GoF test}\index{Cram\'er–von Mises GoF test}\index{statistical tests!Cram\'er–von Mises GoF test}\mbox{}\\\\
	In statistics the Cramér–von Mises GoF test (considered by many as a nonparametric test) is used for judging the goodness of fit of a cumulative distribution function $F^{*}$ compared to a given empirical distribution function $F_{n}$, or for comparing two empirical distributions (the criterion is named after Harald Cramér and Richard Edler von Mises who first proposed it in 11928–11930 according to holocene calendar). It is also used as a part of other algorithms, such as minimum distance estimation. It is commonly defined as following (as we have already introduced it just earlier above):
	
	but should better be written as:
	
	In one-sample applications $F^{*}$ is the theoretical distribution and $F_{n}$ is the empirically observed distribution. Alternatively the two distributions can both be empirically estimated ones; this is named the "two-sample case".
	
	We want to derive the Cram\'er-von Mises test statistic (said to be with $n$-degrees of freedom):
	
	where $U_i = F_0(X_{i:n})$ is the theoretical order statistics from the original (that's why it's a parametric test! We need to estimate the parameters of the original distribution):
	
	Notice that this statistic is tabulated\footnote{Mainly and sadly only for the Normal distribution also in some of the most important statistical softwares.} against the null hypothesis $H_0$ that $\hat{F}_{n}(t)=F_{0}(t)$.
	
	Let $X_1,\ldots,X_n$ be a random sample of size $n$ from $f(x)$. The empirical CDF is:
	
	Let $U_i = F_0(X_{i:n})$, then (the reader will see that there are some quite clever simplifications):
	
	Now we use:
	
	Therefore:
	
	Therefore:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Consider we have the following parameters from a sample:
	$$\bar{X}=6.4034 \qquad\text{and}\qquad \sigma_X=3.299319$$
	Assuming a Normal distribution $\mathcal{N}(\bar{X},\sigma_X)$, we get for each of the values:
	\begin{gather*}
		F_0(X) = (0.04943849, 0.05925460, 0.06477221, 0.07101753, 0.13526439, 0.25218337,\\
		0.31746223, 0.38651837, 0.48830421, 0.67550752, 0.68425558, 0.69965168, 0.78797818,\\
		0.79458401, 0.79661282, 0.83025028, 0.83051069, 0.83202204, 0.83609969, 0.84033359)
	\end{gather*}
	Let us calculate now the Cramér-von Mises statistic using:
	
	We get here $W^2=0.1902842$ that we report in the Cramér-von Mises table for $20$ degrees of freedom, which gives a $p$-value of $0.006<5\%$. Hence, we reject $H_0$ and we conclude that the distribution function of $X$ is very likely not drawn from a Normal distribution.
	\end{tcolorbox}
	
	\pagebreak
	\subsubsection{Kernel density estimation}\label{kernel smoothing}
	In statistics, "\NewTerm{kernel density estimation}\index{kernel density estimation}" (KDE) or "\NewTerm{kernel smoothing}\index{kernel smoothing}" is a nonparametric way to estimate the probability density function of a random variable, but also to do some special regressions (\SeeChapter{see section Numerical Methods page \pageref{kernel regression}}). Kernel density estimation is a fundamental data smoothing problem in business and science where inferences about the population are made, based on a finite data sample. In some fields such as signal processing and econometrics it is also termed the "\NewTerm{Parzen–Rosenblatt window method}\index{Parzen–Rosenblatt window method}", after Emanuel Parzen and Murray Rosenblatt, who are usually credited with independently creating it in its current form.
	
	The nonparametric estimation of a probability density function is very important in all domains using statistics! We already know that the technique making use of histograms is very delicate and questionable because it is necessary to choose the right interval width for each of the bars, which is very subjective and the Sturges rule that we have seen earlier can be discussed!
	
	To introduce the subject we first recall that we have in the univariate case:
	
	Or what is sometimes written more elegantly:
	
	Based on the method of numerical integration with rectangles (\SeeChapter{see section Numerical Methods page \pageref{rectangle integration method}}) we can write:
	
	Therefore, we have:
	
	The above probability can be estimated by the relative frequency of observations:
	
	What is sometimes written:
	
	An alternative non-trivial writing (and equal in numerical value!) is:
	
	where $x_1,x_2,\ldots,x_n$ are the observed values and:
	
	is named the "\NewTerm{weight of the kernel}" function or "\NewTerm{univariate Rosenblatt kernel function}\index{Rosenblatt kernel function}". The later relation is also often denoted:
	
	With (notation a little unfortunate in my point of view...):
	
	and where $h$ is named the "\NewTerm{bandwidth}".

	In general, a "\NewTerm{kernel function}\index{kernel function}" $K(x)$ is a function that must satisfy the following properties:
	\begin{enumerate}
		\item[P1.] The kernel function is greater than or equal to zero $K(x)\ge 0$
		\item[P2.] The kernel function is maximum in $x=0$
		\item[P3.] The kernel function is symmetric with respect to the vertical axis passing through $x=0$
		\item[P4.] The kernel function tends to zero away from $x=0$ such that $\lim_{|x|\rightarrow +\infty} K(x)=0$
		\item[P5.] The kernel function is a probability density function
	\end{enumerate}
	The property P1 is obvious to control and it does not require any proof even in more complicated cases than the kernel function taken as an example previously.

	The property P2 is more an implementation constraint rather than a property. To be satisfied, we must simply center the values (measurements) with the which we work and assume that the distribution is unimodal.
	
	The property P3 is also more an implementation constraint than a property. It will be useful to us to simplify certain calculations such as the bias of the expected mean or of the standard deviation (see below).
	
	The property P4 applies by common sense and derives from the property P2. In the case of unimodal probability density functions (reason why this property is often omitted from the statements).
	
	To check property P5 it is sufficient to show that the cumulative probability is unitary.
	\begin{dem}
	We know by construction of the previous kernel function that on each point is placed a rectangle of height $1 / 2h$ and of width $2h$. Then, the idea is simply to write:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Some examples of kernel functions that satisfy these properties are (the most important ones have already been dissected in the previous subsections) to a given normalization constant:
	\begin{itemize}
		\item The Gaussian kernel:
		
	
		\item The Cauchy kernel:
		
	
		\item The Picard kernel:
		
	
		\item The Epanechnikov kernel:
		
	\end{itemize}
	Let us now consider some important properties of the empirical density function.

	Let us calculate its expected mean (as it is a matter of calculating the expected mean of the density function itself, this will ultimately give us the bias of the expected mean):
	
	For example, for a Gaussian kernel we have:
	
	From this point on, the variable of which we take the expectation, that is:
	
	is no longer a probability density function but a simple transformed random variable. We will therefore extend to the continuous case and multiply it by its true probability. The very clever idea being that if the bias is zero we should find:
	
	and if we don't fall back on this result. The difference will give us the bias we are looking for!
	
	Let us check that:
	
	Let us put to simplify the further developments:
	
	and therefore:
	
	Let us develop the function $f(x-hz)$ as a Taylor series development (\SeeChapter{see section Sequences and Series page \pageref{taylor series}}) on $z=0$:
	
	Then we have:
	
	But, we know that if the kernel function is symmetric and maximal at zero (unimodal) then we can invert the sign of the first integral:
	
	The first integral is equal to $1$ since the kernel function is a probability density function. Then it remains:
	
	The second integral calculates the expected mean. Now, since the kernel function is supposed to be symmetric and maximal in zero, it follows that its expected mean is zero and therefore it remains:
	
	And therefore the bias of the expected mean is:
	
	Thus the bias decreases if we decrease $h$ (but we will immediately show that it is the opposite for the variance).
	
	Now therefore let us calculate the variance:
	
	Because the $x_i$, $i=1,2,\ldots,n$, are independently distributed. Now using Huygens Theorem:
	
	Therefore:
	
	Substituting $z=(x-t)/h$ we get:
	
	Applying again a Taylor approximation yields:
	
	Notice that if $n$ becomes large and $h$ becomes small then the above expression become approximately:
	
	we notice that the variance decreases as $h$ increases.
	
	Selection of the bandwidth $h$ of kernel estimator is a subject of considerable research. There exist more than one heuristic popular method but at least they are all more robust than the Sturge rule!
	
	Here are some examples without the script of some Kernel smoothing:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/kernel_smoothing_normal_random_variables.jpg}
		\captionsetup{width=0.7\linewidth}
		\caption[Kernel density estimation of $100$ normally distributed random numbers]{Kernel density estimation of $100$ normally distributed random numbers using different smoothing bandwidths (source: Wikipedia)}
	\end{figure}
	and a comparison between a histogram and a Kernel smoothing:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/arithmetics/kernel_smoothing_histogram.jpg}
		\captionsetup{width=0.8\linewidth}
		\caption[]{Comparison of histogram (left) and kernel density estimate (right) constructed from the same data. The individual $6$ kernels are the red dotted curves, the density of the kernel estimates the blue curve. The data points are the small vertical lines the horizontal axis (source: Wikipedia)}
	\end{figure}
	And another interesting figure highlighting the issue of histograms \cite{zucchini2003applied}:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.9]{img/arithmetics/kernel_smoothing_histogram_issue.jpg}
		\captionsetup{width=0.8\linewidth}
		\caption{Histograms with different bin widths and a kernel estimate of $f(x)$ for the same sample}
	\end{figure}
	The reader interested can see our \texttt{R} and MATLAB™ companion books to get the details on how to run Kernel smoothing on scientific softwares.
	
	\pagebreak
	\subsubsection{Likelihood-ratio tests}\label{likelihood ratio tests}\index{statistical tests!maximum likelihood ratio test}
	Many statistics software returns, in complementary of classical statistical results, an output that is sometimes named "\NewTerm{maximum likelihood test}\index{maximum likelihood test}" or "\NewTerm{maximum likelihood ratio test}\index{maximum likelihood ratio test}" (LRT) for a test whose result is already given. This maximum likelihood test is also sometimes the unique output as classical methods does not provide a computable or accurate output (this is the case of the G-test and Poisson test of means). As we will see just below, the principle is basically ... extremely simple!
	
	
	Let us recall that we have proved that for a Normal law during our study of likelihood estimators that:
	 
	For example, if we know the standard deviation but we try to estimate the mean then this last relation can be written:
	 
 	Then nothing prevents us from writing:
	
	Therefore after some very small elementary algebra and arithmetic manipulations:
	
	In the case of following hypothesis (two sided Student $T$-test) with the assumption of homoscedasticity:
	
	We have:
	
	Therefore under the null hypothesis:
	
	That is often denoted:
	
	and is the "\NewTerm{likelihood ratio test for the comparison of the means from a Normal law}" or "\NewTerm{maximum likelihood goodness-of-test for normality}".
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Some softwares use for reasons I was mathematically not able to prove a chi-square distribution $\chi_{n-d-1}^2$, where $d$ is the number of parameters of the implicit assumed distribution law ($d=1$ for a Poisson law or $d=2$ for a Normal distribution, etc.).
	\end{tcolorbox}
	Therefore we see already, before continuing, that a likelihood ratio test is a statistical test used for comparing the goodness of fit of two models, one of which (the null model) is a special case of the other (the alternative model). The test is based on the likelihood ratio, which expresses how many times more likely the data are under one model than the other. This likelihood ratio, or equivalently its logarithm, can then be used obviously to compute a $p$-value, or compared to a critical value to decide whether to reject the null model in favour of the alternative model. When the logarithm of the likelihood ratio is used, the statistic is known as a "\NewTerm{log-likelihood ratio statistic}\index{log-likelihood ratio statistic}", and the probability distribution of this test statistic, assuming that the null model is true, can be approximated using "\NewTerm{Wilks' theorem}\index{Wilk's theorem}" (approximation by a chi-squared distribution).
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In the case of distinguishing between two models, each of which has no unknown parameters, use of the likelihood ratio test can be justified by the Neyman–Pearson lemma, which demonstrates that such a test has the highest power among all competitors. But this is out of the scope of this book (at least actually...).
	\end{tcolorbox}
	In practice we find more often the test of the ratio of maximum likelihood with the Poisson's law. We recall that for the latter we have also proved during our study of likelihood estimators:
	
	Therefore:
 	
	As for the case with the Normal law, we can write:
	
	Therefore:
	
	What it is customary to write:
	
	Suppose we want to test the fact that our measurements do not deviate too much from the true expectation, that is to say:
	
	We have the under the null hypothesis:
	
 	So.... arrrrgh. We cannot make inference on such a result. So it's very annoying to deal with such a likelihood ratio ... Fortunately we know that in the case where:
	
	the Poisson's law tends to a Normal law (if $\mu$ is greater than about $10$, then the Normal distribution is a good approximation if an appropriate continuity correction is performed). Then in these conditions we use the previous result of the Normal law!
	
	More generally we can (sadly) found the above final relations in the following forms in various textbooks:
	
	So many common test statistics such as the $Z$-test, the $F$-test, the $T$-test, the Pearson's chi-squared test and the $G$-test are tests for nested models and can be phrased as log-likelihood ratios or approximations thereof.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Some softwares propose another adequation test for discrete distribution, named the "\NewTerm{minimum chi-square estimation goodness-of-fit}\index{minimum chi-square estimation goodness-of-fit}". The idea is quite simple and we will illustrate it with a Poisson distribution. Consider the following sample ($n=8$ for a total sample size equal to $20$):
	\begin{gather*}
	{\begin{array}{cc}{\textbf{Value}}&{\textbf{Frequency}}\\\hline 0&1\\1&2\\2&4\\3&5\\4&3\\5&3\\6&1\\7&0\\8&1\end{array}}
	\end{gather*}
	The minimum chi-square estimate of the population of parameter $\lambda$, is the $\lambda$ value that minimize:
	
	Numerical computation shows that the value of $\lambda$ that minimizes the chi-square statistic is about $3.5242$. That is the minimum chi-square estimate of $\lambda$. For that value of $\lambda$, the chi-square statistic is about $3.062764$. There are $10$ cells. If the null hypothesis had specified a single distribution, rather than requiring $\lambda$ to be estimated, then the null distribution of the test statistic would be a chi-square distribution with $10-1 = 9$ degrees of freedom.
	\end{tcolorbox}
	
	\subsection{Robustness and Nonparametric Statistics}
	In the area of inferential statistics and hypothesis testing, robustness is a recurring concept (the banks are compelled to stress testing / crash testing of their risk models for example). We have already mentioned this fact previously.
	
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] A test is named "\NewTerm{robust test}\index{robust test}" if it remains valid while the assumptions for the application are not all met. This can be a somewhat low sample size or a probability distribution (Normal distribution for parametric tests) that is not very well satisfied, or the outliers that influence too much the result of the test. For example, ANOVA is robust with respect to the Normality assumption but not compared to that of homoscedasticity.
		
		\item[D2.] An indicator is named "\NewTerm{robust indicator}\index{robust indicator}" if it is not very sensitive to the presence of outliers (the correlation coefficient or the average, for examples, are not very robust indicators).
		
		\item[D3.] More generally, a model is named "\NewTerm{robust model}\index{robust model}" when it allows an extension of the results (in time or for a population). The robustness is equally applicable to a multiple regression, statistical tests or to a scorecard, or project planning.
	\end{enumerate}
	Therefore, unless to be only descriptive, your studies will have to respect some rules so that their findings are generalisable.
	
	First condition of a good robustness: the data! Intuitively, we all know that we do not transform a special case in generality (which would not fall in the field of statistics but to countertop discussions). Sufficient data builds reliable and solid models. For example, forecasts derived from a time series showing seasonality require at least three or four years of history.
	
	However, the amount is not enough, we need quality (for most manager Quality is a very difficult concept)!! Indeed, it is best to avoid doing a study on unreliable information that can lead to costly decisions. Furthermore, it is recommended to be very careful with the manipulation of outliers. If this is not possible, we turn to more appropriate methods, such as those that using the median rather than the mean, ranks rather than values, or filtering techniques.
	
	\textbf{Definition (\#\thesection.\mydef):} "\NewTerm{Nonparametric statistics}\index{nonparametric statistics}" is the branch of statistics that is not based solely on parametrized families of probability distributions (common examples of parameters are the mean and variance).
	
	The term "nonparametric statistics" has been imprecisely defined in the following two ways (among others...):
	\begin{enumerate}
		\item They do not rely on assumptions that the data are drawn from a given probability distribution. We then say that they are "\NewTerm{distribution free methods}\index{distribution free}".

		\item They do not rely on any arithmetical operation of all the individual data (like "ranks" or "ranges" for example!)
	\end{enumerate}
	
	As we will see further below and as we have also already seen some special cases earlier above, there is quite a lot of nonparametric statistics and nonparametric tests (keep in mind that even if their are not based on parametrized families of distributions, they have however other assumptions that are visible only when your study their mathematical proof!!!). Rather than listing them all here, we rather want to answer the most common question from many practitioners that is a non-exhaustive list of the equivalent nonparametric to some parametric tests and statistics. So here is a table that summarize for the most important one that question request:
	\begin{table}[H]
		\centering
		\resizebox{\textwidth}{!}{\begin{tabular}{|l|l|l|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|c|}{\cellcolor[gray]{0.75}\textbf{Parametric Tests}} & \multicolumn{1}{c|}{\cellcolor[gray]{0.75}\textbf{Nonparametric Tests}} & \multicolumn{1}{c|}{\cellcolor[gray]{0.75}\textbf{Main characteristics}} \\ \hline
		 & $1$-sample sign test & \begin{tabular}[c]{@{}l@{}}Test on the median for data from a \\ nonsymmetric distribution\end{tabular} \\ \cline{2-3} 
		\multirow{-2}{*}{$1$-sample $T$ or $Z$ test} & $1$-sample Wilcoxon test & \begin{tabular}[c]{@{}l@{}}Test on the median for data from a \\ symmetric distribution\end{tabular} \\ \hline
		$2$-sample $T$-test & Mann-Whithney test & \begin{tabular}[c]{@{}l@{}}Test on two medians using ranks of \\ the sample data\end{tabular} \\ \hline
		 & Kruskal-Wallis test & \begin{tabular}[c]{@{}l@{}}Test on the equality of medians from\\ two or more populations. More\\ powerful than Mood's median test,\\ but less robust to outliers\end{tabular} \\ \cline{2-3} 
		\multirow{-2}{*}{One-Way ANOVA} & Mood's median test & \begin{tabular}[c]{@{}l@{}}Test on the equality of medians from\\ two or more populations. More\\ robust to outliers than the Kruskal-\\ Wallis test, but less powerful.\end{tabular} \\ \hline
		Two-Way Randomized Block ANOVA & Friedman test & \begin{tabular}[c]{@{}l@{}}Test on medians, using randomized\\ block experiments\end{tabular} \\ \hline
		Pearson's Correlation Coefficient & Spearman's rank correlation coefficient & \begin{tabular}[c]{@{}l@{}}Measures statistical dependence between \\ two variables using ranks\end{tabular} \\ \hline
		\begin{tabular}[c]{@{}l@{}}Anderson-Darling, Shapiro-Wilk, \\ Kolmogorov-Smirnov GoF\end{tabular} & Mann–Whitney U or Wilcoxon rank sum test & \begin{tabular}[c]{@{}l@{}}Tests whether two samples are drawn from \\ the same distribution, as compared to a given\\  alternative hypothesis.\end{tabular} \\ \hline
		\end{tabular}}
		\caption{Summary of some naive parametric tests/statistics and their nonparametric equivalent}
	\end{table}
	For a much more exhaustive list of parametric and non-parametric tests, the reader has to refer to the Data Science mind map at page \pageref{mindmap of data science}.
	
	Or for those who prefer a decision tree:
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/arithmetics/classical_statistical_methods_flowchart.pdf}
		\caption[Decision tree for naive statistical tests]{Decision tree for naive statistical tests (author:  Antoine Soetewey)}
	\end{figure}
	
	\pagebreak
	\subsubsection{Rank Statistics}
	\textbf{Definition (\#\thesection.\mydef):} The Rank Statistics, also named "\NewTerm{Order Statistics}\index{order statistics}" are defined as all the techniques of statistical calculations and statistical inference that have for main objective of getting rid of the knowledge of a parametric distribution and for using it ranks only (order) of the measurements. This is a very powerful and practically used tool for nonparametric statistics!
	
	As we have already mentioned, we are talking about parametric tests when we stipulate that the data are from a given known distribution. In this case, the characteristics of the data can be summarized using the parameters estimated on the sample, the subsequent test procedure then only focus on these parameters.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The reader interested to learn more but without detailed is referred to the excellent book of Gopal K. Kanji which contains a summary presentation of the most used $100$ parametric and nonparametric test with for each a small practical example. Sadly this book should be published in a new edition with the $200$ most used statistical tests to be almost complete and perfect.
	\end{tcolorbox}	
	
	Nonparametric tests (such as the chi-square tests already seen) eliminate the necessary step consisting to estimate the parameters of the distribution prior to doing the hypothesis test. In general, conclusions drawn from nonparametric tests are not as powerful as the parametric ones. However, as nonparametric tests make fewer assumptions, they are more flexible, more robust, and applicable to non-quantitative data.
	
	When the data are quantitative, most common nonparametric tests transforms values into ranks. The name "\NewTerm{ranks test}\index{rank test}" is then often encountered. When the data are qualitative, only the nonparametric can be used.
	
	\paragraph{$L$-Statistics}\label{L-statistics}\mbox{}\\\\
	Before addressing nonparametric distributions and tests, let us give some definitions that the reader may have seen be in the hyperspecialized literature and that we avoided to use so far...
	
	The median, mean and range suggest the use of linear combinations of the components of the order statistics vector.
	
	\textbf{Definition (\#\thesection.\mydef):} Thus, let us denote by $X_{(1)}\geq X_{(2)},\ldots ,\geq X_{(n)}$, the order statistics (values ordered in descending order by their rank and numbered). We define then the "\NewTerm{$L$-statistic}\index{$L$-statistic}" by:
	
	The first most known "\NewTerm{$L$-estimator}\index{$L$-estimator}" is the arithmetic mean for which:
	
	The second most known $L$-estimator is the median for which we have when $n$ is odd:
	
	and when $n$ is even:
	
	The third best $L$-estimator known is the range for which we have:
	
	There are other empirical $L$-estimators but we will stop here because the list is quite long.
	
	\paragraph{Ranks Distribution law}\mbox{}\\\\
	The $k$-th order statistic is the $k$th smallest value in sample of $n$ from a random distribution $F(x)$. Let the random variables $X_1,X_2,\ldots ,X_n$ be independent and identically distributed. Label the smallest value $X_{(1)}$, the next smallest $X_{(2)}$ and so on up to $X_{(n)}$. Then the value of the $k$th order statistic is $X_(k)$, for $1\leq k\leq n$.
	
	Let $F(x)$ denote the distribution function of the $X_i$'s and $f(x)$ the density function. Let us first determine the distribution of the $k$th order statistic, that is:
	
	To have $X_{(k)}\leq x$ we need to have at least $k$ of the $n$ $X_i$'s less than or equal to $x$. Each of the $n$ independent trials has probability $F(x)$ of being less than or equal to $x$, thus the number of trials less than or equal to $x$ has a Binomial distribution with $n$ trials and probability $F(x)$. That is:
	
	To find the density function we take the derivative with respect to $x$:
	
	Written in this form, it is obvious that the majority of terms will cancel. This leaves:
	
	
	\paragraph{Distribution free confidence intervals for percentiles}\mbox{}\\\\
	Earlier we learned (among a lot of others stuff) how to calculate a sample percentile as a point estimate of a population (or distribution) percentile. Just as it is a good idea to calculate confidence intervals for other population parameters, such as means and variances, it would be a good idea to learn how to calculate a confidence interval for percentiles of a population. That's what we'll work on just below. As the title of the subject suggests it, we won't make any assumptions about the distribution of the data, that is, other than it being continuous!
	
	As is generally the case, let's motivate the method for calculating a confidence interval for a population median $M_e$ by way of a concrete example\footnote{It's a well knows example that you can find in numerous textbooks and websites which original author seems impossible to determine.}. Suppose $Y_1 < Y_2 < Y_3 < Y_4 < Y_5$ are the order statistics of a random sample of size $n = 5$ from a continuous distribution. Our previous work tells us that $Y_3$ serves as a good point estimator of the median $M_e$. Let's see what we can come up with for a confidence interval given we have these order statistics at our disposal. Well, suppose we suggested that the interval constrained by the first and fifth order statistics, that is, $[Y_1, Y_5]$ would serve as a good interval. How confident can we be that the interval $[Y_1, Y_5]$ would contain the unknown population median $M_e$? To answer that question, we simply need to calculate the following probability:
	
	Calculating the probability reduces to a simple binomial calculation once we figure out all the ways in which the population median $M_e$ is sandwiched between $Y_1$ and $Y_5$. Well, the population median $M_e$ is sandwiched between $Y_1$ and $Y_5$, if the first order statistic is the only order statistic less than the median $M_e$:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[scale=7]
		\draw[->, thick] (-0.1,0) -- (1.2,0);
		\foreach \x/\xtext in {0/$\color{red}{Y_1}$,0.2/$M_e$,0.4/$\color{red}{Y_2}$,0.6/$\color{red}{Y_3}$,0.8/$\color{red}{Y_4}$,1.0/$\color{red}{Y_5}$}
		    \draw[thick] (\x,0.5pt) -- (\x,-0.5pt) node[below] {\xtext};
		\end{tikzpicture}
	\end{figure}
	The population median $M_e$ is sandwiched between $Y_1$ and $Y_5$, if the first two order statistics are the only order statistics less than the median $M_e$:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[scale=7]
		\draw[->, thick] (-0.1,0) -- (1.2,0);
		\foreach \x/\xtext in {0/$\color{red}{Y_1}$,0.2/$\color{red}{Y_2}$,0.4/$M_e$,0.6/$\color{red}{Y_3}$,0.8/$\color{red}{Y_4}$,1.0/$\color{red}{Y_5}$}
		    \draw[thick] (\x,0.5pt) -- (\x,-0.5pt) node[below] {\xtext};
		\end{tikzpicture}
	\end{figure}
	The population median $M_e$ is sandwiched between $Y_1$ and $Y_5$, if the first three order statistics are less than the median $M_e$, and the fourth and fifth order statistics are greater than $M_e$:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[scale=7]
		\draw[->, thick] (-0.1,0) -- (1.2,0);
		\foreach \x/\xtext in {0/$\color{red}{Y_1}$,0.2/$\color{red}{Y_2}$,0.4/$\color{red}{Y_3}$,0.6/$M_e$,0.8/$\color{red}{Y_4}$,1.0/$\color{red}{Y_5}$}
		    \draw[thick] (\x,0.5pt) -- (\x,-0.5pt) node[below] {\xtext};
		\end{tikzpicture}
	\end{figure}
	And, the population median $M_e$ is sandwiched between $Y_1$ and $Y_5$, if the fifth order statistic is the only order statistic greater than the median $M_e$:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[scale=7]
		\draw[->, thick] (-0.1,0) -- (1.2,0);
		\foreach \x/\xtext in {0/$\color{red}{Y_1}$,0.2/$\color{red}{Y_2}$,0.4/$\color{red}{Y_3}$,0.6/$\color{red}{Y_4}$,0.8/$M_e$,1.0/$\color{red}{Y_5}$}
		    \draw[thick] (\x,0.5pt) -- (\x,-0.5pt) node[below] {\xtext};
		\end{tikzpicture}
	\end{figure}
	This means that in order to calculate the probability $P(Y_1 \leq M_e \leq Y_5)$, we need to calculate the probability of each of the above events. Now, if we let $W$ denote the number of $X_i < M_e$, then $W$ is a binomial random variable with $n$ mutually independent trials and probability of success $p = P(X_i < M_e) = 0.5$. And, reviewing the events as depicted above, the desired probability is calculated as:
	
	The binomial probability mass function (or, alternatively, the binomial table) makes the calculation straightforward:
	
	So, the probability that the random interval $[Y1, Y5]$ contains the median $M_e$ is $0.9376$. We aren't always so lucky with arriving at a decent confidence coefficient on our first try. Sometimes we have to try again aiming to get a confidence coefficient that it as least $90\%$, but as close to $95\%$ as possible. In this case, the confidence coefficient for the interval $[Y2, Y4]$ is:
	
	Clearly, we would be better served to stick with the interval $[Y_1, Y_5]$ in this case.
	
	Notice that in general we then have:
	
	The method that we learned for finding a confidence interval for the median of a continuous distribution can be easily extended so that we can find a confidence interval for any percentile $\pi_p$. The only thing we have to change is the probability of a success, that is, that $X_i$ is less than $\pi_p$:
	
	Then, the exact confidence coefficient is calculated just as before using the binomial distribution with parameters $n$ and $p$:
	
	
	\paragraph{Wilcoxon Rank Sum Test}\index{statistical tests!Wilcoxon rank sum test}\mbox{}\\\\
	The idea of the "\NewTerm{Wilcoxon rank sum test}\index{Wilcoxon rank sum test}\label{Wilcoxon rank sum test}" is the following: if we collect two samples of measurement, and that we store the values in order, the alternance of the $X_i$ (of size $n_x$) and $Y_i$ (of size $n_y$) should be fairly steady if both samples distribution law $F$ and $G$ respectively follow the same probability distribution. It is therefore like a "fit check".
	
	This is in fact equivalent as saying that the null hypothesis is: «\textit{The distribution of the ranks in the two groups are equal}».
	
	Formally:
	
	 In plain language the null hypothesis is that the probability of a random observation from group $A$ exceeding the paired observation drawn from group $B$ is one half (i.e. a random observation in group $A$ has just as much probability of being greater than, as being less than its paired observation in group $B$).
	 
	 The practical value of this is hard to see, and thus in many places, including textbooks, the null hypothesis is presented as the \og two populations have equal medians \fg{}. The actual null hypothesis can be expressed as the latter median hypothesis, but only under the additional assumption that the shapes of the distributions are identical in each group!

	In other words, our interpretation of the test as comparing the medians of the distributions requires the location-shift-only alternative to be the case. Since this is rarely true, and never assessed, we should probably use extreme caution in using, and especially in interpreting, the Wilcoxon rank-sum test.
	
	It is therefore not such as Chi-square adequation test to compare measurements at a theoretical law, but to other measurement of a hypothesized same law.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The Wilcoxon rank sum test is a nonparametric test, used therefore for "Nonparametric Null Hypothesis Significance Tests" (NNHST), because we have no need of use of any measure of dispersion or position of the random variables. Moreover, it is a test say to be "robust" in the sense that it does not assume the Normality of the data.
	\end{tcolorbox}	
	Let us take an example before tackling the theoretical aspect. Here are two samples of size $10$ ($n_x=n_y$) of quantitative variables:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The Wilcoxon rank sum test may well be used for ordinal variables (but only as long as they are in an acceptable number). Typically, the Wilcoxon rank sum test is also used to analyse the response to business surveys using a Likert scales of $7$ points.
	\end{tcolorbox}	
	Here are the order statistics of the sample of size $20$ ($N=n_x+n_y$) regrouped and ordered (the $10$ values of the first sample are underlined):
	
	The values of the first sample $X$ (named "\NewTerm{treatment values}\index{treatment values}") in this example looks like to be smaller than those of the second  sample $Y$ (named "\NewTerm{control values}\index{control values}") that we often represent in a diagram as the following (by cheating a bit with Microsoft Excel 11.8346):
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/wilcoxon_rank_sum_test_ordered_values.jpg}
		\caption[]{Comparing ordered values of two samples in Microsoft Excel 11.8346}
	\end{figure}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	You must be careful about the Wilcoxon test we are studying here and the Kruskal-Wallis test that we will see later. Indeed, many people think that because they are nonparametric they don't have any assumptions... However as we know this is wrong!!! A nonparametric test is indeed independent of distribution parameters but there may have other assumptions. In the case of the Wilcoxon and the Kruskal-Wallis test, the assumption is (quite obviously...) that the scale parameters between the both samples are equal (otherwise it would be a non-sense to compare two things that we already know as being not comparable!).
	\end{tcolorbox}	
	The idea is then to find out if this trend is statistically significant? That is to say, whether we have a difference of the kind $F<G$ between their respective distributions laws:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/wilcoxon_rank_sum_distribution_comparison.jpg}
		\caption[]{Generic example of the comparison of two distributions in Microsoft Excel 11.8346}
	\end{figure}
	or whether they can be considered as identical. For this, we must examine the concept of "\NewTerm{rank}\index{rank}":
	
	\textbf{Definition (\#\thesection.\mydef):} Given a random $n$-sample $X_1,X_2,\ldots ,X_n$ of any continuous statistical law, we denote by $R_i$ the rank of the $X_i$ ordered in a sample population. The rank $i$ is a non-zero integer strictly positive and between $1$ and $n$.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	In:
	\begin{gather*}
		1.6,\underline{1.7},\underline{2.5},\underline{3.2},\underline{3.2},3.4,\underline{4.1},4.6,\underline{5.3},5.5,\underline{5.7},5.7,\underline{6.9},7.1,\underline{7.4},7.9,8.1,\underline{8.4},8.5,8.7
	\end{gather*}
	We have respectively the following "order statistics":
	
	Once the concept of "rank" defined and calculated, let us look at the sum in the context of our example with two samples:
	\begin{gather*}
		1.6,\underline{1.7},\underline{2.5},\underline{3.2},\underline{3.2},3.4,\underline{4.1},4.6,\underline{5.3},5.5,\underline{5.7},5.7,\underline{6.9},7.1,\underline{7.4},7.9,8.1,\underline{8.4},8.5,8.7
	\end{gather*}
	The sum rank denoted traditionally $W_x$ ($W$ for Wilcoxon) for the first sample is thus:
	
	and for the second sample:
	
	\end{tcolorbox}
	The values $W_x,W_y$ are named "\NewTerm{Wilcoxon Statistics}\index{Wilcoxon statistics}".
	
	We can therefore already see that there is indeed a difference that seems a priori significant in terms of ranking between both samples. The problem remains to build a rigorous mathematical tool to infer a fact with some certainty.
	
	For this, let us first introduce the average of the ranks using the result shown in the section of Sequences and Series by considering only one sample:
	
	By calculating this, we notice quickly that this is the expected mean of the uniform discrete distribution study earlier in this section for a discrete random variable with values between $1$ and $n$, which is exactly the definition of rank! Thus, we have the rank that will have for mean and variance for the entire population:
	
	For those who find this analogy questionable we give just right below the proof of the variance using the Huygens relation and the sum of squares of positive integers proved in the section of Sequences and Series:
	
	But obviously for a single sample this has no interest! Let us take again our two series $X_i,Y_i$ respectively of equal size $n_x=10,n_y=10$ without distinction:
	
	We then have the statistical indicators of ranks without distinction (you must remember that we still do not know at this level of the mathematical development if this will be helpful or not):
	
	and the statistical indicators of ranks but this time with distinction:
	\begin{gather*}
		1.6,\underline{1.7},\underline{2.5},\underline{3.2},\underline{3.2},3.4,\underline{4.1},4.6,\underline{5.3},5.5,\underline{5.7},5.7,\underline{6.9},7.1,\underline{7.4},7.9,8.1,\underline{8.4},8.5,8.7
	\end{gather*}
	We then have the following local statistical indicators:
	
	These calculations now done, we have nothing concretely yet rigorous regarding the Wilcoxon's rank sum test which purpose is for reminder to check whether the two samples follow the same law or not (and therefore have the same moments as the expected mean, variance, median, etc.).
	
	To move forward, let us consider the $n_x$ values of the sample $X$. We know (\SeeChapter{see section Probabilities page \pageref{binomial coefficient}}) then that there is:
	
	number of possible arrangements of the $X_i$ in the population of the samples and if the Wilcoxon's rank sum test is verified (that is to say, the probability laws are the same for both samples), the various arrangements should be equally likely.
	
	For example, if we take $2$ samples with respectively each $2$ measures ($2$ random variables of treatment and $2$ control random variables), we have:
	
	arrangements all different:
	
	But... sadly... this is not what we want in our case because we would like already to be able to distinguish the two samples and also do not take into account the arrangements that consist only of a permutation of the variables of the same sample. We then have (\SeeChapter{see section Probabilities page \pageref{binomial coefficient}}):
	
	possible combinations! Effectively with two samples having two treatment variables ($X$) and two control variables ($Y$), we have ($W_S$ is the sum of ranks of last column):
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|c|c|c|}
					\hline
	\multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Possible ranks} \\ \textbf{(Controls)}}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Possible ranks} \\ \textbf{(Treatments)}}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}$W_S$} \\ \hline
			$1,2$ & $3,4$ & $7$\\ 
			$1,3$ & $2,4$ & $6$\\ 
			$1,4$ & $2,3$ & $5$\\ \hline
			$2,3$ & $1,4$ & $4$\\ 
			$2,4$ & $1,3$ & $4$\\ \hline
			$3,4$ & $1,2$ & $3$\\ \hline
		\end{tabular}
		\end{center}
		\caption[]{Ranks representation of 2 variables of treatment and of control}
	\end{table}
	If the null hypothesis of the Wilcoxon's sum test rank is not rejected, the $6$ rankings are equally likely. We conclude the following table:
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|c|c|c|c|c|c|}
					\hline
	\multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Value of $W_S$} }} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$3$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$4$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$5$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$6$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$7$}}\\ \hline
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Probability}}} & $\dfrac{1}{6}$ & $\dfrac{1}{6}$ & $\dfrac{2}{6}$ & $\dfrac{1}{6}$ & $\dfrac{1}{6}$\\ 
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Cumulative}}} & $\dfrac{1}{6}$ & $\dfrac{2}{6}$ & $\dfrac{4}{6}$ & $\dfrac{5}{6}$ & $\dfrac{6}{6}$\\ \hline
		\end{tabular}
		\end{center}
		\caption[]{Probabilities associated to the Wilcoxon's rank sum test}
	\end{table}
	This table being constructed, suppose that we observe for the sum of rank of treatment variables: $W_S=7$. The threshold of a one-sided test would then give in conformity with the table above:
	
	or if we get $W_S=3$:
	
	So we would reject the null hypothesis of an identical distribution between the two samples at any upper limit (or lower, respectively) fixed in advance by laboratory policy ... in unilateral or bilateral test (reason why some statistical softwares give unilateral test values + bilateral test values at the same time).
	
	Two very important things you need to notice for what will follow is that:
	\begin{enumerate}
		\item First in the construction of the above table (where we take again the first part here):
		\begin{table}[H]
			\begin{center}
				\definecolor{gris}{gray}{0.85}
					\begin{tabular}{|c|c|c|c|c|c|}
						\hline
		\multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Value of $W_S$} }} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$3$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$4$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$5$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$6$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$7$}}\\ \hline
				\multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Probability}}} & $\dfrac{1}{6}$ & $\dfrac{1}{6}$ & $\dfrac{2}{6}$ & $\dfrac{1}{6}$ & $\dfrac{1}{6}$\\ \hline
			\end{tabular}
			\end{center}
		\end{table}
		we see there is a symmetry at the value $5$, which means that the law $W_S$ is symmetrical in this particular case. But if we take another example with two samples respectively including two control variables and three of treatments (two random variables) we would have:
		\begin{table}[H]
			\begin{center}
				\definecolor{gris}{gray}{0.85}
					\begin{tabular}{|c|c|c|c|c|c|c|c|}
						\hline
		\multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Value of $W_S$} }} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$6$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$7$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$8$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$9$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$10$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$11$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$12$}}\\ \hline
				\multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Probability}}} & $\dfrac{1}{10}$ & $\dfrac{1}{10}$ & $\dfrac{2}{10}$ & $\dfrac{2}{10}$ & $\dfrac{2}{10}$ & $\dfrac{1}{10}$ & $\dfrac{1}{10}$\\  \hline
			\end{tabular}
			\end{center}
		\end{table}
		the reader can check that whatever the number of samples and the number of variables and control treatment, the probability table above is always balanced (well there is a mathematical proof of this but I find it inelegant). But in fact it is pretty intuitive, like combinations $C_{n_x}^{n_x+n_y}$ are independent of the fact that the ranks are sorted in ascending or descending order, it is therefore necessary that there is a symmetry.
		
		\item Secondly the values of the measured variables do not come into account in this parametric statistics but only the tabulated values of the ranks with their associated probabilities. Indeed, as you may have noticed, we did not need explicit values of the random variables to build the table above!
	\end{enumerate}
	
	Now, knowing that the law $W_S$ is symmetrical and discreet we would like to calculate its expected mean.
	
	The smallest possible value of $W_S$ is assuming it is in the sample $X$ (computer algorithms automatically determine in which sample but anyway in practice, samples almost always have the same size):
	
	The largest possible value is naturally (remember that $N=n_x+n_y$):
	
	The expected mean of the sum of the ranks of one of the two samples is then:
	
	Then finally:
	
	To calculate the variance, which will be useful to us to make if necessary an approximation that we will see further below, appears (unfortunately) the covariance because knowledge of the ranks give partial information about other ranks. So we have:
	
	We already know with what we have just proved above that:
	
	The problem remains the term with covariance. For its calculation there are rigorous techniques into several pages and... a tip that is much shorter. The trick is to use the global ranking variable which we denote $T_i$ with $i=1\ldots n+m$. As the sum of the $T_i$ is a constant, then we have:
	
	It comes then:
	
	We can then take back the initial calculation by replacing the covariances by their expression, the last relation obtained for the covariances calculated on the $T_i$  applying also (which is not necessarily intuitive ... but the trick works) to the $R_i$:
	
	Finally:
	
	This is the same result as the rigorous methodology that can be found in some rare references.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us turn to a practical case for the exact case. So consider $2$ samples having $2$ treatments variables $(X)$ and $2$ control variables $(Y)$ (it is a bit simplistic and absurd as an example but it facilitates the educational aspect ...) we have:
	\begin{gather*}
		X:5.7,3.2\\
		Y:8.1,5.5
	\end{gather*}
	Thus (the treatment variable therefore has the ranks $1$ and $3$ which makes a sum of rank of $4$):
	\begin{gather*}
		\underline{3.2},5.5,\underline{5.7},8.1
	\end{gather*}
	Therefore:
	\begin{gather*}
		X:R_1=3.2,R_3=5.7\\
		Y:R_2=5.5,R_4=8.1
	\end{gather*}
	We have the following table as we have shown above:
	\begin{table}[H]
			\begin{center}
				\definecolor{gris}{gray}{0.85}
					\begin{tabular}{|c|c|c|c|c|c|}
						\hline
		\multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Value of $W_S$} }} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$3$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$4$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$5$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$6$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$7$}}\\ \hline
				\multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Probability}}} & $\dfrac{1}{6}$ & $\dfrac{1}{6}$ & $\dfrac{2}{6}$ & $\dfrac{1}{6}$ & $\dfrac{1}{6}$\\  \hline
			\end{tabular}
			\end{center}
	\end{table}
	with in this case:
	
	If we choose the traditional bilateral confidence threshold level to $5\%$ we have according to the table above that:
	
	So in other words we see that there is:
	
	of cumulative probability that $W_S$ is between $3$ and $7$ (the bar above the $6$ means for reminder that this digit is repeated to infinity). So $4$ is necessarily included in the bilateral range of $95\%$ ... and we do not the reject the null hypothesis as what the two samples are not different. The corresponding $p$-value in bilateral is then the half of $33.\bar{3}\%$.\\
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In fact if we wanted to make an interesting computational example by playing with a bilateral threshold of $5\%$ (or $2.5\%$ on either side) we should have at least $2$ samples with $4$ random variables, that is to say $70$ possible combinations of  ranks. Below $4$ random variables per sample, it is clear that the two-tailed test at a level of $95\%$  will be such that we will almost never reject the null hypothesis of equality...
	\end{tcolorbox}
	\end{tcolorbox}
	If the size of the two samples is large enough (most practitioners consider that each sample must be at least $20$ individuals), it has been shown by simulations that we can make the following approximation (used by many statistical software):
	
	obviously always determining the $p$-value in bilateral. With the previous example (with only $4$ measurement in total), we have therefore:
	
	Which corresponds to a cumulative probability of $21.93\%$. So the corresponding bilateral $p$-value is about $(1-22\%)/2\cong 34\%$ (compared to the value of about $33\%$ with the exact case).
	
	\paragraph{Mann-Whitney Rank Sum Test}\index{statistical tests!Mann-Whitney rank sum test}\mbox{}\\\\ 
	The "\NewTerm{Mann-Whitney rank sum test}\index{Mann-Whitney rank sum test}" is also a nonparametric adequation test, very simple, which can be deduced from the Wilcoxon rank sum test. Furthermore it is inspired to the point that we sometimes name it in the industry "\NewTerm{Wilcoxon-Mann-Whitney test}\index{Wilcoxon-Mann-Whitney test}" or "\NewTerm{Wilcoxon-Mann-Whitney adequation test}\index{Wilcoxon-Mann-Whitney adequation test}" or "\NewTerm{MWW test}" (without specifying each time in the name that it is based on the sum of ranks).
	
	The purpose of this test, as for Wilcoxon rank sum test, is to find a way to verify that two independent samples not necessarily of the same size are from the same law or not (verbatim come from a same population or not) but with a different approach!
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Just as the  Wilcoxon rank sum test, the Mann-Whitney rank sum test may well be used for ordinal variables (categorical but so long as they are in an acceptable number).
	\end{tcolorbox}
	Some softwares also generate confusion because they propose the Wilcoxon rank sum test under the name of Mann-Whitney test... and vice-versa ... and most do not indicate or do not offer always the choice between the exact or approximate version... And furthermore the Wilcoxon rank sum test and that of the Signed rank test that we will see further below are not differentiated .... so be careful! This is typically a problem whose source is the absence of an ISO standard to define the terminology and options that should be available in scientific softwares...
	
	To see what is this test, let us build the rank table using two samples including two control variables and three measurement variables, then we have:
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|c|c|c|}
					\hline
	\multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Possible ranks} \\ \textbf{(Controls)}}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Possible ranks} \\ \textbf{(Treatments)}}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}$W_S$} \\ \hline
			$1,2$ & $3,4,5$ & $12$\\ 
			$1,3$ & $2,4,5$ & $11$\\
			$1,4$ & $2,3,5$ & $10$\\  
			$1,5$ & $2,3,4$ & $9$\\ \hline
			$2,3$ & $1,4,5$ & $10$\\
			$2,4$ & $1,3,5$ & $9$\\  
			$2,5$ & $1,3,4$ & $8$\\ \hline
			$3,4$ & $1,2,5$ & $8$\\ 
			$3,5$ & $1,2,4$ & $7$\\ \hline
			$4,5$ & $1,2,3$ & $6$\\ \hline
		\end{tabular}
		\end{center}
		\caption[]{Representation of ranks 2 control variables and 3 treatment variables}
	\end{table}
	from which we deduce the following table:
	\begin{table}[H]
			\begin{center}
				\definecolor{gris}{gray}{0.85}
					\begin{tabular}{|c|c|c|c|c|c|c|c|}
						\hline
		\multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Value of $W_S$} }} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$6$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$7$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$8$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$9$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$10$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$11$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$12$}}\\ \hline
				\multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Probability}}} & $\dfrac{1}{10}$ & $\dfrac{1}{10}$ & $\dfrac{2}{10}$ & $\dfrac{2}{10}$ & $\dfrac{2}{10}$ & $\dfrac{1}{10}$ & $\dfrac{1}{10}$\\  \hline
			\end{tabular}
			\end{center}
	\end{table}
	Now imagine that we have another experience to be analysed using two samples having $3$ control variables and $2$ treatment variables (therefore the symmetric or the previous case!!!), then we have:
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|c|c|c|}
					\hline
	\multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Possible ranks} \\ \textbf{(Controls)}}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Possible ranks} \\ \textbf{(Treatments)}}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}$W_S$} \\ \hline
			$3,4,5$ & $1,2$ & $3$\\ 
			$2,4,5$ & $1,3$ & $4$\\
			$2,3,5$ & $1,4$ & $5$\\  
			$2,3,4$ & $1,5$ & $6$\\ \hline
			$1,4,5$ & $2,3$ & $5$\\
			$1,3,5$ & $2,4$ & $6$\\  
			$1,3,4$ & $2,5$ & $7$\\ \hline
			$1,2,5$ & $3,4$ & $7$\\ 
			$1,2,4$ & $3,5$ & $8$\\ \hline
			$1,2,3$ & $4,5$ & $9$\\ \hline
		\end{tabular}
		\end{center}
		\caption[]{Representation of ranks 3 control variables and 2 treatment variables}
	\end{table}
	from which we deduce the following table (the reader will notice that this is exactly the same table as the previous regarding the probabilities!!):
	\begin{table}[H]
			\begin{center}
				\definecolor{gris}{gray}{0.85}
					\begin{tabular}{|c|c|c|c|c|c|c|c|}
						\hline
		\multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Value of $W_S$} }} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$3$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$4$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$5$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$6$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$7$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$8$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$9$}}\\ \hline
				\multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Probability}}} & $\dfrac{1}{10}$ & $\dfrac{1}{10}$ & $\dfrac{2}{10}$ & $\dfrac{2}{10}$ & $\dfrac{2}{10}$ & $\dfrac{1}{10}$ & $\dfrac{1}{10}$\\  \hline
			\end{tabular}
			\end{center}
	\end{table}
	Well the idea of the Mann-Whitney rank sum test is quite simple. Indeed, rather than tabulate symmetric situations, it is enough to simply subtract at each value $W_S$, the value $W_{S,\min}$ so that each table is identical and only one of the two is helpful. Let us see this with the first table:
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|c|c|c|}
					\hline
	\multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Possible ranks} \\ \textbf{(Controls)}}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Possible ranks} \\ \textbf{(Treatments)}}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}$W_s-\dfrac{1}{2}n_x(n_x+1)
$} \\ \hline
			$1,2$ & $3,4,5$ & $6$\\ 
			$1,3$ & $2,4,5$ & $5$\\
			$1,4$ & $2,3,5$ & $4$\\  
			$1,5$ & $2,3,4$ & $3$\\ \hline
			$2,3$ & $1,4,5$ & $4$\\
			$2,4$ & $1,3,5$ & $3$\\  
			$2,5$ & $1,3,4$ & $2$\\ \hline
			$3,4$ & $1,2,5$ & $2$\\ 
			$3,5$ & $1,2,4$ & $1$\\ \hline
			$4,5$ & $1,2,3$ & $0$\\ \hline
		\end{tabular}
		\end{center}
		\caption[]{Symmetrization ($2$ control/$3$ treatments)}
	\end{table}
	We deduce the following table:
	\begin{table}[H]
			\begin{center}
				\definecolor{gris}{gray}{0.85}
					\begin{tabular}{|c|c|c|c|c|c|c|c|}
						\hline
		\multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Value of $W_{XY}$} }} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$0$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$1$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$2$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$3$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$4$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$5$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$6$}}\\ \hline
				\multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Probability}}} & $\dfrac{1}{10}$ & $\dfrac{1}{10}$ & $\dfrac{2}{10}$ & $\dfrac{2}{10}$ & $\dfrac{2}{10}$ & $\dfrac{1}{10}$ & $\dfrac{1}{10}$\\  \hline
			\end{tabular}
			\end{center}
	\end{table}
	Now imagine that we have another experiment to analyse using two sample having $3$ control variables and $2$ treatments variables, we have therefore using the same idea:
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|c|c|c|}
					\hline
	\multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Possible ranks} \\ \textbf{(Controls)}}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Possible ranks} \\ \textbf{(Treatments)}}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}$W_s-\dfrac{1}{2}n_x(n_x+1)
$} \\ \hline
			$3,4,5$ & $1,2$ & $0$\\ 
			$2,4,5$ & $1,3$ & $1$\\
			$2,3,5$ & $1,4$ & $2$\\  
			$2,3,4$ & $1,5$ & $3$\\ \hline
			$1,4,5$ & $2,3$ & $2$\\
			$1,3,5$ & $2,4$ & $3$\\  
			$1,3,4$ & $2,5$ & $4$\\ \hline
			$1,2,5$ & $3,4$ & $4$\\ 
			$1,2,4$ & $3,5$ & $5$\\ \hline
			$1,2,3$ & $4,5$ & $6$\\ \hline
		\end{tabular}
		\end{center}
		\caption[]{Symmetrization ($3$ control/$2$ treatments)}
	\end{table}
	From which we deduce this time  again exactly the same table as before:
	\begin{table}[H]
			\begin{center}
				\definecolor{gris}{gray}{0.85}
					\begin{tabular}{|c|c|c|c|c|c|c|c|}
						\hline
		\multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Value of $W_{YX}$} }} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$0$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$1$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$2$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$3$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$4$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$5$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$6$}}\\ \hline
				\multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Probability}}} & $\dfrac{1}{10}$ & $\dfrac{1}{10}$ & $\dfrac{2}{10}$ & $\dfrac{2}{10}$ & $\dfrac{2}{10}$ & $\dfrac{1}{10}$ & $\dfrac{1}{10}$\\  \hline
			\end{tabular}
			\end{center}
	\end{table}
	this is why the literature mention that we can take whatever we want!
	
	So to summarize, the Mann-Whitney variant (in the specific case here it is the variant named "\NewTerm{exact Mann-Whitney variant}\index{exact Mann-Whitney test}") consists to tabulate for symmetrical situations a variable denoted by $W_{XY}$ naturally defined by:
	
	Denoted also often in the literature by:
	
	since $W_{XY}\in \left\lbrace 0,1,2,\ldots \right\rbrace$ and therefore:
	
	In the tables that we can find in books, the probabilities are given with the normalized value of $U$. Thus, if we take our previous example, but with the usual notations in practice ($U$ instead of $W_{YX}$):
	\begin{table}[H]
			\begin{center}
				\definecolor{gris}{gray}{0.85}
					\begin{tabular}{|c|c|c|c|c|c|c|c|}
						\hline
		\multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Value of $U$} }} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$0$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$1$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$2$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$3$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$4$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$5$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$6$}}\\ \hline
				\multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Probability}}} & $\dfrac{1}{10}$ & $\dfrac{1}{10}$ & $\dfrac{2}{10}$ & $\dfrac{2}{10}$ & $\dfrac{2}{10}$ & $\dfrac{1}{10}$ & $\dfrac{1}{10}$\\  \hline
			\end{tabular}
			\end{center}
	\end{table}
	We see that the cumulative probability that $U=2$ is of $0.4$. The above table is sometimes given in literature as follows:
	\begin{table}[H]
		\begin{center}
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{ |c|c|c|c| }
			\hline
			\multicolumn{4}{ |c| }{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Factor} $n_2=3$}} \\
			\hline
			$U/n_1$ & $1$ & $2$ & $3$ \\ \hline
			 $0$ & $0.250$ & $0.100$ & $0.050$ \\ \hline
			 $1$ & $0.500$ & $0.200$ & $0.100$ \\ \hline
			 $2$ & $0.750$ & \textbf{0.400} & $0.200$ \\ \hline
			 $3$ & $1$ & $0.600$ & $0.350$ \\ \hline
			 $4$ &  & $0.800$ & $0.500$ \\ \hline
			 $5$ &  & $0.900$ & $0.650$ \\ \hline
			 $6$ &  & $1$ & $\ldots$ \\ \hline
		\end{tabular}
		\end{center}
		\caption[]{Classic representation of the Mann-Whitney test}
	\end{table}
	where we have put in bold the value corresponding to our example ($U=2,n_1=2,n_2=3$). 
	
	Then the practitioner has to choose what he wants to do with these tables if he wishes to make a bilateral or unilateral test.
	
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} It is important to remember that we have showed with an example that we can also take:
	
	than:
	
	as they generate the same tables!\\
	
	\textbf{R2.} $W_{XY}$ is traditionally denoted $U$ by practitioners as we already have seen it, this is why in some books this test is given under the name "\NewTerm{Mann-Whitney $U$ test}\index{Mann-Whithney $U$ test}" with the associated tables under the same name. But take care not to make confusion with the "\NewTerm{Wilcoxon $U$ test}\index{Wilcoxon $U$ test}" sometimes named "\NewTerm{Wilcoxon inversion text}\index{Wilcoxon inversion test}" which is base on the alternation of sample values when grouped (this test will not be developed in this book).
	\end{tcolorbox}
	To see the approximate version (asymptotic) of the Mann-Whitney $U$ test we need the expression of the expected mean and variance. For this, remember that we have seen that the sum of normalized ranks was given by:
	
	But we can also use as we saw:
	
	and as:
	
	with for reminder:
	 
	We therefore have:
	
	The average of the both $U$ is therefore the arithmetic average of the sum. We have therefore:
	
	This means that $U_1$ or $U_2$ must be different enough of the latter average so that we reject the null hypothesis $H_0$ that suppose (for recall) that both samples are issued from the same distribution law. But to determine the $p$-value, we also need the standard deviation. So let us search it!
	
	The standard deviation is the same a for the Wilcoxon rank sum test (as the second term in the expression of $U$ is a constant and therefore is variance is equal to $0$). Therefore, it only remains the variance of the sum of ranks that we have already proved earlier as having for value:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us take the same example as done with the Wilcoxon rank sum test but slightly modified (for the example to be more easy to understand) that is to say:
	\begin{gather*}
		X:\; 5.7,3.2\\
		Y:\; 8.1,5.5,1.2
	\end{gather*}
	That is to say grouped and sorted:
	\begin{gather*}
		\underline{1.2},3.2,\underline{5.5},5.7,\underline{8.1}
	\end{gather*}
	We then have:
	
	So we can choose anyone for the test since both $U$ are equal. If we look at the table created above, with $(U=3,n_1=2,n_2=3)$, we have therefore a cumulative probability of $60\%$ that $U$ is equal to $3$. So we do not reject the null hypothesis $H_0$ (in unilateral) that the two samples are from the same distribution law.\\
	
	The approximation by a Normal distribution gives then:
	
	Then the cumulative probability is $50\%$ with the Normal approximation which corresponds to a $p$-value of $50\%$ in unilateral. Again we do not also not reject the null hypothesis $H_0$ here.
	\end{tcolorbox}
	
	\subparagraph{Treatment of equal ranks in Ranks based tests}\mbox{}\\\\
	When we do a Wilcoxon-Mann-Whitney rank sum test Mann-Whitney or other, equality of ranks may occur.
	
	As theoretical introduction let us take an example:
	
	with the following data:
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|c|c|c|c|c|c|}
			\hline
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Data}}} & $17$ & $17$ & $17$ & $19$ & $21$\\ 
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Rank}}} & ? & ? & ? & $4$ & $5$\\ \hline
		\end{tabular}
		\end{center}
		\caption[]{Example of problem in case of equalities}
	\end{table}
	A conventional solution (among others ...) is to assign to each "?" the average rank. So in this case we have:
	
	The table:
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|c|c|c|}
					\hline
	\multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Possible ranks} \\ \textbf{(Controls)}}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Possible ranks} \\ \textbf{(Treatments)}}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}$W_S$} \\ \hline
			$1,2$ & $3,4,5$ & $12$\\ 
			$1,3$ & $2,4,5$ & $11$\\
			$1,4$ & $2,3,5$ & $10$\\  
			$1,5$ & $2,3,4$ & $9$\\ \hline
			$2,3$ & $1,4,5$ & $10$\\
			$2,4$ & $1,3,5$ & $9$\\  
			$2,5$ & $1,3,4$ & $8$\\ \hline
			$3,4$ & $1,2,5$ & $8$\\ 
			$3,5$ & $1,2,4$ & $7$\\ \hline
			$4,5$ & $1,2,3$ & $6$\\ \hline
		\end{tabular}
		\end{center}
		\caption[]{Representation of ranks 2 control variables and 3 treatment variables}
	\end{table}
	becomes in this case:
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|c|c|c|}
					\hline
	\multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Possible ranks} \\ \textbf{(Controls)}}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Possible ranks} \\ \textbf{(Treatments)}}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}$W_S^{*}$} \\ \hline
			$2,2$ & $2,4,5$ & $11$\\ 
			$2,2$ & $2,4,5$ & $11$\\
			$2,4$ & $2,2,5$ & $9$\\  
			$2,5$ & $2,2,4$ & $8$\\ \hline
			$2,2$ & $2,4,5$ & $11$\\
			$2,4$ & $2,2,5$ & $9$\\  
			$2,5$ & $2,2,4$ & $8$\\ \hline
			$2,4$ & $2,2,5$ & $9$\\ 
			$2,5$ & $2,2,4$ & $8$\\ \hline
			$4,5$ & $2,2,2$ & $6$\\ \hline
		\end{tabular}
		\end{center}
		\caption[]{Representation of ranks 2 control variables and 3 treatment variables\\ in case of equalities}
	\end{table}
	where $W_S^{*}$ (notice the small upper right $^{*}$!) is the Wilcoxon statistics when we are in the presence of statistical equalities. The law $W_S^{*}$ can be more or less different from $W_S$. Indeed:
	\begin{table}[H]
			\begin{center}
				\definecolor{gris}{gray}{0.85}
					\begin{tabular}{|c|c|c|c|c|c|c|c|}
						\hline
		\multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Wilcoxon statistics} }} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$6$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$7$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$8$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$9$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$10$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$11$}} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{$12$}}\\ \hline
				\multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Probability of $W_S$}}} & $\dfrac{1}{10}$ & $\dfrac{1}{10}$ & $\dfrac{2}{10}$ & $\dfrac{2}{10}$ & $\dfrac{2}{10}$ & $\dfrac{1}{10}$ & $\dfrac{1}{10}$\\  \hline
				\multicolumn{1}{c}{\cellcolor[gray]{0.75}\pbox{20cm}{\textbf{Probability of $W_S^{*}$}}} & $\dfrac{1}{10}$ & $0$ & $\dfrac{3}{10}$ & $\dfrac{3}{10}$ & $0$ & $\dfrac{3}{10}$ & $0$\\  \hline
			\end{tabular}
			\end{center}
	\end{table}
	
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Caution! From the mathematical derivation we have seen above, we can claim that at the opposite of the urban legend, the Mann-Whitney test compares the mean ranks - it does not compare medians and does not compare distributions (i.e. stochastic dominance/stochastic superiority)!!! More generally, this test answers this question: What is the probability that a randomly selected value from the population with the larger mean rank is greater than a randomly selected value from the other population? However if the compared distributions are independent and identically distributed and symmetric, then YES it's equivalent as comparing obviously two medians!  It is possible, although not very common, for groups to have different rank sums and yet have equal or nearly equal medians (hence the Mann-Whitney test will be significant even if the medians are equals).
	\end{tcolorbox}
	
	\pagebreak
	\paragraph{One sample Wilcoxon rank sum signed test}\mbox{}\\\\
	The purpose  of the "\NewTerm{Wilcoxon signed rank sum test}\index{Wilcoxon signed rank sum test}" also sometimes named "\NewTerm{Wilcoxon median test}\index{Wilcoxon median test}", is to use a nonparametric technique for checking the symmetry or not of a distribution, and therefore make verbatim a hypothesis on the value of the median. The idea is both simple and subtle.
	
	The principle is that if we compare the differences denoted $D_i$ between individuals of a sample relatively to the median, we know that if we have (for example) an odd number of individuals all different  (not equal), then we have $50\%$ of the data above and below the median. Then, to control that distribution of the values satisfies a certain symmetry, the idea (simple but clever) then consists in:
	\begin{enumerate}
		\item Calculate the differences $|D_i|$ in absolute values relatively to the median.
		
		\item Order these absolute differences by ascending order and assign them a respective rank.
		
		\item Calculate the sum of the ranks of the differences $D_i$ that are negative.
		
		\item Calculate the sum of the ranks of the differences $D_i$ that are positive.
	\end{enumerate}
	and if the sample has a symmetric unimodal distribution (so the median is then confused with the average), there should be a sum of negative ranks $S_{-}$ that is not statistically significantly different from the sum of positive ranks $S^{+}$.
	
	We notice therefore that this hypothesis test for it to work is that the statistical distribution is symmetrical and unimodal!!
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	For recall, during our study of Wilcoxon or Mann-Whitney tests independent samples  seen above (samples which do not necessarily have the same size), we ordered all the values of the two samples and we make a calculation on the ranks of these values. In tests for paired samples (i.e. of the same size), we ordered the differences values (not the original values!) and works on the ranks of the \underline{differences}!!!
	\end{tcolorbox}
	According to the idea (principle) described above, the sum of the rank bearing the sign $-$ has then for average:
	
	But we have already shown that the expected mean of the binomial distribution is:
	
	And as in our case $N$ is equal to $1$ (only one value ...) and $p$ is equal to $1/2$ (one in two chance of having a negative sign), it comes immediately using the proofs of the section Sequences and Series:
	
	and for the variance using also the results of the section Sequences and Series:
	
	and again using the variance of the binomial distribution and the results of section Sequences and Series:
	
	Obviously the sum of ranks of negative differences (respectively positive) will be at the minimum equal to zero and at maximum $n(n+1)/2$. Therefore, the expected mean in the case of a bilateral test should not be too close to one of these two extreme values.
	
	In the case where $n$ is large enough (more than thirty), we can use the approximation of the reduced centered Normal distribution for the variable:
	
	where $S_{-}$ is the sum of ranks of negative differences.
	
	Finally let us notice that empirically if some differences from the median are zero, they will not be included in the ranks. If differences are equal we will take an average rank...
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us start with the case with one sample compared to its experimental median (at the opposite with the comparison to a hypothesized a priori median when we consider the distribution symmetric and unimodal). Consider that we measured the following values for the diameter of a piece:
	\begin{gather*}
		39, 20.2, 40, 32.2, 30.5, 26.5, 42.1, 45.6, 42.1, 45.6, 42.1, 29.9, 40.9
	\end{gather*}
	We wish to know if the calculated experimental median (which value is equal to $40$ in this case) of this sample can not be rejected as a main indicator to threshold level of $5\%$ in bilateral (which will be the case if the number of positive and negative differences is fairly balanced). We then construct the following table:
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|c|c|c|c|c|c|}
					\hline
					\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Measurement}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Differences}}  & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Absolute value}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Rank}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}$R_{+}$} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}$R_{-}$}\\ \hline
		$39$ & $-1$ & $1$ & $2$ & & $2$ \\ \hline
		$20.2$ & $-19.8$ & $19.8$ & $11$ & & $11$ \\ \hline
		$40$ & $0$ & $0$ &  & &  \\ \hline
		$32.2$ & $-7.8$ & $7.8$ & $6$ & & $6$  \\ \hline
		$30.5$ & $-9.5$ & $9.5$ & $8$ & & $9$  \\ \hline
		$26.5$ & $-13.5$ & $13.5$ & $10$ & & $10$  \\ \hline
		$42.1$ & $2.1$ & $2.1$ & $4$ & & $4$  \\ \hline
		$45.6$ & $5.6$ & $5.6$ & $6.5$ & & $6.5$  \\ \hline
		$42.1$ & $2.1$ & $2.1$ & $4$ & $4$ &  \\ \hline
		$45.6$ & $5.6$ & $5.6$ & $6.5$ & $6.5$ &  \\ \hline
		$42.1$ & $2.1$ & $2.1$ & $4$ & $4$ &  \\ \hline
		$45.6$ & $5.6$ & $5.6$ & $6.5$ & $6.5$ &  \\ \hline
		$42.1$ & $2.1$ & $2.1$ & $4$ & $4$ &  \\ \hline
		$29.9$ & $-10.1$ & $10.1$ & $9$ & & $9$  \\ \hline
		$40.9$ & $0.9$ & $0.9$ & $1$ & $1$ &  \\ \hhline{|=|=|=|=|=|=|}
		& & \textbf{Sum:} & & $26$ & $46$ \\ \hline
			\end{tabular}
		\end{center}
	\end{table}
	At first glance the equality of ranks does not look great but nevermind... we will going a little bit further...\\
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	According to some textbooks the sum of ranks does not give the same value as there are several techniques to calculate the ranks of values that are the equals... However, we have chosen the one used by Minitab software that is customary in the scientific community and which corresponds to that which we have already presented the rules earlier.
	\end{tcolorbox}
	If we consider that the number of individuals is sufficient ... we use the approximation (even if in this case the conditions are not satisfied):
	
	Thus with this example:
	
	and respectively:
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	The first case corresponds with the approximation using a Normal distribution to a cumulative probability of $29.13\%$ obtained with the English versions of Microsoft Excel 14.0.6123 using the function:
	
	\begin{center}
	\texttt{=NORM.S.DIST(-0.549,TRUE)}
	\end{center}
	
	and thus obviously a bilateral $p$-value of about $2\cdot 29.13\%\cong 58.26\%$.\\
	
	The second case corresponds with the approximation using a Normal distribution to a cumulative probability of $84.62\%$ obtained with the English versions of Microsoft Excel 14.0.6123 using the function:
	\begin{center}
	\texttt{=NORM.S.DIST(1.02,TRUE)}
	\end{center}
	which corresponds to a bilateral $p$-value of about $2\cdot(1-0.8462)/2=30.76\%$. Minitab gives a bilateral $p$-value of $32\%$ as it does not use the Normal approximation.\\
	
	Therefore for the both calculations we can prudently (and sadly) do not reject the null hypothesis $H_0$ as $40$ (the median) is in the middle of the confidence interval (also the sign test leads to the same conclusion).
	\end{tcolorbox}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	A software like Minitab even if offering the Wilcoxon median test for $1$ sample gives for the median a value of $36.5$ and gives for the median confidence interval the values $31.1$ and $42.1$. If we apply the bootstrapping method presented in detail in section of Theoretical Computing we get for estimated median $40$ (and an average of $38.733$) and as interval $30.50$ and $42.10$... Well in any case we anyway not to reject the null hypothesis $H_0$ but it is still boring not to know how these values are calculated in Minitab...
	\end{tcolorbox}
	
	\pagebreak
	\paragraph{Wilcoxon rank sum signed test for two paired samples}\mbox{}\\\\
	The "\NewTerm{Wilcoxon rank sum signed test for two paired samples}\index{Wilcoxon rank sum signed test for two paired samples}" or simply "\NewTerm{Wilcoxon signed rank test}\index{Wilcoxon signed rank test}" is $100\%$ based on the principle of the test sample with one sample. The only difference is that the null or alternative hypothesis are based on the difference in the median of the data taken in pairs (two by two) of each of the samples. In most cases, the null hypothesis $H_0$ is that the median of the differences is zero against the alternative hypothesis $H_A$ that it is statistically significantly different from zero.
	
	Like the $T$-test for paired samples, the Wilcoxon rank sum test applies to two-sample designs involving repeated measures, matched pairs, or "before" and "after" measures!!
	
	As the mathematical developments are the same as for the one sample test let us attack it directly by an example.
	
	First let us just emphasize that by extension, that an hypothesis for that test to work is that the statistical distribution of differences is therefore symmetrical and unimodal!!
	
	Many introductory texts motivate the signed rank test as a test of median difference, or more rarely in our experience, mean difference, without mentioning that two fairly strict assumptions are required for this interpretation:
	\begin{itemize}
		\item The distribution of both groups must have the same shape.

		\item The variance of both groups must be equal.
	\end{itemize}

	If both these assumptions are true, then the signed rank test can validly be interpreted as having a null hypothesis of equal medians (or equal means).
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Notice that in many statistical softwares, $R_+$ (see below) is denoted by the letter $V$.
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	We have two different softwares ($S1, S2$) to compare and we want to submit to $12$ tasks ($T1, T2, T3, \ldots , T12$) of specific but similar calculations for each of the software. We would like to know if the software have a processing time that is statistically significantly different or not and if so which one is better.\\
	
	We then have the following table where the time is in minutes and where the differences $ x_i-y_i $ are denoted $ d_i $:
	\begin{table}[H]
		\centering
			\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|c|c|c|c|c|c|c|c|}
					\hline
					\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Task}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}$S1$}  & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}$S2$} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}$d_i$} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}$|d_i|$} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Rank}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}$R_{+}$}  & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}$R_{-}$}\\ \hline
		$T1$ & $24.0$ & $23.1$ & $0.9$ & $0.9$ & $1$ & $1$ & \\ \hline
		$T2$ & $16.7$ & $20.4$ & $-3.7$ & $3.7$ & $4$ & & $4$\\ \hline
		$T3$ & $21.6$ & $17.7$ & $3.9$ & $3.9$ & $5$ & $5$ & \\ \hline
		$T4$ & $23.7$ & $20.7$ & $3.0$ & $3.0$ & $2.5$ & $2.5$ & \\ \hline
		$T5$ & $37.5$ & $42.1$ & $-4.6$ & $4.6$ & $6$ & & $6$ \\ \hline
		$T6$ & $31.4$ & $36.1$ & $-4.7$ & $4.7$ & $7$ & & $7$ \\ \hline
		$T7$ & $14.9$ & $21.8$ & $-6.9$ & $6.9$ & $10$ & & $10$ \\ \hline
		$T8$ & $37.3$ & $40.3$ & $-3.0$ & $3.0$ & $2.5$ & & $2.5$ \\ \hline
		$T9$ & $17.9$ & $26.0$ & $-8.1$ & $8.1$ & $11$ & & $11$ \\ \hline
		$T10$ & $15.5$ & $15.5$ & $0.0$ & $0.0$ & $-$ & &  \\ \hline
		$T11$ & $29.0$ & $35.4$ & $-6.4$ & $6.4$ & $9$ & & $9$  \\ \hline
		$T12$ & $19.9$ & $25.5$ & $-5.6$ & $5.6$ & $8$ & & $8$  \\ \hhline{|=|=|=|=|=|=|=|=|}
		& & &  & & \textbf{Sum:} & $8.5$ & $57.5$ \\ \hline
			\end{tabular}
	\end{table}
	We already see that the software $S1$ is generally faster than $S2$ without using any tables of the Wilcoxon's exact sign test, we can therefore already say that qualitatively the difference is statistically significant.\\
	
	If we consider that the number of individuals (sample sizes) is sufficient ... we use the Normal approximation (even in this special example the conditions are not satisfied):
	
	That is to say in this example:
	
	and respectively:
	
	The first case corresponds in approximation to a Normal distribution with a cumulative probability of $0.836\%$ obtained with the English version of Microsoft Excel 14.0.6123 using the function:
	\begin{center}
	\texttt{=NORM.S.DIST(-2.392,TRUE)}
	\end{center}
	and thus a bilateral $p$-value of $2\cdot 0.836\%\cong 1.68\%$.\\
	
	The second case corresponds in approximation to a Normal distribution with a cumulative probability of $92.668\%$ obtained with the English version of Microsoft Excel 14.0.6123 using the function:
	\begin{center}
	\texttt{=NORM.S.DIST(1.453,TRUE)}
	\end{center}
	and thus a bilateral $p$-value of $2\cdot (1-0.92774)\%\cong 14.452\%$. With a software like Minitab 15.1.2 which does not offer the Wilcoxon test for paired samples but for which there is a trick to run it anyway, we get a $p$ -value of $3.3\%$ (same result as with the statistical software \texttt{R} which, on the other hand, has this Wilcoxon paired test implemented!). Other softwares gives a $p$-value always less than $5\%$ (but the values differ from one software to another...). \\

Therefore, with manual calculations and using the Normal approximation, we would reject the null hypothesis $H_0$ at the threshold of $5\% $ (if we only focus on the first result). It is the same with \texttt{R} and Minitab softwares, we would also reject the null hypothesis $H_0$ at the threshold of $5\%$!
	\end{tcolorbox}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	As far as we know there are at least two different ways of calculating the variance of the Wilcoxon rank sum signed test for paired samples (and this is without taking into account the continuity correction which adds a $+0.5$ to the numerator). This is why most of the times results differs between statistical softwares.
	\end{tcolorbox}
	
	\pagebreak
	\paragraph{Kruskal-Wallis test}\index{statistical tests!Kruskal-Wallis test}\mbox{}\\\\
	The Kruskal-Wallis test is a nonparametric test often compared (a bit faster ...) to a nonparametric one-way canonical ANOVA with fixed to compare if two or more populations have the same median (null hypothesis $H_0$) except that it does not requires the assumptions necessary for the parametric version of the ANOVA. When several compared populations pass through this test, the KW-test does not say which population is statistically significantly different but only that there is at least one who is. In fact, as we will show, the KW-test is only an extension of the Mann-Whitney U test seen  earlier for a number of populations greater than or equal to three.
	
	To study this test, we will assume that we have only two populations and we will afterwards make an intuitive generalization. This approach is that one that would have used Wilcoxon before that Kruskal and Wallis make the rigorous proof of the generalized case.
	
	To study this test, let us first recall that (relations whose origin and verbatim the proof have already been explained during our study of the Mann-Whitney test seen above) the average of the sum of ranks and the variance of the sum of ranks is given by:
	
	in the case where there is no duplicate values. Under this assumption, remember that $\bar{R}$ can be assimilated to the rank of the median value (in the case of an odd number of values).
	
	Let us recall that the average values of $n$ draws without replacement among $N$ will be close to a Normal distribution when $N$ big enough, and we have already proved at the beginning of this section that:
	
	and if the population is not very large, the variance must be corrected by the correction factor on finite population that we have already prove earlier:
	
	Therefore it comes:
	
	We then have if we are concerned with the ranks (the variance of the ranks being the true variance: there is no estimator!):
	
	Now, to form a reduced centered Normal variable $Z$ we can center and reduce the random variable $\hat{\bar{R}}$ obtained by sampling by writing:
	
	where $\hat{\bar{R}}$ is the average of the sum of the ranks of a sample of the population. And in fact all the trick of the idea behind  the Kruskal-Wallis test is here: the statistical distribution of the average of the sum of the ranks of a large number of samples of $N$ values has approximately a Normal distribution (read again if necessary our study of limits of drawings samples of a population at the beginning of this section)!
	
	Let us take the square:
	
	The approximation by the chi-square law is only valid if $n$ is large enough as we have already discussed in detail during our study of the test chi-square adjustment.
	
	And so the parenthesis of the first equality is equal to the square of the deviation of the rank to the median. This is why we often say that this is a median test (but that is an abusive shortcut).
	
	Before continuing, let us insist on the fact that the scenario in which we find ourselves is that of a random sample of $n$ items from $N$, which is equivalent to end up with two samples (one of size $n$ and the other of size $N-n$) of the same law (verbatim from the same population/distribution). It then comes that (relations that we will use later):
	
	and by extension of the case with one sample, if we denote by $R_i$ the sum of ranks of the sample numbers $i$, we also have:
	
	It follows that if we write for what will follow:
	
	where $R$ is the sum of the ranks of the sample $i=1$, we have:
	
	If we now write the relation proved above:
	
	as follows (this is a clever development in reverse order... starting from the third line):
	
	and we find ourselves at the end with the fact that we worked from the beginning with two samples, one of size $n$ and therefore the other (verbatim by sampling) of size $N-n$.
	
	The previous result (which was the one desired from the beginning) can be generalized as follows under the named "\NewTerm{Kruskal-Wallis H test}\index{Kruskal-Wallis H test}" to a given confidence level in unilateral (sometimes this relation is written without the parentheses around the sum which can lead to a bad reading):
	
	and if all the $n_i$ are equals, we fall back on a well known notation of the previous relation:
	
	The approximation according to a chi-square law, however, can be discussed when the sample size $(c)$ is small (refer to our study of the Chi-square law earlier).
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us take the original example of Kruskal-Wallis. We consider that we have three machines that are the same at the origin but two of them have undergone some modifications. We measured the daily production a number of times and got the following table:
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|c|c|c|c|c|c|c|c|}
					\hline
					\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Task}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Standard}}  & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Rank}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Modified} $1$} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Rank}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Modified} $2$} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Rank}} \\ \hline
		$340$ & $5$ & $339$ & $4$ & $347$ & $10$ &\\ \hline
		$345$ & $9$ & $333$ & $2$ & $343$ & $7$ &\\ \hline
		$330$ & $1$ & $344$ & $8$ & $349$ & $11$ &\\ \hline
		$342$ & $6$ &  & & $355$ & $12$ &\\ \hline
		$338$ & $3$ &  &  &  &  &\\ \hline
		\multicolumn{7}{|c|}{- - - - - - - - - - - - - - - - - - - -}\\ \hline
		$n$ & $5$ &  & $3$  &  & $4$ & $12$\\ \hhline{|=|=|=|=|=|=|=|=|}	
		$R$ & $24$ &  & $14$  &  & $40$ & $78$\\ \hline
		$R^2/n$ & $11.5$ &  & $65.33$  &  & $400$ & $580.53$\\ \hline
			\end{tabular}
			\caption[]{Example table for the Kruskal-Wallis test}
		\end{center}
	\end{table}
	We then have well:
	
	and:
	
	Be we have:
	
	That can be obtained easily with Microsoft Excel 14.0.7166 in English with the function:
	\begin{center}
	\texttt{=1-CHISQ.DIST(5.656,2,TRUE)}
	\end{center}
	In the present case, to a threshold level of $5\%$, so we are at the limit with the approximation by a chi-square distribution. As shown by Kruskal and Wallis, a Monte Carlo simulation gives a $p$-value of $0.049$.\\
	
	In short, in this situation it would be better have to reject the null hypothesis as that  the productions are similar. And therefore preferred the alternative hypothesis such that these are rather different. A recommendation is to redo the test by pairs to see what is statistically significantly one by one.
	\end{tcolorbox}
	
	\pagebreak
	\paragraph{Friedman Test}\index{statistical tests!Friedman test}\mbox{}\\\\
	The Friedman test, recommended by the norm NF ISO 8587 for sensory analysis (ranking test), consider an experiment with two factors (the first being considered as the treatment and the second as the blocks of tests as well as the ANOVA with two-fixed factor without repetition) that is analysed using the ranks, as the measurement values do not satisfy the conditions for the application of ANOVA. However, instead of ANOVA, Friedman test applies to paired data as we shall see now.
	
	Let us associate, as we have already done it several times, the theory to an example starting from the following table where $8$ subjects (blocks) denoted by $B$ under hypnosis were subjected to $4$ emotions (treatments) denoted by $T$. Their epidermal electrical potential was measured (millivolts) in each case (and the order of the treatments was randomized):
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
			\hline
			\rowcolor[gray]{0.75}& \multicolumn{8}{|c|}{\textbf{Subjects ($B$)}}\\ \hline
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Emotions ($T$)}} & 
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$1$}}  & 
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$2$}} & 
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$3$}} & 
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$4$}} & 
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$5$}} & 
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$6$}} &
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$7$}} &
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$8$}} \\ \hline
			\cellcolor[gray]{0.75}\textbf{Fear} & $23.1$ & $57.6$ & $10.5$ & $23.6$ & $11.9$ & $54.6$ & $21.0$ & $20.3$ \\ \hline
			\cellcolor[gray]{0.75}\textbf{Joy} & $23.1$ & $57.6$ & $10.5$ & $23.6$ & $11.9$ & $54.6$ & $21.0$ & $20.3$ \\ \hline
			\cellcolor[gray]{0.75}\textbf{Sadness} & $22.5$ & $53.7$ & $10.8$ & $21.1$ & $13.7$ & $39.2$ & $13.7$ & $16.3$ \\ \hline
			\cellcolor[gray]{0.75}\textbf{Calm} & $22.6$ & $53.1$ & $8.3$ & $21.6$ & $13.3$ & $37.0$ & $14.8$ & $14.8$ \\ \hline
			\end{tabular}
			\caption[]{Example table for the Friedman test}
	\end{table}
	The central and subtle idea is not to assign a rank to the entire population of the measures as is the case for the Kruskal-Wallis test (we would then lose the concept of the blocks: verbatim the second factor) but block by block all supposed therefore independent of each other.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	We will not deal (as well as in our study of the Kruskal-Wallis test) the situation where some measurements are equal with some others in the same block, the existing proofs being not really convincing in my point of view.
	\end{tcolorbox}
	So, to every value $\left\lbrace x_{tb}\right\rbrace_{T\times B}$ of the table we now associate the rank $\left\lbrace r_{tb}\right\rbrace_{T\times B}$ corresponding to each treatment. Which will give us:
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
			\hline
			\rowcolor[gray]{0.75} & \multicolumn{8}{|c|}{\textbf{Subjects ($B$)}}\\ \hline
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Emotions ($T$)}} & 
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$1$}}  & 
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$2$}} & 
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$3$}} & 
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$4$}} & 
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$5$}} & 
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$6$}} &
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$7$}} &
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$8$}} \\ \hline
			\cellcolor[gray]{0.75}\textbf{Fear} & $4$ & $4$ & $3$ & $4$ & $1$ & $4$ & $4$ & $3$ \\ \hline
			\cellcolor[gray]{0.75}\textbf{Joy} & $3$ & $2$ & $2$ & $1$ & $4$ & $3$ & $1$ & $4$ \\ \hline
			\cellcolor[gray]{0.75}\textbf{Sadness} & $1$ & $3$ & $4$ & $2$ & $3$ & $2$ & $2$ & $2$ \\ \hline
			\cellcolor[gray]{0.75}\textbf{Calm} & $2$ & $1$ & $1$ & $3$ & $2$ & $1$ & $3$ & $1$ \\ \hline
			\end{tabular}
	\end{table}
	Well now that we have built such a kind of nonparametric ANOVA table with two fixed factors without repetitions what do we do? What is the idea? Well the basic idea is the same as the Kruskal-Wallis test: we use the property of the average of the sum of the ranks but while having in mind that this time that the numbering (ranking) was not made on all measures of the table but block by block.
	
	In the context of our particular example we have therefore:
	\setlength\extrarowheight{5pt}
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
			\hline
			\rowcolor[gray]{0.75} & \multicolumn{8}{|c|}{\textbf{Subjects ($B$)}} & {} & {}\\ \hline
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Emotions ($T$)}} & 
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$1$}}  & 
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$2$}} & 
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$3$}} & 
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$4$}} & 
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$5$}} & 
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$6$}} &
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$7$}} &
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$8$}} & {} & {}\\ \hline
			\cellcolor[gray]{0.75}\textbf{Fear} & $4$ & $4$ & $3$ & $4$ & $1$ & $4$ & $4$ & $3$ & $R_1=27$ & $\bar{R}_1=3.375$\\ \hline
			\cellcolor[gray]{0.75}\textbf{Joy} & $3$ & $2$ & $2$ & $1$ & $4$ & $3$ & $1$ & $4$ & $R_2=20$ & $\bar{R}_2=2.50$ \\ \hline
			\cellcolor[gray]{0.75}\textbf{Sadness} & $1$ & $3$ & $4$ & $2$ & $3$ & $2$ & $2$ & $2$ & $R_3=19$ & $\bar{R}_3=2.375$ \\ \hline
			\cellcolor[gray]{0.75}\textbf{Calm} & $2$ & $1$ & $1$ & $3$ & $2$ & $1$ & $3$ & $1$ & $R_4=14$ & $\bar{R}_4=1.75$  \\ \hline
			\end{tabular}
	\end{table}
	\setlength\extrarowheight{0pt}
	and in case of no influence of treatments, we expect to have:
	
	or alternatively (this is equivalent):
	
	If there is no influence of the treatments this last four values should be equal and fluctuate around:
	
	We can feel that the fluctuation of the $\bar{R}_t$ around $\mu_R$ must follow a Normal centered distribution if there is not any influence (there is a proof of this in the original article of Friedman but it has sometimes some unexplained gaps that refrain us to present it). We can the also reduce the Normal law such as:
	
	It is not always intuitive that the standard error is obtained by dividing the root of $B$ (the number of blocks) because the majority of practitioners have intuition to divide by the root of $T$ of the number of treatments when they study the theoretical aspect of Friedman test. But this can be verified with a numerical application or by remembering that the calculation of the variance $\sigma_R$ is done from the ranks of $B$ for a given treatment, ranks which the values (in the example above these values are $8$ time between $1$ and $4$) are obviously assumed for a given treatment to be independent and identically distributed.
	
	So we have:
	
	Unlike the Kruskal-Wallis, we don't do any sampling, so we should not correct the deviation with the correction factor on finite population (fpc) to decrease its value.
	
	The idea of Friedmann (at least that is how we will present it) is to say that the standard deviation of the sum of ranks of treatments obtained in the same way as in the Kruskal-Wallis test (which origin has been detailed in our study of the Mann-Whitney test):
	
	is this time only an estimator of the true standard deviation and we must used the relation between the unbiased and biased estimator to correct this estimate (relation proved during our study of the estimators):
	
	Therefore:
	
	where we have removed a degree of freedom to the chi-square for the reason already met many times in this section.
	
	Then after some elementary simplifications we get the "\NewTerm{Q Friedman test}\index{Q Friedman test}" (which is therefore a nonparametric test):
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Going back to our example, then it comes:
	
	The critical value of $\chi^2(T-1)$ at the $5\%$ threshold is $7.81$. So we sadly do not reject the null hypothesis $H_0$ as what treatments have no effect (no difference between the treatments). The cumulative probability corresponding to $6.45$ (thus the $p$-value) is $90\%$.
	\end{tcolorbox}
	
	\pagebreak
	\paragraph{Spearman Rank correlation coefficient}\mbox{}\\\\
	The "\NewTerm{Spearman's rank correlation coefficient}\index{Spearman's rank correlation coefficient}", denoted $R_S$ is the correlation coefficient of the sequence $\left(R(i),S(i)\right),i=1\ldots n,$ of the ranks naturally inspired by the linear correlation coefficient of Pearson that we saw at the beginning of this section:
	
	Let us also take an example before we tackle the theoretical aspect. Consider that the measurement of a sample of size $10$ (we took the same values as that taken for the previous nonparametric rank test studies):
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|c|c|}
			\hline
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}$X$}  & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}$Y$} \\ \hline
			$5.7$ & $8.1$  \\ \hline
			$3.2$ & $5.5$  \\ \hline
			$8.4$ & $3.4$  \\ \hline
			$4.1$ & $7.9$  \\ \hline
			$6.9$ & $4.6$  \\ \hline
			$5.3$ & $1.6$  \\ \hline
			$1.7$ & $8.5$  \\ \hline
			$3.2$ & $7.1$  \\ \hline
			$2.5$ & $8.7$  \\ \hline
			$7.4$ & $5.7$  \\ \hline
			\end{tabular}
	\end{table}
	with their respective ranks in accordance with the idea of Kendall (simple idea but that had to be found!):
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|c|c|c|c|}
			\hline
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}$X$}  & 
	 \multicolumn{1}{c}{\cellcolor[gray]{0.75}$R(i)$}  & \multicolumn{1}{c}{\cellcolor[gray]{0.75}$Y$} & \multicolumn{1}{c}{\cellcolor[gray]{0.75}$S(i)$} \\ \hline
			$5.7$ & $7$ & $8.1$  & $8$\\ \hline
			$3.2$ & $3$ & $5.5$ & $4$  \\ \hline
			$8.4$ & $10$ & $3.4$ & $2$ \\ \hline
			$4.1$ & $5$ & $7.9$ & $7$\\ \hline
			$6.9$ & $8$ & $4.6$ & $3$ \\ \hline
			$5.3$ & $6$ & $1.6$ & $1$  \\ \hline
			$1.7$ & $1$ & $8.5$ & $9$  \\ \hline
			$3.2$ & $4$ & $7.1$ & $6$ \\ \hline
			$2.5$ & $2$ & $8.7$  & $10$ \\ \hline
			$7.4$ & $9$ & $5.7$ & $5$ \\ \hline
			\end{tabular}
	\end{table}
	Now let us show that the above given relation is simplified drastically because the values of $R$, such as those of $S$, browse the first $n$ integers. For this, remember that we have proved in the section of Sequences and Series, that:
	
	Therefore:	
	
	Hence:
	
	We also have proved in the section of Sequences and Series that:
	
	therefore:
	
	Therefore it comes:
	
	Now let us play a little bit more to get a more simplified expression by observing that:
	
	Then it comes:
	
	Then we have:
	
	But we proved that:
	
	Therefore:
	
	Thus we find the famous relation available in all (good) statistics and Monte Carlo simulation\footnote{Indeed, the Spearman coefficient is quite important for the modelling of random correlated variables as we will see it at page \pageref{simulating correlations and dependencies} in the section of Numerical Methods.} textbooks:
	
	The Spearman coefficient takes the same essential properties as the Pearson coefficient namely that:
	
	and is equal to $0$ when the variables are correlated (dot not forget the important subtleties already mentioned in our study of the Pearson coefficient!!!).
	
	Note that this coefficient seems to be defined only for a pair of variables (I have never seen a generalization to a multivariate case at this day).
	
	\paragraph{Kendall Tau-Correlation Coefficient}\label{Kendall tau-correlation coefficient}\mbox{}\\\\
	The "\NewTerm{Kendall tau-b correlation coefficient}\index{Kendall tau-b correlation coefficient}", $\tau_b$, is a nonparametric (rank-statistics) measure of association based on the number of concordances and discordances in paired observations.
	
	It's used in many fields of sociology, engineering and quantitative finance (for an example in quantitative finance see the reference \cite{meissner2013correlation}) and often compared or communicated at the same time with the Pearson's correlation coefficient and Spearman's correlation coefficient!

	Suppose two observations $(X_i , Y_i)$ and $(X_j , Y_j)$. They are said to be \textit{concordant} if they are in the same order with respect to each variable. That is, if:
	\begin{enumerate}
		\item[(1)] $X_{i}<X_{j}$ and $Y_{i}<Y_{j}$, or if
		
		\item[(2)] $X_{i}>X_{j}$ and $Y_{i}>Y_{j}$
	\end{enumerate}	
	They are \textit{discordant} if they are in the reverse ordering for $X$ and $Y$, or the values are arranged in opposite directions. That is, if:
	\begin{enumerate}
		\item[(1)] $X_{i}<X_{j}$ and $Y_{i}>Y_{j},$ or if
		
		\item[(2)] $X_{i}>X_{j}$ and $Y_{i}<Y_{j}$
	\end{enumerate}	
	The two observations are tied if $X_i = X_j$ and/or $Y_i = Y_j$.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The Kendall tau-c $\tau_c$ ignore ties. We won't discuss Kendall’s tau-c as it's not often used anymore. In the absence of ties, both formulas yield identical results.
	\end{tcolorbox}
	
	The total number of pairs that can be constructed for a sample size of $n$ is:
	
	$N$ can be decomposed into these five quantities:
	
	where $P$ is the number of concordant pairs, $Q$ is the number of discordant pairs, $X_0$ is the number of pairs tied only on the $X$ variable, $Y_0$ is the number of pairs tied only on the $Y$ variable, and $(XY)_0$ is the number of pairs tied on both $X$ and $Y$.
	
	The Kendall tau-b for measuring order association between variables $X$ and $Y$ is given by the following formula:
	
	If there are no ties $X_0=Y_0=0$ then we fall back on a common expression that we can find in many textbooks of the Kendall tau-b:
	
	If we use the symmetric difference distance (see first example below), that latter can be written:
	
	This value becomes scaled and ranges between $-1$ (perfect negative monotonous relation among two variables: a lower score on variable $X$ is always associated with a higher score on variable $Y$) and $+1$ (perfect positive monotonous relation: a lower score on variable $X$ is always associated with a lower score on variable $Y$). 
	
	Unlike Spearman's rank correlation coefficient it does estimate a population variance as:
	
	Furthermore, if two variables are independent, $\tau_b = 0$ but the reverse does not always hold: a curvilinear or other non monotonous relation may still exist.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. Suppose that two experts order four wines denoted $\{a, b, c, d\}$ The first expert gives the following order: $E_1=[a, c, b, d],$ which corresponds to the following ranks $R_{1}=[1,3,2,4]$ and the second expert orders the wines as $E_{2}=[a, c, d, b]$ which corresponds to the following ranks $R_{2}=[1,4,2,3] .$ The order given by the first expert is composed of the following $6$ ordered pairs:
	\begin{gather*}
		\text{P}_{1}=\{[a, c],[a, b],[a, d],[c, b],[c, d],[b, d]\}
	\end{gather*}
	The order given by the second expert is composed of the following 6 ordered pairs:
	\begin{gather*}
		\text{P}_{2}=\{[a, c],[a, b],[a, d],[c, b],[c, d],[d, b]\}
	\end{gather*}
	The set of pairs which are in only one set of ordered pairs is:
	\begin{gather*}
		\{[b, d][d, b]\}
	\end{gather*}
	which gives a value of $d_{\Delta}\left(\text{P}_{1}, \text{P}_{2}\right)=2 .$ With this value of the symmetric difference distance we compute the value of the Kendall rank correlation coefficient between the order given by these two experts as:
	\begin{gather*}
		\tau_b=1-\frac{2 d_{\Delta}\left(\text{P}_{1}, \text{P}_{2}\right)}{n(n-1)}=1-\frac{2 \cdot 2}{12}=1-\frac{1}{3}=\dfrac{2}{3} \cong 0.67
	\end{gather*}
	\end{tcolorbox}
	
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	Notice that the result is equal to:
	\begin{gather*}
		\tau_b=\dfrac{P-Q}{P+Q}=\dfrac{5-1}{6}=\dfrac{2}{3} \cong 0.67
	\end{gather*}
	
	E2. Two interviewers ranked $12$ candidates (A through L) for a position. The results from most preferred to least preferred are:
	\begin{itemize}
		\item Interviewer 1: ABCDEFGHIJKL
		
		\item Interviewer 2: ABDCFEHGJILK
	\end{itemize}
	we want to calculate the $\tau_b$.\\
	
	For that purpose we make a table of rankings. The first column, \textit{Candidate} is optional and for reference only. The rankings for \textit{Interviewer 1} should be in ascending order (from least to greatest):
	\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline 
		\rowcolor[gray]{0.75}\textbf{Candidate} & \textbf{Interviewer 1 }& \textbf{Interviewer 2} \\
		\hline A & 1 & 1 \\
		\hline B & 2 & 2 \\
		\hline C & 3 & 4 \\
		\hline D & 4 & 3 \\
		\hline E & 5 & 6 \\
		\hline F & 6 & 5 \\
		\hline G & 7 & 8 \\
		\hline H & 8 & 7 \\
		\hline I & 9 & 10 \\
		\hline J & 10 & 9 \\
		\hline K & 11 & 12 \\
		\hline L & 12 & 11 \\ \hline
	\end{tabular}
	\end{center}
	After we count the number of concordant pairs, using the second column. Concordant pairs are how many larger ranks are below a certain rank. For example, the first rank in the second interviewer’s column is a $1$, so all $11$ ranks below it are larger:
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\begin{center}
	\begin{tabular}{|c|c|c|c|c|}
		\hline 
		\rowcolor[gray]{0.75}\textbf{Candidate} & \textbf{Interviewer 1 }& \textbf{Interviewer 2} & \textbf{Concordant} & \textbf{Discordant}  \\
		\hline A & 1 & 1 & 11 & \\
		\hline B & 2 & 2 & &  \\
		\hline C & 3 & 4 & &  \\
		\hline D & 4 & 3 & &  \\
		\hline E & 5 & 6 & &  \\
		\hline F & 6 & 5 & &  \\
		\hline G & 7 & 8 & &  \\
		\hline H & 8 & 7 & &  \\
		\hline I & 9 & 10 & &  \\
		\hline J & 10 & 9 & &  \\
		\hline K & 11 & 12 & &  \\
		\hline L & 12 & 11 & &  \\ \hline
	\end{tabular}
	\end{center}
	However, going down the list to the third row (a rank of $4$), the rank immediately below ($3$) is smaller, so it doesn't count for a concordant pair:
	\begin{center}
	\begin{tabular}{|c|c|c|c|c|}
		\hline 
		\rowcolor[gray]{0.75}\textbf{Candidate} & \textbf{Interviewer 1 }& \textbf{Interviewer 2} & \textbf{Concordant} & \textbf{Discordant}  \\
		\hline A & 1 & 1 & 11 & \\
		\hline B & 2 & 2 & 10 &  \\
		\hline C & 3 & 4 & 8 &  \\
		\hline D & 4 & $\xcancel{3}$ & &  \\
		\hline E & 5 & 6 & &  \\
		\hline F & 6 & 5 & &  \\
		\hline G & 7 & 8 & &  \\
		\hline H & 8 & 7 & &  \\
		\hline I & 9 & 10 & &  \\
		\hline J & 10 & 9 & &  \\
		\hline K & 11 & 12 & &  \\
		\hline L & 12 & 11 & &  \\ \hline
	\end{tabular}
	\end{center}
	When all concordant pairs have been counted, it looks like this:
	\begin{center}
	\begin{tabular}{|c|c|c|c|c|}
		\hline 
		\rowcolor[gray]{0.75}\textbf{Candidate} & \textbf{Interviewer 1 }& \textbf{Interviewer 2} & \textbf{Concordant} & \textbf{Discordant}  \\
		\hline A & 1 & 1 & 11 & \\
		\hline B & 2 & 2 & 10 &  \\
		\hline C & 3 & 4 & 8 &  \\
		\hline D & 4 & 4 & 8 &  \\
		\hline E & 5 & 6 & 6 &  \\
		\hline F & 6 & 5 & 6 &  \\
		\hline G & 7 & 8 & 4 &  \\
		\hline H & 8 & 7 & 4 &  \\
		\hline I & 9 & 10 & 2 &  \\
		\hline J & 10 & 9 & 2 &  \\
		\hline K & 11 & 12 & 0 &  \\
		\hline L & 12 & 11 & &  \\ \hline
	\end{tabular}
	\end{center}
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	Now we count the number of discordant pairs and insert them into the next column. The number of discordant pairs is similar to the previous steps, only we are looking for smaller ranks, not larger ones:
	\begin{center}
	\begin{tabular}{|c|c|c|c|c|}
		\hline 
		\rowcolor[gray]{0.75}\textbf{Candidate} & \textbf{Interviewer 1 }& \textbf{Interviewer 2} & \textbf{Concordant} & \textbf{Discordant}  \\
		\hline A & 1 & 1 & 11 & 0\\
		\hline B & 2 & 2 & 10 & 0 \\
		\hline C & 3 & 4 & 8 &  1\\
		\hline D & 4 & 4 & 8 &  0\\
		\hline E & 5 & 6 & 6 &  1\\
		\hline F & 6 & 5 & 6 &  0\\
		\hline G & 7 & 8 & 4 &  1\\
		\hline H & 8 & 7 & 4 &  0\\
		\hline I & 9 & 10 & 2 &  1\\
		\hline J & 10 & 9 & 2 &  0\\
		\hline K & 11 & 12 & 0 &  1\\
		\hline L & 12 & 11 & &  \\ \hline
	\end{tabular}
	\end{center}
	And we sum these two new columns:
	\begin{center}
	\begin{tabular}{|c|c|c|c|c|}
		\hline 
		\rowcolor[gray]{0.75}\textbf{Candidate} & \textbf{Interviewer 1 }& \textbf{Interviewer 2} & \textbf{Concordant} & \textbf{Discordant}  \\
		\hline A & 1 & 1 & 11 & 0\\
		\hline B & 2 & 2 & 10 & 0 \\
		\hline C & 3 & 4 & 8 &  1\\
		\hline D & 4 & 4 & 8 &  0\\
		\hline E & 5 & 6 & 6 &  1\\
		\hline F & 6 & 5 & 6 &  0\\
		\hline G & 7 & 8 & 4 &  1\\
		\hline H & 8 & 7 & 4 &  0\\
		\hline I & 9 & 10 & 2 &  1\\
		\hline J & 10 & 9 & 2 &  0\\
		\hline K & 11 & 12 & 0 &  1\\
		\hline L & 12 & 11 & &  \\ 
		\hline  &  & \textbf{Total:} & 61 & 5 \\ \hline
	\end{tabular}
	\end{center}
	And, inserting the totals into the formula we get:
	
	\end{tcolorbox}
	We don't need to have two judges to compare to each other. For example in sensitivity analysis we may consider that a company wants to bring a new food product to the market. Potential customers taste $10$ versions of this product which have different lipid levels. Customers then rate each version. Once aggregated across all tasters, these ratings are not studied as such but are classified so as to obtain an overall order of preferences. We want to know if this classification can be linked to the lipid level:
	\begin{center}
	$\begin{array}{|c|c|}\hline 
	\rowcolor[gray]{0.75}\textbf{Lipid concentration} & \textbf{Preferences}  \\  \hline 13 & 10 \\  \hline 14 & 7 \\  \hline 15 & 9 \\  \hline 16 & 8 \\  \hline 17 & 5 \\  \hline 18 & 6 \\  \hline 19 & 4 \\  \hline 20 & 3 \\  \hline 21 & 1 \\ \hline  22 & 2 \\ \hline  \end{array}$
	\end{center}
	Then we apply the same procedure as above comparing both columns and we get $\tau_b=-0.822$.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Notice that when the Spearman's rank correlation coefficient can be thought of as the regular Pearson's product moment correlation coefficient as computed from ranks, Kendall tau rather represents a probability. Specifically, it is the difference between the probability that the observed data are in the same order for the two variables versus the probability that the observed data are in different orders for the two variables. 
	\end{tcolorbox}
	
	The Kendall correlation coefficient depends only the order of the pairs, and it can always be computed assuming that one of the rank order serves as a reference point (e.g. with $N=4$ elements we assume arbitrarily that the first order is equal to 1234). Therefore, with two rank orders provided on $n$ objects, there are $n!$ different possible outcomes (each corresponding to a given possible order) to consider for computing the sampling distribution of $\tau_b$. We can compute the probability $p$ associated with each possible value of $\tau_b$. For example, we find that the $p$-value associated with a one-sided test for a value of $\tau_b=\frac{2}{3}$ is equal to:
	
	Therefore for our Wine example above we cannot reject the null hypothesis, and we cannot conclude that the expert displayed a significant disagreement in their ordering of the wines.
	
	The computation of the sampling distribution is always theoretically possible because it is finite. But this requires computing $n!$ coefficients of correlation, and therefore it becomes practically impossible to implement these computations for even moderately large values of $n$. This problem, however, is not as drastic as it seems because the sampling distribution of $\tau_b$ converges towards a Normal distribution (the convergence is satisfactory for values of $n$ larger than $10$), with a mean of $0$ and a variance equal to\footnote{We went as far as we could (11933 according to holocene calendar) to find the original detailed proof of this variance but we were not able to found it. Sadly we don't have the time and the fun to rewrite the proof on our own... So without proof - as always in science (!) - use that relation at your own risks!}:
	
	Therefore, for $n$ larger than $10$, a null hypothesis test can be performed by transforming $\tau_b$ into a $Z$ value as:
	
	This $Z$ value is Normally distributed with a mean of $0$ and a standard deviation of $1$.

		
	\subsubsection{Range Statistics}\label{range statistics}
	The "\NewTerm{range statistics}\index{range statistics}" is a very important tool in finance and quality engineering (to mention only the two best-known examples). As the reader will see in what follows below, these statistics are by construction a subdomain of order statistics and the result that we will obtain here will be absolutely useful to us in the context of quality control charts (\SeeChapter{see section Engineering page \pageref{quality control charts}}) and therefore also in trading signals in finance.
	
	Given $X_1,X_2,\ldots ,X_n$ supposed independent and identically distributed random variables from a law of distribution $F$ and density $f$. Let us recall that we define the order statistic $X_{(i)}$ by:
	
	By writing:
	
	The variables $W_n$ and $M_n$ define the extreme order statistics and their difference:
	
	is named the "\NewTerm{extreme deviation}\index{extreme deviation}". 
	
	For what will follow, we will consider as obvious the relation:
	
	Let us now determine the distribution function of the maximum $M_n$:
	
	because writing that $M_n\leq x$ is equivalent to write that for every $X_{(i)}$ we have $X_{(i)}\leq n$ (not easy to guess that you must have this approach...).
	
	We then have as the variables are independent (\SeeChapter{see section Probabilities page \pageref{joint probability}}):
	
	and therefore we obviously have the distribution function:
	
	Respectively based on the same idea:
	
	and therefore we obviously have the distribution function:
	
	Hence:
	
	having used the linearity of the expected mean and using the fact that for the two distribution functions we are working on  the same random variable.
	
	Using an integration by parts (\SeeChapter{see section Differential and Integral Calculus page \pageref{integration by parts}}):
	
	remembering that $F(-\infty)$ and $F(+\infty)=1$.

	Now let us consider the special case where the distribution function follows a Normal centered reduced distribution such as:
	
	Then we have:
	
	Let us do a change of variable:
	
	Then we have:
	
	and then we found the relation given (almost $99\%$ of the time without proof) in the (good) books about statistical process control:
	
	named "\NewTerm{Hartley constant}\index{Hartley constant}\label{hartley constant}" (Leonard Henry Caleb Tippett was the first to develop this theory in the year 11925 according to holocene calendar) and therefore:
	
	This constant is as far as we know not possible to calculate to calculate formally. Either we have to use Taylor series approximations of the terms of the integral, which becomes a nightmare for large $n$, either through a calculation using Monte Carlo simulation (\SeeChapter{see section Numerical Methods page \pageref{monte carlo simulations}}). As it is relatively long to implement in a spreadsheet, quality engineers prefer to use tables in which we find for example:
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|c|c|}
			\hline
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Values of} $n$}  & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Values of} $d_2(n)$ \textbf{with a Normal distribution}} \\ \hline
			$2$ & $1.128$  \\ \hline
			$3$ & $1.693$  \\ \hline
			$4$ & $2.059$  \\ \hline
			$5$ & $2.326$  \\ \hline
			$6$ & $2.534$  \\ \hline
			$7$ & $2.704$  \\ \hline
			$8$ & $2.847$  \\ \hline
			$9$ & $2.970$  \\ \hline
			$10$ & $3.078$  \\ \hline
			$\ldots$ & $\ldots$  \\ \hline
			\end{tabular}
			\caption{Tabulated values of the Hartley constant $d_2(n)$}
		\end{center}
	\end{table}
	The previous relation is mainly know by quality engineers under the form of an estimated of the unknown process standard deviation:
	
	where $n$ is a sample sub-group sample size.
	
	Let us now see the variance of the range using always the Huygens theorem:
	
	The calculation of $\text{E}(R^2)$ is not nice (at least I have found nothing that satisfies the pedagogical goal of this book), the smallest complete proof held on three to four A4 pages and formally bring anything because we end with an integral that cannot be calculate by hand (by cons if anyone has a simple, elegant and detailed proof do not hesitate to send it to us!). It is for this reason that after having written:
	
	...if we write now as do many technical books:
	
	Then we have:
	
	But as we do not know the  maximum likelihood unbiased estimator of the standard deviation $\sigma$, we will use the proven relation:
	
	To finally have a biased estimator of the variance of the range:
	
	Here are some tabulated values of $d_3(n)$:
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|c|c|}
			\hline
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Values of} $n$}  & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Values of} $d_3(n)$ \textbf{with a Normal distribution}} \\ \hline
			$2$ & $0.852$  \\ \hline
			$3$ & $0.888$  \\ \hline
			$4$ & $0.879$  \\ \hline
			$5$ & $0.864$  \\ \hline
			$6$ & $0.848$  \\ \hline
			$7$ & $0.833$  \\ \hline
			$8$ & $0.819$  \\ \hline
			$9$ & $0.807$  \\ \hline
			$10$ & $0.797$  \\ \hline
			$\ldots$ & $\ldots$  \\ \hline
			\end{tabular}
			\caption{Tabulated values of the $d_3(n)$ constant}
		\end{center}
	\end{table}
	In a typical statistical process control (SPC) application involving sample ranges, data are collected in subgroups of a constant size, say $n$. For each subgroup, the range is computed and plotted. The average range, $\bar{R}$, for $k$ such subgroups is used to calculate trial control limits from which statistical control may be judged. If the process has been in a state of statistical control and is approximately normally distributed, an estimate of the process standard deviation, $\sigma$, based on $\bar{R}$ can be obtained. This estimator is $\bar{R} / d_2$. The factor $d_2$ depends only on the subgroup sample size $n$ and that the process is normally distributed.
	
	\paragraph{Tukey's Range Test}\label{Tukey's range test}\index{statistical tests!Tukey's range test}\mbox{}\\\\
	Let us suppose that we have we have $Z_k$ independent centered reduced random variables. And let us denote by $U$ a random variable following a chi-square law with $v$ degrees of freedom.

	Let us now define, for reasons that will seem obvious a little further below, the "\NewTerm{Studentized range}\index{studentized range}" (the origin of the name comes from its resemblance with the definition of the Student law) by:
	
	and let us try to determine if this relation follows a known law and has a possible application (the find in the numerator what we defined earlier as the "extreme deviation" but with a different notation).
	
	For this, let us show that we fall on the above definition by considering a somewhat more general case where we have $X_i$ independent random variables that follow a Normal distribution $\mathcal{N}\left(\mu,\sigma\right)$ and the standard deviation:
	
	And let us study the ratio:
	
	Let us now proceed to the classical transformations already seen and proved and used many times since the beginning of this section:
	
	and therefore we have:
	
	So that's it already for the first step For the moment, even if we do  not know if this definition follows a known distribution law, we can already write the following very interesting definition (notice that the term on the left is always positive):
	
	or written in another way:
	
	and therefore we can calculate what is the cumulative probability of a range of measurements compared to a critical range $R_{X,\text{crit}}$ directly corresponding to a prescribed threshold $\alpha$. Which brings us to the possibility to write:
	
	Let us now recall that we have seen that the distribution function of the extreme deviation was given by a relation that was to our knowledge not calculable analytically:
	
	So the distribution function $Q_{k,v,1-\alpha}$ can therefore not be assimilate to a known law when $F(x)$ is any law. We must therefore unfortunately tabulate this distribution using Monte Carlo simulations (\SeeChapter{see section Numerical Methods page \pageref{monte carlo simulations}}) or refer to existing tables.
	
	Now to continue, we make a detour to the one-way fixed factor ANOVA that we studied earlier. Let us first recall that we have shown that for independent and identically Normal distributed random variables we had:
	
	and since a the ANOVA with fixed factor is also based on the assumption that:
	
	this implies that asymptotically the estimators have the same property:
	
	We also know that the standard deviation of the mean of a sample of a fixed-factor ANOVA is given in the framework and assumptions of the one-way fixed factor ANOVA by:
	
	But within the framework of one-way fixed factor ANOVA, we have also proved that under the assumptions imposed, we had:
	
	Therefore it comes:
	
	that is an estimator of:
	
	And as we have proved that:
	
	Therefore it comes:
	
	Therefore, we are naturally led to conclude that the relation we defined earlier:
	
	may be used in the study of the one-way fixed factor ANOVA under the form:
	
	to do a pre- or posterior-test (post hoc) to a one-way fixed factor ANOVA to check the hypothesis of equality of means and identify which are the aberrant means (multiple comparison test). So the Tukey's test is often accompanied by the Cochran C-test that we have already discussed earlier when we make a one-way fixed factor ANOVA.
	
	Therefore within the ANOVA framework, we must reject the hypothesis of equality of the sample means if:
	
	or written in another way:
	
	In this case, it is then almost immediate we can build the following confidence interval:
	
	This is the "\NewTerm{Tukey's range test}\index{Tukey's range test}" or also simply named "\NewTerm{Tukey's test}".
	
	It should be known now that there is a Tukey's post-hoc test for the one-way fixed factor ANOVA where the relation:
	
	will not take the two most extreme means but will compare all the means pairwise (and why not after all!) with the highest average (well we could also have fun making all possible combinations as do some statistical softwares). In this case the relation to use is the same as above except that if we have for example a one-way fixed factor ANOVA with $4$ levels then we will have $3$ pairwise comparisons (average differences must always be positive!!!). Thus, imagining that the third average is the largest and that in descending order the biggest averages are the 4th, 2nd and 1st (the first is therefore smaller) then it comes:
	
	This approach (to extend the basic principle of Tukey's test), is named the "\NewTerm{Newman-Keuls test}\index{Newman-Keuls test}" or "\NewTerm{Student-Newman-Keuls test}\index{Student-Newman-Keuls test}" (SNK).
	
	\pagebreak
	\subsubsection{Extreme Value Theory}
	"\NewTerm{Extreme value theory}\index{extreme value theory}" or "\NewTerm{extreme value analysis}\index{extreme value analysis}" (EVA) is a branch of statistics dealing with the extreme deviations from the median of probability distributions. It seeks to assess, from a given ordered sample of a given random variable, the probability of events that are more extreme than any previously observed. Extreme value analysis is widely used in many disciplines, such as structural engineering, finance, earth sciences, traffic prediction, and geological engineering. For example, EVA might be used in the field of hydrology to estimate the probability of an unusually large flooding event, such as the 100-year flood. Similarly, for the design of a breakwater, a coastal engineer would seek to estimate the 50-year wave and design the structure accordingly.
	
	\StickyNote[2.5cm]{\LARGE To finish depending on donations}[6.5cm]
	
	\pagebreak
	\subsection{Multivariate Statistics}
	Obviously Multivariate statistics is a subdivision of statistics encompassing the simultaneous observation and analysis of more than one  random variable.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Certain types of problem involving multivariate data, for example simple linear regression and multiple regression that we saw earlier above, are not usually considered as special cases of multivariate statistics because the analysis is dealt with by considering the (univariate) conditional distribution of a single outcome variable given the other variables.
	\end{tcolorbox}	 
	There are many different models, each with its own type of analysis that we will try to address in this book as always with a maximum of details and as accessible as possible (with softwares examples in the companion book for sure!):
	\begin{enumerate}
		\item Principal Components Analysis (PCA) that creates a new set of orthogonal variables that contain the same information as the original set. It rotates the axes of variation to give a new set of orthogonal axes, ordered so that they summarize decreasing proportions of the variation (see below page \pageref{principal component analysis}).
		
		\item Factor Analysis that is similar to PCA but allows the user to extract a specified number of synthetic variables, fewer than the original set, leaving the remaining unexplained variation as error. The extracted variables are known as latent variables or factors; each one may be supposed to account for covariation in a group of observed variables (see section of Numerical Methods page \pageref{factor analysis}).

		\item Canonical Correlation Analysis (CCA) that finds linear relations among two sets of variables; it is the generalised (i.e. canonical) version of bivariate correlation.

		\item Correspondence Analysis (CA), or reciprocal averaging, that finds (like PCA) a set of synthetic variables that summarise the original set. The underlying model assumes chi-squared dissimilarities among records (cases). The fundamental formal difference is that PCA decomposes relations between columns only (e.g. by decomposing their covariance matrix), treating rows as "cases"; while CA decomposes columns and rows simultaneously. Where conventional Factor Analysis determines which variables cluster together, correspondence analysis determines which category values are close together.

		\item Canonical (or "constrained") Correspondence Analysis (CCA) for summarising the joint variation in two sets of variables; combination of correspondence analysis and multivariate regression analysis. The underlying model assumes chi-squared dissimilarities among records (cases).

		\item Multidimensional Scaling comprises various algorithms to determine a set of synthetic variables that best represent the pairwise distances between records. The original method is principal coordinates analysis (PCoA based on PCA).

		\item Discriminant Analysis (linear or quadratic), or canonical variate analysis, attempts to establish whether a set of variables can be used to distinguish between two or more groups of cases.

		\item Simultaneous Equations Models involve more than one regression equation, with different dependent variables, estimated together.
		
		\item Multivariate Analysis of Variance (MANOVA) that extends the analysis of variance to cover cases where there is more than one dependent variable to be analyzed simultaneously (see page \pageref{MANOVA}).

		\item Multivariate Analysis of Covariance (MANCOVA) that is an extension of analysis of covariance (ANCOVA) methods to cover cases where there is more than one dependent variable and where the control of concomitant continuous independent variables (covariates) is required. 
		
		\item Mixed Models that contains both fixed effects and random effects. These models are useful in a wide variety of disciplines in the physical, biological and social sciences. They are particularly useful in settings where repeated measurements are made on the same statistical units (longitudinal study).
	\end{enumerate}
	
	\subsubsection{Principal Component Analysis}\label{principal component analysis}
	Principal component analysis (P.C.A.) is a mathematical method of graphical data analysis that consist to look for directions in space that best represent the correlations between $n$ random variables (supposed to have linear relations between them). In other terms it is a dimension reduction process (as for the singular values decomposition process - SVD - proved in the section of Linear Algebra or the Linear Discriminant Analysis proved in the section of Numerical Methods among many others techniques as visible on the Data Science mind map at page \pageref{mindmap of data science}) as it gives the possibility to the analyst to choose what are the variables in a model that explain the best the variability.
	
	Simply said, a P.C.A. allows for example to find in a dataset buying behaviour similarities between observed classes.
	
	Even if the PCA is mainly used to visualize data, we must not forget that this is also a way to:
	\begin{itemize}
		\item To decorrelate the data. In the new base, consisting in new axis, the points have a zero correlation (we will prove it).
		
		\item To classify data into correlated clusters (in the industry it is mainly this possibility that is interesting!).
	\end{itemize}
	\begin{tcolorbox}[title=Remarks,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	\textbf{R1.} There are several versions of P.C.A. known under the name of "\NewTerm{Karhunen-Loeve transformation}\index{Karhunen-Loeve transformation}" or "\NewTerm{Hotelling transformation}\index{Hotelling transformation}" and that can also be applied without programming in spreadsheet softwares or in specialized one (where the computing time will be shorter by cons ... and the results most accurate too...).\\
	
	\textbf{R2.} Depending on the authors and the point of view P.C.A. belong to the field of statistics named "Explanatory statistics".
	\end{tcolorbox}
	When we consider only two effects, it is customary to characterize their joint effect via the correlation coefficient. When we stands in two dimension, the available points (the sample of points drawn following the joint distribution) can be represented in a plane. The result of the P.C.A. in this plane is the determination of the two axes that best explain the dispersion of the available points.

	When there are more than two effects, for example three effects, there are three coefficients of correlations to be taken into account. The issue that gave rise to the P.C.A. is how to have a quick intuition of the joint effects?

	In dimension larger than two, a P.C.A. will always determine the axes that best explain the dispersion of the cloud of available points.

	The objective of the P.C.A. is to graphically describe a data array of individual with large quantitative variables:
	\begin{table}[H]
	\begin{center}
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|c|c|}
				\hline
				\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{individuals/variables}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{$\text{var}_1,\ldots,\text{var}_j,\ldots,\text{var}_n$}} \\ \hline
				$\text{ind}_1$ & \\	
				$\vdots$ & \\	
				$\text{ind}_i$ & $x_{ij}$ \\
				$\vdots$ & \\
				$\text{ind}_n$ & {} \\				
				\\ \hline
		\end{tabular}
	\end{center}
	\caption{Generic representation of a P.C.A. table}
	\end{table}
	In order not to complicate the presentation of this method and to allow the reader to completely redo the calculations, we will work on the theory with a direct famous example.
	
	Let us consider for example a study of a botanist who measured the dimensions of $15$ iris flowers (the P.C.A. is also widely used in finance to determine the elements that most influences the volatility of a portfolio). The three  variables ($p=3$) measured are:
	\begin{itemize}
		\item $x_1$: Length of sepal

		\item $x_2$: Width of sepal

		\item $x_3$: Length of the petal
	\end{itemize}
	
	The data are the following:
	
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|c|c|c|c|}
			\hline
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Flower} $n^{\circ}$}  & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}$x_1$} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}$x_2$} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}$x_3$}\\ \hline
			$1$ & $5.1$ & $3.5$ & $1.4$  \\ \hline
			$2$ & $4.9$ & $3.0$ & $1.4$  \\ \hline
			$3$ & $4.7$ & $3.2$ & $1.3$  \\ \hline
			$4$ & $4.6$ & $3.1$ & $1.5$  \\ \hline
			$5$ & $5.0$ & $3.6$ & $1.4$  \\ \hline
			$6$ & $7.0$ & $3.2$ & $4.7$  \\ \hline
			$7$ & $6.4$ & $3.2$ & $4.5$  \\ \hline
			$8$ & $6.9$ & $3.1$ & $4.9$  \\ \hline
			$9$ & $5.5$ & $2.3$ & $4.0$  \\ \hline
			$10$ & $6.5$ & $2.8$ & $4.6$  \\ \hline
			$11$ & $6.3$ & $3.3$ & $6.0$  \\ \hline
			$12$ & $5.8$ & $2.7$ & $5.1$  \\ \hline
			$13$ & $7.1$ & $3.0$ & $5.9$  \\ \hline
			$14$ & $6.3$ & $2.9$ & $5.6$  \\ \hline
			$15$ & $6.5$ & $3.0$ & $5.8$  \\ \hline
			\end{tabular}
			\caption[]{Practical tabulated data example for P.C.A.}
		\end{center}
	\end{table}
	For us, such a data table is simply a real components matrix with $n$ rows (the individuals) with $p$ columns (the variables):
	
	Thereafter the index $i$ will correspond to the line index and therefore to individuals. We will identify the individual $i$ with point line $x_{i.}=(x_{i1},\ldots,x_{ip})$ which will be considered as a point in an affine space (\SeeChapter{see section Vector Calculus page \pageref{affine space}}) of dimension $p$. The index $j$ will correspond to the index column so the variables. We will identify the variable $j$ with the column vector:
	
	this is therefore a vector in the vector space of dimension $n$ in $\mathbb{R}^n$.
	
	We will place ourselves in what will follow in two perspectives: either we will take the table data as $n$ points an affine space of dimension $p$, or we will take this table as $p$ points in a vector space of dimension $n$. We will see that there are dualities between these two perspectives.
	
	The mathematical tool that we will use here is linear algebra (\SeeChapter{see section Linear Algebra page \pageref{linear algebra}}), with the concepts of dot product, Euclidean norm and Euclidean distance.
	
	To simplify the presentation, we will initially consider that each individual as each variable has the same importance, the same weight. We also consider the case of the Euclidean distance.
	
	We will start by centering the data, that is to say to put origin of the reference frame on the center of gravity of the point cloud. This does not change the appearance of the cloud, but allows us to get the coordinates of a point $M$, equal to the coordinates of the vector $\overrightarrow{GM}$, and thus to place ourselves in the vector space to be able to do the math! Since we assume throughout what will follow that the weights of the individuals are identical, we will take $m_i=1/n$ with $i=1\ldots n$.
	
	We consider the orthonormal reference frame $(\text{O},\vec{e}_1,\vec{e}_2,\ldots,\vec{e}_p)$ in the canonical basis $(\vec{e}_1,\vec{e}_2,\ldots,\vec{e}_p)$ of $\mathbb{R}^p$. Given $G$ being therefore the center of gravity of the point cloud. As each variable and each individual is assumed to have the same weight, then $G$ has for coordinates the reference frame $(\text{O},\vec{e}_1,\vec{e}_2,\ldots,\vec{e}_p)$:
	
	with:
	
	We then have yet graphically:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/pca_gravitypoint.jpg}
		\caption[]{Measurement points and center of gravity}
	\end{figure}
	We name "\NewTerm{centered matrix}\index{centered matrix}" the matrix:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The matrix of centered data contains the centered coordinates (which we will denote by $xc_{ij}$) of the individuals in the reference frame $(G,\vec{e}_1,\vec{e}_2,\ldots,\vec{e}_p)$. We place ourselves for what will follow always in this reference frame for point cloud of the individuals and we will take $\text{O}=G$.
	\end{tcolorbox}	
	For our example we have:
	
	and for the centered matrix:
	
	and graphically:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/pca_centered_measured_points.jpg}
		\caption[]{Centered measured points }
	\end{figure}
	To give an equal importance to each variable so that the type of units of the measurement does not influence the analysis (and also as we have proved it at page \pageref{correlation matrix} that the correlation matrix is equal to the variance-covariance matrix!), we will work with reduced centered data (\SeeChapter{see section Statistics page \pageref{reduced centered variable}}). For this, we will denote first:
	
	where the reader may have perhaps noticed that we take the biased variance. But in the reality, we will take obviously the variance estimator and we will therefore divide by $n-1$ rather than by $n$.
	
	The variance of the sample centered variable is equal near to a factor of $1 / n$ to the norm of this same variable but centered. The matrix of reduced centered data (dimensionless) is therefore:
	
	If we denote by $D_{1/\sigma}$ the following diagonal matrix:
	
	Then we have:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Each component of the matrix $Y$ is therefore of zero mean and unit variance (which is equivalent to say that the norm of the standard reduced variable is equal to the unit as we will prove it immediately).
	\end{tcolorbox}	
	We define the "\NewTerm{matrix of centered normalized data}\index{matrix of centered normalized data}" by (we then speak of "\NewTerm{normalized PCA}\index{normalized PCA}" which is not mandatory but simplifies interpretation):
	
	Or also (it is simply the mean squared error that we have introduced earlier in the section):
	
	The terminology comes of course from the fact that the sum of the square of the components of each column of the matrix $Z$ has a unit norm. Indeed:
	
	Which gives:
	
	We have graphically:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/pca_centered_reduced_measured_points.jpg}
		\caption[]{Centered reduced measured points }
	\end{figure}
	Represent the point cloud of the reduced centered data or reduced normalized data don't change the shape of it. Indeed, the difference between the two is only a change of scale.
	
	The interesting information on individuals is the distance between the points! Indeed more this distance is great between two individuals $z_{i.}$ and $z_{i'.}$ plus the two individuals will be different and better we can characterize them. But we must first choose a distance. We will take the Euclidean distance (\SeeChapter{see section Topology page \pageref{topology}}):
	
	The following figures show the orthogonal projections in space of this scatter cloud respectively in the planes $(\text{O},\vec{e}_1,\vec{e}_2),(\text{O},\vec{e}_2,\vec{e}_3)$ and finally in $(\text{O},\vec{u}_1,\vec{u}_3)$ that is the best projection, named "\NewTerm{factorial plane}\index{factorial plane}" (or sometimes "\NewTerm{scores diagram}\index{scores diagram}"), in the sense that it best respect the distances between individuals (verbatim, it deforms the least the cloud of points in space). The objective of the principal component analysis is to determine this best plan and we will prove now how!
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/pca_projection_cloud_xy.jpg}
		\caption[]{Projection of the points on the horizontal plane of the center reduced basis}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/pca_projection_cloud_yz.jpg}
		\caption[]{Projection of the points on the vertical plane of the center reduced basis}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/pca_projection_cloud_best.jpg}
		\caption[]{Projection of the points on the factorial plane of the center reduced basis}
	\end{figure}
	And the plane view of each of the projections:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/arithmetics/pca_plane_views.jpg}
		\caption[]{Plane view of each of the projections}
	\end{figure}
	Before determining the factorial plane, we will first now try to detect possible links between variables.
	
	We recall (\SeeChapter{see section Statistics page \pageref{covariance}}) that the covariance between two variables $x_{.j}$ and $x_{.j'}$ is given by:
	
	and that the linear correlation coefficient (\SeeChapter{see section Statistics page \pageref{linear correlation coefficient}}) is:
	
	We will denote for later:
	
	the matrices of variance-covariance and of correlation (both being for recall square and symmetric matrices) with $j=1\ldots p,j'=1\ldots p$.
	
	We see quite easily that the matrix of covariance is at a given coefficient $1 / n$ near, the matrix of canonical dot products of vectors of the centered reduced matrix $X_c$ (in other words, each component of the variance-covariance matrix is equal to the dot product of the centered variables). We deduce the following relation:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	For a given matrix $A$, the expression $A^TA$ plays an important role in statistics. For example, it is an important part of the analytical solution of ordinary least squares. Or, for PCA, its eigenvectors are the principal components of the data. Geometrically, matrix $A^TA$ is named "matrix of scalar products" (as it contains all the inner products of all columns in $A$). Algebraically, it's named "\NewTerm{sum-of-squares-and-cross-products matrix}\index{sum-of-squares-and-cross-products matrix}" (SSCP). Dividing SSCP matrix by $n$, the sample size or number of rows of $A$, we get what is commonly named the MSCP (mean-square-and-cross-product) matrix. If we center the columns of $A$ the $A^TA/(n-1)$ is the covariance matrix. If we $Z$-standardized the columns of $A$ the $A^TA/(n-1)$ is the Pearson correlation matrix.
	\end{tcolorbox}
	The matrix of variance-covariance (since, as we was it in the section Statistics, the diagonal contains the variances ... for recall!) is a well known interpretation in this book. By cons, what is new and we will be very useful to determine the factorial plane is the matrix of linear correlations that can also be written as follows:
	
	Which gives for our example where we have three variables (very easy to calculate using a spreadsheet software like Microsoft Excel), the following square matrix (the data are centered or not the components of the matrix are the same):
	
	To continue, always with the aim to determine the factorial plane, let us define the concept of inertia of a point cloud.
	
	\textbf{Definition (\#\thesection.\mydef):} We name "\NewTerm{inertia of a point cloud}\index{inertia of a point cloud}" the quantity:
	
	where $G$ is the center of gravity of the point cloud and $M_i$ a point of $\mathbb{R}^p$ of coordinates $x_i^T$.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The square of the distance is taken by anticipation of the developments that will follow.
	\end{tcolorbox}
	Then we prove the following relation:
	
	\begin{dem}
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	We will in all the remaining text with normalized centered data, verbatim with the matrix $Z$. The points $M_i$ will therefore have here for coordinates $z_i^T$.
	
	The problem now is to find the best affine space of dimension $p$ in the sense that it respects the best the distances between points. For this, we will seek the best vectorial line $\Delta_{\vec{u}}$ that is perfectly determined by the vector $\vec{u}$. Let us denote $H_i$ the orthogonal projection of $M_i$ on the line $\Delta_{u}$. So our problem is to find the line (verbatim the vector $\vec{u}$) that makes the sum of the squares of the distances between the points $H_i$ is maximized. We write the problem as a quadratic programming problem (\SeeChapter{see section Numerical Methods page \pageref{nonlinear optimization}}):
	\begin{equation}
		\begin{aligned}
		& \underset{\vec{u}}{\text{maximize}}
		& & \sum_{i,i'} d^2(H_i,{H'}_i) \\
		& \text{subject to}
		& & \vec{u}\in \mathbb{R}^p \\
		&&& \|\vec{u}\|=1
		\end{aligned}
	\end{equation}
	But here we have:
	
	Indeed, the projected centroid (barycentre) of the cloud points  is also the origin. Consequently, our problem can be written:
	\begin{equation}
		\begin{aligned}
		& \underset{\vec{u}}{\text{maximize}}
		& & I \\
		& \text{subject to}
		& & \vec{u}\in \mathbb{R}^p \\
		&&& \|\vec{u}\|=1
		\end{aligned}
	\end{equation}
	Itself therefore being equivalent to:
	\begin{equation}
		\begin{aligned}
		& \underset{\vec{u}}{\text{maximize}}
		& & \sum_{i,i'} d^2(\text{O},H_i,) \\
		& \text{subject to}
		& & \vec{u}\in \mathbb{R}^p \\
		&&& \|\vec{u}\|=1
		\end{aligned}
	\end{equation}
	Let us solve this problem:
	
	First, since $H_i$ is the orthogonal projection of the point $M_i$ on $\Delta_{\vec{u}}$ we have $\overrightarrow{\text{O}H}_i=\alpha_i \vec{u}$ for all $i$ with $\alpha_i =\overrightarrow{\text{O}M}_i\circ \vec{u}$. Following this, the coordinates of the points $H_i$ on the straight line $\Delta_{\vec{u}}$ are:
	
	If follows that we have:
	
	Here we seek the unit vector $\vec{u}$. The matrix $Z$ is perfectly known to us. But, we have:
	
	The correlation matrix $R$ is symmetrical therefore, following the spectral theorem proved in the section of Linear Algebra  page \pageref{spectral theorem}, it is diagonalizable in an orthonormal basis of eigenvectors. Thus, we have proved that in the spectral theorem:
	
	is diagonal (up to us to choose the content) if $R$ is symmetrical and $S$ is orthogonal (which is in our special example a square $3\times 3$ matrix). Then we deduce the following relation (factorization) that it is common to name the "\NewTerm{eigenvalue decomposition}\index{eigenvalue decomposition}" of $R$:
	
	and as $S$ has been proven as being orthogonal (and that there exists a family of eigenvectors for this!), we have (\SeeChapter{see section Linear Algebra page \pageref{orthogonal matrix}}):
	
	Therefore:
	
	where we choose for $\Lambda$ the diagonal matrix of the eigenvalues sorted in descending order: $\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_p$.
	
	We then have:
	
	In the literature, this sum is often denoted as follows (in statistical software often named "\NewTerm{eigenvalue decomposition}"):
	
	But $S$ being orthogonal, we have therefore:
	
	and this is comes from the fact that the orthogonal matrix is as we proved in the section Linear Algebra an isometry (it conserves the norm!).
	
	Since the eigenvalues are in descending order, we will write:
	
	Or the term in brackets is strictly less than or equal to $1$ by  the previous involvement. Therefore:
	
	Therefore:
	
	But remember that our objective is to maximize this inequality. In other words to seek $w_1$ such that the equality is respected. We see quickly enough that will be the case if $w_1=1$ and that the other terms are zero. Thus, a trivial solution to our maximization problem is:
	
	either because:
	
	which is then the first eigenvector of the matrix $R$ (since $R$ is diagonalized in this base) associated with the largest eigenvalue $\lambda_1$. Hence the fact that this solution is often denoted as:
	
	always with $\Lambda=S^{-1}RS$ (it is therefore quite easy to determine $S$ using softwares when $R$ and $\Lambda$ are known).
	
	Once we have found the first vector line, we seek a second orthogonal line in the subspace to the vector line that maximizes the inertia of the projected point cloud. We prove, and guess, that the solution is given by the vector line directed by the eigenvector associated to the second eigenvalue of the correlation matrix and so on ...
	
	Thus, we obtain a new basis $(\vec{u}_1,\ldots,\vec{u}_p)$ whose one of the plane is the factorial plane. However, we need to know the components of $Z$ in this base. As this base was built under the condition that $R$ is diagonalizable via the matrix $S$ then  the latter matrix is the linear application that will allow us to express $Z$ in the base $(\vec{u}_1,\ldots,\vec{u}_p)$ via the relation:
	
	Thus, in our example the three eigenvalues of the correlation matrix $R$ are (\SeeChapter{see section Linear Algebra page \pageref{eigenvector}}):
	
	and therefore:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Some softwares indicate the weights in respective and cumulated $\%$ for each of the eigenvalues. Thus we have in our special case the following respective weights in $\%$ of the total:
	
	So the first component explains $66.67\%$ of the effect. The first two components explain $96.15\%$, etc. This is why, for example, that in finance that among ten or more components, we will take only those that "explain" for example the $95\%$.
	\end{tcolorbox}
	By having the three eigenvalues, to determine the three eigenvectors $(\vec{u},\vec{u}_2,\vec{u}_3)$ that form the main base, we need to solve the following system of three equations with three unknowns (\SeeChapter{see section Linear Algebra page \pageref{linear systems}}) for each eigenvalue:
	
	Which gives therefore (expected if someone request the details we will not provide them here as they can be simply made by hand or with any spreadsheet software) for the matrix of eigenvectors:
	
	which satisfies therefore:
	
	or written in another way (following the remark  from a reader who wanted to check the calculations and was trapped):
	
	We then have for coordinates of the points $M_i$ in the base $(\vec{u}_1,\vec{u}_2,\vec{u}_3)$ using:
	
	The following matrix:
		
	The coordinates of the projection of the point cloud in the best plane defined by the vectors $(\vec{u}_1,\vec{u}_2)$ are then the first two columns of the previous matrix (thus corresponding the sepal length and sepals width).

	Indeed, we immediately see that these are the two columns that will maximize the sum of the norms in the given plane:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/pca_factorial_plane.jpg}
		\caption[]{Factorial plane already shown above ...}
	\end{figure}
	A software like Minitab 15.1 (reference in quality management industry) gives the following information for the eigenvalues (not very useful info graphically ... in my opinion):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/arithmetics/pca_eigenvalues.jpg}
		\includegraphics{img/arithmetics/pca_eigenvalues_details_minitab.jpg}
		\caption[]{Eigenvalues for the PCA as given by Minitab 15.1 ("scree plot")}
	\end{figure}
	and the following factorial plane (remains the question how the values are calculated in Minitab because they are not identical to those we got here my manual calculations... but the graphic is corresponding and this is the most important!):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/arithmetics/pca_factorial_plane_minitab.jpg}
		\caption[]{Factorial plane as given by Minitab 15.1}
	\end{figure}
	If the reader has carefully followed all the steps we did so far, he may have notice that we did:
	\begin{enumerate}
		\item Zero-center the data (mean normalization)
		\item Reduced the data
		\item Decorrelate the data by:
		\begin{enumerate}
			\item Calculating the covariance matrix
			\item Calculating the eigenvectors of the covariance matrix
			\item Applying the matrix of eigenvectors to the data (this will apply the rotation)
		\end{enumerate}
	\end{enumerate}
	Furthermore some softwares and practitioners scale the uncorrelated matrix in order to obtain a covariance matrix corresponding to the identity matrix (ones on the diagonal and zeros on the other cells). To do that we scale the decorrelated data by dividing each dimension by the square-root of its corresponding eigenvalue.
	
	All the $7$ steps enumerated above are named "\NewTerm{PCA whitening}\index{PCA whitening}" or just "\NewTerm{Whitening}\index{whitening}" (also sometimes "\NewTerm{sphering data}\index{sphering data}"). So "whitening" data means that we want to transform them in a way to have a covariance matrix that is the identity matrix ($1$ in the diagonal and $0$ for the other cells) and is named "whitening" in reference to "white noise".

	To close this subject, the reader must know that many software use the fact that the vectors $z_{.j}$ are of unit norm to do the scalar product which corresponds in this case simply the cosine between vectors such as:
	
	and as we have proved earlier above that:
	
	Therefore it comes:
	
	and as in our special example we have $3$ vectors $z_{.j}$, there is therefore $3$ possible dot products if we omit the scalar products of the vectors with themselves. Therefore the matrix:
	
	also contains the angles (in radian) between the vectors $z_{.j}$.

	Finally, let us indicate that the PCA being sensitive to outliers, it is sometimes better to transform the values of the original array in their ranks (see the study of rank statistics earlier above!) and applying exactly the same algorithm or using the Spearman Rank correlation coefficient. We then speak of a "\NewTerm{PCA by ranks}\index{PCA by ranks}".
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	A user-friendly software like XL-Stats proposes automatically in the PCA menu five "different types" of Principal Component Analysis that are respectively: Pearson ($n$), Pearson ($n-1$), Spearman, Kendall, Covariance. Obviously the same can be done with other statistical softwares but actually is not visually simple available choices (we have to write very little scripts).\\
	
	There is also a group of robust PCA methods that have been developed and that are better suited for handling high-dimensional data in the situation where the sample size is lower than the dimension. These methods are: Robust PCA by projection-pursuit (PP-PCA), Spherical PCA (SPCA), Robust PCA (ROBPCA), Robust Sparse PCA (ROSPCA).\\
	
	We can also compute the variance of the PCA using bootstrapping techniques (\SeeChapter{see section Numerical Methods page \pageref{bootstrap}}) like for all statistical analysis!
	\end{tcolorbox}	
	
	\paragraph{SVD and PCA}\mbox{}\\\\
	Now we will prove an important theorem that is not quite obvious.
	\begin{theorem}
	The PCA above (using the spectral theorem, ie eigendecomposition) is a special case of the SVD (that latter is therefore less restrictive)!!!
	\end{theorem}
	\begin{dem}
	We have proved before that:
	
	Using the properties of the transposed matrices (\SeeChapter{see section Linear Algebra page \pageref{transposed matrix}}):
	
	And that:
	
	And we have build in the section of Linear Algebra the singular value decomposition (\SeeChapter{see section Linear Algebra page \pageref{spectral theorem}}):
	
	That is in our case here:
	
	Therefore:
	
	and since $V$ is an orthogonal matrix ($V^TV=\mathds{1}$):
	
	So if we compare:
	
	 the correspondence is quite easily seen!
	 
	 So that means in order for the spectral theorem of $R$ to be equal to its SVD, we need $R$ to:
	 \begin{itemize}
	 	\item Have orthonormal eigenvectors (meaning all it does is to scale its input along $n$ orthogonal directions, where $R$ is $n\times n$)
	 	
	 	\item Have positive eigenvalues (colloquially, it must be a real matrix and not "flip" anything)
	 \end{itemize}
	 It turns out that the above conditions are equivalent to the following:
	\begin{itemize}
	 	\item $R$ must be a symmetric real matrix (this is equivalent to $R$ having real eigenvalues and orthonormal eigenvectors)
	 	
	 	\item $R$'s (real) eigenvalues must be positive
	 \end{itemize}
	 So, we say: Both are equal when $R$ is symmetric positive-semidefinite (\SeeChapter{see section Linear Algebra page \pageref{positive semidefinite matrix}})! This can be denoted technically as $R \succeq 0 \ \land \ R^\dagger = R$.
	 
	In facts, using the SVD to perform PCA makes much better sense numerically than forming the covariance matrix to begin with, since the construction of $ZZ^T$ can cause loss of precision. Indeed in some cases the PCA can diverge very quick and give inaccurate results! This is why many statistical softwares gives the option to run a PCA based on SVD (singular value decomposition theorem) or on the spectral theorem.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	A common question on Internet forums is: \textit{Intuitively, what is the difference between Spectral Theorem (eigendecomposition) and Singular Value Decomposition?}. Also, if: \textit{Is one of them more general than the other? Is either a special case of the other?}
	
	First, according to what has been written earlier, it should be quite obvious that  eigendecomposition arises from asking in which directions a \underline{quadratic form} has the greatest impact, when the SVD by construction arises from asking in which directions a \underline{linear transformation}\footnote{Hence the fact that PCA produces linear transformations, thereby capturing linear relationships between the original variables so that we might lose predictive information that are non-linear!} has the greatest impact!
	
	Now the reader must keep in mind that the spectral theorem describes the effect of a matrix $A$ on a vector as the following $3$-steps process $A=Q\Lambda Q^{-1}$:
	\begin{enumerate}
		 \item An invertible linear transformation ($Q^{-1}$, hence $Q$ must be invertible, ie square!) 
		 
		 \item A scaling ($\Lambda$ with $\Lambda=\text{diag}(\vec{\lambda})$)
		 
		 \item The inverse of the initial transformation ($Q$)
	\end{enumerate}
	This decomposition doesn't always exist, but the spectral theorem describes the conditions under which such a decomposition exists.
	
	When for the SVD we have the following $3$-steps process $A=U\Sigma V^{T}$:
	\begin{enumerate}
		 \item A first rotation in the input space ($V$ is a rotation matrix as $V^TV=\mathds{1}$)
		 
		 \item A simple positive scaling that takes a vector in the input space to the output space ($\Sigma$ diagonal with positive entries!)
		 
		 \item And another rotation in the output space ($U$ is a rotation matrix as $U^TU=\mathds{1}$)
	\end{enumerate}
	The fundamental theorem of linear algebra says that such a decomposition always exists (especially because no inverse is involved and therefore doesn't apply only to square matrices).
	
	\pagebreak
	\subsubsection{Chi-square Correspondence Factor Analysis (C2CFA)}\label{chi-square correspondence factor analysis}
	The Chi-square factor correspondence analysis, abbreviated C2CFA is a method of data analysis widely used in biostatistics and survey analysis. The C2CFA technique is mainly used for large tables to compare all data (if possible all expressed in the same unit, like a currency, a dimension, a frequency or any other measurable quantity). It may in particular allow the study of contingency tables (or co-occurrence cross tables) and describe the connection between two variables. It serves to identify and prioritize all the dependencies between the rows and columns of the table.
	
	If more than two variables are to be taken into considerations we speak of Chi-square multiple correspondence analysis (C2MCA).
	
	Let us tackle now directly the theory with an example. For this we consider the following table (with two variables) of the areas of the types of tree that stands in Picardy (France) in 11984 (holocene calendar) in hectares:
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|c|c|c|c|c|}
					\hline
					\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Leafy}}  & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Resinous}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Mixes}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Total by Area}}\\ \hline
				{\cellcolor[gray]{0.75}\textbf{Aisne (A)}} & $106,500$ & $3,380$ & $1,470$ & $111,350$ \\ \hline
				{\cellcolor[gray]{0.75}\textbf{Oise (O)}} & $101,700$ & $310,000$ & $0$ & $111,700$ \\ \hline
{\cellcolor[gray]{0.75}\textbf{Somme (S)}} & $45,200$ & $4,350$ & $50$ & $49,600$ \\ \hline
{\cellcolor[gray]{0.75}\textbf{Total}} & $253,400$ & $17,730$ & $1,520$ & $272,650$ \\ \hline
			\end{tabular}
			\caption{Contingency table (crosstab) of CFA}
		\end{center}
	\end{table}
	The experts in the domain sometimes name the totals for rows and columns, respectively, the "\NewTerm{line margins}\index{line margins}" and "\NewTerm{column margins}\index{column margins}". When the whole table is put in percentages, relatively to the total of the totals, it is named "\NewTerm{joint frequency representation}\index{joint frequency representation}":
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|c|c|c|c|c|}
					\hline
					\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Leafy}}  & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Resinous}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Mixes}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Total by Area}}\\ \hline
				{\cellcolor[gray]{0.75}\textbf{Aisne (A)}} & $39.06\%$ & $1.24\%$ & $0.54\%$ & $40.84\%$ \\ \hline
{\cellcolor[gray]{0.75}\textbf{Oise (O)}} & $37.30\%$ & $3.67\%$ & $0\%$ & $40.97\%$ \\ \hline
{\cellcolor[gray]{0.75}\textbf{Somme (S)}} & $16.58\%$ & $1.60\%$ & $0.02\%$ & $18.19\%$ \\ \hline
{\cellcolor[gray]{0.75}\textbf{Total}} & $92.93\%$ & $6.50\%$ & $0.56\%$ & $100\%$ \\ \hline
			\end{tabular}
			\caption{Joint frequency table of CFA}
		\end{center}
	\end{table}
	We wish to analyse whether there are degrees of similarity and difference between the variables. Let us notice that we are not trying to compare equality of means or variances therefore statistical tools seen in previously are not suitable for this kind of analysis.
	
	If we choose the Euclidean distance (\SeeChapter{see section Vector Calculus page \pageref{euclidean distance vector}}):
	
	on the raw data to measure the differences between departments, we get the following differences:
	
	and so on for the other regions. We then get:
	
	We see by looking at the table and before any calculation that the departments of Aisne and Oise seems to be similar and that the department of the Somme differs significantly. The distances put in evidence that observation.

	But in the above table the profiles of the Oise and the Somme, with a very small mixed forest, are however very similar in proportion.

	In this context, we see that the Euclidean distance transcribed quite well the mass differences between the departments. In other words, the Aisne and the Oise are similar because their surfaces are close. To eliminate the artifact related to the orders of magnitude, we need to transform the data into percentages (percentages of regions). We then get:
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|c|c|c|c|c|}
					\hline
					\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Leafy}}  & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Resinous}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Mixes}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{\% Area}}\\ \hline
				{\cellcolor[gray]{0.75}\textbf{Aisne (A)}} & $95.6\%$ & $3.0\%$ & $1.3\%$ & $40.8\%$ \\ \hline
{\cellcolor[gray]{0.75}\textbf{Oise (O)}} & $91.0\%$ & $9.0\%$ & $0.0\%$ & $41.0\%$ \\ \hline
{\cellcolor[gray]{0.75}\textbf{Somme (S)}} & $91.1\%$ & $8.8\%$ & $0.1\%$ & $18.2\%$ \\ \hline
			\end{tabular}
			\caption{Transformation into percentages of the contingency table}
		\end{center}
	\end{table}
	where the experts in the field sometimes name the column \%Area "\NewTerm{marginal profile rows}\index{marginal profile rows}" or "\NewTerm{mass}" (and respectively when they indicate the rows of the percents for the trees).

	If we choose the Euclidean distance on the proportions (relative data), we get:
	
	Therefore:
	
	This time, the Oise and Somme appear well as have the most similar forests. We see that work with relative data seems more relevant in this case!

	Now let us borrow an idea to economists who, when they have tables of the same kind as the previous one, calculate what they named the "\NewTerm{index}\index{index (economy)}" or "\NewTerm{elasticity}" (also often name "\NewTerm{specificity index}\index{specificity index}" in statistics) and which is given by the ratios between the joint frequency and the marginal frequency:
	
	Here is an example obtained with the Pivot Table tool in Microsoft Excel 11.8346 which includes natively the Index function. First the starting table:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/index_starting_pivottable.jpg}
		\caption[]{Microsoft Excel 11.8346 Pivot Table for Index analysis}
	\end{figure}
	and by activating the Index function in the Pivot Table properties we get:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/index_final_pivottable.jpg}
		\caption[]{Microsoft Excel 11.8346 Pivot Table with Index values}
	\end{figure}
	To understand where these values comes from, let us look for example the article \textit{Desk} in the region \textit{Alberta}. It has a return (joint frequency) of:
	
	compared to all regions, which is above the value of $33.33\%$ that this article would have for yield for all regions if there is no region of preference!

	The \textit{Alberta} region a yield (marginal rate) of:
	
	with respect to all regions which is below to the yield of $33.33\%$ that it would have if there were no regions of preference. Thus, this index table shows whether the differences are qualitatively significant!!
	
	The ratio therefore gives:
	
	which shows a strong shift between the value obtained and the value we would have if the proportions were respected (over-representation of $283\%$).
	
	So it is a kind of conformity compliance: if the ratio was equal to $1$, it is that the regional sales performance of this particular article would be consistent with respect to all sales of this region with respect to the national market. There would be no anomalies. Let us see this example for our trees where we had seen the effectives:
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|c|c|c|c|c|}
		\hline
		\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{}} & 
		\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Leafy}}  & 
		\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Resinous}} & 
		\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Mixes}} & 
		\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Total by Area}}\\ \hline
		{\cellcolor[gray]{0.75}\textbf{Aisne (A)}} & $106,500$ & $3,380$ & $1,470$ & $111,350$ \\ \hline
		{\cellcolor[gray]{0.75}\textbf{Oise (O)}} & $101,700$ & $10,000$ & $0$ & $111,700$ \\ \hline
{\cellcolor[gray]{0.75}\textbf{Somme (S)}} & $45,200$ & $4,350$ & $50$ & $49,600$ \\ \hline
{\cellcolor[gray]{0.75}\textbf{Total}} & $253,400$ & $17,730$ & $1,520$ & $272,650$ \\ \hline
		\end{tabular}
		\caption[]{Contingency table (crosstab) of CFA}
	\end{table}
	and for which we get the following Pivot Table of the effective index in Microsoft Excel 14.0.7166:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/index_trees_forests.jpg}
		\caption[]{Microsoft Excel 14.0.7166 Pivot Table for Index analysis}
	\end{figure}
	and we still see clearly with this table that it is well the Oise and Somme are most alike!

	Before continuing, we might ask ourselves the following most important question: What are the theoretical numbers that we would have been obtained if the proportions of the trees in the regions were strictly equivalent to the overall proportions (i.e. so that the index are all unitary)?

	Well simply by making the following calculations (it is merely a rule of three calculated in each cell) where the reader must - if possible - understand the meaning without applying this foolishly:
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|c|c|c|c|}
		\hline
		\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{}} & 
		\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Leafy}}  & 
		\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Resinous}} & 
		\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Mixes}}\\ \hline
		{\cellcolor[gray]{0.75}\textbf{Aisne (A)}} & \scriptsize\parbox{4cm}{$=(253,400/272,650)\cdot 111,350$\\$=103,488$} & \scriptsize\parbox{4cm}{$=(17,730/272,650)\cdot 111,350$\\$=7,241$} & \scriptsize\parbox{4cm}{$=(1,520/272,650)\cdot 111,350$\\$=621$} \\ \hline
		{\cellcolor[gray]{0.75}\textbf{Oise (O)}} & \scriptsize\parbox{4cm}{$=(253,400/272,650)\cdot 111,700$\\$=103,813$} & \scriptsize\parbox{4cm}{$=(17,730/272,650)\cdot 111,700$\\$=7,264$} & \scriptsize\parbox{4cm}{$=(1,520/272,650)\cdot 111,700$\\$=623$} \\ \hline
{\cellcolor[gray]{0.75}\textbf{Somme (S)}} & \scriptsize\parbox{4cm}{$=(253,400/272,650)\cdot 49,600$\\$=46,098$} & \scriptsize\parbox{4cm}{$=(17,730/272,650)\cdot 49,600$\\$=3,225$} & \scriptsize\parbox{4cm}{$=(17,730/272,650)\cdot 49,600$\\$=276$} \\ \hline
{\cellcolor[gray]{0.75}\textbf{Total}} & \scriptsize $253,400$ & \scriptsize $17,730$ & \scriptsize $1,520$ \\ \hline
		\end{tabular}
		\caption[]{Proportions respect of CFA}
	\end{table}
	Or in a more readable way:
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|c|c|c|c|c|}
					\hline
					\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Leafy}}  & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Resinous}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Mixes}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Total by Area}}\\ \hline
				{\cellcolor[gray]{0.75}\textbf{Aisne (A)}} & $E_{11}=103,488$ & $E_{12}=7,241$ & $E_{13}=621$ & $r_1=111,350$ \\ \hline
				{\cellcolor[gray]{0.75}\textbf{Oise (O)}} & $E_{21}=103,813$ & $E_{22}=7,264$ & $E_{23}=623$ & $r_2=111,700$ \\ \hline
{\cellcolor[gray]{0.75}\textbf{Somme (S)}} & $E_{31}=46,098$ & $E_{32}=3,225$ & $E_{33}=276$ & $r_{3}=49,600$ \\ \hline
{\cellcolor[gray]{0.75}\textbf{Total}} & $c_1=253,400$ & $c_2=17,730$ & $c_3=1,520$ & $N=272,650$ \\ \hline
			\end{tabular}
	\end{table}
	Notice that in the table above, each cell (denoted $E_{ij}$), is obtained by computing:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Notice that if the cells have to be given in percentage, rather than in counts, we would then have to calculate instead: $p_{ij}=E_{ij}/N=(c_j\cdot r_i)/N^2$. Hence obviously: $E_{ij}=Np_{ij}$.
	\end{tcolorbox}	
	
	And we get with these new values of the table the following theoretical index with Microsoft Excel 14.0.7166:
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/index_trees_forests_theoretical.jpg}
		\caption[]{Microsoft Excel 14.0.7166 Pivot Table for Theoretical Index analysis}
	\end{figure}
	which shows now that the proportions are now observed! Parenthesis closed (but on which will come back later)!
	
	Well when we want to make a factorial correspondence analysis, our relation:
	
	Therefore becomes:
	
	hence:
	
	Again, the Oise and the Somme appear as the most similar.
	
	The distance above is named the "\NewTerm{Chi-square metric}\index{chi-square metric}" because it looks like (but that's all!)  to the distance used in the independence test of the same name (\SeeChapter{see section Statistics page \pageref{chi-square test of independence}}) but here, it only helps to establish a hierarchy in the context of a contingency table and to observe similar the variables in a more easy way!!
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	There is another way to calculate a CFA based on an Euclidean distance but by taking care beforehand to transform in a special way to the contingencies table so that the calculation are the same that when we use the Chi-square metric.
	\end{tcolorbox}	
	
	\subsubsection{Chi-squared Test of Independence}\label{chi-square test of independence}\index{statistical tests!$\chi^2$ test of independence}
	The $\chi^2$-test is applied when you have two categorical variables from a single population. It is used to determine whether there is a significant association between the two variables.
	
	For example, in an election survey, voters might be classified by gender (male or female) and voting preference (Democrat, Republican, or Independent). We could use a chi-square test for independence to determine whether gender is related to voting preference. 
	
	Suppose that a categorical variable $A$ has $r$ levels, and another categorical variable $B$ has $c$ levels. The null hypothesis $H_0$ states that knowing the level of variable $A$ does not help you predict the level of variable $B$. That is, the variables are independent.
	
	We will present this test with a companion example because we know from experience that it is more effective for learning and understanding.

	Let us recall that during the introduction of the previous method for comparing numbers (values) and detect which were the closest we gave the following observed effectives:
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|c|c|c|c|c|}
					\hline
					\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Leafy}}  & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Resinous}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Mixes}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Total by Area}}\\ \hline
				{\cellcolor[gray]{0.75}\textbf{Aisne (A)}} & $106,500$ & $3,380$ & $1,470$ & $111,350$ \\ \hline
				{\cellcolor[gray]{0.75}\textbf{Oise (O)}} & $101,700$ & $3,380$ & $1,470$ & $111,700$ \\ \hline
{\cellcolor[gray]{0.75}\textbf{Somme (S)}} & $45,200$ & $4,350$ & $50$ & $49,600$ \\ \hline
{\cellcolor[gray]{0.75}\textbf{Total}} & $253,400$ & $17,730$ & $1,520$ & $272,650$ \\ \hline
			\end{tabular}
			\caption{Contingency table (crosstab) for $\chi^2$ analysis}
		\end{center}
	\end{table}
	and we also showed how to find the table of theoretical effectives (rounded to the nearest whole number) where the proportions should have possibly be respected:
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|c|c|c|c|c|}
					\hline
					\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Leafy}}  & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Resinous}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Mixes}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Total by Area}}\\ \hline
				{\cellcolor[gray]{0.75}\textbf{Aisne (A)}} & $103,488$ & $7,241$ & $621$ & $111,350$ \\ \hline
				{\cellcolor[gray]{0.75}\textbf{Oise (O)}} & $103,813$ & $7,264$ & $623$ & $111,700$ \\ \hline
{\cellcolor[gray]{0.75}\textbf{Somme (S)}} & $46,098$ & $3,225$ & $276$ & $49,600$ \\ \hline
{\cellcolor[gray]{0.75}\textbf{Total}} & $253,400$ & $17,730$ & $1,520$ & $272,650$ \\ \hline
			\end{tabular}
			\caption{Contingency table (crosstab) for $\chi^2$ analysis with theoretical effective}
		\end{center}
	\end{table}
	We know that building the last table above assumes that the three regions are in identical conditions for everything related to the growth and multiplication of trees and the number of trees is in direct causal relation (!!!!) with regions and there is no other intermediate causes ... what is a strong assumption!
	
	But under this assumption, let us suppose that we would like know if the differences observed between the number of trees and the regions are statistically significant or purely random because of the experimental sample? In other words, we want to know if the number of trees actually depends on the regions in which they grow or if these values are only due to the sample chance? This is why this test is named the "\NewTerm{Chi-square test of independence}\index{chi-square test of independence}". 
	
	So keep in mind that in the Chi-square test of independence, all subjects/units are collected at random from a population, and two categorical variables are observed for each unit.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	 The chi-square test of independence is recommended in sensory analysis by the standard ISO 8588:1987 under the name "test A-Not A".
	\end{tcolorbox}
	To answer this questions we must first have a reference. And this reference is precisely the assumption of direct causal link (proportions respected) that we have given just previously.

	If we consider that each cell of the table of observed effective corresponds to a random variable of unknown law and that each cell of theoretical table is considered as following a random binomial variable (and asymptotically as a Normal distribution) then as we proved it earlier we know that we can use the chi-square test of adjustment (see page \pageref{chi-square test of adjustment}):
	
	to have a good idea (but still pretty rough considering the assumptions!) if the differences between the observed effective values are due to chance or are significant. But, if $D$ is small, the probability that it is due to chance is great but if $D$ is large so we have a real - significant - difference (so we use the chi-square test of adjustment but in reverse way in fact!).
	
	It remains to determine the number of degrees of freedom of the $\chi^2$ law that follows this sum in this configuration!
	
	In the particular case (but quite easily generalized by recurrence) of a table with two inputs with two categorized variables $X$ with $l$ levels and $Y$ with $c$ we will have respectively the $l$ rows and $c$ columns.
	
	Thus, the table will have obviously $l\cdot c$ cells. In the table of theoretical effectives (where each cell is treated as a random variable) each cell will be entirely determined by the sum of the other so that the number of degrees of freedom will be logically as we have seen during our study of degrees of freedom:
	
	Thus, taking our example of forests, the total of total is $272,650$ which allows us to write this last relation and thus determine the value of a possibly empty cell, all other data being known!

	A chi-square test on this type of table tests the hypothesis of independence against the alternative hypothesis of non-independence. Under the assumption of independence we believe we need only:
	
	values on the $N$ one to determine all  of them (assuming implicitly known the sum of all rows and columns).

	So if we have a table of $2$ rows by $2$ columns, we only need if we know the total rows and columns, to know $2$ values (i.e. $(2-1) + (2-1)=2$) to determine the $2$ missing values. The reasoning is the same for a table of $3$ rows by $3$ columns where we just have to know at least $4$ values (i.e. $(3-1) + (3-1)=4$) to determine the missing $5$ values.
	
	The degrees of freedom for the chi-square is then:
	
	It is this relation that we say us (trivially!) that if in a table of $2$ rows by $2$ columns so with $4$ cells (total of rows and columns being known!) that being known only one single values (df being equal to $1$), we can determine the other $3$ missing values.
	
	As we know a possible definition of the number of degrees of freedom $\text{df}$ is that it is the maximum number of values of the model such that none of them is calculable from the others.
	
	Similarly, for a table of $3$ rows by $3$ columns with $9$ cells as is the case of our example above with forests, the knowledge of only $4$ cells allows us thanks to total lines and columns to determine the other $5$ that would possibly not be known.
	
	Hence the relation in the context of the application of chi-square for the final equation:
	
	by making use of notations used in the industry. The term:
	
	is often named "\NewTerm{square of standardized residual}\index{square of standardized residual}". The squared root is more commonly used and named the  "\NewTerm{standardized residual}\index{standardized residual}\label{khi-squared standardized residual}":
	
	The standardized residual is a measure of the strength of the difference between observed and expected values. It's a measure of how significant your cells are to the chi-square value. When you compare the cells, the standardized residual makes it easy to see which cells are contributing the most to the value, and which are contributing the least. If your sample is large enough, the standardized residual can be roughly compared, as we already know it, to a $Z$-score! Standardization can work even if your variables are not Normally distributed.
	
	A general rule of thumb for figuring out what the standardized residual means, is:
	\begin{itemize}
		\item If the standardised residual is less than $-2$, the cell observed frequency is less than the expected frequency. 

		\item Greater than $+2$ and the observed frequency is greater than the expected frequency. 
	\end{itemize}
	If your standardised residuals are $\pm 3$, then it means that something extremely unusual is happening. If you get $\pm 4$, its something from the Twilight Zone! This makes sense if you think about the $68$ $95$ $99.7$ rule: if your data is normally distributed, $95\%$ of your data should be within two standard deviations from the mean. If you have something greater than that, then you're looking at an outlier.
	
	The ratio:
	
	is often named the "\NewTerm{contribution to the Chi-square of independence}\index{contribution to the Chi-square of independence}".
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	To use this test properly, the practitioner should check first that the differences (numerator) follow a normal distribution or that all terms of the sum follow a Chi-square distribution or approximately (asymptotically) a Normal centered reduced distribution centered and that the effective in each cell are greater than $5$ otherwise Monte Carlo simulations have to be used to determine the $p$-value.
	\end{tcolorbox}	
	In our example, we have:
	
	and the $p$-value of this value with the chi-square distribution with $4$ degrees of freedom:
	
	is so close to zero (not statistically significant) that we have no chance of being mistaken in asserting that the observed differences in the table are statistically significant between the $3$ and so that there is very likely independence.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Let us recall that there are plenty of statistical tests with the name that contains "chi-square" in it. For example:
	\begin{itemize}
		\item Chi-square Goodness-of-fit test (allow us to determine whether a specified population distribution is valid)
		
		\item Chi-square test of association/independence (allow us to determine whether the distribution of one variable has been influenced by another variable)
		
		\item Chi-square test of homogeneity (allow us to compare two or more population proportions), also named Chi-square test of heterogeneity 
		
		\item Chi-square test for outliers (allow us to detect if there is at least one outlier in comparison of a specific distribution)
		
		\item Chi-square test for difference of two count data (special case of homogeneity test with only one category!)
	\end{itemize}
	Even if all those tests have different purposes, their mathematical framework and procedure are EXACTLY the same and therefore any of these five tests can be run to get at the same time the five conclusions with the same $p$-values and the same critical chi-square values!!!
	\end{tcolorbox}	
	
	\paragraph{Equivalence of $Z$-test and $\chi^2$ test for proportions}\mbox{}\\\\
	We have mentioned during our study of the $Z$-test for the comparison of two proportions that softwares like \texttt{R} use the $\chi^2$ test to compare proportions rather than the $Z$-test. It's time now to see why!
	
	Say now that we have a study of two categorical variables each with only two levels. One of the response levels is considered the \textit{Success} response and the other the \textit{Failure} response (typical of an A/B test as used sensitivity analysis and especially well known in webdesign...). A general $2 \times 2$ table of the observed counts would be as follows:
	\begin{table}[H]
		\centering
		\begin{tabular}{lccc} 
		& \textbf{Success} & \textbf{Failure} & \textbf{Total} \\
		\hline \textbf{Group 1} & $A\; (O_{11})$ & $B\;(O_{12})$ & $n_1=A+B$ \\
		\textbf{Group 2} & $C\;(O_{21})$ & $D\;(O_{22})$ & $n_2=C+D$
		\end{tabular}
	\end{table}
	The observed counts in this table represent the following proportions:
	\begin{table}[H]
		\centering
		\begin{tabular}{lcc} 
		& \textbf{Success} & \textbf{Failure}  \\
		\hline \textbf{Group 1} & $\hat{p}_{1}=\frac{A}{A+B}=\frac{A}{n_1}$ & $1-\hat{p}_{1}$ \\
		\textbf{Group 2} & $\hat{p}_{2}=\frac{C}{C+D}=\frac{C}{n_2}$ & $1-\hat{p}_{2}$ 
		\end{tabular}
	\end{table}
	What is also often denoted in this special case:
	\begin{table}[H]
		\centering
		\begin{tabular}{lccc} 
		& \textbf{Success} & \textbf{Failure} & \textbf{Total} \\
		\hline \textbf{Group 1} & $p_1$ & $q_1$ & $100\%$ \\
		\textbf{Group 2} & $p_2$ & $q_2$ & $100\%$\\ 
		\hline Total & $p$ & $q$ & $100\%$
		\end{tabular}
	\end{table}
	Let us now use the table above blindly as our "contingency" table by noticing that if we take the above table as reference, then $N=100\%=1$.
	
	The usual $\chi^2$ of this table written after we substitute proportions instead of frequencies in its formula, looks like this (as $N=100\%=1$ and the total rows are also equal to $100\%$ it simplifies things quite a lot):
	
	But this can be simplified even more. Indeed, if you remember how we computer the $p_{ij}$, these are the multiplication of the row total and column total. But in our special case above (with a table containing only proportions!), the rows are always equal to $100\%=1$. Therefore only the total column terms need to be written. This bring us to write:
	
	We can simplify even more:
	
	Remember that:
	
	Plug-in that latter in the numerator in the above relation, we get:
	
	We divide both numerator and denominator by the $(n_1n_2^2+n_2n_1^2)$ and get:
	
	and as the $\chi^2_1$ is equal to $Z^2$ this lead us to:
	
	that is the squared $Z$-statistic of the $Z$-test of proportions (pooled version as seen at page \pageref{pooled z-test proportions}) for \textit{Success} response!!
	
	Thus, the $2 \times 2$ homogeneity Chi-square statistic (and test) is equivalent to the $Z$-test of two proportions. The so called expected frequencies computed in the chi-square test in a given column is the weighted (by the group $n$) average vertical profile (i.e. the profile of the "average group") multiplied by that group's $n$. Thus, it comes out that chi-square tests the deviation of each of the two groups profiles from this average group profile, - which is equivalent to testing the groups' profiles difference from each other, which is the $Z$-test of proportions. This is why quite a significant percentage of webdesigner refer to A/B proportion test as $Z$-test or $\chi^2$ test because they are simply the same (however rigorously we should always mention if we use the pooled or unpooled version with or without Yates correction...)
	
	This result demonstrates of a link between variables association measure ($\chi^2$) and a group difference measure ($Z$-test statistic). Attribute associations and group differences are (often) the two facets of the same thing.
	
	Recall from our $Z$-test of two proportions that our null hypothesis is that the two population proportions, $p_{1}$ and $p_{2}$, were assumed equal while the two-sided alternative hypothesis was that they were not equal.
	
	This null hypothesis would be analogous to the two groups being independent.
Also, if the two success proportions are equal, then the two failure proportions would also be equal.

	\begin{tcolorbox}[enhanced,title=Remark,colframe=black,arc=10pt,drop lifted shadow,after skip=15pt plus 2pt]
	For a practical (i.e. numerical) example of the equivalent between the pooled proportions $Z$-test and the pooled $2\times 2$ contingency $\chi^2$ test, the reader can refer to our \texttt{R} companion book where we will show that in facts \texttt{R} always use the chi-square version whatever the function we use (\texttt{prop.test()} function or \texttt{chisq.test()} both use the chi-square test anyway!).
	\end{tcolorbox}
	
	\pagebreak
	\paragraph{Cramér's V}\mbox{}\\\\
	The "\NewTerm{Cramér's V}\index{Cramér's V}" (sometimes referred to as "\NewTerm{Cramér's phi}\index{Cramér's phi}" and denoted as $\varphi_c$) is a measure of association between two nominal variables, giving a value between $0$ and $+1$ (inclusive). It is based on Pearson's chi-squared statistic and was published by Harald Cramér in 11946 (holocene calendar).
	
	As we will prove it, $\varphi_c$ is the intercorrelation of two discrete variables and may be used with variables having two or more levels. $\varphi_c$ is a symmetrical measure, it does not matter which variable we place in the columns and which in the rows. Also, the order of rows/columns doesn't matter, so $\varphi_c$  may be used with nominal data types or higher (ordered, numerical, etc.)
	
	We have seen just previously that the chi-square test of independence may be used to measure the degree of association of two categorical variables in a contingency table of $l$ rows and $c$ columns:
	
	and that this distance follows a chi-square distribution with $(l-1) (c-1)$ degrees of freedom. We will prove intuitively that the maximum value of the distance $D$ is given by:
	
	and that maximum value is achieved if and only if each row or column contains exactly one non-zero value. Under the latter condition, we can always rearrange the contingency table so to have all non-zero terms on the diagonal of the table.
	
	Obviously if the table is not square as below:
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|l|c|c|c|c|c|}
		\hline 
		{\cellcolor[gray]{0.75}\textbf{Row/Column}} & {\cellcolor[gray]{0.75}$\boldsymbol{c_1}$} & {\cellcolor[gray]{0.75}$\boldsymbol{c_2}$} & {\cellcolor[gray]{0.75}$\boldsymbol{c_3}$}  & {\cellcolor[gray]{0.75}$\boldsymbol{c_4}$}  & {\cellcolor[gray]{0.75}\textbf{Total}}\\ 
		\hline 
		{\cellcolor[gray]{0.75}$\boldsymbol{l_1}$} & $a_1$ & $0$ & $0$ & $0$ &  $a_1$ \\  \hline 
		{\cellcolor[gray]{0.75}$\boldsymbol{l_2}$} & $0$ & $a_2$ & $0$ & $0$ &  $a_2$ \\  \hline 
		{\cellcolor[gray]{0.75}$\boldsymbol{l_3}$} & $0$ & $0$ & $a_3$ & $0$ &  $a_3$ \\  \hline 
		{\cellcolor[gray]{0.75}$\boldsymbol{l_4}$} & $0$ & $0$ & $0$ & $a_4$ &  $a_4$ \\  \hline 
		{\cellcolor[gray]{0.75}$\boldsymbol{l_5}$} & $0$ & $0$ & $0$ & $0$ & $0$ \\ \hline  
		{\cellcolor[gray]{0.75}\textbf{Total}} & $a_1$ & $a_2$ & $a_3$ & $a_4$ & $N$ \\ 	\hline 
		\end{tabular}
		\caption[]{Rectangular contingency table}
	\end{table}
	The case that maximize $D$ request diagonal terms on the smallest dimension in row or column (row dimension in the example above) denoted by tradition $q$. In the special example above we have:
	
	In this special case and obviously theoretical one, the rows that have only zero values can be omitted and therefore the previous table can be reduced to:
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|l|c|c|c|c|c|}
		\hline 
		{\cellcolor[gray]{0.75}\textbf{Row/Column}} & {\cellcolor[gray]{0.75}$\boldsymbol{c_1}$} & {\cellcolor[gray]{0.75}$\boldsymbol{c_2}$} & {\cellcolor[gray]{0.75}$\boldsymbol{c_3}$}  & {\cellcolor[gray]{0.75}$\boldsymbol{c_4}$}  & {\cellcolor[gray]{0.75}\textbf{Total}}\\ 
		\hline 
		{\cellcolor[gray]{0.75}$\boldsymbol{l_1}$} & $a_1$ & $0$ & $0$ & $0$ &  $a_1$ \\  \hline 
		{\cellcolor[gray]{0.75}$\boldsymbol{l_2}$} & $0$ & $a_2$ & $0$ & $0$ &  $a_2$ \\  \hline 
		{\cellcolor[gray]{0.75}$\boldsymbol{l_3}$} & $0$ & $0$ & $a_3$ & $0$ &  $a_3$ \\  \hline 
		{\cellcolor[gray]{0.75}$\ldots$} & $\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ \\ \hline  
		{\cellcolor[gray]{0.75}$\boldsymbol{l_q}$} & $0$ & $0$ & $0$ & $a_q$ &  $a_q$ \\  \hline 
		{\cellcolor[gray]{0.75}\textbf{Total}} & $a_1$ & $a_2$ & $\ldots$ & $a_q$ & $N$ \\ 	\hline 
		\end{tabular}
		\caption[]{Squared rectangular contingency table}
	\end{table}
	Of course, ignoring the rows or columns that have only zero values bring us to assume that for the distance $D$ of the chi-square we request that:
	
	which is quite indisputable ... For what will follow, we will need the following relations:
	
	and:
	
	Therefore it comes:
	
	We then define the following value:
	
	as being the "\NewTerm{Cramér's V coefficient}\index{Cramér's V coefficient}" (the majority of software, however, give the value of $V$ squared). The latter is such that it never exceeds $1$ and allows a more intuitive interpretation of the degree of association in a contingency table.
	
	In the case where the table size is of $2$ rows and $2$ columns, the previous relation reduces then immediately to:
	
	Relation that is traditional to note in this case as follows:
	
	and to name "\NewTerm{Cramér's phi}\index{Cramér's phi}" or "\NewTerm{phi coefficient}\index{phi coefficient}".
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Consider the following table (even if the conditions are not met for a chi-square test of independence):
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|l|c|c|c|}
		\hline 
		{\cellcolor[gray]{0.75}\textbf{Projects}} & {\cellcolor[gray]{0.75}\parbox{3.5cm}{\textbf{Certified} \\\textbf{Project Manager}}} &  {\cellcolor[gray]{0.75}\parbox{3.5cm}{\textbf{Non-Certified} \\\textbf{Project Manager}}} & {\cellcolor[gray]{0.75}\textbf{Total}}\\ 
		\hline 
		{\cellcolor[gray]{0.75}\textbf{Deadlines met}} & $8$ & $1$ & $9$ \\  \hline 
		{\cellcolor[gray]{0.75}\textbf{Deadlines not met}} & $4$ & $5$ & $9$  \\  \hline 
		{\cellcolor[gray]{0.75}\textbf{Total}} & $12$ & $6$ & $18$  \\ 	\hline 
		\end{tabular} 
		\caption[]{Example of a contingency table for calculating the Cramér's V}
	\end{table}
	with the theoretical effectives:
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|l|c|c|c|}
		\hline 
		{\cellcolor[gray]{0.75}\textbf{Projects}} & {\cellcolor[gray]{0.75}\parbox{3.5cm}{\textbf{Certified} \\\textbf{Project Manager}}} &  {\cellcolor[gray]{0.75}\parbox{3.5cm}{\textbf{Non-Certified} \\\textbf{Project Manager}}} & {\cellcolor[gray]{0.75}\textbf{Total}}\\ 
		\hline 
		{\cellcolor[gray]{0.75}\textbf{Deadlines met}} & $(12/18)\cdot 9=6$ & $(6/18)\cdot 9=3$ & $9$ \\  \hline 
		{\cellcolor[gray]{0.75}\textbf{Deadlines not met}} & $(12/18)\cdot 9=6$ & $(6/18)\cdot 9=3$ & $9$  \\  \hline 
		{\cellcolor[gray]{0.75}\textbf{Total}} & $12$ & $6$ & $18$  \\ 	\hline 
		\end{tabular} 
		\caption[]{Contingency table of the example with theoretical effectives}
	\end{table}
	Then we have:
	
	And to a threshold level of $95\%$, we get with Microsoft Excel 14.0.6123:
	
	or with the corresponding $p$-value:
	
	It then comes immediately:
	
	We are a bit at the limit here ... since the $p$-value is very close to the traditional $0.05$. However, take a decision in this case where the effectives are so low would be equivalent to conclude anything and even more that this statistical tool is built on an accumulation of approximations.
	\end{tcolorbox}
	
	\subsubsection{Pearson's phi coefficient}\label{Pearson's phi coefficient}
	In statistics, the "\NewTerm{Pearson's phi coefficient}\index{Pearson's phi coefficient}" is an another measure of association for two binary variables. Introduced by Karl Pearson, this measure is similar to the Pearson correlation coefficient in its interpretation. In fact, a Pearson correlation coefficient estimated for two binary variables (Bernoulli variables) will return the Pearson's phi coefficient.
	
	To introduce this correlation coefficient and proof that it is only a special case of the Pearson correlation coefficient, let us consider the following table:
		
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|c|c|c|}
		\hline
		\multicolumn{2}{|c|}{\cellcolor[gray]{0.75}} & \multicolumn{3}{c|}{\cellcolor[gray]{0.75}\textbf{$Y$}}                                             \\ \hline
		\cellcolor[gray]{0.75}{} & \cellcolor[gray]{0.75}{} & \cellcolor[gray]{0.75}\textbf{$Y=1$} & \cellcolor[gray]{0.75}\textbf{$Y=0$} & \cellcolor[gray]{0.75}\textbf{Total}\\ \hline
		\cellcolor[gray]{0.75}  & \multicolumn{1}{l|}{\multirow{1}{*}{\cellcolor[gray]{0.75}\textbf{$X=1$}}} & $n_{11}$ & $n_{12}$ & $n_{1.}$ \\ \cline{2-5} 
		\parbox[t]{2mm}{{\rotatebox[origin=c]{90}{\cellcolor[gray]{0.75}\textbf{$X$}}}} & \multirow{1}{*}{\cellcolor[gray]{0.75}\textbf{$X=0$}}                       & $n_{21}$ & $n_{22}$ & $n_{2.}$ \\ \cline{2-5}  
		 \cellcolor[gray]{0.75} & \multirow{1}{*}{\cellcolor[gray]{0.75}\textbf{Total}} {\cellcolor[gray]{0.75}} & $n_{.1}$ & $n_{.2}$ & $n$ \\ \cline{2-5}  \hline
		\end{tabular}
		\caption[]{Dichotomous example of a table of contingencies}
	\end{table}
	or written in another way:
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|c|c|c|}
		\hline
		\multicolumn{2}{|c|}{\cellcolor[gray]{0.75}} & \multicolumn{3}{c|}{\cellcolor[gray]{0.75}\textbf{$Y$}}                                             \\ \hline
		\cellcolor[gray]{0.75}{} & \cellcolor[gray]{0.75}{} & \cellcolor[gray]{0.75}\textbf{$Y=1$} & \cellcolor[gray]{0.75}\textbf{$Y=0$} & \cellcolor[gray]{0.75}\textbf{Total}\\ \hline
		\cellcolor[gray]{0.75}  & \multicolumn{1}{l|}{\multirow{1}{*}{\cellcolor[gray]{0.75}\textbf{$X=1$}}} & $a$ & $b$ & $a+b$ \\ \cline{2-5} 
		\parbox[t]{2mm}{{\rotatebox[origin=c]{90}{\cellcolor[gray]{0.75}\textbf{$X$}}}} & \multirow{1}{*}{\cellcolor[gray]{0.75}\textbf{$X=0$}} & $c$ & $d$ & $c+d$ \\ \cline{2-5}  
		 \cellcolor[gray]{0.75} & \multirow{1}{*}{\cellcolor[gray]{0.75}\textbf{Total}} {\cellcolor[gray]{0.75}} & $a+c$ & $b+d$ & $1$ \\ \cline{2-5}  \hline
		\end{tabular}
	\end{table}
	Let us recall that the Pearson's correlation coefficient:
	
	Now let $X$ and $Y$ be Bernoulli random variables (see page \pageref{bernoulli distribution}). We don't assume independence or identical distribution, but we do assume that all $4$ of the following probabilities are non-zero.

	Let:
	
	We have first:
	
	We also have:
	
	and:
	
	and:
	
	Finally, by substitution into the equation for $\rho_{XY}$:
	
	Doing the right substitutions, this lead us to:
	
	If we replace $n$ by $n_{11}+n_{12}+n_{22}+n_{21}$ and $n_{1.}$ by $n_{11}+n_{12}$ and $n_{.1}$ by $n_{11}+n_{21}$ then we get after some very easy simplification:
	
	The interpretation for the phi coefficient is similar to the Pearson correlation coefficient. The range is from $-1$ to $+1$, where:
	\begin{itemize}
		\item $0$ is no relationship
		\item $+1$ is a perfect positive relationship: most of your data falls along the diagonal cells.
		\item $-1$ is a perfect negative relationship: most of your data is not on the diagonal
	\end{itemize}
	The Political Science Department at Quinnipiac University posted this useful list of the meaning of Pearson’s Correlation coefficients. The same rule of thumb can be used for the Phi coefficient. Note that these are "crude estimates" for interpreting strengths of relationships:
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|c|c|}
			\hline
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Interval}}  & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Interpretation}} \\ \hline
			$0.7\leq \varphi \leq 1$ & Very strong positive relationship\\ \hline		
			$0.4\leq \varphi < 0.70$ & Strong positive relationship\\ \hline
			$0.3\leq \varphi < 0.4$ & Moderate positive relationship\\ \hline
			$0.2\leq \varphi < 0.3$ & Weak positive relationship\\ \hline
			$0.0< \varphi < 0.2$ & No or negligible positive relationship\\ \hline
			$\varphi=0$ & No relationship\\ \hline
			$0.0> \varphi > -0.2$ & No or negligible negative relationship\\ \hline
			$-0.2> \varphi > -0.3$ & Weak negative relationship\\ \hline
			$-0.3> \varphi > -0.4$ & Moderate negative relationship\\ \hline
			$-0.4> \varphi > -0.70$ & Strong negative relationship\\ \hline
			$-0.7> \varphi \geq 1$ & Very strong negative relationship\\ \hline
			\end{tabular}
			\caption{$\varphi$ Pearson correlation coefficient}
		\end{center}
	\end{table}
	Although computationally the Pearson correlation coefficient reduces to the Pearson's phi coefficient in the $2\times 2$ case, they are not in general the same. The Pearson correlation coefficient ranges from $-1$ to $+1$, where $\pm 1$ indicates perfect agreement or disagreement, and $0$ indicates no relationship. The phi coefficient has a maximum value that is determined by the distribution of the two variables if one or both variables can take on more than two values.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Consider the following table (even if the conditions are not met for a chi-square test of independence):
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|l|c|c|c|}
		\hline 
		{\cellcolor[gray]{0.75}\textbf{Projects}} & {\cellcolor[gray]{0.75}\parbox{3.5cm}{\textbf{Certified} \\\textbf{Project Manager}}} &  {\cellcolor[gray]{0.75}\parbox{3.5cm}{\textbf{Non-Certified} \\\textbf{Project Manager}}} & {\cellcolor[gray]{0.75}\textbf{Total}}\\ 
		\hline 
		{\cellcolor[gray]{0.75}\textbf{Deadlines met}} & $8$ & $1$ & $9$ \\  \hline 
		{\cellcolor[gray]{0.75}\textbf{Deadlines not met}} & $4$ & $5$ & $9$  \\  \hline 
		{\cellcolor[gray]{0.75}\textbf{Total}} & $12$ & $6$ & $18$  \\ 	\hline 
		\end{tabular} 
		\caption[]{Squared rectangular contingency table}
	\end{table}
	Then we have:
	
	\end{tcolorbox}
	We can prove that Pearson's phi coefficient is equal to Cramèr's phi coefficient seen just before (that's by the way why the notation is the same!) and that was given for recall:
	
	To simplify the notation, let us put:
	
	We then have for a contingency table of size $2\times 2$:
	
	With same as for our study of the Cramèr's V coefficient:
	
	Which implies:
	
	But we have:
	
	and:
	
	and:
	
	and:
	
	Hence:
	
	Notice that:
	
	This last result bring us to:
	
	Hence:
	
	Pearson's phi coefficient is the source of inspiration of "\NewTerm{Matthews correlation coefficient}\index{Matthews correlation coefficient}\label{Matthews correlation coefficient}" (M.C.C.) that is used in Machine Learning as a measure of the quality of binary (two-class) classifications. Only the vocabulary differs. Indeed, it is defined as:
	
	While there is no perfect way of describing the confusion matrix $\mathcal{C}$ of true and false positives and negatives by a single number (we will see in the chapter of Numerical Methods what are "confusion matrices"!), the Matthews correlation coefficient is generally regarded as being one of the best such measures in this beginning of the 121st century (holocene calendar).
	
	In this relation, TP is the number of true positives, TN the number of true negatives, FP the number of false positives and FN the number of false negatives. If any of the four sums in the denominator is zero, the denominator can be arbitrarily set to one; this results in a Matthews correlation coefficient of zero, which can be shown to be the correct limiting value.
	
	\subsubsection{Exact Fisher's Test}\label{exact Fisher test}\index{statistical tests!exact Fisher test}
	When the effective in the contingency table are too small or that the values are really too irregular, the use of chi-square (Pearson's test) is not possible because the application conditions are no longer valid. We will see that the Fisher exact test can be formalized analytically in $2\times 2$ contingency tables (the majority of statistical software only support this particular scenario for the Fisher exact test) otherwise what must be used Monte Carlo simulations.
	
	The principle of the "\NewTerm{Fisher's exact test}\index{Fisher exact test}" (usable both in bilateral or unilateral even if this latter is far more common in practice), so based on the crossover frequency, is to determine whether the pattern observed in the table contingency is an extreme situation with respect to all possible situations. As we will prove it, this test has the special property that any cell of the table can be referred for the test because the underlying distributions (marginals) of probability are equivalent.
	
	To study this test, as often in this section, we use directly an example as theoretical companion. 
	
	Let us consider the following contingency table (which now is known to us ...):
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|l|c|c|c|}
		\hline 
		{\cellcolor[gray]{0.75}\textbf{Projects}} & {\cellcolor[gray]{0.75}\parbox{3.5cm}{\textbf{Certified} \\\textbf{Project Manager}}} &  {\cellcolor[gray]{0.75}\parbox{3.5cm}{\textbf{Non-Certified} \\\textbf{Project Manager}}} & {\cellcolor[gray]{0.75}\textbf{Total}}\\ 
		\hline 
		{\cellcolor[gray]{0.75}\textbf{Deadlines met}} & $8$ & $1$ & $9$ \\  \hline 
		{\cellcolor[gray]{0.75}\textbf{Deadlines not met}} & $4$ & $5$ & $9$  \\  \hline 
		{\cellcolor[gray]{0.75}\textbf{Total}} & $12$ & $6$ & $18$  \\ 	\hline 
		\end{tabular} 
		\caption[]{Squared rectangular contingency table}
	\end{table}
	which is not really suitable for chi-square test of independence since the content of the cells is less than $10$ units and the number of degrees of freedom would be equal to unity.
	
	The same table in percentages will give (even if it is unnecessary for the study of this test):
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|l|c|c|c|}
		\hline 
		{\cellcolor[gray]{0.75}\textbf{Projects}} & {\cellcolor[gray]{0.75}\parbox{3.5cm}{\textbf{Certified} \\\textbf{Project Manager}}} &  {\cellcolor[gray]{0.75}\parbox{3.5cm}{\textbf{Non-Certified} \\\textbf{Project Manager}}} & {\cellcolor[gray]{0.75}\textbf{Total}}\\ 
		\hline 
		{\cellcolor[gray]{0.75}\textbf{Deadlines met}} & $88.88\%$ & $11.12\%$ & $50\%$ \\  \hline 
		{\cellcolor[gray]{0.75}\textbf{Deadlines not met}} & $44.44\%$ & $55.56\%$ & $50\%$  \\  \hline 
		{\cellcolor[gray]{0.75}\textbf{Total}} & $66.66\%$ & $33.34\%$ & $100\%$  \\ 	\hline 
		\end{tabular} 
	\end{table}
	The theoretical effectives are given by (even if it is as useless to study this test but statistical software also frequently communicate these values):
	\begin{table}[H]
		\begin{center}
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|l|c|c|c|}
		\hline 
		{\cellcolor[gray]{0.75}\textbf{Projects}} & {\cellcolor[gray]{0.75}\parbox{3.5cm}{\textbf{Certified} \\\textbf{Project Manager}}} &  {\cellcolor[gray]{0.75}\parbox{3.5cm}{\textbf{Non-Certified} \\\textbf{Project Manager}}} & {\cellcolor[gray]{0.75}\textbf{Total}}\\ 
		\hline 
		{\cellcolor[gray]{0.75}\textbf{Deadlines met}} & $(12/18)\cdot 9=6$ & $(6/18)\cdot 9=3$ & $9$ \\  \hline 
		{\cellcolor[gray]{0.75}\textbf{Deadlines not met}} & $(12/18)\cdot 9=6$ & $(6/18)\cdot 9=3$ & $9$  \\  \hline 
		{\cellcolor[gray]{0.75}\textbf{Total}} & $12$ & $6$ & $18$  \\ 	\hline 
		\end{tabular} 
		\end{center}
	\end{table}
	The question we will begin to ask ourselves is the following: knowing the totals for each row and each column, what is the probability of having the values present in each cell?!
	
	This question can be reformulated if we change the table in the following generic form:
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|l|c|c|c|}
		\hline 
		{\cellcolor[gray]{0.75}\textbf{Projects}} & {\cellcolor[gray]{0.75}\parbox{3.5cm}{\textbf{Certified} \\\textbf{Project Manager}}} &  {\cellcolor[gray]{0.75}\parbox{3.5cm}{\textbf{Non-Certified} \\\textbf{Project Manager}}} & {\cellcolor[gray]{0.75}\textbf{Total}}\\ 
		\hline 
		{\cellcolor[gray]{0.75}\textbf{Deadlines met}} & $a=k$ & $b$ & $a+b=m$ \\  \hline 
		{\cellcolor[gray]{0.75}\textbf{Deadlines not met}} & $c$ & $d$ & $c+d$  \\  \hline 
		{\cellcolor[gray]{0.75}\textbf{Total}} & $a+c=p$ & $b+d$ & $a+b+c+d=n$  \\ 	\hline 
		\end{tabular} 
	\end{table}
	Explicitly and relatively to our example by adopting the notation in use of the hypergeometric law, the question is to know what is the probability of having $8$ ($a = k$) projects among the $18$ ($n$) whose deadlines were met by certified project managers knowing that there are $9$ projects ($m$) at total whose deadlines were met and $12$ projects $(p$) at total managed by certified project managers.
	
	We saw earlier that in this case it is an exhaustive drawing, we then have to use the hypergeometric law given by for recall by as proven earlier above by:
	
	Thus with of Microsoft Excel 11.8346:
	\begin{center}
		\texttt{=HYPGEOM.DIST(k,p,m,n,FALSE)}\\
		\texttt{=HYPGEOM.DIST(8,12,9,18,FALSE)=0.06108597}
	\end{center}
	where for recall, $k$ is the number of successes in the sample, $p$ is the sample size, $m$ the number of successes in the population and $n$ the size of the population.

	In fact, as already mentioned, the probabilities are all equal regardless of the selected cell of the contingency table !! This can be checked numerically for sceptics using again a spreadsheet software like Microsoft Excel 14.0.6123 by creating the following structure: 
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.85]{img/arithmetics/exact_fisher_text_excel.jpg}
		\caption[]{Exact Fisher test symmetry with Microsoft Excel 14.0.6123}
	\end{figure}
	and therefore each time that the reader presses the F9 keyboard key he can check that all probabilities are always equal.
	
	This can also be checked formally by selecting a cell of the table and writing:
	
	and for another cell of the same column, we will have:
	
	and therefore:
	
	and so on...

	Anyway, that being said, so we have in the upper left cell the value $8$ while the theoretical effective is $6$. The first thing we can answer is whether this value of $8$ is unusually large or not compared the theoretical effective. For this, we calculate by example the unilateral cumulative probability of being less than or equal to $8$. We have then with Microsoft Excel 14.0.6123 and later (the last parameter of $1$ - equivalent to \texttt{TRUE} - of the function indicate to the software we want the cumulative probability):
	\begin{center}
	\texttt{=HYPGEOM.DIST(8,12,9,18,1)=0.995475113}
	\end{center}
	It therefore appears with a threshold of $5\%$ in unilateral that this value is unusually large. We are then in an extreme situation.
	
	By cons, even if the probabilities are equal for all the cells, the cumulative probability is not! Thus we have for example for the value of the lower left cell (to check if it is abnormally small compared to the theoretical effective of $6$):
	\begin{center}
	\texttt{=HYPGEOM.DIST(4,12,9,18,1)=0.06561086}
	\end{center}
	So this is a value that is not abnormally small. However, we would like to have a test to conclude if the entire table is or is not an extreme configuration. However, by making the calculation cell by cell, we will not get too much...
	
	The idea then is (at least on the "paper") to build all tables whose marginal frequencies are $9;9$ and $12;6$ and to calculate the probability of a given cell (the advantage of this technique is that the conclusion will be the same regardless the chosen cell as reference for the calculations):
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|l|c|c|c|}
		\hline 
		{\cellcolor[gray]{0.75}\textbf{Projects}} & {\cellcolor[gray]{0.75}\parbox{3.5cm}{\textbf{Certified} \\\textbf{Project Manager}}} &  {\cellcolor[gray]{0.75}\parbox{3.5cm}{\textbf{Non-Certified} \\\textbf{Project Manager}}} & {\cellcolor[gray]{0.75}\textbf{Total}}\\ 
		\hline 
		{\cellcolor[gray]{0.75}\textbf{Deadlines met}} & $9$ & $0$ & $9$ \\  \hline 
		{\cellcolor[gray]{0.75}\textbf{Deadlines not met}} & $3$ & $6$ & $9$  \\  \hline 
		{\cellcolor[gray]{0.75}\textbf{Total}} & $12$ & $6$ & $18$  \\ 	\hline 
		\end{tabular} 
	\end{table}
	\begin{center}
	\texttt{=HYPGEOM.DIST(9,12,9,18)=0.004524887}
	\end{center}
	
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|l|c|c|c|}
		\hline 
		{\cellcolor[gray]{0.75}\textbf{Projects}} & {\cellcolor[gray]{0.75}\parbox{3.5cm}{\textbf{Certified} \\\textbf{Project Manager}}} &  {\cellcolor[gray]{0.75}\parbox{3.5cm}{\textbf{Non-Certified} \\\textbf{Project Manager}}} & {\cellcolor[gray]{0.75}\textbf{Total}}\\ 
		\hline 
		{\cellcolor[gray]{0.75}\textbf{Deadlines met}} & $8$ & $1$ & $9$ \\  \hline 
		{\cellcolor[gray]{0.75}\textbf{Deadlines not met}} & $4$ & $5$ & $9$  \\  \hline 
		{\cellcolor[gray]{0.75}\textbf{Total}} & $12$ & $6$ & $18$  \\ 	\hline 
		\end{tabular} 
	\end{table}
	\begin{center}
	\texttt{=HYPGEOM.DIST(8,12,9,18)=0.06108597}
	\end{center}
	
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|l|c|c|c|}
		\hline 
		{\cellcolor[gray]{0.75}\textbf{Projects}} & {\cellcolor[gray]{0.75}\parbox{3.5cm}{\textbf{Certified} \\\textbf{Project Manager}}} &  {\cellcolor[gray]{0.75}\parbox{3.5cm}{\textbf{Non-Certified} \\\textbf{Project Manager}}} & {\cellcolor[gray]{0.75}\textbf{Total}}\\ 
		\hline 
		{\cellcolor[gray]{0.75}\textbf{Deadlines met}} & $7$ & $2$ & $9$ \\  \hline 
		{\cellcolor[gray]{0.75}\textbf{Deadlines not met}} & $5$ & $4$ & $9$  \\  \hline 
		{\cellcolor[gray]{0.75}\textbf{Total}} & $12$ & $6$ & $18$  \\ 	\hline 
		\end{tabular} 
	\end{table}
	\begin{center}
	\texttt{=HYPGEOM.DIST(7,12,9,18)=0.244343891}
	\end{center}
	
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|l|c|c|c|}
		\hline 
		{\cellcolor[gray]{0.75}\textbf{Projects}} & {\cellcolor[gray]{0.75}\parbox{3.5cm}{\textbf{Certified} \\\textbf{Project Manager}}} &  {\cellcolor[gray]{0.75}\parbox{3.5cm}{\textbf{Non-Certified} \\\textbf{Project Manager}}} & {\cellcolor[gray]{0.75}\textbf{Total}}\\ 
		\hline 
		{\cellcolor[gray]{0.75}\textbf{Deadlines met}} & $6$ & $3$ & $9$ \\  \hline 
		{\cellcolor[gray]{0.75}\textbf{Deadlines not met}} & $6$ & $3$ & $9$  \\  \hline 
		{\cellcolor[gray]{0.75}\textbf{Total}} & $12$ & $6$ & $18$  \\ 	\hline 
		\end{tabular}
	\end{table}
	\begin{center}
	\texttt{=HYPGEOM.DIST(6,12,9,18)=0.380090498}
	\end{center}
	
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|l|c|c|c|}
		\hline 
		{\cellcolor[gray]{0.75}\textbf{Projects}} & {\cellcolor[gray]{0.75}\parbox{3.5cm}{\textbf{Certified} \\\textbf{Project Manager}}} &  {\cellcolor[gray]{0.75}\parbox{3.5cm}{\textbf{Non-Certified} \\\textbf{Project Manager}}} & {\cellcolor[gray]{0.75}\textbf{Total}}\\ 
		\hline 
		{\cellcolor[gray]{0.75}\textbf{Deadlines met}} & $5$ & $4$ & $9$ \\  \hline 
		{\cellcolor[gray]{0.75}\textbf{Deadlines not met}} & $7$ & $2$ & $9$  \\  \hline 
		{\cellcolor[gray]{0.75}\textbf{Total}} & $12$ & $6$ & $18$  \\ 	\hline 
		\end{tabular} 
	\end{table}
	\begin{center}
	\texttt{=HYPGEOM.DIST(5,12,9,18)=0.244343891}
	\end{center}
	
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|l|c|c|c|}
		\hline 
		{\cellcolor[gray]{0.75}\textbf{Projects}} & {\cellcolor[gray]{0.75}\parbox{3.5cm}{\textbf{Certified} \\\textbf{Project Manager}}} &  {\cellcolor[gray]{0.75}\parbox{3.5cm}{\textbf{Non-Certified} \\\textbf{Project Manager}}} & {\cellcolor[gray]{0.75}\textbf{Total}}\\ 
		\hline 
		{\cellcolor[gray]{0.75}\textbf{Deadlines met}} & $4$ & $5$ & $9$ \\  \hline 
		{\cellcolor[gray]{0.75}\textbf{Deadlines not met}} & $8$ & $1$ & $9$  \\  \hline 
		{\cellcolor[gray]{0.75}\textbf{Total}} & $12$ & $6$ & $18$  \\ 	\hline 
		\end{tabular}
	\end{table}
	\makebox[\textwidth]{\texttt{=HYPGEOM.DIST(4,12,9,18)=0.061085973}}
	
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|l|c|c|c|}
		\hline 
		{\cellcolor[gray]{0.75}\textbf{Projects}} & {\cellcolor[gray]{0.75}\parbox{3.5cm}{\textbf{Certified} \\\textbf{Project Manager}}} &  {\cellcolor[gray]{0.75}\parbox{3.5cm}{\textbf{Non-Certified} \\\textbf{Project Manager}}} & {\cellcolor[gray]{0.75}\textbf{Total}}\\ 
		\hline 
		{\cellcolor[gray]{0.75}\textbf{Deadlines met}} & $3$ & $6$ & $9$ \\  \hline 
		{\cellcolor[gray]{0.75}\textbf{Deadlines not met}} & $9$ & $0$ & $9$  \\  \hline 
		{\cellcolor[gray]{0.75}\textbf{Total}} & $12$ & $6$ & $18$  \\ 	\hline 
		\end{tabular} 
	\end{table}
	\makebox[\textwidth]{\texttt{=HYPGEOM.DIST(3,12,9,18)=0.004524887}}
	
	To summarize with the values of $k$ (corresponding to the top left cell):
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|c|c|c|}
		\hline
		\multicolumn{1}{c}{\cellcolor[gray]{0.75} $k$}  & 
	    \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Probability}} \\ \hline
		$9$ & $0.00452489$ \\ \hline
		$8$ & $0.06108597$ \\ \hline
		$7$ & $0.24434389$ \\ \hline
		$6$ & $0.3800905$ \\ \hline
		$5$ & $0.24434389$ \\ \hline
		$4$ & $0.06108597$ \\ \hline
		$3$ & $0.00452489$ \\ \hline \hline
		\textbf{Sum:} & $1$ \\ \hline
		\end{tabular}
		\caption[]{Probability of the hypergeometric law corresponding\\to the combination}
	\end{table}
	As in the column of the original table which we just worked with the smallest value is $4$ and the biggest $8$, we will take the tail probabilities to get what is the $p$-value to be greater or equal $8$ and less than or equal $4$ (this is therefore a bilateral test). Then we have:
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|c|c|c|}
		\hline
		\multicolumn{1}{c}{\cellcolor[gray]{0.75} $k$}  & 
	    \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Probability}} \\ \hline
		$9$ & $0.00452489$ \\ \hline
		$8$ & $0.06108597$ \\ \hline
		$4$ & $0.06108597$ \\ \hline
		$3$ & $0.00452489$ \\ \hline \hline
		\textbf{Sum:} & $0.131221719$ \\ \hline
		\end{tabular}
		\caption[]{Selection of values of interest}
	\end{table}
	Therefore the $p$-value is $13.12\%$. We can therefore not say that our original table is in an extremal configuration if we chose an empirical threshold value of $5\%$. Many softwares only communicate the $p$-value.
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The choice of the bounds in this test are subject to debate. Indeed, if we chose for example to focus on the probability to be in the closed bounded interval of $4$ and $8$ (inclusive!), we would have a result of $99.09\%$ and then we should consider that we are in an extreme configuration if we made the $5\%$ empirical threshold choice. Therefore the choice of the bounds with a discrete distribution like the Hypergeometric law many times something not obvious at the opposite of a test based on a continuous distribution that suffers absolutely not of this kind of problems. The majority of softwares that we know at this date take an open bounder interval (this corresponds therefore to the first calculation we made with the $p$-value of $13.12\%$.
	\end{tcolorbox}	
	Finally, to close this subject, the reader can check that he will find the same results whatever he takes for reference cell. 

	Let us also indicate that the Fisher exact test can obviously be used in Machine Learning to measure the performance of a binary classification!!!!!
	
	\subsubsection{Cohen's kappa agreement}
	If our judgements reflect our thinking, they are rarely in agreement with those of others.
	
	This inter-individual variability beneficial to humans, however, is disadvantageous in many scientific disciplines, where it is often necessary to assess and improve the agreement between similar information applied to the same object in the context of a quality control requirement or in sensory analysis.
	
	The nonparametric Cohen's kappa test allows for example to quantify the binary (dichotomous) agreement between two or more researchers or technicians when the judgements are qualitative.
	
	This kappa statistic is frequently used to test "\NewTerm{inter-rater reliability}\index{inter-rater reliability}". The importance of rater reliability lies in the fact that it represents the extent to which the data collected in the study are correct representations of the variables measured. Measurement of the extent to which data collectors (raters) assign the same score to the same variable is named "interrater reliability". While there have been a variety of methods to measure interrater reliability, traditionally it was measured as percent agreement, calculated as the number of agreement scores divided by the total number of scores. In 11960 (holocene calendar), Jacob Cohen critiqued use of percent agreement due to its inability to account for chance agreement. He introduced the Cohen's kappa, developed to account for the possibility that raters actually guess on at least some variables due to uncertainty. Like most correlation statistics, the kappa can range from $-1$ to $+1$. While the kappa is one of the most commonly used statistics to test interrater reliability, it has limitations. Judgements about what level of kappa should be acceptable for health research are questioned. Cohen's suggested interpretation may be too lenient for health related studies because it implies that a score as low as $0.41$ might be acceptable. Kappa and percent agreement are compared, and levels for both kappa and percent agreement that should be demanded in healthcare studies are suggested.
	
	Many situations in the healthcare industry rely on multiple people to collect research or clinical laboratory data. The question of consistency, or agreement among the individuals collecting data immediately arises due to the variability among human observers. Well-designed research studies must therefore include procedures that measure agreement among the various data collectors. Interrater reliability is a concern to one degree or another in most large studies due to the fact that multiple people collecting data may experience and interpret the phenomena of interest differently. 
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	There are a number of statistics that have been used to measure interrater and intrarater reliability. A partial list includes percent agreement, Cohen's kappa (for two raters), the Fleiss kappa (adaptation of Cohen's kappa for three or more raters) the contingency coefficient, the Pearson $R$ and the Spearman rho, the intra-class correlation coefficient, the concordance correlation coefficient, and Krippendorff's alpha (useful when there are multiple raters and multiple possible ratings). Use of correlation coefficients such as Pearson's $R$ may be a poor reflection of the amount of agreement between raters resulting in extreme over or underestimates of the true level of rater agreement.
	\end{tcolorbox}	
	Let us take the case in the medical field where two or more practitioners examining the same patient or even offer different diagnoses or different therapeutic decisions. In the absence of a reference, this proliferation of advices does not bring the expected security of a perfect diagnostic or therapeutic agreement for the physician and the patient. It is therefore important that the agreement in a team or between teams is the best possible guarantee for the quality and continuity of care.
	
	One solution here is to realize a session of "concordance experiment" between physicians to estimate their rate of agreement using the kappa coefficient and study their disagreements to address them.

	To illustrate the concepts, let us consider the very important case of two quality managers that analysed $11$ pieces to reject or accept them. They get:
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|c|c|c|}
		\hline
		\multicolumn{2}{|c|}{\cellcolor[gray]{0.75}} & \multicolumn{3}{c|}{\cellcolor[gray]{0.75}\textbf{Bob}}                                             \\ \hline
		\cellcolor[gray]{0.75}{} & \cellcolor[gray]{0.75}{} & \cellcolor[gray]{0.75}\textbf{Rejected} & \cellcolor[gray]{0.75}\textbf{Accepted} & \cellcolor[gray]{0.75}\textbf{Total}\\ \hline
		\cellcolor[gray]{0.75}  & \multicolumn{1}{l|}{\multirow{1}{*}{\cellcolor[gray]{0.75}\textbf{Rejected}}} & $3$ & $2$ & $5$ \\ \cline{2-5} 
		\parbox[t]{2mm}{{\rotatebox[origin=c]{90}{\cellcolor[gray]{0.75}\textbf{Alice}}}} & \multirow{1}{*}{\cellcolor[gray]{0.75}\textbf{Accepted}}                       & $1$ & $5$ & $6$ \\ \cline{2-5}  
		 \cellcolor[gray]{0.75} & \multirow{1}{*}{\cellcolor[gray]{0.75}\textbf{Total}} {\cellcolor[gray]{0.75}} & $4$ & $7$ & $11$ \\ \cline{2-5}  \hline
		\end{tabular}
		\caption[]{Dichotomous example of table of contingencies}
	\end{table}
	The theoretical frequencies being obtained always by a rule of three (same calculation of rule of three that for the  chi-square and Fisher test of independence exact seen above):
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|c|c|c|}
		\hline
		\multicolumn{2}{|c|}{\cellcolor[gray]{0.75}} & \multicolumn{3}{c|}{\cellcolor[gray]{0.75}\textbf{Bob}}                                             \\ \hline
		\cellcolor[gray]{0.75}{} & \cellcolor[gray]{0.75}{} & \cellcolor[gray]{0.75}\textbf{Rejected} & \cellcolor[gray]{0.75}\textbf{Accepted} & \cellcolor[gray]{0.75}\textbf{Total}\\ \hline
		\cellcolor[gray]{0.75}  & \multicolumn{1}{l|}{\multirow{1}{*}{\cellcolor[gray]{0.75}\textbf{Rejected}}} &  $(4/11)\cdot 5=1.82$ & $(7/11)\cdot 5=3.18$ & $5$ \\ \cline{2-5} 
		\parbox[t]{2mm}{{\rotatebox[origin=c]{90}{\cellcolor[gray]{0.75}\textbf{Alice}}}} & \multirow{1}{*}{\cellcolor[gray]{0.75}\textbf{Accepted}}                       & $(4/11)\cdot 6=2.18$ & $(7/11)\cdot 6=3.82$ & $6$ \\ \cline{2-5}  
		 \cellcolor[gray]{0.75} & \multirow{1}{*}{\cellcolor[gray]{0.75}\textbf{Total}} {\cellcolor[gray]{0.75}} & $4$ & $7$ & $11$ \\ \cline{2-5}  \hline
		\end{tabular}
		\caption[]{Theoretical frequencies}
	\end{table}
	Cohen's kappa is defined by the ratio:
	
	Therefore in our example:
	
	This value of $0.441$ indicates a moderate agreement between the two individuals (Alice and Bob).

	If instead of having the frequencies, we work in percentages (proportions) of the total, the Kappa is then:
	
	Which will give obviously in our example:
	
	That it is usage to write in the following condensed form:
	
	with:
	
	where $+1$ corresponds to a perfect agreement and $-1$ to a perfect non-agreement. Obviously to have a perfect agreement, we must have the cells (Rejected, Rejected) and (Accepted, Accepted) that are equal and that the other cells are zero.
	
	The following table of interpretation of the positive part of the $\kappa_C$ is of common usage (the negative one having not too much interest...):
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|c|c|}
		\hline
		\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Interval}}  & 
		\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Interpretation}} \\ \hline
		$0.8\leq \kappa \leq 1$ & Very good agreement\\ \hline		
		$0.6\leq \kappa < 0.8$ & Good agreement\\ \hline
		$0.4\leq \kappa < 0.6$ & Moderated agreement\\ \hline
		$0.2\leq \kappa < 0.4$ & Poor agreement\\ \hline
		$0.0\leq \kappa < 0.2$ & No agreement\\ \hline
		\end{tabular}
		\caption{$\kappa_C$ usage interpretation table}
	\end{table}
	However, the practitioner must be very critical (as always!) by using this type of tool. Understanding its construction also helps - also as always - to identify its weaknesses and assumptions that are quite questionable.

	Let us finally indicate that the Cohen's Kappa can obviously be used in Machine Learning to measure the performance of a binary classification! A quite well known example in that field is given by the following illustration made by Danny Butvinik:
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/arithmetics/cohen_kappa_classification.jpg}
		\caption[Cohen Kappa statistics in binary classification models]{Cohen Kappa statistics in binary classification models (author: Danny Butvinik)}
	\end{figure}
	How do we get the relation visible in the figure above?:
	
	where TP are the true positives, FP are the false positives, TN are the true negatives, and FN are the false negatives\footnote{In this case, Cohen's Kappa is equivalent to the Heidke skill score known in Meteorology. The measure was first introduced by Myrick Haskell Doolittle in 11888 (holocene calendar).}. It's quite simple (but boring)!
	
	So based on the above table we see that:
	
	and:
	
	Injecting that into:
	
	we get the expected result visible in the figure above! Indeed, let us put TP$=a$, TN$=b$, FP$=c$, FN=$d$. Then (on request of reader...):
	
	That's it!
	
	\subsubsection{McNemar's test}\label{mcnemar test}\index{statistical tests!McNemar's test}
	McNemar's test may well be calculated along the Cohen's Kappa (the first being a statistical hypothesis test and the second one only an empirical point estimator of concordance). The idea is that under the null hypothesis $H_0$ (in this case named "symmetry hypothesis"), one of the diagonals of the table should have equal values. In other words in the form of proportions and only by focusing on one of the two diagonals:
	
	or of frequencies:
	
	Knowing that:
	
	and under the condition that $n$ is large enough, we can write based on a binomial law whose behaviour is asymptotically Normal:
	
	We can realize that this is equivalent to write:
	
	In the literature, we often find this last relation in the form:
	
	To return to our original relation, some take the square and then approximate the square of $Z$ as a chi-square law (... !) with one degree of freedom (but good in reality approximate a Normal centered reduced distribution by the chi-square law with one degree of freedom is bulls... in our point of view...):
	
	which is often the relation defined in the books (without proof...) as the "\NewTerm{McNemar test}\index{McNemar test}".

	The test is normally conducted bilaterally. The advantage of the McNemar test is the ease with which we can build a confidence interval of the difference in the diagonal. Indeed, starting from the estimator of the difference:
	
	Therefore using the variance (and covariance) proved at page \pageref{covariance trinomial distribution}, we get:
	
	and therefore we can do an approximate confidence intervals of the following if the usual conditions are met:
	
	That is to say a little bit more explicitly:
	
	with as we just proved it:
	
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	During a social audit, a survey is conducted on $200$ employees about the work organization. After reorganization, the same question is asked. Can we consider that there has been real changes?
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|c|c|c|}
			\hline
			{\cellcolor[gray]{0.75}}  & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Yes (before)}}  & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{No (before)}} \\ \hline
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Yes (after)} }& $55$ & $38$ \\ \hline		
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{No (after)}} & $38$ & $82$ \\ \hline
			\end{tabular}
			\caption[]{Paired before/after simple survey}
		\end{center}
	\end{table}
	We then have taking arbitrarily the diagonal $(25, 38)$ the for analysis:
	\begin{gather*}
		Z^2=\dfrac{\left(\dfrac{n_{11}}{n}-0.5\right)^2}{\dfrac{0.5(1-0.5)}{n}}=\dfrac{(b-c)^2}{b+c}=2.684
	\end{gather*}
	The majority of statistical software will not give you $2.683$ as they apply an empirical correction due to the rough approximation of that is the chi-square law. You will then have on the softwares display the value of $2.285$ for $Z^2$.\\

	The $p$-value it is given (without correction) with a spreadsheet software like the Microsoft Excel 14.0.6123:

	\begin{center}
	\texttt{= 1-CHISQ.DIST(2.683, 1, 1)} $= 10.14\%$
	\end{center}
	and with the correction use by statistical software we would get about $13\%$. So in both cases the $p$-value is greater than the traditional $5\%$ threshold, we therefore can not reject the null hypothesis $H_0$ as the difference is large.\\

	We will not calculate in this example the confidence interval of the difference because almost all statistical softwares have different methods for calculating this value.\\
	
	Finally, to close the subject on the McNemar test, let us indicate an empirical indicator often used that is named the "\NewTerm{Yule coefficient}\index{Yule coefficient}" or "\NewTerm{Yule's $Q$}\index{Yule's $Q$}\label{Yule's Q}" and defined by:
	\begin{gather*}
		Q=\dfrac{P\left(X \text{ and } Y \text{ are concordant}\right)-P\left(X\text{ and } Y \text{ are discordant}\right)}{P\left(X\text{ and } Y \text{ are concordant}\right)+P\left(X \text{ and } Y \text{ are discordant}\right)}=\dfrac{ad-bc}{ad+bc}
	\end{gather*}
	Where $a,b,c,d$ must be positive non null integers corresponding to  position values of contingency tables similar to:
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|c|c|c|}
			\hline
			{\cellcolor[gray]{0.75}}  & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Yes}}  & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{No}} \\ \hline
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Positive} }& $a$ & $b$ \\ \hline		
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Negative}} & $c$ & $d$ \\ \hline
			\end{tabular}
		\end{center}
	\end{table}
	\end{tcolorbox}
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Yule’s $Q$ is just the a $2\times 2$ version of the Goodman and Kruskal's gamma coefficient. We can also obviously calculate an odds ratio using $(a\cdot d)/(b\cdot c)$ (and then calculate its confidence interval according to the derivation at page \pageref{odds ratio confidence interval}). We can convert also this odds ratio to a $-1$ to $+1$ scale using $(\text{OR}-1)/(\text{OR}+1)$.
	\end{tcolorbox}
	Yule’s $Q$ is always a number between $-1$ and $+1$. Here is a table with possible interpretation of the values of Yule's $Q$:
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|c|c|}
			\hline
			\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Interval}}  & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Interpretation}} \\ \hline
			$Q=0$ & No association between the variables\\ \hline		
			$0 \leq |Q|\leq 0.29$ & A negligible or very small association\\ \hline
			$0.30 \leq |Q|\leq 0.49$ & A moderate association between the variables\\ \hline
			$0.50 \leq |Q|\leq 0.69$ & A substantial association between the variables\\ \hline
			$0.70 \leq |Q| < 1$ & A very strong association\\ \hline
			$|Q| = 1$ & A perfect association between the events\\ \hline
			\end{tabular}
			\caption{Yule's $Q$ usage interpretation table}
		\end{center}
	\end{table}
	
	The "\NewTerm{Cochran-Armitage test for trend}\index{statistical tests!Cochran-Armitage test for trend}" (see just below), also named "\NewTerm{Chi-square test for trend}\index{chi-square test for trend}" in many statistical softwares, is used in categorical data analysis when the aim is to assess for the presence of an association between a variable with $2$ categories and a variable with $k$ categories. It modifies the Pearson chi-squared test to incorporate a suspected ordering in the effects of the $k$ categories of the second variable. For example, doses of a treatment can be ordered as 'low', 'medium', and 'high', and we may suspect that the treatment benefit cannot become smaller as the dose increases. The trend test is often used as a genotype-based test for case-control genetic association studies.
	
	\paragraph{Derivation for Cochran-Armitage's trend test for the $2\times 3$ genotype table}\mbox{}\\\\
	We will derive here a special case as the general one is out of the scope of this book. Consider for this that we have sampled $R$ cases and $S$ controls:
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|c|c|c|c|}
			\hline 
			\rowcolor[gray]{0.75} & \multicolumn{3}{c|}{\textbf{Number of disease alleles}} &  \\ 
			\hline 
		      &  0 & 1 & 2 & \textbf{Totals}\\
			\hline 
		\textbf{Cases} & $r_0$ & $r_1$ & $r_2$& $R$\\
			\hline 
		\textbf{Controls} & $s_0$ & $s_1$& $s_2$& $S$\\
			\hline 
		\textbf{Totals} &$n_0$& $n_1$ & $n_2$ & $N$ \\
			\hline
		\end{tabular} 
	\end{table}
	We want to test $H_0$ against $H_1$. The idea is:
	\begin{itemize}
		\item $H_0$: All entries in the table are proportional, vs.
		
		\item $H_1$: Within a column, the absolute value of the difference between the probability of an observation being classified as "Case" or "Control" increases monotonically across the table.
	\end{itemize}
	We'll work with the difference between the values in the column so we first standardize the rows to have the same sums:
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|c|c|c|c|}
		\hline 
		\rowcolor[gray]{0.75} & \multicolumn{3}{c|}{\textbf{Number of disease alleles}} &  \\ 
		\hline 
		&  0 &1& 2 & \textbf{Totals}\\
		\hline 
		\textbf{Cases} & $S\cdot{r_0}$ & $S\cdot{r_1}$ & $S\cdot{r_2}$& $R\cdot S$\\
		\hline 
		\textbf{Controls} & $R\cdot{s_0}$ &$R\cdot{s_1}$& $R\cdot{s_2}$& $R\cdot S$\\
		\hline
		\end{tabular} 
	\end{table}

	We choose a set of scores $x_1$, $x_2$, and $x_3$ and form the test statistics:
	
	
	Under $H_0$:
	
	Letting $S = N-R$, $s_i = n_i-r_i$, and choosing scores $x_0 = 0$, $x_1 = 1$,
	and $x_2 = 2$, the test statistic becomes:
	
	Returning to generic scores $x_i$, we calculate the variance of $U$ as:
		
	Under $H_0$ (we use the variance and covariance\footnote{If the reader is not able to find where was our proof of that covariance, you can find it here at page \pageref{covariance trinomial distribution}} of the Hypergeometric distribution ignoring the finite population correction factor):
	
	Let $S = N - R$. Choose scores $x_0 = 0$, $x_1 = 1$, and $x_2 = 2$ and the latter relation reduces to:
	
	For $N$ large (this justifies the fact that we ignored the finite population correction factor earlier), we then have:
	
	So, using Sasieni's (11997 according to holocene calendar) notation:
	
	Again we notice that the Cochran-Armitage test for trend use a Wald Statistics and therefore belongs to the family of Wald tests.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	For a practical example and implementation of that test our reader can refer to the \texttt{R} companion book.
	\end{tcolorbox}
	
	\pagebreak
	\subsection{Survival Statistics}\label{survival analysis}
	Survival analysis is a branch of statistics for analysing the expected duration of time until one or more events happen, such as death in biological organisms, failure in mechanical systems, time to product reorder of customers, etc. 
	
	This topic is named "\NewTerm{reliability theory}\index{reliability theory}" or "\NewTerm{reliability analysis}\index{reliability analysis}" in engineering (\SeeChapter{see section Industrial Engineering page \pageref{preventive maintenance}}), "\NewTerm{duration analysis}" or "\NewTerm{duration modelling}\index{duration modelling}" in economics, and "\NewTerm{event history analysis}\index{event history analysis}" in sociology. Survival analysis attempts to answer questions such as: what is the proportion of a population which will survive past a certain time? Of those that survive, at what rate will they die or fail? Can multiple causes of death or failure be taken into account? How do particular circumstances or characteristics increase or decrease the probability of survival?

	To answer such questions, it is necessary to define "lifetime". In the case of biological survival, death is unambiguous, but for mechanical reliability, failure may not be well-defined, for there may well be mechanical systems in which failure is partial, a matter of degree, or not otherwise localized in time. Even in biological problems, some events (for example, heart attack or other organ failure) may have the same ambiguity. The theory outlined below assumes well-defined events at specific times; other cases may be better treated by models which explicitly account for ambiguous events.

	More generally, survival analysis involves the modelling of time to event data; in this context, death or failure is considered an "event" in the survival analysis literature – traditionally only a single event occurs for each subject, after which the organism or mechanism is dead or broken.
	
	In practice we consider:
	\begin{itemize}
		\item The life tables (\SeeChapter{see section of Population Dynamics page \pageref{life table}}) to describe the survival times of members of a group.
		
		\item The Kaplan-Meier method to also describe the survival times of members of a group (see just below).

		\item The Cochran-Mantel-Haenzel test (Log-Rank test) to compare the survival times of two or more groups (see just below).

		\item The Cox's proportional\footnote{The Cox proportional regression assumes hazards are proportional (PH), which means that the relative hazard remains constant!} hazard model (Cox regression) to describe the effect of categorical or quantitative variables on survival (see just below).
		
		\item The Threshold Regression Models in that also describe the effect of categorical or quantitative variables on survival however without the proportional hazard assumption of the Cox model!
	\end{itemize}
	Here is a more exhaustive (i.e. however still incomplete) taxonomy of survivals analysis methods:
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/arithmetics/taxonomy_survival_analysis_methods.jpg}
		\caption[Taxonomy of the methods developed for survival analysis]{Taxonomy of the methods developed for survival analysis\\(source: ACM Computing Surveys, Vol. 1, No. 1, Article 1, April 12018)}
	\end{figure}

	\pagebreak
	\subsubsection{Kaplan-Meier Survival Rate}
	OK... we don't like to do this in this book!!! But for the comfort of our readers and because of some readers feedback let us copy/paste here the theory about the Kaplan-Meier Survival Rate as introduced initially in the section of Industrial Engineering at page \pageref{Kaplan-Meier survival model}.
	
	In areas such as high-level industry, high-level medicine or high-level biology, we are often interested in the:
	\begin{enumerate}
		\item Survival duration after a serious event
		\item Duration of remission after treatment or surgery
		\item Duration of symptoms after an abnormality
		\item Duration of a symptom-less infection
	\end{enumerate}
	We seek very often to distinguish at least "the event of interest":
	\begin{enumerate}
		\item System shutdown after serious event
		\item End of remission 
		\item End of a symptom after anomaly
		\item Beginning of a symptom during an infection
	\end{enumerate}
	of the variable to explain "duration before the event of interest":
	\begin{enumerate}
		\item Elapsed duration before shutdown
		\item Elapsed duration before the end of remission
		\item Elapsed duration before the end of the symptom
		\item Elapsed duration without symptoms
	\end{enumerate}
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] We name "\NewTerm{remission}\index{remission}", the reduction of a disease or a temporarily malfunction.
		
		\item[D2.] The "\NewTerm{survival time}\index{survival time}" or "\NewTerm{lifetime}\index{lifetime}" $T$ means the time which elapses from an initial time (start of treatment, diagnosis or failure, etc.) until the occurrence of a final event of interest (patient death, relapse, remission, cure, repair, etc.). We say that the object of the study "survives at time $t$" if at this moment the final interest event has not yet occurred.
	\end{enumerate}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Although this type of study can be associated with preventive maintenance (\SeeChapter{see section Industrial Engineering page \pageref{preventive maintenance}}), statisticians associate this type of study rather to the domain of "\NewTerm{survival analysis}\index{survival analysis}".
	\end{tcolorbox}
	
	We will focus in this book on a particular context but that can be easily generalized:
	\begin{itemize}
		\item Cohort/Clinical trial where we study the survival time of each patient (machine).
		
		\item All patients (machines) do not have the same observation time (different instants of entry into the study).
		
		\item We have information on the survival time of each patient (machine) but we do not know exactly when it happens.
	\end{itemize}
	From the last two points, we conclude that the survival time can be censored. So the usual statistical techniques does not apply directly as censored data require special treatment (of course, if we remove the censored data we lose information). It goes without saying that this situation is extremely common and therefore the study of the Kaplan-Meier estimator is very important.
	
	\textbf{Definition (\#\thesection.\mydef):} The duration $T$ is said to be censored if the duration is not observed completely. The different types of censoring are:
	\begin{itemize}
		\item Type I censoring: fixed right (right censored). In this situation, the time is not observable beyond a maximum, fixed, named "\NewTerm{fixed-censoring}\index{fixed-censoring}" and imposed. So either we have the opportunity to observe the real duration of the event of interest for the item if it occurs before the fixed-censoring, or we limit ourselves to the fixed-censoring time if the vent of interest has not occurred before.
		
		\item Type I censorship: fixed left (left censored). In this situation, the study duration is not observable before a known date named  "\NewTerm{waiting censorship}\index{waiting censorship}" and imposed. So either the event of interest takes place at the time of the wait censorship or after. If applicable later, we will consider the length of time between the date of censorship and the date of the event of interest.
		
		\item Type II censoring: wait. In this situation, we observe the lives of $n$ individuals until $m\leq n$ individuals have seen the event to occur (deceased). The duration considered is therefore that of the beginning of the experiment until the event of interest for the $m$-th.
		
		\item Type III censoring: random left. In this situation, we do not know when the event of interest has occurred (because we started to observe the subject of study too late). We can not then deal with "durations" in the measurable sense and we have to limit ourselves to a simple count.
		
		\item Type III censoring: random right. In this situation, we do not know when the event will take place (because we stopped to observe the subject before it occurs  for any reason). We can not then deal with "durations" in the measurable in the measurable sense and we must limit ourselves to a simple count.
		
		\item Type IV censoring: random intervals. In this situation, we have a mixture of random left and right censoring. That is to say that for some study subjects, we do not know when the event of interest has begun, and for others we do not know when it will occur (if any...).
	\end{itemize}
	In the machinery industry, we often deal with the type I censoring: fixed right. In the medical field, in clinical trials, we often deal with a type III censoring: random right. In the case of pandemics, we are dealing with type III censoring: random left.
	
	To introduce this subject, rather than doing obscure theory, as always in this book we prefer at pragmatic approach. Suppose that the study is a clinical trial involving two groups of patients receiving two types of treatments. Two important questions raised to the physicians in a Phase II clinical trial (phase I is for non-toxicity approval to human and phase 0 for animals):
	\begin{enumerate}
		\item Is one of the two treatments more effective than the other in terms of improving patient survival?
		
		\item Can we highlight prognostic factors, that is to say that improve / deteriorate survival?
	\end{enumerate} 
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.75\textwidth]{img/arithmetics/clinical_trials.jpg}
		\caption{Drug development and clinical trials}
	\end{figure}
	
	To answer the first question we can develop statistical methods that will allow us to compare the two groups of patients who receive both types of treatment.
	
	To answer the second question we propose a model that links patient survival time for explanatory variables and highlight the prognostic factors.
	
	Let us as always assist the theory with an example. For this consider the following table where two cohorts of patients (we move from mechanical engineering to human engineering...) of same initial size with acute leukemia drug test (6-MP) against a placebo\footnote{Let us recall that in the science of drugs, a "pure placebo" is a treatment without any active substance; an "impure placebo" is a pharmacologically active product but has no effect on the pathology treated, or whose efficacy has \underline{not been sufficiently demonstrated}.} (of course blindly).
	
	We have the following remission times for $21$ patients (the table of $21$ lines therefore indicates the number of weeks for where patient is considered cured after treatment before falling again ill):
	\begin{table}[H]\centering
	\begin{center}
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|c|c|}
				\hline
				\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{6-mercaptopurine (6-MP) Group}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Placebo Group}} \\ \hline
				 $6$ & $1$ \\ \hline
				 $6$ & $1$ \\ \hline
				 $6$ & $2$ \\ \hline
				 $6+$ & $2$ \\ \hline
				 $7$ & $3$ \\ \hline
				 $9+$ & $4$ \\ \hline
				 $10$ & $4$ \\ \hline
				 $10+$ & $5$ \\ \hline
				 $11+$ & $5$ \\ \hline
				 $13$ & $8$ \\ \hline
				 $16$ & $8$ \\ \hline
				 $17+$ & $8$ \\ \hline
				 $19+$ & $8$ \\ \hline
				 $20+$ & $11$ \\ \hline
				 $22$ & $11$ \\ \hline
				 $23$ & $12$ \\ \hline
				 $25$ & $12$ \\ \hline
				 $32+$ & $15$ \\ \hline
				 $32+$ & $17$ \\ \hline
				 $34+$ & $22$ \\ \hline
				 $35+$ & $23$ \\ \hline
		\end{tabular}
	\end{center}
	\caption[]{Two cohort survival analysis with right censoring}
	\end{table}
	The "$+$" sign corresponds to patients who left the study for that week. They are therefore censored! For example, the fourth patient was lost of view for any reason after $6$ weeks of treatment with 6-MP: it has therefore a duration of remission \underline{greater} than $6$ weeks! So in the study 6-MP, there are $21$ patients and $11$ of them are censored data.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The theoretical model assumes that censoring is independent of the survival time (not informative censoring). But if censoring is due to the discontinuation of treatment, the independence assumption is not valid anymore!
	\end{tcolorbox}
	For the placebo group it is simple to make a survival curve. It is sufficient to produce the following table (for the omitted weeks, obviously we impose the number of remission as constants):
	\begin{table}[H]\centering
	\begin{center}
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|c|c|c|}
				\hline
				\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Week $i$}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Total \# remission at week $i$}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Proportion (probability) of remission at week $i$}} \\ \hline
				 $0$ & $21$ & $100\%$ \\ \hline
				 $1$ & $19$ & $19/21=90\%$ \\ \hline
				 $2$ & $17$ & $17/21=81\%$ \\ \hline
				 $3$ & $16$ & $16/21=76\%$ \\ \hline
				 $4$ & $14$ & $14/21=67\%$ \\ \hline
				 $5$ & $12$ & $12/21=57\%$ \\ \hline
				 $8$ & $8$ & $8/21=38\%$ \\ \hline
				 $11$ & $6$ & $6/21=29\%$ \\ \hline
				 $12$ & $4$ & $4/21=19\%$ \\ \hline
				 $15$ & $3$ & $3/21=14\%$ \\ \hline
				 $17$ & $2$ & $2/21=10\%$ \\ \hline
				 $22$ & $1$ & $1/20=0.05\%$ \\ \hline
				 $23$ & $0$ & $0\%$ \\ \hline
		\end{tabular}
	\end{center}
	\caption[]{Two cohort survival analysis with right censoring}
	\end{table}
	So if the data are not censored, the survival $S(t)$ can be estimated by the proportion of individuals surviving at time $t$, that is customary to write under the following mathematical form:
	
	The idea is therefore to estimate:
	
	by the proportion of patients who survived until time $t$.
	
	If the data are censored, the estimated survival function requires specific tools. Kaplan and Meier have proposed in this particular case the following calculation:
	
	Let's see it in a slightly more mathematical form. If we denote by $X(1)\leq X(2)\leq \ldots \leq X(n)$ the moments (sorted) where an event occurred (death or censored), then we have:
	
	With of course:
	
	We estimate:
	
	where $d_k$ is the number of deaths (failures) observed in the time corresponding to the event $X(k)$ and $R_k$ is the number of individuals at risk (at risk of death/failure) just before $X(k)$.
	
	We define the "\NewTerm{Kaplan-Meier estimator}\index{Kaplan-Meier estimator}" for any $X(0)\leq t <X(k)$ by:
	
	where $\hat{\lambda}_k$ is the discrete hazard (i.e. conditional probability of death).
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	There is a much simpler estimator named "\NewTerm{Nelson-Aalen estimator}" and simply defined as:
	
	That latter estimator is just a sum over the observed survival times.
	\end{tcolorbox}
	
	Therefore we get doing now for 6-MP group (not the placebo group!!!!!) the following:
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
			\resizebox{\textwidth}{!}{\begin{tabular}{|c|c|c|c|}
				\hline
				\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Remission duration}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Items in remission}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Probability of not relapse}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Survival probability of}} \\ 
  				\multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{(uncensored) observed}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{at the \underline{beginning} of $k$}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{at $k$ knowing that we are at}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Kaplan-Meier}} \\
   \multicolumn{1}{c}{\cellcolor[gray]{0.75}} & 
   \multicolumn{1}{c}{\cellcolor[gray]{0.75}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{at $k-1$ ($\hat{P}_k$)}} &  \multicolumn{1}{c}{\cellcolor[gray]{0.75}} \\ \hline
				 $0$ & $21$ & $21/21=100\%$ & $100\%$\\ \hline
				 $6$ & $21$ & $18/21=1-3/21=85.7\%$ & $100\%\cdot85.7\%=85.7\%$ \\ \hline
				 $7$ & $17$ & $16/17=1-1/17=94.1\%$ & $85.7\%\cdot94.1\%=80.7\%$\\ \hline
				 $10$ & $15$ & $14/15=1-1/15=93.3\%$ & $80.7\%\cdot93.3\%=75.3\%$\\ \hline
				 $13$ & $12$ & $11/12=1-1/12=91.7\%$ & $75.3\%\cdot91.7\%=69\%$\\ \hline
				 $16$ & $11$ & $10/11=1-1/11=90.9\%$ & $69\%\cdot90.9\%=62.7\%$\\ \hline
				 $22$ & $7$ & $6/7=1-1/7=85.7\%$ & $62.7\%\cdot85.7\%=53.8\%$\\ \hline
				 $23$ & $6$ & $5/6=1-1/6=83.3\%$ & $53.8\%\cdot83.3\%=44.8\%$ \\ \hline
		\end{tabular}}
		\caption[]{6-MP survival analysis}
	\end{table}
	With the corresponding survival curves (including the confidence interval):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.9]{img/arithmetics/kaplan_meier_survival_curve.jpg}
		\caption{Kaplan-Meier Typical survival curves in \texttt{R} software}
	\end{figure}
	We thus find the same values as those given for example by Minitab Statistical Software 15.1.1 and \texttt{R} (see the companion books on Minitab or \texttt{R} for the details).
	
	If we assume that the $\hat{\lambda}_k$'s are just binomial proportions given the $R_k$'s we have the following normalized variance and mean of binomial random variable (see page \pageref{normalized variance and mean of binomial distribution}):
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Sadly it is a tradition in survival analysis to denote the normalized variance of $\hat{\lambda}_k$ by $\text{V}(\hat{\lambda}_k)$ instead of $\text{V}(\hat{\lambda}_k/r_k)$...
	\end{tcolorbox}
	And we will assume also that the $\hat{\lambda}_k$'s are asymptotically independent (...).
	
	Since $\hat{S}_{\text{KM}}(t)$ is a function of the $\hat{\lambda}_k$'s, we can estimate its variance using the univariate Delta method (\SeeChapter{see section Numerical Methods page \pageref{delta method}}) that has for important result for recall that:
	
	with:
	
	 Therefore instead of dealing with $\hat{S}_{\text{KM}}(t)$ directly, we will look at its natural logarithm:
	
	Thus, by the approximate independence of the $\hat{\lambda}_k$'s and keeping in mind that we just choose $g()$ as the natural logarithm and using the univariate delta method:
	
	And applying on it the univariate delta method:
	
	then rearranging and injecting the previous result we get:
	
	The following approximation:
	
	is named the "\NewTerm{Greenwood's relation}\index{Greenwood's relation}". If we take the square root then the result is associated to the standard error (because of the division by $R_k$):
	
	In order to construct an error bar for $\hat{S}_{\text{KM}}(t)$ we need to make some sort of distributional assumption. The simplest is to assume that $\hat{S}_{\text{KM}}(t)$ is normally distributed such that we can write:
	
	Or more explicitly:
	
	A snag with this, of course is that the distribution of $\hat{S}_{\text{KM}}(t)$ is not really normal. Problems arise when $\hat{S}_{\text{KM}}(t)$ is close to $0$ or $1$ as indeed the confidence interval can yield values to $>1$ or $<0$. A common empirical approach (among others!) is then to take for the confidence interval the complementary log-log transformation:
	
	Since this quantity is unrestricted, the confidence interval will be in the right range when we transform back:
	
	To see why this works:
	
	To get a confidence interval we need now also to find the standard error corresponding to this log-log transformation, and for this purpose we need to calculate:
	
	From our previous calculations, we know:
	
	Applying the univariate delta method again, we get:
	
	We take the square root of the above to get the standard error:
	
	And therefore we have:
	
	Since $\hat{S}_{\text{KM}}(t) = e^{-e^{\hat{W}(t)}}$, the ("modern") confidence bounds for the confidence interval of $\hat{S}_{\text{KM}}(t)$ are finally (note that the upper and lower bound switch!):
	
	Substituting $\hat{W}(t)=\ln\left(-\left(\hat{S}_{\text{KM}}(t)\right)\right)$ back into the above bounds, we get the definitive "modern" relation for the Kaplan-Meier estimator confidence bound:
	
	
	\subsubsection{Cochran–Mantel–Haenszel tests}\label{cochran mantel test}\index{statistical tests!Cochran-Mantel-Haenszel test}\index{statistical tests!log rank test}
	In statistics, the Cochran–Mantel–Haenszel tests are a collection of test statistics used in the analysis of stratified categorical data. One of these test statistics is the "\NewTerm{Cochran–Mantel–Haenszel (CMH) test}\index{Cochran–Mantel–Haenszel test}", which allows the comparison of two groups on a dichotomous/categorical response.
	
	This test, also named "\NewTerm{log rank test}\index{log rank test}" or "\NewTerm{Cochran-Mantel(-Haenszel) test with stratified time}\index{Cochran-Mantel(-Haenszel) test with stratified time}" or "\NewTerm{Mantel-Cox test}\index{Mantel-Cox test}", or "\NewTerm{Cochran-Mantel-Haenszel Chi-square test}\index{Cochran-Mantel-Haenszel Chi-square test}" or furthermore "\NewTerm{multi-strata proportions test}\index{multi-strata proportions test}" ... has for main objective in practice to test the null hypothesis $H_0$ as what two survival curves (control group\footnote{The irony with antivaxers saying they don't want to be part of an experiment is that they don't realize that they are the control group...} versus test group), such as those visible in the figure below (where the surviving population was normalized on the $y$-axis), are significantly different or not under the assumptions that:

	\begin{enumerate}
		\item[H1.] Each stratum (layer) is independent of the previous one.

		\item[H2.] For each stratum we are expecting that the expected proportion of survivors / death is the same all things being equal... or \og \textit{ceteris paribus} \fg{} for those that prefer Latin (see below if this is not clear).

		\item[H3.] Each stratum is distributed according to the hypergeometric law.

		\item[H4.] The hypergeometric distribution can approximated by a Normal distribution (what we will recall, can only be done under certain conditions!!!).
	\end{enumerate}
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,1076); %set diagram left start at 0, and has height of 1076
		
		%Shape: Axis 2D [id:dp38550390203737495] 
		\draw  (163,283.28) -- (546.86,283.28)(174.86,73) -- (174.86,296.28) (539.86,278.28) -- (546.86,283.28) -- (539.86,288.28) (169.86,80) -- (174.86,73) -- (179.86,80) (194.86,278.28) -- (194.86,288.28)(214.86,278.28) -- (214.86,288.28)(234.86,278.28) -- (234.86,288.28)(254.86,278.28) -- (254.86,288.28)(274.86,278.28) -- (274.86,288.28)(294.86,278.28) -- (294.86,288.28)(314.86,278.28) -- (314.86,288.28)(334.86,278.28) -- (334.86,288.28)(354.86,278.28) -- (354.86,288.28)(374.86,278.28) -- (374.86,288.28)(394.86,278.28) -- (394.86,288.28)(414.86,278.28) -- (414.86,288.28)(434.86,278.28) -- (434.86,288.28)(454.86,278.28) -- (454.86,288.28)(474.86,278.28) -- (474.86,288.28)(494.86,278.28) -- (494.86,288.28)(514.86,278.28) -- (514.86,288.28)(169.86,263.28) -- (179.86,263.28)(169.86,243.28) -- (179.86,243.28)(169.86,223.28) -- (179.86,223.28)(169.86,203.28) -- (179.86,203.28)(169.86,183.28) -- (179.86,183.28)(169.86,163.28) -- (179.86,163.28)(169.86,143.28) -- (179.86,143.28)(169.86,123.28) -- (179.86,123.28)(169.86,103.28) -- (179.86,103.28) ;
		\draw   ;
		%Straight Lines [id:da6091198022356801] 
		\draw [color={rgb, 255:red, 2; green, 60; blue, 124 }  ,draw opacity=1 ][line width=1.5]    (173,95) -- (257.86,95) ;
		%Straight Lines [id:da4353973695998905] 
		\draw [color={rgb, 255:red, 2; green, 60; blue, 124 }  ,draw opacity=1 ][line width=1.5]    (261,111) -- (297.86,111) ;
		%Straight Lines [id:da7189638339647504] 
		\draw [color={rgb, 255:red, 2; green, 60; blue, 124 }  ,draw opacity=1 ][line width=1.5]    (299,131) -- (320.86,131) ;
		%Straight Lines [id:da17283514428316016] 
		\draw [color={rgb, 255:red, 2; green, 60; blue, 124 }  ,draw opacity=1 ][line width=1.5]    (321,158) -- (331.86,158) ;
		%Straight Lines [id:da25422431270147317] 
		\draw [color={rgb, 255:red, 2; green, 60; blue, 124 }  ,draw opacity=1 ][line width=1.5]    (332,192) -- (342.86,192) ;
		%Straight Lines [id:da1426955564464938] 
		\draw [color={rgb, 255:red, 2; green, 60; blue, 124 }  ,draw opacity=1 ][line width=1.5]    (343.86,240.28) -- (357.86,240.28) ;
		%Straight Lines [id:da37574480287432754] 
		\draw [color={rgb, 255:red, 2; green, 60; blue, 124 }  ,draw opacity=1 ][line width=1.5]    (357.86,276.28) -- (396.86,276.28) ;
		%Straight Lines [id:da9185449506805985] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (173,95) -- (338.8,95) ;
		%Straight Lines [id:da1976338895556493] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (338.8,112.28) -- (406.86,112.28) ;
		%Straight Lines [id:da26674069004606404] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (405.86,137) -- (446.86,137) ;
		%Straight Lines [id:da3905522057775457] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (448.86,171) -- (467.86,171) ;
		%Straight Lines [id:da8167722247352276] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (469.86,212) -- (488.86,212) ;
		%Straight Lines [id:da4893220230523627] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]    (487.86,270) -- (515.86,270) ;
		%Straight Lines [id:da06465428636652448] 
		\draw [color={rgb, 255:red, 2; green, 60; blue, 124 }  ,draw opacity=1 ][line width=1.5]  [dash pattern={on 1.69pt off 2.76pt}]  (257.86,95) -- (257.86,111) ;
		%Straight Lines [id:da8084854110424791] 
		\draw [color={rgb, 255:red, 2; green, 60; blue, 124 }  ,draw opacity=1 ][line width=1.5]  [dash pattern={on 1.69pt off 2.76pt}]  (297.86,111) -- (297.86,131) ;
		%Straight Lines [id:da2431881680680168] 
		\draw [color={rgb, 255:red, 2; green, 60; blue, 124 }  ,draw opacity=1 ][line width=1.5]  [dash pattern={on 1.69pt off 2.76pt}]  (320.86,131) -- (320.86,158) ;
		%Straight Lines [id:da49398000189479885] 
		\draw [color={rgb, 255:red, 2; green, 60; blue, 124 }  ,draw opacity=1 ][line width=1.5]  [dash pattern={on 1.69pt off 2.76pt}]  (331.86,158) -- (331.86,192) ;
		%Straight Lines [id:da662876025796717] 
		\draw [color={rgb, 255:red, 2; green, 60; blue, 124 }  ,draw opacity=1 ][line width=1.5]  [dash pattern={on 1.69pt off 2.76pt}]  (342.86,192) -- (342.86,240.28) ;
		%Straight Lines [id:da6493793702754991] 
		\draw [color={rgb, 255:red, 2; green, 60; blue, 124 }  ,draw opacity=1 ][line width=1.5]  [dash pattern={on 1.69pt off 2.76pt}]  (357.86,240.28) -- (357.86,276.28) ;
		%Straight Lines [id:da23409600594330415] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]  [dash pattern={on 1.69pt off 2.76pt}]  (338.8,112.28) -- (338.8,95) ;
		%Straight Lines [id:da9591376418637922] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]  [dash pattern={on 1.69pt off 2.76pt}]  (406.86,137) -- (406.86,112.28) ;
		%Straight Lines [id:da6489220767065538] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]  [dash pattern={on 1.69pt off 2.76pt}]  (446.86,171) -- (446.86,137) ;
		%Straight Lines [id:da8490853401770226] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]  [dash pattern={on 1.69pt off 2.76pt}]  (467.86,212) -- (467.86,171) ;
		%Straight Lines [id:da030434869861333258] 
		\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=1.5]  [dash pattern={on 1.69pt off 2.76pt}]  (488.86,270) -- (488.86,212) ;
		
		% Text Node
		\draw (162,284.4) node [anchor=north west][inner sep=0.75pt]    {$0$};
		% Text Node
		\draw (144,52.4) node [anchor=north west][inner sep=0.75pt]    {$\hat{S}( t)$};
		% Text Node
		\draw (156,86.4) node [anchor=north west][inner sep=0.75pt]    {$1$};
		% Text Node
		\draw (551,277.4) node [anchor=north west][inner sep=0.75pt]    {$t$};
		% Text Node
		\draw (306,193.4) node [anchor=north west][inner sep=0.75pt]    {$\hat{S}_{2}( t)$};
		% Text Node
		\draw (377,83.4) node [anchor=north west][inner sep=0.75pt]    {$\hat{S}_{1}( t)$};
		
		\end{tikzpicture} 
		\vspace*{3mm}
		\caption[]{Cox Typical survival curves (without their confidence intervals)}
	\end{figure}
	In the CMH test, the data are arranged in a series of associated $2\times 2$ contingency tables, the null hypothesis is that the observed response is independent of the treatment used in any $2\times 2$ contingency table. The CMH test's use of associated $2\times 2$ contingency tables increases the ability of the test to detect associations (the power of the test is increased).
	
	In other words, the null hypothesis $H_0$ of the test is that treatment (medical, mechanical or other) has no influence between the control group and the test group in one or multiple stratum (corresponding to different hospitals, cohorts, laboratories, etc.), this is written as:
		
	To introduce this test we create a $2\times 2$ contingency table for each stratum, that can correspond to a time interval $[t_i,t_{i+1}]$ where $i=1,2,\ldots,n$ and that can also be assimilated to a different hospital/laboratory for a same clinical/reliability study, which have typically the following structure:
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|l|c|c|c|}
		\hline
		{\cellcolor[gray]{0.75}}  & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Observed Deaths}}  & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Observed Survivors}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Total}} \\ \hline
		\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Group I (control)} }& $a_i$ & $b_i$ & $L_{1i}$\\ \hline		
		\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Group II (test)}} & $c_i$ & $d_i$ & $L_{2i}$ \\ \hline
		\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Total}} & $C_{1i}$ & $C_{2i}$ & $n_i$ \\ \hline
		\end{tabular}
		\caption{Typical $2\times 2$ contingency table for the CMH test}
	\end{table}
	We speak then of "\NewTerm{general stratified $2\times 2\times n$ table}\index{general stratified $2\times 2\times n$ table}" or of "\NewTerm{contingency table in three dimensions}\index{contingency table in three dimension}"...
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Let us recall that if we have a single table and that we only want to compare the proportion of survivors or deaths for both groups, a test for differences in proportions will be applied as seen earlier above. We can also do a chi-square test if we always have only one single table (see also earlier above) and the ad hoc conditions are met or also a Fisher exact test if the conditions of the chi-square test are not met. This is why in good  statistical software (like Minitab for example), these three tests are available next to each other.
	\end{tcolorbox}
	Thus, under the assumption that all other things remaining equal (H2), and this is the core of this test (!!!!), the expected number $E$ of individuals for each cell in the period $i$ will like as for the exact Fisher test  or the Kohen kappa agreement coefficient be equal to\footnote{$L$ states for "Line" and $C$ for "Column"}:
	
	So it must be well understood that, for example, $E_{a,i}$ then represents the expected number of dead individuals from the control group if it would behaved like this of all individuals of the Control + Test groups. So if the two groups (or survival curves) behave identically, the expected value should be equal to the observed value. To be sure that the reader understand, let us illustrate this concept with a specific example:
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	We consider the following observed values:
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|l|c|c|c|}
		\hline
		{\cellcolor[gray]{0.75}}  & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Observed Deaths}}  & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Observed Survivors}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Total}} \\ \hline
		\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Group I (control)} }& $200$ & $800$ & $1000$\\ \hline		
		\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Group II (test)}} & $280$ & $1120$ & $1400$ \\ \hline
		\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Total}} & $480$ & $1920$ & $2400$ \\ \hline
		\end{tabular}
		\caption[]{Special case of observed values for the CMH test}
	\end{table}
	Here, the expected values gives:
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|l|c|c|c|}
			\hline
			{\cellcolor[gray]{0.75}}  & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Observed Deaths}}  & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Observed Survivors}} & 
	  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Total}} \\ \hline
			\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Group I (control)} }& $200$ & $800$ & $1000$\\ \hline		
			\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Group II (test)}} & $280$ & $1120$ & $1400$ \\ \hline
			\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Total}} & $480$ & $1920$ & $2400$ \\ \hline
			\end{tabular}
		\end{center}
	\end{table}
	\end{tcolorbox}
	We see in the above this particular case, the expected values are equal to the observed one (simply for the reason that the ratio $800/1,000$ and $1,120/1,400$ represent in the table the same percentage - the same proportion - of $80\%$). Obviously, if the proportion of survivors observed for the two groups are all other things remaining equal, it is the same for the observed dead. So what the reader must understand when we do a MCH test is that we are free to choose the column to analyse (because ultimately it is the same!).
	
	Now we can notice that each of the relationships:
	
	represents in fact the mean of a binomial or hypergeometric distribution (see the proofs earlier above during our study of the corresponding laws) since the two laws have the same expression for the mean. However, as the size of individuals in the cells could be significant relatively to the total of rows or columns, the drawing could not be independent. Then we must come back on the hypergeometric law (that is to say the third hypothesis enumerated before).
	
	We also see that by symmetry of the above relation, that the variable of interest is the column attribute or line attribute does not change anything in reality the result of the calculation as (for example):
	
	The variance for each cell will be that of the hypergeometric law we have already proved during it study and that was given by:
	
	with for recall $l=n-k$.
	
	If we adapt the notation of the previous relation to that of our table above, this provides us for all cells (by the analytic form of variance of the hypergeometric law the variance has the same expression for each cell!):
	
	with respectively (for those who want to make the analogy with our detailed study of hypergeometric law):
	
	Here again, we see that whether the variable of interest in either line or column, the calculated value of the variance remains the same!!!
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	A common question that arise frequently for those studying this test is to know why we can not simply do the sum of all stratum (layers) in a single table (because it would be much easier, of course...)? Well we can not put together the tables into a single one because they do not necessarily follow the same law (not identically distributed in the sense that the hypergeometric law has different settings from one table to another) and unfortunately, the hypergeometric distribution is not stable by the addition.
	\end{tcolorbox}	
	And so what...??? How will this help us to compare if the two survival curves are identical through time (or across different hospitals/labs if this is a clinical/reliability test) ?!

	Well let us take as companion example  the table with the following observed values:
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|l|c|c|c|}
		\hline
		{\cellcolor[gray]{0.75}}  & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Observed Deaths}}  & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Observed Survivors}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Total}} \\ \hline
		\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Group I (control)} }& $200$ & $900$ & $1100$\\ \hline		
		\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Group II (test)}} & $280$ & $1120$ & $1400$ \\ \hline
		\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Total}} & $480$ & $2020$ & $2500$ \\ \hline
		\end{tabular}
		\caption[]{Special case of observed values for the CMH test}
	\end{table}
	We then get for the expected values:
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|l|c|c|c|}
		\hline
		{\cellcolor[gray]{0.75}}  & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Observed Deaths}}  & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Observed Survivors}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Total}} \\ \hline
		\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Group I (control)} }& $211.2$ & $888.8$ & $1100$\\ \hline		
		\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Group II (test)}} & $280$ & $1120$ & $1400$ \\ \hline
		\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Total}} & $268.8$ & $1'131.2$ & $2500$ \\ \hline
		\end{tabular}
		\caption[]{Expected values of previous table}
	\end{table}
	Thus the differences:
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|l|c|c|}
		\hline
		{\cellcolor[gray]{0.75}}  & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Observed Deaths}}  & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Observed Survivors}} \\ \hline
		\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Group I (control)} }& $-11.2$ & $11.2$ \\ \hline		
		\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Group II (test)}} & $11.2$ & $-11.2$  \\ \hline
		\end{tabular}
		\caption[]{Difference between expected values and observed values}
	\end{table}
	We understand then better why it has no influence to choose a particular cell to make the test. We simply take the one that we will arrange us the more (depending on what will follow...).
	
	So taking randomly (since the choice does not affect - at least for now .... - the test result as we have shown in our previous calculations) the column Survivors and particularly the observed group Test. Its expected value is then:
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|l|c|}
		\hline
		{\cellcolor[gray]{0.75}}  & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Observed Survivors}} \\ \hline	
		\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Group II (test)}} & $E_{d,1}=\dfrac{L_{21}C_{21}}{n_1}=\dfrac{1400\cdot 2020}{2500}=1131.2$   \\ \hline
		\end{tabular}
		\caption[]{Expected value of the Tests-Survivor cell}
	\end{table}
	The difference between observed and expected gives:
	
	That difference will be obviously zero if the observed was equal to the expected! With for variance:
	
	Now, under the conditions:
	
	as seen during our study of the hypergeometric law, we can approximate it by a Normal law (hypothesis H4).

	So in our case, this approximation is not acceptable (the third condition is disqualifying) and so the test can not be performed (usually we use to take in the table the column and the line that allow this approximation since the choice does not influence on the value of the test but on the authorization to use the mentioned approximation!). But if it had been, we could approximate the hypergeometric distribution by a Normal distribution such that:
	
	Which can be obviously reduced to a centered reduced Normal distribution if we take for the cell of our table:
	
	And therefore it is sufficient for a stratum (layer) $i$ to know if we are outside of the confidence interval that we have set. But ... we have several strata (layers)! The idea then is to sum on the $T$ independent strata (then we fall back on the hypothesis H1) such that:
	
	which was not quite practical at the time when not everyone had a computer since only $\mathcal{N}(0,1)$ tables we available. This is why we prefer to do the following sum:
	
	Or in condensed form (again regardless of the choice of the cell!):
	
	And if the differences between observed and expected is not too great across all strata, the value of this expression well be found within a certain confidence interval of the Normal distribution. If it is outside, the hypothesis of equality of the two survival curves will be rejected.
	
	However, the majority of statistical softwares take the square of that relation.

	It comes then that the square follows a chi-squared law with one degree of freedom (we have proved this earlier in this section) such that we fall back on the final form such that the log-rank test  can be found in many books:
	
	So this is a Wald statistic (see earlier above) and therefore the CMH test belongs to the family of Wald tests\index{Wald's test}!

	For practical reasons, we add a $0.5$ term to the sum, this provides a better approximation to the Normal distribution. Therefore we can see sometimes in books:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us consider the following observed values of two hospitals $A$ and $B$ for a clinical trial:
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|l|c|c|c|}
		\hline
		{\cellcolor[gray]{0.75}}$A$  & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Observed Deaths}}  & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Observed Survivors}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Total}} \\ \hline
		\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Group I (control)} }& $288$ & $4$ & $292$\\ \hline		
		\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Group II (test)}} & $400$ & $50$ & $450$ \\ \hline
		\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Total}} & $688$ & $54$ & $742$ \\ \hline
		\end{tabular}
	\end{table}
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|l|c|c|c|}
		\hline
		{\cellcolor[gray]{0.75}}$B$  & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Observed Deaths}}  & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Observed Survivors}} & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Total}} \\ \hline
		\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Group I (control)} }& $300$ & $10$ & $310$\\ \hline		
		\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Group II (test)}} & $450$ & $40$ & $490$ \\ \hline
		\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Total}} & $750$ & $50$ & $800$ \\ \hline
		\end{tabular}
	\end{table}
	We don't want to know if the two hospitals are different or not (this is not the purpose!), but whether the differences between the control group and test group across all hospitals is significantly different or not.\\
	
	Well we already intuitively guess the result when we see the values ... but still let us do the calculations.\\

We are already seeing that for hospital $A$, the column Survivors meets the three conditions for the approximation by a Normal distribution (which is not the case for the column of the Deaths):
	
	Similarly for the hospital $B$ (and this is fortunate because once a column selected in one stratum, it requires that the approximation condition is applicable to the same column of all other strata!!!):
	
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|l|c|c|}
		\hline
		{\cellcolor[gray]{0.75}}$A$  & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Observed Survivors}}  & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Total}} \\ \hline
		\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Group I (control)} }& $21.25$ & $292$ \\ \hline		
		\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Group II (test)}} & $32.75$ & $450$ \\ \hline
		\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Total}} & $54$ & $742$ \\ \hline
		\end{tabular}
	\end{table}
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|l|c|c|}
		\hline
		{\cellcolor[gray]{0.75}}$B$  & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Observed Survivors}}  & 
  \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Total}} \\ \hline
		\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Group I (control)} }& $19.38$ & $310$ \\ \hline		
		\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Group II (test)}} & $30.62$ & $490$ \\ \hline
		\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Total}} & $50$ & $800$ \\ \hline
		\end{tabular}
	\end{table}
	Which gives for the differences:
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|l|c|}
		\hline
		{\cellcolor[gray]{0.75}}$A$  & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Observed Survivors}}   \\ \hline
		\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Group I (control)} }& $-17.25$ \\ \hline		
		\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Group II (test)}} & $17.25$ \\ \hline
		\end{tabular}
	\end{table}
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|l|c|}
		\hline
		{\cellcolor[gray]{0.75}}$B$  & \multicolumn{1}{c}{\cellcolor[gray]{0.75}\textbf{Observed Survivors}}  \\ \hline
		\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Group I (control)} }& $-9.38$  \\ \hline		
		\multicolumn{1}{l}{\cellcolor[gray]{0.75}\textbf{Group II (test)}} & $9.38$ \\ \hline
		\end{tabular}
	\end{table}
	We then quickly see why once the column chosen, the choice of the line does not matter anymore (either a sum of negative values or a sum of positive values and as we take the square of the sum, it changes nothing finally!). Let us arbitrarily choose the second line. Then we have:
	
	And we have with a spreadsheet software like Microsoft Excel 14.0.6123 in left-sided with a threshold risk of $5\%$:

	\begin{center}
		$\chi_{95\%}^2(1)$\texttt{=CHISQ.INV(95\%,1)}$\cong 3.841$
	\end{center}
	So on the cumul of the two hospitals (strata) the Control group is significantly different from the Test group. The $p$-value of the test is typically given with Microsoft Excel 14.0.6123:
	\begin{center}
		\texttt{=1-CHISQ.DIST(31.845,1,1)}$\cong 0.000002 \%$
	\end{center}
	\end{tcolorbox}

	Now let us quote a famous situation\footnote{Dr. E. E. Peacock, Jr., quoted in Medical World News (September 1, 11972 according to holocene calendar), p. 45, as quoted in Tufte's 11974 (holocene calendar) book \textit{Data Analysis for Politics and Policy}.}:
	
	«One day when I was a junior medical student, a very important Boston surgeon visited the school and delivered a great treatise on a large number of patients who had undergone successful operations for vascular reconstruction.

	At the end of the lecture, a young student at the back of the room timidly asked, “\textit{Do you have any controls?}” Well, the great surgeon drew himself up to his full height, hit the desk, and said, “\textit{Do you mean did I not operate on half the patients?}” The hall grew very quiet then. The voice at the back of the room very hesitantly replied, “\textit{Yes, that’s what I had in mind.}” Then the visitor’s fist really came down as he thundered, “\textit{Of course not. That would have doomed half of them to their death.}”

	God, it was quiet then, and one could scarcely hear the small voice ask, “\textit{Which half?}”»
	
	\subsubsection{Cox (Proportional Hazard) Model}\label{Cox proportional hazard model}
	The "\NewTerm{Cox proportional hazard model}\index{Cox proportional hazard model}" (not to be confused with "Box-Cox transformations") in continuous time, also known as the "\NewTerm{proportional hazards semi-parametric model}\index{proportional hazards semi-parametric model}" or in abbreviated version "\NewTerm{Cox PHM}" or also "\NewTerm{Cox regression model}\index{Cox regression model}" (RCM), is a model of risk ratio analysis. It expresses the ratio of the instantaneous risk, the "\NewTerm{hazard rate}\index{hazard rate}", to know the (risky) event studied after a given duration of exposure according to a linear combination of explanatory factors. This model is used extensively in clinical studies, as well as in preventive maintenance and bank credit risk.
	
	This model belongs as the Kaplan-Meier model to the "duration models" or "survival models" and makes use of some of the results demonstrated in the section of Industrial Engineering as part of the study of preventive maintenance (see page \pageref{Kaplan-Meier survival model}).
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The Kaplan-Meier model makes it possible to compare several curves of survival, the Cox's model allows not only to do the same but also to be able to do so on the basis of multiple factors and also to make predictions for values not observed during the experiment.
	\end{tcolorbox}
	
	For the sake of simplification and essentially pedagogical, we will be interested here only in the case where the data are uncensored, the covariates without interactions and themselves independent of time (for example, the covariate "age" would be independent ... of the time...). We will make the presentation more complex as this book evolves.
	
	First of all it is necessary to know that the Cox regression model is, as one of its other names indicates it, based on the instantaneous hazard rate (risk) $h(t)$! Why this choice when we could take the survival or reliability functions ??? Simply because the survival functions $R(t)$ and reliability $F(t)$ have constraints that would complicate the construction of the regression model. Whereas the only thing we impose on the hazard rate is that it is between $[0,+\infty[$.
	
	The goal of the Cox model is to be able to quantify and test the effects of individual characteristics such as gender, level of education, social class, nationality, past experience, vaccination, etc. on transition risks (life/death or sick/not sick, etc.).
	
	The basic idea is to be able to say "the risk rate of such group is $X$ times greater than the risk rate of the other group" or in other words "the risk of such a group is in a proportion of $X$ relative to such another group" and a naive first approach and mathematically easy way to manage this is to say that the risk rate is of the form (in the univariate case\footnote{So a first assumption is that we assume continuous covariates have a linear form and this should always be checked}):
	
	where $x$ is an explanatory variable most often categorical (and rather of the binary type) and where $\beta_0$ is therefore in extenso the risk rate corresponding to $x=0$.
	
	Unlike the "relative risk" (i.e. risk ratio) or "odds ratio", it is a model that is more predictive and is adapted to more than two variables (and not just binary!).
	
	However it is a bad design for the moment for the simple reason that the definition domain of $h(x)$ is in actually not limited to $[0,+\infty[$. An idea would be to do as for the logistic model, take the exponential such that:
	
	Obviously, we can have more than one explanatory variable. From then on, as we are already know it, the model would be written:
	
	where $h_0$ is obviously the baseline risk when all covariates are zero. In practice, we will see a little further that the determination of this basic risk is of no interest because it is eliminated. This last relation is sometimes written in the following form to highlight the risk part of the characteristic factors of the individuals studied:
	
	However, we must consider that the hazard rate is time dependent, as we saw earlier in our study of maintenance techniques, such that we have to define the "\NewTerm{instantaneous risk}":
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Therefore the standard situation of the application of this survival method in clinical research projects assumes that a homogeneous population is investigated when subject under different conditions (e.g. experimental treatment and standard treatment). The survival model then assumes that the survival data of the different patients are independent form each other and that each patient's individual survival time distribution is the same (independent and identically distributed failure times)!\\
	
	However, in the field of clinical trials one observes in many most practical situations that patients differ substantially. The effect of a drug, a treatment or the influence of various explanatory variables may differ greatly between subgroups of patients. To account for such unobserved heterogeneity in the study population Vaupel et al. (11979 according to holocene calendar) introduced "\NewTerm{univariate frailty models}" into survival analysis (available in R). The key idea is, that individuals possess different frailties, and that those patients who are most frail will die earlier than the others. Consequently, systematic selection of robust individuals (that means patients with low frailty) takes place. We will not introduce that mathematical model in this book!
	\end{tcolorbox}
	It is important to note that in a simple case we have:
	
	that is why the Cox model always has the $y$-intercept (i.e. $\beta_0$) that is zero because it is embedded in the fundamental hazard $h_0(t,\vec{\alpha})$.
	
	In reality, if we take a step back, we could see in the previous expression an analogy with the approach by the method of separation of variables as it is done quite often when solving certain differential equations where the solution is constructed such that each factor depends only on certain parameters.
	
	What is particularly appreciated by the practitioners in this empirical model and which explain the origin of its name is that we do not need to know $h_0(t,\vec{\alpha})$. Indeed, if we consider the ratio of the same model but for different values of covariates at the same time, we have the "\NewTerm{hazard ratio}\index{hazard ratio}":
	
	Most of time the ratio is of the following type:
	
	We see therefore that an assumption of this approach is that the two models are assumed to have the same initial properties (assumption often hidden in clinical studies)!!
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. For a single dichotomous covariate, say with values $0$ and $1$, the hazard ratio is:
	
	Thus, the relative risk of two individuals with different covariate values should be independent of time or constant at all times. This is an inherent assumption of the Cox model (and any other proportional hazards model).\\
	
	E2. With two covariates we have:
	
	Consider now for example that we keep $x_1$ fixed such that $x_1^{\prime}=x_1$ and that we take $x_2^{\prime}=x_2+1$ ($x_2$ may be an integer value like the rounded age of the patient for example). Then the previous relation will be written:
	
	And notice that whatever the value of $x_2$ (and keeping fixed the other covariates), each incrementation of this covariate by $+1$ will multiply the instantaneous risk by a factor $e^\beta$ (this value is often written in percentage obviously).
	\end{tcolorbox}
	The consensus is to read the values of the hazard ratio as following:
	\begin{itemize}
		\item If $\text{H.R.}=0.5$: At any particular time, half as many patients in the treatment group are experiencing an event compared to the control group.

		\item If $\text{H.R.}=1.0$: At any particular time, event rates are the same in both groups.

		\item If $\text{H.R.}=2.0$: At any particular time, twice as many patients in the treatment group are experiencing an event compared to the control group.
	\end{itemize}
	Assuming that the previous assumption is satisfied, it is thus very interesting in the practice of "having fun" to vary the characteristics of individuals in order to calculate how the difference $(\vec{x}_i-\vec{x}_j)$ increases or decreases the risk ratio in percentages (this is where lies the power of investigation and the fun of the Cox model!).
	
	So not only is the basic risk eliminated when we look at relative risk (this is by assumption!!!), but time dependence too! 

	So the fundamental assumption of the Cox model, as already mentioned, is that we assume for different groups that the basic hazard is equal!!! This assumption of proportionality is trivially visible when we use qualitatively survival graphs like those shown later below with two groups\footnote{Population under study is considered as consisting of two sub-populations with different risks.} (i.e. two stratas) and we see that they start at the same point at $t=0$ (otherwise we can use other models likes the Threshold Regression Models).
	
	So for recall, the assumption so far are:
	\begin{enumerate}
		\item[H1.] The model is multiplicative (i.e. the link between $h(t)$ and $h_0(t)$ must be of the form $h(t)=c^{te}h_0(t)$)
		
		\item[H2.] We assume continuous covariates have a linear form
	
		\item[H3.] We assume the covariates to be time independent (hence proportional hazards must hold!)
	
		\item[H4.] We assume the initial risk rate to be equal
	\end{enumerate}
	
	 A classical test for time independence is to transform the survival curves by taking:
	
	and check if the both resulting linearised curve a parallel. Indeed, let us recall that we have\footnote{We just change the notation corresponding to that of the medical tradition, with $S(t)=R(t)$ and $h(u)=\lambda(u)$} (\SeeChapter{see section Industrial Engineering page \pageref{reliablity hazard failue rate}}):
	
	And the Cox hypothesis is:
	
	Therefore:
	
	Taking the natural logarithm:
	
	and once again:
	
	Therefore for two sets of observations:
	
	Then space between the natural logarithm of the two curves should be constant over time!
	 \begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In practice a common test to check the assumption of proportionality is by using the "Schoenfeld residuals" against the transformed time. 
	\end{tcolorbox}
	So let us see some example of two strates of Cox PH model regression to make some qualitative judgement based on the plots:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/arithmetics/cox_ph_regression_qualitative_analysis_00.jpg}
	\end{figure}
	The above plot (regression) violates the following assumptions:
	\begin{enumerate}
		\item The distance between the curves are surely not constant as the curves cross
		
		\item We cannot judge of the underlying Normality of the data
		
		\item The blue curve seem not be based on a linear model (that we take the natural logarithm or not!)
		
		\item Surely the hazard risk ratio is equal at the beginning (this is quite bad) but even worst... there seem on the long term to no have any asymptotic constant hazard risk ratio
	\end{enumerate}
	The plot below is a bit better relatively to the respect of the assumption of the model:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/arithmetics/cox_ph_regression_qualitative_analysis_01.jpg}
	\end{figure}
	Indeed the curves don't cross anymore. But sadly the distance doesn't seems to be constant.
	
	The next regression is better than the two previous one (seems to have an asymptotic constant distance\footnote{We think that the reader understand now that if we can by vertical translation superpose perfectly the both regression than hypothesis H3 holds!}):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/arithmetics/cox_ph_regression_qualitative_analysis_02.jpg}
	\end{figure}
	Well it is nice to be able to compare logically the ratio of the hazard rate at the same time and for different values of covariates but how do determine the coefficients of the model??? A first reflex would be to take the natural logarithm of the model to make a simple linear regression:
	
	But we see immediately that the time component will be problematic! Then comes the second reflex: work with maximum likelihood (see above page \pageref{likelihood estimators})! But then a question arises ...: the hazard function is not strictly speaking a probability then what will we maximize ???

	Well for this we are going to use conditional probabilities (see above page \pageref{bayesian inference}), even if at first sight it is not easy to see what connects conditional probabilities to the hazard\footnote{The term "\NewTerm{hazard}\index{hazard}" refers to the potential source of harm, danger, or adverse event, while "probability" is the likelihood of that harm or adverse event occurring.} function but wait a little, denoting by $x_{i}$ the particular character that identifies a certain class of individuals.
	
	Consider for this $E$, the set of elements including individuals that survived $S$ and those do not survived denoted $\bar{S}$. Such that for a given identical time:
	
	such that:
	
	as well by construction:
	
	Since the intersection is empty it will be difficult to reveal the hazard function we may think ... Well that's where a trick comes!
	
	It is true that if we take the set $\bar{S}$ of those who have not survived always at the same instant that those who have survived $S$ the two sets will always be disjoint, such as by involving the variable time, we have:
	
	On the other hand, if we mix the times, we have for example:
	
	which is not necessarily empty anymore since they can overlap
partially when taken at different times!

	Since there is a superposition of the two sets, when taken at different times, we can now cleverly write a conditional probability:
	
	Assuming that the number of survivors decrease, that latter can we written:
	
	Or in everyday language:
	
	or say differently:
	
	However, we recognize here in the numerator, if we take again the usual notations preventive maintenance (\SeeChapter{see section Industrial Engineering \pageref{preventive maintenance}}), $\mathrm{d}F(t)$ the cumulative failure function! Therefore:
	
	and we recognize $R(t)$, the reliability function, at the denominator such that:
	
	Hence:
	
	And we recognize here a ratio that we already see during our study of preventive maintenance !!! So we can write:
	
	We might think that it would be possible to use this conditional probability to apply the maximum likelihood technique, but ... the problem is that there is implicitly in $h(t)$, a function dependent of time that we have to eliminate in order to do something. From then on, the idea is to make a ratio of the hazard functions for the same given instant so the temporal part will be eliminated! And who says ratio of probabilities, says again conditional probability!

	Let us recall that we have just got:
	
	That is the conditional probability that an individual without any special characteristics does not survive during the given time slot while all the others (having also no special characteristics either) as well as himself survived until time $t+\Delta t$. Now, if we introduce the characteristics, then we have the possibility to introduce another layer of conditional probability. For this, let us take the special case but quite easily generalizable of a single special characteristic (covariate) and an event of interest on an individual denoted $i$ among all the other individuals $j$ (including the individual $i$) that are also tracked with this same characteristics. The previous conditional probability can then be written:
	
	Now, let's compare this conditional probability of a particular individual conditionally to all other individuals (including $i$ himself for recall!) with all the same characteristics:
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The attentive reader may have noticed that the model as constructed for the moment assumes that non-survivors are unique for a given value of the characteristic. This is why we are talking about: "Cox proportional hazard regression model, ungrouped, uncensored and time-independent covariates". However, we will see below how to deal with the case where two individuals with different or equal characteristics do not survive at the same time.
	\end{tcolorbox}
	But in the particular case which concerns us $\{\bar{S}_{t+\Delta t}/S_t\}_i\in \{\bar{S}_{t+\Delta t}/S_t\}_j$. We then have:
	
	But that we can also write more explicitly (don't forget that $j$ represents a set of individuals different from $i$) as each individual event related is assumed as a disjoint probability (\SeeChapter{see section Probabilities page \pageref{disjoint probability}}):
	
	But the reader must keep in mind two things:
	\begin{enumerate}
		\item The arrow of time is oriented
		\item Hence the events are ordered!
	\end{enumerate}
	What does that mean, or what are the implications? Just that the event related to an individual $i$ can occur only before some other event have maybe occur to other individual with a rank index small than $i$. That  means that we should write instead:
	
	or more explicitly:
	
	This relation is often written as following in textbooks:
	
	and is the "risk probability" of individual $i$ at time point $t_i$, relatively to covariate $x$ (unique covariate in the special case!), where individual events are disjoints with proportional initial hazard and where $\mathcal{R}(t_i)$ is the set of all individuals surviving or functioning at time $t_i$ (risk set at $t_i$).
	
	The last relation explains the probability we will use to maximize likelihood. Since time-dependent basis hazard does not appear, it is customary to speak of "\NewTerm{partial likelihood}\index{partial likelihood}" of $n$ individuals\footnote{Because we are suppressing the actual times of events even though they are known we use the name "partial likelihood"}:
	
	Or in some textbooks it is written (including the case of multiple covariates and possible censored data):
	
	where the power $\delta_i$ means the individuals having the events (failure/death) actually contribute to the likelihood, but not the right-censored cases.
	
	We can think of the partial likelihood as the joint density function for subjects' ranks in terms of event order, if there were no censoring and no tied event times.
	
	Since likelihood can only be maximized with a computer (Newton's method), it is customary not to go further by taking the natural logarithm (but it would not change the result!). But in practice the coefficient are however estimated by minimizing the negative of the log-likelihood:
	
	This writing is still not easy to use in practice. For this reason, we will make a numerical example.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	To see how, consider the data below with a single covariate (deaths in clinical study in weeks after the start of the test based on BMI\footnote{Body Mass Index}):
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		\multicolumn{1}{|l|}{\cellcolor[gray]{0.75}\textbf{$\pmb{i}$}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{$\pmb{t_i}$ {[}weeks{]}}} & \multicolumn{1}{l|}{\cellcolor[gray]{0.75}\textbf{BMI $\pmb{x_i}$}} \\ \hline
		$1$ & $6$ & $31.4$ \\ \hline
		$2$ & $98$ & $21.5$ \\ \hline
		$3$ & $189$ & $27.1$ \\ \hline
		$4$ & $374$ & $22.7$ \\ \hline
		\end{tabular}
	\end{table}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Since there are in this small example obviously no qualitative variables, we are talking about "non-stratified time-dependent Cox proportional hazard model".
	\end{tcolorbox}
	There are therefore four events with the respective data of BMI $ 31.4 $, $ 21.4 $, $ 27.1 $ and $ 22.7 $. As we have seen, time no longer intervenes but only the rank $ i $, that is, the order in which the events of interest take place.\\
	
	It then corresponds four conditional probabilities which we will denote for the sake of simplification of writing $P_1$, $P_2$, $P_3$, $P_4$. Therefore:
	
	We then have the partial likelihood that is given by:
	
	And a numerical application that maximizes this with the solver of the spreadsheet software Microsoft Excel  (or other spreadsheet or tool) immediately gives:
	
	This is consistent with what returns statistics software like MedCalc 12.7.5 or \texttt{R} (see the example in the corresponding companion book)! This gives us:
	
	From there we can easily compare at a given moment the ratio of chance between two individuals with different characteristics.\\
	
	We can also give the corresponding survival plot (here made with \texttt{R} and including the confidence interval that we will see further below):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.65]{img/arithmetics/cox_ph_r_application_confidence_interval.jpg}
		\caption{Cox PH typical survival curves in \texttt{R} software}
	\end{figure}
	\end{tcolorbox}
	We notice after this short overview that at no moment we needed to know or assume the underlying distribution law (Weibull, Exponential or other) and their associated parameters. The Cox model is therefore a nonparametric technique.
	
	It exists (although at the base it is not the goal of the Cox model), estimators of $h_0(t)$, the best known of which is the Breslow estimator, but it is considered unreliable (it would have been proved it is inconsistent) and applicable only for the cumulative rate of hazard and not the instantaneous rate (so beware of softwares that give $h_0(t)$....). And the interest of the Cox model is to be interested in the risk ratio and not in estimating the complete survival/reliability function hazard (Kaplan-Meier is then better adapted). So actually it is not planned to have a detailed introduction of this estimators (and some other one) in this book!
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	In the \texttt{R} software, the function \texttt{basehaz()} defaults to the Nelson-Aalen estimate of the cumulative hazard with a Breslow-type estimate of survival. Alternatively, a Kaplan-Meier estimator for the cumulative hazard can be selected.
	\end{tcolorbox}
	
	Let us now discuss a bit about simultaneous events! In principle, the durations are supposed to be the realizations of a continuous variable and the covariates also (that continuous underlying variable being the time obviously), the probability that two or more individuals know the event at the same moment is theoretically null. In practice, the data are often recorded according to a certain time division (weekly, monthly data, etc.) so that it is common for the same duration of survival to characterize several different individuals.
	
	Basically this does not seriously complicate the previous writing, simply the number of calculations can become such if one uses the exact contribution of these individuals to partial likelihood that approximations have been proposed (available all three in SAS software). However as part of this book we will focus on the exact version!
	
	So suppose that for a certain time, two individuals have known the event interest and that at the same time, three other individuals are also at risk. If we start by treating the case of the first individual, then there are $5$ individuals at risk. When we treat the case of the second individual, only $4$ individuals are risky since the first one has already been taken into account. In this case, the probability of having both events simultaneously is consistent with that of $2$ non-incompatible but independent events (\SeeChapter{see section Probabilities page \pageref{disjoint probability}}):
	
	Naturally, the fact of starting with the first individual is arbitrary since one can just as easily admit to starting with the second. In total, the contribution of this duration is two individuals at partial likelihood then is:
	
	Let us note (for those who may have a doubt that it is always smaller or equal to unity as required by the axiom of probabilities) than in the case where we have a population with two individuals this reduces to:
	
	which is consistent with the axiom of certain event in the field of probabilities! And when the population is larger, it is immediate that the denominator of each term will also larger, the total probability is then still less or equal to unity!

	If we repeat the manoeuvre with three individuals simultaneously, we will have $6$ terms that will add up. More generally, if there are $k$ individuals among $n$ which lives the event of interest at the same time, the calculation of the contribution involves $k!$ terms. The calculation time can therefore become very penalizing, for example with $k$ worth $10$, we have $3,628,800$ terms...
	
	Now as for the Kaplan-Meier survival estimator, let us see how we can construct the confidence intervals for the hazard ratio.

	As earlier we will use again the univariate Delta method (\SeeChapter{see section Numerical Methods page \pageref{delta method}}) to get standard error for $e^{\hat{\beta}}$. Let us recall that this method gives us that:
	
	This gives immediately for the variance:
	
	Hence assuming that the $\beta$ are Normally distributed (and is a scalar...):
	
	The problem here is that we can have the lower bound of $e^{\beta}$ that can be negative...
	
	Hence we prefer to use:
	
	If we consider individuals that are identical apart from one covariate $x$, the hazard ratio becomes as we already know:
	
	The confidence interval of the hazard ratio is therefore:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners,breakable]
	\textbf{{\Large \ding{45}}Example:}\\\\
	A Cox proportional hazard model was used to model the survival times of cancer patients. Tumour size (in [mm]) was included as a covariate, with coefficient $\beta$. The maximum likelihood estimate of $\beta$ was $\hat{\beta}= 0.0176$ with standard error $0.004$.\\

	We want to calculate an estimate of the hazard ratio between two individuals with tumours measuring $46$ [mm] and $37$ [mm] who are identical in other ways and construct a $95\%$ confidence interval for the hazard ratio.\\
	
	We have for hazard ratio:
	
	Using the estimate:
	
	The $95\%$ confidence interval for $\beta$ is:
	
	That is:
	
	The $95\%$ confidence interval for hazard ratio is therefore:
	
	\end{tcolorbox}
	
	\paragraph{Stratified Cox Model}\index{stratified Cox model}\mbox{}\\\\
	Suppose the observations of a variable define sub-categories of the initial population. An obvious example is given by a sex covariate coded $0$ or $1$.

	Suppose again that one is not interested in a direct measure of the impact of this variable on risk or that one thinks that this variable does not verify the proportional risks hypothesis so that we refuse its inclusion in the list of covariates (in other words, instead of assuming that the \textit{sex} binary covariate on the survival risk is constant in time and multiplicative on the risk, we can assume that it is the basis risk that is different by men and women and therefore we have two different stratum!).
	
	It is possible, however, that we want to take into account an effect of the sex of the individuals to measure the impact of the other variables. The solution as already mentioned in the parenthesis before is then to stratify. In this case the explanatory coefficients are constrained to be identical in each of the strata but a partial likelihood function is constructed separately on each of them, the function that is maximized to find the parameter estimators being the product of these partial likelihoods. As a result, obviously, the basic risk differs between strata. For example, instead of having (special simple case with two covariates):
	
	where $x_2$ would be the \textit{sex} covariate, we have rather have the following system:
	
	In summary:
	
	Thus, for two individuals of the same stratum we will have more chance to have a proportional hazard (parallel survival curves) as the ratio cancels the baseline hazard. On the other hand, it is obvious that the comparison of two individuals of different strata will have very little chance of being independent of time since the ratio of basic hazards will not cancel each other out.
	
	However if they cancel each other out, we have then a kind of an evidence that their are proportional and that it is reasonable to model the stratification term in the regular manner!
	
	Therefore "\NewTerm{stratified Cox models}":
	
	where $h_{0k}$ is the  baseline  hazard  for stratum $k$ are a useful extension of the standard Cox models (where for recall $h_0$ is common to all the individuals in a study):
	
	to allow for covariates with non-proportional hazards.
	
	The partial likelihood that we have introduce earlier:
	
	is computed for stratified model as:
	
	Finally, in the case of stratification, since the responsible variable of stratification is not explicitly measured, we can not use it as an explanatory variable.
	
	So what we can summarize so far? We have seen three kinds of models with for each at least one special mathematical model (but there are much more in reality when you become an expert in the field as we have already shown it in a figure earlier summarizing all most famous survival analysis methods!):
	\begin{itemize}
		\item Parametric models: Where the baseline hazard and covariates are all assumed to have the same distributions (e.g. Weibull or Exponential models most of the time and this is typically the case of the Threshold Regression Models).
	
		\item Nonparametric models: Where the baseline hazard and the covariates have no specific distribution assumed (e.g. Kaplan-Meier model).
	
		\item Semi-Parametric models: Where the baseline hazard has no special distribution assumed but the covariates have some distribution assumed (e.g. Cox proportional hazard model).
	\end{itemize}
	
	\pagebreak
	\subsection{Propagation of Errors (experimental uncertainty analysis)}\label{propagation of errors}
	It is impossible to know (measure) the exact value of a physical quantity experimentally, it is very important therefore to determine its uncertainty.
	
	We obviously name "\NewTerm{error}\index{error}", the difference between the measured value and the exact value. However, since we do not know the exact value, we can not know the error anyway .... The result is still uncertain. That is why we speak of "\NewTerm{measurement uncertainty}\index{measurement uncertainty}".
	
	We distinguish two main types of uncertainty:
	\begin{enumerate}
		\item The "\NewTerm{systematic errors}\index{systematic error}": they affect the result and this constantly in the same direction (errors of measurement devices, accuracy limits, etc.). It must then be eliminate, or correct the outcome, if possible!

		\item The (statistical) "\NewTerm{accidental errors}\index{accidental error}": we must the repeat the measurements and calculate the average estimate uncertainty using statistical techniques.
	\end{enumerate}
	
	The second type of error makes a very big use of all statistical tools we have presented so far. So we will not repeat them here and then we will focus only on a few new concepts.
	
	\begin{figure}[H]
		\centering
		\includegraphics{img/arithmetics/errors_propagation.jpg}
		\caption[]{Errors propagation (source: xkcd)}
	\end{figure}
	
	\subsubsection{Absolute and Relative Uncertainties (direct calculation of bias)}
	If the true value of a variable is $x$ (theoretically supposed to be known) and the measured value $x_0$, then $\delta x_0$ is the "\NewTerm{absolute uncertainty}\index{absolute uncertainty}" (uncertainty due to measurement devices) or "\NewTerm{absolute error}\index{absolute error}".

	The interval of fluctuation is therefore denoted by:
	
	or:
	
	The "\NewTerm{uncertainty}\index{uncertainty}" or "\NewTerm{relative error}\index{relative error}" is itself defined by:
	
	The absolute uncertainty is used to find an approximation of the last significant digit thereof. By cons, when we want to compare two measurements having absolute uncertainties to identify which was the largest margin of error, we calculate the relative uncertainty of this number by dividing the absolute uncertainty by the measurement itself, and transform the result typically in percentage.
	
	In other words, the relative uncertainty gives an idea of the accuracy of the measurement in \%. If we make a measurement with an absolute uncertainty of $1$ [mm], we will not know if this is a good measure or not. It depends if we measured the size of a coin, of our neighbour, or of the Paris-Marseille distance of the Earth-Moon distance. In short, it depends on the relative uncertainty (that is to say the ratio of the absolute uncertainty and of the measurement).
	
	Therefore in the case of a law of the type $f(x,y,z,t)$ for which we seek the error we would calculate the following (one time with everything positive and another with all terms negative):
	
	This method is quite boring as it must be calculated for each value $(x,y,z,t)$.
	
	\subsubsection{Statistical Errors}
	In most measurement, we can estimate the error due to random phenomena, named "\NewTerm{random error}\index{random error}", by a series of $n$ measurements $x_1,x_2,\ldots ,x_i, \ldots , x_n$ and this opposed to the "\NewTerm{systematic error}\index{systematic error}" which is the not  random part of the error.
	
	Random error allows to introduce the concepts of:
	\begin{itemize}
		\item Repeatability: This is defined as the closeness of agreement between the results of successive measurements of a same item, made with the same method, by the same operator with the same measuring instruments, in the same laboratory (conditions), and in rather short time intervals  (see a little further below a little bit more rigorous definition in line with international standards).
		
		\item Reproducibility (sometimes named a "rightness"): which is defined as the closeness of agreement between the results of successive measurements of the same quantity, in the case where individual measurements are made: by different methods, using different instruments extent by different operators in different laboratories!
	\end{itemize}
	These two type of errors are almost always grouped under the labels "\NewTerm{R\&R}" or "\NewTerm{R\&R Study}\index{R\&R studies}" in the industry. In general, the agreement is less good when it comes to reproducibility.
	
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	There are softwares running two-way fixed factor ANOVA with repetition as Minitab that generate very detailed reports for R\&R analysis.
	\end{tcolorbox}	
	These two types of errors can be illustrated by the target shooting in a more general way:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,891); %set diagram left start at 0, and has height of 891
		
		%Shape: Circle [id:dp5799479532281786] 
		\draw  [fill={rgb, 255:red, 106; green, 170; blue, 250 }  ,fill opacity=1 ] (170.24,149.09) .. controls (170.24,122.22) and (192.03,100.43) .. (218.91,100.43) .. controls (245.78,100.43) and (267.57,122.22) .. (267.57,149.09) .. controls (267.57,175.97) and (245.78,197.76) .. (218.91,197.76) .. controls (192.03,197.76) and (170.24,175.97) .. (170.24,149.09) -- cycle ;
		%Shape: Circle [id:dp1027099820231232] 
		\draw  [fill={rgb, 255:red, 255; green, 80; blue, 99 }  ,fill opacity=1 ] (186.31,149.09) .. controls (186.31,131.09) and (200.91,116.5) .. (218.91,116.5) .. controls (236.91,116.5) and (251.5,131.09) .. (251.5,149.09) .. controls (251.5,167.09) and (236.91,181.69) .. (218.91,181.69) .. controls (200.91,181.69) and (186.31,167.09) .. (186.31,149.09) -- cycle ;
		%Shape: Circle [id:dp6417616993241322] 
		\draw  [fill={rgb, 255:red, 245; green, 166; blue, 35 }  ,fill opacity=1 ] (203.81,149.09) .. controls (203.81,140.76) and (210.57,134) .. (218.91,134) .. controls (227.24,134) and (234,140.76) .. (234,149.09) .. controls (234,157.43) and (227.24,164.19) .. (218.91,164.19) .. controls (210.57,164.19) and (203.81,157.43) .. (203.81,149.09) -- cycle ;
		%Shape: Circle [id:dp6798640633972899] 
		\draw   (153.63,149.09) .. controls (153.63,113.04) and (182.86,83.82) .. (218.91,83.82) .. controls (254.96,83.82) and (284.18,113.04) .. (284.18,149.09) .. controls (284.18,185.14) and (254.96,214.37) .. (218.91,214.37) .. controls (182.86,214.37) and (153.63,185.14) .. (153.63,149.09) -- cycle ;
		%Shape: Star [id:dp4860131896529385] 
		\draw  [color={rgb, 255:red, 238; green, 238; blue, 238 }  ,draw opacity=1 ] (218.91,83.82) -- (218.91,149.09) -- (239.08,87.01) -- (218.91,149.09) -- (257.27,96.29) -- (218.91,149.09) -- (271.71,110.73) -- (218.91,149.09) -- (280.99,128.92) -- (218.91,149.09) -- (284.18,149.09) -- (218.91,149.09) -- (280.99,169.26) -- (218.91,149.09) -- (271.71,187.46) -- (218.91,149.09) -- (257.27,201.9) -- (218.91,149.09) -- (239.08,211.17) -- (218.91,149.09) -- (218.91,214.37) -- (218.91,149.09) -- (198.74,211.17) -- (218.91,149.09) -- (180.54,201.9) -- (218.91,149.09) -- (166.1,187.46) -- (218.91,149.09) -- (156.83,169.26) -- (218.91,149.09) -- (153.63,149.09) -- (218.91,149.09) -- (156.83,128.92) -- (218.91,149.09) -- (166.1,110.73) -- (218.91,149.09) -- (180.54,96.29) -- (218.91,149.09) -- (198.74,87.01) -- (218.91,149.09) -- cycle ;
		%Straight Lines [id:da45946669275511476] 
		\draw    (86,73) -- (562.86,73) ;
		%Straight Lines [id:da18851797961983152] 
		\draw    (101.86,54.19) -- (101.86,650.19) ;
		%Straight Lines [id:da05294117433922452] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (337.86,54.19) -- (337.86,652.19) ;
		%Straight Lines [id:da5706482870819345] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (87,365) -- (562.86,365) ;
		%Shape: Circle [id:dp6295160362838159] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (154.81,101.59) .. controls (154.81,99.06) and (156.87,97) .. (159.41,97) .. controls (161.94,97) and (164,99.06) .. (164,101.59) .. controls (164,104.13) and (161.94,106.19) .. (159.41,106.19) .. controls (156.87,106.19) and (154.81,104.13) .. (154.81,101.59) -- cycle ;
		%Shape: Circle [id:dp5577933558486465] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (144.81,160.59) .. controls (144.81,158.06) and (146.87,156) .. (149.41,156) .. controls (151.94,156) and (154,158.06) .. (154,160.59) .. controls (154,163.13) and (151.94,165.19) .. (149.41,165.19) .. controls (146.87,165.19) and (144.81,163.13) .. (144.81,160.59) -- cycle ;
		%Shape: Circle [id:dp8139309490762519] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (158.05,146.09) .. controls (158.05,143.56) and (160.11,141.5) .. (162.64,141.5) .. controls (165.18,141.5) and (167.24,143.56) .. (167.24,146.09) .. controls (167.24,148.63) and (165.18,150.69) .. (162.64,150.69) .. controls (160.11,150.69) and (158.05,148.63) .. (158.05,146.09) -- cycle ;
		%Shape: Circle [id:dp7328101559387779] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (190.05,191.09) .. controls (190.05,188.56) and (192.11,186.5) .. (194.64,186.5) .. controls (197.18,186.5) and (199.24,188.56) .. (199.24,191.09) .. controls (199.24,193.63) and (197.18,195.69) .. (194.64,195.69) .. controls (192.11,195.69) and (190.05,193.63) .. (190.05,191.09) -- cycle ;
		%Shape: Circle [id:dp6921971935075115] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (194.14,215.77) .. controls (194.14,213.23) and (196.2,211.17) .. (198.74,211.17) .. controls (201.27,211.17) and (203.33,213.23) .. (203.33,215.77) .. controls (203.33,218.3) and (201.27,220.36) .. (198.74,220.36) .. controls (196.2,220.36) and (194.14,218.3) .. (194.14,215.77) -- cycle ;
		%Shape: Circle [id:dp8090804859687755] 
		\draw  [fill={rgb, 255:red, 106; green, 170; blue, 250 }  ,fill opacity=1 ] (411.24,148.09) .. controls (411.24,121.22) and (433.03,99.43) .. (459.91,99.43) .. controls (486.78,99.43) and (508.57,121.22) .. (508.57,148.09) .. controls (508.57,174.97) and (486.78,196.76) .. (459.91,196.76) .. controls (433.03,196.76) and (411.24,174.97) .. (411.24,148.09) -- cycle ;
		%Shape: Circle [id:dp9950927028216827] 
		\draw  [fill={rgb, 255:red, 255; green, 80; blue, 99 }  ,fill opacity=1 ] (427.31,148.09) .. controls (427.31,130.09) and (441.91,115.5) .. (459.91,115.5) .. controls (477.91,115.5) and (492.5,130.09) .. (492.5,148.09) .. controls (492.5,166.09) and (477.91,180.69) .. (459.91,180.69) .. controls (441.91,180.69) and (427.31,166.09) .. (427.31,148.09) -- cycle ;
		%Shape: Circle [id:dp02079691968431252] 
		\draw  [fill={rgb, 255:red, 245; green, 166; blue, 35 }  ,fill opacity=1 ] (444.81,148.09) .. controls (444.81,139.76) and (451.57,133) .. (459.91,133) .. controls (468.24,133) and (475,139.76) .. (475,148.09) .. controls (475,156.43) and (468.24,163.19) .. (459.91,163.19) .. controls (451.57,163.19) and (444.81,156.43) .. (444.81,148.09) -- cycle ;
		%Shape: Circle [id:dp3560529503932053] 
		\draw   (394.63,148.09) .. controls (394.63,112.04) and (423.86,82.82) .. (459.91,82.82) .. controls (495.96,82.82) and (525.18,112.04) .. (525.18,148.09) .. controls (525.18,184.14) and (495.96,213.37) .. (459.91,213.37) .. controls (423.86,213.37) and (394.63,184.14) .. (394.63,148.09) -- cycle ;
		%Shape: Star [id:dp45102824876450387] 
		\draw  [color={rgb, 255:red, 238; green, 238; blue, 238 }  ,draw opacity=1 ] (459.91,82.82) -- (459.91,148.09) -- (480.08,86.01) -- (459.91,148.09) -- (498.27,95.29) -- (459.91,148.09) -- (512.71,109.73) -- (459.91,148.09) -- (521.99,127.92) -- (459.91,148.09) -- (525.18,148.09) -- (459.91,148.09) -- (521.99,168.26) -- (459.91,148.09) -- (512.71,186.46) -- (459.91,148.09) -- (498.27,200.9) -- (459.91,148.09) -- (480.08,210.17) -- (459.91,148.09) -- (459.91,213.37) -- (459.91,148.09) -- (439.74,210.17) -- (459.91,148.09) -- (421.54,200.9) -- (459.91,148.09) -- (407.1,186.46) -- (459.91,148.09) -- (397.83,168.26) -- (459.91,148.09) -- (394.63,148.09) -- (459.91,148.09) -- (397.83,127.92) -- (459.91,148.09) -- (407.1,109.73) -- (459.91,148.09) -- (421.54,95.29) -- (459.91,148.09) -- (439.74,86.01) -- (459.91,148.09) -- cycle ;
		%Shape: Circle [id:dp3456515897176302] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (495.14,123.77) .. controls (495.14,121.23) and (497.2,119.17) .. (499.74,119.17) .. controls (502.27,119.17) and (504.33,121.23) .. (504.33,123.77) .. controls (504.33,126.3) and (502.27,128.36) .. (499.74,128.36) .. controls (497.2,128.36) and (495.14,126.3) .. (495.14,123.77) -- cycle ;
		%Shape: Circle [id:dp2125845838255942] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (506.12,114.32) .. controls (506.12,111.78) and (508.18,109.73) .. (510.71,109.73) .. controls (513.25,109.73) and (515.31,111.78) .. (515.31,114.32) .. controls (515.31,116.86) and (513.25,118.91) .. (510.71,118.91) .. controls (508.18,118.91) and (506.12,116.86) .. (506.12,114.32) -- cycle ;
		%Shape: Circle [id:dp887032520334764] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (520.12,112.32) .. controls (520.12,109.78) and (522.18,107.73) .. (524.71,107.73) .. controls (527.25,107.73) and (529.31,109.78) .. (529.31,112.32) .. controls (529.31,114.86) and (527.25,116.91) .. (524.71,116.91) .. controls (522.18,116.91) and (520.12,114.86) .. (520.12,112.32) -- cycle ;
		%Shape: Circle [id:dp4932454803025961] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (493.68,99.88) .. controls (493.68,97.34) and (495.74,95.29) .. (498.27,95.29) .. controls (500.81,95.29) and (502.87,97.34) .. (502.87,99.88) .. controls (502.87,102.42) and (500.81,104.47) .. (498.27,104.47) .. controls (495.74,104.47) and (493.68,102.42) .. (493.68,99.88) -- cycle ;
		%Shape: Circle [id:dp24661846934667242] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (505.68,101.88) .. controls (505.68,99.34) and (507.74,97.29) .. (510.27,97.29) .. controls (512.81,97.29) and (514.87,99.34) .. (514.87,101.88) .. controls (514.87,104.42) and (512.81,106.47) .. (510.27,106.47) .. controls (507.74,106.47) and (505.68,104.42) .. (505.68,101.88) -- cycle ;
		%Shape: Circle [id:dp6424219162409122] 
		\draw  [fill={rgb, 255:red, 106; green, 170; blue, 250 }  ,fill opacity=1 ] (170.24,441.09) .. controls (170.24,414.22) and (192.03,392.43) .. (218.91,392.43) .. controls (245.78,392.43) and (267.57,414.22) .. (267.57,441.09) .. controls (267.57,467.97) and (245.78,489.76) .. (218.91,489.76) .. controls (192.03,489.76) and (170.24,467.97) .. (170.24,441.09) -- cycle ;
		%Shape: Circle [id:dp36100096532379045] 
		\draw  [fill={rgb, 255:red, 255; green, 80; blue, 99 }  ,fill opacity=1 ] (186.31,441.09) .. controls (186.31,423.09) and (200.91,408.5) .. (218.91,408.5) .. controls (236.91,408.5) and (251.5,423.09) .. (251.5,441.09) .. controls (251.5,459.09) and (236.91,473.69) .. (218.91,473.69) .. controls (200.91,473.69) and (186.31,459.09) .. (186.31,441.09) -- cycle ;
		%Shape: Circle [id:dp6987696043172553] 
		\draw  [fill={rgb, 255:red, 245; green, 166; blue, 35 }  ,fill opacity=1 ] (203.81,441.09) .. controls (203.81,432.76) and (210.57,426) .. (218.91,426) .. controls (227.24,426) and (234,432.76) .. (234,441.09) .. controls (234,449.43) and (227.24,456.19) .. (218.91,456.19) .. controls (210.57,456.19) and (203.81,449.43) .. (203.81,441.09) -- cycle ;
		%Shape: Circle [id:dp7445450201974526] 
		\draw   (153.63,441.09) .. controls (153.63,405.04) and (182.86,375.82) .. (218.91,375.82) .. controls (254.96,375.82) and (284.18,405.04) .. (284.18,441.09) .. controls (284.18,477.14) and (254.96,506.37) .. (218.91,506.37) .. controls (182.86,506.37) and (153.63,477.14) .. (153.63,441.09) -- cycle ;
		%Shape: Star [id:dp5744137314410487] 
		\draw  [color={rgb, 255:red, 238; green, 238; blue, 238 }  ,draw opacity=1 ] (218.91,375.82) -- (218.91,441.09) -- (239.08,379.01) -- (218.91,441.09) -- (257.27,388.29) -- (218.91,441.09) -- (271.71,402.73) -- (218.91,441.09) -- (280.99,420.92) -- (218.91,441.09) -- (284.18,441.09) -- (218.91,441.09) -- (280.99,461.26) -- (218.91,441.09) -- (271.71,479.46) -- (218.91,441.09) -- (257.27,493.9) -- (218.91,441.09) -- (239.08,503.17) -- (218.91,441.09) -- (218.91,506.37) -- (218.91,441.09) -- (198.74,503.17) -- (218.91,441.09) -- (180.54,493.9) -- (218.91,441.09) -- (166.1,479.46) -- (218.91,441.09) -- (156.83,461.26) -- (218.91,441.09) -- (153.63,441.09) -- (218.91,441.09) -- (156.83,420.92) -- (218.91,441.09) -- (166.1,402.73) -- (218.91,441.09) -- (180.54,388.29) -- (218.91,441.09) -- (198.74,379.01) -- (218.91,441.09) -- cycle ;
		%Shape: Circle [id:dp6071801998399693] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (222.14,451.77) .. controls (222.14,449.23) and (224.2,447.17) .. (226.74,447.17) .. controls (229.27,447.17) and (231.33,449.23) .. (231.33,451.77) .. controls (231.33,454.3) and (229.27,456.36) .. (226.74,456.36) .. controls (224.2,456.36) and (222.14,454.3) .. (222.14,451.77) -- cycle ;
		%Shape: Circle [id:dp8687839126849253] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (242.14,493.77) .. controls (242.14,491.23) and (244.2,489.17) .. (246.74,489.17) .. controls (249.27,489.17) and (251.33,491.23) .. (251.33,493.77) .. controls (251.33,496.3) and (249.27,498.36) .. (246.74,498.36) .. controls (244.2,498.36) and (242.14,496.3) .. (242.14,493.77) -- cycle ;
		%Shape: Circle [id:dp04673768038139747] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (239.14,415.77) .. controls (239.14,413.23) and (241.2,411.17) .. (243.74,411.17) .. controls (246.27,411.17) and (248.33,413.23) .. (248.33,415.77) .. controls (248.33,418.3) and (246.27,420.36) .. (243.74,420.36) .. controls (241.2,420.36) and (239.14,418.3) .. (239.14,415.77) -- cycle ;
		%Shape: Circle [id:dp24645088472506638] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (179.14,411.77) .. controls (179.14,409.23) and (181.2,407.17) .. (183.74,407.17) .. controls (186.27,407.17) and (188.33,409.23) .. (188.33,411.77) .. controls (188.33,414.3) and (186.27,416.36) .. (183.74,416.36) .. controls (181.2,416.36) and (179.14,414.3) .. (179.14,411.77) -- cycle ;
		%Shape: Circle [id:dp7243412384970735] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (187.14,481.77) .. controls (187.14,479.23) and (189.2,477.17) .. (191.74,477.17) .. controls (194.27,477.17) and (196.33,479.23) .. (196.33,481.77) .. controls (196.33,484.3) and (194.27,486.36) .. (191.74,486.36) .. controls (189.2,486.36) and (187.14,484.3) .. (187.14,481.77) -- cycle ;
		%Shape: Circle [id:dp7704243655489136] 
		\draw  [fill={rgb, 255:red, 106; green, 170; blue, 250 }  ,fill opacity=1 ] (413.24,441.09) .. controls (413.24,414.22) and (435.03,392.43) .. (461.91,392.43) .. controls (488.78,392.43) and (510.57,414.22) .. (510.57,441.09) .. controls (510.57,467.97) and (488.78,489.76) .. (461.91,489.76) .. controls (435.03,489.76) and (413.24,467.97) .. (413.24,441.09) -- cycle ;
		%Shape: Circle [id:dp1600155309510971] 
		\draw  [fill={rgb, 255:red, 255; green, 80; blue, 99 }  ,fill opacity=1 ] (429.31,441.09) .. controls (429.31,423.09) and (443.91,408.5) .. (461.91,408.5) .. controls (479.91,408.5) and (494.5,423.09) .. (494.5,441.09) .. controls (494.5,459.09) and (479.91,473.69) .. (461.91,473.69) .. controls (443.91,473.69) and (429.31,459.09) .. (429.31,441.09) -- cycle ;
		%Shape: Circle [id:dp5197469483866035] 
		\draw  [fill={rgb, 255:red, 245; green, 166; blue, 35 }  ,fill opacity=1 ] (446.81,441.09) .. controls (446.81,432.76) and (453.57,426) .. (461.91,426) .. controls (470.24,426) and (477,432.76) .. (477,441.09) .. controls (477,449.43) and (470.24,456.19) .. (461.91,456.19) .. controls (453.57,456.19) and (446.81,449.43) .. (446.81,441.09) -- cycle ;
		%Shape: Circle [id:dp15784579774508334] 
		\draw   (396.63,441.09) .. controls (396.63,405.04) and (425.86,375.82) .. (461.91,375.82) .. controls (497.96,375.82) and (527.18,405.04) .. (527.18,441.09) .. controls (527.18,477.14) and (497.96,506.37) .. (461.91,506.37) .. controls (425.86,506.37) and (396.63,477.14) .. (396.63,441.09) -- cycle ;
		%Shape: Star [id:dp34342348321746785] 
		\draw  [color={rgb, 255:red, 238; green, 238; blue, 238 }  ,draw opacity=1 ] (461.91,375.82) -- (461.91,441.09) -- (482.08,379.01) -- (461.91,441.09) -- (500.27,388.29) -- (461.91,441.09) -- (514.71,402.73) -- (461.91,441.09) -- (523.99,420.92) -- (461.91,441.09) -- (527.18,441.09) -- (461.91,441.09) -- (523.99,461.26) -- (461.91,441.09) -- (514.71,479.46) -- (461.91,441.09) -- (500.27,493.9) -- (461.91,441.09) -- (482.08,503.17) -- (461.91,441.09) -- (461.91,506.37) -- (461.91,441.09) -- (441.74,503.17) -- (461.91,441.09) -- (423.54,493.9) -- (461.91,441.09) -- (409.1,479.46) -- (461.91,441.09) -- (399.83,461.26) -- (461.91,441.09) -- (396.63,441.09) -- (461.91,441.09) -- (399.83,420.92) -- (461.91,441.09) -- (409.1,402.73) -- (461.91,441.09) -- (423.54,388.29) -- (461.91,441.09) -- (441.74,379.01) -- (461.91,441.09) -- cycle ;
		%Shape: Circle [id:dp46239018899546536] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (465.14,451.77) .. controls (465.14,449.23) and (467.2,447.17) .. (469.74,447.17) .. controls (472.27,447.17) and (474.33,449.23) .. (474.33,451.77) .. controls (474.33,454.3) and (472.27,456.36) .. (469.74,456.36) .. controls (467.2,456.36) and (465.14,454.3) .. (465.14,451.77) -- cycle ;
		%Shape: Circle [id:dp10480382823483092] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (455.31,445.69) .. controls (455.31,443.15) and (457.37,441.09) .. (459.91,441.09) .. controls (462.44,441.09) and (464.5,443.15) .. (464.5,445.69) .. controls (464.5,448.22) and (462.44,450.28) .. (459.91,450.28) .. controls (457.37,450.28) and (455.31,448.22) .. (455.31,445.69) -- cycle ;
		%Shape: Circle [id:dp25212075375306475] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (446.81,441.09) .. controls (446.81,438.56) and (448.87,436.5) .. (451.41,436.5) .. controls (453.94,436.5) and (456,438.56) .. (456,441.09) .. controls (456,443.63) and (453.94,445.69) .. (451.41,445.69) .. controls (448.87,445.69) and (446.81,443.63) .. (446.81,441.09) -- cycle ;
		%Shape: Circle [id:dp7040809310858036] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (457.31,430.59) .. controls (457.31,428.06) and (459.37,426) .. (461.91,426) .. controls (464.44,426) and (466.5,428.06) .. (466.5,430.59) .. controls (466.5,433.13) and (464.44,435.19) .. (461.91,435.19) .. controls (459.37,435.19) and (457.31,433.13) .. (457.31,430.59) -- cycle ;
		%Shape: Circle [id:dp4481812147571629] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (467.81,441.09) .. controls (467.81,438.56) and (469.87,436.5) .. (472.41,436.5) .. controls (474.94,436.5) and (477,438.56) .. (477,441.09) .. controls (477,443.63) and (474.94,445.69) .. (472.41,445.69) .. controls (469.87,445.69) and (467.81,443.63) .. (467.81,441.09) -- cycle ;
		%Shape: Axis 2D [id:dp9535569342639982] 
		\draw  (142.86,628.33) -- (286.86,628.33)(157.26,551.1) -- (157.26,636.91) (279.86,623.33) -- (286.86,628.33) -- (279.86,633.33) (152.26,558.1) -- (157.26,551.1) -- (162.26,558.1)  ;
		%Straight Lines [id:da18575728614625153] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (221,558.19) -- (221,628) ;
		%Curve Lines [id:da4559578126224806] 
		\draw [line width=1.5]    (157.26,628.33) .. controls (190.86,627.19) and (196.86,575.19) .. (220.86,575.19) .. controls (244.86,575.19) and (248.86,627.19) .. (283.86,628.19) ;
		%Straight Lines [id:da00888530596433701] 
		\draw    (173.86,636) -- (264.86,636) ;
		\draw [shift={(266.86,636)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(171.86,636)}, rotate = 0] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Axis 2D [id:dp23543165329354676] 
		\draw  (381.86,628.33) -- (525.86,628.33)(396.26,551.1) -- (396.26,636.91) (518.86,623.33) -- (525.86,628.33) -- (518.86,633.33) (391.26,558.1) -- (396.26,551.1) -- (401.26,558.1)  ;
		%Straight Lines [id:da019235121392213506] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (460,558.19) -- (460,628) ;
		%Curve Lines [id:da27362805541958446] 
		\draw [line width=1.5]    (396.26,628.33) .. controls (456.86,627.19) and (446.86,575.19) .. (459.86,575.19) .. controls (472.86,575.19) and (461.86,628.19) .. (521.86,628.19) ;
		%Straight Lines [id:da560847139661574] 
		\draw    (440,636) -- (481.86,636) ;
		\draw [shift={(483.86,636)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(438,636)}, rotate = 0] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Shape: Axis 2D [id:dp6104562144929226] 
		\draw  (142.86,336.7) -- (286.86,336.7)(157.26,259.46) -- (157.26,345.28) (279.86,331.7) -- (286.86,336.7) -- (279.86,341.7) (152.26,266.46) -- (157.26,259.46) -- (162.26,266.46)  ;
		%Straight Lines [id:da21576268384712338] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (221,266.55) -- (221,336.37) ;
		%Curve Lines [id:da0021233167236687223] 
		\draw [line width=1.5]    (147.4,336.64) .. controls (168.4,336.64) and (170.86,280.19) .. (181.86,280.19) .. controls (192.86,280.19) and (197.65,335.89) .. (228.65,336.89) ;
		%Straight Lines [id:da6618053460753583] 
		\draw    (160.86,344.37) -- (207.86,344.37) ;
		\draw [shift={(209.86,344.37)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(158.86,344.37)}, rotate = 0] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da46974778941497464] 
		\draw    (186,287.46) -- (219,287.46) ;
		\draw [shift={(221,287.46)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(184,287.46)}, rotate = 0] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da6748812793035475] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (181.86,280.19) -- (181.86,336.19) ;
		%Shape: Axis 2D [id:dp001503499765295846] 
		\draw  (379.86,336.7) -- (523.86,336.7)(394.26,259.46) -- (394.26,345.28) (516.86,331.7) -- (523.86,336.7) -- (516.86,341.7) (389.26,266.46) -- (394.26,259.46) -- (399.26,266.46)  ;
		%Straight Lines [id:da16710753217555707] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (458,266.55) -- (458,336.37) ;
		%Curve Lines [id:da10422935338980177] 
		\draw [line width=1.5]    (483.86,336.19) .. controls (498.86,336.19) and (493.86,281.19) .. (501.86,281.19) .. controls (509.86,281.19) and (503.86,336.19) .. (518.65,336.89) ;
		%Straight Lines [id:da36056893139800383] 
		\draw    (489.86,343.37) -- (516.86,343.37) ;
		\draw [shift={(518.86,343.37)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(487.86,343.37)}, rotate = 0] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		%Straight Lines [id:da9588719109024109] 
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (501.86,281.19) -- (501.86,337.19) ;
		%Straight Lines [id:da5839978749194252] 
		\draw    (460,287.46) -- (498.86,287.46) ;
		\draw [shift={(500.86,287.46)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw [shift={(458,287.46)}, rotate = 0] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		
		% Text Node
		\draw (113,221.73) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{165pt}\setlength\topsep{0pt}
		\begin{center}
		{\footnotesize \textbf{neither true or accurate ("imprecise"})}\\{\footnotesize (random error + systematic)}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (266,24) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\LARGE \textbf{PRECISION}}};
		% Text Node
		\draw (43.3,449.86) node [anchor=north west][inner sep=0.75pt]  [rotate=-270.48] [align=left] {{\LARGE \textbf{EXACTITUDE}}};
		% Text Node
		\draw (205,50) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{NO}};
		% Text Node
		\draw (447,51) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{YES}};
		% Text Node
		\draw (79.5,225.5) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {\textbf{NO}};
		% Text Node
		\draw (79.5,524.5) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {\textbf{YES}};
		% Text Node
		\draw (390,220.73) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{100pt}\setlength\topsep{0pt}
		\begin{center}
		{\footnotesize \textbf{not true but accurate}}\\{\footnotesize (systematic error)}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (155,513.73) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{100pt}\setlength\topsep{0pt}
		\begin{center}
		{\footnotesize \textbf{true but not accurate}}\\{\footnotesize (random error)}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (377,513.73) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{125pt}\setlength\topsep{0pt}
		\begin{center}
		{\footnotesize \textbf{true and accurate ("precise")}}\\{\footnotesize (low errors)}
		\end{center}
		
		\end{minipage}};
		% Text Node
		\draw (278,615) node [anchor=north west][inner sep=0.75pt]  [font=\tiny] [align=left] {value};
		% Text Node
		\draw (147,622) node [anchor=north west][inner sep=0.75pt]  [font=\tiny,rotate=-270] [align=left] {probability density};
		% Text Node
		\draw (193,552) node [anchor=north west][inner sep=0.75pt]  [font=\tiny] [align=left] {reference value};
		% Text Node
		\draw (207,643) node [anchor=north west][inner sep=0.75pt]  [font=\tiny] [align=left] {precision};
		% Text Node
		\draw (517,616) node [anchor=north west][inner sep=0.75pt]  [font=\tiny] [align=left] {value};
		% Text Node
		\draw (386,622) node [anchor=north west][inner sep=0.75pt]  [font=\tiny,rotate=-270] [align=left] {probability density};
		% Text Node
		\draw (432,552) node [anchor=north west][inner sep=0.75pt]  [font=\tiny] [align=left] {reference value};
		% Text Node
		\draw (446,643) node [anchor=north west][inner sep=0.75pt]  [font=\tiny] [align=left] {precision};
		% Text Node
		\draw (277,323.37) node [anchor=north west][inner sep=0.75pt]  [font=\tiny] [align=left] {value};
		% Text Node
		\draw (147,330.37) node [anchor=north west][inner sep=0.75pt]  [font=\tiny,rotate=-270] [align=left] {probability density};
		% Text Node
		\draw (193,260.37) node [anchor=north west][inner sep=0.75pt]  [font=\tiny] [align=left] {reference value};
		% Text Node
		\draw (173,350.37) node [anchor=north west][inner sep=0.75pt]  [font=\tiny] [align=left] {precision};
		% Text Node
		\draw (184,274.37) node [anchor=north west][inner sep=0.75pt]  [font=\tiny] [align=left] {exactitude};
		% Text Node
		\draw (515,324.37) node [anchor=north west][inner sep=0.75pt]  [font=\tiny] [align=left] {value};
		% Text Node
		\draw (384,330.37) node [anchor=north west][inner sep=0.75pt]  [font=\tiny,rotate=-270] [align=left] {probability density};
		% Text Node
		\draw (430,260.37) node [anchor=north west][inner sep=0.75pt]  [font=\tiny] [align=left] {reference value};
		% Text Node
		\draw (487.86,350.37) node [anchor=north west][inner sep=0.75pt]  [font=\tiny] [align=left] {precision};
		% Text Node
		\draw (461,274.37) node [anchor=north west][inner sep=0.75pt]  [font=\tiny] [align=left] {exactitude};
		\end{tikzpicture}
		\vspace*{3mm}
		\caption{Type of measurement errors}
	\end{figure}
	As we have seen earlier above, the arithmetic mean value will be in the univariate case:
	
	and the standard deviation (biased estimator as proved earlier) always in the univariate case (the $n$th case has already been proved during our detailed study of the variance):
	
	and the unbiased standard deviation (as also proven earlier):
	
	and we have proved that the standard deviation of the mean was given by (under some assumptions!): 
	
	and as we have proved it, after a large number of independent measurements, the distribution of errors on a measure follows a Normal distribution so that we can write for the fluctuation interval (if we do not have enough measurement, we then use the fluctuation interval based on the Student law):
	
	In short, we can use all statistical tools seen so far in the field of measurement in laboratories or elsewhere!
	
	The result of a measurement (or estimation and this even in the field of project management!!!) must include rigorously at least 4 elements. For example for a symmetrical error:
	
	or an asymmetrical one:
	
	Where we have:
	\begin{enumerate}
		\item The numerical value with the correct number of decimal places.

		\item Unit of measurement according to the international measurement system.

		\item Expanded uncertainty of $k\cdot \sigma$ (fluctuation interval)

		\item The integer value of $k$ used for fluctuation interval.
	\end{enumerate}
	This method is quite more useful than the previous one as we don't need to recalculate it each time for each experimental value. It's this method that for example the famous CERN (European Organization for Nuclear Research) use\footnote{Add to this the Six Sigma measurement methodology studied in the Industrial Engineering section}.
	
	In astrophysics papers we can also found the following notation separating the part of the variation from the measurement tool and from what is measured:
	
	
	\begin{tcolorbox}[enhanced,colback=red!5!white,colframe=black!50!red,boxrule=1pt,arc=0pt,outer arc=0pt,drop lifted shadow,after skip=10pt plus 2pt]
	\bcbombe Caution! It happens many many many times... in scientific papers and also in textbooks that a measurement is provided without indication of its approximation. It is  then generally admitted in practice that the absolute incertitude isn't greater than a unit of the power order indicated by the last digit.
	\end{tcolorbox}
	
	\subsubsection{Repeatability}
	The repeatability $r$, the likely measurement difference between two measurements of similar objects in the same laboratory under similar operating conditions, is normatively defined (in the norms ISO 5725:1987 and AFNOR NF X 06-041) in the one-dimensional (univariate) case by:
	
	where $p$ is a high value probability, usually equal to $95\%$ and $X_1,X_2$ two independent and identically distributed random variables according to a Normal distribution with unknown mean and unknown variance $\mathcal{N}(\mu,\sigma)$. By the stability of the Normal distribution, then it comes:
	
	But, we saw earlier in this section in the context of the study of the confidence interval of the mean:
	
	So verbatim:
	
	and therefore using the tables, we have:
	
	and therefore:
	
	Either with the notation respecting the norms for laboratories:
	
	But in the present case, we have a double variance. So it comes:
	
	So we fall back on the relation available in the norms with the famous $2.77$ coefficient. Obviously after it is clear that the value of $r$ should be minimized!
	
	\subsubsection{Error propagation (linearised approximation)}
	Given a measurement $x+\delta x$ and $y=f(x)$ a function of $x$. What is the uncertainty on $y$ it if we know only the uncertainty of a measurement device, but that would not be given as a statistical standard deviation?
	
	In this type of situation we speak of "\NewTerm{indirect measurement}\index{indirect measurement}". This is typically the case if we want to measure an intensity $I$ by measuring it indirectly by making the ratio of the voltage $U$ by the resistance $R$ used for the measurement as $I=U/R$. It is indeed obvious that in the latter situation we can not make the sum of the voltage incertitude and resistance incertitude because the system is not homogeneous at the unit level!!!
	
	When $\delta x$ is small, $f (x)$ is replaced in the neighbourhood of $x$ by its tangent (it is simply the derivative of course!):
	
	but if $y$ depends of several variables $x, z, t$ measured with the uncertainties $\delta x,\delta z,\delta t$:
	
	the maximum possible error is then the exact total differential (\SeeChapter{see section of Differential and Integral Calculus page \pageref{total exact differential}}):
	
	Using first order Taylor expansion series (\SeeChapter{see section Sequences and Series page \pageref{taylor series}}) we can write:
	
	What we denote often as the sum of the partial derivatives with their respective uncertainty:
	
	and this works very well as long as the increments $\Delta x$ are sufficiently small. Even highly curved functions are nearly linear over a small enough region!
	
	The latter relation is the "\NewTerm{propagation law}\index{propagation law}" of the studied problem. The partial derivative in factor of the uncertainty is in the science of measurement, named the  "\NewTerm{uncertainty coefficient}\index{uncertainty coefficient}".
	
	For sure we can also write to get the relative error:
	
	
	Which bring us to:
	
	and:
	
	Therefore the relative incertitude of a product or of a quotient of uncertain values is equal to the sum of the relative incertitudes. It is thus clear that a mathematical operation cannot improve the uncertainty of the data.
	
	For the above we deduce immediately:
	
	and therefore:
	
	Here is a summary of this as a table (with a small change in notations):
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75} 
		${z=}$ & ${x+y}$ & ${x-y}$ & ${xy}$ & ${\dfrac{x}{y}}$ \\ \hline
		\cellcolor[gray]{0.75}${\Delta z=}$ & $\Delta x+\Delta y$ & $\Delta x+\Delta y$ & $x\Delta y+y\Delta x$ & $\dfrac{x \Delta y+y \Delta x}{y^2}$ \\ \hline
		\cellcolor[gray]{0.75}${\dfrac{\Delta z}{z}=}$ & $\dfrac{\Delta x+\Delta y}{x+y}$ & $\dfrac{\Delta x+\Delta y}{x-y}$ & $\dfrac{x\Delta y+y\Delta x}{xy}=\dfrac{\Delta x}{x}+\dfrac{\Delta y}{y}$ & $\dfrac{\Delta x}{x}+\dfrac{\Delta y}{y}$ \\ \hline
		\end{tabular}
		\caption{Absolute and Relative errors common relations}
	\end{table}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	The result of multiplication, division, subtraction or addition is rounded to as many significant digits that the data that has the smallest number of them.
	\end{tcolorbox}	
	Obviously this propagation law (linear) is valid only in the range where the function can be approximated as linear. So be careful in its use! Otherwise you have to take a Taylor series approximation of higher order.
	
	If the uncertainty of the measurement is given in a statistic form (standard deviation), it is evident therefore that we will use the variance properties already seen at the beginning of this section... at least for simple cases.
	
	Once again the pitfall of this method in comparison to the statistical one is that you have the calculate the error for each measurement...
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	We want to calculate the incertitude  of the measurement of the density of a cube of a given material. We then know that:
	
	\end{tcolorbox}
	
	\pagebreak
	\subsubsection{Error propagation (statistical approach)}
	Finally, there is one additional point to discuss: Suppose we measure a quantity $u$ several times, or by several different methods, and for each measurement $u_i$ we estimate its uncertainty $\sigma_i$. The $\sigma_i$ are not necessarily equal; some of the measurements will be
better than others, because of larger sample sizes (more repetitions), or because of other factors—like better apparatus. How do we determine our best estimate of $u$, and how do we find the uncertainty in that estimate?

	For example, suppose a length $x$ is measured by one person $n_1$ times and by another person $n_2$ times, so that the first person finds:
	
	while the second person finds:
	
	Here $\sigma_1$ is the uncertainty in $u_1$, $\sigma_2$ is the uncertainty in $u_2$, and $\sigma^2$ is the population variance of the $x$-values. How should $u_1$ and $u_2$ be combined to yield an overall $u$, and what is the uncertainty in this final $\bar{u}$? Since $n_1=\sigma^2/\sigma_1^2$ and $n_2=\sigma^2/\sigma_2^2$, we have:
	
	with by definition:
	
	In general, if there are $n$ values of $u$, here is the generalized result, in a form that depends only on each $u_k$ and its uncertainty $\sigma_k$:
	
	Note how more measurements, or more accurate measurements, reduce the uncertainty by increasing its reciprocal.

	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	The field of particle physics provides numerous situations where the previous relations and also the $\chi^2$ can be applied. A particularly simple example\footnote{This example was provided by Patricia Burchat, a UCSC physics faculty member from 11988-11994 (according to holocene calendar), and now at Stanford.} involves measurements of the mass $M_Z$ of the $Z^0$ boson by experimental groups at CERN. The results of measurements of $M_Z$ made by four different detectors (L3, OPAL, Aleph and Delphi) are as follows\footnote{More recent measurements are listed at \url{http://pdg.lbl.gov/2014/listings/rpp2014-list-z-boson.pdf}}:
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|}
		\hline
		\rowcolor[gray]{0.75} 
		\textbf{Detector} & \textbf{Mass in GeV/$c^2$} \\ \hline
		L3 & $91.161 \pm 0.013$ \\ \hline
		OPAL & $91.174 \pm 0.011$ \\ \hline
		Aleph & $91.186 \pm 0.013$ \\ \hline
		Delphi & $91.188 \pm 0.013$ \\ \hline
		\end{tabular}
	\end{table}
	The listed uncertainties are estimates of the $\sigma_i$, the standard deviations for each of the measurements. The figure below shows these measurements plotted on a horizontal mass scale (vertically displaced for clarity):
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		%uncomment if require: \path (0,471); %set diagram left start at 0, and has height of 471
		
		%Straight Lines [id:da6242553365750514] 
		\draw    (153,194) -- (490.37,194) ;
		\draw [shift={(492.37,194)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-4.9) .. controls (6.95,-2.3) and (3.31,-0.67) .. (0,0) .. controls (3.31,0.67) and (6.95,2.3) .. (10.93,4.9)   ;
		%Straight Lines [id:da16977732351519625] 
		\draw    (162.03,188.82) -- (162.03,193.82) ;
		%Straight Lines [id:da4039297018913566] 
		\draw    (205.37,188.82) -- (205.37,193.82) ;
		%Straight Lines [id:da24758601921837053] 
		\draw    (247.7,188.82) -- (247.7,193.82) ;
		%Straight Lines [id:da29070973860764204] 
		\draw    (290.03,189.49) -- (290.03,194.49) ;
		%Straight Lines [id:da6095387339968548] 
		\draw    (332.03,189.16) -- (332.03,194.16) ;
		%Straight Lines [id:da8502227798216395] 
		\draw    (375.03,189.16) -- (375.03,194.16) ;
		%Straight Lines [id:da9753875423524521] 
		\draw    (417.03,189.16) -- (417.03,194.16) ;
		%Straight Lines [id:da6293026231739005] 
		\draw    (460.7,188.82) -- (460.7,193.82) ;
		%Straight Lines [id:da65164921300273] 
		\draw    (196.94,80.19) -- (307.03,80.19) ;
		\draw [shift={(307.03,80.19)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (0,5.59) -- (0,-5.59)   ;
		\draw [shift={(196.94,80.19)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (0,5.59) -- (0,-5.59)   ;
		%Straight Lines [id:da6637766889321961] 
		\draw    (260.61,109.19) -- (353.37,109.19) ;
		\draw [shift={(353.37,109.19)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (0,5.59) -- (0,-5.59)   ;
		\draw [shift={(260.61,109.19)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (0,5.59) -- (0,-5.59)   ;
		%Straight Lines [id:da4797647484125045] 
		\draw    (301.61,137.85) -- (413.37,137.85) ;
		\draw [shift={(413.37,137.85)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (0,5.59) -- (0,-5.59)   ;
		\draw [shift={(301.61,137.85)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (0,5.59) -- (0,-5.59)   ;
		%Straight Lines [id:da714785374509975] 
		\draw    (310.61,166.85) -- (421.7,166.85) ;
		\draw [shift={(421.7,166.85)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (0,5.59) -- (0,-5.59)   ;
		\draw [shift={(310.61,166.85)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (0,5.59) -- (0,-5.59)   ;
		%Shape: Circle [id:dp12755082391047057] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (249.74,80.19) .. controls (249.74,78.95) and (250.75,77.95) .. (251.99,77.95) .. controls (253.23,77.95) and (254.23,78.95) .. (254.23,80.19) .. controls (254.23,81.43) and (253.23,82.43) .. (251.99,82.43) .. controls (250.75,82.43) and (249.74,81.43) .. (249.74,80.19) -- cycle ;
		%Shape: Circle [id:dp7406451007655768] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (304.74,109.19) .. controls (304.74,107.95) and (305.75,106.95) .. (306.99,106.95) .. controls (308.23,106.95) and (309.23,107.95) .. (309.23,109.19) .. controls (309.23,110.43) and (308.23,111.43) .. (306.99,111.43) .. controls (305.75,111.43) and (304.74,110.43) .. (304.74,109.19) -- cycle ;
		%Shape: Circle [id:dp17904062934760456] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (357.49,137.85) .. controls (357.49,136.62) and (358.49,135.61) .. (359.73,135.61) .. controls (360.97,135.61) and (361.97,136.62) .. (361.97,137.85) .. controls (361.97,139.09) and (360.97,140.1) .. (359.73,140.1) .. controls (358.49,140.1) and (357.49,139.09) .. (357.49,137.85) -- cycle ;
		%Shape: Circle [id:dp3688900703775684] 
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (363.91,166.85) .. controls (363.91,165.62) and (364.92,164.61) .. (366.15,164.61) .. controls (367.39,164.61) and (368.4,165.62) .. (368.4,166.85) .. controls (368.4,168.09) and (367.39,169.1) .. (366.15,169.1) .. controls (364.92,169.1) and (363.91,168.09) .. (363.91,166.85) -- cycle ;
		
		% Text Node
		\draw (145.27,204.41) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$91.14$};
		% Text Node
		\draw (190.94,204.41) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$91.15$};
		% Text Node
		\draw (233.61,204.74) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$91.16$};
		% Text Node
		\draw (274.61,204.41) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$91.17$};
		% Text Node
		\draw (318.61,204.41) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$91.18$};
		% Text Node
		\draw (358.61,204.41) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$91.19$};
		% Text Node
		\draw (402.27,204.41) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$91.20$};
		% Text Node
		\draw (446.61,204.41) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize]  {$91.21$};
		% Text Node
		\draw (256.27,233.07) node [anchor=north west][inner sep=0.75pt]    {$M_{Z} \ \left[\text{GeV} \cdot c^{-2}\right]$};
		% Text Node
		\draw (243.61,60.52) node [anchor=north west][inner sep=0.75pt]   [align=left] {L3};
		% Text Node
		\draw (287.27,90.85) node [anchor=north west][inner sep=0.75pt]   [align=left] {OPAL};
		% Text Node
		\draw (342.61,119.85) node [anchor=north west][inner sep=0.75pt]   [align=left] {Aleph};
		% Text Node
		\draw (348.61,146.85) node [anchor=north west][inner sep=0.75pt]   [align=left] {Delphi};
		\end{tikzpicture}
	\end{figure}
	The question arises: Can these data be well described by a single number, namely an estimate of $M_Z$ made by determining the weighted mean of the four measurements?\\
	
	We know that we can calculate the weighted mean $\bar{M}_Z$, and its standard deviation $\sigma_{\bar{M}_Z}$ like this:
	
	to find:
	
	Then we form $\chi^2$:
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	We know that we must expect this value of $\chi^2$ to be drawn from a chi-square distribution with $3$ degrees of freedom.\\
	
	This correspond to a $p$-value of $0.426803$. The result is not significant at $p < 0.05$. We cannot therefore reject the null hypothesis that the four measurements of the $Z^0$ boson mass are consistent with each other.
	\end{tcolorbox}
	
	\subsubsection{Significant Numbers}
	In small schools (and sometimes of higher level and in corporations), it is required to transform a measurement expressed in a certain unit into another unit.
	
	For example, taking the tables, we can have the following conversion:
	\begin{gather*}
		140\; \text{[lb]}=140\cdot 0.45349237\; \text{[kg/lb]}=63.5029318 \; \text{[kg]}
	\end{gather*}
	Then comes the question (that the student or practitioner may have forgotten ...). Starting from a measurement with an accuracy of about $1$ [lb] (therefore of the order of $0.5$ [kg]), could a simple unit conversion lead to an accuracy of $1/10$ [mg]????
	
	From this example it is necessary to retain that a margin of uncertainty is associated with all measured values and any values calculated from measured values.
	
	In the exact sciences (and also soft skills science as management), all reasoning, any analysis must take this uncertainty into account!!!
	
	But why are some digits significant and others not? Because in sciences we only report what has been observed objectively (principle of objectivity). Accordingly, we limit the writing of a number to the digits reasonably reliable figures despite the uncertainty: the significant digits! The accuracy that additional digits could then seem to bring is then illusory.
	
	We must then know rounding according to some rules and conventions:
	\begin{itemize}
		\item When the digit of the highest rank being dropped is greater than $5$, the last digit is increased by $1$ (example: $12.66$ rounds to $12.7$). In the English version of Microsoft Excel 11.8346 this is given with:\\
		\begin{center}
		\texttt{=ROUND(12.66,1)=12.7}
		\end{center}
		
		\item When the digit of the highest rank being dropped is less than $5$, the previous digit remains unchanged (example: $12.64$ rounds to $12.6$). In the English version of Microsoft Excel 11.8346 this gives:
		\begin{center}
		\texttt{=ROUND(12.64,1)=12.6}
		\end{center}
		
		\item When the digit of the highest rank being dropped is equal to $5$ if one of the digits that follow is not zero, the preceding digit is increased by $1$ (example: $12.6502$ rounds to $12.7$). In the English version of Microsoft Excel 11.8346 this is given with:
		\begin{center}
		\texttt{=ROUND(12.6502,1)=12.7}
		\end{center}
		
		\item When the digit of the highest rank we drop is a terminal $5$ (which is not followed by any number!) or is followed only by zeros, we increase of $1$ the previous digit of the number rounded if it is odd, otherwise we leave it unchanged (examples: $12.75$ rounds to $12.8$ and $12.65$ to $12.6$). In the latter case, the last digit of the rounded number will always be an even number. Spreadsheets softwares do not really respect this last rule, actually with the English version of Microsoft Excel 11.8346 we have:
		\begin{center}
		\texttt{=ROUND(12.75,1)=12.8\\
		=ROUND(12.65,1)=12.7}
		\end{center}
	\end{itemize}
	In fact, in practice these rules are rarely used because software (spreadsheets softwares mainly) do not incorporate appropriate these rules. It is then customary to just to round up to the value of the nearest decimal.
	
	Significant digits of a value include all its digits determined with certainty and the first digit which carries an uncertainty (this latter significant occupies the same rank as the order of magnitude of the uncertainty).
	
	Often, data sources do not mention fluctuation interval (that is to say the indication $\pm \ldots$). For example, when we write $m=25.4\;\text{[kg]}$ we conventionally consider that uncertainty is of the same order of magnitude as the rank of the last significant digit (thus: the uncertain digit!).
	
	In fact, only the decimal place of uncertainty is implicit: the real margin is unspecified.
	
	However, additional information about precision can be conveyed through additional notations. It is often useful to know how exact the final digit(s) are. For instance, the accepted value of the unit of elementary charge can properly be expressed as $1.602176487(40)\cdot 10^{-19}$ [C], which is shorthand for $1.602176487\pm 0.000000040 \cdot 10^{-19}$ [C].
	
	\pagebreak
	\subsection{Spatial statistics}
	Spatial descriptive statistics are used for a variety of purposes in geography, particularly in quantitative data analyses involving Geographic Information Systems (GIS).
	
	\subsubsection{$2$D Poisson's spatial distance model}
	It would be in the early 11920s (holocene calendar) that the first scientific publications (by environmentalists) appeared concerning statistical tests for the random spatial distribution of organisms (spatial point pattern analysis). But this type of statistic is also used in Marketing to determine whether the distribution of customers is random or not or in Social Science do determine if social categories of human population are well mixed or not.
	
	\textbf{Definition (\#\thesection.\mydef):} "\NewTerm{Complete spatial randomness (CSR)}\index{complete spatial randomness}" describes a point process whereby point events occur within a given study area in a completely random fashion. It is synonymous with a "\NewTerm{homogeneous spatial Poisson process}\index{homogeneous spatial Poisson process}". Such a process is modelled using only one parameter $\rho$ , i.e. the density of points within the defined area. 
	
	In spatial statistics and related fields, a "\NewTerm{Poisson point process}\index{Poisson point process}" (also named "\NewTerm{Poisson random point field}\index{Poisson random point field}") is a type of random mathematical object that consists of points randomly located on a mathematical space. The point process has convenient mathematical properties, which has led to it being frequently defined in Euclidean space and used as a mathematical model for seemingly random processes in numerous discipline.
	
	Data in the form of a set of points, irregularly distributed within a region of space, arise in many different contexts; examples include locations of trees in a forest, of nests of birds, of nuclei in tissue, of ill people in a population at risk. We name any such data-set a "spatial point pattern" and refer to the locations as "events", to distinguish these from arbitrary points of the region in question. The hypothesis of complete spatial randomness for a spatial point pattern asserts that the number of events in any region follows a Poisson distribution with given mean count per uniform subdivision. The events of a pattern are independently and uniformly distributed over space; in other words, the events are equally likely to occur anywhere and do not interact with each other!
	
	The typical models of random mean distances are based on three fundamental assumptions (hypothesis):
	\begin{itemize}
		\item[H1.] The probability that an element is in a surface region $C$ included in a larger area $S$, such that $C\subset S$, is proportional to the surface $C$ such that:
		
		this assumption seems to be named "\NewTerm{Laplace spatial principle}\index{Laplace spatial principle}".
	
	 	\item[H2.] The position of the elements in a surface $S$ is not influenced by its neighbours in the same surface (no correlation!).
	
		\item[H3.] If the probability that an element is in the surface $C$ is given according to H1 by:
		
		Then the probability that it is not there is given by:
		
	\end{itemize}
	From these three assumptions, we can then assert (but in practice we have to check that this is satisfied!) that the probability of finding $k$ elements in a surface $C\subset S$ over a total of $n$ points is then given by a Binomial law:
	
	In a population of $n$ elements included in a surface $S$ with therefore a density $\rho=n/S$ the distance $r_i$ between each element and its closest neighbour is measured and we denote $\bar{r}$ the mean arithmetic distance such that:
	
	\begin{theorem}
	We will prove in the following that if the distribution of the elements is really random and not-correlated, then the expected mean of the distance is:
	
	And thus the "\NewTerm{randomization ratio criterion}\index{randomization ratio criterion}":
	
	is used as a measure of the degree of departure of the random distribution. If the ratio tends to $1$, then the distribution will be considered random and uncorrelated. If the elements are very aggregated (correlated), then this ratio will tend to $0$.
	\end{theorem} 
	\begin{dem}
	For this we start from:
	
	We recall that we have proved that the Binomial distribution tends towards a Poisson distribution:
	
	with $\mu=np$ and $n\rightarrow +\infty$ and for small values of $k$.
	 
	If a main surface $S$ supposed to be a disc of radius $R$ (yes ... in biology the containers passed through microscopes are often circular in order to avoid corners effects) is cut into $D$ equal secondary surfaces, then we can write:
	
	Which gives, by analysing the units, the average number of points per cut. Since then:
	
	is the probability of finding $k$ elements on a random surface of dimension $\pi R^2 D^{-1}$. If we set $k=0$, then we have the following probability of finding zero elements in a random surface of dimension $\pi R^2 D^{-1}$:
	
	Respectively if we define a disk centered on an existing element, then we have the probability that no other element is in the disc of radius $R$ which is equal to:
	
	Consequently:
	
	Is the probability that a nearest neighbour is beyond the distance $R$. By taking the differential of this last expression, we then have the infinitesimal probability as a function of the distance of having a closer neighbour:
	
	Hence the expected mean of the distance to the nearest neighbour of an element centered in a disc of radius $R$ ranging from $0$ to $+\infty$ is:
	
	Let us put:
	
	We then have:
	
	
	We have already calculated this integral during our study of the Normal law (Gauss-Laplace) and we have proved by integrating by parts that:
	
	and therefore:
	
	and as in practice, we often have $D=1$, it comes then that:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\pagebreak
	\subsection{Surveys}
	"\NewTerm{Survey methodology}\index{survey methodology}" studies the sampling of individual units from a population and the associated survey data collection techniques, such as questionnaire construction and methods for improving the number and accuracy of responses to surveys. Survey methodology includes instruments or procedures that ask one or more questions that may, or may not, be answered.

	Statistical surveys are undertaken with a view towards making statistical inferences about the population being studied, and this depends strongly on the survey questions used. Polls about public opinion, public health surveys, market research surveys, government surveys and censuses are all examples of quantitative research that use contemporary survey methodology to answer questions about a population. Although censuses do not include a "sample", they do include other aspects of survey methodology, like questionnaires, interviewers, and non-response follow-up techniques. Surveys provide important information for all kinds of public information and research fields, e.g., marketing research, psychology, health professionals and sociology.
	
	A single survey is made of at least a sample (or full population in the case of a census), a method of data collection (e.g., a questionnaire) and individual questions or items that become data that can be analysed statistically. A single survey may focus on different types of topics such as preferences (e.g., for a presidential candidate), opinions (e.g., should abortion be legal?), behaviour (smoking and alcohol use), or factual information (e.g., income), depending on its purpose. Since survey research is almost always based on a sample of the population, the success of the research is dependent on the representativeness of the sample with respect to a target population of interest to the researcher. That target population can range from the general population of a given country to specific groups of people within that country, to a membership list of a professional organization, or list of students enrolled in a school system. The persons replying to a survey are named "respondents", and depending on the questions asked their answers may represent themselves as individuals, their households, employers, or other organization they represent.
	
	Survey methodology as a scientific field seeks to identify principles about the sample design, data collection instruments, statistical adjustment of data, and data processing, and final data analysis that can create systematic and random survey errors. Survey errors are sometimes analysed in connection with survey cost. Cost constraints are sometimes framed as improving quality within cost constraints, or alternatively, reducing costs for a fixed level of quality. Survey methodology is both a scientific field and a profession, meaning that some professionals in the field focus on survey errors empirically and others design surveys to reduce them. For survey designers, the task involves making a large set of decisions about thousands of individual features of a survey in order to improve it.
	
	The most important methodological challenges of a survey methodologists include making decisions on how to:
	\begin{itemize}
		\item Identify and select potential sample members.
		\item Contact sampled individuals and collect data from those who are hard to reach (or reluctant to respond)
		\item Evaluate and test questions.
		\item Select the mode for posing questions and collecting responses.
		\item Train and supervise interviewers (if they are involved).
		\item Check data files for accuracy and internal consistency.
		\item Adjust survey estimates to correct for identified errors.
	\end{itemize}
	It is a common trend that many people would doubt about the results of a survey, unless they find proof that the survey was done "scientifically". Well, conducting a survey requires the use of the scientific process, a course that is basically followed by all types of research. Having this in mind, a survey that has critically gone through the steps of the scientific process posits a higher percentage of validity and reliability of the results.

	Not all surveys can be conducted in such a way that each member of the population can be studied upon because that would be a very expensive and thus, an impractical way of doing survey research. In executing a survey, the researcher will select participants through a random sampling technique, and these people will be the representatives of the entire target population. Using a random sampling method does not mean that the survey isn't scientific; rather, it increases the validity of the results as bias in choosing the participants is eliminated, thereby making the process scientific and the results valid. Obviously there are always some traps (the best know one being that a significant percentage of people like to answer randomly and stupidly to surveys, $3\%$ to $30\%$ for the worst case related to politics especially when extremism is involved).
	
	There are two different types of questions that can be used to collect information. The first is named a "\NewTerm{structured or fixed response question}" and the second is named "\NewTerm{non-structured or open question}" and often refers to the concept of "testimonial". It is important to understand when and how to use these questions when designing your survey.

	Validity and reliability are also often discussed in the field of psychometrics, but not so much in market research, although it is assumed they are present.
	
	Does the survey measure what needs to be measured? This is the question that can only be answered through verifying the "validity" of the survey. In scientific research, validity tells us how accurate the survey is by checking the representativeness of the sample and the precision of the questions. There are four important types of validity included survey research:
	\begin{itemize}
		\item Face validity: Do the questions appear reasonable to acquire the data you want to collect?
		\item Content validity: Are the questions all about the issue and other subjects related to it?
		\item Internal validity: Do the questions imply the outcome that you want to achieve from the survey?
		\item External validity: Do the questions elicit answers that are generalizable (i.e. reflects the response of the entire target population)?
	\end{itemize}
	In survey methodology, "reliability" refers to whether the questions elicit similar information or the same characteristic even if the wordings or questionnaire structures are changed. Reliability of the survey relates to the consistency of the questions and statements in a questionnaire.
	
	\pagebreak
	\textbf{Definitions (\#\thesection.\mydef):}
	\begin{enumerate}
		\item[D1.] A "\NewTerm{population}\index{population}" is a finite set of objects on which a study is being done. These objects are named "\NewTerm{individuals / statistical units}". A population is denoted by $U=\{u_1,\ldots,u_N\}$ (the origin of the "$U$" is that of "Universe" as for probabilities) where $N$ is the number of individuals in the population and $i\in \{1,\ldots,N\}$, $u_i$ is the $i$th individual.
	
		\item[D2.] We name "\NewTerm{survey basis}\index{survey basis}" a list that lists all individuals in a population.
	
		\item[D3.] A "\NewTerm{character}" is a quality that is studied in the individuals of a population. A character is denoted $Y$. For all $i\in\{1,\ldots,N\}$, we denote by $y_i$ the value of $Y$ for the individual $u_i$.
	
		\item[D4.] A statistics based on the whole population is in surveys methodology denoted for example $\bar{y}_U$ for the average, or $\sigma_U$ for the standard deviation and so on!
	\end{enumerate}
	To calculate / evaluate population parameters, two common methods are used in survey methodology:
	\begin{itemize}
		\item The "\NewTerm{census}\index{census}": we have access to all individuals and we can measure the values of $Y$ for each of them. However, this is not always possible for reasons of cost, time or because of certain constraints such as the destruction of the individuals studied.
	
		\item The "\NewTerm{survey}\index{survey}": the values of $Y$ are studied on a subset of individuals from the population.
	\end{itemize}
	
	\textbf{Definition (\#\thesection.\mydef):} A sample is a subset of individuals from a population. A sample is denoted in survey methodology $\omega$!

	To constitute a representative sample of the population at least two questions arises:
	\begin{itemize}
		\item How do we proceed?
		\item How many individuals should be chosen?
	\end{itemize}
	
	What follows aims to provide answers to these questions!
	
	
	
	\subsubsection{Survey designs}
	\textbf{Definitions (\#\thesection.\mydef):}
	
	A "\NewTerm{survey design}\index{survey design}" or more commonly a "\NewTerm{sampling design}\index{sampling design}" is a procedure for selecting a sample In a population. A sample design is said to be:
	\begin{itemize}
		\item[D1.] A "\NewTerm{random sample}" if each individual in the population has a known probability of being in the sample.
	
		\item[D2.] A "\NewTerm{simple sample}" if each individual has the same probability as another to be selected. The probabilities are then equal at such design is denoted RSEP for "random sample with equal probabilities".
	
		\item[D3.] A "\NewTerm{random sample without redrawal}", abbreviated RSWOR, if a single individual can only appear once in the sample.
	
		\item[D4.] A "\NewTerm{random sample with redrawal}", abbreviated RSWR, if the same individual can appear several times in the sample and If the order in which the individuals appear counts.
	\end{itemize}
	\begin{tcolorbox}[title=Remark,arc=10pt,breakable,drop lifted shadow,
  skin=enhanced,
  skin first is subskin of={enhancedfirst}{arc=10pt,no shadow},
  skin middle is subskin of={enhancedmiddle}{arc=10pt,no shadow},
  skin last is subskin of={enhancedlast}{drop lifted shadow}]
	Usual estimation formulas in survey methodology are associated with a random sampling design with rethdrawal (RSWR). To simplify the situation, they are generally used with the without redrawal (RSWOR) case when $n$ is much smaller than $N$. An existing convention is $N\ge 10n$.
	\end{tcolorbox}
	In fact there is a lot more of sampling methods. Here is a quite exhaustive view of them:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/arithmetics/sampling_methods.jpg}	
		\caption{Some common sampling methods}
	\end{figure}
	More in details (they almost used all advanced specific mathematics techniques):
	\begin{itemize}
	\item Non-probability sampling techniques\index{non-probability sampling techniques}:
		\begin{itemize}
			\item "\NewTerm{Quota sampling}\index{quota sampling}": In quota sampling, a population is first segmented into mutually exclusive sub-groups. Then judgement is used to select the subjects or units from each segment based on a specified proportion without respecting the other group attributes (properties). What makes it non-random is that it is human that will select the items of the groups. 
			
			\item "\NewTerm{Judgemental sampling}\index{judgemental sampling}\index{Haphazard sampling}": In Judgement sample a non-random sample is selected based on the opinion of an expert. Results obtained from a judgement sample are subject to some degree of bias, due to the frame and population not being identical (in proportion AND in attributes) but it can also be a good double-check method after a random type sampling. The frame is a list of all the units, items, people, etc., that define the population to be studied. It is also assimilated to a haphazard sampling as the expert will try to make it random without the use of the computer but in fact the sample will look a lot different from a real random sample.

			\item "\NewTerm{Convenience sampling}\index{convenience sampling}\index{opportunity sampling}": A convenience sample is a type of non-probability sampling method where the sample is taken from a group of people easy to contact or to reach. For example, standing at a mall or a grocery store and asking people to answer questions would be an example of a convenience sample. This type of sampling is also known as grab sampling or availability sampling. There are no other criteria to the sampling method except that people be available and willing to participate. In addition, this type of sampling method does not require that a simple random sample is generated, since the only criteria is whether the participants agree to participate.
			
			\item "\NewTerm{Snowball sampling}\index{snowball sampling}\index{volunteer sampling}":  A snowball sampling is a  technique where existing study subjects recruit future subjects from among their acquaintances. Thus the sample group is said to grow like a rolling snowball. As the sample builds up, enough data are gathered to be useful for research. This sampling technique is often used in hidden populations, such as drug users or sex workers, which are difficult for researchers to access. As sample members are not selected from a sampling frame, snowball samples are subject to numerous biases. 
		\end{itemize}
	\item Probability sampling techniques\index{probability sampling techniques}:
		\begin{itemize}
			\item "\NewTerm{Simple random sampling}\index{simple random sampling}": A simple random sample is a subset of individuals (a sample) chosen from a larger set (a population). Each individual is chosen randomly and entirely by chance, such that each individual has the same probability of being chosen at any stage during the sampling process, and each subset of $k$ individuals has the same probability of being chosen for the sample as any other subset of $k$ individuals.
			
			\item "\NewTerm{Systematic sampling}\index{systematic sampling}": Sequential sampling is a non-probability sampling technique wherein we pick a single or a group of subjects in a given time interval, we conducts the study, analyses the results then picks another group of subjects if needed and so on. It is used by physicians and also by industrial quality engineers (especially for control charts).
			
			\item "\NewTerm{Stratified random sampling}\index{stratified random sampling}": Stratified random sampling is a method of sampling that involves the division of a population into smaller groups known as "\NewTerm{strata}\index{strata}". In stratified random sampling, the strata are formed based on members' shared attributes or characteristics. A random sample from each stratum is taken in a number proportional to the stratum's size when compared to the population. These subsets of the strata are then pooled to form a random sample.
			
			\item "\NewTerm{Cluster sampling}\index{cluster sampling}": Cluster sampling is a sampling plan used when mutually homogeneous yet internally heterogeneous groupings are evident in a statistical population. It is often used in marketing research. In this sampling plan, the total population is divided into these groups (known as clusters) and a simple random sample of the groups is selected. The elements in each cluster are then sampled. If all elements in each sampled cluster are sampled, then this is referred to as a "one-stage" cluster sampling plan. If a simple random subsample of elements is selected within each of these groups, this is referred to as a "two-stage" cluster sampling plan. 
			\includegraphics[scale=0.8]{img/arithmetics/stratified_cluster_sampling.jpg}
			
			\item "\NewTerm{Restricted random sampling}\index{restricted random sampling}": There is not general consensus about the definition. But in the framework of this book, a restricted random sampling will be related to a sampling with redrawal of a design of experiment matrix where the purpose is to avoid undesirable cofactors (lurking variables).
			
			\item "\NewTerm{Size proportional sampling}\index{size proportional sampling}": When information on a size measure $G$ exists for every element in the population and this size measure stores valuable information about the importance of element $i$ to be included in the sample, we can use this information in the sample design. Sample designs that make explicit use of such size measures are named probability proportional to size\footnote{The probability will be $\pi_i^{\text{pps}}=\frac{G_i}{\sum_i G_i}$} (PPS) sample designs. Sample designs with pps are often used in business surveys when it is important to include the largest firms in an industry in the sample since they contribute a large amount to the industry's production of goods or services. 
			
			\item "\NewTerm{Unequal probability sampling}\index{unequal probability sampling}": Unequal probability sampling is more a family of sampling techniques. For example, designs with pps are unequal probability sampling, but also judgement sampling and convenience sampling and so on...
			
			\item "\NewTerm{Two-stage sampling}\index{two-stage sampling}\index{double sampling}": The two-stage sampling also refers to cluster sampling but in the industry it the idea of sampling a batch a second time in the idea to give it a second chance if during the first sampling it was rejected.
			
			\item "\NewTerm{Spatially-balanced sampling}\index{spatially-balanced sampling}": Spatially-balanced refers to samples that are evenly distributed across a study area. Spatially balanced sampling is much more efficient than simple random sampling if the population being sampled is more-or-less evenly distributed across the area being sampled. While a systematic sample design can achieve complete spatial balance, it lacks randomization that is desirable in statistical sample designs and it is difficult to apply when the units being selected for sampling are not contiguous within the study area (e.g., selecting lakes or wetlands to sample). There are several different techniques for creating spatially-balanced sample designs, but one of the most common ones is the Generalized Random-Tessellation Stratified (GRTS) design described by Stevens and Olsen.
			
			\item "\NewTerm{Adaptive sampling}\index{adaptive sampling}": Adaptive sampling refers to a technique where the sample design is modified in the field based on observations made at a set of pre-selected sampling units. Perhaps the best way to describe adaptive sampling is through an example. Consider sampling for the presence or abundance of rare plants. A random selection of sample units will yield many sample units where the plant is not detected, but the rare plant is likely to occur in sample units nearby to those units where it was detected. With adaptive sampling, the detection of the rare plant at one site triggers the selection and sampling of additional nearby sites that were not originally selected as part of the sample set. Thus the biggest difference between adaptive sampling and many other random selection techniques is that the observed conditions at one sampling unit influence the selection of other sampling units.
			
			\item "\NewTerm{Over-sampling}\index{over-sampling}" and "\NewTerm{under-sampling}\index{under-sampling}": Unbalanced data is sometimes an issues in the field of Machine Learning. With under-sampling, we randomly select a subset of samples from the class with more instances to match the number of samples coming from each class. With oversampling, we randomly duplicate samples from the class with fewer instances or we generate additional instances based on the data that we have, so as to match the number of samples in each class.
			\begin{figure}[H]
				\centering
				\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
				%uncomment if require: \path (0,562); %set diagram left start at 0, and has height of 562
				
				%Shape: Rectangle [id:dp30929937401412233] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (81,289.28) -- (124.86,289.28) -- (124.86,310.28) -- (81,310.28) -- cycle ;
				%Shape: Rectangle [id:dp20798422426022434] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (81,264.7) -- (124.86,264.7) -- (124.86,285.7) -- (81,285.7) -- cycle ;
				%Shape: Rectangle [id:dp754491068989658] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (81,240.13) -- (124.86,240.13) -- (124.86,261.13) -- (81,261.13) -- cycle ;
				%Shape: Rectangle [id:dp8818553809517837] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (81,215.56) -- (124.86,215.56) -- (124.86,236.56) -- (81,236.56) -- cycle ;
				%Shape: Rectangle [id:dp09972675119702523] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (81,190.99) -- (124.86,190.99) -- (124.86,211.99) -- (81,211.99) -- cycle ;
				%Shape: Rectangle [id:dp9316972265015122] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (81,166.42) -- (124.86,166.42) -- (124.86,187.42) -- (81,187.42) -- cycle ;
				%Shape: Rectangle [id:dp674108284740897] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 245; green, 102; blue, 119 }  ,fill opacity=1 ] (81,137.85) -- (124.86,137.85) -- (124.86,158.85) -- (81,158.85) -- cycle ;
				%Shape: Rectangle [id:dp2996298029203248] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 245; green, 102; blue, 119 }  ,fill opacity=1 ] (81,113.28) -- (124.86,113.28) -- (124.86,134.28) -- (81,134.28) -- cycle ;
				%Shape: Rectangle [id:dp9975617139492348] 
				\draw  [dash pattern={on 4.5pt off 4.5pt}] (75.86,107.14) -- (129.86,107.14) -- (129.86,162.42) -- (75.86,162.42) -- cycle ;
				%Shape: Rectangle [id:dp10669976151393712] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (176,289.28) -- (219.86,289.28) -- (219.86,310.28) -- (176,310.28) -- cycle ;
				%Shape: Rectangle [id:dp3705392387916484] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (176,264.7) -- (219.86,264.7) -- (219.86,285.7) -- (176,285.7) -- cycle ;
				%Shape: Rectangle [id:dp4025455874036419] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (176,240.13) -- (219.86,240.13) -- (219.86,261.13) -- (176,261.13) -- cycle ;
				%Shape: Rectangle [id:dp8398166816490948] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (176,215.56) -- (219.86,215.56) -- (219.86,236.56) -- (176,236.56) -- cycle ;
				%Shape: Rectangle [id:dp9072133720846542] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (176,190.99) -- (219.86,190.99) -- (219.86,211.99) -- (176,211.99) -- cycle ;
				%Shape: Rectangle [id:dp7985136446848728] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (176,166.42) -- (219.86,166.42) -- (219.86,187.42) -- (176,187.42) -- cycle ;
				%Shape: Rectangle [id:dp3494483815177438] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 245; green, 102; blue, 119 }  ,fill opacity=1 ] (176,137.85) -- (219.86,137.85) -- (219.86,158.85) -- (176,158.85) -- cycle ;
				%Shape: Rectangle [id:dp5258456559450453] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 245; green, 102; blue, 119 }  ,fill opacity=1 ] (176,113.12) -- (219.86,113.12) -- (219.86,134.12) -- (176,134.12) -- cycle ;
				%Shape: Rectangle [id:dp38672444080396273] 
				\draw  [dash pattern={on 4.5pt off 4.5pt}] (170.86,11.28) -- (224.86,11.28) -- (224.86,162.42) -- (170.86,162.42) -- cycle ;
				%Straight Lines [id:da6313422632833725] 
				\draw    (130,136) -- (167.86,136) ;
				\draw [shift={(170.86,136)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
				%Shape: Rectangle [id:dp6818521022698221] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 245; green, 102; blue, 119 }  ,fill opacity=1 ] (176,88.41) -- (219.86,88.41) -- (219.86,109.41) -- (176,109.41) -- cycle ;
				%Shape: Rectangle [id:dp6134916919188576] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 245; green, 102; blue, 119 }  ,fill opacity=1 ] (176,63.7) -- (219.86,63.7) -- (219.86,84.7) -- (176,84.7) -- cycle ;
				%Shape: Rectangle [id:dp8533342286319583] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 245; green, 102; blue, 119 }  ,fill opacity=1 ] (176,38.99) -- (219.86,38.99) -- (219.86,59.99) -- (176,59.99) -- cycle ;
				%Shape: Rectangle [id:dp3992520525840557] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 245; green, 102; blue, 119 }  ,fill opacity=1 ] (176,14.28) -- (219.86,14.28) -- (219.86,35.28) -- (176,35.28) -- cycle ;
				%Shape: Rectangle [id:dp4379602338890085] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (283,289.28) -- (326.86,289.28) -- (326.86,310.28) -- (283,310.28) -- cycle ;
				%Shape: Rectangle [id:dp17573337158005953] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (283,264.7) -- (326.86,264.7) -- (326.86,285.7) -- (283,285.7) -- cycle ;
				%Shape: Rectangle [id:dp319512373377248] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (283,240.13) -- (326.86,240.13) -- (326.86,261.13) -- (283,261.13) -- cycle ;
				%Shape: Rectangle [id:dp8840260056185021] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (283,215.56) -- (326.86,215.56) -- (326.86,236.56) -- (283,236.56) -- cycle ;
				%Shape: Rectangle [id:dp19108146588946684] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (283,190.99) -- (326.86,190.99) -- (326.86,211.99) -- (283,211.99) -- cycle ;
				%Shape: Rectangle [id:dp6362969205966031] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (283,166.42) -- (326.86,166.42) -- (326.86,187.42) -- (283,187.42) -- cycle ;
				%Shape: Rectangle [id:dp25471732915959966] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 245; green, 102; blue, 119 }  ,fill opacity=1 ] (283,137.85) -- (326.86,137.85) -- (326.86,158.85) -- (283,158.85) -- cycle ;
				%Shape: Rectangle [id:dp4593243106633875] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 245; green, 102; blue, 119 }  ,fill opacity=1 ] (283,113.28) -- (326.86,113.28) -- (326.86,134.28) -- (283,134.28) -- cycle ;
				%Shape: Rectangle [id:dp6247374617045236] 
				\draw  [dash pattern={on 4.5pt off 4.5pt}] (277.86,163.28) -- (331.86,163.28) -- (331.86,314.28) -- (277.86,314.28) -- cycle ;
				%Shape: Rectangle [id:dp366247757721343] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (379,289.28) -- (422.86,289.28) -- (422.86,310.28) -- (379,310.28) -- cycle ;
				%Shape: Rectangle [id:dp5640745663131572] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (379,264.7) -- (422.86,264.7) -- (422.86,285.7) -- (379,285.7) -- cycle ;
				%Shape: Rectangle [id:dp007058365152331936] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 245; green, 102; blue, 119 }  ,fill opacity=1 ] (379,234.85) -- (422.86,234.85) -- (422.86,255.85) -- (379,255.85) -- cycle ;
				%Shape: Rectangle [id:dp09648788173079326] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 245; green, 102; blue, 119 }  ,fill opacity=1 ] (379,210.28) -- (422.86,210.28) -- (422.86,231.28) -- (379,231.28) -- cycle ;
				%Shape: Rectangle [id:dp5961290010711195] 
				\draw  [dash pattern={on 4.5pt off 4.5pt}] (373.86,260.28) -- (427.86,260.28) -- (427.86,314.28) -- (373.86,314.28) -- cycle ;
				%Straight Lines [id:da45060223276614964] 
				\draw    (333.14,285.7) -- (371,285.7) ;
				\draw [shift={(374,285.7)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
				
				% Text Node
				\draw (110,140) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{1,1,1}{B}}};
				% Text Node
				\draw (110,115.85) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{1,1,1}{A}}};
				% Text Node
				\draw (78,322) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {Original\\Dataset};
				% Text Node
				\draw (53.98,265) node [anchor=north west][inner sep=0.75pt]  [rotate=-270.02] [align=left] {{\footnotesize Majority Class}};
				% Text Node
				\draw (52.98,170) node [anchor=north west][inner sep=0.75pt]  [rotate=-270.02] [align=left] {{\footnotesize Minority Class}};
				% Text Node
				\draw (205,140) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{1,1,1}{B}}};
				% Text Node
				\draw (205,115.85) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{1,1,1}{A}}};
				% Text Node
				\draw (162,322) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {\begin{minipage}[lt]{60pt}\setlength\topsep{0pt}
				\begin{center}
				Re-sampled\\Dataset
				\end{center}
				
				\end{minipage}};
				% Text Node
				\draw (205,91) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{1,1,1}{B}}};
				% Text Node
				\draw (205,66.85) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{1,1,1}{A}}};
				% Text Node
				\draw (205,42) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{1,1,1}{B}}};
				% Text Node
				\draw (205,16.85) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{1,1,1}{A}}};
				% Text Node
				\draw (280,322) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {Original\\Dataset};
				% Text Node
				\draw (255.98,265) node [anchor=north west][inner sep=0.75pt]  [rotate=-270.02] [align=left] {{\footnotesize Majority Class}};
				% Text Node
				\draw (254.98,170) node [anchor=north west][inner sep=0.75pt]  [rotate=-270.02] [align=left] {{\footnotesize Minority Class}};
				% Text Node
				\draw (95,360) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{85pt}\setlength\topsep{0pt}
				\begin{center}
				\textbf{Over-Sampling}\\\textbf{Technique}
				\end{center}
				
				\end{minipage}};
				% Text Node
				\draw (297,360) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{90pt}\setlength\topsep{0pt}
				\begin{center}
				\textbf{Under-Sampling}\\\textbf{Technique}
				\end{center}
				
				\end{minipage}};
				% Text Node
				\draw (311,139) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{1,1,1}{B}}};
				% Text Node
				\draw (311,114.85) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{1,1,1}{A}}};
				% Text Node
				\draw (407,237) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{1,1,1}{B}}};
				% Text Node
				\draw (407,212.85) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{1,1,1}{A}}};
				% Text Node
				\draw (365,322) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {\begin{minipage}[lt]{60pt}\setlength\topsep{0pt}
				\begin{center}
				Re-sampled\\Dataset
				\end{center}
				\end{minipage}};
				\end{tikzpicture}
				\vspace*{3mm}
				\caption{Over and under-sampling underlying idea}
			\end{figure}
			
			\item Besides over- and under-sampling, there are hybrid methods. Two of the most popular are "ROSE" (Random Over-Sampling Examples)\index{ROSE (Random Over-Sampling Examples)} and "SMOTe" (Synthetic Minority Over-sampling Technique)\index{SMOTe (Synthetic Minority Over-sampling Technique)}.
			\begin{figure}[H]
				\centering
				\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
				%uncomment if require: \path (0,562); %set diagram left start at 0, and has height of 562
				
				%Shape: Rectangle [id:dp30929937401412233] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (183,356.28) -- (226.86,356.28) -- (226.86,377.28) -- (183,377.28) -- cycle ;
				%Shape: Rectangle [id:dp20798422426022434] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (183,331.7) -- (226.86,331.7) -- (226.86,352.7) -- (183,352.7) -- cycle ;
				%Shape: Rectangle [id:dp754491068989658] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (183,307.13) -- (226.86,307.13) -- (226.86,328.13) -- (183,328.13) -- cycle ;
				%Shape: Rectangle [id:dp8818553809517837] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (183,282.56) -- (226.86,282.56) -- (226.86,303.56) -- (183,303.56) -- cycle ;
				%Shape: Rectangle [id:dp09972675119702523] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (183,257.99) -- (226.86,257.99) -- (226.86,278.99) -- (183,278.99) -- cycle ;
				%Shape: Rectangle [id:dp9316972265015122] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (183,233.42) -- (226.86,233.42) -- (226.86,254.42) -- (183,254.42) -- cycle ;
				%Shape: Rectangle [id:dp674108284740897] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 245; green, 102; blue, 119 }  ,fill opacity=1 ] (183,204.85) -- (226.86,204.85) -- (226.86,225.85) -- (183,225.85) -- cycle ;
				%Shape: Rectangle [id:dp2996298029203248] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 245; green, 102; blue, 119 }  ,fill opacity=1 ] (183,180.28) -- (226.86,180.28) -- (226.86,201.28) -- (183,201.28) -- cycle ;
				%Shape: Rectangle [id:dp9975617139492348] 
				\draw  [dash pattern={on 4.5pt off 4.5pt}] (177.86,176.28) -- (231.86,176.28) -- (231.86,229.42) -- (177.86,229.42) -- cycle ;
				%Shape: Rectangle [id:dp10669976151393712] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (278,356.28) -- (321.86,356.28) -- (321.86,377.28) -- (278,377.28) -- cycle ;
				%Shape: Rectangle [id:dp3705392387916484] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (278,331.7) -- (321.86,331.7) -- (321.86,352.7) -- (278,352.7) -- cycle ;
				%Shape: Rectangle [id:dp4025455874036419] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (278,307.13) -- (321.86,307.13) -- (321.86,328.13) -- (278,328.13) -- cycle ;
				%Shape: Rectangle [id:dp8398166816490948] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (278,282.56) -- (321.86,282.56) -- (321.86,303.56) -- (278,303.56) -- cycle ;
				%Shape: Rectangle [id:dp9072133720846542] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (278,257.99) -- (321.86,257.99) -- (321.86,278.99) -- (278,278.99) -- cycle ;
				%Shape: Rectangle [id:dp7985136446848728] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (278,233.42) -- (321.86,233.42) -- (321.86,254.42) -- (278,254.42) -- cycle ;
				%Shape: Rectangle [id:dp3494483815177438] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 245; green, 102; blue, 119 }  ,fill opacity=1 ] (278,204.85) -- (321.86,204.85) -- (321.86,225.85) -- (278,225.85) -- cycle ;
				%Shape: Rectangle [id:dp5258456559450453] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 245; green, 102; blue, 119 }  ,fill opacity=1 ] (278,180.12) -- (321.86,180.12) -- (321.86,201.12) -- (278,201.12) -- cycle ;
				%Shape: Rectangle [id:dp38672444080396273] 
				\draw  [dash pattern={on 4.5pt off 4.5pt}] (272.86,178.28) -- (326.86,178.28) -- (326.86,229.42) -- (272.86,229.42) -- cycle ;
				%Straight Lines [id:da6313422632833725] 
				\draw    (232,203) -- (269.86,203) ;
				\draw [shift={(272.86,203)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
				%Shape: Rectangle [id:dp6818521022698221] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 247; green, 181; blue, 80 }  ,fill opacity=1 ] (278,152.41) -- (321.86,152.41) -- (321.86,173.41) -- (278,173.41) -- cycle ;
				%Shape: Rectangle [id:dp6134916919188576] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 247; green, 181; blue, 80 }  ,fill opacity=1 ] (278,127.7) -- (321.86,127.7) -- (321.86,148.7) -- (278,148.7) -- cycle ;
				%Shape: Rectangle [id:dp8533342286319583] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 247; green, 181; blue, 80 }  ,fill opacity=1 ] (278,102.99) -- (321.86,102.99) -- (321.86,123.99) -- (278,123.99) -- cycle ;
				%Shape: Rectangle [id:dp3992520525840557] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 247; green, 181; blue, 80 }  ,fill opacity=1 ] (278,78.28) -- (321.86,78.28) -- (321.86,99.28) -- (278,99.28) -- cycle ;
				%Shape: Rectangle [id:dp4379602338890085] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (385,356.28) -- (428.86,356.28) -- (428.86,377.28) -- (385,377.28) -- cycle ;
				%Shape: Rectangle [id:dp17573337158005953] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (385,331.7) -- (428.86,331.7) -- (428.86,352.7) -- (385,352.7) -- cycle ;
				%Shape: Rectangle [id:dp319512373377248] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (385,307.13) -- (428.86,307.13) -- (428.86,328.13) -- (385,328.13) -- cycle ;
				%Shape: Rectangle [id:dp8840260056185021] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (385,282.56) -- (428.86,282.56) -- (428.86,303.56) -- (385,303.56) -- cycle ;
				%Shape: Rectangle [id:dp19108146588946684] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (385,257.99) -- (428.86,257.99) -- (428.86,278.99) -- (385,278.99) -- cycle ;
				%Shape: Rectangle [id:dp6362969205966031] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (385,233.42) -- (428.86,233.42) -- (428.86,254.42) -- (385,254.42) -- cycle ;
				%Shape: Rectangle [id:dp25471732915959966] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 245; green, 102; blue, 119 }  ,fill opacity=1 ] (385,204.85) -- (428.86,204.85) -- (428.86,225.85) -- (385,225.85) -- cycle ;
				%Shape: Rectangle [id:dp4593243106633875] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 245; green, 102; blue, 119 }  ,fill opacity=1 ] (385,180.28) -- (428.86,180.28) -- (428.86,201.28) -- (385,201.28) -- cycle ;
				%Shape: Rectangle [id:dp6247374617045236] 
				\draw  [dash pattern={on 4.5pt off 4.5pt}] (379.86,231.28) -- (433.86,231.28) -- (433.86,382.28) -- (379.86,382.28) -- cycle ;
				%Shape: Rectangle [id:dp366247757721343] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (481,356.28) -- (524.86,356.28) -- (524.86,377.28) -- (481,377.28) -- cycle ;
				%Shape: Rectangle [id:dp5640745663131572] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 237; blue, 116 }  ,fill opacity=1 ] (481,331.7) -- (524.86,331.7) -- (524.86,352.7) -- (481,352.7) -- cycle ;
				%Shape: Rectangle [id:dp007058365152331936] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 245; green, 102; blue, 119 }  ,fill opacity=1 ] (481,301.85) -- (524.86,301.85) -- (524.86,322.85) -- (481,322.85) -- cycle ;
				%Shape: Rectangle [id:dp09648788173079326] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 245; green, 102; blue, 119 }  ,fill opacity=1 ] (481,277.28) -- (524.86,277.28) -- (524.86,298.28) -- (481,298.28) -- cycle ;
				%Shape: Rectangle [id:dp5961290010711195] 
				\draw  [dash pattern={on 4.5pt off 4.5pt}] (475.86,327.28) -- (529.86,327.28) -- (529.86,381.28) -- (475.86,381.28) -- cycle ;
				%Straight Lines [id:da45060223276614964] 
				\draw    (435.14,352.7) -- (473,352.7) ;
				\draw [shift={(476,352.7)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
				%Shape: Rectangle [id:dp511166015417805] 
				\draw  [dash pattern={on 4.5pt off 4.5pt}] (272.86,74.28) -- (326.86,74.28) -- (326.86,175.28) -- (272.86,175.28) -- cycle ;
				%Shape: Rectangle [id:dp2984126183002529] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 245; green, 102; blue, 119 }  ,fill opacity=1 ] (481,252.85) -- (524.86,252.85) -- (524.86,273.85) -- (481,273.85) -- cycle ;
				%Shape: Rectangle [id:dp869417945347635] 
				\draw  [draw opacity=0][fill={rgb, 255:red, 245; green, 102; blue, 119 }  ,fill opacity=1 ] (481,228.28) -- (524.86,228.28) -- (524.86,249.28) -- (481,249.28) -- cycle ;
				%Shape: Rectangle [id:dp784705932134184] 
				\draw  [dash pattern={on 4.5pt off 4.5pt}] (475.86,223.28) -- (529.86,223.28) -- (529.86,324.28) -- (475.86,324.28) -- cycle ;
				%Shape: Rectangle [id:dp9324669734108677] 
				\draw  [dash pattern={on 4.5pt off 4.5pt}] (379.86,177.14) -- (433.86,177.14) -- (433.86,228.28) -- (379.86,228.28) -- cycle ;
				%Straight Lines [id:da9434358314220583] 
				\draw    (434.14,204.7) -- (473.36,272.68) ;
				\draw [shift={(474.86,275.28)}, rotate = 240.01] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
				
				% Text Node
				\draw (212,207) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{1,1,1}{B}}};
				% Text Node
				\draw (212,182.85) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{1,1,1}{A}}};
				% Text Node
				\draw (180,389) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {Original\\Dataset};
				% Text Node
				\draw (155.98,332) node [anchor=north west][inner sep=0.75pt]  [rotate=-270.02] [align=left] {{\footnotesize Majority Class}};
				% Text Node
				\draw (154.98,237) node [anchor=north west][inner sep=0.75pt]  [rotate=-270.02] [align=left] {{\footnotesize Minority Class}};
				% Text Node
				\draw (307,207) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{1,1,1}{B}}};
				% Text Node
				\draw (307,182.85) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{1,1,1}{A}}};
				% Text Node
				\draw (264,389) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {\begin{minipage}[lt]{60pt}\setlength\topsep{0pt}
				\begin{center}
				Re-sampled\\Dataset
				\end{center}
				
				\end{minipage}};
				% Text Node
				\draw (307,155) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{1,1,1}{...}}};
				% Text Node
				\draw (307,130.85) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{1,1,1}{...}}};
				% Text Node
				\draw (307,106) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{1,1,1}{...}}};
				% Text Node
				\draw (307,80.85) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{1,1,1}{...}}};
				% Text Node
				\draw (382,389) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {Original\\Dataset};
				% Text Node
				\draw (357.98,332) node [anchor=north west][inner sep=0.75pt]  [rotate=-270.02] [align=left] {{\footnotesize Majority Class}};
				% Text Node
				\draw (356.98,237) node [anchor=north west][inner sep=0.75pt]  [rotate=-270.02] [align=left] {{\footnotesize Minority Class}};
				% Text Node
				\draw (210,430) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{52.96pt}\setlength\topsep{0pt}
				\begin{center}
				\textbf{SMOTe}\\\textbf{Technique}
				\end{center}
				
				\end{minipage}};
				% Text Node
				\draw (416,430) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{52.96pt}\setlength\topsep{0pt}
				\begin{center}
				\textbf{ROSE}\\\textbf{Technique}
				\end{center}
				
				\end{minipage}};
				% Text Node
				\draw (413,206) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{1,1,1}{B}}};
				% Text Node
				\draw (413,181.85) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{1,1,1}{A}}};
				% Text Node
				\draw (509,304) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{1,1,1}{B}}};
				% Text Node
				\draw (509,279.85) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{1,1,1}{A}}};
				% Text Node
				\draw (467,389) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {\begin{minipage}[lt]{60pt}\setlength\topsep{0pt}
				\begin{center}
				Re-sampled\\Dataset
				\end{center}
				
				\end{minipage}};
				% Text Node
				\draw (250.98,183) node [anchor=north west][inner sep=0.75pt]  [rotate=-270.02] [align=left] {{\footnotesize Syntetic Minority Class}};
				% Text Node
				\draw (509,254) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{1,1,1}{B}}};
				% Text Node
				\draw (509,229.85) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{\textcolor[rgb]{1,1,1}{A}}};
				\end{tikzpicture}
				\vspace*{3mm}
				\caption{SMOTe and ROSE sampling underlying idea}
			\end{figure}
			SMOTe, takes pairs of examples from the minority class, creates lines between these pairs, and takes points from these interconnected lines as synthetic data. To visualize SMOTe in action, the minority class (green dots) is $5\%$ of the data, versus the majority class (blue dots) at $95\%$ of the data. The SMOTe technique generates synthetic data (red dots) from the minority class, which can be used to train the algorithm to better identify the minority class:
			\begin{figure}[H]
				\centering
				\includegraphics{img/arithmetics/smote.jpg}
				\vspace*{1mm}
				\caption[]{SMOTe example in $\mathbb{R}^2$}
			\end{figure}
		\end{itemize}
	\end{itemize}
	There are very good theoretical textbooks and practical textbooks on these sampling topics of respectively more than 600 pages like \cite{lohr2021sampling} and more than 200 pages like \cite{lu2021r}.
	
	\pagebreak
	\paragraph{Simple random sampling design without replacement and equal probabilities (RSEPR)}\mbox{}\\\\
	The simple random sampling design without replacement and equal probabilities it the most easy one to execute in the reality and the cheaper in matter of time and costs. However it has the issue (that we have to solve later) to not respect the original proportions of the population for some characters (properties) that we know as being potentially explanatory variables for our survey result! Therefore this type of sampling if frequent in industrial cases but when the survey is related to socials subjects (politics, marketing, etc.) it should be avoided.
	
	For such a design we sample $n$ individuals without redrawal from a population $U$ with $N$ individuals. Then the equal probability will be denoted according to the number of combinations without redrawal (\SeeChapter{see section Probabilities page \pageref{choice function}}):
	
	with $\omega\in W(\Omega)$ where $P$ denotes the equal probability and $W(\Omega)$ denotes the set of all samples of $n$ possible individuals with such a design.
	
	We name the "\NewTerm{sampling rate} the ratio:
	
	In the case of the equal probability sampling without replacement, the probability that the individual belonging to $W$ is obviously:
	
	for all $i\in\{1,\ldots,N\}$. By extension, obviously, if $(i,j)\in \{1,\ldots,N\}^2$ with $i\neq j$, the probability that $u_i$ and $u_j$ belongs to $W$ are therefore immediately given by (we assume it is obvious enough so that we don't need to give the proof):
	
	What will follow now is obvious but we wrote it to show that the notation is a bit specific the field of survey methodology.

	The estimator of the mean on the sample is denoted:
	
	or it is also sometimes written:
	
	where $S=\{(i_1,\ldots,i_n)\in\{1,\ldots,N\}^n,i_1\neq \ldots\neq i_n; u_{i_1}\in W,\ldots,u_{i_n}\in W\}$.
	
	We have already proved long time earlier that $\bar{y}_W$ is the unbiased estimator of $\bar{y}_U$.
	
	\subsubsection{Survey Sampling Error and Non-Sampling Error}
	The U.S. presidential elections in 12016 (holocene calendar) and the U.K.'s European Union referendum in 12016 (holocene calendar) have thrown election polling into the public spotlight at the time. And with so many reports of polls being skewed, it's time to talk about what causes pollsters and polling agencies to get things so wrong.

	Let's use Brexit as an example. On June 23 12006 (holocene calendar), Great Britain held a referendum, and voters ultimately elected to leave the European Union. Many polling agencies incorrectly predicted that Britain would stay in the EU, while others were right on the money. Why the disparity? What were these two groups doing differently?
	
	Statistical sampling — as opposed to wild guessing — provides a measurement of how certain the polling agency is about their predictions. When making projections based on polls, statisticians report a margin of error so that they can state a $95\%$ confidence interval; a useful tool in its own right. For example, YouGov estimated that $52\%$ of Britain would vote to leave the EU, and gave a margin of error of $2\%$. In that case, YouGov was saying that they were $95\%$ certain that the true percentage of people voting to leave the EU was $52\%$, plus or minus $2\%$. However, these intervals - including margin of error - are often misused.  
	
	The industry secret is that the reported margin of error only includes sampling error (random error), yet ignores what is conveniently named "non-sampling error" (systematic error). Imagine five forces simultaneously pushing and pulling a poll's estimate in either direction — forces that are not being reported by the pollster. These systematic forces include:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/arithmetics/survey_errors.jpg}
		\caption{Type of errors in surveys methodology}
	\end{figure}
	\begin{itemize}
		\item "\NewTerm{Systematic error}\index{systematic error}", or bias, results from mistakes or problems in the research design or from flaws in the execution of the sample design. Systematic error exists in the results of a sample if those results show a consistent tendency to vary in one direction (consistently higher or consistently lower) from the true value of the population parameter. Systematic error includes all sources of error except those introduced by the random sampling process. Therefore, systematic errors are sometimes named "\NewTerm{nonsampling errors}\index{nonsampling errors}". The nonsampling errors that can systematically influence survey answers can be categorized as sample design error and measurement error.
	
		\item "\NewTerm{Sample design error}\index{sample design error}" is a systematic error that results from a problem in the sample design or sampling procedures. Types of sample design errors include frame errors, population specification errors, and selection errors and are presented below.
	
		\item The sampling frame is the list of population elements or members from which units to be sampled are selected. "\NewTerm{Frame error}\index{frame error}" results from using an incomplete or inaccurate sampling frame. The problem is that a sample drawn from a list that is subject to frame error may not be a true cross section of the target population. A common source of frame error in marketing research is the use of a published telephone directory as a sampling frame for a telephone survey (or typically Internet online surveys where a huge group of people having a special interest - or being especially concerned by the subject of the survey - could massively share the link to bias the survey result). Many households are not listed in a current telephone book because they do not want to be listed or are not listed accurately because they have recently moved or changed their telephone number. Research has shown that those people who are listed in telephone directories are systematically different from those who are not listed in certain important ways, such as socioeconomic levels. This means that if a study purporting to represent the opinions of all households in a particular area is based on listings in the current telephone directory, it will be subject to frame error. 
	
		\item "\NewTerm{Population specification error}\index{population specification error}" results from an incorrect definition of the population or universe from which the sample is to be selected. For example, suppose a researcher defined the population or universe for a study as people over the age of $35$. Later, it was determined that younger individuals should have been included and that the population should have been defined as people $20$ years of age or older. If those younger people who were excluded are significantly different in regard to the variables of interest, then the sample results will be biased.
	
		\item "\NewTerm{Selection error}\index{selection error}" can occur even when the analyst has a proper sampling frame and has defined the population correctly. Selection error occurs when sampling procedures are incomplete or improper or when appropriate selection procedures are not properly followed. For example, door-to-door interviewers might decide to avoid houses that do not look neat and tidy because they think the inhabitants will not be agreeable to doing a survey. If people who live in messy houses are systematically different from those who live in tidy houses, then selection error will be introduced into the results of the survey.
	
		\item "\NewTerm{Measurement error}\index{measurement error}" is often a much more serious threat to survey accuracy than is random error. When the results of public opinion polls are given in the media and in professional marketing research reports, an error figure is frequently reported (say, $\pm 5\%$). The television viewer or the user of a marketing research study is left with the impression that this figure refers to total survey error. Unfortunately, this is not the case. This figure refers only to random sampling error. It does not include sample design error and speaks in no way to the measurement error that may exist in the research results. Measurement error occurs when there is variation between the information being sought (true value) and the information actually obtained by the measurement process. Our main concern in this text is with systematic measurement error. Various types of error may be caused by numerous deficiencies in the measurement process. These errors include surrogate information error, interviewer error, measurement instrument bias, processing error, non-response bias, and response bias that we will describe below.
	
		\item "\NewTerm{Surrogate information error}\index{surrogate information error}" occurs when there is a discrepancy between the information actually required to solve a problem and the information being sought by the researcher. It relates to general problems in the research design, particularly failure to properly define the problem. A few years ago, Kellogg's™ spent millions developing a line of 17 breakfast cereals that featured ingredients that would help consumers cut down on their cholesterol. The product line was named \textit{Ensemble}. It failed miserably in the marketplace. Yes, people want to lower their cholesterol, but the real question was whether they would purchase  a line of breakfast cereals to accomplish this task. This question was never asked in the research. Also, the name "\textit{Ensemble}" usually refers to either an orchestra or something you wear. Consumers didn't understand either the product line or the need to consume it.
	
		\item "\NewTerm{Interviewer error}\index{interviewer error}", or "\NewTerm{interviewer bias}\index{interviewer bias}", results from the interviewer's influencing a respondent - consciously or unconsciously - to give untrue or inaccurate answers. The dress, age, gender, facial expressions, body language, or tone of voice of the interviewer may influence the answers given by some or all respondents. This type of error is caused by problems in the selection and training of interviewers or by the failure of interviewers to follow instructions. Interviewers must be properly trained and supervised to appear neutral at all times. Another type of interviewer error occurs when deliberate cheating takes place. This can be a particular problem in door-to-door interviewing, where interviewers may be tempted to falsify interviews and get paid for work they did not actually do.
	
		\item "\NewTerm{Measurement instrument bias}\index{measurement instrument bias}" (sometimes named "\NewTerm{questionnaire bias}\index{questionnaire bias}") results from problems with the measurement instrument or questionnaire. Examples of such problems include leading questions or elements of the questionnaire design that make recording responses difficult and prone to recording errors (the most typical error being the missing choice "other..."). Problems of this type can be avoided by paying careful attention to detail in the questionnaire design phase and by using questionnaire pretests before field interviewing begins.
	
		\item "\NewTerm{Input errors}"\index{input error} may be due to mistakes that occur when information from survey documents is entered into the computer. For example, a document may be scanned incorrectly. Individuals filling out surveys on a smart phone or laptop may hit the wrong keys.
	
		\item If a sample of $400$ people is selected from a particular population, all $400$ of those individuals should be interviewed. As a practical matter, this will never happen. Response rates of $5\%$ or less are common in mail surveys. The question is, \og Are those who did respond to the survey systematically differ in some important way from those who did not respond? \fg{} Such differences lead to "\NewTerm{non-response bias}\index{non-response bias}". We recently examined the results of a study conducted among customers of a large savings and loan association. The response rate to the questionnaire, included in customer monthly statements, was slightly under $1\%$ percent. Analysis of the occupations of those who responded revealed that the percentage of retired people among respondents was $20$ times higher than in the local metropolitan area. This over-representation of retired individuals raised serious doubts about the accuracy of the results.
	
		\item If there is a tendency for people to answer a particular question in a certain way, then there is "\NewTerm{response bias}\index{response bias}". Response bias can result from deliberate falsification or unconscious misrepresentation. Deliberate falsification occurs when people purposefully give untrue answers to questions. There are many reasons why people might knowingly misrepresent information in a survey. They may wish to appear intelligent, they may not reveal information that they feel is embarrassing, or they may want to conceal information that they consider to be personal. For example, in a survey about fast-food buying behaviour, the respondents may have a fairly good idea of how many times they visited a fast-food restaurant in the past month. However, they may not remember which fast-food restaurants they visited or how many times they visited each restaurant. Rather than answering \og Don't know \fg{} in response to a question regarding which restaurants they visited, the respondents may simply guess. Unconscious misrepresentation occurs when a respondent is legitimately trying to be truthful and accurate but gives an inaccurate response. This type of bias may occur because of question format, question content, or various other reasons.
\end{itemize}

	Therefore true margin of error is immeasurable; we estimate it with the margin of sampling error (lower bound). As it is currently being used by pollsters, the margin of sampling error is a very liberal estimate of prediction accuracy.  

	\pagebreak
	\subsection{A World without statistics}
	The following essays illustrate the importance of statistics in research and everyday life. To do this we ponder the question of what would things be like in a world (business, job, private life) without statistics\footnote{The critics (bias) of the "\textit{statistics always lie}" statement has already been treated in the Introduction (and the non-constructive arguments of statistics hater as "\textit{your statistics are funny}" or "\textit{I go prepare the meal to my $1.2314$ children...}" belongs to the same bias!)}?
	
	Science would be pretty much ok without statistics. Isaac Newton didn't need statistics for his theories of gravity, motion, and light, nor did Albert Einstein need statistics for the theory of relativity. Thermodynamics and quantum mechanics are fundamentally statistical, but lots of progress could have been made in these areas without statistics. The second law of thermodynamics is an observable fact (in isolated systems and at respectable scales!), ditto the two-slit experiment and various experimental results revealing the nature of the atom. The A-bomb and, almost certainly, the H-bomb, maybe these would never have been invented without statistics, but on balance we can think that most people would feel that the world would be a better place without these particular scientific developments. Without statistics, we could forget about discovering the Higgs boson etc, but that doesn't seem like such a loss for humanity.
	
	However:
	\begin{itemize}
		\item Statistics helped to win World War II, most notably in cracking the Enigma code (\SeeChapter{see section Cryptography page \pageref{enigma}})

		\item Statistics helps to assess observational data facts (weather change, new medication approval, new management method, new agricultural method, etc.) be eliminating the human cognitive bias and therefore to get a general consensus more quick than people debating with subjective bias.

		\item Statistics with automated softwares gives the possibility to corporations to automate decisions and analysis and therefore to be more competitive and sell less expensive services than the competitors that do the jobs manually.

		\item Statistics help to search optimum in R\&D by reducing the number of experiments that can be huge or very expensive in some industrial fields. Therefore those that use statistics will be more competitive.

		\item Statistics helps to build survey pool in an efficient way to minimize the costs and maximize the reliability of the result (this also include Quality Sampling for lot rejection in the industry!). They help also to calculate the tolerance interval (error) of the resulting survey.

		\item Statistics is used in insurance as they have been proven to be more efficient that human chosen criteria (\SeeChapter{see section Quantitative Management Techniques page \pageref{insurance}}).

		\item Statistics are used in fraud detection (extreme value analysis tools) as manually the analysis would be impossible on populations of more than a hundred millions (\SeeChapter{see section Numerical Methods page \pageref{data mining}}).

		\item Statistics are used in modern finance as they are automated with computer (therefore less expensive than a human broker) and more reliable and efficient on the long term (\SeeChapter{see section Economy page \pageref{economy}}).

		\item Statistics are used on critical large size project management to estimate margin errors on budgeting and time planning  (\SeeChapter{see section Quantitative Management page \pageref{probabilitic pert}}). In some projects the customer will not accept a proposal without modern estimations methods (otherwise the supplier will be suspected as being highly amateur).

		\item Statistics are used in Data Mining and Machine Learning to anticipate trends on financial markets or also on social networks behaviours or publicity targeting (\SeeChapter{see section Numerical Methods page \pageref{data mining}}).

		\item Statistics make it possible to predict to a certain extent catastrophic events (earthquakes, tsunami, extreme weather conditions, machine breakdowns, reliability of means of transport) to evacuate the population / users and also to optimize agricultural and industrial methods.

		\item Statistics help computers to understand natural language input in search engine or also in typing error correction.

		\item Statistics would have helped to have better modern computer keyboards (as originally they were adapted for typewriters).

		\item Statistics are used to automate the analysis of the stability (capability) of production methods and therefore to be more competitive and sell less expensive products that competitors that do this manually (\SeeChapter{see section Industrial Engineering page \pageref{six sigma}}).

		\item Statistics are used in Quantum Mechanics and Statistical Mechanics (\SeeChapter{see section Statistical Mechanics page \pageref{statistical mechanics}}) and helped to the possibility to imagine devices that would not have been discovered so quickly without (LASER, Transistor, etc.).

		\item Statistics are used in genetics in the context of micro-array analysis (see the \texttt{R} companion book) and help to do discover faster and develop vaccines sometimes very quickly.

		\item Statistics are used in Neural Networks, Deep learning and therefore Artificial Intelligence (\SeeChapter{see section Numerical Methods page \pageref{neural network}}) that will change (it's already starting) radically the 121st century (holocene calendar) and further...

		\item Statistics are used in some court judgements when the evidence of an unprobable correlation seems to appear (see Lucie de Berk case on Internet for example).

		\item Statistics are used in astronomy to support statistically the observation of new planets or new kind of stars.

		\item Statistics are used for products guarantee demonstrations plans and the calculation of optimal lifetime guarantee of new products that are sell on the market in huge quantities (\SeeChapter{see section Industrial Engineering page \pageref{design of reliability tests}}).

		\item Statistics are used in modern countries to make decisions based on evidence presented by Economists who cannot convince anyone with anecdotal evidence but with conclusions derived from Econometric models or Mathematical Programming models or variants of both.
		
		\item Statistics are used nation-wide to get information about its people: unemployment, gender proportion, wage distribution, health trends, aging, and so one and it would be impossible to take political decisions and do accurate communication without such statistics in modern countries.
	\end{itemize}
	The reader must however keep in mind that affect, fear and emotion are our on-board tags; always with us, they are operational in a lot of situations. But for others situations it is the rational brain that should take over. Averages, frequencies, standard deviations, correlations, randomized experiments, etc. ... these are things that none of our biological sensors are designed to recognize. That's why a significant percentage of the population reject the scientific method and most of it reject statistics (they're a uncomfortable with the scientific evidence).
	
	\begin{fquote}[Fred Mosteller]It is easy to lie with statistics, but easier to lie without them.
 	\end{fquote}
	
	\subsubsection{Data fallacies}
	So far we have presented in details some common human problems and interpretation issues in statistics as (in addition to the deplorable way that the mass media communicate scientific results as seen on page \pageref{scientific mainstream media communication}) the fact that correlation is not always causation, a zero correlation doesn't mean an absence of pattern, the Simpson's paradox, the Anscombe's quartet, the usage of the wrong statistics indicators or tests, $p$-hacking, etc.
	
	Some of them can be summarized in the following poster (remember that poster for when we will study Data Mining on page \pageref{data mining})\label{data fallacies}:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.2]{img/arithmetics/data_fallacies.pdf}	
		\caption[Common data fallacies]{Common Data fallacies\\ (credit: Geckoboard, source: \url{https://www.geckoboard.com/learn/data-literacy/})}
	\end{figure}
	
	\begin{fquote}Even a rough estimate, even a false one, is better than no estimate at all.
 	\end{fquote}
	
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{90} & \pbox{20cm}{\score{3}{5} \\ {\tiny 125 votes, 62.5\%}} 
	\end{tabular} 
	\end{flushright}